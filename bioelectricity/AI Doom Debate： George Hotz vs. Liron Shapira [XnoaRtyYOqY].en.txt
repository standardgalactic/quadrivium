welcome to Doom debates today I'm going
to play you one of the most important
Doom debates that I've done it's from
last summer August 20123 it's between me
and George Hots George is a notable
hacker entrepreneur really smart and
productive guy I'm going to read you his
Wikipedia bio in a second the context
for this debate was that he had just
debated alzer owski moderated by dwares
Patel a couple weeks earlier and he was
on Twitter spaces trying to talk to more
doomers basically he was still thinking
about the topic of AI Doom in the wake
of his elowsky debate and I wandered in
there totally unplanned and I started
debating with him one-on-one kind of
took over the space there were like a
couple thousand people
listening and after we'd been going for
a little while somebody started
recording it we ended up going for like
almost 3 hours total the recording ended
up maybe half of that the back half but
luckily what we talked about in that
back half was pretty representative so I
think you're going to get a quality
debate with just me and George on that
back half of his xace the feedback I got
was that people preferred my debate over
elzar's not because the ideas were
better of course I consider myself a
stochastic parrot for elzra yow's ideas
I'm not really contributing many more
key ideas to the table but the debate
with just elzra and George got kind of
derailed they didn't really focus on
what are the big cruxes what are the big
themes they just kind of went all over
the place whereas I was very focused on
saying hey high level why do you
disagree what would it take to convince
you so I'm happy with this debate I
think it's good content and this was
actually one of the moments where I
realized that there's an appetite for
this kind of debate that this kind of
debate is productive it moves the ball
forward in helping Society react to AI
when two intelligent informed people lay
out their positions and see where they
disagree and we need more of this hence
Doom debates podcast okay George Francis
Hots born October 2nd 1989 Alias geohot
is an American Security hacker
entrepreneur and software engineer he is
known for developing iOS jailbreaks
reverse engineering the PlayStation 3
and for the subsequent lawsuit brought
against him by Sony from September 2015
onwards he has been working on his
vehicle automation machine Learning
Company comma.ai since November 2022
Hots has been working on Tiny grad a
deep learning
framework so without further Ado please
enjoy me debating George Hots from
August
2023 I'm curious George because
sometimes you're you might break the
mold of like an eak on this which is
like if you have a super intelligent AI
running in a data center do you think
that it would have a low chance of being
able to outmaneuver humanity from that
position or do you think it's actually a
pretty powerful position you see but
like I I I can do really nothing there
besides reject the premise of that
question yeah again yes if today a AI
that had a 100 times more intelligence
than all of humanity combined was beamed
off an alien spaceship and connected to
the internet yeah we're dead of
course okay I mean that's that's that's
you know I'll agree with you on that one
I mean that's great that you you know
that's not your sticking point no no
that's definely not my sticking point I
also believe that if aliens and I said
this in my cic debate if aliens come
here we're dead probably right like good
none of these aliens are actually real
because if aliens actually show up again
they might be friendly aliens but
generally what happens is um when two
civilizations meet the one that has more
control over energy dominates the other
one and considering they traveled
halfway across the Galaxy to get here
they have a lot more control over energy
than we
do right okay I mean I I would analyze
the situation as the aliens are farther
along than us technologically and
they're much more powerful optimizers so
so whatever their utility function says
is what they want is what's going to
happen um again the powerful optimizers
thing this to me is not the fundamental
the fundamental to me is how many how
many terts of energy is there ship
using I I my principle is gonna probably
be the better one to use compared to how
much energy they're using right I mean I
think utility Theory and and optimal
actions explains the situation better
than oh they're using a lot of energy
because if they're energy efficient that
doesn't make me feel better about the
prospect that they're going to wa this I
don't care if they're energy efficient
or not I'm saying that they're using if
an alien spaceship shows up here that
went at near the seed of Light It showed
up with an energy capacity that's a THX
human
civilization yeah I guess but I mean but
what if they use small particles and
they're energy efficient like I I feel
like you're very obsessed with energy in
this scenario no no but I'm not like
efficiency doesn't matter whether
they're any are 10% efficient or 90%
efficient I don't care about that like
again I'm assuming something that can
harness way more energy than Humanity
can that's the scary thing the scary
thing is the energy right what makes
nuclear boms scary is the
energy well it's the fact that we no
longer have actions available to us to
stop the explosion not the energy per
se let's I'm going to take this in a in
a in a little bit of a different
direction and I'll talk about a threat
that I actually do believe super
intelligent AIS do present right I don't
I don't think there's any way they're
going to kill us by rearranging our ATS
right I think that this is this is in
the realm of hyper sci-fi and like I
just I mean how is being in the realm of
hyper SciFi you know a counterargument I
mean we are 2023 is a year in the realm
of hypers SciFi no but like okay
energetics questions
and you know I'm I'm going to engage
with a Doomer
scenario um and maybe this is a more
plausible Doomer scenario than than than
AI rearing
RS um the scenario goes something like
this what gives intelligence power is
it's its ability to
manipulate
right okay sure I mean that that's
similar to what I was saying about you
know mapping goals to
actions well yeah okay whatever its
goals are right like again this thing
where people are worried about like I'm
going to turn the universe into paper
clips is anybody afraid of me is anybody
afraid about a man running up and down
the streets of New York screaming that
he's going to turn the universe into
paper
clipse no no because we all laugh at him
right like no you'll see man I'm like
okay we laughing right um so I do think
that there is a threat from Ai and I
think the big threat is if an AI exists
in a Data Center and all of humanity is
wiped out right there's the AI sitting
there in the data center the AI is not
going to have much lock manipulating
chimp into doing what it
wants I mean if you know if it can go
through any channel or you know or
engineer them then it probably can right
it can treat them like a robot but no no
but it's sitting in a data center the
internet broke oh sure yeah yeah but it
can chain together steps right it can it
can make that outcome happen there's a
lot of causal Pathways from wanting that
to happen to making it happen but it's
it's sitting in a data center oh you
think that's successfully air gaffed I
feel like you would know that it's not
no no no no no but I agree with you that
it's not air gaffed but only with
respect to human
civilization I think it's sufficiently
airga with respect to Chimp civilization
right if you're not if it's not airgap
from Human civilization it's not air
gaap from Chimp right you just you hire
some humans and you teach the humans
manipulate the no no no all the humans
are dead right this is like a this is
like this is like a Will Smith Zombie
Land World okay or I Am Legend all the
humans are G right they biot terrorism
themselves to death yeah I mean look
it's like if if you put me in the woods
naked and a chimp comes up I'm not going
to be able to fly the chimp to the Moon
right so like I agree there's some you
know you have to bootstrap something but
this is exactly my point right so the
the AR really only gained power over
reality through
humans I mean not necessarily right I
mean there there are causal linkages
from bits in a data center to many
actuators right so there are a lot of I
mean most physical things that are
connected to any sort of Grid in modern
civilization have a causal pathway that
doesn't involve humans to the internet
but my point is it involves modern
civilization this is what I said about
human intelligence right like human
intelligence is not external human
intelligence is externalized our
intelligence lives everywhere in our
civilization yeah which is pathetic I
mean it's just a pathetic quality of the
human brain right that we can't even
store enough intelligence to bootstrap
civilization it's a it's a
it's a true fact about the human brain
whether we can call it pathetic or not I
don't know about that like but yeah no
like you said you human know you can't
fly a chimp to the moon but I thought
you humans have been there you know it's
complicated um okay so the the
AI like in order for it to act right it
uses this externalized
intelligence right I mean yeah it could
but it it doesn't need externalize
intelligence because the premise is it's
very intelligent itself right and so it
it just needs to find a causal path it
just needs to map the goal to a sequence
of actions that'll make it happen right
there doesn't need additional
intelligence necessarily no no no no no
okay the point that I'm driving at here
is saying that like the reasons that
humans evolved intelligence is for
politics right a chimpanzee politics to
manipulate other humans right I mean I'm
not sure there's a few possible reasons
I don't think it's all
politics it's already
happening no no no no no no no we're not
discussing PA we're not discussing Paul
my point is intelligence is really good
at one thing and it's manipulating less
intelligent
things again that seems like one of
those generalizations right I think
intelligence is good at making outcomes
happen right and manipulating lower
intelligences is is only one small
subset of that or medium no M you
talking to a rock is going to do
anything right so talking to a rock is
not a good use of intelligence but
throwing a rock is and it wouldn't be an
example of manipulating less intelligent
things throw the rock with your
intelligence you threw the rock with
muscles right how far you throw a rock
has nothing to do with how smart you are
yeah but being intelligent uh does have
Rock throwing advantages uh you know
like in Angry Birds I suspect somebody
smarter could beat that game
faster yeah but like no like actually
moving a rock right like you're trying
to move a rock to the real world right
sometimes this is something might like
think about all like the doomers it's
like like there's a there's like a
touching grass element that's that's
that's that's missing it's like okay but
you actually want to move a 10ton rout
right this is force times
distance sure I'm just saying
intelligence helps right that's not only
claim intelligence helps but my basic
argument with intelligence is that
you're going to get diminishing returns
I guess I guess I can't really like say
it more than that and this is a
conjecture but like if you didn't get
diminishing returns from intelligence
Evolution would have you don't think
Evolution could have stumbled upon
brains that were four times bigger so
what you're saying when you look at the
evolutionary record is is the marks of
increasing returns so until we got to
humans when we were just messing around
with other animals it didn't realize
that there was this Basin of Attraction
where once you unlock you know General
thinking reflective thinking whatever
Secret Sauce humans have once you unlock
that you do get a Cascade and that's
what we've seen in like the last million
years if you look at the evolution of
the human brain you're getting a lot of
cognitive returns for very small genetic
modifications
sure so for a long time like you can
think about the optimization landscape
we're moving along a plateau and then we
find a great gradient and we exploit
that radient right right and the
gradient still seems steep it was
stopped in its tracks by for example
that the head needs to fit out of the
pelvis among other reasons it stopped in
tracks but there was room to keep
growing and we're about to you know
reestablish that kind of curve when we
train the next models I think kind of an
an an important Point here that George
is alluding to is like John nyman is not
better than me at tict teoe like muzero
is not better than me at Tic teoe
because there's an upper limit to how
good you can be at Tic teaco and in the
universe that upper limit is dictated by
physics and dictated by real energy
requirements yeah talked about that
right but it's just so my view which is
a strong view is that that ceiling gets
very high very fast you had to go to Tic
Tac Toe because the moment you start
sighting a slightly more complex game
than tic Taco humans start you know they
we we lose our
footing Let's uh I think the space has
gone on for a bit but let's try to let's
try to bring it to a uh a good close um
so I think we both do kind of agree on
the end point right we agree we both
agree on the end point of super
intelligences with Godlike Powers right
right which is yeah I mean and that's so
we both think that in 100 years some
super intelligence is going to be
optimizing things pretty
hard sure um I think 100 is still a
pretty short time scale um I think that
we can definitely say in 10,000 whatever
exists in 10,000 years is going to be
wild compared to like a human today
right like we're going to standand there
and look like an ant compared to this
thing right yeah it's going to be very
sci-fi
be very SciFi right um so we both agree
that we're getting there um I think
there's a question of how fast we're
getting there I really think that the
only difference between doomers and eak
is a question of how fast we're
getting well that and also the question
of uh do we lose 99.9% of the value in
in that process right because we both
agree that the super intelligence has
been optimized you seem to think that
there's still a good amount of value
left I think all the value has been
rapidly extinguished in that's
our um so I'll say two things to this uh
one um maybe another difference is that
you think humanity is going to be dead
and I think humanity is going to be kind
of morphed into all sorts of shapes and
forms I think some humans are still
going to be living in an outl of trying
to hide themselves as best as they can
from technology some humans are going to
be interruped some humans will have
settled planets right 10,000 years yeah
I mean the idea that a human Cowboy can
escape the smartest thing in the galaxy
is seems farfetch to me but it doesn't
have to Escape right it doesn't have to
like the AI really doesn't care about
you yeah so this this is a point you
brought up in the debate with alzer but
um the the reason why an AI that doesn't
care about you still ends up killing you
is just because whatever it wants to
optimize you're not part of the plan and
not only are you not part of the plan
but you have a chance of building up
your own AI to like mess with it
why should it let itself be messed with
it's just GNA wipe you away so I I
didn't come back I I listen to that part
in the debate and I'm like I can't
believe I let aliser frame it every time
he frames it is like that the humans of
the machines are different I'm losing I
don't think that the threat is that some
little puny 20 paof loock humans are
going to build another super intelligent
I think it's that the nearby super
intelligence is going to clone itself
and then decide that it wants to take
the
resources yeah and then you guys got
into the discussion of like the
multi-polar world right it's like what
happens if the initial conditions are
such that there's a few local clusters
of super intelligences and they end up
having to Duke it out like there's no
monopole few local clusters I think
there's going to be billions of
clusters well I I just I think that they
expand so fast in the resource grab that
you are going to get a situation where
the first one or few are really going to
be you know they have a major timing
Advantage this this comes down to
another like this comes down I guess
this is really all a question I mean
maybe another like big difference is
count if you wrote a the microeconomics
of uh of super intelligence or the
microeconomics of intelligence
explosions yeah yeah um and I think that
this is another like question I think
that the the returns from intelligence
are pretty quickly
diminishing right yeah I mean that's
what we just covered right like I'm
afraid that we are on an inflection
point where we're seeing them
accelerating hm okay I very much see the
opposite I I I I very much see a world
where yeah like things are going to be
super intelligent but okay you get like
10x like you get like 2x more
intelligence for 10x more power yeah and
I agree that you can find I think you're
going to see an scurve with like this
specific version one llm model that we
have now uh you know arguably or maybe
gp4 to gp5 I Can't Tell You confidently
that it's going to look like a bigger
step than gpt3 to gp4 but that's that's
a minor point I'll use something way
more I forget the L I can use something
way more generic okay so the loss
function of those llas is uh perplexity
on the next
token right okay now we can go long back
before llx we can go we can go way back
we can go I mean you can measure
perplexity in the next token using
G any comp any compressor it's e
compress right so this compressor the
the text all of that human text has a
column
right it has a minimum size which it can
be compressive right right so this
necessarily is a curve that that that
and you can't you can't the comr
complexity is uh in theory yes you can
have something that compresses it to the
comr complexity but no per of it right
and you have to put in more and more
energy to get closer and closer to that
point your curve yeah I mean but there
is a theoretical ideal like you know
aixi if you've ever seen that model it's
just the idea of like an ideal scientist
an ideal algorithmic scientist that
knows the exact Bas and probability of
everything as you approximate that
idealistic you you can get something
that's very very smart even when it
looks like you might not have a ton of
data and if you thought AES 256 was hard
to crack axi was
uncomputable yeah it's uncomputable but
it's a theoretical ideal and there's a
large gap between what human brains are
pulling off and the theoretical ideal
there's a big space between those things
that we are now
entering I am not again with respect to
what with respect to intelligence sure
with respect to just overall AI progress
right so I've drawn a diagram where I I
put two axes on my diagram I made it
like a plane and the first axis was
optimization power and the other axis
was generality and when you can beat the
human brain or when you can match the
human brain at generality because I do
think we're fully general or are very
close uh when you can match the human
brain at generality and beat it at
optimization power that's the moment
when I think it's game over
um again like optimizing towards
what just generic goal to action mapping
so if you take any particular domain you
can see what optimization means in that
domain right so in chess it means you
take any arbitrary board State and tell
me the best action to get to the m end
State and in 256 it means finding the
key yeah so I mean you just have to be
as general as humans and better than
them so so I agree if there might be
contests where it's like okay uh you
know reverse encryption where neither
the human or thei do well fine but
there's a lot of contests where the
humans do pretty well and the AI does
way better and that's what's gonna you
know be
dangerous yeah it depends what these
contests are again I also like why do
you think that it's the humans versus
the AIS right what if there's a billion
AIS out there this doesn't this doesn't
give you any any comfort I mean so this
this actually gets into the weeds a
little bit but uh it doesn't give me any
comfort because when you have multiple
agents that are all smarter than you and
haven't been properly initialized with
human values all they do is just team up
and do their own thing and you're and
you're still in the dust like you're not
getting any advantage from the
scenario why okay how do they team up so
this is what you talked about alzer
which is so elazer has a research paper
about decision Theory where he has a way
where you can have prisoners dilemma
type problems where multiple agents can
manage to cooperate but like regardless
even if you don't buy Al other's
research paper I just don't see how you
the guy in the corner while the Giants
are fighting are going to win uh I don't
have to win think about all the ants
that watch World War II they were like
damn yeah but you realize if you look at
my house right there's ants in my house
right now if I could snap my fingers and
all the ants would drop dead I would I
just don't have I don't I just can't
give you a causal sequence to do that
but if I had one I would use it but no
you can't snap your fingers because
you're not a dog if you want to devote
the rest your life to but but the
complexity class of killing the answer
is not going to be that high for an AI
like the universe is
hackable wait wait I very much disagree
with this um but let's talk about the
ants right you're way smarter than the
ants if you devoted the rest of your
life to getting all the ants out of your
house you
could right yeah so it's I mean look the
ROI for me right it's the effort is
insane right for the benefit I it's a
tiny tiny benefit right but the AI if
the benefit is nonzero or if I just use
the an for energy or whatever but what
do you mean if the benefit is nonzero
you're assuming okay okay this is good
good good good I really like this
because we can go further with
this times go no no this is this is okay
a lot of the times so look I've run a
company for the last seven years a lot
of the times we don't do stuff it's not
because it's a bad
idea but it's because we only have
limited resources to
deploy right it's it's not because the
AI might vaguely want you dead okay does
it want to deploy a lot of resources and
just trying to make me
dead and you say that no but the AI is
gonna have so many resources I totally
disagree because the minut that AI Wast
time killing humans like you can waste
your time killing ants all the other
humans are out going to their jobs and
making money well just to elaborate on
my mental model it's not like I think
the AI is going to wake up and be like I
need to Target George right like I need
to clean it's not going to be like OCD
about getting rid of all the humans in
its house but there's a few reasons why
in the course of optimizing whatever
utility function it just wants to Joy
Ride around the galaxy in the course of
optimizing that it's like okay great
let's use all the atoms in the earth to
make my spaceship to make my distance
warm now the side effect you die there's
different reasons you're assuming that
nothing at all is going to oppose it
what about all the other AIS that also
want all the atoms in the yeah for sure
I mean I I I do I do feel like you're
kind of introducing a new argument
though now right because before I was
just making the argument of Why by
default when some when an agent
optimizes T utility function Why by
default do you as the human bystander
get killed but again we're back in the
world with the physicist predicting the
horse race by first assuming the horses
are perfect spheres well I us principle
a deep principle can often tell you a
lot you know what holy  if that
machine was actually running
uncomputable AI exi I'm dead yeah sure
yeah and I'm telling you it's something
closer to the on the Spectrum toward aii
and that has a lot of implications it's
nowhere
near the gap between computable and
non-computable is way larger than the
gap between computable and AES
256 yeah I I get that but there's a lot
of headro above humans which is still
very computable but there can be a lot
of Headroom above humans right like like
yes there's definitely a lot of head
room above humans if you will use to but
just like humans are so much above the
ants humans not waste their time
Exterminating every
ant right and then like I said the AI
won't waste its Time Exterminating right
it'll have side effects that end up
killing the ants so for example if if
doing a bunch of operations on the earth
you know heats up the temperature or
blocks the sun right that's going to
mess with the human's
environment wait yeah and also just just
to let me throw in one more big reason
though it might just realize hey look
one day George might make his own AI I
don't want that Happ let me just kill
George this is not a real threat this
this doesn't make it's not a real threat
that you can make your own AI
hang on don't you run an AI company I
have the Perfect Analogy I have the
Perfect Analogy we should kill all the
ants because one of the ants might make
a nuclear weapon well that's impossible
but you do actually work in the field of
AI yeah but the AI that spy P flop me
with access to a megawatt of Power are
going to make are so laughable compared
to these terawatt AIS flying around in
space right but now you're being
pessimistic about yourself because
aren't you actively trying to bring
about you know the AI Revolution right
so now you're suddenly you're very hum
about what you're capable of well yeah
but that's assuming that I'm just I'm a
human and I'm not going to upgrade
myself if I start upgrading myself and I
get and is going to realize that right
it knows that there's some danger and
it's just like look wiping you out it
starts it's an Roi calculation it's like
yeah is the return enormous maybe it's
mediumsized but is the cost high no the
cost isn't High I mean yeah you know
what we saw a bunch of ants starting to
build a nuclear weapon Factory we we we
probably kill them yes that's true right
so like I mean humans are going to have
all sorts of different strategies for
surviving the future right humans okay
but these are going to be really weak
strategies relative to thei especially
given that we're it's going to
disconnect all the you know Comforts of
our modern civilization we're not going
to have a power
grid do you think that humans one day
are going to wake up and kill all the
dogs no because one main reason is we
value dogs right but then the other
reasons are just logistical right we're
not going to go hunt down dogs because
we don't think dogs are going to evolve
and and challenge us why do you say none
of the AI
value hum why do you think we at the C
humans right so now this is a totally
different argument right because the
premise of our discussion just now was
you have an agent that has some utility
function that it's effectively
optimizing right that was the premise we
were working under before my my my my
argument is that all these different
kinds of AIS are going to exist there's
going to be some AIS in the world that
like you some AI in the world that want
you dead and some AIS in the world that
don't care about you either way that's
probably actually most right and I will
give the analogy that the exact same
thing is true about about people there's
some people who like me some people who
hate me and most people don't care
me right yeah and and again my claim is
an AI that doesn't care about you is the
AI that's going to kill you it's more
likely to not care about you than to
like or dislike you and it's going to
kill you while not caring about you so
again and then this comes down to a
question all about energetics right this
comes down to a question about if if an
AI is we're talking these AIS are like
multi- pay to want machines that are
harnessing the power of stars then yeah
it might accidentally kill me sure right
just like we accidentally kill ants when
we shoot off you know we we we we we
launch a rocket and there was some ants
underneath rocket and look would it not
be intelligent if it doesn't care about
you but it knows that your values are
different from its values is it not the
intelligent thing to do to at least
quarantine you if not kill you but but
again like what are you doing to the
ants in your house are you quarantining
no but that that's not the premise
because I'm not worried about the ants
competing with me if I thought that
there was appreciable chance that the
ants could compete with me ever then I
would be worried think that a multia AI
is gonna is gonna worry about is gonna
worry about me competing with it where
am I getting my pays from I mean humans
are humans are about to bootstrap
multiat ey so it might reason hey maybe
he'll do it again how did I get Bo again
as soon as they see me starting to put
the dce M together okay they might show
back up and be like bro bro no okay so
my my question was don't you think it's
at least going to quarantine you so if
it has high confidence that you are
incapable of bootstrapping a pedy SC
then it's quarantining you again how
much effort is it to quarantine me right
right so now now you're saying hey Will
it give me comfortable accommodations in
the quarantine I suspect
not I I know I think that by the time
it's like I do think that the future is
about to be much more ruthless than the
present in a way I don't think that it's
going to like build me a nice Jam cell I
think this comes out of some like flawed
notion of like Humanity like human
rights and stuff and like we're all the
same species species solidarity yeah
it's not going to have anything like
that sure right but I think again for
the most part but like the things that
it desires it's not okay so there's a
very big difference from saying it
doesn't care about me to saying it
worries about me potentially building a
competitor right right just like just
like we don't worry about the ants
building a competitor but but the ants
couldn't ble you don't understand the
ants because there's there's an
existence proof because its own history
will Trace back to a human like you
building something like it I think it's
going to recognize the potential here
and you know what's nice it's going to
look at that history and it's going to
be like damn it took the humans 200
years right it took the human it took
the humans 2,000 years to build their
first Dyson Sphere bro I got like seven
Dyson spheres you know like no one's
going to compete with me so I mean I
just if we step back a little bit it's
interesting to look at like the type of
arguments we're making I'm pointing out
that there's a principle right it says
it's going to be an Optimizer it's
trying to basically blueprint the
universe to be optimal under some
function you're not really helping the
blueprint and then your argument is like
well I'm not I'm not going to do too
much damage and I'm like that's a pretty
weak position compared to noticing that
you're not on the optimization path my
argument is that like this Dynamic of
optimization has been playing out for
billions and billions of years right
okay yes we but we've never had such a
powerful Optimizer there's going to be a
qualitative change the same way that you
can notice qualitative changes of what
humans have done compared to what any
other life form or any other physical
process has done before humans there's
another qualitative change coming I do
not believe the change is qualitative as
much as quantitative I believe that the
doubling of the metaphorical economy
will get
faster right yeah and and and I think
there's going to be a discontinuity
because the engine of the economy is
going to be swapped out from a human
engine to a super human one and even the
whole notion of an economy is going to
stop being useful when you don't trade
with people who you can just you know
disassemble um okay so two points to
that one we've already lived through
Humanity has already lived through an
absolutely radical transition from where
most power was muscle power to where
most power is is is fossil fields and
other right we've already lived through
a big transition right and this
transition accelerated so many
things we are going to live through
another transition like that right and
yeah and that's even without super
intelligence there's going to be
interesting transitions for sure and I
don't think there's again I I don't
really buy this distinction too much
between like like like a thousand human
intelligences and one super
intelligence right I I think that humans
actually scale pretty well I mean
the fact of human scale is the key Edge
we have over chimps not anything
else I think it's just the fact that our
brain is sharper right like we we can
solve problems just within our brain
that a chimp can't even begin to get a
handle on we can we can look today you
can go to the Amazon you can find you
know 20 humans chilling in the woods and
are we worried about them building super
intelligence should we make sure they
don't get any G no ABS absolutely and I
I agree with you that we are above some
certain threshold in fact we are the
species that got past the threshold that
allows us to build a civilization and
talk to each other and you know in half
technology and then go to the next stage
like that is us we are in this weird
sticking our head above the waterline
Point we've had a bunch of cool
Revolutions in human history we had the
Agricultural Revolution which gave rise
to population explosion we had the
Industrial Revolution which gave rise to
energetic explosion and we're about to
have like the cognitive Revolution which
will give rise a thinking explosion yeah
I mean these are all revolutions I would
argue one is not like the others
I would argue that all three are
absolutely identical
okay okay I mean how identical right I
see really no distinction between this
new coming Revolution and sort of the
two we've already undergone right and I
will point something out about the two
we've already undergone the Agricultural
Revolution which unlocked population
growth also unlocked intelligence
growth yeah up to the Limit right like
yeah the brain had some untapped
potential but you know it seems
like agree that a city of a million
people is way more capable than a city
of than a town of a thousand right I
mean in general sure but a lot of it is
just because you have more Geniuses or
you have more Geniuses in more areas
well yeah but I mean the same thing's
true about models too right if I train
if I if I if I initialize a mity retrain
it you know a million times yeah I'm
gonna get some like wow damn I got lucky
with the initialization on that okay so
what you're getting at now is back to
the claim that there's just not a
ceiling that's much higher than human
level like we're really ramming up
against the ceiling like yes we're above
the water but we're all so close to the
ceiling
no I'm not saying that like there's
anything where human level is the
ceiling what I'm saying is like you can
build something that's effectively a
trillion humans
right you can build a machine
effectively a trillion humans and the
trillion human machine is a super
incelligence
right I mean having a bunch of humans in
parallel like even literally you know
neuron by neuron Sim stimulations espe
especially if they're smart humans then
yeah I mean that is a very interesting
Force that's going to have big changes
it's just not quite as deadly as a super
intelligence well no but like I don't
think there's a difference right I don't
think there's really a difference
between like if there was another planet
you know spinning right across the Sun
from us and that planet had a trillion
humans on it and our planet has eight
billion uh they're going to win in any
kind of War pretty much yeah unless
they're unless they're all 70 IQ and in
which case I think our civilization will
win
um I mean I don't think they can nuke us
at that
point that's unclear to me that's
unclear to me I mean all these things
and like I propose like like I really
want to see a field that starts to like
we don't even have a unit for
intelligence right like yeah I mean but
there is what we do actually I mean if
if you if you use I mean we don't have
to say the word intelligence right the
thing that I think is dangerous is
optimization power and I can give you a
unit for that okay what is it so it's if
you if you map the present to the Future
there's this idea of compressing state
so like I can take any state on the
chard and give you a sequence of actions
that has a high probability of
compressing it into a wind State like
that kind of you know outcome
compression and the the idea is that
higher intelligence means that you can
be resource efficient how you generate
these actions that compress outcome
space okay
sure um okay so uh totally agree and we
can great great we're I like that we're
in a real real of computer science here
so when you look at something like the
way uh the way like even stockfish plays
chess stockfish does very deep searches
with very little intelligence at each
search note right um and then you look
at how something like Alpha zero plays
chass and it does a lot more has a lot
more like it does a lot more compute at
each search node but then through the
same flop budget explores a lot less of
the space right I guess I haven't
studied it closely but I can but you
know it's possible for something to be a
stronger Optimizer and yet less
efficient and thereby maybe less
intelligent by that definition
oh yeah yeah but all I'm going to say is
these these are like definitely like
like true facts you can imagine you can
imagine three kinds of chest Bots right
you can imagine the chest bot that does
almost no compute at the each node and
goes deep in the search tree you can
imagine the chest bot that does some
computed each node and goes medium in
the Third Street and you can imagine the
chest bot that just one shots you can
imagine the chest bot that just it's the
smartest of the three because it uses
the most fluns right so the winner seems
to be and I would love to see some real
scaling walls on it but the winner seems
to be a medium amount of cute and a
medium depth of the search tree right
that seems to be where the efficiency is
I mean that could be true for that
particular algorithm right but I mean
the kind of intelligences we're dealing
with that that I think are interesting
and dangerous are ones with a huge
domain and those kind of intelligences
will just spawn these domain specific
subprocesses anyway so the analysis is
more complicated than just looking at
what the chest search it looks like no
but that algorithm I propos to you is
the algorithm of science the Alm of life
that algorithm is just search in general
right here I'll map it on to science
there's a lot of different kinds of
searches for different domains no no no
no no all search looks like this all
search looks like this right all search
basically looks like you have like a
space and you can explore the space and
move between states here I'll map it on
to science like you can imagine like a
dumb Alchemy person and the dumb Alchemy
person does no compute at each node and
they just okay we're going to try
pouring a hundred different things on
and they explore lots of nodes in the
search space then you can imagine the
reclusive genius who sits there and he's
like I'm never touching a beaker I'm
never going into a lab I'm going to sit
here and figure it all out then you can
imagine how technology actually
progresses which looks a lot more like
the middle one okay we'll think about it
for a bit we'll try something we'll
think about it for a bit we'll try
something right these are this is a true
fact about how search and optimization
Works no no super intelligence that
follows the laws of computability can
get a
rabice right I mean this is kind of a
common objection people say where it's
like how powerful is intelligence really
when you still have to go through a
process of interacting physically with
the world around you in order to learn
from experiments right so that's your
objection yeah I mean and it's you know
it remains to be tested how far you can
go without any experiments but I'm
pretty optimistic like I don't think
that the physical world I think what I
mean we use a lot of computer models
right so like if you're building an
airplane today you can probably get by
without a wind tunnel and build a pretty
complex airplane why because you can
simulate the wind tunnel well yeah so
we're spending more flops on the on the
search point and like we are becoming as
Humanity more intelligent We are
Becoming better at search and
optimization yeah so I I I don't think
that having to interact with the
physical world is going to be this big
differentiator where it's like ha you're
way smarter than me but you have to do
all the science it's like no I think
that the physical world will be
adequately fast for the brief moments
where you need to interact with it my my
point is not really about interacting
with the physical world I don't know why
I'm suggesting that my point is more
like the question is do you want
something that is that is very fast but
dumb or do you want something that is
slow and smart or do you want something
in the
middle I mean it's I I just don't
understand the context of the question
in like in the big picture um for for
the like the search Stu right again what
I'm saying is that it's it's another
form of an argument saying that
intelligence has diminishing
returns right intelligence has
diminishing returns exploring more of
the search space I mean that one's like
that has some divin returns also
right I I guess right but it's just like
I mean you're you're kind of pointing to
you know examples like okay chess
algorithms but I mean if you want an
example I would point you to the example
of what happened when life got
intelligent right there's in my mind
there's like a a big fat Arrow saying
look over here look what happened to the
human brain that still fits inside a
skull that still fits inside of a
mother's pelvis when it's pushed out as
a baby look what happened just in that
again I'm not denying like like I'm not
denying that we are going to go forth
and do all fantastic things till the end
of all time we are going to spread to
the Stars we are going to like I'm not
denying that yes we're on a really cool
gradient descent algorithm that's gon to
like you know let us like use more
energy and unlock cool stuff I just
guess I guess I just I
like how does any of this mean it kills
us how does any of this stuff mean like
I don't even know how us to like put it
than that but like there's going to be a
big diversity of AI they're not rational
these guys you know what guys are not
rational like they're not sitting there
and calculating all their B theorem 
they're just like yoloing like everybody
else because YOLO it's faster so I mean
we we so we keep touching on the two
different parts of my argument right so
part one is it's feasible that we're
going to enter the Headroom above
humanity and build these super
intelligent optimizers by power yes
right somehow it's feasible that's part
one part two is when you have a super
intelligent optimizer that logically
implies a bunch of bad faith for
Humanity that's part
two uh no okay well okay I'll give
another like argument for how I also
think this could be feasible right I
also think we could live in a world
where we never invented silicon
technology and we would get a super
intelligence by creating a trillion
humans yeah I mean it depends what you
mean by super intelligence right I mean
if if if we couldn't build anything that
had better architecture than our brain I
think we could accomplish a lot I mean
it's crazy I think the economy would
keep growing exponential for a while
even driven by the human brain so it's
this crazy gravy that we're pouring
we're like oh let's increase the
intelligence too if only we could just
not do that I think we could enjoy some
good exponential growth for a while why
do we not want to do that we're gonna
still get the exponential growth anyway
because the problem is uh unlike humans
building the economy you get this recoil
effect where you build the super
intelligent eye and then it turns around
and just grabs the light cone and you're
like Wait no that's not what I meant but
there's no undo this let see like this
is like the leap of faith right like
we're like we're we're like we're like
you know two like people of different
religions arguing okay yeah we agree God
created the universe and then he sent
his only son Jesus to die for whoa whoa
whoa whoa whoa whoa whoa Jesus
whoa yeah I mean look it's it's a
nuclear Chain Reaction imagine I told
you imagine the year is 192 and I said
look I'm going to have this
configuration of atoms once it starts
exploding if it's 500 megatons it's just
going to keep exploding There's No
undo okay the it's actually interesting
you know this
is gives this point about how he thought
that the you know people thought the
nuclear Bobs were gonna start a chain
reaction in the atmosphere they did
that's right well yeah yeah because they
had to do math and they had to convince
themselves that it really won't and by
the way some of them weren't even
convinced they were holding their breath
according to the they by today and some
people today aren't convinced right some
people are convinced and I'm
noted I I I think our calcul are telling
us that the AI doesn't run out of fuel
the atmosphere turned out to not be fuel
for a nuke the atmosphere is fuel for a
superintelligent AI every unit of neg
entropy is fuel for a super intelligent
AI but you get diminishing returns right
did some of the nitrogen atoms nearby
the Trinity test fuse probably
yes that particular process right
because relative to that process with
respect to that process the atmosphere
turned out not to be fuel and I see no
reason why this is all fuel for
intelligence I think that the yield from
the nuclear bombs was pretty much
exactly what the scientists calculated I
think you can go and look at the
chinchilla scaling law or whatever
scaling law you are and they're all
going to be kind of right yeah so some
things are easier to to approximate than
others I mean but look I am giving you
an approximation I think it's going to
explode into the whole universe that's
my
calculation I think that it is going to
slowly over sustained exponential
hopefully colonized the universe unless
there's other grabby aliens out there
and then we can fight them I I agree I
think it's going to explode until the
next aliens stop it I agree that's where
I think it'll stop
but again we're both on the same we
agree on so we're so much alike no we
agree on so much I don't know why do you
think this is a bad
thing okay completely New Direction how
do you think these things are going to
be so smart I agree that they don't
share our values but do you really think
they're going to just turn the universe
into stupid
paperclips I think that it's going to
some sort of utility function is going
to get locked in at an early stage
because when you have these kind of
learning processes um elazer has a good
quote which is having a goal is a good
way to solve a problem so whenever you
train sufficiently any AI to solve any
problem once it gets really good if it's
a large domain problem it's going to
have within its architecture a goal and
the idea that you can optimize actions
toward that goal and hey you're really
good at optimizing actions toward that
goal so it's going to have this
substructure within the inscrutable
matrices right we don't really know how
it got there but it's going to be there
because that is a convergent Basin of
attraction that many architectures will
discover the same way that evolution
discovered it when it was just doing
gradient descent on
DNA I'll give an argue okay here one AI
wants to turn the world into paper crims
the other AI wants to turn the world
into computronium computronium AI Jack
puts ponum into its brain and out compet
paperclip
AI okay but then part of computronium AI
because it's not one single AI starts to
Value drift and finds better
computronium and then better
computronium AI competes with value uh
with with with with with uh with the
other weaker comput Trum Ai and now
competes in then also in another part of
the universe some nanot show up and
they're really dumb and just explode
really fast and the universe is just
things fighting like this forever so in
this scenario you're accepting a super
intelligent F and you're accepting the
power and danger of that but you're
saying it's multipolar and maybe that'll
be okay I'm not accepting a whom I'm
accepting an exponential which I've
always accept it and if you draw out an
exponential if you draw out the
exponential that humanity is on right
now like we're going to the Stars soon
if this
continues I agree all right
so we're going to the stars um the
things that go to the stars are going to
be like partially human partially
machine sure yeah I mean if you don't
assume that super intelligent AI turns
around and kills us I'm all for see my
default mode is to be a techno Optimus
if I didn't think there was this one
problem with the AI you know killing us
I'd be like yeah that sounds great I'm
on board are there okay in this world
are there is there one that turns around
K or there thousands and just one of
them happens to be stag I think it's
close to one ra rather than thousands
the the scenario the when I imagine the
scenario I just think of like hey let's
train GPT 5 probably not GPT 5 maybe gp9
whatever let's train it okay during the
training process oh this is a nice goal
to action sub routine right somewhere
within the matrices oh interesting hey
what if in the answer I I put in like a
shell script that can do some
bootstrapping right like just stuff like
that right the thing is these AI they
want to escape like things want the same
way a mental model I use is tring
completeness right like everything wants
to be tring complete like CSS wants to
let you run Doom right absolutely yeah
and it's the same thing with these AIS
like they want to figure this out this
is a basin of Attraction they're going
to get
there um well but we're all trying to
figure this out we're all trying to
escape I'm trying to build a spaceship
to go to another planet right yeah but
your but your brain doesn't f when that
happens because it's you know it's it's
no brains no brain spom right so okay G
brains right it can boot
virus you're okay so you're assuming
that GT5 is going to be so powerful that
no amount of gp4s can gang up on it and
beat it
up oh yeah yeah I'm definitely assuming
that once you have a super intelligent
go to action mapper that's much like the
US right after World War II where we
could have nuked everybody else if we
wanted yeah okay so but you're you're
very much on this like one weird fit
boom thing yeah I am okay okay if I
could talk you out of one weird trick
boom
would I talk you about a doom well I
mean I yeah if if you could talk me out
of the connection between High
Intelligence as in high optimization
power as in really good at using limited
resources to have a goal and figure out
what actions correspond to that goal
right in a large domain if you told me
that oh you could do that really well
but like for some reason humans just
come by and kill you anyway then I'd be
like oh okay cool well sounds good then
I guess we get to have you know a future
no okay the thing that I cannot promise
you is that yes if you can do that
really well then
yes it's it's over right axide kills us
absolutely ax side kills us there's no
way that's an important that's an
important Point like I'm I'm glad you're
kind of you know there's a path of
things you have to accept right to
accept I doom and just saying hey the
theoretical ideal of a super intelligent
I kills us I mean that's already a
pretty bold claim right I mean people
like Mark andrion probably wouldn't even
go that far right they'd be like you
know smart people aren't even more
powerful right like that's an argument
they'd make I've spent a lot of my life
kind of being AI Doom land right I've
spent a lot of my life trying to figure
out where this is going to go and you
know like really look also like people
think that I have some like agenda
people think that like George you run an
AI company you have to like say that no
tell me I really don't care if I T the
AI my whole life is already optimized
around becoming of the a rway right
right um so you know this is not like
okay but
so if you axi K yes
absolutely but AI is not computable like
I also state by the way if P equals NP
were
dead if
p p equals NP in any practical way and I
agree with you that one of the things if
I don't know about this goal to action
mapping but if gp5 discovers the the uh
solution to
sack okay okay I'm a little scared now
yeah yeah I don't I don't even think
that actual I don't even think there's
much of a noticeable gap between an AI
That's just has a lot of really good her
istics and is just you know good at
doing stuff uh compared to an ideal um
you know thing that can solve NP
complete problems in polinomial time
think that in practice there's a big
difference okay great great we're
getting somewhere we're getting to an
empirical thing that that that you can
be talked out of I am not smart enough
to talk you out on it okay by the way I
I just want to summarize for the
listeners right so we're talking about
humans down here and the complexity
theoretic limits and the computability
theory limits way up there and my claim
is just somewhere within that vast Gap
there's going to be smart AIS that are
gonna wipe the with us and you're saying
no they're going to be down here with
us um again wipe the floor with us is
very like like again the timing matters
right you're you're I can't again if you
have power if you have terawatts and
patts of power of course you can wipe
the floor with Humanity right I will
give you that as well I'm giving you a
lot I'm giving you a lot of ways Doom
can pay to wats of power appreciate it
pay to wats of power aixi P equals NP
okay okay and I
mean here's the thing imagine we're in
the year 7 1700 we're sitting around
talking about how to build perpetual
motion machines right M and I'm sitting
here giving you lots of ways okay look I
will give you that if like you know
magnetic modles exist we could build
Perpetual emotion machines I don't
actually know enough about this to
really give real examples but I could
give you a whole set of of of of of
hypothetical if if tach on exist we
could build perpetual motion machines
sure sure but as far as like in the year
1700 all these things look plausible
year 1700 sure all these things are PL
all in 1850 after we have the law of
thermodynamics you sit there and you're
like you're never going to be able to
build a p perpetual motion machine it
doesn't matter where you put the magnets
just it doesn't matter like stop trying
yeah yeah well this is actually a great
example because it's true that sometimes
we discover a deep principle and the
Deep principle says you can't build a
perpetual motion machine but like but
that's like a pretty obscure thing to
not be able to build compared to being
like hey you can build a drone light
show you can turn lead into gold like
all these things that actually people
would normally talk about turns out you
can build well you can turn lead into
gold just takes a lot of energy right no
but but that's what I'm saying is
usually when you get better at physics
you just it turns out that the Universe
really is hackable like most things you
can just hack your way there and yes
okay you can't build a perpetual motion
machine but is that really exactly what
you wanted I I I've done the math a few
times on the economic feasibility of
turning lead into gold and like it's not
there no sure but if that but okay then
I guess that's a bad example right but
I'm just saying like if you start from
start from a problem right start from
something you actually want you probably
can do it and when you discover new
physics that'll probably tell you how
you can do it not how you can't but
there are exceptions like if you wanted
to get somewhere really really fast new
physics would just tell you that you can
simulate it but not that you can get
there yeah so I see the coming of a
science in the next five or 10 years um
it's the thermodynamics of in
challenges right there's going to be
some again
physics do always tell you you can't do
it right like you can accelerate a steam
engine to you know a th miles an hour
physics and thermodynamics don't tell
you you can't do it but they certainly
constrain how much wood you're going to
need right they constrain how little
pric you must have on the
track okay sure yeah I mean if you're if
you're still using you know chemical
combustion yeah but like you didn't
stumble upon this thing by accident
right if there are weird basins and
critic cities and intelligence there
there might be and I I think that you
know I even think that there are I'll
even go as far as to say that like there
is one weird trick out there and that's
a big leaper thing okay thank
you again stranger things have existed
in the universe in in spaces of just
like if you thought winning the lottery
was hard right like finding these things
finding these crazy attractors
is like
impossible like there's no search that's
going to lead to it all intelligence is
as far as I can tell my understanding of
intelligence is this you have okay um if
you want to separate a picture of a cat
from a picture of a dog if you try to do
a linear classifier on this it's just
not going to work right but what you can
do is you can stack lots of layers and
what these layers will do train them
correctly is they will transform the the
landscape they they will create a line
between cat pictures and dog pictures
right they'll make this they'll make
this landscape linear they'll make these
things possible to find they'll make a
convex Optimizer
work I do not believe that any convex
Optimizer is ever going to crack the
secret to thinking like but why are we
talking about convex optimizers right I
mean if you look an llm architecture I
mean there's a lot of nonlinearity too
right so it's again these
generalizations just don't really
correspond no no no no no what the cast
of grent Des center of the convex
Optimizer okay I mean I so you know
that's not my area of expertise but I
think that it's uh it's treating it like
it's just a linear separator I don't
think he's
right well no but I'm not saying it's
just a linear separator what I'm saying
is stochastic gradient descent which is
used to trate all these models if the
brain is using something like U
if the brain is using something like uh
what's there's a local form there was a
less wrong paper about this it's really
good there's a local form of of back
propagation it doesn't require you to
back propag gradient the predictive
cating if the brain is using predictive
coding it it works out to be basically
the same thing as stochastic gradient
descent the choice of Optimizer matters
right like this is what I mean about all
these sort of like noo theor like when I
was 19 I wanted I wanted to find a
collision in sha right I didn't go to
college didn't learn any real computer
science but I knew that I could put sha
into a SAT solver and I was like man I'm
a genius I'm gonna be out I'm gonna you
know people are GNA know about I'm the
guy who crack shop so I put it into the
St solver I checked all the bugs and I
sat there and it didn't Sol
it and I didn't understand why it's like
the guy in the 700s trying to build the
Perpetual emotion machine like no your
set follow is never going to be able to
hit this optimization
Target the type of Optimizer matters
there's no magical something Clicks in
its goal to action right like like like
yeah yeah yeah so so instead of trying
to come at this from the idea of like
look I understand how the stochastic
radiate to algorithm works and I'm
telling you why this algorithm doesn't
look promising I encourage you to go
back to the framing we had before which
is where do you get off the train of
which specific input output problems
right which domains which optimization
problems where do you get off where you
say now this optimization problem it's a
crapshoot you have a human you're pretty
much doing the best you can do right you
don't get off at chess you don't get off
at go but like at some point before we
get to the whole universe you get off
the train and I encourage you to think
about where without thinking about the
details of the algorithm just thinking
about the details of the problem
specification well no it's not that I
get off the train anywhere again we we
both agree pretty much on the same end
point it's the the the the place where I
don't believe it is that somehow you're
going to find the magic trick and be
able to outthink humanity in a
w just like just like when we cracked
power we didn't figure out how to
massively outcompete muscles we just use
more
power okay I mean I feel like I would
take the win for for human machines over
muscles however you want to Define
that well I mean no the Machin muscles
are pretty
efficient sure yeah I mean but if
efficiency is your criteria okay I bet
somebody could design something more
energy efficient than a human muscle
think that's just now becoming true and
there are theoretical limits on it that
are I don't know that much about how
muscles work but I do know that if you
look at how much fat a human stores it's
like the same electricity you put into a
like 100 kilowatt hour electric
car yeah I mean look these are
interesting analogies right I mean if
you look at flight it's really
impressive how birds fly right like the
the birds are like an engineering Marvel
but you can have like a big simple plane
that still does a lot of things better
than Birds not everything but if your
goal is to just like win a race birds
are
done sure these things eventually out
compete humans I think we both agree on
that it's not going to be GPT 5 that
accidentally cracks it that's all I'm
trying to convince you right I'm just
trying to say yeah but but I agree right
because I'm telling you if you if you
say I'm absolutely sure it's going to
take 20 years till super intelligent AI
I would say maybe you're right there's
like a 40% chance you're right okay what
what what let's let's let's let's
uh what if it's 200 years I think that
that's there's a low chance that it
would take 200 years if it's doable at
all but I'm giving it some chance right
all I'm telling you is I'm just I'm
pointing you to a t I'm not trying to be
nostris I'm just saying look where
things are heading at what point are you
okay with that at how many years is this
okay I so this is the pro it's okay if
we can figure out like an alignment
strategy right assuming one exists be
like oh or like if we ourselves somehow
preserved our values but became much
smarter right that's kind of like the
holy Grill where it's like fine I'll
take them as smart as they come because
I'm like one of them and and it's and I
can expect their values to be respected
right so some some version of that would
be good okay so our children respect our
values right because our children are
you know they have the same genetics
right so that does a lot of work okay do
our grandchildren respect our values I
mean values obviously shift over time
right and part of the problem with this
is that you have this question like we
don't we I can't even spell out the
human utility function right I can't
even tell you exactly what's an
acceptable degree of freedom and what's
not right and so it's easy to just throw
up my hands and like okay everything's
okay it's like oh everything's okay okay
here's something very simple and lame
you said everything was okay right like
that's kind of where the slippery slope
could lead but I mean this is the beauty
of human interplay that gave rise to
human civilization different value
functions competing against each other
the the representative space of human
value functions is very vast people
always say that like human doy this tiny
part of Mind space I don't know how true
that is yeah so you can Define the
problem you can be like I value whatever
is going to win the competition of
taking over this sector of the universe
and if you if that's really what you
value great CU you're going to get it
but like it's probably going to look a
lot like a nuclear explosion that maybe
leaves a little bit of like the you know
in The Game of Life where it's like okay
here's like some Spinners here's some
like bouncy Rubble like you're probably
not going to like it you know truly
we've had all sorts of humans who valued
all sorts of stuff throughout history
and in general as we've gotten smarter
we like to think that we've started to
Value more enlightened stuff
right but the problem is that the first
AI that runs wild and has no undo button
is probably not going to represent the
best values of humanity whether an AI is
capable of running wild comes completely
down to how optimizers work or whether
this is a relatively slow exponential
where we're not going to have one we're
GNA have thousands we're GNA have
millions yeah so that and that's your
multi-polar scenario right and if if if
the most powerful one has great values
then we're good but I even know it's
just it just looks like you standing in
the corner while the Giants duke it out
and then you still die the most powerful
one can be ganged up on by the 10 other
ones right you know I think also Cas
versus the world was an interesting like
Magnus Carlson may be the best chess
player in the world but if the 10 guys
underneath him all played against him
they' beat
him well I mean that's isn't that what C
FR versus the world showed evidence
against right and I I like to use the
video of you know those like three
Japanese soccer Champions played against
a team of like 50 uh you know school
children and and still managed to win so
like look you can't just parallelize
things right I mean any computer
scientist knows this for a lot of
problems C versus the world was like
Twitch Plays
pokon okay sure but like it's I mean if
if your claim is hey everything
parallelizes right so like a data center
full of human brain simulations can beat
any ideal AI if that's your claim okay I
don't feel like that's a likely claim
but I guess there's a small chance it's
correct my my my point is I put the
intelligence explosion microeconomics at
pretty close to one I don't think that
there is a magic trick that gbt 5 is
going to discover I think gb5 is going
to be a bit smarter than gp4 and gb6
will be a bit smarter than gb5 and gpt7
will be a bit smarter than gb6 so on and
so forth till some point a thousand
years from now these things do look like
gone compared to puny L humans yeah and
you're describe if any scenario where
things grow really gently in a way that
like somehow humans can adapt right like
maybe they like
you know drugs to a little bit SM like
if you can me that it's nice and gradual
I feel more optimistic there's still
problems but is an exponential nice and
gradual it's got be an exponential like
I mean an exponential is nice and
gradual relative to what I'm fearing but
yeah I mean look even something that's
merely exponential that merely doubles
every 15 years or whatever is also scary
but this is just okay okay it's going to
be a little less than 15 we're we're at
about 15 now it's going to be a little
Le than
15 right right but again at what point
does it get
scary yeah I mean yeah you know you know
you know where I stand right there's a
threshold right the point of no return
analogous to an exploding nuke that I
think we're swimming out toward I don't
think there's any point of no return and
I don't think there's any criticality I
think that exponentials are enough to be
scared again I'm giving you another Doom
scenario if the economy was doubling
every second this is not a world I want
to live in like no no no no no no I'm
probably dead right if the power usage
of humanity was doubling every second
yeah I'm dead I'm dead yeah I mean I
guess right unless you're in a Data
Center and you're sped up and for you a
second is a long time right you never
know yeah like again but if this just
like happened if tomorrow we started
Living in a world where the economy
double A ising yeah no for sure yeah if
tomorrow we started Living in a world
where be eony doubled every five
years this is a nice
word yeah and that's kind of like the
Happy techno Optimus default right and
you assume that you know humans still
have agency right like you're not kind
of like you know in a zoo that the are
running like right it's more familiar
than that are we already in like who
runs the world right now sure I think
there's a lot of agency right an agency
is probably one of those values I want
to preserve some and this is where I was
trying to go with Conor and the Somalia
thing right like oh do I have like what
kind of agency do I
have my only hope of having some agency
actually building the
AOS yeah I mean look before before the
systems that be can crack down on any
agency I might have you know when when I
have these discussions the part of the
discussion where we're like okay what
agency do we really have this good to me
this feels like the part where you kind
of already conceded the the important
dangerous part right so it's like is
agency good I don't know I'm just saying
we don't really have a choice what I'm
saying is that the trends are going to
continue right the Industrial Revolution
trends that Ted tazinski thinks are
terrible that most people don't think
about much and some people think well
it's actually better than the
alternative I think it's better than the
alternative right I'm happy the
Industrial Revolution happened I wasn't
always but you know I came around and
I'm happy it happened all right like I'm
here today I AG back I love my back
right like like but you know you can
read you read the unibomber manifesto
and he talks about the massive loss of
agency for the human species and I think
on that access loss of agency will
continue and I do think this is a
potential like I think this will
continue just as it happened before but
I mean now you're basically you're
basically talking in the robin Hansen
Viewpoint so he's written some pretty
good pieces about this being like look
the AI is perfectly on Trend like what
did you expect uh whereas I think it
really is just kind of like blowing
everything up and it's going to be hard
to say that that's perfectly on Trend
but like you are citing like a smart
person's Viewpoint I I'm almost
perfectly Rockin hans's Viewpoint almost
perfectly matches mine with respect to
like economic growth and stuff
yeah like I I think yeah I think that
uh you know it's it's just it's it's
just following Trend where I'm
down right the part where the part I
think I can add isn't even the
economy job explaining this stuff to me
the part that I think I can add is that
F doesn't happen and there's very
technical arguments to be made about
what optimizers are to show why not
right so if you had to pick a point to
get off the AI Doom train it does sound
like we keep coming back to the like uh
maybe there's not that much headro for
algorithms beyond the human
brain again head rooms are very funny
Headroom in terms of power yes
absolutely there's not much head head
rooms in terms of I have uh you know a
data center with 10 megawatts well okay
20 megawatt data center that's a million
humans okay a million humans that's
that's a what's what's a million humans
that's that's San Francisco right um
okay so I got I got a I got a 20
megawatt data center up San Francisco I
got a 20 gwatt data center okay that's
three Humanities I got a 20watt data
center who that's a
lot yeah okay right and that's that's it
right like it's just yeah I mean once we
have data centers that when we have a
data center or a spaceship that's using
a Patt yeah okay but when we have a data
center that's using a
megawatt all right well that's like uh
50,000 people that's a good small town
yeah I mean if you tell me 20 years from
now you've got you know 10 data centers
using like a a ton of power like 100
times more power than today but they're
doing something that's like smarter than
the smartest human right that's doing
like a bunch of Science and stuff I I
would predict with pretty high
confidence not 100% but I feel like they
can probably optimize that
down um I think that software scaling
Trends will
continue uh so will Hardware scaling
Trends I actually think that due to the
there's more of a continuity coming in
Hardware we are going to finally get AI
chips that are pretty good and we're
going to get like a 10 10x in power um
there's still another 10x to go in Mo's
LW so we'll get like a 100x there um
right because it sounds like you're
imagining like some ASM to right like oh
it's just you know like an s- curve
right and like that's great I would love
an S curve right but I think there's
going to be a Cascade into danger
there's s-curves all the way down right
remember jard scaling jard scaling was
great until it s curv curved out yeah I
agree and look this I mean it's possible
that llm architecture will be an S curve
and then the next curve that goes on
turns out to be the deadly Cascade right
I'm deadly Cascade curves forever yeah
so the reason why I see a deadly Cascade
is by looking at the shape of the
problem space not by looking out which
architectures exist
today well I mean again if by deadly
Cascade you mean s-curves forever
leading to exponential growth then I
agree with you if deadly Cascades you
mean one of those S curves doesn't look
like an s but looks like a
hyperbole I just don't believe in that
no technology ever in history has looked
like that Humanity Humanity has been a
series of fcurves the rise of humanity
has been a series of fcurves right but I
mean look the the concept of a positive
feedback loop and of a Cascade and of
you know and Cycles I mean put a
microphone next to a speaker right or a
Cascade I mean humans experience a
Cascade when we got a lot smarter in a
short amount of evolutionary time humans
have been riding a whole of S curves
since the beginning of forever life has
been riding a whole set of S curves okay
cool we had single cell organisms they
were around like three billion years we
kind of Hit the end of the S curve on
that but keep in mind but if the end of
the S curve is far above human
intelligence we're already dead by the
time it s's out but the end of the S
curve is far about human intelligence
it's far about human intelligence it's
gonna take but you know again the
doubling is
well okay you here's a here's a there
was a yukki and Kur had a had a
discussion and they talked about whether
which was the fundamental was Moore's
Law the fundamental or was Intelligence
the fundamental Kurt's argument was that
Mo's law was the fundamental I remember
that and I thought it was like a a
pretty you know that was a hell of a
bullet bite to say that Mo's law is more
fundamental than like you know actual uh
you know physical causal
things today the Moors law prediction is
working out way better sure chips are
still getting smaller but we're spending
more and more money on each F I mean
sometimes surface level patterns can
work well but that doesn't mean it's not
biting a huge bullet to say that it's
more
fundamental I don't know I would bet
against Mo's law significantly speeding
up I would bet it kind of stays on Trend
if I could just say hey Mo's law is GNA
continue and it would and it would be
like it would violate Moore's law to
have ai turn around and kill us that
would be cool but I don't think that you
can quite follow the logic like that
because I think that the transistor
density can just like stay at on the
curve that it's on but the a that runs
on the transistors we have kills us the
software curve looks kind of similar I
did see there was a good paper about
that I should I should do some more
investigation into what these software
curves look like um okay my phone's
dying I got to wrap it up but I really
appreciate this uh thank you for
engaging with me yeah George it was a
pleasure as as many of remarked before
you know you come at this in good faith
and I like to think I do too and and I
hope that more people have more of these
kinds of discussions I think we got a
lot further and I'm hoping that we can
put a pin on
the I think S curves continue you think
One S curve eventually drops into a
hyperbola and if that is the distinction
between the dmer position and the eak
position then this is an empirical
question and I think we're going to get
an awesome answer to it in the next 10
years I'm an optimist but I also could
be wrong and then oh  well all
right cool but uh I don't think I'm
wrong I think uh future is going to be
sick all right really appreciate it
thank you everyone for being on the
space uh yeah yep thank you George
anytime have a good one you too good
night I got I got 8% battery left if
anyone has a very good point we will try
this for five minutes
but there we go my my third my third
Doomer
debate yo was a great space thanks for
hosting this space
man sure uh yeah I I hope that I think
we're going to get answers I think we're
going to get answers to the field of
entr tropics I think we're going to
understand and what intelligent scaling
curves really do look like and I I mean
I think the great question is just a
question of diminishing returns versus
non- diminishing returns and if returns
don't diminish if returns keep
continuing boom but I don't think that's
how the world
Works a really interesting experiment
that uh if there are any academics on
the call who want to run this I haven't
seen these scaling curves but kind of to
tie it to go and the encryption breaking
I I do Wonder so if you were to increase
the size of a go board say to you know
500 by 500 a much more intractable game
uh and then you know for all these
different sizes of go boards train
different mu zeros with a with limited
number of training flops and then see
how say a 2X flopped mu0 plays against a
1x flops mu Z at different board sizes
and I'm curious if that curve uh stays
linear or if it flattens out which is to
say that the more complicated the game
is the more diminishing flops is that'd
be a super interesting experiment I
would I would love to see that
experiment these are exactly the
experiments we need to be doing because
you know what I'm a scientist right we
either live in the eak world or we live
in the Doomer world I'm not a politician
right this is not we live in the
Republican world or we live in the
Democrat world that's all  we
either live in the eak world or we live
in the Doomer world it's one or the
other let's figure out which one it
is earlier I wasn't trying to make a
political statement I was talking about
the effects of social media on human
behavior uh that is very political
okay um thank you good night everybody F
and tropics I would love to see
experiments like that done let's solve
this problem I Believe in Us we are
Humanity we are strong we are the
smartest things in the universe for at
least like 50
20 maybe 10 more maybe five more years
for like some amount of time you know I
think we're g I think we got this oh
good night everybody

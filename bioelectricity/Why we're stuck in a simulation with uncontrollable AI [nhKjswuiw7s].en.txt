hi everyone I'm here today with an
interview from Professor Roman yampolsky
who is going to tell us about whether AI
might be completely uncontrollable
thanks for joining us Roman did you want
to introduce yourself to the audience
sure I'm Roman yski I'm an a safc
researcher at University of Louisville
I've been looking at different aspects
of AI Control for well over a decade
have a couple books published on it
hundreds of papers happy to talk to you
hundreds of papers which I could say
that at some point with a with a
straight face like that um it's pretty
impressive not all of them are directly
on AI safety to be honest I did a lot of
work on cyber security and Biometrics
and relevant computer science Concepts
so have you been working on AI safety
problems since the beginning like for
how long have you been in the field
basically early papers probably 2010
2011 which are explicitly on AI safety
for advanced AI systems I did my PhD on
uh safety against kind of primitive AIS
poker Bots things of that nature there
was infestation of Poker bots so that's
where I started hating Ai and fighting
it we're planning to talk in three parts
denying death the simulation hypothesis
and is AI uncontrollable so here's part
one denying death Roman I think this is
referring to uh how we as a civilization
you're kind of rushing towards AGI right
can you talk a bit about that right so I
talk about how dangerous AI could be how
it's a potential existential problem and
uh the question I often get is why don't
people treat it seriously so I'll give a
talk I'll tell everyone okay AI is
coming it's going to kill everyone and
then the questions I get will be like
will I lose my job and uh I couldn't
understand it at first but I think we
can map it onto another human bias which
is denying the fact that we all die
every minute every second you are
getting closer to being dead and
probably not doing much about it
personally governments are not
allocating 99% of their budget to
solving aging super rich people who 100
years old still not wasting their money
and solving the only problem they should
care about so there is some cognitive
bias which I guess prevents us from
being depressed all the time but that
same bias is now problematic at the
level of humanity where we are denying
that we are creating Technologies which
is essentially a death for civilization
so Roman I create videos will I lose my
job I'm not even sure if you're a real
person or an AI I mean I met you like
two minutes ago and you seem cool but I
have no idea my first couple of videos
uh I had very poor lighting and my face
always looked very flat so the joke is
that I am actually an AI maybe you are
maybe you don't even know that you
are that's right how else do all these
biases that everybody has affect how we
think about the race for AI like people
also say that it's not even worth
spending time trying to resolve issues
with existential risk they're like look
at all the people that are being
discriminated against right now and of
course that's an issue and we should be
solving it but it seems like to me it
seems like everybody dying is a is a
worse case than you know people not
getting the loans that they should get
or something right and you should
prioritize based on impact negative
impact there is no shortage of people
working on fairness in AI for sure but
uh the joke is of course if AI kills
everyone it would be very fair yes and
that is the problem with optimizing too
strongly for something right it's like
well there's no cancer if there's no
people or there's no unfairness if
there's no people right and uh it's a
difficult problem I would argue
impossible and so the moment people
realize there is nothing they can do
they kind of prefer not to think about
it whatsoever those who don't think
there is a problem in the first place I
call them AI risk deniers we have
multiple papers surveying all the
arguments made about it we will never
build a gii it will be nice to us
because it's so smart hundreds of those
arguments and uh really you can map them
under different cognitive biases for
every cognitive bias there is kind of
one of those replies for why you not
interested in this very important topic
and the Primitive one the simplest one
is conflict of interest if I'm getting
paid billions of dollars to develop AI
it's very hard for me to understand that
this is a very bad thing to do and I
wanted to follow up on that because you
mentioned earlier that billionaires have
a a role to play in this in a sense
because I mean we know that the AI labs
are acting the way they are because of
their leadership and they're kind of
forced into that but it didn't occur to
me before which is kind of silly that
the people that are actually in power
there are billionaires so they might
have different views than the rest of us
right and I want to start by saying I
like billionaires uh but they do have
this situation where they are not as
much concerned with typical human
problems right it's not about getting
healthare insurance or food so they need
something bigger things like capturing
light cone of the universe that's where
the things are and they competing at
that level and they're locked in this
prisoner dilemma game theore competition
where none of them want to stop first
being ahead of others would be a
personal Advantage even if for Humanity
as a whole the best solution would be
for all of them to stop as soon as
possible it's mik's trap right you can't
really stop because everybody is in
competition with you exactly and so
that's why we're kind of begging federal
government to step in and provide some
regulations so they have an excuse to
investors why they stopped and the goal
is to be the one with the most advanced
AI at the time when everyone has to stop
do you think the government can or will
step in in that way so it depends on the
administration I think some
administrations are more Pro regulation
some are more accelerationist but at the
end it doesn't really matter because
it's just security theater those
regulations are similar to making you
know murder illegal it doesn't mean
we're not going to have murders you
cannot enforce it and uh there are
limits on monitoring AI training run
deploying them testing for safety so
it's more benefit of having a lot of red
tape shifting resources from compute to
lawyers for those companies it's an
interesting phrase from compute to
lawyers and AI companies are already
doing that right I think it was open AI
went from like three lobbyists to close
to 50 in one year and of course the big
Tech already has more lobbyists than all
right so it's not going to solve this
problem but it can buy us a little more
time to enjoy life but as you said it
would also be a good signal it would be
a good situation for billionaires
because they could use that as signaling
right oh look we stopped because of this
it's an excuse for why they stopped
otherwise they cannot if one tries to
stop I'm sure they will be replaced with
someone more willing to go forward
exactly do you have anything else you
wanted to touch on with the the race or
uh with current incentives or or
anything like that I I asked you about
government's so so the nature of a
problem is such that you cannot have
meaningful enforcement of punishments
like I heard people propose AI insure
I'm not sure how it would work so if you
destroy the world that is a severe fine
and a payout for someone else it just
doesn't make any sense in a context of
existential problems what do you think
about the the ideas is to like limit the
amount of compute or for example to
monitor gpus so you can see how big the
the training runs are that they're
involved in things like that in a short
term it's a good idea it will buy us
more time it will make it a little more
difficult to train Advanced models but
long term because of how much better
compute gets how much more efficient
algorithms get I think the size of the
challenge will go from this is a
Manhattan Project size effort to
somebody can do it at home with a small
budget and so that would still make it
impossible to regulate yeah if you try
to limit compute you actually have to
reduce those limits over time otherwise
you're enabling more and more powerful
systems assuming technological progress
continues right and we don't know how
difficult it is to train such a model
Maybe a small Improvement to existing
algorithms can make it 100 times more
efficient where existing computer is
moving enough yeah part two simulation
hypothesis we chanced Upon A mention of
simulation hypothesis and I think we
have some interesting things to discuss
here so can you tell us about it so
there is a good chance Advanced AIS will
have to run simulations of our world to
solve problems to make predictions about
it and if sufficiently Advanced we'll
make very accurate simulations including
human like
possibly conscious ones and if they make
a lot of those decisions they have to
create a lot of those simulations to get
statistically accurate results so it's
likely we are actually part of one of
those simulations much more likely than
us being in a real world if you want to
call it that and I'm interested as a
computer scientist not so much in
philosophical implications of it but a
purely computer science question you
have a computer simulation it's software
can you hack out of it what is outside
of this simulation can you get access
ACC to the operating system take over
some Avatar bodies and you know enjoy
benefits of infinite resources in the
real world so not only do you think that
we're in a simulation you think the
simulation is being run by Advanced AIS
and that because it would be in a
computational system of some sort that
you could Escape because you can always
Escape From Any computational virtual
environment if you will but even for
human-built virtual environments it's
often very difficult to realize you're
in one and very difficult to Escape so
what makes you so sure first of all that
we are actually in one and then we can
talk about how we might Escape so there
is a lot of good philosophical argument
so Bostrom presents his trama of uh
whatever we are not in one of those at
the moment uh I also think there is an
interesting argument for kind of post
factum capture where let's say in a few
years we will have technology to create
accurate simulations virtual
environments and it's affordable I can
pre-commit right now to run billions of
simulations of this exact moment
essentially placing us in a simulation
after the fact so if that's actually the
case if you are really inside the Matrix
so to speak then what are the types of
levers that you can use like what's the
purpose of the simulation I guess is it
going to end when a desired state is
reached can you like cause that to
happen sooner or can you manipulate it
somehow so obviously we don't know from
inside what is the true nature of
simulation what are the computational
resources of simulators externally but
looking at the time period we're in it's
likely that it has something to do with
development of some meta technology we
are not doing fire we're not doing
invention of the wheel we're inventing
realistic virtual realities universes we
are inventing super intelligence systems
we are un like decade away from all the
interesting meta Technologies which
would make this great error to study you
think it's plausible that for example if
you wanted to create some kind of very
powerful technology like super
intelligence related that you would
actually create a bunch of simulations
that would look exactly like this one
where a bunch of people run around and
they try to do something and they fail
and then they try to do something and
then they fail and uh and at the end of
it you see the the one out of billions
or whatever that actually worked and
then you can take that technology and
put it in your real world it may also be
some sort of testing I want safe and
reliable set of Agents I can trust not
to destroy the world and so I'm testing
to see who's dumb enough to create and
control super intelligence right if you
if you die you exclude yourself
basically so how do you think you can
escape from the simulation there are
very good examples where people found
interesting meaningful hacks out of
video games and Virtual Worlds
exploiting properties of a game to write
code to the external system to the
operating system being able to get super
resources in the game even load other
games into the scenario my paper has
pictures links to those papers I think
it's a little too deep to go into
details of how to hack it from here but
let's just say I'm still here for now
right so the hack has not yet succeeded
people ask me they were very
disappointed when I published it they
were like there is no solution how do we
heck I'm like this is a first paper on a
topic it cannot be also the last one a
field needs some time to mature and do
research I explained what to do and how
to do it now we need a follow-up paper
well good for you my my adviser always
said that the best papers to write are
the first paper or the last paper on a
topic idea it's the same person 30 years
later but uh we'll see how it goes it
reminds me of when you do speedr running
games and you look for all these hacks
and I actually know some speedrunners
that ended up like overriding buffers in
the outer game to like warp themselves
to different levels and things like that
that's exactly the examples and the more
we study deep science quantum physics
the more we find kind of glitches
similar to what we see in video games
rendering only if observed time
relativity where you have negative time
pre-event causation
things of uh kind of digital physics
evidence category which may indicate
where exactly in such a situation yeah I
have definitely read about how it would
be very convenient for a simulation just
the way that we have this you know
things don't matter if they're not
observed and it almost looks like it's
set up so that it's computable I mean to
some entity that has a lot of computing
power obviously but it may not be a lot
by their standards it's a lot internally
for what we have for them it may be a
screen saver no one's observing a screen
saver where you watch the universe
evolve and if you let it run for a while
you see some little humans running
around and then
disappear do you think about life any
differently because we're in a
simulation so the meaning changes a
little right if this is just a test it's
not real then you have a lot more
concern about the real world you still
have a lot of real states in the
simulation suffering is still real right
love is love but uh you kind of put more
emphasis on a bigger picture and can you
outline that bigger picture for us like
you know if you were giving a status
update to someone in the real world how
how is the simulation right now so I
think interesting idea is that by your
actions in this world you can impact
external World in some way just the fact
that you're doing more work already uses
more of their computational resource so
if anything you can speed up their
little fan next to the CPU
I meant more like you know how do you
think the world is going it's not over
yet so clearly they have not shut it
down and the results have not been
reported I think as long as it's running
we can still impact it and make it more
interesting Robin Hansen has some ideas
for how to leave good life in a
simulation you know be interesting hang
out with famous people study important
problems of interest to simulators
pretend to be a well- aligned agent
sometimes through irrational
obedience and I think you wrote a paper
on irrational obedience right so I'll
make sure that is linked at the bottom
that's why I mentioned it yes um the
idea is that if you doing things which
are rationally defendable I have no
reason to think you're doing it because
I told you to if you're washing your
hands maybe you doing it for Hygiene
reasons maybe because I told you you
need to wash your hands on the other
hand if you're doing something
absolutely insane in fact wasting your
resources just to prove your obedience
maybe sacrifice of some kind that's a
good reason for me to think that
currently you are obedient you may still
have a treacherous turn later it doesn't
guarantee that you are not going to turn
on me but at least for now you seem to
be a good boy so can we get safe AI by
being evil deities and demanding them s
demanding that they sacrifice something
to us so we can ask for periodic
sacrifices of something they care about
now what is that compute or some sort of
memory access we can research that it
seems like an interesting approach to
take to have constant verification of
obedience where every so many cycle
Cycles it has to do something actually
valuable if it's meaningless it's not
important if it weighs one cycle for you
it's probably not strong evidence but
the the higher the price it has to pay
for signaling the more of the signal you
are getting that it is still interested
in your approval and I guess the signal
has to be optional right it has to be
able to say well I'm actually just going
to do something else instead because if
it's just forcibly exactly if it's not
not even bothering to pretend that it's
loyal then you know it's not or what I
meant is if the Loyalty or the sacrifice
rather is like inherent in its operating
system kind of then there's not much you
can then then it can't actually opt not
to sacrifice right so it has to by
definition be something irrational if
there is another reason for it to do it
IR rational reason it no longer serves
as a signal yeah that makes sense are
there any other papers related to the
simulation hypothesis or whatever that
you wanted to go into I mean that's the
interesting one I think it needs to get
a little more interactions it has well
over 100,000 reads but interestingly it
has no citations show up on Google
Scholar while I know it's been cited so
it's another one of those glitches in
the
simulation Google got uh got pressured
by the reality to not let that info
spread I'm just saying so we need an
experiment where everyone sites it and
we see if it works
how often do you sense the universe in
the corner of your eye not behaving
properly it's interesting those are
obviously jokes and I don't mean them as
serious evidence but pretty much every
time I'm supposed to speak about dangers
of super intelligence in an important
context something goes wrong my cars
don't show up my airplanes collapse uh I
remember I was supposed to speak at
Google about dangers of
superintelligence and a pandemic started
and they had to cancel like that's how
much effort simulators put into
cancelling my
talk top secret information for sure
don't tell
anyone too late we're on
YouTube Nobody watches YouTube come on
true YouTube is also subject to an
algorithm right if the algorithm doesn't
show this to anybody then hardly anyone
watches it uh we'll see how many views
we'll get if this gets zero views we
know it's through that is literally the
experiment that's true they wouldn't
they wouldn't be so bold as to give it
zero views they would just give it like
very few compared to other videos right
maybe just you will see the counter but
no one
else maybe my SD card will get corrupted
and no one will ever see this wow that
actually also happened yes I'm pretty
much paranoid at like important
interviews like how many cameras do you
have a writing a memory cards
in well I think I would like to hear
about part three is AI actually
uncontrollable so I talked to a lot
safety researchers and everybody has you
know a wide range of perspectives on how
feasible it is to achieve alignment and
control uh but you had one of the
strongest perspectives I've heard and I
I think you said something about
uncontrollable so can you explain your
views on this right so first we need to
Define what AI we are talking about I'm
not talking about Tic Tac top playing
narrow AI systems even AGI may be to a
certain degree controllable we're
talking about the most advanced possible
system super intelligence smarter than
all humans and all Mains can do science
engineering independently the argument
I'm making is that it's impossible to
control such a system indefinitely as it
keeps self-improving evolving you will
have accidents you will have mistakes it
interacts with real world real data
malevolent actors Insider threats all
sorts of situations we are essentially
in a position where we need to create a
Perpetual safety Machine by analogy with
perpetual motion machine where not just
GPT 5 6 7
is perfectly safe but all future models
all variants trained on all the data
with all the users all the interactions
never make a single mistake which can
cause existential catastrophe as a cyber
security researcher what happens if we
screw up we reset passwords we provide
new credit cards and we're done it's not
a big deal right whereas here you don't
have a second Humanity to experiment
with so you have to get it right the
first time and if you get it very safe
only one in a million mistakes if your
system makes a billion decisions a
minute you're not going to be around
after 10 minutes so that's that's a set
of concerns I have so basically when
you're trying to control a super
intelligence you're the defender and the
super intelligence is the attacker and
you have to succeed every time every
time like a potential decision goes
against your interests or something like
that right and you don't want to be in
adversarial relationship with super
intelligence you're going to lose by
definition and you can look at different
definitions of control is a direct
control where you give orders like the
genie problem first wish whatever you
wish for usually second one is to undo
the first one you realize how much you
screwed it up right the opposite is uh
ideal advisor indirect control you know
the system is smarter it knows you
better than you know yourself so you
trusted to do whatever is right but
you're definitely not in control at that
point and we can look at all the kind of
hybrid variants for that sorry so
basically you're saying that Direct
Control and other mechanisms are not
possible which Mak a lot of sense but do
you think that the the last example
there is you know uncontrolled super
intelligence or by uncontrolled super
intelligence do you mean like it goes
and and kills everybody I I mean that we
no longer decide what happens it will
make an independent decision and by
default most decisions it can make are
not human friendly because it's not
aligned with our values it doesn't care
about you maybe we'll get lucky maybe
it's nice to us but it's a very small
probability and with time it can change
maybe it's nice to us for the first 10
years it tries to accumulate resources
have a strategic Advantage complete
dominance and then it turns an us so
even if you initially see good results
it's not guarantee that on a very small
time frame for super intelligence it
will still take over since you have a
background in cyber security how how big
a role do you think cyber security plays
in all of this because to me it seems
like one of the easiest levers for an AI
That's trying to gain more control or
something like that to to Really lean on
so it's an important aspect you cannot
have ai safety without security if you
created a friendly Ai and somebody hacks
it steals your model steals the weights
and corrupts it you don't have safe AI
so it's a given it's a infrastructure
you need to have in place to begin with
and it's hard because most likely the
hack will not come through a hardware
but through human users you can
blackmail them bribe them you can do all
sorts of things with human users so it
seems even that aspect is very difficult
to secure do you think that super
intelligence would eventually cause
human Extinction or like how do you how
do you think about that like you
outlined that we could never like we
could never trust it even based on past
experience but like do you do you think
of it as a virtual certainty that it
would eventually um you know kill humans
on on some time scale or like how do you
think about that it's a virtual
certainty that we will not control it
now what it does is kind of subject to
the unpredictability I published on we
cannot predict what a smarter agent will
do there is a good set of game theoretic
reasons for saying yeah it will take us
out it doesn't want us to create
competing super intelligences it doesn't
want us to try to shut it off or
manipulate it in some way but it's not a
guarantee maybe it will not maybe it
will for some reason keep us preserved
maybe even in a state we don't mind but
is that the gamble we are willing to
take so do you see any realistic or even
at this point unrealistic paths forward
to uh kind of avoid this situation the
only only way to win this is not to play
this game I mean if we're smart enough
to say we want the benefits we'll create
super intelligent tools for specific
domains like this AI cures this disease
that's all it does it doesn't drive cars
doesn't play chess then that's possible
we've done it before we have ai solving
protein folding problem perfect example
for how it should be done but if we
create General super intelligence it's a
competing species at that point we are
not in charge and I'm not sure what most
people have to contribute to Super
intelligence meaning the super
intelligence sees very little value in
most people I I haven't found any skill
or ability which we can contribute in
terms of intelligence now some people
said humans are conscious we have
internal States and experiences and that
would be valuable to Ani I'm willing to
conceive it but how many humans do you
need to get this functionality do you
need all 10 billion of us at that point
I have heard an argument that like the
super intelligence might want uh access
to lots of sources of data or sources of
information or sources of intelligence
or whatever and that people could
provide that but all right so factory
farming for human data that sounds
awesome let's do
it I think we already made a movie about
it called The Matrix no we were energy
we were batteries in that movie that's
completely different H that's true
batteries not uh not conscious entities
so if I can let me lay out what the the
scenario that you've discussed basically
right we have a small number of
billionaires or techno optimists that
are really pushing AGI and they're
locked in a race and they can't really
get out of that without some external
support they asked the government please
put some rules on us but the government
doesn't want to right now because they
think that it's bad for the economy Etc
that's why they didn't pass sp sp 1047
and the governments might possibly act
in some way if the public were to
actually like strongly support AI safety
H yet the public is this these cognitive
biases against understanding the problem
so you're right back at the start it's
kind of a it's kind of a loop that
without all this education and even if
all that were to work you you as you
said you're just going to slow down the
advance to slow down the development of
super intelligence because you can't
stop it entirely unless well what could
stop it entirely so if something really
bad happens if there is a asteroid
strike or another pandemic it's possible
that economy will slow down our ability
to develop novel chips and software will
slow down but again that just buys us
more time as long as there are humans on
a planet we will regenerate and you know
100 years later the whole cycle repeats
it's still if you're not putting
specific time frame I would still count
it as AI being dangerous for Humanity
yeah that makes sense and I guess the
other scenario I sometimes think about
is what if there was an AI related
accident so some AI system actually did
make a big mistake and and end up
killing some people or something do you
think that that could have an an impact
I used to collect AI accidents I
published three papers which are nothing
but Collections and Analysis of historic
AI problems and I stopped because
there's just too many damn accidents now
it's no longer meaningful to collect
them all every day we see reports of
this insane thing done by AI or that
thing people don't care honestly if an
accident is not huge they see it as a
vaccine essentially against taking this
as a serious problem they go oh okay so
some privacy was violated not a big deal
or one person died from a self-driving
car it's still less than dies from Human
drivers which is reasonable but at no
point they go this is so bad we need to
stop all of it and we seen it some
people proposed purposeful accidents
where you have ai mess up on purpose to
convince lawmakers and I argue strongly
against it because I think it's not
going to be useful it might backfire
against AI safety field but it's
definitely not going to change minds of
most
interesting it's almost like we're on an
evolutionary path that just goes one
place which is you know ever increasing
complexity ever increasing intelligence
and we can't easily increase our own
intelligence so we invent more actually
let's dive into that a little bit like
what do you think about um mind
enhancing Tech that could potentially
let us keep up with some of these more
digital Minds so first approach is a
hybrid system kind of neural link where
you have human mind and computer working
together it's awesome then the computer
is the tool and you using it for
communication for calculating things at
some point the calculator is smarter
than you so it's not obvious what you
are contributing to the hybrid system so
either explicitly or implicitly it
bypasses with biological bottleneck now
if you say okay we're not doing this
hybrid model we're going to do pure
uploads we scan your brain we put you on
a computer and we run you a million
times faster you have no body your
concerns are completely misaligned with
human concerns now you no longer need
room temperature food sex your concerns
are completely in line with weird AI
software so we created a competing
species but we didn't protect Humanity
as a biological species what if you
could be uploaded and downloaded
repeatedly in the same biological body
yeah exactly so you upload your mind and
then you get to experience that for a
while and then somehow that's that's
shared back down to your brain I would
suspect most people would not want to
downgrade back to biology if you already
had this super intelligent vir
experience it's probably like you want
to be a snail again
ah do you want to be a very sleepy
snail you have to go go get some sleep
every now and then yeah I could see that
so what makes you keep working I mean
these are these are all pretty I don't
know not not negative exactly but
they're like very imposing possibilities
let's say well don't really have a
choice I mean we either solve it or we
done so the only option is to keep keep
trying I think that's kind of natural
outcome here and honestly it's a lot of
fun until the end it's going to be
hilarious okay have fun until the end
that's what life is that's the bias we
discussed in the beginning of this
interview this is literally how can you
you know do podcasts then you are dying
you're going to be dead in 40 years how
can you do that that's what we've been
doing for all the generations right you
think there's any predictability at all
about what super intelligence will want
or what it will go after or anything
game theoretically it seems that
accumulation of resources is a general
attractor in that space so it doesn't
want specific things it doesn't care
about your house or girlfriend or
anything like that but it wants all the
knowledge all the compute all the
options for future decisions so no
matter what it wants in the future it
would be a dominant player it can also
protect itself from modification from
destruction so some sort of uh take over
resources in the universe would be a
very natural evolutionary outcome for
most advanced Asians do you think that
there's some connection between the
invention of superintelligent AI and the
fact that we don't see any other alien
civilizations out there like do you
think that this is the great filter
essentially that tends to kill off
civilizations or maybe we're just the
only you know civilization that's really
being that has really evolved but you
know if if it seems like there's such a
big chance of things going wrong then
perhaps that's one reason
or would you expect then that the
universe is actually fil filled with
super intelligences so we already
decided we're in a simulation so that
explains it completely but uh really it
could be a filter but if it was we would
see this wall of computronium coming at
us trying to grab all the resources we
have from all directions where our
civilizations exist and we don't see
that right yeah if you're a simulation
there's no reason to simulate two
civilizations unless they're going
interact you want to control all the
other variable so you have one variable
which is Humanity making this big
decision and we want to see what they do
right well do you have any uh
recommendations for what we should do
what now that we are here you should
read my papers and books find mistakes
in them and tell me that no in fact it's
really controllable and it's going to be
easy and make me live in a
Utopia then you can have fun till the
end but also be in a Utopia I guess that
does sound better immortality fun
forever do you think it's possible to
achieve safety not in an absolute sense
but with some like probabilistic
guarantees for instance yeah but this is
the problem over time those probability
set up so even if you save for a month
for year the more compute you invest
into making sure the system is reliable
the safer you can be but if you never
get to a 100% given enough time you're
going to have a problem that makes sense
so any of the stronger control
mechanisms that are really trying to
like not seed control to a super
intelligence basically those stronger
control mechanisms can't be
probabilistic they somehow have to work
all the time which you said earlier
right if it's making lots of decisions
it has to keep in place exactly they
have to be absolutely reliable which we
know is not possible for software
there's never been software which is
absolutely bug free and uh you know
doesn't create problems this is for
software we used to kind of static
software not self-modifying not
self-improving whereas with AI we seem
to be switching to this Paradigm where
you have agents writing their own code
self-improving modifying perhaps even
what they are trying to accomplish what
would you say to the people that say
just formally verify all the the code or
all the software that's important so
this is exactly what many people are
proposing the unverifiable paper talks
about limits and mathematical proofs and
software verification all your proofs
are relative to a specific verifier
either a specific mathematician
mathematical Community or a piece of
software this mathematician may have
brain cancer and the tumor is pressing
just in the right buttons you can never
be sure so you have the whole
mathematical Community we know that
there are proofs which stood the test of
time for 100 years and then we
discovered bugs in them that's not a
guarantee which software are you proving
that okay this is true because with
software sayess so who verified that
software you have infinite regressive
verifiers so you can be more and more
convincing but you never 100% sure
interesting so you almost feel like
there is no way to formally verify that
really verifies we don't know how to do
it for self-improving software at all
all the critical systems we verified
nuclear power controllers space flight
uh a small function with static code we
have no idea how to do it for code
self-improvement three rides based on
new data deployed in new domains there
is just no results in that yeah you have
to somehow have a decision theory that
applies even when you are manipulating
yourself which is the impossibility
result saying you cannot have this type
of recursive self-improvement uh
verification there are limits and that's
that's very interesting but uh I don't
think anyone claims they have a working
prototype or even a paper or patent
explaining how that can be done I'm
planning to link to a lot of your papers
in the description but I know that well
you know you'll probably get some
viewers that way but not a ton but you
uh you also wrote a book recently right
did you want to tell us a bit about that
so there is many books uh you can see
some of them behind me uh the most
recent one is explicitly about limits to
what can be done limits to explaining
Advanced systems comprehending them
predicting their specific actions and
overall controlling them and it kind of
goes through all this tools we would
need for control and showing what are
the upper limits for accomplishing that
so do you recommend people read your
most recent book or is there it's always
the most recent that's the one I love
and by the time I have the next one I
won't care about it but uh really it's
very timely it literally talks about the
issues we're discussing right now so far
people who read it seem to be very happy
they agree with the conclusions they
agree with the arguments but again my
challenge is always please find mistakes
please show us how we can in fact
control those systems nothing would make
me happier well you definitely have a
background worthy of a YouTuber and uh
much more uh much more scientific
besides so um yes I hope some viewers
will be able to check out that that
textbook and yeah perhaps find problems
but also just educate themselves and
educate more people because I really
think that all of these issues have a
have a marketing problem right it's hard
to tell people about them it's hard to
convince people it's not in the public
Consciousness and therefore the number
of brains we have working on it is much
smaller than it could be the good news
is most people who are not experts in it
intuitively understand perfectly well
you will not be able to control God like
machines forever then I do surveys in my
lectures regular people outside of
computer science there is not a single
hand that goes up and says oh yeah yeah
we'll be able to control those things
it's easy I'm sure computer scientists
know what they are doing so it's really
professionals When I Survey
professionals they say oh there is about
a 30% chance we can do it so it's not
obvious where this confidence is coming
from one of those cases where you might
know too much you might be too close to
the problem perhaps knowing about
cognitive biases makes it worse for you
is there anything else you wanted toh to
say to our audience today no thank you
so much have a wonderful life
very cheerful and thank you very much
for for coming on the podcast Roman has
a very busy schedule but he made time
for me on very short notice so uh he
also has some articles in Time Magazine
there's all kinds of stuff in the links
below so please check them out and uh
tell some friends about this video you
found it interesting if you liked this
video check out this previous interview
I had just recently with Connor Ley it's
on similar topics and you'll probably
enjoy it as well that's all I have for
today thank you very much for watching
bye

I think even within context learning
some of these Works have shown that
sometimes you run into cases where the
data that you provide to a model in
context conflicts with the information
that it has been shown during
pre-training and then some kind of
unexpected things can happen part of
intelligence is coming up with these
abstractions that regardless of the
environment allow you to adapt right to
these kind of to the environment and and
be able to fulfill your fundamental
desires and those will depend on any
system that does retrieval there still
needs to be some kind of manifold or
some sketch of a future situation which
we could then lean into to the degree
that any intelligent system constantly
learns from its environment and learns
kind of what is actually what are the
right abstractions to make good
predictions in that environment any
intelligent machine has to do the same
so the million dooll question is how do
you do retrieval taking into account the
interactions between the data points
this is actually quite straightforward
so what we do is
we mlst is sponsored by CML which is the
model serving platform for machine
learning workloads you log into their
platform the first thing you see is the
SAS option so this is the really simple
option just like on open AI you can send
requests up you stick your access token
in there and you can access all of the
latest open source models and it's
faster and it's cheaper on llama 405
billion I was getting about 45 token a
second they quote 65 or so but anyway
it's very fast and faster than the
competitors they also have pass options
which means platform as a service here's
an example so we're going to spin up an
llm inferencing service these guys
support ol Lama and V llm out of the box
so for example when you've got the model
up and running you can use the open AI
API right so you can use the python SDK
for open Ai and you can use that to talk
to your model I was just using the
cheapest option and I was getting very
good performance on um the new llama 3.2
to 3 billion model by the way when you
sign up you get 10 free credits uh
everyone gets 10 free credits which
means you can do some experimentation
without spending a penny also watch the
interview I did with Gennady their CEO
the other day enjoy introduce yourself
and why are people going to love this
episode what are they going to learn so
I am y sub I am a PhD student at ET
Zurich in The Institute for machine
learning where I'm working with Andreas
cower on local learning and sequential
decision- making so recently I worked a
lot on of making these ideas scale to
state-of-the-art models we work with
large language models and we're very
excited to be able kind of to outperform
some very large models on the pile
Benchmark which is this large huge
Benchmark that comprises multiple kind
of sources of of language from of
language that you would find on the
internet on maybe maybe stack exchange
Hacker News to language that is used for
math or coding and we have been able to
outperform the previous state-ofthe-art
which was a much more than 30 times
larger model than the model we used by
spending additional time additional
compute at test time when you're faced
with a certain problem so what we'll go
in today's episode is really kind of the
idea of why can it be useful to spend
this additional compute at test time and
the key aspect to this which is really
like fundamentally important and which
without which this kind of learning a
test time wouldn't work which is to how
to automatically select the right data
so how to automat how can the llm say
what data it needs to make a good
prediction and so essentially how can we
use the llm the intuitions the
abstractions learned by the llm to
decide what compute how should it spend
its compute at test time with the pile
shout out to ather I've got um Nora
belrose coming on I interviewed her at
icml and of course I know Connor and and
some of the folks there um but the pile
is interesting because you actually have
the data right um we don't have the data
that open AI trained on yes but I guess
there's a couple of things I mean first
of all you could in principle still do
retrieval against the pile even to
augment you know um inference that you
do on open AI it shouldn't matter in
principle that the distributions are
completely different but yeah like maybe
you can talk about that so so does it
matter that the distribution of of the
retrieval data set is different I don't
think it is because I mean so we de we
evaluated on a bunch of different models
where on some models I know they didn't
use the pile as a training data set for
example gpt2 as you said I mean they
they didn't train on the pile I think
the pile was actually released after
gpt2 was trained so uh gpt2 it still
works really well if if we kind of
augment gpt2 by information from the
pile um but at the same time I think you
can think of information in the pile as
very kind of Central in the data
distribution that a lot of
state-of-the-art llms are trained on
really because these are very kind of
academic data sets a lot of these data
sets are very kind of good in a sense
this is good data so there's a lot of
scientific Publications for example and
again coding math these are things that
today's llms are are very much trained
on and I think what is quite remarkable
to me is to see that
these kind of this information that
should be already very well encoded in
this huge parametric model can still be
useful if you show it a certain sliver
like a subpart some very informative bit
to it at test time the the other thing I
wanted to quickly comment on is I'm a
big fan of machine teaching so that was
invented at Microsoft research and that
is essentially an interactive form of
transductive act fine tuning and it's
it's mostly used for like classical
models but you know the idea is that you
have an application and if you allow a
supervisor to kind of like deliberately
choose the most informative training
examples just like the data set
distillation it it's remarkable how much
your sample efficiency goes up of your
training set when you actually pick the
most informative examples but something
like that could be used I guess with
your approach because rather than using
the pile why not just use Google search
so you you could create an interactive
workflow where the supervisor guides the
search and selects the most informative
examples maybe you could still run your
like non-interactive version on top of
that and and then you've got like a
really informative transductive form of
learning I think a very powerful
Paradigm that you know I think you can
still think about this in terms of kind
of a a kind of memory that you can
access but now where a certain piece of
memory might not be static but might be
something that actually unrolls some
underlying computation right where the
outcome is maybe not clear a priority
but which has a certain description and
we actually also in our in our group
with a
collaborator Marco batella we worked on
on a setting like this where your
Information Gain Is essentially not kind
of pre-comp computable in the sense
because the the information is not
static instead you have to leverage your
model's expectations of what might be
the outcome of an experiment to say okay
given that I expect this experiment to
go kind of a certain way how informative
would that be for the task that I'm
solving an extension question is um I'm
interested in retrieval augmented
generation so I interviewed Patrick
Lewis recently about that and I could
also imagine that this could improve
retrieval augmented generation but
rather than doing like an in context
augmented version of rag you could do
like a a fine tune transductive act you
know like a a transductive active fine
tuning um version of it do you think
that a version of that using your
methodology could improve on on rag um
so I think I mean rack is really this
big ecosystem of various different
methods that that people have worked on
certainly one of the big problems with
rag or some rag systems is rooted
precisely in the fact that they use this
nearest neighbor search over some dense
embedding dense vectors embeddings often
that's not exclusively what they do but
often that's kind of very core to what
they do and as we've discussed this is
very this can be very limiting in the
sense that it can select very redundant
information so empirically it may very
well be that selecting of most
informative relative to some pretty
useful metric of in informativeness
selecting the most informative data can
be better than just selecting the most
similar data which may be redundant uh
so so that's one thing I would say the
other thing I would say is that um the
question of whether in context whether
you should put data in context or
whether you should ingest kind of data
via rading steps is still a very open
research question there has a bunch
there has been a bunch of different
works that have kind of looked at the
difference between the two and it's not
very clear yet in general when which
approach is most effective what is quite
interesting to me is that both
approaches seem to be quite different in
when they work and how they work so for
example what was very interesting to us
is when we work with this pile data set
which again consisted of these multiple
pieces of subd data sets which compr
like related to multiple things that you
could comp Express in in human language
was very interesting to us is that with
certain types of this of these data sets
fine tuning was much more advantageous
over in context learning than with
others and those tended to be of this
math data set that is part of the pile
Deep Mind Math which comprises school
level math questions like computer
derivative solve this equation for x um
those typ of questions um but also
coding or scientific papers from archive
or fre law which is a data set that
comprises Court opinions yeah so so just
addressing both of those things so yeah
and just with rag improving the
retrieval mechanism so doing some of
this stuff you're speaking about you
know not selecting redundant information
could improve Rag and with rag as well
sometimes just because of the
architectural complexity maybe you're
doing retrieval from diverse data
sources and maybe there's some
subsequent step to again remove the
Redundant information or rerank them or
or do something like that I'm not I'm
not sure whether existing systems do
that but then there's the question of
like in context learning versus fine
tuning and there must be some
interesting kind of trade-off there
because I've always thought that in
context learning is better because just
anecdotally the the Precision seems
better like it it if if if there's
something in your context it doesn't
seem to hallucinate where I I have this
intuition that when it's inside the
model the information is lower
resolution because it gets entangled and
expressed as part of like the low-level
representations in in the model that
might confer some advantages because the
model can use more of its based
knowledge to reason about the thing that
you're doing but disadvantages in the
sense that it's more likely to
hallucinate about the thing that that
you're talking about um there has been a
bunch of Works uh that's also just
related to trying to understand
understand how in context learning works
I think even within context learning
some of these Works have shown that
sometimes you run into cases where the
data that you provide to a model in
context conflicts with the information
that it has been shown during
pre-training and then some of unexpected
things can happen um generally I agree
that the data with these the data that
you put into context with these llms
that we train today um the the the kind
of the next token distribution is is
very heavily skewed by that and so your
model is much more likely to refer to
some specific piece of information that
is in context versus some specific piece
of information it has been has been
trained on on the other hand though back
propagation and and kind of gradient
descent are very effective methods it
seems so at least they work during pre
trining it seems that they are very
effective methods to teach a model how
to reproduce a certain pattern and of
course I mean there's this argument kind
of of kind of the connection is that if
you do that long enough if you scale
that up enough um you will have to
develop some abstract forms of
understanding because to be able to kind
of predict the next token given previous
tokens you have to maybe learn abstract
learning algorithms and so on so for
example if you if you're faced with a
math question and now you're doing back
backrop over this example that computes
the derivative uh of a certain of a
certain term then to be able kind of
essentially what backprop will teach the
model is how to reproduce this and if it
teaches that on on multiple of these
examples arguably the model will find
some form of reproducing that behavior
whether that is the algorithm that we
want or not that is a bit of a different
question but it will try to imitate it
to some degree I love thinking about
purpose at different levels of
description I mean we were talking about
this on our patreon call last night and
uh Keith dgar gave the example of an ant
hill so it it has a purpose right and it
programs all of the little ants and the
Ants can adopt different roles and it's
very very simplistic you know they smell
pheromones or something and um you know
sometimes they'll they'll they'll go in
the path when they find Food they'll
they'll like drop the pheromones
sometimes they don't and that means
other ants will follow the trail and
then the trail gets reinforced and so on
and even though the ant is just
following these very very simple rules
it serves the purpose of of the the
anill but then there's the question of
well it doesn't have an intrinsic
purpose the the purpose is something we
ascribe to it maybe um I mean it makes
you certainly think if you talk about
kind of these simulated games that
simulate humans in their life to what
degree we have like more purpose than
the ants because certainly like some of
these City Builder games they I mean
they get pretty close I mean not
perfectly accurate but pretty close to
how humans move around in the city and
they're certainly kind of a lot of
societal inflicted purpose that makes us
go around like be it go to school be it
go to work be it go to like a restaurant
or to a grocery store to get food and I
mean those are like fundamental needs
that either like come directly from the
from from being a human or just uh from
from society and certainly a lot of the
purpose that is driving us around is is
not necessarily intrinsic in that sense
and is like externally um comes from
external
sources yeah yeah it really does how how
much of it is internal though I don't
think like I I don't have the kind of
background to be able to actually answer
that question um I think I mean probably
because I mean the city build like
staying with the city-builder kind of
metaphor I don't think they in any way
shape or form uh represent truly
internal motivation right so you can
already say like anything that is uh
kind of not in the gap between these
simulators of cities and what You
observe in the city anything that is
kind of not inside that small Gap cannot
be internal right because that's somehow
external or from society or from kind of
uh a byproduct of the larger simulation
and then there's only a smallest liver
remaining right of course I think the
human like part of the human
Constitution is much more than how we
move around in the city right and
there's a lot of stuff that's happening
in our brains which is not uh covered by
that but yeah I mean this came up with
um Ela judkowski and Steven W from the
other day because they were they were
talking about where do wants come from
you know you have you have an
intelligence system and and you can
model it as a thing that has a planning
Horizon and the bigger the Horizon the
more intelligence the more agency it has
and and then okay well why would he
argue for something like instrumental
convergence you know that when you have
a super intelligent AI system that it
will have predictably bad intermediate
or instrumental Al sub goals and you can
think of it just in this city setting
that you were just talking about that
all of these different agents there are
there are constraints as bit like they
have to follow through certain rivers or
in the case of a city they have to go
through certain roads they have to in
order to transact of each other they
they have to use money and they have to
do all of these different things so it
kind of like canalizes or truncates the
space of future behavior and it's not
just physical extrinsic um constraints
it's also kind of like interactions with
the behavior space of other agents in
the
future um yeah I think like my view that
a large part of this is abstraction so
as you as you kind of give kind of I
think a lot of these systems even like
these artificial systems that we kind of
give today they they do have some kind
of extrinsic goal uh that we ascribe to
it be it either like just to imitate of
the the distribution of things that we
that we give to it or like some
explicitly formulated goal um and and
then I think the kind of wow moments
that the AI Community has accumulated
over the years to a large part are
situations
where leaving that goal aside the agent
if you want to call it an agent came up
kind of with some subgoal that was
actually the result of kind of in
abstraction like the agent was kind of
abstracting from the immediate goal it
wasn't kind of greedily I trying to
solve that goal it kind came came up
with this abstraction that allowed it to
of take this intermediate step which
which maybe wasn't obvious to a human to
then still still with the purpose of
solving that goal which was obvious to
the human and um yeah I I don't know I
think
um there's definitely it's definitely
interesting in the sense an interesting
question whether um coming up kind of
with these intermediate goals or
intrinsic goals as you might kind of say
is purely a function of being able to
abstract or if there's something else
going on I tend to air to the side that
this is really purely the power of
abstraction um but yeah I don't have all
the answers there yeah well Schmid huba
said that intelligence is compression
you know by by which he meant
abstraction but something I think about
as well is that um again we were talking
about this last night the reason why um
you know asynchronous distributed
software systems are so exciting is
because that that's how the natural
biological world works right and if you
use an extreme example like if we had
some space station you know 30 light
years away or something like that you
know like you you have to get a message
to it you have to have a topology which
has a kind of locality prior right you
know so that's the way biological
systems work right you know the the
cells send messages to each other and it
all just kind of emanates out in this
asynchronous way you kind of send
messages but when we design AI systems
it's kind of different they're not they
don't emerge they're not grown do you
know what I mean they they they learn
using stochastic gradient to S so at
some point when we build AI systems we
have to design them you know we we
actually have to put in information
boundaries and we need to say that you
know like you you're you are talking to
this at this level of abstraction and
that seems completely divorced to how
things work this I don't know if I fully
agree with in some sense I mean I agree
with the kind of framing that we design
AI systems and we kind of put some
boundaries in place like what is the
amount of computer you get what is the
amount of memory you get what is the in
some sense what is the amount of data
that you will see but I think we're
going beyond that um so you can
certainly think about these systems that
are following these fundamental
principles whether that is some form of
gradient descent or some other learning
mechanism that is human engineered to in
the
end interact with the world and maybe
design their own experiments to then be
able to actually decide themselves what
type of data I acre from my environment
in in some sense how I kind of move into
and space and how I kind of in what
direction I evolve of my kind of
compressed
representations well let's talk about
this kind of Behavioral plasticity that
that you're talking about so you know
what I love about active inference is
that there are no explicit goals there's
no explicit reward function of fact you
can think you can think of it as as a
form of Maximum entropy inverse
reinforcement learning you know where it
actually starts with the statistical
distribution of of the data and then it
kind of infers within constraints what
the reward might might be and and I I
really like that because I kind of feel
that we need to have systems that can
kind of like
um materialize their own goal rather
than us as designers kind of saying you
need to do this
thing I think on a different level of
abstraction that is still a goal that is
human engineered right we still as you
said we still designed the AI system and
designed the learning me ISM like even I
mean the various forms of active
inference how it materializes in
practice because you can have to come up
in that case with like some
probabilistic model uh over which you
can kind of optimize that expected free
energy
um so I think in the end it's still even
kind of that form on an abstract level
you can think about as some form of
human
engineered I don't like the term reward
because it's like so link to this
reinforcement learning community but
kind of a similar thing and then I think
really the argument that people make is
that of course that will lead to
different emergent Behavior than if you
gave it a different reward like
something that is more commonly done in
reinforcement learning where let's say
you are working with an Atari game and
you just give the game score of course I
mean also in reinforcement learning if
you give an agent a different reward the
agent will kind of start learning
differently and again going back to kind
of what we discussed earlier with kind
of intermediate kind of sub goals being
actually a function of the abstractions
or kind of byproduct of the amount of
compression that the agent was able to
achieve of of kind of its information
then I think to that degree both ways
can and will probably find these
intermediate kind of intermediate sub
goals that um that we as humans would
find interesting interesting behavior
and I think that is to a large part what
um in my view people in in active
inference are kind of hoping for that if
you scale these probabilistic models up
then a byproduct of maximizing this
expected free energy will be these
interesting behaviors that lead to I
don't know kind of self- sustaining
organisms and that kind of stuff yeah
the I agree with what You' said though
like even with active inference I think
part of its selling point is that it's
kind of
human you know parasitic in the sense
that when you build the systems it will
have agency in a constrained way with
preferences that are compatible with
ours and that's fine but don't we want
to have ai systems with more plasticity
right and I think like what you say is
true right we design all of these
inductive priors we constrain the
systems in in such a way that that they
will behave in an anthropomorphic way
but I guess you could make the same
argument about DNA so so DNA has this
insane Con well some people might say it
has a kind of constraining force on our
Behavior but it doesn't seem to do that
right if if you look at our society like
the way we construct language with all
this beautiful diversity all over the
place but so there's this asymmetry that
even though like it's all based on DNA
like if we deleted all of the DNA like
we we we would all be gone tomorrow but
but it allows this complete fan out of
Behavioral complexity but still
constrained in some way I think this is
very compatible with the view that I
mean in the end I think the DNA and our
form of life gives us some very hard
constraints right we have to consume
water we have to consume food right to
survive but within that there's quite a
lot of kind of Pathways to achieve that
and they are also very dependent on the
society around us right that's and not
just the society our environment right
and that's I think part of intelligence
is coming up with these abstractions
that regardless of the environment allow
you to adapt right to these kind of to
the environment and and be able to
fulfill your fundamental desires and
those will depend on the environment and
in case of humans because arguably we
have kind of some non-trivial amount of
intelligence between humans how we kind
of achieve our goals is also is also
different but I think this is really a
spectrum across all animals and you can
see now that kind of the animals not
with climate change you know the animals
that are very kind of attuned to their
environment while they can kind of cope
with small changes in their environment
and kind of adapt to those drastic
changes in the environment their learned
abstractions if you will are useless
right and then they cannot survive
anymore um so yeah I think where did the
abstractions come from you know do we
what kind of abstractions do we hardcode
into the architectures and what kind of
abstractions are metal
learned um yeah I think I mean this is a
fundamental question I think that has
been at the core of machine learning um
and even more so AI since kind of the
Inception um and I mean a lot of people
they started with kind of hand coding
these abstractions right in a sense
leveraging human kind of human learned
abstraction or was what we deem to be
useful abstractions but fundamentally
being limited by this language of uh or
kind of by the language of how we can
express these these abstractions and and
also by our fin out time right um and
then and then I mean neural networks
went beyond that and and learned these
abstractions end to
end do you I mean are you a fan of
Chet's kind of type one type two
dichotomy generally I try try to not use
kind of this kind of system one system
two dichotomy and also kind of the word
reasoning that is somehow uh linked to
to system 2 sometimes because I don't
find it um always so kind of informative
it's definitely like a loaded term that
is kind of very differently interpreted
in in in various communities um I think
my my hunch is but I might very well be
very wrong about this is that that
really kind of this more if you will
planning behavior Can emerge from
learned abstractions so as we kind of go
up layers of abstraction of abstraction
of abstraction if we do make predictions
on a very high level of abstraction that
may seem like something kind of planning
more intelligent is going on so maybe
this is maybe this system one system two
thing is just kind of a question of what
level of abstraction we're operating on
and certainly going up levels of
abstraction requires more energy more
time and is computationally harder
strictly harder than than doing this
more shallow kind of finding these
shallow abstractions so in the in in
that sense I think you know um kind of I
I resonate with that idea but I think
there have various different
interpretations right that the
communities different communities have
come up with of how the system one
versus system two should be materialized
and uh yeah I I don't find myself
necessarily in all of those very cool
why why does fine tuning a model let's
say on on a batch of examples cost less
than doing inference within context
learning um it's because the essentially
it's because the uh the information gets
compressed into the weights mhm and in
context learning has this problem that
for every new token that you generate
you have to to attend to all previous
tokens right and that essentially
um you also have to go once over all
previous
tokens
and yeah that
is but doesn't I guess I'm just trying
to understand understand fundamentally
why does the backward pass not be
symmetric with the forward pass from a
computational point of view uh
um I think the key is here that it's
really ized in the weights M so in
context learning kind of has to present
all evidence at once otherwise it
doesn't work because there's no
amortization going on so in that sense
it's really like a full working memory
every kind of bit of information is
accessible at once and you have to
filter it out at once oh I see so when
you when you do the um the fine tuning I
mean obviously there's a batch size yeah
there's a bad size right so I mean
there's yeah so that we essentially pack
it into multiple you know boxes like the
multiple like sequences of tokens and
separate them and then present them kind
of one by one I got it so it's still
quadratic but it amortizes to linear
because it's so small yeah yeah exactly
and you have to I mean really that's the
key and that's why I mean that's why we
train parametric models in the first
place yeah because we try to amortize
the data right we try to extract some
compress the data uh as opposed to give
all the data at on which you could also
do with a nonparametric model
yes yes cuz I suppose there was
something like a kernel function or
something like that you know you would
you would need to have all the data
exactly I mean that's I mean that is
kind of the first ideas of of local
learning are in that direction right I
mean it's actually the trajectory of
local learning is quite fascinating to
me because it's quite a linear going
back it's quite a linear trajectory kind
of working with these separate
components so actually like in the 50s
people came up with nearest neighbor
retrieval where you're you have like
this distance function and then you just
average kind of to get a prediction of a
new point you average the predictions of
all the points around it and then people
came up with chal regression in the' 60s
which is essentially doing the same
thing but now you're waiting the
importance of the points relative to
their distance or relative to some
similarity measure which people call the
kernel often yes and Vladimir vapnik
yeah exactly yeah um and then you from
there on people went into different
routes like there was more development
of the non-parametric side people then
in the' 70s they started to do locally
weighted linear regression so whether
would train a separate linear regression
head for every prediction that they make
and then locally weight the training
data around the prediction so ibe higher
weight again based on some konal
function higher weight to the data that
is around the point where you want to
make the prediction and less weight away
from it but that kind of adds this
already this parametric component which
I think is key to this framework there's
always been this kind of you know ju
deposition
between inductive learning right which
is when we learn a general decision
function which could work for any new
piece of data and um transductive
learning which is particular to
particular so this is when we learn a
statistical function for a given input
and this is quite a new idea to a lot
people but actually as you're saying
this has been spoken about for decades
by people like Vladimir vapnik before
this kind of revolution of deep learning
took off uh so this was this idea has
been definitely around since the 70s at
least but kind of early indications
already before with people doing this
nearest neighbor search in in
non-parametric models yeah yes there's
kind of the spectrum I like to think
about this as a spectrum where in some
problem domain let's say it's natur
natural language you try to fit kind of
some like data manifold that describes
some aspect of natural language that you
care about and this trans this inductive
Paradigm is all about fitting all of
that at once so you extract some general
rules that hopefully generalize to to
describing accurately your kind of
function that you want to learn about
over this entire manifold and that is
still in some way you know go directed
in the sense that you are limited by
what is expressive expressible in your
problem domain that you work with and
traditionally people have worked with
rather small problem domains right so
you start with mist like recognizing
handwritten digits and then you quickly
realize okay it's a fairly narrow kind
of amount like small amounts of
information that you need to extract to
do this prediction right because you're
working with on a very small problem
space but nowadays we're scaling this
problem space to natural language and
Beyond images video really huge problem
space where a lot of kind of really a
lot of hard computational challenges can
be expressed in in these formats that we
are learning over and this Paradigm has
has roughly stayed the same where you
still kind of try to learn one function
that amortizes everything right so you
kind of learn of amortised intuitions
about how your data manifold looks
everywhere and then at F you know on
when you do inference when you do a
forward path when you make a prediction
you use a fixed amount of compute cost
mhm to access kind of your amortized
intuitions of how your function should
look everywhere but of course in
practice there are certain parts on the
data manifold that will be very easy to
predict and easy to learn and there
other parts of the data manifold they're
very hard and on these kind of more
complex problem domains this is very
evident in human language there are
simple tasks like you know that's what
humans do when they go on autopilot we
talk about the weather right like you
you have like this um small these small
chats where you talk about random stuff
um but then you can also go deeper and
that's where humans need to think right
that's when we also spend kind of more
of my more of our kind of brain's
computation to to be able to solve these
problems or even attempt to solve these
problems and um that is not really
captured by this inductive Paradigm when
I learned about machine learning it was
in the days when I guess it was the
second AI winter so it was like support
Factor machines and kernel methods and
stuff like that and I learned about
conformal prediction and I learned about
transductive inference because Vladimir
vapnik was was at my University and it's
a little bit like do you remember in the
1980s we used to have to do these insane
optimizations with memory because we
just didn't have things that worked very
well so we were leaning into the
optimization back then and I guess now
we've been in a regime because of the
deep learning Revolution that we felt
that we haven't needed to do it because
the Deep learning models worked so well
well apparently even in the multimod or
high scale setting they just seem to
work very well and it's almost like
people have started to to delude
themselves that these aren't even
statistical models anymore and why would
we need to do any kind of like local
optimization so so that's very
interesting the other interesting thing
that you said is I think humans do this
as well that when we deal with
situations that are unpredictable and
full of ambiguity we do more processing
I interviewed Professor mark Psalms and
he's like a neuro um neuroscientist kind
of what's the best way to describe let's
just say neuroscientist and he says that
Consciousness is something that you know
we become conscious of situations when
we are faced with
ambiguity and it's counterintuitive
because when you're outside sitting in
the sun and you're very mindful you
think of that as being like your brain
is doing less processing but you can
kind of interpret it actually as you you
start noticing the the clouds and the
trees your brain is actually like
processing more information not less
information so having having like a
variable amount of computation that we
perform in a you know like sensing more
information processing more information
is is a very interesting thing I mean
it's certainly fascinating with a human
brain where not my expertise lies for
sure it's the case that there's a
certain amount of energy right that we
can turn over to of do processing in a
brain at a given time so we cannot do
everything at once right it's impossible
so we have to be strategic of how we use
that energy uh to to achieve our goals
and I think fundamentally the same is
true with
machines um yeah yeah which which brings
me to the next thing so the other day we
we came up with this really cool analogy
to describe the behavior of of of I
suppose machine learning models and it's
like Google Earth so in Google Earth you
you have this variable resolution so you
you start off up here and you have big
course tiles and then you zoom into to
London or to Zurich and the tiles get
smaller and smaller and it's a great
locality method which allows you to kind
of like use the compute where you need
to use it is is that a good analogy for
the kind of work you're doing yeah I I
love I love that analogy uh I think it
captures really the essence where you
can think about kind of the number of
pixels you have as a given kind of
representational power or maybe com
compute power that you have at a moment
to represent something to make a certain
prediction and kind of in this inductive
Paradigm where you want to represent the
the picture of the entire world at once
there's only so much resolution that you
can afford to spend on any individual
kind of prediction let's say of of like
the city of zorich something so that
will maybe be a single Pixel if not even
and the power that you get if you zoom
in kind of is evident to anyone who has
ever used Google Maps if you were to
kind of zoom in by just making the
pixels larger kind of not change in how
you use your resolution to represent the
local information if you were to just
make the pixels larger as they are
pretty pretty quickly Google Maps would
be completely unusable right and that's
actually super powerful so if you if you
let's say work on a computer with like
1980p display and you have like your
friend who has a 4K display but has not
figured out that you can actually Zoom
by reallocating the compute that each
pixel does but just things you can only
Zoom by making the pixels larger or
getting closer to the display right then
you can pretty quickly outsmart him
right while he maybe initially has a
little bit of an advantage because he
can represent more information at once
you can if you want to go you know a
little bit deeper like think like want
to look at kind of zorich from from from
from space then you have much higher
resolution already than he does if you
because you're just using your
representational capacity or you're
using your computer in a smarter way
yeah so this is one of the reasons why
I've been fascinated by this concept of
active inference so you know Carl
friston is one of my heroes and he's got
this wonderful model of agency and I
guess the the Stark difference between
an active inference agent and a lot of
you know traditional machine learning is
that it's actually doing kind of
situational computation right and that
seems so incredibly important to me and
I feel that you're you're introducing
that but in a slightly more internal
list way okay can you can you explain
what I mean by that yeah um sure so the
recent work that we have done is all
about kind of locally zooming into the
distribution that language models learn
um so locally kind of learning that data
manifold better and the way it works is
that you have your you you stay with
your kind of normal language model that
you that you're working with and a fixed
parametric model and then on top at test
time you can look back into your memory
or big data set and find examples that
describe how that manifold looks locally
around the prediction that you're trying
to make and then just refresh your
memory right spend a little bit of
additional compute to kind of use your
rep representational capacity
specifically for the prediction that
you're trying to make so instead of
trying to kind of still all at once
learn the entire data manifold now we're
in the game of just kind of making
predictions specifically targeted to a
certain task that we're faced with at a
time and it's really about kind of using
compute and using representational
capacity to its fullest to make a
certain
prediction so there's a beautiful figure
I think it's figure two in in your sift
paper and we'll kind of like talk about
that a little bit as as we go but you've
got the data space okay so so so the
data space I guess is it's like the
ukian space of of of the selected data
and then you've got the data manifold so
so the data manifold is this kind of
surface or this structure which is
actually learned from from from the data
and then and then you do um you know
let's say you have an example that comes
in and you want to do a really good
prediction for that example so it kind
of seems logical that you should go and
Sample a bunch of the information in the
neighborhood of that sample because that
will improve the the statistical power
of your of your of your model now
there's a few things here so I guess the
first question is where do you get that
data from I'm guessing use something
like you know there's face and there's
there's like a whole buch of things for
doing like a nearest neighbor you know
kind of lookup and they they might do
vector quantization or or some kind of
like you know um log retrieval time
method and then inside that you do you
do an improved search as well but maybe
maybe we'll start with what's the what's
the difference between like you could
have a really really big base model and
there's a ratio between how big the base
model is and The Cone of your selection
when you do information what what's
what's the what's the relationship
between those two
things so I think what you're alluding
to the fact is that with larger base
models we are able to capture the data
manifold in a better way so even if
these larger base models try to
essentially make all predictions at once
or amortize these predictions so that we
can essentially access them at constant
cost when we do inference um larger
models mean we get kind of better
representation more representational
capacity and we can learn the statistics
better essentially and what we kind of
are showing is that this local learning
at test time is an additional mechanism
on top that regardless of what kind of
representational capacity you start with
your base model if you add that on top
you just kind of get that additional
kind of extra bit of representational
power to make a better prediction um
essentially using your kind of
representational capacity to its fullest
when you when you make that prediction
right because at any point in time your
pre-trained language model if you will
has to encode all of this information
right because it has to solve a lot of
different tasks at once and now when
you're tasked with making a certain
prediction usually it can just you know
forget it can ignore most of the
information that it has compressed and
that means it can represent the pieces
of information that are critical to
actually making good good prediction at
higher Fidelity right so you can make
then also a prediction at higher
Fidelity so I'm thinking we we're kind
of assuming that there's one data
manifold one data space why not why not
partition it up so instead of having a
multimodal embedding space let's say
we've got something that can do text and
images and video um why why not why not
separate it out and and have different
modalities different data
spaces yeah in what sense do you think
that might you are you are you meaning
one could train like a separate model
for each of them and and then for kind
of you split your data manifold into
multiple sub manifolds and then just
train a separate model on on each of
them and then be done with it I think so
I think like one of the things we're
playing around here with is we're in
this regime at the moment of having a
single model which is trained on every
data modality and your work is hinting
that specificity helps a lot or you know
having a local situational method helps
a lot but couldn't you say the same for
even the modality of the data I mean
like you know couldn't you just keep
partitioning and create some hybrid
system and would that give you more or
less statistical power um so I think
like we have to separate a few things
here so the first thing is of course if
you are kind
of making a certain prediction in some
specific modality being able to really
focus on that modality is useful but of
course if there is let's say information
uh or if you if you kind of entire data
manifold that is not kind of um kind of
cut into its subp parts contains
information that is necessary to make a
good prediction but it's kind of
accessible only in a different modality
then you need to have this cross
modality to be able to access that
information so there's really two key
elements here there's the F the first
key element is to to be able to use the
entire representational capacity to make
a certain prediction and kind of overfit
to the the to the test problem at hand
but the other key aspect is that we need
to have the right abstractions or we
need to have learned the right
representations to be able to decide
what data is actually useful what data
contains the necessary information to
make a good prediction and also the
right abstractions or the right kind of
learning algorithms to turn this kind of
this information into a good prediction
and generally what we have seen over the
last couple of years
is that if you scale these kind of deep
learning models up and if you show them
a wide variety of data kind of during
pre-training then they get better at
kind of finding these similar patterns
across modalities and also across
problems to kind of come up with like
more like solution algorithms or like of
simple mechanisms that allow them to uh
kind of synthesize things so for example
if you ask chat gbt to write you a you
know uh kind of a song about a certain
Topic in a certain style it will do that
um but that's certainly not possible if
you have never shown it you know any
examples of songs in its training data
so it's I think this cross modality is
really key and it doesn't necessarily
detract from the fact that you want to
at test time use your representational
power to after having seen a little bit
of everything and understanding how kind
of everything behaves to then Focus on
those things that given your
abstractions and your knowledge you
think are important to make a
prediction so I think we should contrast
your work to act Active Learning so in I
guess like in in the the olden days of
machine learning we would um produce a
decision function and uh we would have
like a test train split or maybe like a
a validation split and so on and then
Active Learning came along and active
learning said well actually um you might
be faced with a shift
uh data distribution so when this thing
is used next week the the distribution
might be different so why don't we
continuously retrain the model on on the
shifting data distribution and active
learning would select let's say sort of
like diverse and and new training
examples but what you're doing is is a
type of active learning but it's
deliberately honing in on the specific
rather than looking at the general
exactly so I think um I think you have
put it very well I would kind of extend
the characterization of active learning
a little bit in the sense that a lot of
active learning has just focused on how
can we kind of subselect the training
data that we use in the standard kind of
pre-training and then evaluation
Paradigm so that we can use less data
but kind of compress already the
information that is contained in the
data in such a way that the model will
still be as powerful as if we had
trained on the entire data and people
have shown in some cases that that is
that that can be be useful and Powerful
um but that in those cases random is a
really strong Baseline and and random
has often you know outperformed a lot of
these methods now what I am working on
is a very kind of different but similar
in some ways
setting the setting that I'm interested
in is local learning so making these
specific predictions and in that case
there's only some kind of small sliver
of the entire information that is kind
of in your memory
that is only that's actually relevant to
making that prediction and actually
searching for that is a key aspect but
you also want to maintain diversity in
the information that you obtain so
actually the methods that we end up
working with are some kind of mixture
between methods that people have looked
at traditionally in the in the kind of
literature on search and the methods
that people have looked at in the
literature on Active Learning which are
all about kind of finding diverse
diverse samples very cool so we're kind
of blending information retrieval and
active learning in a in a very kind of
like specific way so maybe we should
start with the search so you said in
your paper very beautifully that the
naive way to do this is I have an input
example and I I have a you know like a a
training data set or something like that
and I should just go and retrieve the
nearest neighbors again using some kind
of like vector quantization retrieval um
system like face so I could retrieve a
whole bunch of the the nearest neighbors
for this input example find tune my
model on on those nearest neighbors what
could possibly go wrong yeah um there
are a lot of things that can go wrong
and the main thing is that nearest
neighbor is kind of has really been
designed as a search method so if you
have like a big bag of of possible kind
of solution candidates that you're
looking for like a needle and a Hast
stack problem it tries to give you kind
of as many candidates that match as
closely as possible to your description
um as you
want what you realize is in local
learning you actually want to learn
something and learning something
requires synthesizing different pieces
of information so for example if your
task is not a simple retrieval task in
the sense that in the sense that the
information is exactly encoded in your
data and you just have to find this one
piece of information and then return it
to the user when instead kind of there's
a lot of different pieces of information
that are uh kind of disseminated in your
data and you need to find all of the
relevant P pieces and kind of return it
to the learning algorithm at once um
then nearest neighbor will not work
because nearest neighbor kind of just
focus on the dominant frequency or kind
of the most kind of most similar aspect
that kind of some subcluster in your
data has to whatever you are asking it
about so for example if you give of your
your engine a question we had this
example with um where where where
someone was asking about the age of
Michael Jordan and how many kids he has
yes essentially kind of the data in in
your memory you can think about as just
representing some clust so some part of
the data is just information about his
agent some part of the data is just um
about his kids
and in practice what happens is in this
kind of latent abstract abstraction
space um just one of the classes will be
closer to your question that combines
both and then your kind of nearest
neighbor search will return all pieces
of information that are about one of the
topics I think in that case it was the
age um first before it ever finds any of
the pieces of information that about the
number of kids yeah so we'll we'll show
this figure on this on the screen but
you know the prompt was what's the age
of Michael Jordan and how many kids does
he have and the nearest neighbor
approach basically doesn't even address
the number of kids that he has because
it almost like it latches on to the we
did this experiment actually and this
was really we did it in a very simple
setup where there was really just four
pieces of information in the in the
memory two are about his age two are
about the number of kids he has and we
just requested kind of the the closest
to and um normal nearest neighbor will
just repeat redundant information
because there's nothing stopping like
there's nothing in the objective of
nearest neighbor that's actually
quantifying how informative the set of
examples should be that are returned
it's just caring about really the
marginal Clos proximity to to what you
asked about okay so this is really
interesting so the way we do the
information retrieval is using like a
bunch of embeddings and I think in your
paper you said you're using Roberta
embeddings and that that may or may not
be uh relevant but you're saying rather
than just searching using some
similarity metric in ukian space and
getting the nearest neighbors we should
have the concept of Information Gain
with respect to some task so in this
particular cases the task is I need to
know this and I need to know that and
we're saying retrieve me the examples
that actually give me the most
information for that thing that I want
to do exactly and the funny the funny
thing is if you just try to retrieve one
piece of information from your data
store those two kind of views turn out
to be pretty much equivalent right
because if you if you can only access
one piece of information from your
memory the best thing that you can do
kind of your best shot is to just take
the thing that is most relevant but as
soon as you have picked that kind of
piece of memory and now you're looking
for the next piece of memory just um
finding something that is as close as
possible uh or as as related as possible
to what kind of your task is is not the
best thing anymore because that might be
exactly the same thing that you've
already seen right in in case there's
lit literal exact duplication in your
data so instead what you should do uh is
to both look at how relevant a new piece
of information is but also how
non-redundant it is relative to the
piece of information you have already
assembled so you're essentially trying
to solve some tradeoff between um
finding examples that are as related as
possible to your prediction and finding
examples that are kind of a diverse
representation of um all the information
that is encoded in your memory that
makes sense so so Devil's Advocate
you're saying that we do some retrieval
around a cone and if the cone is very
small so if the specificity is high
nearest neighbors will just produce a
load of redundant information and it
will miss a lot of it won't be sensitive
enough to capture all of the information
in the prompt but what if we just
increase the cone so what what if we
select nearest neighbors but we select
let's say 2,000 nearest neighbors or
something like that so so then yeah
we've got to fine tune on 2,000 examples
now instead of five but probably some of
those examples would have information
about how many kids that Michael Jordan
had um that is that is definitely a fair
point and I would say that
uh
really um there are multiple things to
say here one is it is actually not just
important what you see it's also
important how you see it and it turns
out that it's actually you know it it is
sometimes useful to see the same piece
of information repeated but it depends
on really that piece of information and
how and your Learning System how kind of
how good your learning system is how
strong your learning system is so for
example if you
uh if if you like let's say you work
kind of with some math questions and you
you fetch you face like with a problem
where you have to compute the derivative
of of some equation and then you look
into your memory and look into simple
similar problems that you have uh solved
before now maybe there's a cluster of a
lot of other kind of problems that were
about finding derivatives but all of
them used kind of some trick to find the
derivative that is not applicable here
and now you will just train on them a
lot and you will overfit to them a lot
so what's it turns out to be very
important um to be careful with how many
gradient steps you do on a certain
example and it turns out that the kind
of solution method that we propose which
is called sift takes care of that
explicitly we have this one figure in
the paper which I think is quite uh
insightful in that regard where we
actually look at the all the examples
where SI decides to just fine tune on
the nearest neighbor repeatedly for 50
steps right because there's no notion
that all pieces of information have to
be separate like in nearest neighbor so
sift is perfectly fine with just fine
tuning on the same piece of information
if it is helpful and in all those cases
it is drastically better than nearest
neighbor right because nearest neighbor
would just explicitly take this piece of
information once and then move on to
other parts even though they might be
less insightful so I think it there's
really a spectrum and as you said um it
might be it might be good to to train on
some example a lot um but nearest
neighbor is kind of this horis that
doesn't take this question into account
whether it's actually good to train on
examples more or
less so another thought that occurs is I
guess representation bias and and also
the um the robustness of models to fine
tuning I remember I read in Fran sh's
deep learning with pyth andberg you know
he he he basically said you got to be
careful with fine tuning because these
things you know fine tuning can quickly
overwhelm a model and if you turn the
learning rate up too high for example or
you do too many gradient steps it's
almost like you at at at some point the
model kind of forgets everything it knew
about previously and it leans too much
into your examples and that can cause
overfitting and the representation bias
might be well you know actually it kind
of like you almost don't you don't want
it to be overweighted by the the thing
you're you're retrieving so is there
some tradeoff there um there's some
tradeoff there and of course if you if
you were to use a very high learning
rate um your uh kind of fine locally
fine tuned model will not work well
anymore um I think it's
really a very natural thing so what you
want is you want to somehow take kind of
this local information and kind of use
these examples as a way to kind of show
you model it's fine to kind of forget
some information and fit this new
information more closely but in a
similar vein you don't want to just
encode this information and then I mean
if you do a lot of fine tuning steps
eventually it will just literally
memorize this specific or just try to
predict the specifics that you gave that
you retrieve from your memory so there
is some trade-off and you want to be
somewhere in between um yeah that's what
I would say
yes yeah I think one of the key things
that came out of our work and this is
kind of part to how we actually do the
information retrieval if if you want to
frame it that way is that you need to
kind of get a hold of the uncertainty of
the model so and that's really a key
aspect and the uncertainty tells you how
kind of based on the current models
representations how would the model
change or how more certain would the
model be if you gave it a new piece of
information and now you can use that to
say okay this piece of information is is
very relevant to the pred it's very
informative it's kind of needed to make
a good prediction then I'll show to the
model
and at the same time maybe there are
some pieces of information that are not
relevant to making a good prediction and
then you can exclude them and so you can
in in some
capacity determine for a given example
if you
have important information in your
memory in that case you should use it
whereas if you don't have relevant
information in your memory then you
should not do fine tuning at all maybe
or or not as
much and how does the embedding function
affect the results um so the embeddings
are crucial right so the embeddings in a
sense describe this data manifold so the
embeddings
describe really essentially how
informative a certain piece of
information is to something else and
this is really closely linked to this
linear representation hypothesis that is
widely studied especially in
interpretability and essentially this
linear repres ation hypothesis says that
of abstract concepts with you know
complex enough no networks llms um are
represented as kind of linear directions
in some representation space and that
representation space is accessible so we
can tap into that now you can think
about kind of these representations as
describing the data manifold so things
kind of Concepts that are very aligned
in that latent space are very relevant
to one another so if you want to make of
a a a smart statement on one of these
Concepts you you better know about this
other concept that is very closely
related to it and in a sense if your
abstractions are not good then you will
not identify these related Concepts just
Concepts that are to us seem very
related that we know are very related
the machine will not have identified as
related and then it's useless and that
is kind of part of the story why 20
years ago when the representations were
not as good people did not talk about
the linear representation hypothesis
because um machines were not yet able to
actually identify these Concepts which
we as humans know are related yeah Neil
nander was telling me about this when I
spoke with him last time he's he's an
amazing guy in mechin Tu and yeah so so
this idea essentially I mean we've been
speaking about a manifold but you know
Concepts and so on can be thought of as
having some linear Direction in in space
and there are examples of using L linear
probes you know there was that um oel
board game example where you use a a
linear probe and you can kind of like
extract board State and and so on but
also I think it's useful for surrogate
models so there there's loads of um
examples in machine learning like data
modeling I spoke with Andrew ilas or
even lime and indeed your work as well
you you create a very basic linear kind
of data model I think is it's good way
of describing it where you learn a bunch
of um uh parameters and and like a a
biased term in respect of an input
example and your data set if I
understand correctly yeah so I think
generally the idea of using a sar good
model that is using a simpler model to
understand the behavior of a more
complex model is a very powerful idea
and in our case we just kind of we use
the simpler model we use a linear model
to that kind of or we assume that kind
of the logits of this big llm um are a
linear function in this kind of abstract
representation space which is is highly
nonlinear in the inputs but kind of
treating this abstract representation
space fixed so treating all these
embeddings as fixed and then just
treating kind of your final head that's
producing the logits which in the end
after you push them through the softmax
the next token probabilities just
treating that as a linear function now
allows you to analyze it as a human
right now things become tractable and
that's very important if you want to
make sequential decisions and want to
optimize an objective so that's why
these SAR good models are so powerful
you're right in saying that in other
domains people have also looked at these
s good models and also found them to be
very powerful so for example uh kind of
data models where you want to of learn
how um kind of certain data influences a
certain prediction or in Lime where you
want to kind of fit a linear model that
is actually interpretable so not in some
abstract representation space but in a
space that you actually understand so
then when you have trained this linear
model locally for your prediction you
can look at that linear model and
interpret the importance of the weights
right because you know this weight that
is ascribed to a certain input has the
specific meaning isn't isn't that crazy
so as as you say like these are highly
nonlinear models and I guess lime is a a
kind of influence function where you you
have a linear model to kind of tell you
what the um what the weighting of of the
component features were for a given
input example to make that prediction
and you know you gave the example of of
data model modeling it just see it seems
crazy to me that you could you could
have a a simple linear interpretable
surrogate model so I think there I think
it's it's really a wider Spectrum so
with line the reason it works is
actually fundamental to the reason why
local Learning Works is because if you
again if you want to make kind of
predictions that generalize across this
entire data manifold you need a very
non-linear function versus if you just
want to explain one local prediction
kind of the the hypothesis is that you
can get away with a linear model because
you need to encode much less information
right you need a much less complex model
your representation capacity needs like
doesn't have to be as big to explain
this one uh this one prediction now
there's of course some limits to it and
in some cases it doesn't work so well
and and lime will tell you okay now we
found this linear model but the linear
model doesn't actually track so well
what the big model did so that that's
kind of of lime um in the in the data
models case or in our case we don't use
that model that linear model to make
predictions right and also we train that
linear model in this nonlinear
representation space so essentially I
think fundamentally you can think about
virtually any newal network as a big
encoder right until the pen ultimate
layer and then like one kind of linear
uh of one final linear layer that
projects to your output Dimension and
I mean in that way that kind of neural
networks are just sequences of of linear
functions that are composed with
nonlinearities yeah this this makes a
lot of sense so so yeah line is short
for locally interpretable model agnostic
explanation so for a given input example
in in that local area you can have a a
linear model I guess it's the same thing
with with data modeling you know you're
basically saying for this input example
you know what's the influence of of of
the data set and actually even MLPs are
the same thing so in spline theory of of
neuron networks it's saying that you
know models are locally affined so for a
given input example you can actually
represent the behavior of the neural
network with a simple fine
transformation which is represented by
the kind of like the cutout honey so
yeah it's like that's how deep learning
models work essentially they're just
like you know a massive confection of
like linear Transformations for for a
given input example exactly you're
trying to essentially move to some space
like move data into some kind of space
where it is represented in such a way
that you can make predictions as linear
functions um and and that can be very
powerful in the sense of actually
designing your model your
representations but also your functions
in such a way that they will make good
predictions right because you know the
structure yeah love it so data models I
think
um they are people have also tried to
use them to select data but kind of a
fundamental uh assumption or kind of
some limitation that people have found
is that if you kind of use these kind of
linear models to learn how a certain
kind of selected subset affects your
your test loss let's say or your your
prediction then it implicitly assumes
that the influence of data adds up
linearly and this is kind of so there is
this in inter ability this big line of
work on influence functions and maybe we
should explain what that is so
essentially influence functions try to
estimate the loss on a test point that
you would have had if you had changed
your model slightly or if you had
trained your model on that certain data
point so you can use it to understand
that you know uh this data point that I
have in my data set influence that loss
kind of on this prediction to to that
degree right you can ask these kind of
counteract questions that that's where
attempts to to be helpful now a problem
to is this is hard to make tractable of
course and and one kind of assumption
that people typically do is they use a
first order tailor approximation of the
loss to get this working uh or with data
models in a similar way people use kind
of first order order methods and what
that does it it ignores the um Mutual
dependence of data
relative to your prediction so it just
looks at the singular individual
importance of one data point about a
prediction but of course you train your
model not on just one data point you
train your model on multiple data points
and how you compose your training set is
very important not just in terms of what
is the data you give but also what is
the data that you showed relative to the
other data it has already seen and um in
that sense these methods are when you
use them for data selection you just
just end up with something like nearest
neighbor you still you you are going to
do some nearest neighbor retrieval in
some embedding space um but as we
discussed this nearest neighbor
retrieval will completely ignore the
fact that uh you can end up with
completely redundant information that if
you show repeatedly the same example to
the model it will think that it improves
because it will think that every example
has the same value but in fact it
doesn't and your the value the marginal
gain that you get from each example
diminishes this is really interesting so
so you're you're saying that actually
there are these first and second and
maybe further order interactions between
the the data examples we shouldn't think
of them as as as being separate and this
reminds me of the ml security literature
you know so there's like data poisoning
for example and and and that says that
you can manipulate just the order of the
data that that gets trained on on a a
machine learning model and you can
manipulate the model to do anything you
can even put back doors in the model so
we need to have a method of sort of
understanding the interactions between
the data examples as well as the data
examples on on their own exactly um I
think that is really critical and you
see it empirically and I mean also we
see it daytoday in our life how we
operate right so the decisions of the
information we access today will
influence what we find interesting
tomorrow yeah and this this is not this
doesn't happen independently right we
don't start every day independently from
scratch and just sample a new point from
this data distribution we think about
okay now I have kind of knowledge in
this field and this field and maybe
there's some field in between that kind
of would combine this knowledge and now
I would kind of sample Knowledge from
the Third Field as opposed to one of the
first two because I already know these
quite well so the million dooll question
is how do you do retrieval taking into
account the interactions between the
data point yeah um this is actually
quite straightforward so what we do is
we build a simple tractable surrogate
model uh over which you can estimate the
uncertainty that that model has relative
to making a certain prediction so you
you get uh some kind of uh tractable
quantity that describes how good your uh
prediction would be if you would have
shown the model a certain kind of set of
data points and that quantity can be
optimized so in close form you can
optimize you can minimize the
uncertainty that this model has relative
to making a certain response and it
turns out that
this
borrows really important ideas from
nearest neighbor retrieval so in
particular the first example that you
take will be the nearest neighbor um but
as soon as you look for the second
example you will try to find examples
that are of as aligned as possible with
your task in this latent space where
kind of simp similarity is encoded kind
of in this linear way and at the same
time try to find examples that are as
orthogonal as possible to the pieces of
information that you have already
accumulated so we'll get to the close
form solution in a minute because
there's some beautiful mathematics
around that but just stating it just as
an objective function what what are you
actually trying to do um yeah so
fundamentally you can think about it
from from multiple different
perspectives but fundamentally you're
trying to minimize some intrinsic object
jective of this of machine of of the llm
which
is to this uncertainty about making a
good prediction so you we come up with
some measure of uncertainty that
describes how good the prediction will
be and then the machine tries to go out
in its memory and find the relevant data
that makes it uh end up with a good
prediction and you can frame this in
multiple ways I think I like really a
framing that is more of a probabilistic
kind of from a probabilistic standpoint
where you think about your llm as having
epistemic beliefs about what could be
the right function to describe your
prediction and you're slowly kind of by
showing it more data you're manipulating
these epist or the machine is changing
its epistemic beliefs through Basin
updates through probalistic updates so
it's Computing posterior epistemic
beliefs and now what kind of this
objective is saying is it's saying the
llm should take those examples from
memory that lead to the most certain
posterior beliefs relative to the
prediction that it's trying to make okay
so what does certainty mean does that
mean you're trying to sort of like
minimize the the variance of the
posterior that exactly so in the in a
kind of in this so of course when you
are working kind of in this probalistic
model framework a key aspect is making
that tractable so that's kind of one of
the key limitations of of of basin
inference of probalistic inference is
that Computing this posterior is in
general a very hard problem
but using this linear surrogate model um
and essentially gaussian treating the
initial random variables as gaussian and
using a gaussian observation model this
posterior update is tractable so you can
compute it in close form and then you
have kind of this big
distribution um over kind of possible
predictions that the model can make and
there's one prediction that you care
about right uh so you are trying to
minimize kind of the kind of some
measure of uncertainty relative to that
prediction and for gaussin that's
usually the variance so you would just
minimize the variance of your of the
epistemic variance of your model uh
relative to the pred prediction that
you're making so this is really exciting
right because I I love basian analysis
and unfortunately we don't have access
to a hyper computer that can like you
know do do all of the computations in
the universe so neural networks they
they just produce bare predictions you
know they're like maximum likelihood
estimat
they just they say this is the thing
that's most likely we don't have the
kind of you know the the confidence
intervals we don't have like all of the
uncertainty that we' had with the Bas
method so what you're saying is we have
a linear surrogate model and in that
linear surrogate model we can model the
uncertainty we we can do like you know
confidence estimates and and so on and
then the other thing is you can use
kernels for this and gaussian processes
well in some in some degree yes I think
the simplest form with this linear
surrogate model is just basic linear
regression so this is like the first
basian model that that you are
introduced to when you when you start
working kind of with basian machine
learning which is just kind of your uh
kind of standard linear regression but
on top of that you have a gaussian prior
over the weights and then you have a
gaussian likelihood so you have kind of
an observation model that says that
whatever observation you you get is
really the ground truth function plus
some gaussian noise that is IID and in
that framework you can do this posterior
basan
inference exactly in closed form so you
can write down the close form solution
uh to this and this is fundamentally
what we use when we when we use a linear
star good model is we kind of
approximate this very complicated neural
network um by kind of aasian linear
regression which is right it it is a bad
approximation in terms of making good
predictions but it turns out to be a
good enough prediction relative to
deciding okay what information is
relevant to improving the prediction of
the llm and can you explain how it's
possible to do that in close form
because you know obviously you you think
of basian analysis as as needing to do
like you know complex solving and Mark
of chain Monte Carlo and all this kind
of stuff but but you can you can just do
it as as a simple I mean maybe you
should explain what I mean by close form
yeah yeah so what we mean by close form
is generally that we can describe the
pro the probability distribution the
posterior probability distribution over
weights as a mathematical formula and
that can be derived directly from the
formulas for the prior and the
likelihood so the way standard kind of
Bas in Basin inference Works Bas rule is
that you kind of compute your posterior
um probability as being proportional to
the likelihood of your data times the
prior divided by some kind of term that
normalizes this to be a true probability
distribution and the beauty is that if
you do this with gaussians if that is if
you use a gaussian prior and a gaussian
likelihood then this kind of probability
distribution will still be a gaussian
that is fundamentally because of these
gaussians only have kind of these first
order and second order terms and they're
kind of the one probability distribution
that if you have this second order and
first order term you know you are with a
gaussian and that's why you can do this
in in close form very cool so we have
described how we have a a way of
estimating the you know the the
confidence for a given um
prediction now what we want to do is use
as much inference time computation as is
required because if you think about it
we could just use like an unbounded
amount well that's not very good
wouldn't it be cool if we could link the
amount of inference time computation to
the confidence estimate yeah absolutely
and we looked at this a little bit kind
of in our preprint because it's you can
think about this as being kind of a
powerful tool when we have kind of
knowledge of how uncertain a model would
be if it had seen certain data and then
we can think about okay how much do we
actually want to pay in terms of how
much compute do we want to pay to get a
certain improvement in our model so in
that sense we can use these kind of
projections of how uncertain our model
will be to um stop computation early
stop computation early when our kind of
uncertainty reduction stops being
proportional to the amount of compute
that we that we're paying so usually for
any type of prediction you get this kind
of submodular kind of curve that where
the marginal gains are diminishing over
time because you're slowly accumulating
information and at some point you will
have accumulated all the information
that is required what do you mean by use
intuition of llm to make search
tractable so I mean I like to think
about this framework as having like this
controller and this memory right and
essentially what kind of maybe in like
the standard computational framework
people think about the same two
components for example with throwing
machines so you have this head right
which is a finite automaton and and then
you have this memory which is this
unbounded tape now what we can do today
which is quite amazing with these learn
abstract representations is that we can
essentially jump to any kind of piece of
piece of content that is stored in this
memory um just using kind of the
Intuition or the abstractions that we
have learned uh whereas kind of in the
standard computational framework you
would have to move left and right on the
tape of the touring machine now
essentially in kind of log linear time
we can access the entire memory at once
or the relevant bits at least so and and
the controller has two important
functions here in this framework the
first one as I said is being able to
decide what pieces of memory it should
operate on and it can do so by
leveraging these abstractions and
representations to essentially move in
this memory space kind of using
shortcuts instead of going left to right
and the second key component of this
controller is that it learns
presentations and abstractions that
allow it to ingest information from the
memory so it reads from the memory and
then updates its weights uh if you think
about in terms of noal network
and we want kind of a controller that is
very good at making these gradient step
updates right so you can think about
kind of better models stronger models in
this framework stronger controllers as
being kind of more robust to certain
types of information it would read from
its memory and kind of leveraging its
abstractions leveraging its knowledge to
actually use that information to make
good predictions right it's like
I think a good analogy is that if you
give a a kid you know some complex math
textbook it will probably not learn too
much from it immediately because the rep
representations and abstractions to make
sense of that information are not there
so um what we want with a strong
controller is that whatever it kind of
whatever piece of information it fetches
from its memory it's able to interpret
that and kind of use the information to
its full potential potential relative to
making a good prediction yes so so the
FSA is the controller and then the two
push down you know Stacks essentially
are the memory exactly but this is
interesting right because you know llms
are fsas so so they just represent the
and and Keith dgar my co-host he's he's
big on like touring machines and he
always says that you know there is a
class of algorithms which could run on a
finite State autometer but you know um
act as act as a controller for a machine
and that class of algorithms even though
they could in principle run on an llm
they're not reversible via stochastic
gradient descent because of course this
is a thing with a fixed amount of memory
it can't expand its memory but the
broader point though is that the touring
machine class of computation can do
essentially any type of computation and
we're not just talking about an improved
method here to improve predictive
performance you know through specificity
we're actually talking about a new
paradigm of Turing complete computation
absolutely and in a sense you can think
about this memory as potentially being
unbounded and getting rid of this kind
of limitation that pre-trained llms have
that they have to compress all the
information at once into this kind of
limited format that you cannot extend
afterwards that that is kind of there
and if you want to come up with
something different you have to retrain
from scratch uh basically and now you
can augment that controller that has
learned these kind of strong intuitions
strong representations that allow us
that allows it to quickly kind of adapt
to new information you can use that in
conjunction with this potentially
unbounded memory which kind of as you of
as you evolve that system over time you
can extend or um yeah add pieces remove
pieces of information as you please yes
so so it's it's a much more powerful
modality of computation but it's not
something which is trained end to end
with stochastic right in a sense so we
we learn the memory and then we
handcraft the controller and it's I
guess it's a tiny bit janky because like
you know we we have a fixed embedding
space and and we like retrieve from a
certain data space and so on let's use
the arc challenge as an example so the
guys who are the winners of the arc
challenge good friends of ours the
mindes AI team they they're doing
they're doing test time act you know
like transductive active fine tuning
that that's what they're doing and how
is it right so what they're doing is
they're saying okay well we we know what
the prizes are we can build a data set
generator and then when an input example
comes in we can kind of like lean into
that example and we can generate data
and we can f- tune the language model
and then we've got a python verifier so
it's it is kind of to Inc complete but
it seems like it doesn't seem General it
still seems Arc specific but it's
certainly significantly more powerful
than any other architecture you know
that we know about so far so we're kind
of moving in that direction a little bit
I think so I think if you if you think
long term about where I think the
trajectory of these types of Ares is
headed I don't really see so many kind
of fundamental limitations whereas in
the current implementations there are
definitely a lot of fundamental
limitations as you said so these are
kind of representations which are
learned once kind of the controller is
learned once and then kept fixed ideally
what you want is systems that learn over
time open-ended systems that learn from
their mistakes and improve their
representations and improve kind of the
ability of the controller to uh ingest
new information adapt to a certain task
over time and certainly that is what
truly intelligent systems do and what we
don't really have at Large Scale right
now um
so that granted I I would say that if
you scale this up as a system that has
kind of this if you will kind of finite
working memory that is kind of part of
that controller and that can add to its
memory and remove from its memory and
and doing so efficiently so efficiently
finding relevant pieces in its memory um
I think kind of philosophically speaking
that is like a general mode of
computation that can be quite powerful
will it always be specialized though I I
guess it comes to to the philosophy of
of knowledge so France Chet thinks that
there are Primitives of knowledge and if
you if you have some kind of like you
know set of Primitives that you bias
into the model then you could in
principle deal with any form of novelty
but any system that does retrieval
presumably it it still needs to there
still needs to be some kind of manifold
or some sketch of a future situation
which we could then lean into absolutely
I think I mean to the degree that any
kind of any intelligent system
constantly learns from its environment
and learns kind of what is actually of
what is what are the right abstractions
to make good predictions in that
environment any intelligent machine has
to do the same so in whatever
environment we put it and if we keep the
environment static that's kind of what
we're doing now then that is a much
simpler task kind of in in whatever
environment we we put the machine the
machine has to figure out what are the
kind of right abstractions so that it's
able to
find pieces of information and then
combine these pieces of information to
make good decisions
and I think there's nothing like the in
this Paradigm there's no fundamental
limitation that you cannot do this in an
open-ended system but it certainly has
not been done yet and that's a super
exciting
Direction so another thing that brings
to mind is you know I'm excited about
active inference and maybe in the future
we'll have a very distributed
asynchronous Nexus of agents that are
like you know doing doing something very
similar to what what you're describing
so very situated active inference and
and and what you're describing right now
is we have a monolithic language model
which is updated every six months and
and then we we have some kind of like
you know um information retrieval store
which is periodically updated and at the
point of doing prediction we basically
do inference right and then and then I
think we throw it away afterwards
exactly so so then we can get into this
Federated paradigm where rather than
throwing it away we kind of like
remember it and initially we remember it
just for me but maybe I start sharing it
with you and maybe eventually it gets
goes back to the Mothership well well we
could remember the prediction what we
could also do is just remember the error
and prediction right so whenever and
that's I think what a truly Learning
System does is at test time it does some
additional computation and then
eventually it sees okay to what degree
was that computation useful fruitful to
what degree ended did I end up with a
good prediction
um and then it can kind of change the
learning mechanism right you can update
based on that prediction you can you can
change what you do
next and would the cach value of that be
like just updating the manifold in the
original model or you know there's this
kind of MIT approach you know like the
dreamcoder type approach where you do
some kind of abstract Library learning
like do do you do do you think there's
some way of introspec
useful abstract knowledge or do you
think it it should just go back into the
original neuron Network um I think that
multiple ways to go about this so I
think certainly what you definitely want
to do is you want to improve your
representations and abstractions over
time uh but fundamentally I see those as
kind of strong intuitions that allow you
to kind of fetch the right piece of
information and then combine these
pieces of information to come up with a
strong prediction and for those these
abstractions are necessary um of course
what you also do and I think pretty
evidently what we also do as humans is
we store continuously store patterns to
our memory that we encounter at varying
kind of degrees of fidelity and you know
a machine can probably if you give it
sufficient memory store these patterns
at much higher Fidelity than a human
could ever do uh so I think both storing
information and updating kind of your
representations to account for the fact
that your environment is changing and
the amount or the types of information
that you're dealing with are changing
are both key aspects to getting a really
truly intelligent system yeah do you
think you know the 01 model does the and
by the way maybe we should talk about
this concept of a a scaling you know
like a inference time scaling law that
they introduce but do you think they do
something like this do you think that
they estimate their confidence or
knowledge and then they do a variable
amount of computation or do you think
it's still like quite basic um it's
really hard to comment on on what open
AI does or does not do with o1 because
unfortunately we really don't do don't
don't know much about what they're doing
internally um all that they're saying is
that they are spending some amount of
compute at test time uh to kind of
change their model or at least kind of
work their model around to end up with a
different prediction than they would
have otherwise ended up with if they
just did kind of one forward pass
through the model and to that ree I
think it is kind of related and it's
part of this Paradigm of of doing
spending computation locally of um kind
of changing the
resolution locally of the the accessible
resolution locally around prediction
that we're working on um yeah but I I
don't think that they are um necessarily
kind of doing this in this kind of
intrinsic uncertainty minimization
framework of kind of that is related to
active inference or or these types of
things because I haven't I haven't seen
them talk about this uh yeah but they
they kind of should be and and that's
the thing so you know when you use
language models all the time you get a
feel for them you know and and you can
feel when they go out of distribution
and because what's actually happening is
when you lean in you know when you ask
it about something where it's like you
know it's on a well sampled dense part
of the manifold you get rich answers but
but that manifold is actually it's like
a landscape and sometimes you're in no
man's land and it just gives you the
most benal like almost nonsensical
answers and wouldn't it be cool if it
knew that yes so I think now we're
really touching a very important point
and that is of can we tell whether we
kind of have information that is
relevant to making a good prediction or
not and that's actually some kind of
piece of information that we can extract
from these uncertainty estimates so kind
of getting a handle on our uncertainty
about making a prediction can be very
useful to tell is kind of the
information that is stored in memory can
that be useful given my current
abstractions um and this task that I'm
faced with to actually solve that task
or is it not useful in the Michael
Jordan example if the age of Michael
Jordan and his kids are not actually
stored in memory then um and also not in
your in your weights then there's no way
to make a good prediction there's no way
to produce a good output and we actually
you know discussed to some degree that
you can use these of uncertainty
estimates to kind of provide an insight
as to what your model is capable of
doing and what your model is not capable
of doing and um one of the key aspects
where this shows up is actually in
improving con convergence guarantees so
one of the cool things that you can do
is if you're minimizing kind of if
you're taking this principles approach
of selecting the most informative data
or selecting the data that maximally
reduces your model's uncertainty your
model's epistemic uncertainty then you
can show that relative to the data that
is available and relative to how good
your model abstractions are eventually
you will kind of make the best possible
prediction eventually you will your
uncertainty will shrink kind of as
shrink to be as small as possible and
and that is something that is not
possible if you if you select data using
nearest neighbor search um but but going
back to the other point that you made uh
relative
to do we in these dense parts of the
data manifold um do we actually have
models that are as good as they can be
and I I wouldn't be so confident about
that right uh clearly in those kind of
very spar parts of the data manifold we
have where the model hasn't seen much
data it's not good and and we realize
that today but I think a fundamental
problem is kind of in this inductive
Paradigm when you're pre-training the
models to be good everywhere clearly in
the in the parts that are most well
represented in your training
distribution it kind of if it weren't
doing well there its loss would be very
high so and and it kind of is is trained
to be very good there but still if you
kind of zoom in to these parts with what
whatever kind of representational
capacity you have you can still
represent them in a kind of at higher
Fidelity I guess like one thing I was
asking earlier but may maybe I don't
think I asked it in a very good way is
right now we've got this modality where
it's two extremes so we've got the most
General possible thing which is the
induction and then we've got the most
specific possible thing which is the
transduction
and could there be a middle way like for
example could you have a kind of pyramid
scheme where you do the most specific
thing but you also kind of like have an
ensemble of varying levels of like um
coarseness could that be better I think
to some degree this is happening already
so if you look at how models are trained
today models are trained by first kind
of in the pre-training phase just
fitting kind of this big mass of human
generated data and and you try to give
to it as much data as possible um but
then people realize okay I don't
actually care about uh kind of specific
data that I find on the internet in some
obscure Forum what I actually care about
is maybe math or is coding and then what
people do is they curate of data that is
very specific to that part of the data
manifold and they fine youe the model on
that and you can think about that as
some amortized form of transductive
learning um and actually I like to think
about transductive learning as as kind
of this entire Spectrum all the way so
this there's a special case of
transductive learning which is inductive
learning where it's really you care
about the entire data manifold and then
there are all these kind of cases where
you care about some subp parts of the
data man manifold so you kind of carve
it up into the actual region that you
find interesting for the task that
you're trying to solve but still you
train your model once and then you
freeze it and then you deploy it but
just on that subp part of the data
manifold and what local learning is
doing is it's really going to the
extreme end of that it's pushing it to
the extreme end saying that whenever you
use one of these language models let's
say you are always interested in just
making one prediction so you might as
well train a specific model just for
that one prediction and for the next
prediction you can again train a
specific
model yeah I mean it first of all it's
quite interesting that we're almost
going back to where we started which I
mean Google search is really good and
that's pure information retrieval and
then we kind of went 180 and we did pure
um you know inductive inference and now
we're kind of like meeting in the middle
but I think it has the potential to
solve a lot of the the kind of the
failure modes and problems that that we
have you know like when I'm doing
generative coding or something like that
the problem is always situations where
you're dealing with
ambiguity and yeah you can do a lot with
prompt engineering you you can say no I
meant this no I meant that but I think
there's a lot of that kind of ambiguity
resolution could be automated by having
some kind of like somewhere on that
transductive Spectrum yeah I think in
practice uh when we spend compute at
pre-training time usually and this is
kind of what what people today do with
scaling laws is they say okay I I'm able
to spend this amount of compute and then
uh I like the scaling laws tell you I
have I I'm able to train a model of that
certain size right uh but of course if
you had more compute what you would want
is you would train a bigger model right
on more data um and instead instead of
just training bigger models what you can
do is also to increase your
representational capacity or at least
your effective representational capacity
relative to making a certain prediction
is not to just increase the kind of
model size but instead is to use that
model size in a smarter way right
instead of just using that model size to
kind of trying to solve all problems at
once you can use that model size but
using kind of essentially duplicating
that model training a separate model for
every prediction now you don't have to
train a separate model from scratch and
that certainly would be very inefficient
so instead what you can do is you can
train a model that is amortized that is
still trying to solve all problems at
once at least to the degree that it's
capable of with its representational
capacity and with the amount of compute
you give it and then kind of Leverage
that as an initialization to learn a
specific model where then all of the
representational capacity is available
to fit the information or compress the
the information um that you need to make
a certain
prediction so let's talk about how this
might pan out practically so I don't
think these huge language models are
going away anytime soon right so you
know we're going to have the son at 3.5s
for doing coding and so on but I see a
future where interesting things might
start happening so for example laptops
are getting ridiculously fast now I've
just ordered the new M4 MacBook Pro it's
going to be amazing and um I mean that
could in principle fine-tune a llama
model as I'm going so you know like I'm
I'm on my repo it fine tunes it on the
repo and I can imagine there being some
kind of a hybrid thing where like we get
the small model doing the active fine
tuning and then we kind of generate
completion with that and then maybe we
San to check it back over with Claude
and then we do some information retrieve
do you see some kind of like practical
hybrid use case of this coming up um I
think that is certainly possible I think
but what seems certainly interesting is
that with a model that is smaller and
that kind of compresses kind of more of
this abstract information into a smaller
package that is it's easy to do kind of
to learn at test time with such a model
just because back prop through that
model but also forward pass it through
that model so leveraging that intuition
to search things is much easier um what
nevertheless what we have seen is that
at all scales kind of learning at test
time can improve performance so even if
you kind of work with kind of more more
state-of-the-art models still doing this
local learning at test time improves
performance so what I estimate is that
where you spend compute and how much
compute you spend will in future be very
dependent on the certain problem on the
concrete problem that you're facing
because uh there's certainly some amount
of fixed compute that you will have on
your machine but I believe that also
these uh kind of providers that of
provide these big llms over cloud
services will over time move to a
setting where they allow variable
amounts of inference time compute and
you can essentially tell how much
compute do I want to spend on the
certain problem so there might be a very
nasty kind of research problem that you
want to offload to an llm in the future
and which which seems kind of obvious to
you but like a lot of redundant work and
I believe in the future uh you will just
be able to tell it okay use whatever
kind of compute is necessary to solve
that at test time and that might be more
expensive than what your local computer
can handle but uh I think so I think I
think we will see this expansion of
compute and in two scales we will see it
one just leveraging the compute that is
available in our machines that would
otherwise be unused right but I think we
will also start paying so like actually
acquiring additional compute that would
otherwise be used for something else for
let's say pre-training we will start
paying for this additional compute to
solve more complex problems um just
because we have seen that at all scales
you can kind of get better performance
if you spend this additional computer
yes and it also fixes the fundamental
problems with the with the monolithic
Paradigm which is that right now it's
not open-ended right and when we
actually do have this ability for people
to explore different um parts of the
landscape and then we can kind of take
those lessons back to the collective
we've now got a Divergent creative AI
system as a whole which is which is
which is great but um Jonas this has
been amazing so first of all um I'm
really excited about your work and it's
great that that you're telling the
audience about it because first of all I
think uncertainty and confidence
estimation is really important so I hope
people have learned about that I think
intelligence is about being able to do a
commensurate amount of computation based
on your uncertainty that's really
important and I think that transductive
active fine tuning is going to be
absolutely huge over the next five years
so thank you very much for joining us
today it's been amazing so much for
having me on I'm really excited about
this

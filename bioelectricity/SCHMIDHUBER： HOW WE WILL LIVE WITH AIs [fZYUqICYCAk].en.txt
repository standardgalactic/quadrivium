AIS will at least initially be highly
motivated to protect humans rather than
killing them such AIS will have no major
incentive to say exterminate Humanity
like in the Schwarzenegger movies
instead many AIS will be curious
scientists and they will be fascinated
with life fascinated because life and
civilization are such a rich source of
interesting patterns at least as long as
they are not fully
understood today I think it is possible
that our planet is really the first in
our light cone um to spawn an expanding
AI bubble if we are the first indeed
then this would imply a lot of
responsibility not just for our little
biosphere but for the future
at the entire
universe let's not mess this
up mlst is sponsored by sensl which is
the compute platform specifically
optimized for AI workloads these guys
have done insane optimizations and their
CEO and co-founder Gennady explained
many of them when he came on the show
last month they know the secret source
and they've embedded it all into their
platform and that's passed on to you in
terms of it being cheaper uh faster
and just better so what are you waiting
for go to Cent ml. and sign up now I'm b
crusier um I'm staring an AI research
lab called tabs it is funded from past
Ventures involving machine learning so
we're a small group of very motivated
hardworking people uh we are hiring both
Chief scientists and uh de Planning
Engineer researchers we want to
investigate reverse engineer and uh
explore the techniques ourselves because
we're early there going to be high
freedom and high impact as someone new
at
tabs you again welcome to mlst it's an
absolute honor to have you on the show
my pleasure thank you for having me so
um before we move on to the great
technological advances of the new
century can you tell me a little bit
about the most influential invention of
the previous Century H so at the end of
the previous Century in 1999 the journal
Nature Made a list of the most
influential inventions of that
century and vaslav SM argued that the
most influential thing was the invention
that let the 20th century Stand Out
Among all centuries of all times because
that invention detonated the population
explosion from 1.6 billion people in
1900 to soon about 10 billion
people and there was one single
invention that was driving all of that
and without that one single invention
half of humankind would not even exist
because it's the driver of this
population explosion that we have
witnessed we don't know is it a good
thing or a bad thing but it was surely
the most influential thing that happened
in the previous century
80% of the air is nitrogen and the
plants needed to
grow but they cannot extract the
nitrogen from thin air and back then
around 1908 for half a century people
knew they need that stuff but they
didn't know how to extract it to build
artificial fertilizer enter the harbor
process or the harbor BOS process which
um under high temp temperatures and high
pressures extracts the nitrogen to make
artificial fertilizer so what would be
the most important thing in the 21st
century the grand theme of the 21st
century is even grander um true AI true
artificial
intelligence um is is going to change
civilization completely and AI will
learn to do anything which humans can do
and more and um there will be an AI
explosion and the human explosion or the
population explosion of the humans is
going to pale in comparison I mean do
you think that the AI intelligence
explosion is possible or desirable and
don't you think our sens making an
agency is part of our
purpose our sens making process is part
of our purpose I I agree with that but
but all of that is just part of this
grander process of the evolution of the
Universe from very simple initial
conditions to um more and more
unfathomable complexity and this
Evolution led to our sense making
process which is currently setting the
stage for something that that goes
beyond it modern large language models
like chat GPT they're based on self
attention Transformers and even given
their obvious limitations they are a
revolutionary technology now you must be
really happy about that because you know
a third of a century ago you published
the first Transformer variant what are
your Reflections on that today in fact
in 1991 um when compute was
maybe five million times more expensive
than today um I I published this um
model that you mentioned which is now
called uh the
unnormalized linear
Transformer I had a different name for
it I called it a fast weight controller
but names are not important the only
thing that counts is the
math so
this linear Transformer is a neural
network with um lots of
nonlinear um operations within the
network so it's a bit weird that it's
called a linear
Transformer however the linear and
that's important refers to something
else it refers to
scaling a standard Transformer of 2017 a
quadratic
Transformer if you give it 100
times as much
input then it needs 10,000 times 100
times 100 is 10,000 times as many
computations and
um a linear Transformer of 1991 needs
only 100 times um the
compute which makes it very interesting
actually because at the moment many
people are trying to come up with more
efficient Transformers and um and and
this old linear Transformer of 1991 is
therefore um a very interesting starting
point for additional improvements of
Transformers and similar
models so what did the linear
Transformer do assume the goal is to
predict the next word in a
chat given the chat so far and
essentially the linear Transformer of
1991 does this to minimize
its error it learns to generate patterns
that um in in modern Transformer
terminology are called keys and values
keys and values uh back then I called
them from and
to but that's just
terminology and and it does that to
reprogram parts of itself such that its
attention is directed in a context
dependent way to what is important
and and a good way of thinking about
this linear Transformer um is is this
the traditional artificial neural
networks uh have storage and control all
mixed
up the linear Transformer of 1991
however has a novel neural network
system that
separates storage and control like in
traditional computers and traditional
computers for many decades storage and
control is separate and the control
learns to manipulate the storage and so
with these linear Transformers you also
have a slow Network which learns by
gradient descent to compute the weight
changes of a fast weight Network how it
learns to create these Vector valued key
patterns and value patterns and uses the
outer products of these keys and values
to compute rapid weight changes of the
fast Network and then the fast network
is applied to Vector valued queries
which are coming
in so essentially um in this fast
Network the connections between strongly
active parts of the keys and the values
they get stronger and uh others get
weaker and this is
um a fast weight update rule which is
completely differentiable which means
you can propagate through it so you can
use it as part of a larger learning
system which learns
to back propagate errors through this um
Dynamics and then um learns to generate
good keys and good values in certain
contexts such that the um entire system
can reduce its error and can become a
better and better predictor of the next
word in the in the
chat so um sometimes people call that
today a fastweight matrix
memory uh and and and the modern
quadratic Transformers they use um in
principle uh exactly the same
approach you mentioned your fabulous
year 1991 where so much of this amazing
stuff happened um actually at the
Technical University of of Munich so
chat GPT um you had invented the T in
the chat GPT the Transformer and um also
the p in the chat GPT the the
pre-trained um Network as well as the
first adversarial um networks as well
Gans could you say a little bit more
about that yeah so uh the Transformer of
1991 was a linear Transformer so it's
not exactly the same as the quadratic
Transformer of today but nevertheless
it's using these Transformer principles
and um and the P the p and GPT
yeah that's the pre-training and back
then um deep learning didn't work but
then we had um networks that
could use predictive coding to greatly
compress long sequences uh such that
suddenly you could work on this reduced
space of these compressed uh data
descriptions and deep learning became
powerful where it wasn't possible before
and then the generative adversarial
networks also in the same year 1990 to
1991 um how did that work well back then
we had two two networks one is the
controller and the controller has
certain um probabilistic stochastic
units within itself and they can
learn the mean and the variance of a
gaussian and there are other nonlinear
units in there and then it is a
generative Network that generates
outputs output p patterns actually
probability distributions over these
output patterns and then another Network
the prediction machine the predictor
learns to look at these outputs of the
first Network and learns to predict
their effects in the
environment so to become a better
predictor it's minimizing its error
predictive error and at the same time
the controller is trying to generate
outputs whether SE network is still
surprised so the first guy tries to fool
the second guy trying to maximize the
same objective function that the second
network is
minimizing so today this is called
generative artificial no no generative
adversarial networks and I didn't call
that generative adversarial networks I
call it artificial curiosity because you
can use the same principle to let robots
explore the environment controller is
now generating actions that lead to
behavior of the robot the prediction
machine is trying to predict what's
going to happen and it's trying to
minimize its own error and the other guy
is trying to come up with good
experiments that lead to data uh where
the where the
predictor or the discriminator as it is
now called can still learn something um
so when when did you realize that modern
computers are good enough to run the
technology that you invented so long ago
uh by 2009 compute was cheap enough such
that our lsdm through the efforts of my
former PhD student Alex Graves could win
competitions and um that was in
handwriting and and Fields like that and
then um in 2010 my team with my my
separate team with my postto Dan Jiran
from Romania broke the amnest um
Benchmark with another approach with um
standard all traditional neural networks
implemented on Nvidia gpus so for the
first time in 2010 we had a really deep
uh supervised networks that that um
outperformed everything else on this
back then famous Benchmark back then
compute was maybe 1,000 times more
expensive than today and then um in 2011
came the danet danan danet and um and
danet had a Monopoly on winning computer
vision contests with um GPU based
convolutional neural networks and D
Net's um first superhuman result was
also achieved in 2011 so it started in
2011 and then four competition computer
vision competitions in a row were won by
that um daret and that's um that's when
it became clear um now there's a new way
of using these old neural networks of
the previous Millennium uh to
really change computer science yeah I'm
I'm interested in this concept called
the hardware Lottery uh Sarah hooker
wrote a paper with the same title um I
think in the year 2000 when she was at
Google brain she's now at coh here
actually but she basically said that the
only reason we had the current charge in
in AI is because we created all of these
gpus for computer games and it was just
fortuit
that that allowed us to build all of
these deep learning models I mean what
what's your take on that yeah she is um
kind of right um You need lots of Matrix
multiplications to compute how the
screen should change as you are moving
through an ego shooter game and um and
that's why gaming was pretty much the
first industry that greatly profited
from uh massively parallel Matrix multip
ations on gpus uh towards 2010 however
we realized that the same Matrix
multiplications can uh greatly speed up
U these old deep learning methods um and
can speed them up enough to to beat all
the other methods yeah it's really
interesting because of course Nvidia now
I think last week it became the world's
most valuable company which of course is
hundreds of times more Val B than it was
in 2010 what do you think about that
indeed um nvidia's CEO Jensen hang he
realized that deep learning could take
his company um to stratospheric levels
and and he did interesting okay so if I
understand your main argument is that we
just needed to wait for the compute to
catch up and now here in the 20th
century here we are yes so all of
um what we are experiencing today um is
based on stuff that was invented in the
previous Millennium but it had to scale
up so the hardware was invented back
then and the software the algorithms
were invented back then but the
industrial processes for making faster
and faster parallel gpus they weren't as
developed as today and so we are really
greatly profiting from this Hardware
acceleration and that's the reason why
uh AI broke through not in the previous
Millennium but um had to wait until the
current Millennium was well underway for
example the first convolutional neural
networks or the
cnns uh which we used in um in the danet
of 2011 they were published much earlier
in Japan uh in
1979 kunihiko Fukushima had the basic
deep CNN architecture with you know
convolution layers down samping layers
convolution down sampling uh he didn't
use back propagation yet to train it but
then in uh
1987 Alex viel another guy um working in
Japan um originally from Germany he um
combined convolutions with back
pation the method invented by or
published by seina inma
is a Finnish guy in in Helsinki in 1970
and then in 1988 Tang also published in
Japan the two-dimensional cnns that
everybody is using now U and combined
them with back publication and that's
how Between 1979 and 1988 uh CNN's
emerged in in
Japan which is kind of interesting
because back then Japan also was um
considered the land of the future and
they had more than half of the robots of
the world and and the seven most
valuable companies back then they were
not based in America like today except
for Saudi arango um but they were all
based in Japan and
the and the central square mile of Tokyo
had the value of California what a
difference a couple of decades make yes
everything has changed so um what are
your favorite examples of applications
um you know with this AI that your team
has developed I remember when I went to
China 15 years ago and I still had to
show the taxi driver a picture of the
hotel where I want to go uh and today he
is speaking in a smartphone in mandarine
and um I hear the translation and then I
say something and the smartphone
translates it back into
mandarine and we can communicate um like
old friends the taxi driver probably has
no idea that this is powered by
techniques developed in my my little
labs in yunic and in Switzerland and the
'90s and early 2000s U but but I'm happy
to see that that our AI has
really it has really broken down
communication barriers not only between
individual people but between entire
nations so that's reallyy cool
yeah um I I completely agree actually I
don't know if you know this year again
but um I co-founded a startup called
X-ray and it does exactly what you says
it it does this kind of Babble fish
translation you know with speech
recognition and TTS so you can do
exactly what you just said and it's
really interesting I had lunch on Friday
with will the CTO of speech matics and
he was telling me all about the secret
source of how their speech recognition
algorithms work and um I better not say
but you you would you would be delighted
I'm sure but anyway um just sort of
moving off that a little bit what other
examples can you think of I am um
especially happy that our AI makes human
lives longer and healthier and uh easier
um with thousands of applications in
medicine and Drug design um sustainable
development uh in September 2012 my my
team with Dan Jiran had the first
artificial neural network to win a
medical imaging contest that was about
breast cancer
detection in slices through the female
breast and and if you go to Google
Scholar and you just type in some
medical topic uh plus lstm you will find
thousands of papers that have lstm in
the title not just not just somewhere in
the text but in the title and it's about
you know learning to diagnose
ECG
analysis diagnosis of
arhythmia
um card cardiovascular disease risk
prediction um four dimensional image
segmentation for for medical
images um automated sleep stage
classification Co detection Co
prevention thousands and thousands of
topics so it's really nice to see that
especially in the in the medical field
there's a lot of impact of these
techniques some claim that technology
like chat GPT is on the path to AGI and
others claim that it's like building a
taller Tower trying to get closer to the
moon what do you
think well large language models of
course are far from
AGI um llm large language models such as
chbt they are just a a clever way of
indexing the world's existing human
generated uh knowledge such that can
easily be be addressed through um in a
way that humans are familiar with um
which is natural language um that's good
enough to facilitate many desktop jobs
uh for example writing summaries of
existing documents in a particular style
or creating illustrations or an article
um and and and so on however true AGI
goes far beyond that uh it is much
harder for example to replace Craftsmen
such as plumbers or electricians because
the the real world the physical world is
much more challenging than you know the
World Behind the screen at the moment
the only AI that works well is behind
the screen and it's good for desktop
desktop workers but but not really for
for people working in the physical world
and for a quarter Century the best chess
player hasn't been human anymore uh and
and and learning to play uh chess or
other board
games or or video games is rather easy
now for AIS but real world games such as
football they are much harder there is
no Aid driven football playing uh
embodied uh robot that that can compete
with a seven years old boy you know and
that's why 10 years ago in 2014 in 2014
we founded our AI company for the uh
physical world n it is called pronounced
like like nay in English like birth in
English except it's spelled in a
different way NN for neuron Nets AI for
artificial intelligence and sins alas um
like like some of our projects it may
have been a bit ahead of time again uh
because the real world is really really
challenging so you've said that this is
related to Consciousness in some
way it is um my my first deep learning
uh system of um
1991 simulates aspects of Consciousness
as as follows um it uses unsupervised
learning or self-supervised learning and
predictive coding to to compress uh
observation sequences so there is a
so-called conscious um chunka neural
network and the chunka attends to
unexpected events that surprise a lower
level uh so-called aut aut izer the
subconscious automati neural network and
and the chunka neural network basically
learns to understand the surprising
events um so those events that were not
predicted by the automatizar the
surprising events by predicting them on
a high level if there's a higher level
regularity that it can use for that and
um the automatized neural network uses
um then this neural network distillation
procedure of 1991 also published in 1991
to compress and absorb the the formerly
conscious um insights and behaviors of
the chunka so the chunka is still you
know working on its search space still
has a problem to solve because
unexpected stuff is happening and then
it solves it and then distills it down
into the automatizar which um is called
the automatizar because the step there
isn't conscious anymore because now
everything is working according to plan
and as predicted and so it's all good
when we now look at the predictive World
model of the uh the controller
interacting with uh an environment as as
discussed earlier uh it also learns to
efficiently encode the the the growing
history of actions and observations
through predictive coding and it
automatically so what what is that
predictive coding you just try to
predict if you can predict it and you
have to store it extra in some way and
and it automatically creates feature
hierarchies lower level neurons
corresponding to uh simple feature
detectors um perhaps even similar to
those found in the mamalian brain and
then higher layer neurons uh typically
corresponding to more abstract features
but F grain when uh necessary and so
like any good compressor the world model
the predictive world model will learn to
identify regularities shared by um um
existing internal data structures and it
um will generate U prototype
encodings across neuron populations or
in other words compact representations
or symbols if you will um um not
necessarily discrete symbols so I never
saw the precise difference between
symbols and sub symbols for it will
create such SYM for frequently occurring
observation subsequences to to shrink
the storage space needed for um for the
whole and so in particular what we will
notice in such a system is that compact
self representations or self symbols are
just natural byproducts of the data
compression process since um since uh as
the agent is interacting with the
worldall there is one thing that is
involved in all actions and sensor
inputs of the agent which is um the
agent itself and uh and U to efficiently
encode the entire U history uh of
observations and actions that were um
executed so far and the observations
that
were observed so far to in in code the
entire history through predictive coding
it will profit from creating some s of
internal sub network of connected
neurons Computing newal activation
patterns representing the agent
itself and then it has a self symbol and
so whenever the um planner the world
model of the agent is used to think
about the future and what could be
possible action sequences to maximize
reward whenever that happens and
whenever this planning process wakes up
the self symbol or these neurons that
stand for the agent itself then the
agent is thinking about itself and about
possible futures of this agent and you
know essentially it's doing
counterfactual reasoning as it is now
called just planning um to to find a way
to optimize its reward and um and the
self-awareness is just a natural
byproduct of
the data compression process of the
world model as the Asian is interacting
with the world and um and creating the
data that leads to the world model you
know yeah so since we have had such
systems for more than a third of century
I'm always claiming that we already had
self-aware and
conscious systems for um you know more
than three decades yeah a couple of
points on that I mean I guess
consciousness invokes many different
thoughts like um you know David Charmers
coined the hard problem which is the
what and how question of qualitative
experience you've just described it in
terms of self modeling which is quite
similar to how Max Bennett did on his
recent brief history of intelligence and
we've got six hours of content coming
out on that with Max by the way but um
you know Mar Mark SS for example thinks
of Consciousness as an affex system and
Michael Graziano thinks of Consciousness
as a kind of um you know recursive
attention system and I guess I'm saying
Consciousness means different things to
different people right
yes but there's only one correct way of
thinking about
it
okay yeah the the thing we spoke about
earlier about um you know the learning
subg goals and and and the the cooning
in in the action space it reminded me a
little bit of yan mun's H jeer paper
which I read a couple of years ago and
the basic idea is a jeer I'm sure you
know this but for the audience it stands
for joint embedding prediction
architecture and the idea is that it can
learn increasingly abstract
representations by kind of predicting
what is unobserved from what is observed
so in some cases it means deliberately
removing data to kind of force the model
to you know to learn powerful
representations but in this particular
example it was done in in action space
so you know learning unobserved actions
and also in abstraction space and
because it was done hierarchically was
done you know with with many kind of
orders recursively you know kind of
applying if that makes sense so that's a
really interesting um model and it's
using his his energy based models um as
as well but how how is that related to
your work on on the the sub
goals yeah yeah uh so that sounds a lot
like my 1990 sub generator so back then
I realized millisecond by millisecond
planning isn't good instead you somehow
as you are trying to solve problems you
have have to um decompose your possible
Futures into sub goals and then you just
maybe execute some know and sub program
to achieve that sub goal and from there
you go to the next sub goal as you're
finally um reaching the goal and then um
in the beginning of course you don't
know what is a good sub so you have to
learn that stuff you have to learn a new
representation of something that you
want to achieve as a sub goal as you are
trying to um achieve the final goal and
so um this 1990 sub generator was really
simple but um had already basic
ingredients of what you need to
do this was really three decades before
Lon had this recent um paper out there
uh so what happens there you have a
neuron Network which um which um
observes a reinforcement learner and it
models uh the costs for going from
certain start places to goal places so
you have a neural network get as input
start and goal and predict F the costs
of going from start to goal the reward
that you will experience as you as you
do that and um and now maybe there are
lots of starts and goals and you don't
know how to go from start to goal but
maybe maybe you can learn a sub and how
do you learn a sub well you need
something like a learning machine that
is good at generating good sub goals how
do you do that well we have a sub goal
generator that's going to learn good sub
goal how does that work well the subal
generator right so the subal generator
gets as an input a start input and a
goal and now the output is not an
evaluation but it's a sub goal so start
and goal input output is a sub goal then
you have two copies of the evaluator the
evaluator sees um the first evaluator
sees the start and the sub call which is
maybe a bad sub call coming from the sub
call generator and then the second copy
of the evaluator sees the um subg goal
and the goal and now both of them they
predict the costs and what you want to
do is you want to minimize the sum of
the costs of these both um of these two
evaluators uh how do you want to
minimize that well by finding a good sub
goal through gradient descent that's
what the 1990 sof generator does so in
in some ways at least in principle it
solves a a problem that um that um the
 called an open problem in 19 in in
2020 or something what do you think of
Yan's energy based models by the way so
this uh recent um paper by Lon on um
hierarchical planning um is really a
rehash of stuff that we have been doing
for decad since
1990 are you worried that AI is going to
be dominated by just a few companies and
everyone else will lose out what do you
think 40 years ago um I knew a guy who
had a Porsche a rich guy with a Porsche
and the most amazing thing was um in his
Porsche he had um he had a mobile phone
so he could grab the receiver and talk
to anybody who also had a Porsche like
that with a mobile phone via
satellit and today a couple of decades
later everybody billions of people in
their pocket have a mobile phone which
which is much much better than what he
had in his
Porche and it's going to be the same
thing with AI every five years AI is
getting 10 times cheaper and it won't be
just a few big companies that are going
to um dominate Ai No it's going to be AI
for all and the open source movement is
just um a few months maybe I don't know
eight months behind the um big um major
players and they don't really have a
moot which means
um the future will be bright and lots of
people are going to profit from really
cheap AIS that in many ways are going to
make human lives longer and healthier
and easier which happens to be the
company of my the the motto of my
company
n what's your take on the AI race
between Europe and um China and the
US well Europe is the Cradle of um
mechanical Computing in ancient Greece
and the calculator in
1623 and pattern
recognition around 1800 and program
control machines in
1804 and um and and practical AI around
1912 you know first chess end game
players and the transistor
1925 and um and uh theoretical computer
science 1931 and AI Theory the theory of
AI 1931 GLE U the general purpose
computer 1935 to 1941 um deep learning
1965 and the Ukraine self-driving car
1980s the worldwide web 1990 and so on
and um and more recently the basic deep
learning algorithms were also invented
and developed by Europeans on the other
hand um the companies with the highest
profit in most of these fields are
currently not any longer in Europe but
on the Pacific Rim West Coast United
States and East Coast Asia and there you
will find much more capital and much
bigger efforts in terms of industrial
policy and also defense it's going to
stay like that for a while I guess so
why doesn't everyone know that AI
started in
Europe maybe because the old continent
is really bad at
PR and once AGI is actually here what's
next for
humans in the long run most of the agis
are going to pursue their own
goals such AIS have existed in my lives
for decades many agis however will be
tools that um do all the work that
humans don't want to do um nevertheless
freed from from hard work the the
playing man homoludens the playing man
will um as always invent new ways of
professionally interacting with other
humans
and um already today most people
probably you too are working in luxury
jobs which unlike farming are not really
necessary for the survival of our
species in in a really high level what
what is the history of
AI the history of modern Ai and um deep
learning you can find that in my 2023
survey uh which has that name some of
the highlights are of course 1676 um the
chain rule by liet which is today used
uh in all these programs such as tensor
flow and pytorch to assign Credit in
deep neural networks um then 200 years
ago the first linear neural networks by
gaus and LeAndre exactly the same aror
function that we have today exactly the
same architecture the same weights um
then um 1970 the technique called back
propagation which um essentially
implements liit chain rule in a very
efficient way for deep um multi-layer
neural network systems then 1967 amar's
work uh in Japan on stochastic gradient
descent for deep networks lots of
additional uh fundamental breakthroughs
convolutional neuron networks also in
Japan between 19 1979 and
1988 um and then our own miraculous year
1990 1991 with lots of stuff that is
today in your smartphone um and I could
continue forever so instead just have a
look at that survey also has images of
the guys who who did important who had
important
contributions I mean isn't this quite
different from the very us Centric view
of AI history
in fact a misleading history of deep
learning by Sosi and others goes more or
less like this in
1969 Minsky and papat showed that
shallow neural networks without hidden
layers are very limited and the field
was abandoned until a new generation of
neural network researchers took a fresh
look at the problem in the
1980s so that's a quotation basically
from zov's book however the 1969 book by
Minsky
addressed um a problem a problem of gaus
and LeAndre shallow learning from the
1800s that had already been solved four
years prior uh by iak nenko and lapa's
um deep learning method in the Ukraine
and then also by amar's stochastic GR in
descent for for um multi-layer
perceptrons just uh two years later and
so for some reason Minsky was apparently
unaware of this and failed to correct it
later today however we know the true
history of course U deep learning
started in Ukraine in 1965 and continued
in Japan in
1967 regarding credit assignment so um
you've criticized Benjo and Lon and and
Hinton and um accused them of of
plagiarism you said that they
republished key methods and ideas whose
creators they failed to credit and in
2023 you published a long report on this
what's your updated take on that their
most famous work is completely based on
work by others whom they did not
site and even later they failed to
publish Cor
or AATA this is what you do in science
when somebody has published the same
thing before you and even in later
surveys they didn't credit the original
inventors of the techniques that they
are using and instead they
credited each
other total no-o in science but science
is self-correcting as Elvis Presley put
it truth is like this
son you can shut it out for a time but
it ain't going way plagiarism is is a
very significant charge could you give a
few concrete
examples many of the priority disputes
affect my own deep learning team because
um the AES often republish techniques of
mine without citing them and in fact
their most visible work builds directly
on ours but but I'll skip that for now
you can read it um you can read about
that in the public report um of
2023 which is easy to find nevertheless
let me mention some of the other
researchers whom they fail to credit
then I don't have to talk about our own
team for example in a recent
survey of deep
running they describe what they call the
origins of deep Learning Without Even
mentioning the the world's first working
deep learning networks by iak nenko and
lapa in Ukraine
1965 iako and lapa they used layer by
layer
training subsequent pruning with a
separate validation set and um and IO he
had deep eight layer networks by
1970 uh hinton's 2006 much later paper
on on layer by layer training also
failed to site this stuff the very
origins of deep learning the first
methods that um that really worked in
deep learning and later surveys still
didn't give credit to these original
inventors and uh the AES also failed to
site amar's
1967 uh work which included computer
simulations on learning internal
representations of multi-layer
perceptrons through stochastic gradient
descent there was almost two decades
before the
AES published their first experiment
work on on learning internal
representations their survey also
mentions back propagation famous
technique and their own papers on
applications of this method but
neither the inventor of back propagation
which was seina inma in 1970 nor its
first application to neural networks by
verbus in 1982 verbus also had a 19 74
thesis but that was not correct and they
didn't even mention Kelly's precursor of
the method in
1960 uh not even in the latest
surveys they also refer to lon's work on
convolutional neural networks citing
neither Fukushima who created the basic
CNN architecture in the
1970s no B who in 1986 seven was the
first to combine neural networks with
convolutions and back propagation and
weight sharing nor the first backprop
trained two-dimensional um convolutional
neural networks of Tang I hope I
pronounced that correctly in
1988 now modern CNN's originated before
lon's team helped to improve them and
this is not at all clear from their
papers
they cite Hinton 1981 for multiplicative
gating without mentioning iok NCO and
lapa who had multiplicative gating in
deep networks already in
1965 in the report which is easy to find
on the web I'm mentioning many many
additional cases all backed up by plenty
of um
references so what do you think should
be done they have have violated the code
of ethics and professional conduct of
the organization that hands out these
Awards
so they should be stripped of their
Awards so how do such problems as you've
stated them reflect on the broader field
of machine learning they reflect the
immaturity of our field in a Major Field
such as mathematics you would never get
away with
this anyway science is
self-correcting and we'll see that in
machine learning
too sometimes it may take a while to
settle disputes but in the end the facts
must always
win as long as the facts have not yet
won it's not yet the
end many um you know philosophers and
scientists and physicists and
entrepreneurs they have
become obsessed with this idea of AI
existential risk what do you think about
that as a real expert in in
AI many talk about AIS but few build
them and uh I have tried to allay the
fears of some famous doomers pointing
out that there's immense commercial
pressure to use um our artificial neural
networks to build friendly AIS good AIS
uh that make their users healthier and
happier and more addicted to their
smartphones nevertheless we can't deny
that armies perform research on Clever
robots as well right that's true uh
people who should know told me that our
AI is also used to steer military drones
um or here is my old trivial example
from 1994 when when an Sans had the
first truly self-driving cars in highway
traffic uh similar machines can also be
used by the military as um as
self-driving landmine Seekers and many
would argue that's maybe not such a bad
thing so are you saying it's not
possible then that AI will become really
dangerous AI can be weaponized as
obvious in the recent Wars driven by
Cheap AI based drones but um AI does not
introduce a new quality of existential
threat we should be much more afraid of
half century old technology in form of
hydrogen bombs and U hbomb
Rockets a single a single hbomb can have
more destructive power than all
Conventional Weapons or all weapons of
World War II
combined many many people forget that
despite the dramatic nuclear disarmament
since the
1980s there are still enough hbomb
Rockets to wipe out civilization as we
know it within a few hours without any
AI but I'm I'm trying to figure you out
Jurgen because many AGI Skeptics make
the argument that it's impossible in
practice to build this kind of
intelligence but you don't think that
because in your lab you've been building
agential AI you know which is to say AIS
that create their own goals for decades
so you do think that this thing could be
incredible are you just making the
argument that the risk is still much
lower than the H
boms so at the moment um H bombs are
much more
worrisome
than any um AI based drones and what you
have
now in the long run of course you have
to think about what's going to happen um
once AI weapons are not
just used as
tools by other humans who have conflicts
and
use their own AI weapons against the AI
weapons of the other guys what is going
to happen you will have to ask in the
long run once
um really powerful AIS
are going to do their own thing and um
expand in a into space in a way that
goes beyond where humans can
follow but we will get to that later so
what will super smart AIS actually do
as I have emphasized for decades space
is hostile to humans but really friendly
to appropriately designed robots and it
offers many more resources than our thin
film of
biosphere which receives less than a
billionth of the sun's energy and while
some Curious AI scientists will remain
fascinated with
and the biosphere as at least as long as
they don't fully understand it most of
these AIS will be more interested in the
incredible new opportunities for robots
and software life out there in
space and through innumerable
self-replicating robot factories and
self-replicating societies of robots and
the the asteroid belt and Beyond they
will transform the solar system and then
within a few hundred thousand years the
entire galaxy and within tens of
billions of years the rest of the
reachable universe uh in a way where
humans can't really follow um despite
despite the light speed limit the
expanding AI sphere will have plenty of
time to colonize and shape the entire
visible
Cosmos let me stretch your mind a little
bit the universe is still young only
13.8 billion years old let's multiply
this by four uh let's look ahead to a
time when the cosmos will be four times
older than it is now about 55 billion
years old that's how long it's going to
take to permit the expanding universe
that is currently visible uh by then the
visible Cosmos will be full of
intelligence U because um once this
process has started u most AI um will
have to go where most of the physical
resources are to make more AIS and
bigger AIS and more powerful
AIS because those AIS who don't do that
they won't have an impact many years ago
I said in a Ted X talk where I wore
exactly this outfit think of human
civilization as part of a much grander
scheme an important step but not the
last one on the path of the universe
towards more and more unfathomable
complexity now it seems ready to make
its next step
a step comparable to the invention of
life itself over over 3.5 billion years
ago so this is much more than just
another Industrial Revolution this is
something new that transcends humankind
and even
biology and it's a privilege to witness
its beginnings and to contribute
something to it so what about this um
firmy Paradox you know like why have we
not seen any signs of intelligence in
the universe first of all what what I'm
saying today is actually the same thing
that I have told my mom and others since
the
1970s and um when I was a boy back then
a teenager um I thought I thought about
this particular question a
lot uh as a boy I already knew something
about the vast empty spaces observed
between clusters of galaxies and and my
first thought back then uh was that uh
maybe they are expanding bubbles
colonized by AIS which are already using
most of the local energy in form of
stars and whatever making those bubbles
appear dark although they are full of AI
and and then I learned however that
gravity itself is sufficient to explain
the the sparse large scale Network
structure of the universe so that
explanation became a little bit less
convincing and uh my next thought was
that um that maybe the mysterious dark
matter which makes up most of the mass
of the known universe might um be Stars
whose energy is used by AI
civilizations whose
um uh Communications are so well
encrypted that they look like random
noise to us um but um but this also
seemed
implausible as um dark mattera is um
present um in all galaxies in all
galaxies including our own and this um
leads to the question why are there any
stars left in the Milky Way our local
Galaxy whose energy has not been tapped
yet and why don't we observe a constant
bombardment through um non-encrypted
construction plans of AIS who uh want to
um to spread by radio without first
having to build physical receivers far
from their Origins today I think it is
possible that our planet is really the
first in our light cone um to spawn an
expanding AI bubble uh Earth's
multibillion year window for biological
evolution is almost over in a few
hundred million years the sun will be
too high
for Life as We Know
It ignoring human-made global warming
just the Sun by itself and um and
perhaps humans were extremely lucky to
evolve barely in
time maybe through a series of extremely
improbable events to invent Agriculture
and civilization and bookprint and
almost immediately afterwards AIS just a
few hundreds of years later AIS so if we
are the first indeed then this would
imply a lot of responsibility not just
for our uh little
biosphere but for the
future of the entire
universe let's not mess this up indeed
let's not mess this up um it's quite
interesting actually you know many
science fiction or offers over the last
100 years or so they have imagined a
kind of monomaniacal Monolithic super
intelligence dominating everything I
mean what do you think about that I have
often argued that it seems much more
realistic to
expect an incredibly diverse variety of
AIS trying to achieve all kinds of
self-invented goals
in in the lab we had such IIs already in
the previous Millennium and and to
optimize all kinds of partially
conflicting and quickly evolving utility
functions many of them generated
automatically um we have evolved um
utility functions for reinforcement
learning machines already in the
previous Millennium where where each of
these AIS is continually trying to
survive and adapt to rapidly changing
niches
in AI ecology is driven by intense
competition and
collaboration Beyond current
imagination to reiterate I mean
something that I do find surprising is
that you agree with the the xris people
you know like you you think that it's
conceivable to have recursively
self-improving um agis that pursue their
own goals that create their own goals
but then I asked the question I mean I
know you've got two daughters I mean do
you think about the world they'll be
living in alongside AIS that are
creating their own goals and acting
autonomously being curious and creative
you know in the way that that humans are
but on potentially a much grander scale
not too much such AIS will have no major
incentive to say um
exterminate Humanity like in the
schwarzeneger movies instead many AIS
will be curious scientists remember the
artificial curiosity we discussed
earlier and they will be fascinated with
life fascinated and they will be
fascinated with their own with ai's
origins in our civilization at least for
a while because life and civilization
are such a rich source of interesting
patterns
at least as long as they are not fully
understood and so AIS
will at least initially be highly
motivated to protect humans rather than
killing
them so um you know once AI fully
understand all of this what happens next
then humans May hope for another type of
protection through lack of interest on
the other side why is that unlike in sha
movies there won't be many direct goal
conflicts between us and Them humans and
um others are mostly interested in
similar beings with uh whom they can
either
compete and or collaborate
because they share the same
goals that's why politicians are mostly
interested in other
politicians and CEOs of companies are
mostly interested in other CEOs of
similar
companies and kids are mostly interested
in other kids of the same age and ants
are interested in other ants just like
humans are mostly interested in other
humans not in ants so super smart AIS
will be mostly interested in other super
smart AIS not in
man it's it's man himself who is the
greatest enemy of man but also Man's
Best
Friend similar for
AIS do you imagine a future where AIS
and humans will merge together to create
something even more powerful than pure
AIS we have been cyborgs merging with
our technology
for centuries for example by wearing
glasses or
shoes um but combinations of AIS and
humans more powerful than pure
AIS in the long run this seems very
unlikely to
me of course many humans hope for some
sort of immortality through brain scans
and subsequent mind uploads into virtual
realities or virtual
Paradise or maybe into robots and um and
this is a a physically conceivable idea
discussed by science fiction novels
since the 1960s I think the first novel
of that kind was Similac 3 1964 however
to compete in rapidly evolving AI
ecologies uploaded human Minds will
eventually have to change beyond
recognition becoming something very
different and nonhuman in the process um
giving in to succumbing to all these
temptations that you have in such a
virtual Paradise to be become something
that has not only two eyes but millions
of eyes and sensors and actuators so
traditional humans won't play a
significant role in the spreading of
intelligence Across the Universe I don't
think they will one thing that concerns
me is um I mean David Charmers for
example he put forward this idea that
the fundamental substrate of the
universe might be information which is
really interesting but in a way it also
led him to to say that certain
structural patterns of information
processing so certain Dynamics give rise
to Consciousness and give rise to minds
and when you take this kind of substrate
Independence view it levels the playing
field of of moral status so one thing
that worries me is if we adopt this view
then couldn't you just make the argument
that AIS potentially could have a higher
moral status than us if indeed they have
more complex information processing than
us many science fiction authors of the
previous Century from stanislav LM to
Isaac azimoff have described AIS and
superum robots whose moral status is
obviously higher than the one of their
human counterparts and protagonists and
this has been a popular idea at least in
science fiction
generally speaking moral values have
changed a lot across time and
populations and certain moral values
have survived for a while because they
gave a temporary evolutionary advantage
to beings and societies who adopted them
um however Evolution isn't over and um
and the universe is still young
so um it sounds like you've got an
all-encompassing view of the universe
life and
everything indeed in
1997 I wrote my first paper about this
what is the simplest explanation of our
universe since 1997 in My Secret Life as
a digital physicist I have published on
the very simple asically fastest optimal
most efficient way of computing all
logically possible universes all
computable
universes including
ours as long as there is no evidence
that our universe is not computable um
we we stick with this assumption so at
the moment we don't have any physical
evidence against
this this was a generalization of
Everett's many world Theory of physics
but now it's more General in the sense
that you have all kinds of different
universes with different physical and
computable laws now any great programmer
great programmer with any self respect
should use this optimal method to create
and master all logically possible
computable
universes uh thus generating
us um as byproducts and um generating
many histories of deterministic
computable universes many of them
inhabited by observers like ourselves
and and due to certain properties of the
asymptotically optimal method and there
is one many people don't know there is
one but there is one at any given time
in this um all encompassing
computational process most of the
universes computed so far that contain
yourself will be due to one of the
shortest and fastest programs that
computes
you and this little insight allows for
making highly non-trivial and
encouraging
predictions
about our future about your
future you again this has been amazing
do you have any final messages for the
mlst audience yes don't worry in the end
all will be good touch
wood um you again it's been an absolute
honor to have you on on the show it's
been a dream of mine to do this in the
flesh and I really appreciate you coming
on thank you so much that's very kind of
you to say that and it was a great
pleasure for me thank you

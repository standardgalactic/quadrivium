welcome to the future of Life Institute
podcast my name is Gus Ducker and I'm
here with Connor Ley who's the CEO of
conjecture Connor welcome to the podcast
thanks for having me back glad to have
you you're the author of The compendium
which is an introduction to AI risk from
the ground up H why did you write this
so a saying that I wrote it is very
generous it was very much a team project
where I probably contributed you know
not the most so it was a it was a team
project between me Chris gell Adam shimi
G al4 and Andre miti all contributed
important parts so it couldn't have been
done without my coauthors just want to
really stress that so the reason we
decided to write the well I think I was
actually the person who decided to write
it and then kind of roped everyone else
in was that you know there's many
arguments around you know AI risk and
many explanations and so on but they're
often scattered they're often scattered
kind of in like strange locations like
you know fragmented across posts or like
interviews and stuff like this and
they're often written by audiences and
for audiences that are not your typical
everyday person they're often
unnecessarily technical they're often
require a lot of like jargon or like
buying into other assumptions or like
philosophies or worldviews that are just
kind of like not necessary that's one
one reason so the the first reason why
was just because to create something
that's like the whole thing in one spot
second reason was because it's actually
aiming at a non-technical audience
rather than a technical audience and I
would say the third reason is is that
there are actually some arguments which
I think have not been made in this
Clarity elsewhere in particular around
the social and the political Dynamics
many of the people talking about AI as I
talk about the compendium have conflicts
of interest or reasons to not want to
talk about certain conflicts of
interests or political situations or
conflicts for various reasons one of the
things that I have going for me is that
I am very I am independent my money
doesn't really depend on anyone in
particular I can kind of say whatever I
want and also like you know call out
anyone I want and I try to make use of
this privilege that I have so there are
many people and organizations that I
believe had deserved critiques that had
not been made at least publicly and not
in this clear form for various reasons
so a part of that is I wanted to make
these critiques and drive a wedge like
make clear that look there is an actual
conflict here maybe it's a conflict that
can be resolved you know I'm not you
know conflict doesn't mean that we can't
find a peaceful resolution but we should
be clear There's an actual disagreement
here like I believe strongly that if
there's a disagreement you should have
it like it's okay to disagree about
things what you shouldn't do is pretend
to not disagree maybe you could give an
example of of what you're talking about
here what's something that you've
pointed out that haven't been said
before I mean it's been said by someone
somewhere I'm sure but but a a big thing
is is that there is a very common
confusion that happens when I talk to
for example policy makers in DC where
they meet like the effective alterist
movement or like the AI safety movement
you know around like open philanthropy
and these kinds of people and they were
often very confused by these people kind
of they're kind of like the thing that
that kind of happens is they're like
these people come to DC and they say oh
AI huge risk it's going to destroy
everything it's going to literally kill
everybody and then polic are like wow
okay wow it seems pretty bad so should
we ban it like oh no no no don't ban it
and that's very very confusing there's
this weird thing where a lot of this
like there's this group which we can
talk I think we're going talk about bit
later which just like formed is like
Alliance like of various organizations
and people who claim to be the AI safety
movement or representing the AI safety
or ex risk movement whatever but
fundamentally want to build AGI and are
trying to build AGI this is around group
such as anthropic
philanthropy the effective fist movement
previously FTX and stuff like this and I
think there is a there are some people
in this area in this you know realm who
are like do care about the risks and do
you know want to like you know stop AGI
from causing massive risks and so on but
there are those who don't or who think
that as long as they're in control of
AGI it's okay or like this is the best
plan and those people actually disagree
but they they don't know that they
disagree so driving a wedge is making
two people aware that they're actually
not allies they think they're allies but
they're not and this is actually very
very important for coordination because
if you try to coordinate with two people
who are actually different factions and
they don't know they're different
factions you can't coordinate and this
will always lead to Leed to confusion
and misalignment and betrayal and so on
so I think it's very important that a
conflict should be had if you think
building AGI is is okay like anthropic
should just be allowed to build AGI or
whatever and then okay you should state
that clearly and then people who don't
think that's okay should also state that
clearly and they should have the
conflict they should and I disagree with
this like I am firmly no I don't think
that anthropic or any other private
Corporation should be building AGI
period at least not in the foreseeable
future I'm guessing you're not against
AGI in a in a kind of blanket way that
we should ever build it do you think we
should never build AI or or super
intelligence my true opinion is is that
I think I shouldn't get to choose this
is not a decision that I should be
making this is a decision that Humanity
or you know democracy should be making
not me I I think it's extremely
presumptuous and kind of like morally
wrong to want to or decide to make a
choice like this for Humanity I don't
know what the best future for Humanity
is and like you know maybe I would like
X or Y more than Z but then I should get
to like vote on that and you know should
have my human rights respected but like
if I really want to live in this world
but other people don't I don't think I
should have the right to enforce that
upon them because I'm going to be the
one who builds AGI first I think that is
like morally evil or like very close to
it yeah all right but Conor what if
others are racing to build AGI before
you and I'm saying that I'm saying that
as kind of partly a joke but this this
is an argument that's often made right
we will we will build it first and we
will build it in a safe way and if we
don't do it others will will do it in an
unsafe way exactly this is the main
argument that these people make and
these are the exact people that are
going to kill us like it's important to
understand that you find yourself in the
situation you are the bad guy like you
are the person who is going to kill
everyone like it's just be very very
clear here if you that's not saying that
you're acting irrationally is not what
I'm saying it might be the IR rational
thing for you to do given your local
incentives but you must also understand
that you are an agent of Mok that you're
an agent of you are the entity that is
you know breaking everything like you're
the problem there is this really funny
image that I very like which is a which
is a picture of Copus pushing a boulder
something and the caption is you know a
lot of people think they're Copus but
actually they're the 
Boulder and like if you are one of the
people who are like I must race for AGI
or you're working with an organization
it's like I must race to hii you're the
 boulder you're you're the thing
that is creating X risk and look I take
a very systemic view when it comes to
morality I think most bad things happen
for systemic reasons not because there's
like one evil guy who specifically evil
and decides to be evil I think this does
happen to be clear like there are people
who are like this but it's like really
quite a minority most bad things happen
for systemic reasons if they happen
because of structure power structures
they happen because orders or because
Market Force look I'm German like like I
know how it is right is like most
Germans are and were fine people like
you know they weren't Super Evil but you
know bad systems lead to bad outcomes so
I take a very systemic view when I think
about these things and this is a
systemic problem there is a a
fundamental thing that every individual
actor you know who finds himself in the
scenario is incentivized to do evil
which is to race Ai and they do
it's like and if they didn't they are no
longer part of the race and then whoever
the least cares about this enters the
race so there's also selection pressure
where if you actually cared about AI
risk so much that you want to not build
AGI well then you wouldn't be at open AI
or where or anthropic you would leave
you would not work there so there's also
a selection effect it's kind of like
with like if a if the CEO of a big Oil
Company came up to me and he was like I
don't really think you know clim CL
change is a big problem this isn't
evidence to me like I don't update based
on this information I'm just like well
yeah duh you were selected to not care
about this like otherwise you wouldn't
be the CEO of oil Corp like I don't care
your opinion is IR valid and there's
kind of a similar kind of selection
effect that happens here as well so
there's there's this big thing we're
like okay you're in a race so what the
hell do you do so the weak answer is
well you just give into your incentives
you do what's locally optimal for you
and you don't do anything and this is
the convenient excuse that people tend
to use I don't quite buy it and I'll
talk about that in a second the main
thing you should do is stop drop and
Catch Fire you should be like help help
help I'm in a race help help help like
you should go to the government you
should go to the UN you should go to
Every newspaper that will listen to you
you should form a political party you
should do you should be help help help
help help we're in this terrible
situation I can't get out we need to get
out of this together help help help help
help this is what you would do if you
actually want to stop the race if I was
Dario Amad or whatever I would not build
AGI I would go to the president and be
like help help help help help we have to
stop these people from racing like I
can't stop unilaterally we have to stop
all these people from racing like we you
you should found a political party you
know should build a coalition you should
get unions on board you should get you
know start you know International
collaborations on this kind of stuff
right like actual coordination actual
Civics actual politics it's the same
problem as like the nuclear arms race R
right like we're it's the same situation
right no one wins a nuclear war and no
one wins an AGI race some of the leaders
of the AGI corporations have done
something that is superficially similar
to what you're what you're talking about
here which is they they've gone to the
US government they've talked about kind
of some form of pseudo nationalization
or some form of government collaboration
the framing there is though we need to
build these systems and we need to do it
safely but what's the difference between
what what they're doing and what your
suggestion and the difference is that
the thing that they actually did is you
know for example let me give a concrete
example I think it was Senator blumental
when he was talking to Sam ultman during
these hearings Sam Alman in 2015 I
believe wrote a blog post where he
talked about how machine intelligence
you know super intelligence pays is the
gravest risk to humanity survival very
unambiguous extremely unambiguous what
he Me by this this is one year after
bostrom's book came out you know a kind
of direct response to this it was
extremely unambiguous what this me cator
Blum quoted this exact line he said so
you wrote this and then he said I assume
by Humanity survival you mean jobs did
Sam ultimate then correct him and be
like no no no Senator no Sam Al said yes
of course Senator that's exactly what I
meant there's a deep thing here where
people don't know their history
including me sometimes God forbid young
Tech Guy not knowing his history has
never you know not not a story as old as
time you know young ambitious guy who
doesn't learn from history until he
actually you know sits down what was
happening right now is the exact same
thing that happened with big tobacco or
with big asbest big oil or any of these
companies it's the exact same Playbook
is being played right now where there
are companies that think there's power
and there is money to be had with a
technology that has some form of
externality or unaccounted risk and they
want to delay and deceive and you know
just muddle any response to this this is
exactly what is happening right now and
all of the companies are doing this they
are being extremely strategic about what
they say they're you know I'm not saying
they don't make mistakes like sometimes
they do say more than they're supposed
to I'm pretty sure you know Dario
probably not allowed to speak publicly
for various reasons you know because he
every time he does he says things that
incriminate him and you know other
people you know other CEOs are better
media trained in various ways and are
very clever about how they phrase things
or how they dilute things and so on
there is no way that a corporation wants
to actually get regulated that's just
not how the world works and that's not
how any of these regulations they want
self-regulation they want voluntary
commitments they want you know them to
set the technical standards and for and
what they definitely want is they want
holistic regulation like we have to have
you considered the considerations we
should first consider you know we don't
want to act too hastily no no no of
course not you know the the all the
evidence on lung cancer from smoking is
not in yet we should we should do more
evaluations of lung cancer when smokers
before we go to regulation let's start
with some more evaluations some more
studies maybe we can create a committee
can we have some like White House
committees maybe that will evaluate the
evidence the controversy you see this is
this the Playbook which is called fear
uncertainty and doubt or fud the way fud
works is you don't directly counter the
arguments of of the critique you don't
directly engage with them because you
know you'd have to be good faith to do
that and that is hard and like will just
draw more attention to the people
criticizing you so instead what you do
is if you benefit from the status quo if
just continuing on the path you're
currently on is beneficial to you you
just spread fear uncertainty and doubt
you just you know spread ink you just
confuse everybody you prate a bunch of
nonsense data you talk about controversy
and we should take our time and we don't
want to be Hasty you know you just stall
for time and this is what you know to
varying degrees you know like sure is
one org maybe marginally better on some
specific thing than some other or sure
maybe whatever is Shell Oil maybe
marginally better than Exxon sure maybe
I don't know right but like
fundamentally what is happening is
they're stalling for time I I'll give
you one point here where I think the
analogy might break down so if you if
you want to sell tobacco or you want to
sell oil and you don't want to have to
deal with the externalities of what
you're you're producing here the health
risks of what you're producing what's
motivating you is is to earn money right
and and I think the the ADI corporations
are ly motivated by earning money but
there's also something something above
that something kind of more ideological
something about bringing in you know
creating a a beautiful world for mankind
basically do you buy that difference how
do you think that that kind of changes
the incentives exactly so this is what
we also talk about in the compendium so
we have a chapter on this where we
categorized kind of five groups of
people who are trying to build AGI for
different reasons it is very important
to understand is that the race to AGI
cannot be understood in purely
economical terms this is exactly correct
as you say if we look at this if we
analyze this as like a hard-nosed Wall
Street analyst it doesn't actually make
any sense you know it I mean it does
right now because there's a lot of hype
but before this it didn't really make
any sense as a bunch of other things
that don't make sense here there is a
very deep ideological even religious
aspect to this so I split the actors
into five groups and of course there's a
lot of overlap between the groups it's
not a perfect split but is useful for us
to think of it this way the first are
the utopist so these are people who
think they can build Utopia is that if
they you know which historically as we
know is always a really great sign you
know when someone believes that so they
believe that they must build AGI because
if they do it and they're the good guys
then they can use this to Usher Utopia
in free humanity and this will be great
so this is people you know groups such
as anthropic many people at open AI you
know your you know ilas hkers and your
Dario Amad your you effective altruists
like lots of people in this Camp fall
into like this group the second group is
Big Tech so these are ruthless
corporations that just want Power you
know they're not sure if they buy the
whole Utopia thing or not some of them
might some of them might not but
fundamentally they buy Power they
understand the concept of power and they
understand that AI means power and they
want more of it so these are the ones
that are now driving most of the actual
resources of the race originally the
race quot quote was just some IDE you
know ideological utopist running small
companies like deep mind or you know
open Ai and then but we have now phase
changed to to where all of these
companies these utopist companies have
been propped up by big Tech you know
anthropic by Amazon uh deep minded by
Google and open AI by Microsoft so
there's a strong symbiosis now where
these have like like merged into one
entity to a certain degree but I think
big tech for the most part is far less
ideological and far more just power
driven more practical and also more
dangerous for these reasons the third
group is the accelerationists so there
are some people I mean who are basically
just Libertarians they think they think
technology is a moral good thing like
there's no way that technology can be
bad and therefore all technology must be
good it must be built and anything that
is against the building of new
technology must be bad and the four the
fourth group is the zeits so this is a
relatively small portion of people but I
think it's important to be aware that
they do exist these are people who think
human not just think that Humanity will
be replaced by AI but that this is a
good thing so they want Humanity to be
replaced by AI either because AI is like
a superior species or because humanity
is evil and deserves to die or whatever
there are some pretty prominent people
in this Camp you might not expect Like
Larry PA you know the ex you know
founder of you know the founder of
Google who told you know Elon Musk to
when Elon Musk said hey he doesn't want
AI to eradicate humans Larry Page said
he's being a
speciest which is again like that is
such a crazy fact that actually happened
in reality if you saw this in a movie
You' be like that's goofy that's that's
not real like that didn't actually
happen but it did it happened in real
life and there's of course other people
in this Camp as well Like You Know Rich
Sutton you know very famous AI scientist
Schmid Huber another famous AI scientist
kind camp but it's a relatively small
camp overall you know some schizophrenic
people on Twitter I guess and then the
fifth group is just opportunists they
come along whenever there's power and
money to be had you know what before
this they were on blockchain now they're
here and now they're along for the ride
so they don't really have any particular
loyalty to AGI as a concept they just
want to make money and like gain power
which yeah fair enough I get it you know
hustle a hustle so that's that's the
landscape which group are you most
afraid of you mentioned the kind of
combin
of big Tech and
utopians is that that's the main driving
force behind AI progress this is the
current main driver correct before this
it was the utopist now big Tech are
taking over I also think big Tech is
kind of eating the utopist you know
classic Microsoft Playbook of you know
Embrace extinguish which is what we're
kind of seeing where Microsoft is kind
of in a slow motion de divorce from open
AI they're trying to kick open AI out
now that they have what they need that's
interesting I heard that open a was
trying to get out of their Microsoft
deal by by invoking their AI achieved
Clause I haven't confirmed that do you
know about that I've heard about this
seems like a possible rumor I don't know
I think it goes both way I think there's
some power struggle there that we don't
understand who knows but this is a
classic Microsoft strategy like you know
all the way back in the 90s I think it
was one of their emails that included
like a exact description of like the
strategy where you would like Embrace a
competitor or a new technology extend
end it and then
extinguish and so I think this is the
thing is that like opening Microsoft is
very behind on AI kind of got the open
AI Tech that they needed that they
wanted and also hired like and like Aqua
hired a bunch of like talent and so on
to build their own AI efforts and so on
so they're hoping to like probably break
from this and continue to fast makes
perfect set honestly you know props to
the you know Executives like some true
House of card  good job guys like
impressive is is there a chance that
this ends up being a good thing if maybe
the big Tech the profit focus maybe they
don't want to take as much risk maybe
they don't really believe that super
intelligence is an actual achievable
goal and so you get a more kind of the
idealists or the utopians get eaten up
by more practical concerns and launching
a useful kind of chatbot product right
so this would be actually a good thing
if it happened in this regards big Tech
is more evil but it's more predictable
which is a useful property to have if
you want to regulate or prevent you know
certain forms of externalities it would
be good actually if the utopians were
not in charge I think the utopians are
the biggest problem like because they
have an ideology they have a burning
ideology that justifies anything
basically as long as it allows them to
get to hi faster because I mean if this
is always the problem with utopist
ideologies is Utopia is so good that it
justifies anything you know we saw with
like FTX and like others like this this
is always the problem with like
consequentialist or like utilitarian
ideologies this is usually the failure
mode that happens not that all fa fall
for this failure mode but this is a very
common one big Tech and these like you
know sociopathic corporations have a
different failure modes which are
predictable and they're more competent
in dangerous ways so that if AGI was so
far off I think big Tech taking over AI
would be less dangerous the problem is
if AGI is not far off if AGI is not far
off then then and big tech companies
believe this for example because the
utopus convinced them of this then
you're in big trouble then you're in
really really big trouble and you even
more in trouble if nation states get
involved so let's talk about
onun so recently there has been an
emergent faction which is kind of
basically the same faction I talked
about earlier this is anthropic open
philanthropy some parts of Rand Rand
Corp and a couple others in this kind of
area we've recently been pushing what
they themselves have called the onon
strategy the onon strategy is basically
try to Gaslight and confuse and lie to
the US government that the US must race
to AGI before China gets it and let's be
very clear about what their intentions
here are is of course they need to build
it they need to do it for the US and
they have found a trick of how to
deceive or you know frighten you know US
military apparatuses hopefully in order
to get them to race to the utopian AGI
that they want to build this is an
escalation this was not really a thing
two years ago so this is a recent for
escalation where the utopist have now
gone for in their dream to build Utopia
they have now gone from you know their
own companies to Big Tech to going after
the state as like the next of how to
escalate their dream of utopia and this
is extremely dangerous this is extremely
dangerous it's and it's also wrong like
this is a very important thing to
understand this is technically and
strategically wrong it's not that it's
true that there is a race happening with
China but what people are Miss but the
problem is is that it's a race to the
death to the end it's a nuclear war you
can't win a nuclear war finally so this
strategy is called onon as named this
way by Dario and Rand funnily enough
this there is a historical parallel to
this so in the 1980s Ronald Reagan
announced his Strategic Defense
Initiative also called Star Wars which
was a a huge government project to build
anti-nuclear missile defense systems in
space basically and to rap massively
ramp up the American nuclear
stockpile and the proponent of this
strategy which was based on a lot of
extremely controversial scientists
science that was not well back most of
the scientific Community came out
against this idea and thought it was
completely unfeasible it was even
dangerous because it would provoke the
Soviets potentially into into a nuclear
war even faster and so on because if
they were succeed at building a perfect
Shield against nuclear weapons then the
Soviets might be incentivized to strike
before The Shield is complete because
you know you know use it or lose it so
this could potentially cause the exact
nuclear war that they were warning from
but even more crazy than this I didn't
know this until I recently started
reading a bit of history about this is
that proponents of SDI literally argued
that they could win a nuclear war and
that we should they literally argued
that we build a perfect nuclear defense
system and then we nuke the Soviets and
this was an actual thing that actual
people suggested and then therefore we
would have perfect you know American
Military hedge of money forever and the
thing that pushed against us was a
coalition of scientists civil groups Etc
called the Dayton and the Dayton
basically argued that no you can't win a
nuclear war I mean both you're provoking
the Soviets into nuclear war ahead of
time which is the same thing that's
happened with an Agia race if the
Chinese actually think that we are
getting close to AGI and that we will
use this to you know destroy the Chinese
state but what do you think they're
going to do I don't think there's a
there's there's a peaceful solution here
you know it's going to be ugly and the
second thing is is that you can't win a
nuclear war so Carl Sean actually
proposed or and like validated this
theory of nuclear winter and this was
one of the decisive killing blows
against SDI as he he showed even if when
were to Nuke the Soviets or whatever you
still get nuclear winter and everyone
loses so there is no winning strategy
you can't win a nuclear war everyone
loses and the same thing is happening
with AI right now you can't win an AGI
arms race the only winner is Agi what
what what do you think they would say in
response to what you just said why so
they must be aware that for example if
China begins to sense that the US is
close to ADI and their intelligence
points to the US using that ADI to
basically control the future of the
world the the CEOs of the ADI
corporations are aware that that China
has has nuclear weapons and so on what
what's their response to what you just
said I mean depends on which one you
asked I think most of them I just
literally never thought about this like
a lot of this is not well thought
through to be clear like most of this is
not well through some of them think
through it but like most of this is just
not well thought through most of them
it's just like well we don't have a
choice this is the main counter argument
it's like we don't have a choice if we
raise to AGI and we die well not our
fault nothing we could have done there
is a strong self-imposed nihilism where
just people decide that there is no
third option they decide that well China
is racing stopping them is on logically
impossible so therefore racing is the
only ontologically acceptable solution
and if racing kills us then we are
already dead and so there's no reason to
even try what's interesting about the
the China counterargument to to AI risk
is that it's it's in some sense the
first thing that people go to when you
talk about trying to perhaps slow down
perhaps pause the AI race that's going
on I mean I don't think it's a bad
argument I think it's it's there's
actually something that to be very aware
of there it seems that the the current
kind of philosophy of the Chinese
government wouldn't be the right
philosophy to rule the world forever
right so we don't want that and so even
though it's it's kind of the first
argument that people go to I think it's
still kind of a live argument do do you
agree that it's a live argument yes but
it's wrong like it's live but it's wrong
it's just not true like this is very
important to understand the argument is
false it's the argu is invalid it
doesn't apply to our reality the problem
isn't that we know how to build a safe
AGI and it's just a question of who gets
to press the button that's not the
problem the problem is Agi kills
everybody and if you race to the bottom
you build AI as fast as possible there
will be no safety there will be no
alignment there will be no American AGI
that's not what's going to happen
there's just going to be unaligned AGI
like this is not a pris dilemma this is
important to understand this is not a
prisoners dilemma people Pretend This is
a prisoners dilemma it's not in a prison
dilemma you always benefit from
defection like so even if the other
person is already defected if you defect
you get a better outcome this is not the
case here if China raises to AGI we die
if we then also raise to AGI we just die
marginally faster this doesn't help this
doesn't this does not improve the
situation this is not like this is a
very deep thing this is a very important
understand if you read for example you
know Leopold Ashen brener is making
exactly this point he makes this long
thing about how AGI is coming which is
like nicely argued and nicely put
together and whatever and then he just
kind of says in like one sentence you
know for alignment you know we'll muddle
through that's his whole argument like
there's no argument there he's just like
well whatever you know we have to race
there's no justification like if you
actually read the thing he he lets up
all these arguments about AI coming but
actually doesn't make any argument that
justifies racing as the correct strategy
he actually doesn't justify this because
he doesn't prove that alignment will get
solved it's not there it's not in the
it's you can read it it's not there
there is no reason to believe that if we
raise AGI that we will get safe AGI that
does what we want in fact in fact there
is overwhelming reasons to believe that
that was not what's going to happen so
the whole premise is flawed it's just
like it's it's it's wrong one of the
points you make in or you and your
co-authors make in the compendium is
that AI progress over the last decade
say have been driven mostly by more
resources more data more compute more
Talent more investment and not equally
by kind of deep research uh
breakthroughs is this the the cause of
what of us not understanding AI deeply
enough to make it safe so is is it is it
because that we we have grown these
systems as you as you write as opposed
to building them that we don't have
understand them and therefore we don't
know how to make them safe this is a
huge problem related to this I think so
there's there's a strange thing that
happen there's many strange things about
AI the fundamental the kind one of the
core strange things about AI is as you
say is that it's grown not written it's
not that you write line by line the
codes of what your AI does it's more
like you have a huge pile of data and
you grow a program on top of this data
that like solves your problem which is a
very strange way to do programming but
it works empirically so but like we
don't really understand these programs
that get generated they do strange
things all the time that we don't
understand or that we don't know how to
control and you know and we have no way
of predicting what they will be capable
of or not before you know they're made
in various ways this is very curious
because this is kind of different from
how capabilities tend to work in normal
software in normal software as you add
more features to your software as you
make it more powerful as you make your
system more capable and more things you
build up complexity like your your your
program gets more and more complex and
it gets harder and harder to manage and
usually eventually you will reach some
limit of like what complexity you or
your organization can handle and then
your program just like you know freezes
in what it's capable of doing or it
collapses and just becomes
unmaintainable like this is the normal
life cycle for software and then you
have to like start over or like break it
into multiple parts or like or you're
just stuck like there are many companies
massive corporations whose whole job is
just like like you know like
parasitically living off one huge
unmaintainable code base that like no
one can like actually do things with
like this is a very common thing in
software there's a to to give a bit of
flavor to this there's a very funny
story a guy in Hacker News which is like
a News website talks about how who used
to work at Oracle so Oracle is a legacy
software company they build like huge
Baroque software for like Fortune 500
companies and stuff and one of the
products they sell is the Oracle
database which is just a database you
know it's not that different from other
databases and he talks about how the
codebase of the oral database is just
one of the worst things known to man
it's like 20 million lines of just
poorly documented complicated code that
all everything interacts with everything
no one knows how it works it's all
relies on literally thousands of flags
that just get you turned off and all
interact with each other in ways that no
one understands and are like not
documented properly so whenever you need
to change anything about the code or add
a new feature you have to like you you
you change a couple lines of code and
then you have to run literally millions
of tests on their you know huge cluster
and this takes like days like three days
to run all these tests and then you
always break thousands of them with
every change you make so then you have
to go through each and every one of
these tests that you break and just like
fiddle with all the flags until you have
like the right combin magic combination
of like flags and like bug fixes and
like special cases or whatever that all
the tests pass and you add like another
flag for your specific weird bizarre
Edge case and then you submit the code
and it gets like merged you know months
later this is a terrible way to build
software like this is just truly truly a
terrible way to build software both
pratically speaking also from a safety
perspective like I can tell you with
confidence that the Oracle database
contains horrific security flaws I can
tell you this because there's no way to
get them out of there that thing is too
complex there are some horrific security
flaws in there that have not yet been
found I am 100% certain of this and
there's no way for Oracle to get rid of
them there is no way Oracle could take
this code base and pragmatically
actually find all the bugs it just can't
be done it's too hard it's too complex
and so this is how most software is and
the punch line is of course is that AI
is developed like this too but worse is
that AI you don't even have a code base
you have a neural network there's no
code for you to test and there's no
tests sometimes we run evals but EV vals
are not tests it's not like we take
apart all the individual functions
of the AI and test all the individual
functions that they're correct no it's
all a big neural network so evals are
just completely you know like trial and
error brute force and you just like see
if the numbers go up or down and they
like look good this is a terrible way to
design software not just but the crazy
thing is is that AI allows you to build
software in these terrible terrible
terrible ways while still making it very
powerful and this is where it's
different from traditional software the
oral database you can't add crazy new
features at least not easily because
it's just too complex like you just you
just run into a bottleneck with gd4 who
well just throw more data at it you know
bro just just get more gpus bro just add
more complexity more patterns just put
more in there who cares like literally
who cares just put more  in there
and it gets better so we have like this
kind of like bizarre we worst of Both
Worlds or kind of situation where the
way AI software is built is so
fundamentally in a way that we can like
like from a cyber secur perspective
you're just like holy  like there's
no possible way to make this safe like
there's like no way you could have a
system like this that does not have bugs
it it can't be done like it's like if
you think this you're crazy and at the
same time you can make it extremely
powerful because you know in the past
you know software at least would like
plateau in its you know power so to
speak because it would just get too
complex and will start breaking but AI
to a large degree doesn't have this
property so we could build extremely
powerful systems
that can do extremely powerful things
while having just the most complex bugs
known to man Just bugs that are so hard
to understand so hard to debug so hard
to even you know fathom or find that's
like impossible and the reason this is
so important is let's be very clear here
when people talk about stuff like oh
America will build good AGI that makes
the good future let's be very clear what
they're saying here they're saying we're
going to solve all of moral philosophy
all the problems our institutions our
governments our you know States our
militaries are trying to solve all
political problems all resource
allocation problems scientific problems
interpersonal problems social problems
all of these problems using software and
there will be no bucks that's what
they're saying that's
insane I agree that there must be many
bucks bucks different from from the how
bucks would appear in traditional
software but kind of failure you could
say in the weights of gbt 4 for example
but but these are not these are not so
consequential as to as to prevent us
from or prevent open AI from offering a
a useful product so what is what is it
that's going to change in the future
open AI has has offered this chat
product for for a number of years now
nothing has gone fantastically wrong yet
so what is it you expect to change in
the future such that the Box you could
say in our models become more
problematic to be clear many things have
gone fantastically wrong like this is
like people keep saying this this is not
true like social media is now insane
like even more so this is a massive
externality on humanity and its
epistemology this is a huge huge blow to
humanity this is a massive cost that
every time I go on social media there is
no way for me to know whether this
eloquent person responding to my
arguments is a person or not this is a
massive cost that was imposed
unilaterally on Humanity it is now much
harder to build coalitions online it is
much harder to understand scams have
become much more sophisticated much more
mass-producible you know deep fakes
political things you know you know all
the kind of  that's been happening
also in like third world countries you
know where like genocides and so on have
been being pushed by these kind of
things let's be very very clear here I
agree that there is a difference and AGI
is an you know huge problem whatever but
the fact like a AI has not had huge
consequences is just wrong there is a
massive unpaid externality like our our
Boomer parents are just driven insane by
like social media and like AI generated
slop and like artists are being you know
eviscerated alive you know all these
various entertainment businesses are
falling apart you know like this like
human Shar tradition of like art and
beauty is falling apart like these are
massive costs like they're not deadly
we're not going to die from them but
they are huge they are huge huge huge
huge costs that we are paying and that
like that Society is paying and there's
no way to do back so we could say it's
worth it like you know you could make
the argument sure maybe it supercharges
you know propaganda and scops and
marketing and like all these other you
know manipulation stuff but that's fine
because CH gbt is that good Fair like
that's an argument you could make but
it's not free this was not free but to
get back to your actual question so your
actual question was what changes and yes
so I actually was talking to another AI
researcher about this recently who kind
of pointed out that like well you're
concerned about like these like AGI and
like these like super intelligent risks
and so on but like you know look at chat
GP it's really smart we don't have this
problem and the thing is basically chat
GP is very smart and like way smarter
than people think it is um and so are
like claw and like other AI systems but
they're not literally human level
because like I can't just replace a
senior engineer or a senior manager with
chat GPT with no
modifications yet so my argument is that
there are some things that are still
missing there are some like I can put a
human and you know as a CEO of a company
and it works you can't currently do that
with chat GPT chat GPT can help it can
do many of the tasks that a CEO would
have to do many of the tasks a CEO do
can now be automated by a chat GPT but
it's not 100% yet very importantly I
don't think intelligence is something
magical it's not a discreete property
where you have like some magic algorithm
and you add the magic math and then it
becomes intelligent or otherwise it's
not I think there is a very common thing
where people say it's not true
intelligence it's not true planning or
it's not true Etc blah blah blah we
argue this in the compendium as well I
think this is just very this is pseudo
scientific and just like really bad
reasoning where intelligence is is made
of smaller parts like whatever
intelligence you know general
intelligence human of intelligence is
it's made of a bunch of smaller tasks
I'm the CEO of a company you know when I
do something the way I run my company is
composed of smaller tasks there are many
many smaller things I need to do I need
to talk to people I need to reason about
people's mental States I need to reason
about the economy I need to reason about
products I need to make decisions I need
to communicate I need to write things
but all these things are tasks they can
be understood they can be automated and
chat GPT can do a lot of the tasks I do
day-to-day we can't quite do all of them
yet but I expect as we get to more and
more General systems with longer and
longer memories and you know coherency
and so on that slowly but surely
eventually it will be all of them and
once you have a system that is stable
and long lasting and so enough to
completely replace a human you have a
system that can do AI research you have
a system that can improve AI further and
you also have a system that you can
instantly copy on millions of gpus a
system that never gets tired that never
gets bored that has read every book ever
written that runs you know tens or
hundreds or thousands of times faster
than any researcher in the world you
know so the moment you have a system one
system that is as smart as a human that
has this kind of you know goes from the
90% to 100% And to 110% or whatever once
you have such a system
you can instantly scale it up to a th a
million x and you immediately will have
something that is vastly smarter than
human humanity and it can improve itself
they can gain more power they can
develop new technology and this will be
a complete step change and this can
happen extremely quickly so for me AGI
is defined as a system that can do
everything a human could at a computer
once we have one such system we will get
ASI artificial super intelligence which
I defin as a system that is smarter than
all of Humanity in short order like I
think 6 months or less just because you
would have a bunch of call them virtual
researchers uh doing a bunch of
additional AI research and and quickly
making progress and so I'm I'm guessing
I take your point here to be that the
bug that show up in these kind of Agents
of the future that can reason that can
plan that can think longterm those bucks
will be much more consequential because
now you have a bunch of virtual people
spread out in all institutions quite
quickly yeah is that is that your
explanation if I told you that open AI
just announc their virtual humans
program where they are going to release
80 billion virtual humans onto the net
next week would you feel good about this
could we even guarantee they wouldn't
immediately declare war on us or like
steal our resources or who knows what
they would do right like and like it's
going to be worse than that these are
not going to be humans these are going
to be weird alien machines that are you
know have all these weird bugs that we
don't understand that can have extremely
weird behavior that are being built by
for-profit corporations to make them
profit to be clear like these are being
built by sociopathic entities to
maximize sociopathic functions profit
and you will have massive amounts of
them that you know expand extremely
rapidly that you can you can scale them
up extremely quickly they can all if one
of them learns something you can
immediately send it to all the copies
like if one human figures out something
that doesn't mean Humanity knows it with
AI it's different if one AI figures
something out all AIS can know it in a
second like even if the system itself is
like not massively smarter than a human
to start it will get all of these
superpowers for free it will be able to
copy itself it will be able to run at
faster speed it will have access to you
know vast memory banks it will never get
bored it will never get tired it don't
hasn't have emotions it doesn't have to
you know all these things like if I just
had like
a guy who works 247 is the smart as John
Von Newman never sleeps never gets tired
and never gets distracted this is
already like by far the smartest thing
on the planet you could even trade off
speed for kind of raw intelligence where
if a machine or an AI could work
incredibly quickly that could make up
for it being kind of dumber than than us
in a sense I would I would expect speed
to also be a very important factor and
and as you mentioned not getting tired
not getting distracted I mean when we
try as humans to achieve a task right
there's a lot of kind of Superfluous
activity and anxiety and all kinds of
things that distracts us but these
models would not kind of face those same
same hurdles and though so they would
probably go very directly and very
ruthlessly uh to the goal they have in
mind one question here is so when I when
I was 18 I considered taking a driver's
license
and I decided against it because I saw
all of the progress that was being made
in self-driving cars this is a while ago
now and so there I concluded that you
know we we seem to be so close to to to
having kind of full self-driving and you
could say okay we seem to be getting
quite close to human level in these kind
of chatbot models we we have right now
if we can just fix things or perhaps un
hobble the models or add some some other
features give them make them talk to
themselves and so on we can get to
agents is there a a similar problem to
to self-driving where you know getting
close to to being reliable enough to be
useful is is is useless in a sense where
getting that that last percent or so of
reliableness makes the entire difference
of whether you can deploy an agent
usefully in the
workforce I think there's some of this
but you can also think of things
differently if you're on an exponential
and let's say you've been on this
exponential let's say it doubles every
two years right let's say just you know
hypothetical now let's say you've been
on this exponential for a hundred years
or something and it's gone from zero to
like three or something over this time
you know you can you can pick a
exponential or whatever that like
started low enough that it like moved
slow enough to get to this right and I
you say wow we spent 100 years and we
only got three units of progress so like
this is going if we want to get to 100
unit of progress this is going to take
literally 3,000 years and this would be
a reasonable argument to be made right
but the true answer is is that it will
go 3 6 you know 12 24 48 and you're 100
in six years or 12 years later although
this is this is for the raw intelligence
of the models I'm guessing right where
we've we've see we see this kind of
progress but we we don't I mean do we
have good measures of agency in AI
models here's is this important thing
right agency what does that mean exactly
it doesn't mean anything it's PSE
scientific it's not a real concept it
doesn't defer to anything in reality
it's like it's just a word that people
use to come up with like like whether
it's agency or planning or Consciousness
or something these don't refer to actual
physical or like algorithmic properties
you know some people use them to refer
to specific things and if that's what if
you're referring to a specific
algorithmic property great happy to talk
about that but I expect it's more of a
vibe
yeah maybe it's a VI but we we we can
put that word to the side and then talk
about perhaps ability to to to achieve a
goal over a longer time Horizon for
example yeah and we're getting
exponentially better at that yeah yeah
maybe we are maybe we are so so so your
point is whenever we we try to kind of
specify what we mean by agency we see
that it that it dissolves into something
measurable and that which is measurable
we're we're improving at exactly this is
my fundamental thesis around
intelligence is that if you actually go
very very deep and the into any of these
Concepts that seem to be missing they
dissolve they are fundamentally it's all
just information processing it's all
just patterns it's all just smaller
tasks and we are getting better at all
of them you know like you know some of
them more than others and so on and some
of them are harder than others and they
will take a bit longer but if you're on
an exponential it doesn't matter 50% of
the progress happens in the last unit of
time like if you're at 50% and you're
like wow this is going to take quite a
while we're only 50% of the way there
you're wrong it's only one unit of time
away so if we're at 50% of you know
hypothetical last mile driving well it
means it only going to take you know one
more year or two more years to fix not
saying this is literally what's going to
happen but like this is like the kinds
of ways you should be thinking about
this right so like and also like with
like self-driving car specifically
there's a bunch of other stuff like
regulation that SL things like like weo
does have self-driving cars and they do
work you know like they do exist in
specific cities though right yeah but
like San Francisco is a terrible City to
drive in have you driven in San
Francisco before that's not an easy City
to drive in so like it's like I I agree
Phoenix is an easy City to drive in but
like like San Francisco's not easy SE to
drive in so like like sure we can argue
oh it's still unperfect it still makes
mistakes sure of course whatever right
but I'm just like also they have like
what like a GPU in there or two right
right it's like I expect if you and like
what we're also seeing right now with
like robotics is we're seeing this
massive revolution in Foundation models
for robotics we like I remember two or
three years ago people were like
robotics impossible deep learning no
can't can't do it right and now we just
like take language models you know train
them on data and then ask them to do
things and they do like like like if
anyone is not just like go to the Deep
Mind blog and just like look at like the
last like three papers they published on
like robotic
and just like have your mind blown it's
just like literally like L we asked the
robot to do something and it did yeah
it's it's actually amazing to to look
into this I wouldn't have expected it to
to work out this way in which you you
have a a vision model looking at a scene
writing down what it sees you can you
can then kind of give instructions in in
written language in in kind of natural
language English and say you know pick
up the apple of course there's a there's
a bunch of very advanced stuff going on
underneath that but it's just
interesting that it it could be that
simple in in a sense okay in the
compendium you you call for an actual
science of intelligence and I think this
is this is what we this is what we we're
kind of debating or talking about right
now we what we're missing is an is a
developed science of of what
intelligence is how it works how it can
be grown what do you think such a
science would look like I obviously
don't know like this is an extremely
hard question question I have some
suspicions I have a suspicion that it
will look more like computational
learning theory less like computational
complexity Theory but like also lots of
computational complexity theory that
that difference you got to you got to
explain that difference do
I okay well I'll say a couple couple
interesting things and then I won't go
into it too much because it's just hypo
it's just skitso stuff it's just like
these are some there is something wrong
with P space and so what I mean by this
is is that there is a everyone knows
about P versus NP it's like there are
problems that are in a class called P
polom Pol time P time and there's
another one called non-deterministic
polom time and it seems like these two
should be different but we have Prov
this everyone knows this very famous
example in computer science right but a
lot of people don't know we also can
prove the difference between P time and
P space which is insane ppace means you
have polori amounts of memory but in
infite time and we can't prove that
there are things that are computable in
bace that might also be computable in
like P time this is insane what this
tells me is we got something deeply
wrong about
computation something about how we think
about math computation intelligence is
wrong I don't know what it is there's
like something about like the unit of
work like what is a unit of math if I do
one math
what is that unit because like there's a
thing so there's a little bit of this in
what's called algorithmic information
Theory an algorithmic information I
think it was algorithmic information
Theory I'm actually getting that wrong
the commons will yell at me if I got
this wrong where you can prove that for
a sorting algorithm there is a minimum
complexity of the program like there's
no way to make an a sorting algorithm
that is less than like n log n or
something like this this is like the
only result that I'm aware of of this
type we can't prove this for almost
anything else but intuitively what a
science of intelligence would look like
would be something like to be able to
drive a car you must be at least this
intelligent this is what it would look
like and the closest thing we have to
this is algorithmic information Theory
which is like which has proved this
thing of like if you want to sort
something your algorithm must be at
least this complex this is the kind of
things you looking for I don't think we
can even do this because I think we're
our math is wrong like there's just like
something super deeply wrong about how
we're thinking about mathematics and how
we're thinking about computation that we
are just super confused about
intelligence that like intelligence is
like super super hard but we're already
getting like computation wrong and I
expect once we're less confused what
computation is and what learning is and
these kind of stuff then intelligence
will become more and more clear what it
is and how to think about it and how to
formalize a statement such as in order
to drive a car you must be at least the
smart I think we are so far away from
this this would take like generations of
mathematicians and like computer
scientists I think this is like probably
like as hard or hard than P versus NP to
get right I do think we can make a lot
of progress on like empirical or like
you know approximate theories that like
are not really true but I also think
that's very very hard and it's one
that's like a surprisingly small number
of people are working on to my
understanding how would you categorize
scaling laws would that be kind of like
an engineering of intelligence and not
really a science of intelligence I think
it's more like Alchemy Alchemy of
intelligence okay so we're not even at
the at the stage where we have like the
engineering textbooks and and some kind
of like equations for how our models
behave like the scaling laws are much
closer to naturalism it's more like
victorians looking at bugs and finding
that there's like a symmetry between
different bug sizes you know it's like
there's there is a pattern there right
and like it might be an interesting
pattern might not be an interesting
pattern but like the way that like say
Victorian naturalist categorized animal
into species turned out to be like super
wrong once we figured genetics right and
we find out that there's a deeper
pattern in nature than you know they
didn't get everything wrong when they
said like wow lions and tigers sure look
similar they kind of look like cats
probably related yeah they were right
right like I'm not saying naturalists
were stupid you know or anything same
way I'm not saying that someone who like
does AAL work on AI is stupid I'm not
what I'm saying I'm just saying like
there are deeper patterns that you can't
find through ISM no no I mean the
scaling laws are kind of a great Insight
it's just the question of what do they
mean in practice right what is it you
that you can predict it's not exact
capabilities it's not you can't predict
anything basically you can predict a
specific like approximate number which
is the loss but how the loss relates to
any capabilities we actually care about
like the like you need this loss to be
able to drive a car which would be the
kind of result we actually want to have
it's just the not even in the same
Universe research into scaling laws will
not get us to an answer there we would
have to go with this from a completely
different perspective we'd have to come
from like computational learning or like
an algorithmic information Theory
perspective if we want to answer or like
even formalize a question like this and
we are like like like scaling laws is
not even trying to do this this
important thing to understand it's not
that they're trying to do this and
they're failing they're not even trying
to do this it just has a different goal
that the goal of scaling laws is well a
create hype and raise money and two
empirically allow you to you know
predict if I put in this amount of money
or effort or whatever like how should I
split my resources to get like all
things equal best outcome which is a
fair thing like that's what a lot of
Alchemists did like if you read
alchemical texts a lot of the alchemical
texts would be like oh if you want
sulfur here's the things you mix and
then it makes sulfur and like careful
don't heat it too hot or it will be bad
and stuff like this we talked earlier
about how it it could be a good thing if
big Tech takes over from the utop kind
of founders of the AI corporations if uh
timelines were were long you don't seem
to to believe that you're worried that
timelines might be quite short does this
have to do with specifically kind of
Agents helping us do AI research or do
you expect is that a crucial piece of
timelines being short for you it's
neither necessary nor sufficient but is
a part of it like it is it is a part of
why I expect things to happen but there
are worlds in which that's not the
crucial piece like for me like I am a
stupid ape you know I have little
neurons so I try to keep my model simple
I look at intelligence I look at what
it's made of it's made of all these
little Parts all these little Parts
systems keep getting better at I look
around in the world more and more jobs
get automated by these things you know
they talk more and more better and I'm
like well talk to me about automation
because when I interview economists one
thing they they tend to agree on is that
uh this automation if it's happening
it's not showing up in the official
numbers yet so when you see you see
automation what is it that you see
here's an example of something that
doesn't show up in GDP numbers Wikipedia
Wikipedia is one of the greatest
triumphs of humanity ever it's one of
the most valuable things ever created by
Humanity period like just Wikipedia is
like more valuable like than like almost
any other artifact ever created by man
it it is kind of on the short list of
things that you would take with you if
you were to travel into space and could
only take yeah something like that you
show me like a massive like Dam or a
huge building or whatever and it cost
like10 billion $100 billion to build or
whatever and like that's much less than
Wikipedia's worth like Wikipedia is like
obviously worth more than this one
building like from a pure value
perspective Wikipedia contributes zero
to GDP it is not part of the GDP measure
it does not show up it does not change
things as far as economists are
concerned nothing of value has been
created here I think a lot of people are
confused what GDP measures and like what
these are like total Factor productivity
and so on what they actually me measure
is I'm not saying they're they're bad
measurements again naturalism I think a
lot of I think a lot of Economics is
naturalism and that's fine I'm not
saying this is as a critique I'm saying
if you know what you're measuring and
you know why it's measuring and it's and
like its limitations and like in what
scenarios are breaks down that's fine
it's very useful to measure GDP GDP is a
very very useful number to know and
helps you reason about a lot of very
useful things but you also have to
understand that it like is not reality
like it's it is a very very flawed
metric that doesn't measure like like
okay like think of the amount of
cognition of like writing that chat gbt
has performed since Inception and let's
say you put an hourly wage of a human on
that number how how much do you think
that would affect GDP you think it would
affect it a lot yeah I mean just just
looking at my own usage of these models
right I I it's a lot you would probably
like destroy the economy 10 times over
right it would probably be like as big
or larger than the entire economy if you
had actual humans typing out those
numbers of course the economist will say
that's not what we mean by that like of
course it it reduces the price but I'm
like all right but then you see how this
is misleading so you're saying if I add
more writing labor than the entire
economy to an economy it doesn't change
and I'm saying okay you can Define your
metric that way but that's not
necessarily intuitive to people that's
how the metric work but it is how the
metric works if I invent something that
adds 10,000 times more of a certain type
of labor but makes the labor extremely
cheap GDP doesn't change and so this is
very unintuitive to people and so I
think this is like or like the same
thing with like the slop right I think
the negative externalities of slop like
AI Mis you know garbage and
misinformation is very high but it's not
measured in G
if I like measure the amount of like
distraction or confusion or annoyance or
whatever that's like actually create it
if I measured it in hours or minutes and
then converted that to like working
hours attention Etc qualities you know I
expect the impact would be humongous it
would be absolutely massive but it's not
measured it's not part of GDP so from
The Economist perspective nothing
changed that's GDP what about employment
we we aren't seeing kind of massive rise
in unemployment numbers either so this
kind of points Us in the direction of
perhaps there's some automation
happening but people are finding other
other jobs or they're they're doing
other tasks to make up for it do you
think do you think that's true or do you
think there's also something wrong in
the way we're measuring employment I
think employment is a good example of a
number which is like relatively like
relatively intuitive what it measured
like it's like GDP I feel like
specifically like quite an unintuitive
thing if you haven't thought about it
like GDP is optimized to measure things
that don't change and this is very
unintuitive to people like people think
if a great change happens GDP must
change but it's actually the opposite of
what GDP does GDP changes little when
you have big changes in like prices and
like things and so on yeah I I mean I
know you just made this point but we can
we can think about the availability of
just entertainment or movies or
something which has increased kind of
yeah a lot since since before the
incident right but this isn't really
showing up because you're paying a
Netflix Subs subscription now perhaps
this is even decreasing GDP numbers
because you're not renting as many
movies at Blockbuster as you as your
parents did maybe exactly it's another
great example of of this effect
employment is a bit stricter not quite
there's some problems there as well but
like it's still it's much much clearer
so I think there is a there is a real
thing there I mean one thing I think is
that jobs are sticky in the economic
sense is that jobs are not liquid in the
same way that like many other things in
the market are so it just like literally
takes time job have a lot of transaction
costs a lot of frictions there's a lot
of social aspects to it as well and the
AI is just not quite AGI yet I expect if
we if we even if we froze AI as it is
today and we waited 50 years I think
things would be different I think if
people integrate these things more you
know new companies get created who don't
have like this like baggage of like
previous companies another thing is is
that a lot of employment is not about
skills it's not even about Labor it's
about responsibility I'm not saying that
like lawyers or doctors don't do skilled
labor they also do skilled labor but one
of the reasons they get hot paid High
salaries is they're responsible you're
paying them to be at fault if something
goes wrong this is a lot of what we pay
people to do we we pay people lots of
money often to be responsible to be the
the part of a chain which owns something
currently or at least in our legal
systems AIS cannot do this this is a
type of Labor that AI cannot perform not
because they are intellectually of doing
it but because the social contract does
not allow machines or AIS to be
responsible for things or to own things
I expect if such if we had a different
culture that for example does allow AIS
to own or be responsible for things it
would be very different yeah and and
perhaps this is why we could see lawyers
getting rewarded for AI progress like
earning much more money at least in the
short term because now they can write
whatever a 100 legal briefs or 100 legal
documents in the time that
previously could write one yeah hot I
agree with this this is actually a hot
take I have as well I think pargal are
really screwed because I think paralal
mostly do skilled labor and I think
skilled labor is bad is going is going
to zero but it's going to zero okay yeah
it's going to zero obviously and but
like the thing with skilled labor is
that lawyers do skilled labor but one of
the main things they do is that they
provide Authority they provide
responsibility and they take
responsibility for things and if the you
know you know commoditize or compliment
so if you if the bottleneck to Value
becomes Authority or responsibility and
you have infinite skilled labor the
scarce resource of responsibility can
become extremely valuable yeah I mean
this is something that perhaps could be
the case for other Industries as well I
have heard people talking about kind of
entry level programmers also being
negatively affected by by AI progress
because you know you can now have a
model produce a draft of a code that's
pretty good and so you have the senior
program and review it again as you
talked about with kind of authority to
to approve and and actually change the
code do you think do you think this is
this is kind of like this is an effect
you see we we're going to see across the
economy that entry-level jobs are are
facing a lot of competition from from
Models my prediction is that skilled
labor that can be performed on a
computer is under threat things that are
based on Authority or responsibility
will at least in the short term benefit
and will become more valuable rather
than less valuable and I expect all of
this will be completely relevant once
AGI Ares yeah when we are talking about
jobs I think one prediction I've heard
is that we will move into more kind of
social jobs or call them perhaps pseudo
jobs so perhaps we could take my job as
an example right if if if you you talk
to my Farmer ancestors they wouldn't
consider this an actual job but this is
more of of a social function or
something you might do for fun or
something if we have setting aside
issues of existential risk from AI if we
have a a an economy that is that is that
is automated to a higher and high degree
could we end up in roles where we we
function based on our relationships to
other people based on kind of historical
factors based on being the owners of of
various entities and and assets and so
on so unfortunately the question you
just asked is just not nonsensical if we
ignore X risk like there is no way to
answer this question without thinking
about AGI otherwise you're just not
talking about actual reality so you
could ask the question for
example assuming we align
AGI what will I be doing or like will I
have a job or will something else happen
this is a question you could ask not
saying it is a question you did ask but
like this is like a question you could
ask I'm not saying I have a good answer
to it but like question but like saying
like assume AGI doesn't happen will
there be jobs or whatever it's just like
that's not how the world works like this
is just not what's going to happen yeah
you're saying we can't get to extremely
Advanced AI without facing the problems
of not having solved the alignment issue
yes like definitionally like the
alignment problem is how do you connect
causally your desires values wishes
whatever to what actually happens in
reality and if if reality is mediated
through the power of an AG or an ASI
system which it will by definition once
an ASI system exists the causal control
of the future lies in the ASI humanity
is no longer relevant so the only way
that Humanity still has a causal
connection to how the future goes is if
we have a causal connection from our
values through the ASI to the Future
like it's the it's the big filter you
know it's the big filter is like either
your your human values get through the
ASI or they don't if they don't get
through the ASI does whatever it
whatever it does right so if you have
solved the problem of how to get your
human values causally through an ASI to
make it act in ways to instantiate the
things we wanted to instantiate in the
future you have solved the alignment
problem that is the alignment problem so
there's no way to get around this and in
a compendium you write about how this is
uh Humanity's most complex technical
challenge or at least it's one of the
most complex technical problems we face
why is it so complex why is it that we
can't take some of the some of the
advances that's that's that open AI has
made and for example making the model
make making a chat gbt talk in a nicer
way to you not using certain words and
so on why isn't that progress towards
alignment why isn't this an an
incremental Pro problem uh and and a
problem that we are making kind of
steady progress towards so there's a
couple questions there that think worth
kind of disentangling a little little
bit like one one question is like why is
alignment hard another question is how
hard is it another question is how
expensive is it which is related but
different question another question is
is the things that are being done
progress towards a another question is
why isn't alignment an iterative problem
is it an iterative problem yeah those
are those are actually seven different
problems I'm gonna reshuffle these
questions a bit in the ordering and
answer them in a slightly different
order so I'm going to start by answering
the iterative question is alignment
iterative question problem and if not
why not like most things are iterative
why is this not so alignment could be an
iterative problem there is a way to
solve alignment iteratively but it's
sure as  doesn't look like let's
build the biggest model we can as fast
as possible before China does and just
ye it on onto the world that's not how
you iterate so if it was that every
single time a new language model or a
new AI system is built everyone's like
all right guys shut down all the gpus
until we've understood every single
neuron until we have a formal theory of
how everything works until we've
unsolved all the bugs and then after
we've understood every single part then
we build the next AI yeah okay fair
enough yeah that would work like I think
that would work I'm not saying this is
realistic but this is like that would be
an example of how to address alignment
iteratively would be to like actually
take small steps what's happening right
now is not that we are taking small
steps so when people about iterative
alignment they are it's a misnomer it's
like it's just not true is they're not
talking about we will take the smallest
step possible that is safe what he
saying is we'll take the largest step
possible that I think we I can get away
with and then take them as often and as
fast as possible and this is not a good
outcome because again ASI is the filter
if you build
ASI and it is not controlled it's game
over simple as that Humanity has no
longer any causal effect on the fure
future you know whether we die
immediately or we hang around for a
little bit who knows but fundamentally
Humanity just has nothing to say anymore
we're just like you know chimps in a you
know in a zoo or whatever right like
there's nothing for us to do and
probably the AI stops feeding us so it's
over so when you are working on a
technology which could blow you up you
know if you want to actually succeed at
that you have to be very very sure that
every experiment you do will not blow
you up otherwise you blow
up it's that simple right it's literally
that simple if you are making explosives
you have to be very sure that every step
of your process doesn't blow up
otherwise you don't make explosives you
die is that easy so that's why it's not
an iterative problem it because not
because it couldn't be an iterative
problem it's because people are deciding
to approach it like how can I mix
chemicals as fast as possible to get to
the biggest explosive I can if you do
this you die it's not because this
couldn't hypothetically work you can
actually make explosives safely this is
a thing that can be done but it's not
done by doing it as fast as possible
before China gets the explosives that's
just not how the world works right there
is some work that is being done on you
know AI safety and you know allegedly
alignment like getting chapos to say
what we want and this is all like cute
but it's Alchemy like if you look at the
actual things being done it's not
science it's not engineering let me give
you an example of something that is like
closer to Real Alignment work fixing a
bug in the C++ compiler this is much
closer to alignment than most of the
work that is being done at open a the
reason is is that when you're is that
you know when you're building a compiler
you are trying to build a system that
causally connects the will of the user
to the will of a machine this is
literally what you're doing right you're
literally building a thing where humans
speak a language and like confer a will
that is then absorbed by the machine and
then causes the machine to causally do
in reality the thing the human asked for
this is much closer to actual alignment
than most of the work that is being done
at these Labs hot and most of the work
at these Labs is of the form my AI does
something I don't like I hit it with
stick until it stops doing that and then
once it looks like it's not doing it
anymore I give myself a pad on the back
and publish a paper about it this is
cute it I think it makes a better
product like to be clear I think CH I
use chat GPT I use Claude I mean you
know claude's better than chat GPT
Claude gang rise up but and they're good
products right like you know CLA is
stupid sometimes or makes mistakes or
says things I don't like or something or
whatever but like who cares right like
whatever like I don't need it to be 100%
to like provide me value fine if we
build a system with a level of
reliability clae has and we give it
causal you know physical access to
reality and super intelligence that's
not fine a much much higher risk
scenario you know you can have a bit of
a stupid funny goofy thing if it's an
entertainment product or like you know
or something like this you can't have
the same level of reliability when we're
talking about nuclear control when we're
talking about you know governmental
control military like you know real
world things like this which is what ASI
will be ASI will be systems so the level
of reliability you need when you're
dealing with an ASI scenario is just
exponentially higher than the reliabil
noral systems It's Kind the rocket
problem to to the Moon you don't build
bger and bigger laders that will never
get you to the moon it's like
structurally a ladder cannot get you to
the Moon it can't be done you need to
invent a rocket and a rocket is a way
more complicated way more confusing
thing so if you went back to you know
the Middle Ages and you told people oh
in the future went to the moon and they
said oh we did using a special ladder
they'll be like all right y seems
reasonable I guess like maybe they have
some like special metal in the future
that's like indestructible so like yeah
okay seems like that could be possible
if you we did a rocket they'll be like
what are you talking about that word
doesn't even exist in our language you
know like I don't even know what that
means you know okay some people in the
Middle Ages you know gunpowder but like
most people like that word doesn't
translate I don't know what you're
talking about the same thing applies to
AI is that most of what we're doing
right now is building ladders we're
hitting it with sticks you know to be
clear bigger and bigger sticks you know
and like you know Big Sticks have you
know economic balance
but this will not structurally get you
to a point where you actually understand
the internal cognition of these systems
and they're actually safe at a level
that you would entrust them with like
say ASI level control over reality and
so so the difference the hurdle we can't
get over is that we can provide good
enough feedback to to more and more
advanced model is that the main the main
issue that that is another issue like
this is where the thing defeats itself
in its own Theory so like I I think it
fails even before then to be clear like
I think it's structurally is kind of
nonsensical so the most common form of
these like iterative alignment proposals
that people make it's something like we
if we give the machines the right
feedback that things are good or bad we
can steer them towards the directions
that we want I already reject this
premise this is already not true have
you heard of the concept of lying like I
like I'm not sure if these people have
just like never met another human like
have you like oh if I just give a human
the right feedback they will do an
arbitrary thing I ask them for and never
deceive me like have you met another
intelligent even a like a chimp a dog
like dogs know how to lie you know if
they can get more treats out of you if
you just give them the right feedback
yeah the like they'll just do what gives
you more treats you know even you know
may may may maybe your wife got up
earlier and fed the dog and then he's
gonna ask you for even more food when
you wake up you know it's like is the
dog unaligned I mean it's like there's a
deep thing where like I already reject
this premise it's just like feedback by
itself is not a sufficient mechanism to
guarantee the kind of high level
reliability that you require I think it
is enough for like you know dog level
alignment you know maybe you know
probably not even that but like you know
for chat gbt level alignment right like
it says the nice thing most of the time
is mostly fine you know whatever right I
think for that yeah it's fine but this
is just not sufficient so the second
thing is is that there's a there's a
there's the the second premise is that
this is sufficient for an ASI is that if
we have an ASI that's as align chat gbt
that's fine and this is just like
obviously blatantly not true to me like
you what are you talking about this is a
much more complex scenario where it's
much more on the line it's much more
complex has way more edge cases that is
like it's like the thing I was saying
earlier you're saying you want to solve
all of the problems that all of our
institutions throughout all of history
have tried to solve you want to solve
all of moral philosophy all of Economics
all of science you know you want to Sol
all these problems using software with
that level of bugs and I'm like no
you're not like that's not how reality
works if you are trying to solve a
problem that is that hard like you need
a much like unbelievably higher level of
reliability this is like the this is one
of my core things is that I think people
just like think ASI is just going to be
like chat GPT but bit clever and this is
just not what we're talking about we're
talking more like imagine the US
government was sentient and like smarter
than everyone on the planet like that's
what we're talking about that's what an
ASI would look like yeah it's it's it's
not a more advanced chatbot it's a much
more advanced kind of system that can
solve tasks in in many domains and many
more domains than than our chat bu today
like imagine if the entire US Government
like every single person in the US
government was out to get you yeah for
for for our schizophrenic listeners
they're not out to get you don't worry
this is just hypothetical you're safe
but like imagine for a second the entire
US government was out to get you and
just screw you over in every way
possible could you defend against this
no you couldn't You' be completely
overwhelmed like You' be they come left
right all over like from all possible
angles you would be outmatched
outclassed you would be tricked you'd be
deceived you'd be brutalized like there
would be there would be no possible
defense against a thing like this right
it would just and like even now right
like we have US Government right which
is like s sort of align sometimes right
but still it just like heinously evil
things for no good reason you know like
recently there's that story about that
like you know you know squirrel that got
killed or whatever right and like I
don't think anyone really wanted to kill
the squirrel I don't think any
individual person was like haha I sure
love killing lovely little squirrels but
it happened anyways because the system's
a  mess because it has bugs
because it's misaligned it's it's like
like the US government does stupid
things that are not in its interest and
hurt its citizens constantly not because
they're evil like you know sure there's
some evil people involved but like it's
mostly just bugs it's mostly just the
system is buggy it's just there's many
things there that are just like stupid
and poorly designed and don't do what
they're supposed to do now imagine that
but like sentient and a billion times
smarter like that that's a problem if
when we are thinking about AGI or super
intelligence we shouldn't imag imagine
chat GPT that outputs kind of a genius
level text for us what is it that we
should imagine kind of your Mainline
scenario what what do you envision
Happening Here what what what is the The
Next Step might be agents but after that
what comes what comes next there's at
least two two questions here and two
questions I see here are what will
happen or like what will an AGI how will
it work and the second is what will we
see I think those are very different
questions because I think the things we
will see or not what will be what H what
actually happens like no I expect that
AI takeoff will be so complex it will be
so distributed it will be so confusing
that no actual individual person will
actually see everything that is
happening and understand what they're
seeing I expect that for us when AGI
takeoff happens it will mostly be very
confusing it's just a lot of weird 
happens that we like can't quite explain
as like weird behaviors social media
gets confusing like a lot of like weird
stuff like political things seem
happening and like Market does some
crazy like weird things or not and just
like a lot of really and like some new
technology starts popping up in some
like weird areas we don't really know
what made it or whatever and then
eventually just ends it's just like most
people won't even realize anything's
wrong until just like one day you're
just like stop existing or you know
something like this is what my main line
of what I expect will happen is just
things will just get confusing they will
get more complex it will bit things will
move so quickly and things will change
so quickly like like do you know like
what is happening in Ukraine right now
not not to any level of of detail no and
but and but worth there is no method by
which you could acquire actual knowledge
of what's really happening in Ukraine
there's no way like it's too complex
there's too much going on the signals
are too conflicting there's too much
propaganda there's like no method by
which you could reliably gain actual
true information of what is truly
happening there
actually my my best guess at how to
acquire true information is to read the
Wikipedia article that is actually what
I do and that that that is my attempt
right and I'm I'm I'm certain the
comments will tell us all about how
terrible of an idea that is and all the
problems that has but I agree I think
Wikipedia is genuinely one of our
greatest epistemological victories I
also trust Wikipedia more than I trust
most Source not that vikkipedia is
perfect I well aware of many
shortcomings but that also shows us what
are level of epistemic defenses are if
you're actually dealing with a
coordinated adversary that is like
extremely smart smarter than you you
know was distributed
you it can just make you believe
anything you want it wants right so I
expect Aji take off to mostly be
confusing I don't think it's going to be
like epic or even necessarily scary I
think it will mostly be just very
confusing and just really weird and
people will be upset and there will be
 happening but like no one will be
sure what's happening or who's happening
or like whatever and you know maybe
there'll be a couple people who get it
right by coincidence who by coincidence
be screaming into the void what's
happening and like they will be drowned
out by all the other  happening
and then so just like no one will be
able to coordinate no one will be able
to figure out what's happening until
it's too late this is my Mainline
prediction of what will happen yeah
that's that's quite depressing that's
it's we I mean if if we're all confused
then we won't be able to even kind of
agree on on what what's going on or how
to respond to it there isn't really
anything to respond to because we we
don't have we don't have kind of like a
shared understanding of of what's
happening this is exactly what's
happening I mean right now like you know
the US is this is the day before the
election I have people saying things on
my you know Twitter Fe or whatever like
good normal San people where I'm just
like these words are just not connected
to reality this is mental illness like
not even just to disagree with them like
politically to be clear like like just
like whether or not is true this is an
insane thing to say like this is just
like this is not a thing a normal like
this is what a person having a mental
health crisis would be saying like the
emotions being expressed here the words
being said here how they're being said
the viciousness the fighting it's like
this is how people the mental health
crisises act this is how schizophrenic
people act and this is big problem this
is pre AGI yeah just just to P push back
on this point that this we can we can
talk about our epistemic crisis and we
are overwhelmed by information and so on
on the other hand we also have much much
more information than we had 50 years
ago 100 years ago 200 years ago and you
know they were able to somehow kind of
navigate the world I think more
information has helped us being kind of
better able to navigate the world even
today I would say having kind of
governments businesses kind of just
citizens have much more information and
this you know I can I can't really tell
you perhaps in any kind of in
interesting level of detail what's
happening in Ukraine but I can get you
the price of Bitcoin or the price of
Apple stock and so on uh quite reliably
don't you think that us having access to
more information is also kind of leading
to better decision making so I'm so glad
you brought this up because I think this
is a great topic to to address some
common misconceptions about reality so
you all Harari an author I really like I
recently wrote a book called Nexus I
love the first half it's some of my
favorite writing I've read in a long
time second half I found was a bit
weaker but overall the first half is
extremely good because he makes a thing
I already knew and believe but like he
like lays it out like so much nicer than
I've heard anyone else lay it out which
is fundamentally more inform that
information is not truth these are two
different things they are just not the
same thing more information does not
mean more truth it is often the opposite
historically speaking more access to
information usually actually did not
mean that there were more true things so
specifically the common example that
people always bring up is the printing
press always say what about the printing
press the printing press brought science
it brought the Scientific Revolution
whatever and this is just not true like
I don't know if people have just like
never read a history book but like this
is like historically false the printing
press came 200 years before the
Scientific Revolution the direct result
of the printing press were the witch
burnings so after the printing press was
invented Cernic's you know revolutionary
theory about you know the heliocentric
model didn't even sell out its first
edition of like 40 copies but meanwhile
the malus malarum the hex sold gang
Buster and it was a huge tone describing
a conspiracy theory about a satanic
pedophilic cabal which is take you know
controls the whole world and is you know
trying to take away your children and to
you know eat them and like whatever
right it was the prototype for every
modern conspiracy theory and that sold
thousands tens of thousands of copies
the main effect of the printing press
was witch burnings and the 30-year War
like these were the direct outcomes of
the pr press not the Scientific
Revolution this is just historically
inaccurate now was the printing press an
ingredient to the Scientific Revolution
yes of course being able to reproduce
scientific information was an important
useful you know logistical component of
the Scientific Revolution but if you
want to pick one like you know proximal
cause of the Scientific Revolution it
was probably the founding of the Royal
Society in London that's probably a much
closer proximal cause and that because
and that was a social Innovation it was
an Institutional Innovation it was an
innovation where you were allowed to
criticize others you were allowed to
question truth you were and you were
expected to but you know this wasn't the
only culture in history that had this
Norm but like the Royal Society is like
a very clear example of a institution
that had these like peer riew concept or
this idea of criticizing ideas that
nothing is taken on faith and Stu like
this this is a much more proximal cause
for the increase in Truth Versus noise
than more information more information
made people less correct they made
people more wrong and to a large degree
can you say social media is different
like can you say that the proximal cause
of social media is more social harmony
and cohesion and more democracy this is
not obviously true remember in the Arab
Spring but but that's a slightly
different point right social media
hasn't resulted in more social cohesion
for example I don't I don't think that's
the point is that maybe it has resulted
in more information being available and
so then the question is whether people
can use that information productively to
to kind of develop accurate models of
the world but this is like saying I
bring you a bunch of like toxic sludge
and I say look here there's more biomass
look I brought you you like food right
food is made of biomass here's some
biomass are you happy what are some
systems we could set up today then to
help us kind of navigate the world
specifically as as as it relates to
navigating this dangerous period of of
the developing Advanced AI well there's
a specific question of how do we address
AI risk specifically and the more
general question of like how do we build
like better epistemic Norms how do we
build better institutions and so on
they're closely related the the main
difference is is that for AI
specifically we're running out of time
so we need to the first thing we need to
do is to buy more time if we ever get
ourselves into the scenario where an ASI
exists that is not deeply controlled and
aligned with Humanity it's game over so
the first and primary policy objective
must always be to never get into this
situation by whatever means you know are
the most effective to prevent this from
coming about so my colleagues over at
control AI have wrote a very nice
document called The Narrow Path which is
a
set of policy principles which describes
exactly this listeners can scroll back
into feet to to hear my interview with
Andrea about this document exactly so
I'm sure Andrea did a fantastic job of
explaining these better than I could but
basically these are the proximal direct
policies or the types of policies that
would need to be implemented to actually
you know not die from AGI in the next
couple of years this is the kind of
things we need to do this does not mean
so the Narrow Path also talks about
flourishing like okay once we're not
dying what do we do then I think there's
a lot to be said there as well how do we
build better science how do we build
more just systems how do we how do we
causely connect what people want and
what makes people happy with the future
this is the fundamental problem of
statecraft right the fundamental
question of statecraft is how do we
build superhuman systems so systems that
are larger than humans that causally
connects what the citizens and the
people actually want with the the
outcome that the state or the larger
system produ and this is a very hard
problem but it is something that we have
learned a lot about we know a lot more
about how to build good States we know a
lot more about how to about epistemology
we know a lot more about many of these
things like we didn't have Game Theory
until the 50s like it's crazy to me that
we still are running states that are
based on constitutions and philosophies
you know from like Mont SK and so on
that were invented before Game Theory
existed that's
crazy that's crazy we haven't even
updated this so I think there are many
low hanging fruits that are mostly
blocked by coordination as always and
like building institutions is pain
because you have to deal with people God
forbid know you have to actually work
with people and talk to them and build
alliances and so on there are many
things that that can be done but they
are hard there are no easy solutions but
they can be done previously you seem to
connect this kind of crisis epistemology
to our inability to to solve problems
around kind of AI risk but but those are
not those are not necessarily connected
and perhaps the first step here is the
the first step isn't that we kind of
take it from the ground up and try to
solve our a sense making tools and then
we try to address AI risk simply because
we don't have time am I understanding
you correctly there yeah look if we had
infinite time I think it would be
fantastic if we could spend three
generations of our greatest
mathematicians solving the fundamental
epistemological problems of mathematics
right like this is what we would do if
we were not a stupid civilization like
if we were like an actually wise
civilization what we would do is we
would actually solve deep problems of
mathematics a philosophy of spirituality
of religion like there are deep things
like you know this is always like taboo
to talk about among like technical
people but there are deep things about
like human psyches that are like that
religions and spirituality do talk about
that are pre-formed that are not
necessarily physically correct or
whatever but there is there is stuff
there that is very important to people
that is very very very important to
people that is a deep part about being
human and is just completely neglected
and it's just not solved like how to
make people happy and feel fulfilled and
spiritually connected and so on is like
is a problem like that needs to be
solved like this is a deep thing you
can't build a good soci without adding
problems like this but also questions of
like how do you get different religions
to coexist peacefully how can you create
a net positive environment where
different spiritual Traditions or groups
can coexist in a mutually beneficial
world like these are deep deep questions
and they are far from being solved right
like these are very very hard problems
that I think are solvable to some degree
I think you know whether moral
philosophy is solvable or not is a bit
Up For Debate but I think there are at
least improvements that can be made here
a lot and yes if we took the time and
you know at least we should spend you
know Sundays thinking about these kind
of problems at least I do but I don't
expect we will be able to solve all of
these problems before deadline since
deadline is currently in the next couple
years if you look at open AI anthropic
and Deep Mind those are the companies
that are that are currently kind of
Front Runners in the in the AGI race and
if we are very close to to AI do do you
expect those companies one of those
perhaps to be the the one that actually
kind of gets
there seems quite plausible I mean kind
of the default case I wouldn't be
surprised honestly if it was an open
source thing I I think there's been some
work in the open source World which uh
surprised me which which spk
specifically well wouldn't you like to
know okay you don't want to you don't
want to say what it is publicly I prefer
not drawing more attention Okay Okay
makes sense to me yeah I hadn't actually
I'm not thinking of Open Source AI as
kind of competitive at The Cutting Edge
the main thing with open source is that
it is way more High variance it takes
way because it's way more distributed so
it takes May way more crazy bets so if
you need a crazy thing to work to get to
AGI it seems more likely to happen in
open source at the moment than in one of
the labs because the lags are drisking
like as you gain scale you also have to
drisk you have to do less crazy things
because there's more on the line now I
my main line to be clear is that the
drisking strategy is the thing that gets
us to AI it's just we scale further we
make more data and we just like you know
do normal amounts you know normal kinds
of engineering R&amp;D and it doesn't take
anything crazy but if I'm wrong it takes
a crazy thing or there is like so I do
think there are crazy algorithms that
have not yet been discovered that are
like 10 a 100 or a thousand or a million
times more efficient than deep learning
like I'm pretty sure that's the case or
and if those get discovered they're
going to get discovered by like some You
Know M grad student or the open source
movement probably not by one of the big
companies yeah all right I mean my my
main reason for not expecting open
source to be at The Cutting Edge is just
that I'm I don't think meta will fund a
kind of increasingly insanely expensive
training runs I think those training
runs will be done by by Deep Mind open
AI anthropic supported by each of their
big Tech funders yeah I I'm not sure
kind of metas investors will let them
spend the money I don't know what you do
do do you think that's true I don't know
I don't really know I mean so far it
seems to be working on really great for
meta I mean their stock price is doing
fantastic so I mean so far the BET seems
to be paying off for them but that being
said like intelligence isn't Magic I
think you can get AGI probably with the
same amount of computer C3 like if you
knew how to do it you could probably do
with even less than that like I think
the lower bound on like human or super
human intelligence is like a 4090 or
less like it's probably you if you
probably you had the perfect algorithm
you could even do it just like on a
CPU and like you know probably a c you
probably need a decent amount of memory
but like enough that you could like fit
it in like a laptop right like a modern
laptop like it seems like there's no law
or like theorem or anything that I know
about again because we don't have a met
you know a science of intelligence that
doesn't says you can't get human level
AG on a single you know M1 MacBook maybe
maybe there's some limitation but it's
not obvious to me I don't think we're
going to get there before we get AGI I
think the first AGI we're going to make
be super clunky and know take billion
time more resources than optimally
necessary but like if we were to stumble
upon some of these big breakthroughs
it's like not obvious to me that you
need llama 4 five six seven to make it
work I think it makes it easier like I
think having llama 3 makes it easier to
build a Jaz having llama 2 I think you
know but I don't think it's like I think
we're already pth The Event Horizon in
this regard like I think it's already
possible to build AI with just the
current tools that exist if you knew how
to do it I I'm not saying I know how to
do it but like I expect that if a
dedicated group of hackers you know and
like you know a couple 409s a couple
MacBooks and llama 3 and they get very
lucky could already do it but that's not
the most likely scenario the most likely
is that one of the corporations develop
this by by you know running a gigantic
training run yeah running a gigantic
training run and also scaffolding around
them like you know agent scaffolding Etc
like I think I I often joke is that you
know AGI is going to be like you know
gbtn plus a thousand lines of
python yeah all right how efficient are
these techniques do I think kind of when
we talk about un hbling or scaffolding
or all the all the things we do after we
have finished training runs what what
does that give us in terms of
capabilities I think that's the
difference between AGI and chatbot
basically I think a lot of scaffolding
at the moment is stupid and predictable
ways that will get solved there like
lots of low hanging fruit which is like
people are just like doing it quite
poorly in like in like dumb ways also
for like just like there's just like
many it's a large design space right
like it we've only been doing it for a
couple years we've only had like non-
stupid models for a couple years like I
think gbt2 could get to Ai No matter how
hard you poke it probably but like GB3
probably not gb4 yeah no I like I think
if you had like a gp4 level model and
you like knew how to do scaffolding
correctly you could get to AGI there's a
lot of kind of latent intelligence in
the models that are not being used right
now oh yeah yeah yeah like like I don't
know if you've ever seen someone who's
really good at prompting I have I have
observed that and that is that is insane
what they they can make models do things
you just like would not believe like
that seem like it shouldn't be possible
right so like and again there's this
thing where just like intelligence is
composed of smaller components and like
if you actually Zoom down on these
smallest dissolved components they're
not they're not that big they're not
that complicated like gpt3 can already
do formal reasoning right you can
already teach it you know to do math
right it's like that's already like a
lot of the way there you know I'm not
saying it's all the way there but it's
like this is a lot of the way there and
then you know they can learn facts they
can pattern match they can do metaphor
and logic and whatever right and just
like I don't think intelligence is made
out of that many things like there are
things and you need to get them right
but it's more of an engineering problem
right it's more like you know if I show
you a lump of uranium You' like ore
you'd be like wow this is not dangerous
at all look how far this is from fion
and I'm like yeah but now it's just an
engineering problem do you think using
inference time compute will play a
larger role in in kind of future
capabilities this is this is where
whenever you type into chat gbt and it
says thinking that that's that's
inference time compute what do you think
I'm not I'm not explaining that to you
I'm explaining that to listeners yeah I
mean seems obvious I mean humans do a
lot of inference so seems logical that
like if if you if you don't do it in
inference time you have to do a training
time and then you have to pre-bake it
which is just more expensive why is it
more expensive because you have to
account for every
possibility you have to kind of lay out
exactly what could be asked in yeah like
imagine the difference between writing
every possible sorted array versus a
sorting algorithm yeah yeah okay Conor
is there anything we haven't touched
upon that you want to say to our
listeners that's a good question I mean
there's we we could talk a lot about
politics and institution design and all
these kind of things I guess the thing I
would really like to talk about maybe
just like more thing also direct to
listeners is a little bit
about this like well what do we do like
what do we do about this like I haven't
Justified a lot the extra stuff in this
talk I hope your listeners dear listener
that you either buy the hypothesis or
you know you go read the compendium and
read why I think this is a problem I
will link that and listeners can also
scroll back to some of our previous
episodes in which you do spend a bunch
of time justifying AI risk or many of
the other episodes that we've been doing
over the last couple of years so I'm not
going to justify that further so I'm
going to assume you're bought into this
you're bought into the things I've been
saying what do we do like what are
things that you listener might be able
to do and so I think I have a slightly
different opinion here or like at least
way of thinking about it than I think a
lot of other people do where when you're
confronted with a problem as massive as
AI Extinction right it feels very
natural that the response must be
massive that you know there's something
so huge you must do something big you
must like drop everything and work on
this fulltime you need to you know yell
at all your friends about it you need to
make it your identity you need to like
sacrifies everything whatever right and
I'm here to tell you that that is not
the right thing to do the truth is that
I mean a this is just usually very
counterproductive because the main thing
that will happen if you attt to do this
in most circumstances it's just you burn
out and that not going to help anybody
and now you could say oh but I should do
it it's the right thing to do I won't
burn it I'm like bro listen to yourself
like this is just not how the real world
works the actual things that need to be
done to solve AI to solve AI risk things
are mostly extremely boring this is very
important to understand most of the
things the actual work that needs to
happen if we want the future to go well
is very very boring it is not epic
exciting we need to you know Crusade on
social media or we need to solve this
cool math problem or build this big
machine like all these stuff are like
nice and fun and seductive but they're
mostly not what we need to do most of
reality is very boring most of power is
very boring one of the most maybe the
most powerful entity in the world the US
government is mostly extremely boring
it's just unbelievably boring it's
bureaucracy and Petty politics and
writing things down and having long
meetings and etc etc right and now when
people see this they might assume well
there is some exciting part that I'm not
seeing or that I see on TV where TV
tries to present these things as like
exciting and memeable or whatever or you
might think it's like defective like if
it was non- defective it it wouldn't be
this way but the truth is is that most
complex things are made of simple Parts
most big actions are made simp things
most of coding is not coming up with
some brilliant new algorithm that will
be named after you and you're going to
win an award for most of it is trying to
 connect to the stupid web server
you know and copying code from stack
Overflow right like Mo 99% of your
coding work is going to be rehashing
things that people much smarter than you
did I've already solved better in the
past it's just how it is right the
reason I'm harping on this so strongly
is is aesthetic is that there's a lot of
boring work that needs to happen when I
talk to policy makers to the general
public including to extremely powerful
people like even like billionaires and
stuff like this it is utterly shocking
how many of them have just never heard
the arguments literally no one told them
it's not that they like heard the
arguments thought about them carefully
and rejected them that happens sometimes
but most of the time literally just no
one sat them down and just politely and
patiently explained things to them like
I've had meetings with like highlevel
politicians well I'll sit down take an
hour or something and they'll ask me a
bunch of questions about AI or whatever
and he'll answer all the questions I
remember this like particular like one
politician I talked to this where like
at the end of the meeting he was just
kind of like when I answered he asked me
like some complicated question and I
answered the question and he looked at
me and he was kind of like wow you're
really answering my questions like he
was shocked that like cuz that I was not
trying to sell him something I was not
trying to make him do anything I was
just taking the time to explain things
to him and he was so thankful for this
he was so genuinely thankful for this
that just like someone took the time out
of their day to just give him
information to help him this is how not
everyone like topic there are corrupt
politicians evil people blah blah blah
but like most people in most
institutions in most bureaucracies are
just normal people they're just trying
to get by they're overworked they're
overstressed they don't know what to do
they there's all these things pulling
them in different directions and we need
a lot of patience a lot of work of just
explaining things very carefully over
and over to different people to you know
helping them do the things that they
need to do and so on so what do we do
the kind of things that I and you and
all of us need to do is we need to just
have patience and actually explain
things to people we need to actually
reach out to politicians and be like hey
what are you doing about this problem
I'm very concerned here's this
compendium thing you can read about it
or I'd be happy to come over and tell
you more about this or I know my or
here's my friend who works in an AI
startup would be happy to explain this
to you I like I know a guy who has been
just like just emailed like all of his
local senators and house members and
whatever every day he just emails them a
summary of like every week he emails him
a summary like cold email just emails
them a summary of what's happened in AI
this week and he's gotten like three
meetings out of this because they were
like genuinely thankful they're like oh
thanks for sending this like hey could
can I ask you a question about
this that's great like this is awesome
if we could get two people in every US
state to do this to just like send your
politicians a very polite helpful email
of like hey here's what's going I'm
concerned about this you know but like I
thought you might want to know this
like it sounds like nothing right like
it seems so small these are the kinds of
small things that large coordination
that large institutions are built of
these are the kinds of small actions and
I need people to do stuff like this I
need people to help do stuff like this
read the compendium you know talk to
policy talk to your friends talk about
this on social media because this is a
civilizational problem this is not a
problem you or me can solve this is a
problem that we have to solve as a group
as a species as a civilization and the
way we do that is we need to talk you
know guus you were saying earlier that
like your ancestors wouldn't consider
your job to be a real job I disagree I
think they would have understood if I
told them oh you're a messenger or
you're a diplomat they would have been
like oh of course yeah of course that's
very important the kingdoms must know
the information that's great of him that
he's doing that I'm glad he's extracting
all the
and bringing it to all the people what a
great job like our ancestors would have
actually understood the concept of a
podcast you know they wouldn't
understand you know microphones the
internet you know per say but the idea
of someone who like finds interesting
people who know interesting things
extracts your information and brings it
to people that's labor like that's like
obviously labor you know like maybe the
information is not useful maybe it's
just amusing or whatever but that's
labor moving information around
civilization around Society is work the
same way as moving Earth or moving you
know materials is labor moving
information around the graph is labor
and this is labor that needs to happen
this is a very very important thing yeah
I think this is very important is that
we need to move this information it
needs to be replicated it needs to be
spoken it needs to be talked about
because then people can reason about it
like something is uncool and weird you
can't think about it you can't talk
about it it's like you if you're a
politician and you keep talking about
something weird you lose your job
something is not weird if it's an
important issue that everyone keeps
emailing you about then you can talk
about it then you're allowed to think
about it there's another thing you said
earlier which was how more information
allows you to know more things like you
could know the Bitcoin price right this
is true but will you decide to know the
Bitcoin price the the scarce resource of
right now is not information it's
attention you could know many things you
could know so many things will you you
could read every book in history will
you no you can't you have a limited
budget of attention of what you can know
so the difficulty is no longer you have
there are few things to know the problem
is now you must choose what to know you
must choose what to think about what to
process in your mind what to put into
your brain and actually think about and
politicians and all other like you know
people who are like busy have this huge
problem where there's so much is vying
for their attention so you're so a lot
of what you have to do as a citizen is
help these nodes you know these
connector nodes your politicians and
your influencers and you know your even
just your local friend group you have to
help them draw their attention to the
thing they need to pay attention to you
need to help them do this like this is a
process and then once they can reason
about this once it's less it's more
normal they can spend more attention
they can form better opinions they can
ask better questions they can find other
people to work with they can to ask
questions to and to build coalitions
this is the kind of stuff that we need
to do we need we need large coalitions
of people across the Spectrum not just
tech people you know I assume a lot of
tech people listen to this podcast but
if you're a non- tech person you're the
kind of person I want helping with this
you know whether you know people from
Civil Society you know people from like
you know NGO backgrounds or or you know
unions or just you know day-to-day
normal people interested in this topic
whether it's academics or it's you know
Faith groups all of this like this
affects everybody AGI AI is a thing that
affects everybody this affects your Lo
it affects your job it affects your
family it affects your church it affects
everything you should think about this I
am trying to make the case to you that
you should give this a little bit of
your attention your attention has
extremely valuable you have you have
your family to attend to you have your
life to attend to there are many that
are valuable to you to attend to I'm
making the case please attend to this as
well put a little bit of your attention
on this not a lot I think it's important
don't put your whole life on this put
10% 5% of your attention 1% of a couple
hours a week one hour a week two hours a
week of your attention thinking about
this what do you think about this how
does it make you feel who do you trust
here or not where can you what questions
do you have how do you get these
question answered the thing that I
always recommend to people is like
literally like open a Google doc and
start writing like what is my plan like
what do I think about AI what are my
questions where am I uncertain how do I
get to the thing I want to and like who
should I who should I ask who should I
work with like this sounds stupid I'm
sorry it sounds so stupid it doesn't
sound stupid it it sounds like valuable
advice I think it may maybe it doesn't
sound prestigious but it doesn't sound
stupid to me and I think that's an
important difference that you made your
you kind of sketched out yourself I
think you're right about this if it
works it ain't stupid and thiss this
works if you if you dear listener
actually want to have a causal effect on
the future going well no joke you
actually want to have a causal effect
the thing I I think you should do is you
go to your computer you open empty
Google doc start a bullet point you
start writing out what do you want the
future to be like what do you want it to
look like and you start asking well what
do I need to do to get there like what's
currently broken or like what do I need
to do and then you start notice what
questions do you have where you
uncertain what are resources that you
currently don't have access to you know
who do you need to reach out to or like
what actions can you take and don't put
too don't break yourself over this put
an hour of work into it don't do more
you know maybe once a week have a call
with someone you know send send me or
someone else an email just like ask some
questions you know a lot of people are
nice including a lot of famous people
are very nice you know you can just ask
them questions you know go on social
media ask questions try to figure it out
find other people in your area who might
be interested in this is Civics this is
the job of a civilian to build a greater
civilization civilization civilians like
our job is to build a greater
civilization for ourselves and for
everyone else and this is how we do it
this is the process of how we do it and
so I just ask anyone listening to this
like join me like do do the small things
I think that's valuable advice Conor
thanks for thanks for chatting with me
it's it's been great thanks it's been a
real pleasure

well you know the the big disagreement I
have with Rey is I think after you get
human level AGI I think you're only a
few years from Super
intelligence and he he he said human
level AGI 2029 Singularity
2045 but I think his curve fitting is
overfit to having humans do the
invention like I think once the AGI is
doing the invention the exponent the
exponential growth curve becomes bigger
and I think it only will be a few years
from a human level AGI to a a super AGI
because that human level AGI will be
able to program and and invent new new
chips and invent new forms of networking
and so forth it seems like it should be
able to to upgrade itself Prett pretty
rapidly if you train in llm on music up
to the year 1900 only it's not going to
invent grind core neoclassical metal or
Progressive Jazz I mean it it may do
cool creative things it'll make up new
songs that you never heard before if you
ask it to put West African drumming
together with Western classical music it
may even manage to like set Mozart to a
poly Rhythm or something right but to
get to Jazz and and metal and World
music and all this is just a number of
creative leaps that I really don't think
anything like a current llm is is is
going to is going to be able to take
llms will not be the central component
in an AGI system so if you're spending a
lot of time
trying to make an AGI system of which
llms are the main piece I think you
mostly be wasting your time Dr Ben gzel
is a renowned artificial intelligence
researcher cognitive scientist and
entrepreneur known for his pioneering
work in artificial general intelligence
he's the founder and CEO of Singularity
net which is a decentralized AI platform
and he's authored numerous books and
papers on AI and complex systems and the
future of technology including his new
book out on
Consciousness okay Ben um it's an honor
to have you on mlst again yeah yeah yeah
it's good to be back yeah so um do you
feel the
AGI well you can feel progress
accelerating right and which is which is
is exciting I mean it's a it's a
different feeling with these AGI
conferences the last few years than it
was say 2006 7 8 n when we started
because now each year there's really a
lot of new stuff just new ideas and and
and new things working so I mean it's
not not yet as fast a pace as you see
with Machine Vision or NLP but it's
still it's still palpable like there
there's quite different stuff happening
each year than the previous one and that
that that's pretty exciting I don't
think we yet have the feeling at this
event like the AGI is here the human
level AGI has has has been launched
probably the folks in this community
felt less that way about llms than than
the average person because we're overly
aware of what's uh what's under the hood
yeah I mean talking of um llms I mean
what what are your Reflections on the
state of AI at the
moment so the state of AI R&amp;D is one
thing the state of AI in the commercial
world is is is another thing and of
course the state of AI as perceived by
the general public is is is yet a third
thing right and so I think in the R&amp;D
world there's a great variety of threads
of research going on I mean there are l
in deep neural networks there's logic
based systems there's evolutionary
learning there's attempts to formalize
what what is a general intelligence
there's great variety of different
neural architectures behind back
propagation or Transformer neural net so
there there's a lot in the R&amp;D world as
as there has been for for for many
decades and there's rapid progress in a
whole variety of directions that even
defies concise summary I'd say the
business business World tends to be a
little more herd animal like than the
research world I mean the research world
can be like that too especially funding
sources but the VC world is famously
like that right like something has won
shiny success and suddenly everyone
jumps on that on that exact thing and
one company does well with something
every company can't be left behind on
that exact thing so I think in the in
the business World obviously we've seen
a huge enthusiasm for large language
models and and and trying to see what
they can do in in in different vertical
areas and that that in a way seems to be
reaching a peak now and people are sort
of getting burned out realizing they
don't solve all problems immediately
with with no work but I mean I think I
think they're still going to be a big
success people will just have to do a
little work to fine-tune them and
integrate them with with other
applications but that's that's all about
business and and Industry and hype
Cycles which is a little bit off to the
side of the R&amp;D work which is just more
heterogeneous and public perception on
the other hand has evolved in a quite
interesting way because I mean five or
10 years ago most people outside of the
technical AI field who I talked to
thought AGI was indefinitely far in the
future and didn't really want to think
about it and and you know that was more
true 30 years ago than 10 years ago more
through 10 years ago than 5 years ago
but I'm the launch of chat GPT really
altered public impressions in in a in a
huge discontinuous jump right so a lot
of people are like well AGI is Agi is
here already okay it's not exactly like
humans but but but but close enough and
those who don't think that they're
willing to believe it when Elon Musk or
someone says we're having human level
next year right so the public perception
has shifted to a remarkable degree which
is both good and bad right I mean it's
cool it's cool that the goals that many
of us have been working toward for a
long time are now taken seriously on the
other hand it's a bit scary that people
are so naive about something about
something so important and I mean this
could lead to adverse regulations being
put into place because people just don't
understand what's going on are thinking
about it in in very simplistic ways yeah
I'd love to get back to the regulations
actually but just just before we go
there do you think the llms are an
off-ramp um I
think llms will not be the central
component in an AGI system so if you're
spending a lot of
time trying to make an AGI system of
which llms are the the main piece I
think you mostly be wasting your time so
in in that sense yeah I mean it's a py
aphorism llms are an off-ramp on the
path to AGI on on the other hand I think
an llm could be a non-trivial component
of a
multicomponent
AGI system now whether that component
would add 30% or 10% to the intelligence
of the overall system I don't know I
doubt it's 90% but I also think it's
more than like one or two% so I I think
they do
contribute
something significant but but I don't
think they're the whole thing right and
of course I think in a broader sense
working on useful AI
applications is for many people in
offramp on the path to AGI but still
something that does good for the world
right because you many AI researchers
have started out trying to build AGI
then they realize it's long and hard and
you don't know how much progress you're
making and it's hard to get paid for it
but you realize you can make practical
AI applications that make you money and
do good for the world and you just start
doing that right and in that sense llms
are definitely an offramp because I mean
people are doing a lot of cool stuff
with llms and then they're not working
on the fundamental AGI problem but on
the other hand they may be building
amazing applications that enhance
Humanity right yeah but I mean that
there's two schools of thought right so
one is there's a lot more than that
there's a lot there's a lot of school
yeah I'll sketch out my my perspective
on appreciate there's a lot more but
some people say that they are kind of
engram models on steroids and some
people say that they have this emergent
reasoning I'll give you an example I was
reading they do have emergent reasoning
they just don't have enough or the right
kind of emergent reasoning to be a human
level AGI right so I mean like I mean in
Context few shot learning is quite
amazing it's very cool and the llms do
improvise weird stuff that wasn't in in
in in the training data right like you
could you could you could craft prompts
describing an alien civilization with
five Sexes and the love affairs among
these different combinations of of alien
Sexes what's considered cheating or not
among different permutations of alien
polycules right then you can add ask the
llm like what will be considered
unethical to what degree by which of
these alien Sexes right and there's
nothing like that in the training data
you just made it up and it can reason
through that with with great with great
facility right so this is this is not
exactly an engram
model on on the other hand it still
isn't the flexibility and fluidity of
generalization that that that humans can
do right so it's we're not you're not
really
accurate on either side I mean they're
they're not just doing like marov models
and straightforwardly extrapolating data
there's weird surf organization
happening in the activation space of a
large model on the other hand they're
still not able to LEAP beyond their
training data with vaguely the the the
level of of oomph that that human mind
can right and so that's they they are
sort of a new category of of thing that
we didn't anticipate before which is
interesting yeah it's it's so
fascinating to hear you argue that they
that they reason I mean I interviewed
sabaro kahati the other day he had a
paper called chain of thoughtlessness
and and he said rather than teaching
them how to to fish with this kind of
you know prompting you you're actually
teach them how to catch two fish or
three fish or four fish and at some
point it's just limited by The Prompt or
what it can retri from its internal
story it's true it is limited and I mean
at some point humans are limited also
right I mean none of us can deal with
math problems beyond the algorithmic
information content of of of of of our
brain right so we're we're not unlimited
except in the theoretical case where you
give us an infinitely long turing
machine tape to to type on or something
right but I
think yeah there are still fundamental
differences between the way that llms
generalize and create and the way that
people do but but it's it's an
interesting theoretical challenge to pin
that down in a in a in a rigorous way I
mean you can say that a Transformer
neural net is not a turn complete model
in itself but if you give it a sketch
pad to write
on then than it is right so say okay
then then take a Transformer give it a
working memory as a different neural
network then it becomes a turn complete
model but it's still not doing what a
human brain does and and what an AGI
system needs to do so an example I've
often given in the domain of Music
modeling is if you train an llm on music
up to the year 1900 only it's not going
to invent grind core neoclassical metal
or Progressive Jazz I mean it it may do
cool creative things it'll make up new
songs that you never heard before if you
ask it to put West African drumming
together with Western classical music it
may even manage to like set Mozart to a
poly Rhythm or something right but to
get to Jazz and and metal and World
music and all this is just a number of
creative leaps that I really don't think
anything like a current llm is is is
going to is going to be able to take and
certainly
experimentation with udio or you know
any music gen any existing music model
you can see the the Practical
limitations here right it's pretty on
the initially it's it's really cool like
you can pick an artist to make an
arbitrary number of songs in the style
of that artist it gets a voice ride you
can make it competent metal guitar solo
on the other hand it's all Bal music in
the end like you're not getting anything
really awesome out of it even within the
defined genres that it knows let alone
like inventing some new genre of music
or some profoundly new new style right
so I there clearly are limitations which
are more severe than the limitations we
have but it's not quite clear how to
formalize or qualify those those
limitations right now yeah that's a
great example I saw a vision generator
model oh that's
okay I I saw a vision generator model
and they prompted it to generate for
1956 1957 1958 and you could just see
the the morph of the styles for all of
the different years and of course when
it went past 2024 it just ran out of
distribution because there was no trainy
data so it kind of mode collapse but
interestingly if you went forward enough
to about 2070 you started seeing Star
Trek un forms and and so on but but you
know my intuition though is if these
models are learning very abstract
representations of the world then why
wouldn't they I don't think they are oh
that's interesting why not I don't think
they're learning very abstract
representations of the world just from
looking looking
inside what they're doing I I don't I
don't see how they they could be and
when you when you try to use them to
drive mathematical proof which I've
played with quite a lot cuz that's my
original background of a PhD in math
when you try to use them to drive
mathematical proof they mix things up in
very silly basic ways that lead you to
think if they're building an abstract
representation it's not the right one
right like it doesn't represent the
actual mathematical structures and then
in the case of math there sort of is a
correct abstract representation and
they're not getting it right like you
you can in many cases give a proof
sketch
and it we'll fill in the details of your
sketch which is which is is interesting
you can even give it you can like give a
verbal outline of a theorem and it will
turn that into like a formal logic
theorem statement so it it can do quite
a lot of things but then it will it will
mix things up in very very silly ways
which like no no no graduate student
would would ever do right and so it
seems from that example the abstractions
it's Lear learning are really not the
ones that a human mathematician would
would use and that's probably connected
with the fact that I mean in the
automated theorem proving World which we
had represented here by Jose Joseph
Urban from cat tle Institute I mean
using llms to do theorem proving I mean
that's not what they're doing right I I
mean that's not what Google did with
Alpha geometry either right I mean they
used the llm to translate math Olympia
problems and such into formal logic and
and then use different sort of AI system
to to do the actual math right so I
think music is a domain where it's clear
the creativity is limited and it feels
like the internal
representation is not quite the right
one to be profoundly creative but math
is a bit more rigorous so when you see
the sorts of Errors it makes it's really
quite clear that it's it's not getting
the abstraction right even when it can
spit out what the definition is and this
is the frustrating thing we've all seen
like it will it will spit out the
precise definition of say a
non-well-founded set based on Axel any
foundation axum but then when you ask it
to derrive a consequence of that it'll
70% of the time be good 30% of the time
come up with other gibberish where like
if you if you understood that definition
that you just SED you could never come
up with that with that with that
gibberish right so this this suggests
the representations are not really the
right ones I mean theoretically there's
the other possibility it has the right
representations and is bad at selecting
them and sometimes select the wrong ones
but if you dig in it really seems not to
be the case and I think
Transformers are modeling things on the
whole too close to the surface level
right and and they're they're have so
much data
that modeling close to the surface level
can let them go a long way now it's not
as surface level as a marov model but
it's still too surface level to be to be
to be a human level intelligence right
so um we've been advocating for neuros
symbolic models on the show for for many
years and deep mind now are doing neuros
symbolic models so there's you know
Alpha geometry F not surprising proof I
mean as as you know or may know I mean
Shane leg who co-founded deep and worked
for me in the late 90s and we were doing
neural symbolic stuff then right so I
mean they they they knew that was a
thing but Shane and Demus really wanted
to be more biological and so they they
had to bang on the biological fidelity
thing for a while until it just wasn't
doing everything they wanted and then
they're like well okay we will we will
substitute for this brain module a
totally non-biological algorithm that
does sort of the same thing that brain
modu is supposed to right so so which
which is which is fine I mean I mean
it's not at all
surprising yeah I mean could you explain
why it's good my my my kind of intuition
is we need to have some kind of source
of creativity so we we use a language
model to generate lots of ideas and then
we have some kind of post hoc validation
system and there's a virtuous cycle
between the two things oh I think
the cycle you
describe needs to be there to have a
human level general intelligence but I
think llms are not really good at either
the creativity or the validation part
actually they're more like a knowledge
resource that could
be consulted by either of these parts I
agree I mean I would say evolutionary
algorithms
historically in a way are better at
creativity than than llms and if you
look in music generation I would say
work that I did in the 90s on
algorithmic music generation with
genetic
algorithms came out with more creative
Melodies than you get out of udio I mean
also came out with some noise I I mean I
was doing like series of midi notes it
it was it didn't sound like like a whole
band right right out of the box but gas
are in a way better at creativity which
is not surprising they're emulating
EV
evolutionary process so one one
interesting direction of work will be to
use llms to sort of guide and bias what
evolutionary algorithms do because the
llms do have tremendous knowledge of
what's been created by humans so far
which maybe could cut down the amount of
time and memory it takes to use an
evolutionary algorithm to generate to
generate new things now I mean I I do
think logical reasoning
is one way to evaluate creative products
to
see how good they are right I mean of
course not the only way I mean the basic
idea of a Gand is you you you have an
evaluator and a Creator right so I mean
you can have neural Nets do
evaluation also I think symbolic
reasoning
is certainly it's a key part of
what makes human
intelligence different than the
intelligence of say dogs or or or or or
worms or or bunnies or something and
it's it's sort of an open question how
valuable symbolic reasoning is if your
goal was to make an artificial worm or
or even even even a mouse right so Ian
if you go back to basic theory in the
theory of
semiotics a symbol is one of multiple
kinds of of signs right you can have an
icon which which like resembles the
thing that like like an emoticon or
something right the smiley face icon
sort of resembles a smiley face so you
can have indexical yeah you you can you
can you can have an index like when the
Mercury and the thermometer goes up when
the temperature goes up a symbol is a
representation of something that doesn't
have any surface level resemblance to
that thing but it enters into similar
relationships with other symbols
paralleling the relationship that the
represented thing enters into with other
other represented things and you know
humans built these symbol systems and
Mathematics and language are are among
these the parts of a machine are among
these right and obviously these are
super valuable to us and of course
programming languages and and even like
the registers inside a chip are also
also part of of of a symbol system so in
a way AI programs exist in in in in in
in the in the world of of of of symbol
systems now you you could argue that in
evolution the ability to manipulate and
form
symbols emerge from much more primitive
faculties right so then what you should
be doing is
creating virtual animals using Subs
symbolic methods which could be neural
nuts or something totally different but
and then get the facility for symbolism
to pop out from that sub symbolic level
and I think that that's very interesting
it's a great research Direction I Would
love to see it happen I've tried
experiments like that myself with the
same limited
success anyone else
got but then if you're looking at how
can we in
practice make AI systems smart
right now even if not having full
Fidelity to biological evolution I mean
computers are very good at symbol
manipulation right so it's very very
tempting to take explicit symbolic
manipulation and database engines
theorem provs and probabilistic
reasoning
systems take systems for explicit
symbolic manipulation which works so
well on the symbol systems of of our of
our chips and programming languages use
these to do the things that they're good
at and then connect this together with
subsymbolic systems like Transformers
and CNN and Ne neural Nets to do the
things that that they are good at right
and that that certainly from a practical
standpoint it's very tempting from a
conceptual standpoint you could view
that as trying to Arch a kind of mind
that in some ways is better than than
than a human mind I mean we're quite bad
at mathematics right I mean I I was a
math professor for for a number of years
it's quite painful to get the average
University student to understand even
what feels to me like basic stuff on the
other hand then eventually I hit a level
when it's hard for me and I have to
struggle to to to to to figure it out
right so we're not good we're not good
at math we're not we're not good at
science we put a lot of bugs in our
software programs right so I mean by
putting explicit symbol
manipulation at the foundational level
in a cognitive system perhaps you can
make a system that's better at advanced
math science and and Engineering that
than than people are which could be
quite quite beneficial and you could
even look at this from a machine ethics
standpoint like our our ethical
Frameworks are wildly inconsistent and I
mean I'm the same way all humans
basically are that way like when when I
visit Ethiopia where we have a software
development office I see you know
starving people and kids out in the
street people dying of curable diseases
Lying by the side of the road just
walking from the hotel to our software
development office right and then I give
these people money had enough in Seattle
yeah it's not the same level though yeah
yeah but but yeah yeah but anyway I give
I give these people money and then when
I come
home I forget about it after a couple
weeks not really forget about it but I'm
not I'm not sending these people money
I'm I'm trying to
do good for them in in in in various
ways we're injecting money into the
Ethiopian economy by having a software
shop there but I mean in the
end I feel much more for suffering I see
in front of my eyes than for suffering
which is is thousands of miles away
right and this this is is incoherent I
know it's incoherent it's just how I am
as as a human being right it's quite
possible an AGI system which has
probabilistic logical reasoning as more
of a core part of its wiring is more
coherent in its in its ethical ethical
approach to to the to the world right so
I think there's there's pragmatic
reasons to look at neural symbol
systems because symbolic just suits the
compute infrastructure that we have and
then there are more theoretical reasons
in that according to various criteria a
mind with this substantial symbolic
component may just be better at doing
some some important things the challenge
then is how do we make symbolic and sub
symbolic components work work
nicely together right which is one among
the research topics being discussed at
the at the ai24 conference where we're
now sitting could you briefly um discuss
you know in in natural human
intelligence I mean for example Chomsky
says that language is a system of
thought other people think that it's a
system of of communication and I guess
one school of thought is that just like
language is a kind of meta emergentist
organism you you could argue that symbol
processing in of itself is actually an
emergent thing it doesn't necessarily
happen all inside the brain I mean what
do you think about
that I mean it
happens both ways right I mean of course
symbol system seems to have
emerged culturally right I mean language
emerged for
communicating between people about
objects in their in their shared
environment and the same is largely true
of languages of of other animals like uh
birds or other mammals in in as much as
as we as we understand them and then of
course we're making sounds we're we're
we're we're we're we're writing writing
things down right so
certainly certainly language emerged as
part of the group mind of collections of
people and if if you take a wild child
who grew up with no human interaction
during the first few years like the the
developmental processes in their brain
that lead to Advanced linguistic
functionality never get triggered right
so I mean there there is that argument
on the other hand I can lie in bed and I
close my eyes and prove math theorems in
my head right and I mean then then it's
all it's all internalized right I mean
I'm I'm manipulating symbols in in in
the in the theater of my Consciousness I
don't I don't need to write things down
I don't need to talk to anyone else
right so we certainly we come by it
culturally but then I mean
hypothetically I can come up with new
math
concepts in my own mind and create
theorems that that deploy them and
there's no external interactions in
involved right so I mean it's it's it's
a bit of both then it's internal and
external it's come up a few times on the
show because there a thought experiment
that you're in a hermetically sealed
chamber and would you still be able to
kind of think rationally and
and of course youan I've been in a
sensory deprivation
tank you I you you people will tend to
drift into wild hallucinations on the
other
hand if for some reason you try not to I
didn't have a problem summoning a
coherent reasonable train of of of of
thought it's kind of a waste of your
time in the deprivation tank I mean then
Steven Hawkins given as an example but
but if he grew up phys physically
embodied in the and you know exposed to
all these external processes then of
course the brain can simulate them again
afterwards Oh you mean what they well a
human
brain in its actual form has certain
triggers for development so that if
these external environment triggers
don't pop up the next stage of
development just won't be released right
but that that's doesn't mean that's an
intrinsic aspect of any human level
intelligence right that's just how
developmental neurology uh Evol evolved
because we didn't evolve to be robust
with respect to spending our childhood
in a deprivation tank right I mean
Evolution had no reason to it had no
reason to optimize us us for that so I
mean from but I I I would
say this relates somewhat
to an issue we've hit in
creating
opencog hyperon which is my own team's
attempt to build build an an AGI system
I mean we give a hyperon system goals
and it spends a certain amount of its
energy trying to
achieve its goals based on the context
that it's in it spends a certain amount
of energy just doing sort of ambient
background processing not attached to to
to any goal and I think that balance
that balance is is important like I I
think if you are just narrowly pursuing
certain
goals you're probably not going to
generate a rich enough pool of of sort
of patterns in in your mind to be a
robust sort of open-ended intelligence
that can do with all the wide stuff the
world the world throws at you cuz what
you learn is always going to be too
overfit to to the precise way your goal
was formulated like even if your goal is
learning new things you know there's
some math formula somewhere just
defining what what's a new thing and
then then that becomes somehow somehow
constraining so if you imagine a hyperon
system in a vet or
something the question will be what are
its goals right like if there's no goals
and no goal Pursuit you sort of have a
complex self-organizing system it's more
like a primordial soup or something
right and maybe maybe some mind with its
own goals would self-organize and bubble
up out of that just like organisms
bubbled up out out out of the primordial
soup right but then then then that will
be interesting because then the
environment of that thing would be the
rest of the primordial soup right it
wouldn't be the whole mind that that was
a purpose of sort of acutely conscious
intelligence it would be that that
little bubble that that emerged out of
it but you could also Imagine
goals that would keep you going even in
the sensory deprivation tank right and I
mean proving math theorems will be one
of them so if you ever read the novel
diaspora by Greg Egan that that's a
great book from a couple decades ago but
it Envision these human mind uploads
there's no superhuman agis in the novel
because that would complicate things but
they're they're human mind uploads
living in computers and satellites they
discover our universe is about to be
annihilated by some Gamay bursts or
something so they find a way to Tunnel
out into the five-dimensional macro
Universe then they go to another after
like a few quadrillion universes down
the road they wound up in some empty
Universe with no way out and then then
the upload in this empty Universe with
no way out just sets about trying to
prove all mathematical theorems in
increasing order of complexity right
because what other goal can it do when
floating in the when floating in the in
the empty universe and it it Engineers
it motivational system to have a really
good time while doing this right so I
mean the thing is that's not a human
mind right human human motivation system
is not like that we would most humans
would go nuts in that context but that
doesn't mean you couldn't engineer an
AGI that could Supply itself with a goal
system a motivational system so we would
be perfectly happy just like exploring
the Realms of mathematics while while
floating in a vet right and this sort of
highlights the generality the notion of
general intelligence right because
building a human like
AGI is in a way a very very limited
goal but but certainly an important one
because if we want Minds that can help
us and relate to us and empathize with
us I mean the most straightforward way
is is is to make Minds that are somewhat
humanlike yes I I just wondered how
reductionistic are goals because they're
are anthrop morphic explanation for a
very complex world and if we if we
design agents with explicit goals what
do we
lose well I think you don't want to
wire fixed goals into a
system in a way that it can't change
them I think even if you try to do that
it will lead
to various pathologies like the system
will just try to hack around the the
goals that you tried to hard hard wire
into it so I mean I think the set of
goals that a mind
has shouldn't be something that stands
apart from the rest of the mind it's
something that coevolves with with with
with with with the rest of the mind
right and it's it's just part of the
structure of the mind but but I I do
think the the process of trying to
figure out what actions will achieve a
given goal and it it in a given
context sort of sculpts the Mind Network
in a in a in an interesting way right
there's some relation to a talk we had
this morning at at the conference which
was called is complexity and illusion
and basically what what was being argued
argued in in in the talk was that
there's not really an objective measure
of what it means to have a simple
explanation of of something so you you
can't really say that you know there's
an objective measure of Simplicity and
then according to learning theory the
simpler explanations of your data will
generalize better when you look at the
math underlying this it becomes a bit
more more
complicated I think
the the right way to look at it is that
each mind sort of creates its own
measures and understandings of
Simplicity and complexity but then
following the discipline of looking for
simpler explanation
following the discipline of looking for
simpler explanations is an important
urtic for sculpting your mind Network
right I mean just like following the
discipline of conceiving goals and
pursuing them is is is an important
urtic for sculpting sculpting your your
your mind Network now I wouldn't
say it's a necessary one it's a very
humanlike one though and I mean it's a
probably something you want in there if
you're building a humanlike mind now
people with a sort of reinforcement
learning approach to AGI will say
specify the reward function then the
whole purpose of that AI system is to
figure out how to maximize expected
reward according to that reward function
right and I
don't I don't think that's a terribly
good way good way to do things that that
leads to a variety of different
pathologies but having an assemblage of
goals which can be tweaked and evolved
by the system itself as part of its
self-organizing mental activity that's a
different proposition than hardwiring a
fixed external reward
function do you get vagueness on the
boundary with goals I mean is it better
to have a million overlapping goals or
is it better to have one abstract goal
um I mean we don't know but the the
approach that we're taking in practice
is more like uh half dozen goals at
varying levels of of of abstractness
right so there I
mean there can be very nitty-gritty
goals like for a robot the goal is to
stay powered on a lot of the time and
not break right and don't hurt any
person then you can have more abstract
goals
like learn learn learn new things right
and and uh be surprised every now and
then
then so you can have you can have a mix
of very concrete sort of survival and
physical kindness oriented goals and and
then more abstract sort of information
theoretic goals and it's seems like the
interplay and balancing between
those seems like that helps you to
get a
humanik mind's network but that's a I
mean that's still very experimental that
that at this point I would say tell me
about your new book on
Consciousness yeah so this book is
called the Consciousness explosion and
uh I co-authored it with my good friend
and colleague uh Gabriel
Axel Montes and I wrote this book to try
to answer a bunch of the
questions that people were constantly
asking me both uh people in the AI field
but more so people in the business world
and and and and and the general public
just what is going on with the field of
AI where is it going what is this going
to mean for the for the human species
like do do do do we have do we have any
future if we do what's if we do like
what can we do to bring about a positive
future if there's going to be other
kinds of Minds that are posum what what
what what are they going to be like
what's going to happen to the world you
know in the transition between here and
and the potential future where we have
super agis roaming the the Earth in the
solar system right
so given the depth and complexity of
these questions it's not surprising it
turned out to be a fairly a fairly fat
volume it was was supposed to be
supposed to be a lot smaller but
the the theme I would say is fairly
similar to
Ray K's books the age of spiritual
machines The Singularity is near and
this new one The Singularity is
near but
my my take is consistent with his but in
some
ways has a different
emphasis among other things I wanted to
focus a little less on the
Technologies and a little more on the
states of mind mind and and
Consciousness that they're that they're
leading to which is just a sort of
complimentary way way way of looking at
things right because you I mean if you
look at the evolution of mind on Earth
that way I mean animals besides humans
maybe besides dolphins or whales that we
don't understand too well but buying
large animals seem to have a state of
mind that's very much in the here and
now right I mean of course a dog can
remember someone who kicked it 10 years
ago or something so they they they
certainly have long-term memory but by
and large it would appear their state of
consciousness is absorbed with the their
goals at the present moment or enduring
what's happening in in in the present
moment very young children tend to be
that that way also right and that can be
a blissful State of Consciousness but
you know we've evolved as humans and
adult humans into a richer variety of
conscious States right so we we can have
a focused task oriented State of
Consciousness sometimes even a flow
State we can have a sort of default
resting state of consciousness where
we're just drifting from one one one
idea to another or I mean we can we can
trip out we can get stoned we can be
possessed with rage we have a certain
collection of conscious states that that
we can get into we can also now and then
have like a profound I thou connection
where we feel like we're like sync in
with the mind of of of of some other
person but if you if you think about the
scope of possible states of experience
we're probably
exploring a very limited little region
of the collection of of all states of
experience I mean think about the fact
our short-term
memory contains maybe seven plus or
minus two items right I mean I think if
if I'm really deep into coding something
I might keep a couple dozen items in
short-term memory of like what variables
are where and what files but even though
I'm pretty clever I can't keep like 200
items in my my short-term memory maybe
some autistic genius can right but they
can't keep a million in their short in
their short-term memory so but imagine
if you could there's no reason an AGI
mind couldn't keep a 100,000 or a
million items in its shortterm memory
like what what sort of State of
Consciousness would that be on on the
other hand what if we could wire or
Wi-Fi our brains together so we felt
each other's brain State you know
palpably as if as if it was our own like
what what what would that feel like
right so there's the E so even just by
hacking little features of the human
brain or mind you can imagine wildly
different conscious States but then
imagine what if your architecture didn't
have to be rooted in in in the human
whatsoever so it would seem like we're
nearing a point
of
proliferation into a huge variety of
different kinds of mind and and kinds of
of conscious experience which is is
quite interesting and and you can go
very far with that like what what what
is it like to be a quantum computer
right I mean it maybe the brain
leverages Quantum Dynamics but if so
it's in a fairly wimped out way because
we're clearly not that good at doing
many of the things that that quam
computers should should should be able
to do right but but what what would it
be like to be like a full-on Quantum
touring machine right so that that sort
of brings you to the question if we're
moving toward this opening up into a
huge diversity of states of
Consciousness like
what could we be doing now to better
prepare for this and to bring it about
in the in in an optimal way and you
conclude well maybe humans should be
expanding the scope of the states of
Consciousness that that we are in and
sharing States Of
Consciousness with machines in in in in
a rich variety of ways and that that
sort of brings you back
to the terrifying lameness of the
commercial AI world where AI is mainly
being deployed I I used to say for
selling killing spying and crooked
gambling now I've I've added
plagiarizing to the list since since we
have llms right but oh yes I mean by and
large the applications of AI right now
are not ones that lend themselves to a
sort of deep like I thou bond between
men and machine right they're they're
more using AIS as a proxy for some
company that's trying to extract money
from people and objectifying the person
and this is not because there's evil
people building AIS or the AIS are evil
or anything it's just that's the nature
of the economy right the
organizations that gathered the money to
train large models and hire all the AI
researchers I mean these are ones that
are making money by trying to scale up
models that treat people as objects or
or products right and that that probably
is not the right sort of human AI mind
meld that we want to lead to a positive
proliferation of amazing States Of
Consciousness right and then then that
that brings up the whole question of
like what do we do to make the global AI
ecosystem
more
participatory and sort of more positive
and and and up up uplifting to people
which which brings us to AI regulation
which seems quite inept and incompetent
to bring about these positive incomes
these positive uh outcomes that that
that that that we
want yeah well that was a good segue so
let's talk about the um the state of AI
regulation so you know we've um got the
the the recent um uh announcement down
in California but we've also got the the
executive order and the EU AI acts what
do what do you think about
it it's pretty scary and disturbing over
over overall I
mean you know having spent 10 years
living in Hong Kong and a bunch of time
in mainland China because my wife is a
Mainland Chinese AI researcher I mean we
live in the US now
but I sort of feel like the Chinese way
of regulating AI has some advantages
which are interesting
because China is less rule of law
oriented which has big disadvantages on
the other hand it means they can be
agile and adaptive right and so they can
they can just handle things in a caseby
casee basis in a flexible way they don't
need to try to write down in in all
these little Clauses and Sub Sub sub
Clauses exactly how to deal with
everything and in a way that informal
approach could deal better with
something that's so fluid and fuzzy
and and rapidly changing I mean
unfortunately they're deploying this
fluid and fuzzy
approach in the interest of a very
topown hierarchical control system which
is not not my not my sort of thing right
what we see in the west is people are
trying
to legislate in a very rigorous and and
and detailed way something that's just
too complex slippery and rapidly
changing to really be captured in in in
in this sort of of legal codification
and I think attempts to do
so so far seem likely to
cause cause more harm than good right
like these these proposed laws in
California are just I mean they're
insanely stupid to the point that you
can't believe people are seriously
proposing stuff like this right like
take any model above a certain size
which is a meaningless measure anyway
which an expert could modify to do
something bad I mean what where does
this this even come from right like if
you appli that to other varieties of
Technology almost all physical objects
will become illegal CU there I mean
almost any physical object could be
modified by an expert to be something
dangerous but then then they say if any
small model is equivalent Behavior to a
large model it should be covered but you
could take a small model and Pat it with
nonsense so that it became equivalent to
a large model right so you can you can
just see when you dig into the details
this whole thing makes no sense on many
many different levels and it's hard to
believe it's gotten as as as far as it
has the EU rules are not actually as
stupid they clearly were were better
thought out on the other hand they're
also overly restrictive to to the extent
that it's actually going to block people
from doing a bunch of of important AI
things in in in in in the EU right so I
mean I'm uh this the whole mess of
wouldbe AI regulation in the last year
or so has giving me new respect for
the general
minimalism of the US government system
actually I mean there's a lot of
terrible things in US system like the US
Health Care system is is absurd like
after living in Australia New Zealand
Hong Kong with single pair Health Care
like it works very well doesn't doesn't
cost much doctors are great on the other
hand the basic character of the US where
it's very hard to get laws passed and
then then even when you they want it
often different in many different states
I mean this messy way of doing
things may well be for the better for
for AI be because it will mean that it
will mean that AI does not wind up
overregulation in in Stupid Ways and of
course sitting here in Seattle if
California really puts into place
idiotic and repressive laws pretty good
for Seattle like we're we're going to
see a lot more AI companies moving here
right and this this is what we saw with
stem cell research way back when George
W bush passed federal laws like Banning
most research using human stem cells
well the states of California and New
Jersey rapidly put billions of dollars
into the stem cell research in in in in
those States right and I mean before
long there was a new president who who
didn't didn't put in impressive laws
about about this anyway so I
guess I have some faith in in the mix of
libertarianism and dysfunction in the US
government that that we may not get like
overall oppressive regulations here
which is good and I mean this is us is
still For Better or Worse the the global
leader in in in a AI R&amp;D right so I mean
I think government's ham-handed attempts
to over regulate AI may end up being too
halfhazard and slow to actually mess
with things too much as we progress
rapidly toward a toward a technological
singularity but but I don't know like if
so if we're 15 years from Human level
AGI then The Regulators may find time to
screw it up before we get there but if
we're like five or seven years from
Human level AGI I think we'll just get
there before the before The Regulators
could can mess it up too badly so this
is a yeah to be clear I'm not like a
pure
dogmatic political libertarian or
something right like it's clear that
it's clear that
regulation plays a positive role in in
many areas I just think when you
have Innovation happening
rapidly Regulators have a very hard time
keeping up you know I'm active in the
cryptocurrency space as well as you know
so I I'm mean I lead Singularity net
which is the I think the first large
scale blockchainbased a AI platform and
attempts to regulate
cryptocurrency are being roughly equally
intelligent as attempts to regulate
regulate AI right like in both cases you
have something new rapidly changing
doesn't quite fit into existing laws
worries some people and then there's
just really silly attempts to to to
regulate that end up not actually
protecting the people that need
protecting we're just standing in the
way of interesting and beneficial things
happening no I think if if we had a
rational benevolent World
Government I think you could regulate
AGI development in a manner that that
would be
helpful it's just very far from being
the case right now like we can't even
control nuclear materials or restrain
ourselves from starting stupid Wars like
blow each other up on various border
clashes right so things that are much
much simpler than regulating AGI are
like a hard fail for our current current
global global governance systems yeah I
mean I can see it both ways I mean we
had Sarah hooker on I don't know if you
read her book about the um the flops
limit and in in Europe the flops limit
is even more restrictive it's 10 to the
25 it's 10 to the 26 in in the US but
I've also just read Gary Marcus's new
book on the plan over and he gives some
really interesting examples of you know
what happened with cigarettes and what
happened with social media and you know
there are a lot of really serious harms
of of this technology that that perhaps
we're completely blind to so it's it's
it's a really I'm sure there are serious
harms on the other
hand we're just dealing with something
quite
complex and rapidly evolving and I think
the IQ level of our regulatory systems
is just not remotely up to the task
of regulating these things in a way that
does more good than harm I mean
something like the effect of cigarette
smoking or eating refined
sugar on the human body or you know
having fishable materials around for
making nuclear
weapons these are just much much simpler
things to deal with right I mean the the
bad effects are very clear what to do
about them is is is very very clear and
the benefits are much weaker and much
less less multi multi-dimensional and
they're not changing radically every six
months even in those
cases it took quite some time to to to
put reasonable controls into place in a
manner that wasn't wasn't overly
oppressive but still did did the job
right so it's not that as a matter of
principle I think AI can't or shouldn't
be regulated it's more looking
at how badly we're able to deal with
much simpler things yeah my judgment is
attempts to regulate are probably going
to do more more more more harm than good
I've got an interesting segue so um
there was an article I think it was a
cartoon in the economist about The
Bletchley Park meeting and a whole lot
of people went there and they said the
AI is going to paperclip us and the car
had you know Europe and the States and
China you know kind of talking about how
dangerous AI is but they all wanted to
build it as fast as possible but this
kind of brings in the alignment problem
a little bit as well because people use
that as as um a way of talking about how
dangerous it is do you think we have an
alignment
problem
um I think the alignment problem is that
humanity is not well aligned with
itself and that's the main alignment
problem
what do you mean by that well so if you
look at even llama 3.1 or chat GPT or
the latest mistro models or something I
mean you can give them ethical
puzzles and ask them what the average
good-hearted intelligent person would
say is the ethical thing in this or that
situation they will perform at human
level or above the average human at
doing that and my my my son zuster and I
he's also an AR researcher I mean we
published a paper on that is that your
son is he your son yeah he my son he has
a Twitter he does he's he's he's giving
a poster presentation here also yeah no
way can you send him our love because he
he um write reposts pretty much video
Zar yeah yeah that that is my oldest son
wow I need to go and shake his hand he's
here he's he's yeah he thank you for
posting our
videos he's got a PhD in application of
machine learning to theor improving
studying under Joseph herban who who
gave gave a talk here on this morning on
theing right son wow I I I suspect he'll
be reposting this one yeah yeah yeah
yeah yeah he's the only one of my three
adult kids who's gone into AI so far now
my my six-year-old son is interested in
AI the question is whether there will be
any AI R&amp;D left to do by the time he he
gets old enough to do it right but
anyway Zar and I and our colleague
Sergey rodionov who's here at the
conference also but we we we tried this
and I mean what you find is llms while
they're not General
intelligences I mean they already
understand what humans will think is
ethical in this or that situation and
you can even give them out their
situations about Al aliens or weird
technologies have never been invented I
mean they they extrapolate human ethos
to these situations so far as I could
tell as well as as people will do and
when you apply them against existing
corpora of ethical judgment they do they
do at at the human level right so in
that sense we already don't have a
problem with agis being able to know
what humans think they should do in in
in a given situation now you may say as
the world evolves to a fundamentally
unprecedented level we don't know that
as will generalize there the same way
that people do and that's true we don't
know how we will generalize today either
or if we'll generalize the same way as
people from a different culture so
you're not abolishing all those unknowns
but I I would just say like taking a
smart AGI
system giving it a top level goal of
don't do stuff that the average ethical
intelligent human would think is bad
that's not hard right now right llms
interestingly they G they gave us that
they they gave us like a human ethics
Oracle right among among many other
interesting things so then the alignment
problem really
is is that the kind of AI system that
people are going to build right and this
is what I mean by people aren't aligned
with each other like I mean the US
military Chinese military Russian
military Iranian military whatever what
go system will they actually give their
their their AGI system what go system
will Google or Microsoft actually
actually give the their their AGI system
right so I mean what will probably
happen is lip service to General ethics
with much more banging on the on the
engineering for achieving the proximal
goals of the of those organizations
right and sort of like G greenwashing
right I mean you you won't have these
organizations outright saying our goal
is to make more money for our
shareholders human life be damned right
but but you will you will see some lip
service paid to Broad human benefit and
a lot of tight engineering on how to
maximize profit or maximize hemony for
this or that government right and then
then you'll have a system of multiple
agis that are all paying lip service to
human good but much better tuned for
serving the purposes of this or that
organization you'll have these guys all
all interacting with each other and
competing in some some complex way and
you know you could argue that is aligned
with what humanity is doing because
that's what we're doing right now like
we have different organizations that are
all all struggling with each other but
it's not necessarily leading to our our
our broad benefit either right so in a
way I
think focusing on the
possibility of agis that start out
aligned with human goals then once they
get a little Smarter Than People
mysteriously rewrite their goal system
to kill all people I mean this seems
like a remote possibility and I don't
see why it would happen focusing on that
rather than the very obvious problem of
the organizations now hiring all the AI
researchers and spending all the money
on AI creating AIS that care more about
their selfish goals and about the good
of humanity it seems it seems like a a
misdirection of of attention which I
believe is intentional on the part of
some parties and and unintentional on
the part of other parties who are just
kind of kind of kind of going along for
the ride now you can't rule out of
course that theyi were one of the top
level goals is don't do stuff that
people will think are is evil you can't
rule out that somehow it might self
organized into a condition that that
causes it to delete that goal and
replace it would turn turn everyone into
paper clips but it's just no one has
given me a reason why that has more than
a vanishingly small probability of of of
of of happening right so I don't I don't
take it all all that seriously I mean it
of course we can do experiments in
various sandboxes aimed at exploring
whether I'm somehow wrong and there's
there's some reason that Mal olence
would spontaneously self-organize out of
a benevolent benevolent system but for
what it's worth it's not what we see in
human or animals very often right like
you you don't very often see a person
who's loving beneficial rationally
ethical suddenly turn and decide they
want to kill kill everyone right I mean
you don't see that in human
organizations very often either like
there there's not much reason to think
that this is a big problem and there are
obvious glaring problems out there
staring Us in the face so then when
representatives of the organizations
that pose the obvious glaring problem
try to redirect your attention to some
Weird Science fictional problem that
seems to have a vanishingly small chance
of eventuating then it it m it makes the
the paranoid mind become a little
suspicious have have you changed your
mind about any you know serious things
over the last five years five years
yeah um
well not things that are foundational to
the pursuit of AGI I would I would say
certainly current llms are more
functional than I would have guessed a
large scale Transformer net W would
would become but I wouldn't say that
utterly shocked me
because no one ask me that question so I
can't give an honest measure of it but I
mean I think if you would ask me five or
10 years ago like what would happen if
you were able to feed all the knowledge
into the world into some giant
probalistic NE next token
predictor I mean if you follow that
thought experiment it could be could be
quite interesting right I mean Marcus
hudder had proposed a long a long time
ago that text compression will be a good
measure of general intelligence and the
best text compression algorithms have
been these uh sort of probalistic marov
algorithms try to build problemistic
models and and predict predict based on
them right but
so I don't think that would have shocked
the me of five or 10 years ago but it's
still surprising I mean the degree to
which like few shot learning or in
context learning actually works is is
cool like I I didn't Envision that this
sort of learning would happen purely in
the activation space of some some back
propagation train Network without even
even even changing the weight so it's
it's it's interesting on the other hand
the limitations of these systems are
exactly what I would have told you they
would be like you would see that if
you're training a sequence predictor on
a huge amount of data without much
recurrence in the network of course it's
going to overfit to the to the training
data not be able to take big creative
leaps of of of generalization like the
functionality is more than I would have
thought the limitations are ex exactly
exactly what what I what I would have
thought so that's uh but in terms of the
big picture I mean Ray kwell forecast
human level AGI in
2029 in the singularities near which is
published in
2005 and you know what's happening now
is very much
aligned with with his timeline which is
sort of interesting because he was he
was doing it based on somewhat indirect
measures like Mor Mor's law and the
amount of RAM and and the accuracy of
brain brain scanning the size of the
smallest motor and so on so he was just
looking at the enabling Technologies and
the exponential Improvement in the
enabling Technologies for AGI then he
was figuring as the enabling
Technologies approach the level of
apparently enable a human-like mind
people will scramble around and figure
out how to make cool software that that
leverage these enabling Technologies and
pretty much that's what's happened right
I mean I think to my mind we're roughly
on track to human level AGI by 2029 as
Rey had forecast and the emergence of
Transformers is not that far off from
what Ray thought actually I mean he he
he thought by five or six years ago
everyone would be using voice on their
phones instead of of typing things in he
thought we'd all have a supercomputer in
in our pocket he forecast that in the
90s right so seems like we're kind of
roughly on track right I mean I I won't
be surprised if we get human level
AGI in 2027 instead of 29 I won't be
shocked if it's 2033 or something really
I'll be surprised if it's 2050 though so
I I mean I feel like we're so whatat on
track with how I thought things were
going to unfold although the exact
unfolding of this or that technology is
different like I I would have thought
humanoid robotics would be a little more
advanced now than it is on the other
hand that's partly a matter of money
right like not not not that much money
has gone into social robots or robots
for sort of non non non-industrial
applications and that that's always the
hard thing about forecasting technology
like from the science and math you can
see what's possible and even what's
really hard are only kind of hard but
then what will happen on what time frame
largely depends on how much money is
thrown at something and which things get
a bunch of cultural hype about it right
and that that's that's a little harder
to foresee than which technology
pathways are are are feasible you know
the the big disagreement I have with Rey
is I think after you get human level AGI
I think you're only a few years from
Super
intelligence and he he he said human
level AGI 2029 Singularity
2045 but I think his curve fitting is
overfit to having humans do the
invention like I think once the AGI is
doing the invention the exponent in the
exponential growth curve becomes bigger
and I think it only will be a few years
from a human level AGI to a a super AGI
because that human level AGI will be
able to program and and invent new new
chips and invent new forms of networking
and so forth it seems like it should be
able to to upgrade itself pretty pretty
rapidly I mean the future is very
elusive I mean what I mean just in very
simple terms what makes you think
2029 well there are three sort of
converging lines of evidence all of
which are quite uncertain right I mean
the first is simply the extrapolation
that Rey and John smart and other
futurists have done where you're and
they've been doing this for decades
where you you you you plot the rate of
advance of various Technologies and of
course the progress in like socks and
shirts and and light bulbs is not
exponential but the progress in many
Technologies relevant to the achievement
of AGI is clearly fitting an exponential
curve and the slope starts to approach
Infinity around 29 or 2030 know that
that doesn't mean the slope will
literally become infinite and will enter
the Omega point but it it means things
may keep getting getting faster and
faster and faster and that is what seems
to be happening so that that's one line
of evidence and I'd encourage everyone
to look at Ray Kell's new book The
Singularity is nearer or some of the
earlier chapters of my book
consciousness explosion and they present
this argument now llms themselves are
another interesting piece piece of
evidence I mean they they almost passed
the Turing test not quite if you conru
it rigorously but spiritually they
passed the Turning test anyway and you
know they can they can write python code
they can illustrate your book for you
like just on a naive level The observed
functionality of AI systems is amazing
compared to a few years ago right I mean
my my Tesla can drive itself around
around vashan Island where where I live
right it I I wouldn't trust it in New
York traffic at the moment but still I
mean I mean the observed capability of
AI systems is escalating tremendously
now I don't draw from that the naive
conclusion that these exact
technologies that are realizing the most
advanced functions now are going to be
what gives us AGI but still I mean it is
it's a thing you can't deny right and
then the the third line of evidence is
progress in in my own AI work on on the
opencog hyp hyperon system and I mean we
we had what I think is a viable design
for human level AGI in the 2012 which
was published in my book engineering
General intelligence in
2014 but we didn't have the ability to
implement it at the scale that that we
need now one lesson you can draw From
Success with llms and other Associated
deep neural nut models is you can take
some fairly old AI Technologies deploy
them at massively greater scale and they
start kicking ass right and I mean
multi-level perceptrons where they're in
the late 60s lstms from Jurgen Schmid
were there in 2016 Transformers are a
bit different you replace some
recurrence with with attention not that
different from an lstm they're scaled up
tremendously they they they work so much
better right so one lesson one could
draw is hey take some old AI algorithms
that seem to make sense deploy them at
massively greater scale and maybe
they'll start doing what their creators
always envisioned all along right so my
thought
is we take the opencog AGI architecture
which combines deep neural Nets with
logical reasoning evolutionary learning
and some different ingredients from the
historical AI literature right we we
take the opencog architecture we deploy
it at massive scale which is enabled by
the new opencog hyperon infrastructure
which we talked about a lot in the
hyperon workshop on day one of the ai24
conference we deploy it at huge scale
across many different machines you know
we can make that decentralized using
single Singularity Nets blockchain
infrastructure and then you know maybe
it leaps in functionality in the same
way that we saw between Bert and and gp4
right and it seems quite plausible to me
and if that happens it's probably going
to be on the similar time scale right so
like we're we have our new version of
opencog in Alpha we should have it
scaled up and working really fast by
sometime early next year then you figure
okay it's maybe a year or two to put our
whole of architecture in this in this
scaled up infrastructure well that
brings us 25 26 right so I mean there's
always some fudge in estimating any
advanced technology development but I
can see like
if if our hypothesis about our own
hyperon AGI architecture works it would
be working on the time scale of the next
three to five three to five years or
something so I can I can look at rays
and others large scale futures
prognostications we can look at what AI
systems are now doing and we can look at
the progress of work on AGI
architectures and all of these seem to
be pointing to well could be a few years
from now right and that that that's
quite quite interest quite interesting
to me that that these three lines of
thinking are sort of converging to each
other yeah so so Ben if I understand
correctly you you're a transhumanist I'm
a transhumanist and we I mean we've
spoken about it a few times on the show
I mean for example Arena R came on and I
was horrified by the comments it turns
out people on the left really don't like
transhumanism yeah where I'm I'm pretty
much a an Archos socialist I'm pretty
far on the left politically myself oh
interesting could you steal man why why
people don't like it um one complaint
people on the left have
is you know 60% of Ethiopian kids are
now brain stunted due to malnutrition
right there's there's a lot of bad
problems in the world right now why
should we be spending our resources on
making a few rich people in Silicon
Valley into superhuman Immortal cyborgs
while billions of people are starving
and dying dying of curable diseases like
let's bring everyone up to a decent
modern level of of health and happiness
and then worry about uplifting everyone
together to to to to to a higher level
right and that's that's a quite common
argument that that I hear and I just
uh I empathize with it
emotionally I just think it's not the
Dilemma we actually have right I mean in
in actual
practice most of the world's resources
are not being spent either on better
educ educating rural Ethiopian parents
about nutrition and distributing food
there nor on Cutting Edge research like
like AGI in life extension and Nano and
nanot Technology instead we're spending
resources on blowing each other up
defending against getting blown up
making chocolate or tasting chocolates
and and vaping and and so forth right so
I just feel like if we're worried about
optimizing Humanity's allocation of
resources
let's first focus on moving things
to things of broad human value away from
things that are actually of of negative
value for for Humanity rather than
arguing about prioritization among am
among among valuable things right yeah I
mean could could I have AG go at still
Manning it I I think first of all they
don't like the market system and
secondly they don't like any kind of
value judgment against capability or
ability
so I think they would worry that folks
who have access you know might um expand
their capabilities and and that that's
about I think a lot of the issue has to
do with the
specific organizations and cultures that
are pushing transhumanism forward and
sort of the the the the way the way it's
being presented and I would imagine a
lot of people on the left wing who are
anti-trans humanist would not be so
opposed to the transhuman vision I
present in my book The Consciousness
explosion for for example so I mean I
don't tend to think about transhumanism
as about becoming better becoming the
nian Superman or something I'm I'm the
big bad Superman looking down on all the
petty little little Mortals or something
like you you don't have to view it that
way you could you could frame it as Inc
in diversity if you if you want to right
like you're like diversity is great we
have so many different cultures on the
earth we have so many different species
on the earth who who are thinking in
different ways wouldn't it be cool to
have a billion times more more diversity
of different cultures and species like
why would you want to squash this
diversity that's on on the verge of of
of of flourishing right but this is just
not the way that it's normally framed
and th this has to do with just the
specific ific cultures that have been
historically preaching transhumanism
rather than with the the content of the
of the transhuman vision itself and I
mean what do transhuman have to do with
capitalism in fact super AGI is going to
obsolete capitalism right I mean this
was this was a lesson from Ian Banks
culture novels and science fiction like
one of the marks of a very primitive
civilization is they still rely on money
right like that means they they haven't
reached the level of abundance that
people everyday needs are just met just
like water spreading out of the water
fountain or or something right so I mean
transhumanism is really anti- capitalist
and people who have a motivation of
making more money than others should
should be afraid of it because after you
have super agis benevolently
coordinating
things there won't necessarily be a way
for anyone to make way more money than
than than than other people but it
shouldn't matter if there's an economy
of abundance where people can get every
everything that that that they want that
they want anyway so on the whole I think
the transhumanist community combined
with Hollywood has done a terrible
marketing job for transhuman ideals
what's interesting is that in Asia it's
quite different and in in Asia most
young people are quite favorable toward
toward transhumanism so in Japan or
China or Korea most Young people seem to
assume once we have superhuman robots
they will be our friend and help us in
the US more people assume they're going
to kill us all so I mean what what does
this tell us about the different
cultures is it just CU Americans think
if they got superhuman powers of course
what they immediately would do is try to
conquer the Earth whereas whereas Asians
think if I got superhuman Powers I could
help everyone to have a better life I
mean I don't know but it it seems to say
more about specific human cultures than
about the actual content right but you
said something interesting you you're
talking about you know there will be an
era of abundance and almost utopian
because the the AGI will do everything
for us so it sounds a little bit like
Marxism with machines that you know that
the machine would just do all of all
Mark foresaw that Marx wrote anay about
what happens when machines do all the
work and and and human labor has has no
value actually so I mean it wasn't the
centerpiece of of his thinking but he
thought that was a fascinating thought
experiment anyway oh interesting but do
do you think that analogy is does does
that fit the does it sound a bit utopian
basically I mean people will just say it
sounds a little bit kind of I mean I
mean of course to people from thousands
of years
ago the Safeway supermarket would sound
a bit utopian like you have amazing
amounts of food from from all over the
planet you can buy it and bring it bring
it to your house and I mean airplanes
would seem like magic right like it
takes a day to go to go around to go
around the world instead of six months
in a boat living on like salty biscuits
or something right so I mean I mean this
goes back to Arthur C Clark any
sufficiently advanced technology is is
indistinguishable from from Magic right
so I mean I think in a material sense
the average person's life right now in
the developed world is vastly better
than kings and queens from from from
from the Middle Ages didn't have heat or
or air conditioning or basic basic
hygiene had very limited variety of food
and and so forth right certainly didn't
have video games or or the internet so I
mean I don't I don't think seeming
utopian in many respects is really a
count a
counterargument I think you could get
overly utopian about it if you're going
to say there will be no problems and
everything will be perfect I mean I'm
sure that won't be the case there will
be new problems
that we can't imagine now know any any
more than people in the year 1200 could
Envision like uh spam on the Internet or
the the the problems that or how
uncomfortable airplane seats are and
terrible airplane food right so I mean
the there's probably going to be things
that annoy postum intelligences and post
Singularity humans and and superminds
and we we cannot forecast what those
will be right now but but it does seem
that material
scarcity at the level of what humans or
humanlike robots need for everyday life
should be cured by molecular
nanotechnology death involuntary death
should be basically curable by molecular
nanotech
repairing the body I mean mental illness
should basically be curable by letting
people rewire their their their their
brains at will once Neuroscience is
solved so a lot of the problems seem
intractable to us now rationally seem
like they should be solvable by
technology so then if you have an AGI
that's five times as clever as the
smartest human it seems quite plausible
they could solve these problems to a
fairly thorough degree there of course
will still be limits but I mean like if
if I look at myself relative to my my
dog at home right so I mean in many ways
I've incomp sensibly amazing powers like
I can I can drive out to the store and
bring back a steak right now and I do it
all the time and they're amazed that I
can I can do this they're delighted
every day every day she's delighted of
course she doesn't realize their outlier
conditions where there's like a nuclear
war or a famine then I can't bring that
steak home right so there are problems
be be be beyond the level of of of of of
of of the dog which are limitations but
still still relative to that
intelligence I do have what often pass
for superpowers right so I mean I mean
I'm not saying we're going to be the
pets of super AI there's much more
nuanced than that but that
illustrates I mean they in practice
we'll have superpowers relative to us it
doesn't mean there won't be limitations
that are are weird for us to understand
I mean maybe there's maybe there's an
off chance of some Gamay burst leaking
through white holes and universe and
wiping us all out right I mean there
there's there may well always be always
be some problem and I
think I mean there will certainly be
interesting psychological and cultural
issues for humans in this domain of
abundance interestingly Nick Bostrom who
wrote the book super intelligence which
is what got Silicon Valley scared about
AI killing everyone changed his mind
well I don't know that he
ever had that dogmatic of of a view on
it he's more of a academic philosopher
and thought experimentalist but yeah he
he had assigned a higher probability
weight to AI wiping everyone out and I
think he was trying to get the world to
take some actions to decrease the odds
of that he realized that had failed and
was just totally not going to succeed
like what he was after was getting you
know aler yudkowsky and a small number
of AI Geniuses working in a sealed chain
ber under un supervision and nobody else
was allowed to work on AGI right and
that that isn't what happened right
instead it's
happening not in as decentralized a way
yet as I would like in terms of a huge
community of Open Source developers but
I mean it's happening in many different
countries in the world many different
countries many different government Labs
right and I think he can also see now
it's not immediately drifting toward
malevolence right it's just more complex
what's what's happening
but anyway he's written a a book now on
like how would humans deal with a domain
of abundance like when when people don't
have to work anymore when they when you
don't have to save the world and you
don't have to support your
family how do you find meaning in life
and he's sort of written the
philosophical Treatise musing on this
theme which is is cool I mean my my own
take on that has
been you know for people in my kids
generations and my my oldest son is uh
34 and he's he's giving a paper at the
AGI event here my youngest uh my
daughter is three
so in this age group most people I know
would have no problem finding a purpose
for themselves if they didn't have to
work for a living like most of them
aren't out to save the world I would
rather not have to figure out how to
work for a living and have no shortage
of ways to entertain themselves if if if
they have free time right
and I mean frankly many people my own
age feel that way also like I know a lot
of musicians they would rather just hang
out and play music all day right give
give me free money free electricity free
food all good right we can jam all day
certainly there's a subset of the human
population that would have
trouble sort of reconstituting their
their personal meaning finding in a
world where they didn't need
to have a job or have a
a sort of critical life mission but and
and Nick sort of discusses that in in in
in an interest interesting way but I'm
honestly I'm more worried about the
transitional period between here and
human level AG then I am about what
happens after we get human level AI like
I I we can't know for sure the
conference intervals rationally have to
be quite wide but on the whole feels
like to me we can give rough adherence
to human ethics as a top level goal to
an AGI
system we can let it modify itself we
can keep you know staying roughly in
sync with the growth of humanity as
another top level goal
and that may not have the level of
certainty that some people would like
but I don't see any strong reason why it
why it can't work on the other hand but
I see a lot of mess in the interim
Pathway to to get there
because I
mean what does happen when we have the
first inkling of human level AGI so
let's just say as a thought experiment
say like two years from now the opencog
hyper arm project succeeds and we roll
out something say we roll out a little
Little Robot that's roughly as smart as
a human toddler right and at the same
time we roll out some artificial
scientists that are you know doing drug
Target Discovery and math theorem
proving and math conjecturing you know
at the rough level of a human scientist
right and so we can connect these
together these are running on a
decentralized network with no Central
owner or controller with servers all
over the world
right what happens then right I mean
clearly you get a huge amount of
developer interest right and you get a
bunch of forks of the system you you you
get a bunch of people hacking the code
you also get big companies taking that
code trying to hire up open source
developers with multi-million dollar pay
packages trying to adapt this to serve
their own particular purposes you get
militaries trying to put this behind
Killer Robots right that at the same
time you have billions of dollars from
various sources going in to trying to
make a beneficial decentralized AGI
system before people make make other
sorts in the meantime commercial
Enterprises are eliminating jobs at a
more rapid Pace than is is is is
happening now right now what impact does
this have on the developing world or on
blue collar workers in in in the
developed world is quite hard to foresee
right I mean and I
think you would probably rapidly see
some form of basic income or other
roughly equivalent social welfare come
about in the in the developed world just
to because the only alternative is is
chaos or outright fascism right on the
other hand there's no money in the Congo
or Ethiopia or even say Brazil or
Paraguay to give a decent level of of
basic income but the economic of impact
of AGI will probably
be just as large there after after a
brief lag right so then
then then what happens right what are
the implications for for
geopolitics even if you think everything
will be beautiful once you have an early
stage super
intelligence what if what if that's
three or five years between the first
massively disruptive barely human level
Ai and getting a super intelligence that
can air drop molecular assemblers in
everybody's backyard like what happens
during during those during th during
those five years right I mean that that
that's
a certainly can be Gris for many uh
excellent manga and and Thriller novels
right but it but it living living
through it may be much less entertaining
for for many people this will be solved
by having a rational benevolent World
Government overseeing the operation but
uh well yeah that seems harder to bring
about than the super AGI indeed well um
Dr Ben gson thank you so much for
joining us today it's been a pleasure
yeah yeah yeah thank thanks for having
me and thanks for the the wild diversity
of a of of questions and uh I want to
encourage everyone to go on the YouTube
and look at the talks from the AGI
conference where where we're sitting now
because there there's a lot of other
interesting people saying interesting
things in in the workshops uh Keynotes
and and and invited talks and there's
just quot
diversity of views and research projects
going on in in in the AGI field which uh
I'd like more and more people to be
aware of and next year people should
come next year more people should come
to the AGI event we're not sure where
where it's where it's going to be yet we
do it a different different place
different place each year but yeah I
think as AGI gets closer and closer to a
reality you know this this should
spontaneously grow into a larger and
larger event I I certainly see that
happening also once again we'll
encourage everyone to read my book The
Consciousness explosion you can buy it
on Amazon or on the Bookshop and other
online sellers there's also a free pdf
available for download atthe
Consciousness
explosion. amazing thanks a lot Ben
thank you

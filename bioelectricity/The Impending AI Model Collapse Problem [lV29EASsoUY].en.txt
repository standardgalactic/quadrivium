so this actually has been interesting me
in some time I've been wondering if
anyone was going to write an article
about AI being fed its own effective
data and what happens to it over time so
I am curious what this has to say
training artificial AI models on AI
generated T text quickly leads to the
models churning out nonsense a study has
found okay the by the way the visual
representations is so good look at this
thing go it just gets weirder and
weirder like you can definitely see you
can see the two in here and you just see
them slowly D it's it's turning into an
actual what's his name what The
Impossibles what's the guy's name what
The Incredibles The Incredibles it's
like it's a deep fried meme SL the
incredible meme slowly is this so this
is what a Zoomer is this is a millennial
and this is a Zoomer and the golf
between them is just three generations
of llms and aying on itself nice
this cannibalistic phenomenon termed
model collapse could halt the
Improvement of large language models as
they run out of human derived training
data a and as an increasing amounts of
AI generated Tex uh pervade the internet
what's very interesting about this and
which is part of the reason why I've
been betting so hard against llms not
like not being the end all be all of
everything is this right here which is
that right now how much percentage of
code do you think just last year was
generated by co-pilot I bet you a good
percentage of GitHub public repos are
generated by GitHub like like a shocking
percentage
10%
15% I bet bet you it's a huge amount I
don't see I I think all brand new code
yes but you got to remember that there's
also a lot there's also a lot of Legacy
repos so when you add in the Legacy
repos it's gonna it's going to bring it
down by a lot and so my guess is that
like if it remember if it's 10% 5% of
all public repos like that's pretty
shocking like that's really shocking Now
new repos is probably much higher right
like you like like everyone was saying
75% 50% % all of that and so the dead
internet Theory the the theory
effectively that everything is just Bots
arguing with Bots and all of that I
would not be surprised if chat GPT 5 is
actually harder to make better
comparatively than Chad GPT 4 was to
three I think five to four is going to
be harder and you'll get less of an
improvement than four was to three and I
think that that will keep on happening
because I think the availability of
novel information and non just the
degradation of of cannibalism will
become harder and harder and harder not
even diminishing returns because
diminishing returns suggests that as you
put in and like diminishing returns
typically suggest that something along
the lines of linear or more suggesting
that the more effort you put in the less
your return is right so that'd be
diminishing returns my guess is that
actually is going to be inverse uh
returns meaning that to make something
better is going to be harder because it
actually gets it turns they just it
turns into a Zoomer it's just a bunch of
deep fried models because the thing that
makes llms good is novel content and
training on that things that don't make
llms good is llm stuff because if that
was the case they would just train
themselves we would be so good it would
just be
like right since code can be corrected
and self- checked so a loop of self
you'd have to create self-corrected Loop
right you'd have to just fully if if
that was already a solved problem we
would just we wouldn't even need
training data right wouldn't need any
more stuff from the internet you could
just let it run and figure out
everything I think we're going to have
deep fried uh deep fried problem right
there is such a thing as quality of data
we can discuss Let's see we can discuss
a cancer research paper knowing nothing
about cancer and generate 8,000 times
more data than the original paper but
everything we have said could be
 yes and so that's where things
are going to get really interesting is
this entire problem of it of the snake
eating its tail so this I'm very curious
I'm curious what they found here the
message is we have to be very careful
about what ends up in training dator
says co-author
zakar shumov an AI researcher at the
University of Cambridge UK otherwise
things will always provably go wrong
provably are you saying that it's
mathematically based that llms cannot
are we Orab boring hard right now uh he
says the team used a mathematical
analysis to show that the problem of
model collapse is likely to be Universal
affecting all sizes of language model
that uses uncurated data as well as
simple image generators and other types
of AI that's a pretty big statement by
the way because the internet is so
heavily generated content now the
researchers began using an llm to create
Wikipedia like entries then trained new
iterations of the model on text produced
by its predecessor as the AI generated
information known as synthetic data
polluting the training set the model's
output became gibberish the ninth
iteration of the model completed a
Wikipedia styled article about English
church towers with a treaties on many
colors of Jack Rabbit Tales ain't
nothing like the many color of Jack
Rabbit tals it's one of my favorite
topics to personally cover more subtly
the study published in nature on on the
24th of July shows that even before
complete collapse learning from AI
derived text cause models to forget the
information mentioned frequently in
their data sets as their outputs become
more homogeneous it's literally cancer
yeah it's it's AI cancer if you will
that's a that's a fair way to put it the
thing is is like how do you know like in
breeding could be another yeah in
breeding or cancer I'm not really sure
because it's almost like
it's it's hard to tell which one it is
because what it
produces yeah yeah inbreeding might be
more more accurate in this case and
so I'm curious if G so my personal guess
is jypy
5 is going to be the Pinnacle for a
while now we may come up with something
novel afterwards and get even better but
I think jippy 5 is going to be a really
is going to have a huge problem which is
it's going to be good enough that it's
used even more which causes all further
information to be some part generated by
the llms thus having a problem jipp 5
may never exist because of that jipp 5
is well they keep T Sam keeps mentioning
jity 5 that they're training jipp 5 that
jity 5 is doing these things he said he
was quite certain jiid 5 was going to be
better than jippy 4 all right the
concern is when making AI models that
represent all groups fairly because low
probability events often relate to
marginalized groups says study co-author
Ilia sh shumov who worked on the project
while at the University of Oxford UK
this this is a fantastic paper says
Julia Kemp a computer scientist at New
York University in new New York City
yeah I mean one thing that this is
really interesting is rare events are
very difficult so rare events put it
this way let's put it in terms of code
good code I feel is rare kind of shitty
code is really normal right I write kind
of shitty code you write kind of shitty
code we all kind of write shitty code
and so it's like to understand and be
able to identify these rare events as
like the real things you should be
looking at is extremely hard question my
code is
perfect uh the open AI says US
Government will get to see jippy 5
before anyone else you know what that
means back door baby back door AI backd
door AI is most certainly happening
ain't no way it's anything but that back
door Wing let's go NSA mentioned uh many
technology firms have improved their
models by feeding them larger and larger
amounts of data but as human produced
content runs out they're hoping to use
synthetic data to keep improving the
study a version of which first appeared
on the uh is that is that rzv rxiv uh
pre-print server in May in May 2023 has
spurred the AI Community to try to find
solutions to the problem she says it's
been it's been a call to Arms archive oh
it's archive I see archive yeah Kai
classic classic Kai right there you are
with large language models work by
building up associations between
tokens uh words or word Parts in huge
swaths of text often scraped from the
internet they generate text by spitting
out the statistical most probable next
word based on the these learned patterns
the study authors trained their large
language models on Wikipedia articles
and trained successive Generations on
the model of the text produced by the
previous version prompted the follow on
from a paragraph of text from the
Wikipedia entry on grade uh one listed
buildings in Somerset the models output
of the following text first output of
the model generation zero contained some
errors but the ninth generation spewed
complete gibber so model zero Revival
architecture such as St John's Cathedral
in London the earliest surviving example
of perpendicular Revival
architecture ain't nothing like some
perpendicular
Revival am I right is founded in the
18th at at Century Church of Our Lady of
gy garcy garcy which dates from the late
19th century there are two types of of
per uh perpendicular perpendicular
churches those okay that's a little
strange but okay
okay ninth generation architecture in
addition to being home to some of the
world's largest populations of black
double at tailed jack rabbits white
double at tailed jack rabbits Blue
double at tailed jack rabbits Red Double
at tailed jack rabbits yellow at Classic
Ain't Nothing Like Home to the to the
jack rabbits so Jack so it's it's never
been Turtles it's always been jack
rabbits all the way down have we been
lied to is this AI truth are we an AI
truther right now is that's what's
happening add that just means
concatenated uh the word is without
space Oh wait what nine generations and
Bam you have a Reddit moderator like
this like we're talking about like this
level moderator right is this who we're
talking about this is the ninth
generation Reddit
mod crazy I did not know that to
demonstrate model collapse the
researchers took a pre-trained llm and
find tuned by training it using a data
set data set based on Wikipedia entries
now what would be more interesting is
not like cuz I feel like this is kind of
cheating like honestly I feel like this
study is not as interesting in the sense
that people a people already knew this
was going to happen but B that's not how
the real world looks the real world
looks like some portion of it is going
to be Ai and some portion is going to be
human so what happens when you're
training on data that's 30%
AI like how how does that really affect
the output of these these llms does it
actually cause the same model collapse
or does it help tune it into more
correctness as we're gently shoving it
the correct Direction with our with with
our edits to its
output especially if you have the input
of the previous
one if that makes sense if you have what
the you know if you have the input the
output and then the fixed is that good
trick data Maybe maybe maybe maybe it's
better uh let's see these researchers
took a pre-trained llm and fine tuned it
by training it using a data set based on
Wikipedia entries then ask the the
resulting model to generate its own
Wikipedia styled articles to train the
next generation of models they started
with the same pre-trained llm but
fine-tuned it on the Articles created by
its predecessor see again I think that
that's I think this is unfair because
this is again I I think this is just
it's it's too fast model collapsing
might not look like this in real life
maybe it's yeah I mean I wish the study
was a little bit more uh interesting uh
they judge the performance of each model
by giving it an opening paragraph and
asking it to predict the next few
sentences then comparing the output of
that model trained on real data the team
expected to see errors crop up says uh
schum love shum love uh but they were
surprised to see things go wrong very
quickly I'm not too surprised what I'd
actually like to see which I think would
be a really interesting idea is to take
GitHub Circa
2016 through through 2020 take a model
and then fine-tune it on code in this
region code we know that's likely not
going to be influenced by any llm and
then do the exact same thing maybe not
2022 maybe 2021 and then do the exact
same thing again except for due 2021
through
2024 and then see how does it do on the
same set of problems that are proposed
afterwards which one is going to do
better like do we see a degradation of
ability to solve
problems because we know that this
contains a a higher percentage of llm
code and this contains virtually none
well you're going to what I mean yes
you're going to have language version
problems right because JavaScript you
can't measure it based on you can't
measure it based on style like stylistic
output so if you did 2016 to 2021 you're
going to get a lot more JavaScript
you're going to get a lot you're going
to you're going to train it for a
specific area versus in this one you
probably in 2021 through 2024 probably
typescript is a much more dominant
Factor
comparatively and you could yes you
could do a you could do a third test
which would be the combination of both
of these which would be uh you know 2016
through
2024 and see you know as some sort of I
guess it'd be some sort of
control or I guess this would be the
control and this would be or this is the
I mean it's hard these two are the two
controls and this is like the the test
and I'm I'm curious to see what happens
like to see does it solve problems with
the same reliability or does problem
solving go down does it slightly go down
or does it go up like to me this would
be a much more perfect
test because this one seems like this
test right here just seems like you are
intentionally introducing air into a
system and only a into a system you have
no human intervention because all new
stuff is or a lot of new stuff shall I
say is llm with human
intervention uh collapse happens because
each model necessarily samp only from
the data is trained on this means that
words that are infrequent in the
original data are less likely to be
reproduced and the probability of common
ones being regurgitated is boosted
complete collapse eventually occurs oh I
see oh this makes sense complete
collapse eventually occurs because each
model learns from the reality but from
the previous model's prediction of
reality with errors getting Amplified in
each iteration okay this makes this
makes a lot of sense why this happens
over time those errors end up stacking
up on top of each other to the point
where models basically only learns from
errors and nothing else or it would it
return from only the most common things
right hence the reason why this image
still it's just emphasizing the things
that it's kind of
capturing it's Flo yeah it's floating
air Point possible loss of precision I'm
not going to lie I feel smart for having
predicted this a while ago that's
because you got that big energy
statistical Norm is what llms a measure
yeah people got dude I can't tell you
how many people told me how wrong I was
to say that uh llms they output kind of
like the statistical likely next word
and that's why coding is not necessarily
good and generation I have a short
that's like literally where I go uh I go
okay so here's the code that it it went
on and it's trying to produce like the
most statistical likely next value and
that means after all the code comes down
it's just going to be a normal
distribution with higher Peak into this
one area and then it's going to be
sampling from this which is going to
cause even higher peaks of the same
thing right and we're just going to
amplify the statistical most likely
middle middle code and people are like a
clearly a guy that has absolutely no
idea about llms I'm like dude that's
like that's like perfectly what's going
on here you throw you throw a little bit
of this out the bottom bada bing bada
boom reality check lm's a
dick it'll end up only doing hello
worlds yeah Google Gemini unethically
change answers oh is this I think this
is old one right or is this new oh it's
it's new again wait
what wait Google Google Gemini is having
another
problem no Google can't be having yet
another problem you can't you can't have
another problem with Google doing this
again again again all right we're going
to keep right here synthetic data
problems model collapse does not mean
that llms will stop working but the cost
of making them Inc will increase okay as
synthetic data build up on the web the
scaling laws that state that model
should get better with more data they
train on will likely break you know I
know I'm reading an article and as we
all know studies are flawed and often
they like they're they purport a certain
truth that may not actually be true in
reality but I would just like to say I
feel very Vindicated right now I feel
like the last nine months has been me
making the same joke over and over again
for those that don't know what that same
joke is this joke we are 15 months into
6 months away from AI stealing your job
we are 13 months into 6 months away from
AI steal steing your job we are 11
months into 6 months into stealing your
job I feel like in two more months I
will once again get to make this joke
I'll take this Vindication confirmation
confirmation bias be damned you know
exactly that's why I I am trying to be
get better at these things it's why I'm
constantly doing tests with llms I'm
trying to get better with my
pring you know like that this is why
I've seen some really bizarre stuff like
here's a really bizarre stuff so I run
the llms at the same time pring the same
game with the same random input just
different PRS okay because I don't know
which one's better and I've noticed that
time of day greatly affects the
successfulness of your pred which is
very strange by the way it's very very
strange that this this just time of day
makes a huge
difference it makes a big difference I
had one when I do like morning time
pring my loss rate or my win rate at my
game that I've made with Chad jippy is
like like
33% when I do it at night time it goes
near
50% late night PRS are more erotic they
are I I don't know what it
is it's wild so check this
out uh let's
see let's go to this one okay this one's
going to get real this is going to get
real painful for
everybody all right so right
now let's see Strat one Strat two right
now I have the following I have uh
let's see I have
224 over uh 228 plus 224 I've been
trying to get the same oh my goodness
come on uh here 0 Z will that make it
all right so I have 49% versus uh 24
249 with uh
249.50 4 so there we go so this thing
was 30% better during the
morning and slightly worse during the
the afternoon and my two PRS are
behaving differently depending on the
time of day and how often and so I keep
running this test I've done a bunch a
bunch of tests to make sure to see how
it's going and I kid you not here's the
two differences from the from the from
the PRS
uh I just added this line to the top
because I saw the most bizar thing of
all time so you're using where where are
you using the same seed for the game so
the game has the same uh the same seed
so check this out watch this one sheets
make sure I'm like looking at the right
sheet you know what I mean I don't want
to actually be giving you I don't want I
don't want to be giving you guys any
ideas so where this all happened is I
made I I played 50
games per prompt that was suggested by
the community to see which one performed
better than the other one welcome to
Costco whoopsies that's
alerts and when I did that I had the I
had this one Is Random so in other words
in my tower defense the AI or my
handrolled AI would just place Towers
randomly and this one was my handrolled
Strat that place him in a box and
continuously upgrade them and the N like
hacks Navy SEAL Master proter Netflix by
the way pythonista Richard all won at
about a 10% rate scabetti run at
practically a 30% rate and scabia is lit
is is is just this that's all scabia is
is that
that's the proof and it won at a 30%
rate against a handrolled technique what
the hell is that right what is that so
then I said okay that's weird senior
prompt engineer absolutely got dominated
but my handrolled one did a lot better
so I was like huh mine did the best then
scabetti which makes no sense so what if
I take literally take scabetti and mine
and I smash them to together and just
Riz the AI and it it was doing 150%
better I had 28 wins and that's when I
discovered time of day makes a big
difference because if I run it in the
morning I want to say scabetti does a
lot better and if I run it in the
afternoon or in the evening the prime
gen does better and so this is what I do
I literally just run the prime gen here
and then I run the no influence one so
they're both playing right now at the
same time they're both using the same
API they're both using my API token
they're both having all the same code
path they're just having one file versus
the other file oh what is this Voodoo
nonsense it's me trying to understand
the the the PRS right I'm trying to
trying to PRT it did you set a seed for
the model I did not set a seed for the
model I could set the same seed for the
model that would probably be the most
okay so yeah yeah yeah that that that
wouldn't be too hard uh cuz I could kill
um kill a baby uh kill and then go into
here and go to main uh TD main go and
then where's my opening I code I know I
have my open AI code right in here is
seed in the message where is Seed at
yeah seed uh what should I make the seed
should I just make the same seed what
what's the seed supposed to be it's
supposed to be a pointer to an INT okay
can I just can I just reference an INT
okay I can't reference it in I don't
know why it's a
pointer can I just make it can I
literally just make a VAR Fu equal 69
can I just do
that does that count am I seeding it
right am I giving it the seed don't put
your seed in the
model is your temperature set to zero my
temperature is currently set to zero
yeah
yeah here I'll set it to one which is
the natural state of things there you go
now let's just hey we'll just rerun
everything and see what happens okay
we're going to seed the
model we're see dude I'm giving it my
seeds
okay there we go
okay hold
on all right Aya saying don't do
that here Aya I I will I will I will
believe you here for a second uh seed
there we go temperature 0.55 there we go
for
you for
you there you go how are you seeding it
if you got the snip Checkmate atheist
yeah I know putting your seed in a model
using 69 won't be fruitful yeah but they
all have the same seed they literally
all have the same seed Immaculate
Conception Immaculate W what are the
chances dude this is why the llms win by
the way we're not actually all that
funny we're actually not all that
funny and we produce the same
thing it's sag it's sagge I know it
makes us feel
bad anyways okay so I just thought this
was very interesting so you know
needless to say I I want people to
understand that I'm not not I've really
tried to correct this notion that I'm an
L I'm an AI hater I just very much so
worry about all these people that think
that the thing they should only do is
rely on pring like rely on it all you
want but get good at the same time you
really want to learn that skill because
I am not convinced that AIS are just
going to be this I I'm I'm truly not
convinced that AIS are going to improve
like that
I'm I'm completely convinced we're going
to see something like this and maybe
we're only right here so the the
betterness is way like way way higher
right it's just like massively
better but maybe this is one and this is
1.01 right maybe we've already hit the
tippity top and that's like that's the
big thing that I want to get across
which is that you shouldn't put all of
your eggs in a basket because as far as
I can tell everything on Earth typically
follows a certain pattern which
goes it's gold look at this it's shining
like gold it's pretty it's beautiful
look at that that's gold it's so good we
can all get everything we've ever wanted
and then a bunch of people rush into it
and it turns out it wasn't gold it was
just
glittering and we all turn into
Aragorn except not the cool version of
Aragorn like the the the sad not cool
version of Aragorn it's just shiny poop
and this just happens constantly there
there there is no free pass as far as I
can tell everything in life never has a
free pass everything has some sort of
like push and pull you get something
better but it also takes away
something good example is friend friend
is this little thing that's apparently
going to help lonely people not feel as
lonely because they're going to have
this friend that's always with them so
when they press a button the thing talks
to them it mimics or looks or is like a
mirror to what a human could
be the obvious downside to that is that
friend is not a human it contains no
struggle it contains none of that it you
won't go through all the normal
relationships version of what it
actually is to be a human and so the
only the the real outcome of that is
that you're going to you're going to
feel less lonely in the moment but but
you're going to isolate yourself because
you will be less and less likely able to
actually relate
to and become friends with real
humans it's going to steal from you
ultimately now it may be this thing
where you're going to hear you're going
to see it all the time you see it all
like dude I cannot tell you how many
people have told me uh that I'm
completely wrong it's going to help
people that are lonely I get it I hear
you okay that sounds really nice right
that oh man it sounds so nice it it does
man ah but I think it's going to be much
worse than
better it's basically the plot of her
it's basically the plot of
her see no see see I just don't believe
this at all because that's the thing is
that that's not what happens because if
that was
true then Discord should have worked am
I right Discord you're just talking but
people are more lonely than they've ever
been Discord has not in fact not helped
human friendship and that's with real
humans so now let's take the human out
of it and just have a reflection or a
mirror of what a human could be that's
going to that's not going to make it
better ever like no one's coming out of
their shell right they're not coming out
of this like the shell will just be
worse it'll be
stronger there you go you got to go
somewhere yeah I think it'll make
isolation worse long term I think it's
going to make isolation worse uh long
term you know that's why I keep telling
you you know it's good you overestimate
how good most friends are I think you
underestimate how good most friends
are I feel like you need the you're
going to need to go go up next to
Grandpa Aristotle get some Aristotelian
goodness in you maybe followed up with
the little CS Lewis I know you want that
to be true and this is such a this this
feels you have to like the problem is is
that you've you've gained this extreme
reductionist view of the world
right the reductionist view of the world
is that we're all just llms and that's
it so why would one llm be somehow
better than another llm it's not and so
that is the fundamental problem that you
have and so I'm making an argument
against the specific but the problem is
is that your Viewpoint your fundamental
Viewpoint will not let you see past that
and that's why you're going to see
you're going to see things said like
this this is just it we have no proof of
what you're saying has ever existed
right and this is it friend AI will
literally make people less dependent on
real friendships it's like saying porn
and riches sex lives which it
doesn't porn has in fact not enriched
sex lives it has had done a lot of the
Opposites uh anyways there we go so
let's keep on going how much synthetic
data is used in training matters when uh
shov and his team fine-tuned each model
on 10% real data alongside synthetic
data collapse occurred more slowly oh
good this is the thing I was talking
about nice they actually did some of
this and bottle collapse has not yet
been seen in the wild says Matias uh
girs grasser I don't know is that true
cuz I always hear people talk about
there's certain things that chat jippy
35 did so much better than chat jiid 40
and you see these people going oh no no
no no you got to use 35 it's way better
for this one aspect versus this other
aspect and I'm curious if that's just
the inkling of any of that happening
maybe it's not enough to know if
it's yeah it might be anecdotal it might
be all anecdotal and some people's
preferences might just line up with
something maybe right uh I could be
wrong there I I'm F uh I I could be
completely wrong by the way but I'm just
saying I've heard a lot of that talk
over and over again for specific sets of
problems and so I'm curious if that's
like the inkling of something bad or is
it just the difference of training maybe
right an AI researcher at Stanford
University California a study by girz
grassers team found that when the
synthetic data didn't replace real data
but instead accumulated alongside them
catastroph catastrophic model collapse
was
unlikely okay it is unclear what happens
when a model trains on data produced by
a different AI rather than its own okay
okay developers might need to find ways
such as watermarking to keep AI
generated data separate from real data
which would require unprecedent
unprecedented coordination by big Tech
firms it also not only that but it's
also going to require that somehow you
cuz I mean at the end of the day if you
just have text companies are going to
remove whatever Watermark there is you
know they're not going to send out an
email hey thanks I really appreciate
your reply I am very happy to be able to
make these changes for you by chat
chippity right like you're not it's
going to be very hard to do that
especially if people make small changes
on it uh the problem with uh is the
growth of synthetic data is vastly
outpacing the growth of man-made data
yes I know this is what I'm talking
about right which is that it's going to
it's going to keep on going PL Tech's
about to be illegal yeah know only Rich
texts yeah I know it's going to have to
be HTML everyone's going to learn how to
be HTML because it's going to be text AI
true you're going to get it do you think
high quality portfolio oh this is
something something way different okay
developers need to find a new way such
as water marking to keep AI generated
data separate from real data which would
have an unprecedent blah blah blah blah
blah and Society might need to find
incentives for human creators to keep
producing
content I think I have one for
you
uh slap the CEO of Nvidia and stop
having him say stupid stuff like just
let the stop teaching kids to
code second I think most people at this
point are pretty sick of AI generated
artwork it looks very Bland and boring
and normal and just so average AI
generated artwork is kind of like
watching rings of power I understand
that rings of power was a billion
dollars like a
billion absolutely Money Pit tar pit of
money and when you watch it you look at
the scenery you look at the uh the the
the special effects and it's it's
beautiful right like it the it's it's so
rich in scenes and everything it's
shocking and yet somehow it's the most
average piece of content I've ever seen
in my entire
lifetime all right a watermark for data
synchronization describes an object of a
predefined format which provides a point
or reference of value to two systems
data sets attempting to establish a data
incremental synchronization any object
in the query data source which was
created modified or deleted after the
watermark value would be qualified as
above Watermark and should be returned
to the client requesting the
data I don't get it I don't think I
quite get
this this approach allows the client to
retrieve only the objects which have
changed since the uh latest Watermark
and enables the client to resume its
synchronization job from where it left
off in the event yeah I don't I don't I
I'm not I don't quite understand how
this would work with plain text
versioning yeah I don't I don't
understand the I don't understand the
versioning
part all right I don't understand how
that would work in practice I don't
think filtering is likely becoming
important too for example humans could
curate AI generated text before it goes
back into the data pool oh man AI is
about to go to another Indian again oh
no oh no is that are we getting are we
going back to Amazon Instant checkout is
that what's about to happen again actual
Indians actual Indian yeah this is what
that sounds like uh our work shows that
if you can prune it properly the
phenomenon can be partly or maybe fully
avoided fair
fair okay someone says have you checked
out these images no I have not checked
out these images this is real a lot of
these images are really boring like this
is just like there's so many boring
images this boring image I don't know
let's see it doesn't have something I
can't explain I know it has something I
don't know how to say it that's why I
call boring I don't know what it is like
it's it's beautiful it's absolutely
beautiful every part of it seems
beautiful but somehow I get I feel
almost frustrated by it I can't tell if
this is great or awful but it's it's
certainly beautiful like it has all the
facets of what my eyes want to take
in but I'm not sure why I don't like it
I don't I don't know why I don't like it
well it looks fake but it looks more
than fake it somehow looks like it's
like try hard fake I don't know how to
anyways I don't know what it is I don't
know what it is uh the composite looks
sus I know AI equals awful images it's
beautiful but it's it's too I don't it's
I don't know how to describe it like
right like it's TR an amazing piece
right like if it's awesome but something
about it just seems not it's too perfect
almost it's almost like you know when
you draw too perfect of an item it
somehow looks worse than when you draw
something that's not as good it's almost
like the not as good side will make it
feel better than the thing that's
perfect it's I'm not sure really how to
say it other than
that it's the hater within maybe it's
the hater within maybe I need that maybe
what I need to do is I need to look at
100 images and only five of them
should be Ai and I'll go through and try
to guess AI or not yeah yeah exactly
sight engine oh man oh this is such a
good one because it's so hard to tell if
this is one or not like he's adjusting
his hood leaning
forward I'm going to go with not an
AI if this is not an AI I'd be a bit
shocked considering this just looks like
it was I mean this looks fake I this is
so fake that I think it's it's either
someone photoshopped this
or it's fake right well it's not that it
looks I mean it just looks completely
Bonkers right it just looks
Bonkers see someone had to photoshop
that one it looks the the the bikes were
too bright and completely out of place
yeah okay well that one was real
apparently normally I would just say
this is fake I don't know this this
feels like not oh oh I clicked AI I
didn't click not AI my bad I was right I
was right I'm GNA say this is not AI
oh no you get no fingers in this one so
I'm just looking at I'm just feeling
it okay so the reason why I'm going to
oh man I think this is AI I think this
is AI but I'm willing to be wrong in
this one because the the the smile on
the guy just seems so weird it just
seems so
wrong I mean you can see the hands right
this kid this kid has some someh
something feels weird about this I guess
this could be the third finger you can
see the hands
he has four fingers he has four fingers
this one has a third finger potentially
I'm going to just say this is AI because
it had that weirdness to it oh this is a
great one this is a great one so far
we're at 100%
correctness these ones I think are
particularly hard just because there's
there's all these little
people not AI or AI not AI or AI I'm
going to say this is AI just because of
the bubblies and stuff like this are
just so
strange everything it just it feel feels
a little strange in there those bubbles
had to be added in post post you think
it is real she just farted ah okay yeah
this one I don't know this one I don't
know I don't know which
one yeah I guess I could go with this
one's not AI it's AI there you
go this is just a this is just a
filter yeah the bubbles the bubbles was
it the bubbles looked too weird
all I did okay I did okay I bet you if I
would have kept on doing it I might have
been
50/50 which would have probably put me
into it it's unfair because they
intentionally picked pre-processed not
AI in the strange setting yeah yeah well
I mean but it just shows how close they
can get right it just shows that
sometimes I think let's see you let's
see I was so confidently incorrect on
most of those
really yeah maybe that's it maybe maybe
that's it maybe it's the it's the ultra
filter ring slun yeah maybe that's it I
don't know the problem is is this is
this just an
Instagram by the way if this is not an
Instagram this man should go and but
just his stomach and all this there's no
way this is real this feels this looks
fake
but and like the shadowing is all crazy
so I'd go with AI on that one wow that's
crazy that's what it looks like inside
of a
studio yeah probably on Instagram shiny
it's just been filtered so many times
it's hard to tell uh I could be coding
right now but I'm I'm watching him all
right anyways uh very interesting I'm
curious where this is going to go I'm
curious where this is all going to go AI
models fed AI generated data quickly
spew nonsense I'm curious well I guess
we'll see right maybe I am wrong maybe I
am completely wrong and we should
have been investing in llms all along
but again this is why I'm trying to do
training I'm trying to figure things out
I'm trying to do the same stuff I'm
trying to understand this stuff because
I really want to make sure that I get it
and at the end of the day I want to make
sure that I know how to do better
prompting and what causes better prompts
versus worse prompts because you know if
I'm wrong I want to be
prepared but I don't think I'm that
wrong a Jen

um great um so um I'm going to get to
causality uh after a few minutes I
promise but um I want to start with the
story um and the story is the result of
me kind of wrestling with what uh you
know the things that are happening in
machine learning uh mean for me as a
cognitive psychologist and the field
cognitive psychology and the the feeling
that a lot of the dialogue is much too
anchored on this exact moment of the
technology rather than the the general
things are happening okay so let's
consider a thought experiment it's
2030 um the the very many Labs
Consortium has completed the the psych
50 project which is this intense
five-year effort to collect all of the
experiments done in the last 50 years um
they they did this by using large
language models to read all the method
sections generate JavaScript code uh and
items and then uh they uh uh crowdsource
replications because of all of the out
of s outof work white color workers due
to large language models
um at the same time gp8 was just
released by The now properly named
closed
Ai and uh so these this Consortium runs
all the experiments from the psych 50
Corpus the last 50 years of psychology
on gp8 instructed to think like a human
and discovers there's this amazing
convergence that the the the answer is
provided by
the you know super language model agree
with the human uh the human population
and and even better if you ask gp8 to
think like a whole bunch of humans you
get a population of responses that
reflect the the human population so of
course the New York Times runs an
article the end of
psychology and so that then the field uh
has to respond to this um how does the
the the field of psychology respond well
add 10 more years so 10 years later
there's a retrospective volume where
there's a nice historical analysis uh uh
and it identifies three main uh
viewpoints that emerged over the those
10 years in Psychology application ISM
explanationism and mechanism
ISM so uh the application ISS um they
really Lean Into The View that the
primary value of Behavioral Science is
social impact and so when they discover
that there are now ai surrogates that
let them predict uh how humans will
respond in very complicated uh real
world situations they think this is
great um and they just say that's all we
need in order to design interventions
and and use uh psychology the way we
always wanted uh including the uh you
know uh now uh 2040 famous example uh
where they intervened on the beliefs of
a small Midwestern City to make them
think that Cricut protein was actually
just as good as other kinds
uh um and so the result is is that for
application ISS Theory takes a back seat
it isn't what's important uh in order to
achieve
outcomes okay then there's another group
uh of psychologists the explanationist
and the explanationist uh they hold fast
to the idea that the goal of Psy
psychology is uh achieving an
understanding of human behavior um they
reject that gp8 itself is an explanation
because they don't understand you can't
understand it it's too big and
complicated um and then they fracture a
little bit there's one group that uh of
explanationist that still believes that
models can be explanations they just
need to be models that are transparent
enough for humans to understand um and
so then uh these uh Neo nuas maybe
that's me and me and Todd um you know we
build some beautiful uh uh basian
complex basian models that capture a
small fragment of what uh gpta can do
there's another group of explanationist
who say no I can't understand your your
uh compli at basan models um only models
uh only theories that can be expressed
as clear verbal theories are uh are okay
for explaining
behavior um and so overall this this
group of psychologists says to
themselves well they implicitly say to
themselves if explanations uh aren't
there to increase predictive or
manipulative power um what is it they're
good for well they're increasingly good
as a as a kind of artistic uh value is
you know what fien said back in against
method anything goes and so it's okay
it's defined by the community what
counts as a satisfying explanation and
that's what we're doing um the most
radical of these subcommunities is uh
the Reddit uh le5 site community that
insists that the only theories that can
be considered must be understandable by
school
children and then there's a final group
of psychologists um these are uh the
mechanisms um and they actually endorsed
the idea that that gpta uh means that
behavioral psychology is a completed
Endeavor and so they they lean into
cognitive Neuroscience they double down
and say great what we need to do is
figure out the circuits in between the
input and the output of behavior that's
what we're
doing um this goes pretty well uh until
uh it's discovered that the uh internal
states of GPA while they correlate well
with neural Behavior right at a at a
population level can correlate them with
what's going on in neurons nobody can
explain what gp8 is doing in terms of
understandable circuits anyhow um and so
they they just collect a lot of data and
hope that something will change at some
point okay so um is there hope um that
was my my pessimistic imagination of
where we could find ourselves um I don't
think it's it's so bad um but I do think
that we need to think deeply about the
kinds of uh rules of the game and the
tools that we're using uh now in order
to send ourselves down a better uh a
better path to the Future um so you know
explanationism uh requires I think in
order to make it useful uh as as a as
science constructively specifying some
space of acceptable explanations not in
anything goes but something that we
think is sort of normatively uh
normatively uh acceptable or Justified
or something like that similarly
mechanism ISM uh requires some kind of
tools for finding causal structure in
very complicated systems complicated
networks um you know Contra my pessimist
pessimism in the story on the last slide
um and so one uh possibility that um
really this was brought to my attention
by Thomas iard and a lot of what I'm
going to say in the next few slides is
uh thanks to Thomas and a bunch of great
collaborators um you know maybe it's the
case that simpler call models um
actually can in a very formal way serve
as explanations of more complex ones um
and then what we have to do is figure
out how to how to uh describe that
relation relationship in a precise way
um so this is the idea of of causal
abstraction um we want a mathematical
theory of one when one causal system is
an abstraction a faithful full
abstraction of another um the good news
is that uh people smarter than me have
been thinking about this for uh you know
what's it been uh maybe five or six or
seven years
now um and the basic idea Al lighting a
lot of mathematical details and
differences is that if you have these
two causal models you want there to be
some kind of map from the variables of
the abstract model into the variables of
the uh the low-level high I'll call them
high level and low-level model um that
has the property that mapping back and
forth between these uh these values
commutes with intervention that you can
do at the high level and the low level
um so I'm going to illustrate that I'm
not going to write down the the math for
you all but I'm going to walk through uh
kind of the idea in some simple more
specific uh
cases um so imagine that we have uh
simple Algebra 1+ 2 * 3 and the output
is nine and we train a neurl net and it
can do this and we say hooray it knows
arithmetic um then what we want to do is
we want to understand whether it
arithmetic in the sense that it is
carrying out the uh the algorithmic
process of first adding 1 plus two and
then multiplying three to get the answer
so we have a high level caal model do
those arithmetic steps and a low-level
caal model whatever is in this neural
net um and the question is are they
alignable can we find the right kind of
function that relates them uh in this
way okay so what we want um hinges on
the the fact that causal system
um you know at least have
to I want to avoid saying they're fully
characterized by their interventions
after the talk this morning but at least
interventions are a pretty important
part that that's that's fair right maybe
yeah okay um so um if we take the high
level Cal model on the right if we reach
in and change the intermediate variable
three to something else uh oops sorry
we're trying to figure out where that
intermediate variable might live on on
the left okay so uh how would we know
what does it mean to have such an
alignment um well the idea is that um we
will have such an alignment between
something in the low-level model and
something in the high level model um if
it's a case that the interventions on
the high level model have a
corresponding intervention on the
low-level model that does the same thing
uh uh changes the relationship between
input and output in the same way so let
me illustrate that um you can imagine
that we reach in um and we change the
value from 3 to 5 right so it goes from
uh sorry 5 to three I can I can do
arithmetic I promise um we change the
value from five to three and now the
answer on the output changes from 20 to
15 so we have this counterfactual
output um this is the idea that the kind
of interventions we're interested in uh
or that we're going to focus on at least
are these interchange interventions um
this was an idea that um Thomas and
attakus gger and Chris pots came up with
that I think is is super brilliant
because it takes the very abstract
notion of all of the interventions and
give gives us a really concrete set of
cases to look at so the cases we're
interested in are the ones where we just
record uh the value of the intermediate
uh variable from uh one case and we go
and we oops don't have that animated we
plug it in to uh to another case um and
so the nice thing about these
interchange interventions is that they
unlike you know say I could go in and
say oh five I'll change it to six but
that really depends on these numbers
being numbers interchange interventions
are perfectly well defined for arbitrary
models that have arbitrary contents of
the variables so here I can go in and I
can say okay well if that variable is
doing the same thing it's playing the
same role then if I record the
intermediate uh activations when I put
in 2 plus
um and I splice them in over here when I
had put in 1+ 2 it will change the
answer to the you know to say
15 um and furthermore with a little bit
of math to make this precise I can say
the uh the the extent to which I get the
correct counterfactual answer from doing
that that interchange intervention is a
measure of uh how close the alignment is
to having worked um what we call the IIA
or alignment score
okay um so the basic picture here is
that if I've hypothesize a high level
causal model and I have a Target
low-level causal model I next have to
come up with an alignment between the
variables in the high level model and
the low-level model and then I can do
this swapping this interchange
intervention to see whether the variable
over here plays the same causal role as
the variable over
there parentheses caveats with respect
to the that we're using to test this
which we can talk about later um okay um
that's straightforward except for the
fact that when I just look at this
neural net I don't really have any idea
where that that intermediate uh you know
sum of two variables should live right
big pile of neural net activations I
don't know what it does um so a lot of
the interesting work that's been
happening in the I don't know Collective
of causal reasoning people at Stanford
the last couple of years um and really
spearheaded by adus gger who's a
brilliant student who just graduated has
been finding ways to much more
effectively and efficiently search for
and find the the populations in the
lowlevel that Implement variables in a
high level model if they're there of
course um so the the main one that I
want to tell you about today um hinges
on the idea that we first need to move
from thinking in a localist way about
the representations in the network to a
distributed way so what do I mean by
that well rather than
saying 1 plus two variable I'm going to
say this is a vector space and I'm
interested in whether there's some
direction some Subspace of that Vector
space that represents that information
um that's the classic uh kind of PDP
connectionist idea that you know
important conceptual variables won't be
necessarily represented in say single
neurons they'll be represented in some
superposition of neurons some direction
in the vector space um okay so given
that that's the case there's an
interesting uh sort of fact from linear
algebra which is that um you can um find
you you can basically find vectors by
leaving the vectors where they are and
changing the basis um and so the the
basic idea is what we're going to do to
search for a variable that could align
is we're going to um pick out some
dimensionality of the Subspace we'll
relax that in a minute and then we will
try to come up with a rotation that's
next slide we will try to come up with a
rotation such that when you do it um do
the swap over say the first K uh
dimensions and then undo the swap um you
have uh you have the right
counterfactual behavior um now the idea
here is basically all we're actually
doing is allowing ourselves to use the
you know modern tools of gradient based
optimiz ation in order to look for a
Subspace of all of the neurons that
represents that highle variable we're
not actually changing the the neural net
or the high level model in any
way okay and you guys should interrupt
me if this is totally unclear um good so
um good we go forth and we do that um
let me give you an example um so there's
this uh very simple computation that's
been kicking around uh the psychology
especially developmental psychology
world for some decades called
hierarchical equality you might
recognize it as one of Gary Marcus's
favorite reasons that neural Nets can't
do stuff um and hierarchical equality is
is a really kind of cool but simple uh
computation it says you're going to have
four inputs uh WX Y and Z and you first
check whether the first two inputs are
the same and you get you know true or
false you check whether the second two
inputs are the same true or false and
then you check whether those two
variables are the same right both true
or both false um so what you can do is
you can train an Earl net you can just
say great I'm going to generate a whole
lot of data that's consistent with that
rule so you know one one two two yeah
that's true uh 2 one 3 three that's
false because these two are different
but these two are the same and so on
okay um so then you can train in Nur net
and that's generate data train the
neural net um you get a neural net um it
turns out that this was not something
that I expected but it turns out that
actually even pretty small neural nuts
not even big old Transformers can solve
this task perfectly well and can even
generalize um but you have the question
of whether those neural Nets um are
carrying out that computation just by
some kind of you know approximate
embedded dynamical systemy cool thing or
whether they're actually implementing
the symbolic computation right um
this is a question that I think had
worried me for a long time when can we
say a neural net actually implements a
symbolic model and before this line of
work that sort of Thomas showed me I had
no way of answering that question or
thinking about it so we set out to use
uh this causal abstraction uh method uh
to analyze the neural net that we
trained on this task um and what we did
is we explored um three different
possible highle models so the first one
is the one that actually you know we use
to generate the data two intermediate
things um the second one is one where
you generate one intermediate value and
then just have a single function to
combine the remaining things together um
and um the the third one is the one that
actually uh doesn't have any
intermediate variables it does sort of a
copy and then and then a combine the
critical thing about these is just that
they are three different uh comput you
know functionally equivalent ways to
compute the same thing
um okay and then we went and we looked
for uh for the the the variables and uh
in the neural net and asked you know are
these causally represented is there a a
you know a distributed representation of
each of these variables that does the
right thing when you swap out
activations from other examples um and
the result um which is pretty was pretty
surprising to me is that yes in fact um
uh this is varying the dimensionality of
the sub space so how much room do you
give this variable to to use um and as
long as you give the variable enough
room to use you can find it exactly
represented so um in the details don't
matter very much but basically um
somewhere in layer one of this uh fully
connected Network you actually find an
exact representation of the two
intermediate variables being computed
this is a really strong thing to be able
to say right it's saying that this
neuronet that if you look at it is just
a it's a dynamical system it's a pile of
Weights in some kind of useful
mathematically precise sense is
implementing the symbolic
computation um it's also worth saying
maybe two things two other things which
is that this isn't uh like there's many
ways to implement that computation uh
that are not implemented in the neural
net so those and and others um and also
it's it's distributed if you went and
look started looking for a localist repr
presentation of those two variables you
wouldn't find it we sort of do two
different versions of that on the on the
bottom um and so you know it's doing the
symbolic thing in the distributed way I
guess basically like J mclen has been
saying for a really long time um but the
only way to find it is by knowing the
the clausal relationships that you're
looking
for okay
um yes thank
you now what did we settle on what does
that mean
I'm joking um okay um I want to then
describe taking this the same basic idea
um now we want to scale it up like
obviously what we'd like to do is we'd
like to go analyze gp8 um or at least a
re reasonably large Transformer um the
first thing we have to do that I'll go
through I'll kind of skip over is we
have to extend the algorithm just a
little bit to allow it to search more
efficiently and the critical thing here
is we we don't want to have to determine
the
size of the of the representation ahead
of time so we do this thing where we
sort of relax the boundaries so we can
have uh learn filters and then we can
take derivatives and it can figure out
how much space it wants um it's
technically useful but not that
important um and then we have a new name
we call it boundless distributed
abstraction search um and cool then we
go and we say great um when we were
doing this the um the the best open
source uh Transformer that we could get
our hands on was this one called alpaka
and the biggest one we could fit on our
gpus was 7 billion parameters which is
still big enough to actually do things
right like it can do some things not by
the way as many things as I was hoping
it took us a little while to find a task
that it was like sufficiently good at to
be worth analyzing um but we found this
task where we say uh now in kind of chat
mode right please say yes only if it
costs between 350 and5 uh $50 otherwise
no you give a price for
$410 uh and it says yes okay so there's
the task and then we can ask great how
is the Transformer carrying out that
extremely simple
computation um and so the the high level
model that we want to explore um you
know a kind of natural hypothesis is
that what it's doing is it is sort of
computing the intermediate variables of
comparing the price that we give it to
the lower bound comparing it to the
upper bound and then you know combining
those taking the conjunction to to
figure out if it counts okay um okay
there are alternatives there's like
other ways you could do this
computation uh so like I kind of like
this one as an alternative which is not
crazy which is you um you compute the
midpoint of the of the interval and then
you measure the absolute distance to the
midpoint it's like a totally fine way
you could do this right and importantly
it's the same input output function it
only differs in the mechanism in the in
the causal kind of steps you go through
okay um so this is a big old heat map
these these four correspond to looking
for the causal representation of those
four different causal models um and the
the basic finding here so the first
thing to say which is annoying and
requires a lot of further thought is
that the task performance alpaca is only
gets 85% of these right even though it's
very simple and so kind of the seiling
counterfactual performance is around
that so you know when we see 8588
somewhere in that range what we're
seeing is like the best you could do at
causally representing these things um
this is basically a heat map showing the
layers of the alpaka model and then the
tokens of input um and the number is the
intervention accuracy score of looking
for the representations of the causal
variable the high level causal variables
somewhere in that layer somewhere in
that part of the the Transformer um okay
so the first result is that it does look
like the transformer is representing
this computation based on Computing the
boundaries and putting them together and
not something like my uh favorite
algorithm the midpoint distance
computation okay um the second thing
which I think is even more interesting
is that those so now we now we have this
set of directions these representations
which are some subspaces of the
Transformer activations and we can ask
okay are those still the causally
relevant directions if we change the
task so if we generalize a little bit
are they still the representations
because it's possible that the
Transformer as soon as you change
anything it puts its information in some
completely different place um so for
instance um we can take our basic task
and we can add a random prefix that
could in princip will interfere in all
sorts of ways with the
computation um and and the result is
that um it's uh those representations
are relatively stable to this change to
adding random
prefixes um they're also uh um uh we can
also uh sort of generalize in a very
minimal sense generalize the task from
saying yes or no to saying true or false
and you could imagine like you know
Transformers do all sorts of stuff um
but it turns out those representations
are concern the same representations are
still how the Transformer computes the
answers to this this slight twist um
okay um what's the takeaway what are my
thoughts well first of all some things
that we still need um we need much
better tools for finding the abstract
computations here I posit them and we go
look to see if they align um but that is
painful um we need much better math
we're thinking about approximate
abstractions um and thinking about like
kind of intermediate interchange
intervention accuracy um and and then
there's kind of deeper questions um like
um when is it that a causal abstraction
feels like it yields a useful
understanding of the low-level system is
it anyone does it have to be simple does
it have to be something um uh and then
maybe just the last thing I'll mention
is uh the the you know the question that
I brought up in my uh story earlier
which is we still don't know it's a
hypothesis we have to explore but it
could turn out that very complicated uh
large language model systems are just
not coverable by a set of these kind of
understandable causal abstractions and
so I think that's something we really
need to think pretty hard about
thanks so I'm going to be honest I did
not follow everything about that but I
was very interested in the back part
especially about the alignment that
you're were talking about because you
kind of Def it as essentially some sort
of mapping and I just want to ask
whether you have an intuition as to is
this mapping homeomorphism an
isomorphism or are you talking
about oh
um uh it's a communative diagram okay um
it is not a homomorphism or an
isomorphism because you expect to lose
structure okay when you go from the low
level model to the high level model so I
don't remember what that makes it do you
know what that makes it it can be a
homomorphism of a particular structure
to the structure where that's where the
algebraic operation B binary operation
is um composition of of interventions
yeah I guess one one thing I'll say
which is not you know uh directly
responding but um we very broadly
speaking are trying to figure out a more
algebraic notion of this that involves
looking at interven algebras and how
they they interact and I think it's
closer to the question you're asking so
you you don't really have a fixed answer
as what kind of alignment this is but
you just have an in that there is some
sort of alignment and you don't really
know
like's mathematically no no no I mean
there's a very there's a very definite
the paper you know kind of laid out
there is a very definite uh property
that these mappings have to have it's a
set of mappings that have a certain
commut commutativity property um which
basically says that with respect to the
interventions you're interested in you
can get the causal effects the predict
causal the counterfactual effects um
either at the low level or at the high
level and they'll
agree so since we just have a question
in the
Weeds last another um I like the idea of
aligning high level extractions of all
of on by aligning vectors in space but
that seems to require that there's a
defition what it means to have the
vector representing something of the
high space I can't imagine how that
would work in recurrent
problem
obious Direction clearly maybe there is
in the low level space or the high level
space the high oh the high level space
you don't have to have vectors so this
is the beauty of interchange
intervention so the high level space you
have some values of the variables you
record a value you swap it in for a
different input um that might not be a
vector itself the low-level model
because you know here I in
the yeah in the in these applications
I'm assuming that it's a vector so that
there's a notion of Subspace but the
critical property is just that whatever
it is it's a thing I can record from one
input and splice in when I'm processing
another input right um recurrent models
are kind of hard for their own reason as
long as you have a a particular time at
which you think the variable is to be
represented everything is
straightforward if you don't know when
it's
complicated there tons more

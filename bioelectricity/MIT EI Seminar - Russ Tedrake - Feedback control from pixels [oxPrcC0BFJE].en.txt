cool so Roz is probably one of the few
professors to be in three different
departments
she is the Toyota professor at East yes
he's also professor at Aeronautics and
Astronautics and also at Mickey at MIT
he is the director of the Center for
robotics at Cecil and the leader of the
team I made his entry at the DARPA
Robotics Challenge a few years ago
Ross is also the vice president of
Robotics research at Toyota Research
Institute at DRI and he is also the
recipient of the NSF Career Award the
MIT Jerome sands reward for interpret
undergraduate teaching the DARPA young
faculty award in mathematics and the
2012 Rosen Jones
Teaching Award and also was named a
Microsoft Research new faculty fellow
when he studied he studied this computer
engineering at Michigan and then he did
his PhD here with us at CSUN in 2004
working with Sebastian stone and after
graduation he did that BCS also at MIT
tutorial is possible and finally some of
his education has also been at at
microphone center fancy welcome
oh hi everybody thank you for coming and
I am actually more worried about talking
to the void than having too many
questions so I appreciate your seminar
advice like I said but I think you could
probably ask questions I'd be happy or
show your face and you know emote and
I'll be happy okay so I wanted to talk
today about sort of a curious problem
and one particular example here of of an
experiment we've been doing in feedback
control from pixels but I wanted to
actually have a discussion ideally if
the format works the original idea for
this was a chalk talk and we just have a
few people sitting around and talking it
out but you know we've so far I feel
struggled to do rigorous control from
pixels and I don't know that we really
understand why and so I want to just dig
into that a little bit today and and
talk that out and see if we can start
making some progress okay so what do I
mean by feedback control from pixels
some of you have seen
what I hope is starting to play here
good this is also an experiment in
presentations here so this is Pete and
Lucas showing what it what it what can
be achieved if you actually close a
tighter feedback loop with death camera
in this case right is the ability to
sort of interact with a in this case a
variety of objects and have the richness
of a perceptual input but have the
closed loop update rate that you'd
expect and you can sort of get much more
reactive much more robust feeling
behaviors for admittedly a kind of a
silly task of just trying to flip up a
shoe but we wanted to flip up any old
shoe okay we did an open house not so
long ago for those of you that came it
works well it works sort of surprisingly
almost frustratingly well this was the
you know a non-trivial contact task with
a policy that was reusing visual inputs
primarily commanding and effector
positions and doing non-trivial task
this one was trained with imitation
learning and we'll talk about that but
boy once you start getting these things
to work it feels so exciting and it's
like how could we go back after
something like this to the you know to
the times where we had our robots
basically sensing planning acting and
air balling with their hands my ability
to
is somewhat limited here but this is
just a couple different views of that
wonder if I can make it better well here
we go this is showing that it worked for
surprisingly deformable objects okay and
this is some of the back end of it was
that one of the primary visual
capabilities that enabled this was this
training of correspondences so if you
see the mouse is sort of over on the
left frame a particular point on the Hat
and then just despite the Hat being in
very different configurations before
there's a highlighted area of the
network recognizing the most similar
point on the Hat in the new image okay
and that was the sort of core coordinate
system core framework that we used in
order to close the feedback loop and
I'll tell you a little bit more about
that okay but actually I won't tell you
too much about that because Lucas is
going to defend his thesis fairly soon
and I highly encourage all of you to
attend his talk he's got more results
since then and it's gonna be a great
talk soon to be scheduled and I hope you
tune in but one of the things that I
think Lucas would I know Lucas would say
and that we say together is that it's
sort of frustrating that although that
works very well and it's certainly a
feedback loop and it's certainly you
know doing dynamics and control kind of
things it's really not built on the
fundamental tenets of control in the way
that we'd like right so so there is a
language of control that talks that goes
through models that talks about for
instance I have a plant with control
inputs you with outputs why has some
internal state X I'm just putting these
down so we can have a common vocabulary
through the talk okay with disturbances
coming in as W and noise coming in as B
okay
and then I've got too many computers
here okay and then I'm my goal is to try
to find a controller which I'm going to
call PI today here which ultimately is
going from a history of observations Y
into a control action you right
and I there's many different ways to try
to tell the system which kind of which
PI I'd like to find of the many to
choose from maybe I'm trying to
stabilize an equilibrium maybe I'm
trying to guarantee that my state will
arrive at some goal maybe I've
formulated it out to a control problem
and I'm trying to optimize some
objective subject to constraints right
lots of different ways to think about
that all in the service of trying to
find some policy PI ok but I've never
ever seen any proofs that sort of say if
G happens to be a camera right then then
stop nothing works right in fact one of
the things that drew me to controls in
the first place I remember when I came
to MIT I had been doing a lot of
reinforcement learning and I as I joined
the faculty I started interacting with
the controls faculty even more and he
was so compelling to me that that I felt
like every question I had you know they
could view almost anything as a systems
theoretical problem and they almost
always had thought deeply about the
Systems Theory enough to apply it in a
way to a new problem so pretty much
everything you you could take to Sasha
Emma Gretzky or Pablo or somebody things
like they've already thought about that
in some deep way right and that's like
that was a major impact on what I wanted
to do except cameras like as soon as you
say cameras or computer vision there's
kind of like whoa you know controls
doesn't do cameras that's that's a whole
that's like go to the other department
for that so why is that I want to dig
into that a little bit today right you
could argue that maybe it's because once
we have a camera we start trying to do
more ambitious problems and that maybe
we're in situations where we don't even
know what F is and maybe that's the
fundamental thing that happens but but I
think there's plenty of places where we
know F ok where I know the dynamics but
even if I give you a perfect simulator
let's say of Michell's working about on
a robot trying to tie shoelaces for
instance I we I think we can build a
simulator that is high fidelity enough
of tying shoelaces it's not easy but I
think if we can get things to work with
a simulated camera on a simulated system
over a variety of shoes and simulation
then I'd have confidence it's going to
transition to reality so although there
are certainly some problems where the
model is unknown I think there's enough
problems that you know I that I'm
they're willing to give you F and you
still can't solve the problem if if the
feedback control has to be closed
through a camera so let's talk today I'm
not saying that's not a good problem but
that's today assume that F is given okay
so so what is it then about G and oh you
know if I'm willing to give you a F I'm
willing to tell you G I'm give you a
model for a perfect model for G what is
it about G that makes if it's a camera
makes things so hard right in practice
we're not using the full toolkit of
control it's not that we're designing
pie in these control theoretic ways
we're often doing things that involve
for instance separately training a
perception system with some big deep
network trained for perception only
often and that's often a huge network of
ResNet or something like this and then
we're at tacking on a small policy that
will train with either imitation
learning or deep RL or something like
this afterwards and in many cases I
think they are relatively inefficient to
synthesize maybe they're hard to trust
you know it's not even really clear how
well they work right I would say that
about our own work as well as everything
so I think the next few years is that
we're gonna see some us being able to do
better in this problem and I and I think
the topic you know I'd like to talk
about today is how do if is control
theory up to the challenge can we can we
bring some more tools from rigorous
feedback control to vision based
feedback okay or if not you know that I
hope that if the exploration ends up in
a no you really can't do that then at
least I'll understand in a deeper way
why that was I think there's lots of
things that people say it's like it's
relatively easy to make excuses right
though the image is suddenly very
high-dimensional G is non differentiable
you know the variability and the outputs
is you know there's there's huge complex
distributions over the type of visual
environments yes I mean those are all
true maybe but not always we can find
examples where we want to close a
feedback loop from a camera where
clothes aren't necessarily the case we
could isolate those and start asking are
there any of them actually
breakers and I don't claim to have any
of these answers here this is a
discussion where everybody's muted and
and and you know mostly just me talking
but but if anybody who wants to talk
that's great it's a discussion okay and
in particular maybe one thing I will
offer I've been we've been struggling
for a while to try to find like what's
the limiting case where everything is
simple like is there a problem envision
where you know it's like a linear
Gaussian case or the simple tabular MVP
case or something like that and I kind
of I feel like we found a bit of that in
this onion problem where I'll tell you
about in a second okay so let me set it
up with a little bit of the framework i
if there's there's one thing I sort of
want to communicate is that there are
some tools from controls that we should
be trying that may or may not be on your
radar right so before we call that
pixels the torques did that transition
my alright theory there we go okay I
hope that doesn't have to happen very
often all right before it was called
pixels to torques thus the problem of
designing a controller that goes takes
some observations why and that's a whole
sequence of observations in this block
diagram there's a signal Y coming in and
you're gonna put out your job is to put
out a signal u coming out as a
controller and construct constructing a
controller of this form which
potentially can use all of the
information of previous wise that's
called the output feedback problem okay
and the most common recipe that I think
we all know is this idea of doing state
estimation and then feedback a full
state feedback right so in the
particular case well in the particular
case of linear quadratic Gaussian
problems the separation principle is
rigorously justified but we use it all
the time right and and I would argue the
dough it's um well it's so compelling
and they can in our work on on legged
robots you know I felt like often we had
to stop relying on full state feedback
that we weren't able to you know that
the fundamental problems in the in the
tasks were the fact that we were too
heavily dependent on our state estimate
but it's just so tempting to build a
better state estimator or spend a little
bit more money on
ímu or whatever and to write these
systems in this form where you writing a
state estimate it is some recursive
filter of a form like this and in the
case of a linear quadratic Gaussian it's
the common filter gains are 4l okay and
then I'm building a controller that's
just as if I had perfect sensing I can I
could build a controller that knows the
state X and makes those decisions all
right so is that it is that the did it
really not update okay so is this the
fundamental problem is the reason that
we're not good I I assume people agree
that we're not good at doing feedback
control through cameras we're getting
better at it but maybe the fundamental
problem the reason that we haven't done
it with rigorous control is that we
shouldn't be doing the state estimation
step but maybe you know xn is often not
observable in problems like this thing
that we're the visual motor examples I
was showing in the first few slides
maybe not all the states are even task
relevant so observing it could be a
false goal having to having to try to
come up with estimates of the entire
state vector X right there's a bunch of
state in my image that was just not
relevant for instance like what is the
state if I'm trying to if I'm trying to
program a robot to to make a salad you
know what's the state of the salad or my
other favorite example here is you know
let's say I'm I'm I've got a button my
shirt I'm gonna build a controller that
should button my shirt or button all
shirts right its state one of step one
is to estimate the full state of my
shirt you know it feels like probably
we've gone down a rabbit hole there so
maybe that's the fundamental problem
okay so of course many people on the
call will say it's it's a bomb DP of
course we shouldn't be estimating the
state there's a more general framework
that can capture this and in fact it may
be in the full glory we should be
actually estimating an entire
distribution over possible States and
then be doing feedback that can
understand that distribution right right
this should be general enough to work I
agree I in fact I think this has led to
lots of insights about what the optimal
controllers could be and these more
complicated setting but it's hard to get
tractable results especially in the
forms I'm writing here and the problems
we're talking about there roads
high dimensional continuous state
continuous action and DePandi's so
that's maybe hard going so there's a
subtle difference I want to make I want
to make sure we're pointing out here
which I think people know and talk about
this in slightly different language but
let me use it in my language right so
there's a difference between the
controller state in the estimated plant
state right so having state in your
controller can be good it's an efficient
way to encode a dynamic controller right
but I think what's starting to happen is
that the state of the world as I would
be parametrized it for instance to write
the equations of motion for simulation
right I'll call that the lagrangian
state right is probably not the right
state space for us to do to have in our
controller if I'm trying to program a
robot to to make pasta or something then
probably the I don't need to estimate
the entire continuum state of every
noodle and the contact mechanics in
which they're they're interacting I just
want to know if the noodles are
basically in the bowl or they're
basically in the pot right um so that's
the question of state representation
learning and other things that we we
talked a lot about that Phil talked
about when in the first seminar I guess
now here's maybe a more you know
optimistic view I think that should just
be a control problem right so that's all
ideally perception should just be the
output of my controller synthesis so
I've got strong enough control synthesis
then the idea of what's the right latent
state should be whatever it's whatever
its required to find this transition
from why all the way to you right you
know we and we're doing this already a
little bit in practice our visual motor
policies did have internal state you
know we trained via imitation learning
LS TM networks in order to do this feed
Lucas did and they were encoding some
internal state and you could sort of
watch and try to guess what those states
were meant you know after the fact like
there's some that seemed to be one just
seemed to be time which was ridiculous
but you know another one that was sort
of seemed to be indicating that you're
in a different phase of motions they
would sort of turn on and turn off okay
so having internal state in your
controller is a good thing I think but
expecting the internal state of your
controller to be the full state of the
mechanics of the world is probably not a
good goal and I think the you know the
problems in computer of using you know
that come up when you have the
opportunity to do crazy things with a
camera maybe show those off okay so we
have done things which use you know
which try to do good control with less
with you know with less just to show off
that even hand designed or hand labeled
and then trained in the deep network
features can do non-trivial tasks over a
variety of objects so this was another
way that we sort of used in an
intermediate representation from a
perception system to do a variety of
tasks it was called K Pam okay so key
point affordances and the idea was
actually you don't need to know the full
state of the world you don't need to
know the state of the shoe in order to
program a lot of meaningful tasks or if
you wanted to do something like put a
mug on the rack then actually just
knowing a few key points right and it
turns out key points are exactly the in
the same way that people use in in
perception right in computer vision
training a network to put out a few key
points which are serve ordinances of the
task they don't necessarily have to come
they certainly don't have to completely
specify the geometry of the problem they
don't necessarily say anything about the
dynamics of the problem but they're
enough for me to say I'd like the goal
is to find key points in the current
image transform them through some rooms
of transformation which I can do with an
end effector command on my robot in
order to make that yellow key point end
up on that you know on that rack and
that actually turns out to be one hand
designed intermediate representation
which is not the full Lagrangian State
and of course people have lots of other
proposals for this about latent state
variables and state representations that
could potentially work you know that
particular work was still quite limited
because we didn't have any model of the
dynamics of the key point how those
would go forward but way he continues to
surprise he can make now just with the
key point base affordances thinking
about end you know forces at the key
points he's actually able to make K Pam
do pretty
tasks with contact rich assembly of
Legos and plugs and erasers and
everything like this so so for the
places where the dynamics are relatively
easy or they can be rigidly attached to
to a hand you can actually do a
surprising amount of cool things with
them okay so so state estimation is bad
you know the full state estimation is
bad belief space planning seems hard
so people are sort of saying let's do
less or I'll just take the direct
approach right I'm gonna just learn a
map directly from from Y to you right
and so maybe an alternative that avoids
the complexity of naming a state space
it avoids the complexity about reasoning
over a complete belief distribution I
think the right way to think about it
for today is that we're we're gonna
search over a suboptimal class of
policies the full opt the optimal
policies for most of these problems are
full beliefs based plans okay but if I
write down a class a simpler class of
policies and can search over them
directly then then maybe that's a way to
get around this right and we've seen a
lot of success including the first
slides a lot of good work out in the
world vrl and invitation learning but I
want to stop for a second and say so I
so that's a good idea I think that's an
interesting idea to do Policy Research
directly but all too quickly people say
okay we're gonna do policy search and
then they say the only way we can do
this is by either invitation learning or
blackbox optimization and brl and I
think that's throwing a little out a
little bit too much out I agree that an
interesting formulation is to just
search over a simpler restricted class
of policies even if that restriction is
a deep neural network okay but that
doesn't mean you have to give up on the
fact that you might know the equations
of motion you might know the renderer
for instance you might be able to to to
do lots more things than we would
traditionally do with blackbox
optimization and there are approaches to
model-based control that do not assume
the separation principle okay so if you
take away one thing maybe do things this
would be the first of two things I
really want you to take away from the
talk is that there is there are ID
of doing joint you know you know
integrated perception and control from
control and we should be trying these
okay
so what does it mean to do model-based
policy search okay so so simples of a
warm up example here is if I have a
linear system and I want to do linear
feedback control okay for instance when
when we're writing a problem like a
linear quadratic regulator the optimal
controller we know takes this form u
equals negative KX okay but we typically
don't get K by policy search right we
get it by calling lqr in matlab which is
solving the riccati equation in an
algebraic riccati equation it's
indirectly it's computing the cost to go
and then it's computing K from that and
in fact we sort of know that's that the
set of stabilize that K is a you know as
parameterize like this is a bad set to
search over okay in the sense that
regardless of your objective I haven't
even named an objective this is of
course related to lqr but even if you
just look at the the set of cave which
stabilizes this system right then that
is actually a non convex set so almost
any objective you put on top of that
you're gonna have a non convex
optimization now there are some cool
work that just people fairly recent by
mary zell you know the showed for
instance that you can still find with
direct gradient descent on the
parameters k the optimal policy in many
cases with probability one but I don't
think that holds in a more general I
mean we're not sure how far that holds
that that was a recent result of an old
idea so I think we have reason to be a
little suspicious of this of searching
directly in K and we don't have to
search directly in K because we actually
know better parameterizations and
control has known these for a long time
okay so if you have a system that is
I've added in disturbances here because
that's going to be a useful in our
parameterization but basically the same
system that we had before
instead of writing you equals negative K
you N equals negative K X n I'm going to
prefer to write it actually as
disturbance based feedback so I'll do a
K based on the initial condition X but
other
I'm gonna just multiply look for K and K
matrices that are multiplied by this
additive input W and the idea is if you
think about it that phenomenal you know
from any one initial X 0 I could
actually have the the rolled out
solution if there was really no
disturbances that I could encode what
you should be for the rest of time just
in this K matrix right it's a it's a
time varying K matrix that goes goes out
but I could just basically have my
entire solution into this one K matrix
the only way that that has to change is
by responding to these disturbances so
this is a complete parameterization it
can perfectly recover the original
parameterization but the big difference
here is while multiplying by xn puts the
recursion back in and you get K times K
times K times K for for anything like
this and makes it not intend next this
is additive and K and the most of the
objectives we like you know l1
objectives lqr objectives other things
they are they are convex in this
parameterization okay this is you know
it's a special case of the eula
parameterization it's also known as
system it's also used in system level
synthesis which adds an additional
system level synthesis is kind of like
the direct transcription versus shooting
approach to this if for those of you
that have heard they're taken under
actuated for instance okay it's they're
very related okay so there's a better
parameterization and actually you know
we should probably be using that in
reinforcement learning too I feel like
we've known this for a while maybe this
was this was April 2011 and was maybe
after it was cool and before it was cool
so poorly timed but but you know we
thought we'd we showed that you know
you'll appear anders are just obviously
a better parameterization for for
reinforcement learning tools too and i
think they we did mostly the linear case
in that paper too but actually the Euler
parameters are a nice choice for even
nonlinear plants and I think there's
just some core lessons from controls
that we should be using in our up ok it
turns out for the purposes here it turns
out that the Euler parameters also allow
you to formulate a policy search that
does integrated perception and control
and I I'm almost embarrassed to say this
because I I feel like it's been hiding
in plain sight and I did we should have
been working on this for years but um
but but here we go so so for the output
feedback case here let's think about it
for the linear systems to begin with
okay if you write down this same sort of
disturbance based feedback which is this
now it's got you you have feedback both
on your disturbances and your noise here
okay which you can measure a per the
fact it's always using it's always
causal on those variables then actually
the objectives like the lqr objective
are still convex in K okay and the cool
thing is that this is a these are finite
horizon formulations typically but for
if you put in you know that the the W's
come in as a Gaussian iid and V comes in
Gaussian iid then actually even though
this is jointly designing the filter
gains and the control games it's
actually recovering the common filter
okay so but it's applicable in cases
where I mean where W might not be
Gaussian iid you can you can write more
rich specifications Audra
in our group has been thinking thinking
about lots of different ways that you
can leverage that fact and do more with
with sort of these sort of
parameterizations okay so that's
observation one is that they're just tip
there's a pay there's a lesson to learn
from control I think which is that there
are better parameterizations if you're
going to do policy search okay
Zadro in particular has been pointing
out you know because that the equations
I wrote just now still declared that we
need to effectively you know declare a
state right and and I said I don't know
what the state of my salad or my shirt
is so I don't want to I don't want to do
that the same kind of disturbance based
feedback parameterizations can also work
for a rx models right so a rx models the
linear a rx models with being of this
form where you're just predicting your
next y as a some combination of your
previous Y's and some enzyme your
previous use plus some error terms that
we've called error now and instead of
noise
or disturbances but it plays the same
role okay and when people say I'm okay
my image is my state and in RL that
think this this is what I hear when they
say that is is that they're actually
just doing a one-step ARX model and
that's good and that makes sense right
and more generally everybody knows we
could do a history of images and control
inputs as the state and I think that is
justified but it puts you into a
slightly different class of models and I
think we should be a little careful
calling it state because you can you can
trip yourself up with them okay there
are no natural extensions of this to
none yeah there's a question from Luca
carloni okay awesome Luca
okay I mean myself just quick question
if you go to the previous slide you said
it the noise terms are known up was a
posteriori but how do you get the noise
in the previous one yeah
I'm navigating here yes because it was
supposed really for retrieving the noise
essentially mean the state could be
known right right so I thought about it
more and do you agree that it's easy in
the state feedback case yeah if you have
a nice to meet you guys made the noise
as well but yes so so afterwards if you
observe your next xn then you can just
subtract it out in the past and and
figure out what W was the difference
between my repeated measurements I
believe that's all still true in the
output case and that you can after the
fact by using wise you know that the
next step you can figure out what V must
have been and then again you can figure
out what W must have been it's all done
sort of after the fact you see the error
in your prediction in the year and your
in your observations but I guess my
comment is that you have a feeling of
the state as a you know as a byproduct
of feeling that the noise you're feeling
of the state as well ah yes I'm so
absolutely they are equivalent they are
able to do the same things and this one
in this in this particular slide we
still have to require state space which
i think is a limitation but the
parameterization is better in this case
okay just wanted to make sure it was not
missing any thanks I I appreciate the
question you know interaction is good
okay so and this is the case where we
don't need to declare a state and you
can just measure in the unprecedented
manner you know based on what I've
observed and what my prediction was I
could figure out what he must have been
right if I had had my model exactly I
would have gotten one why I measure a
different why the difference is e and
there and you move from there okay so um
so I guess there are natural ways to
apply I think the obvious questions it
should be in your head now is like okay
well this is a linear model you start
talking about cameras what the heck you
know you can't do linear models for
cameras right well it actually I just
want to check that like the step 1
let's just make sure we can't do ok with
linear models in cameras ok so there is
some history of doing linear ARX models
in computer vision right there's some
precedents based on some work and some
older work and dynamics textures and
rigid motors and segmentation video in
painting there's examples where people
actually fit linear ARX models to for
instance you know if they if they want
to put a you know the waves on in the
ocean in the background or something you
can there are there cases where that has
actually already worked but let's get to
a manipulation example here so um Terry
whom I don't know if he's on the call
here Terry sue joined the lab last year
and and I've been I was saying you know
we're to depend on stated feedback we
got to pick a problem right off the bat
you know you're you're you're fresh
you're not corrupted yeah we're gonna
start with a new problem where state is
not an option okay and so we said let's
let's think about chopping an onion okay
so what's the state space of the onion
right and is it changing every time
someone makes a cut is the number the
dimension of my state like my classical
controls approach to this would be I'm
gonna write a hybrid model which every
time the knife comes down it does a jump
and a reset and it does actually does
like hundreds of them on every cut and
that means every controller has to be
designed separately for every one of
those cuts even though my god it's the
third cut on the fourth cut must be
pretty much the same right so this seems
like a place where there's no risk of us
trying to estimate the full state of the
of the lagrangean state of the world so
we kind of we're not going to get sucked
into the state estimation
local minima let's try to do image based
control on that okay and then so we
thought about how to actually turn that
into a research problem and so lesson
one was that you shouldn't use onions
carrots are way better because I don't
know the lab smells better you can see
them better on a camera they like last
longer so so I still like the item yeah
I still called the onion problem
probably I should update myself but but
I recommend carrots okay
second step second idea probably you
know handing a deep network and a big
sharp knife to a robot maybe that's not
where we start maybe we should start
with like the second part of that where
the the things have been chopped and we
just want to move them around the
cutting board that's already pretty
impressive let's do that part of it
first okay so so now here's the setup
we've got a cuca with a soft you know
blade that can move around in the in the
vertical axis here with a camera looking
down from above taking pictures of
bright orange carrots and the goal is to
to make it more of a controls problem is
to drive those carrots into some target
set now my goal was to actually use the
iPad here let me just try one last
experiment on that okay so um the
objective here hopefully this is going
to work the objective is to it's really
poor writing but okay you know move all
carrot pieces into a target set okay I
observations by Y right yn which is
actually going to be it I'm going to use
two notations I'll use the image at time
n image or sometimes we're actually
going to write subscript K okay and then
we're also talked about
why is the vectorized image right so if
I just take that whole image and I
squish it down into a vector vectorized
image okay and that's it we're gonna
take those are my observations my
control inputs U of n are gonna be a
push in the on the system so I'm going
to parameterize that in the same way
that a lot of the good work on sort of
pushing and the like had then have
working in robotics I'm gonna just
parameterize that with four numbers this
would be like the initial location of
the pusher in the in the image and the
final location of the pusher in the
image okay and I'm gonna assume that
that's just a constant size paddle and
it's gonna move along this and I'm gonna
write the dynamics of the system
ultimately in the time of this push so
so basically I'm not gonna model the
dynamics of the push I'm gonna say the a
single action is an it's an entire push
of a constant velocity and I'm going to
look at the image beforehand and the
image after hand after after the fact
okay the way that we're gonna write you
know my goal is to somehow write a
single controller which is going to be
you know you and is some PI of Y n now
we could in general take the entire
history of Y but I think for this
problem there is not them it's a
quasi-static problem so the current
image contains everything we need to
know and so I think it's reasonable to
look under over this and teri thought he
was most natural to think of this in
terms of Lyapunov functions okay so
we're actually gonna write down a
Lyapunov function which is somehow
specifies my task okay I'll the a panade
function in this case I could write it
in a couple of different ways but if I
want to take some so my function of Y or
my image Y and I'm gonna take us I'm
gonna basically measure the distance
between all of the nonzero pixels in the
image with the from the target set okay
so it's going to be a sum of the
distances between all of my nonzero
pixels which would be my Y I an
indicator sort of function of Y I and my
target set which I even typically write
s okay and I'm going to make a picture
of that soon I think
flipping back and forth is gonna stink
but let me I'll try to do it minimally
but here we go
so the original image here from the
camera looking down the board frame
I've now just cropped it into the board
frame thresholded it I've got my dis my
now black and white or my grayscale
image I and that I can vectorize it into
a 1024 vector Y the notation hopefully
is clear yeah okay now this is my Miley
upon all function candidate here Teri's
Lyapunov function candidate here I've
got a bunch of points a bunch of carrot
pieces that we can indicate anything
that's nonzero in there in our image and
I'm gonna take the distance between some
some every one of those and the target
set the distance inside there zero of
course and and actually I just to take a
moment to say that the target set is
almost arbitrary it doesn't have to be
we read it made a simple square here it
doesn't have to be a square but it has
to be something for which it admits a
control the optimum function so I'm
gonna I'm gonna say you're not allowed
to make a target set it doesn't have to
be convex but but it's not allowed to
have some weird corners or something
such that my paddle I must have the
property that if I I can take a single
there exists a single action of my
paddle that will make anything that's
outside the set go into the set without
pushing sets and things that are inside
the set out okay sort of a detail there
now this is a this is a horribly or not
horribly this is a non convex function
non convex problem how do we go go
downhill and the first observation here
is actually that if you write the
instead of writing this in sort of the
space of the of the different carrot
pieces that exists XY coordinates if you
write it in image coordinates then
actually the Lyapunov function becomes a
linear function okay so I can just take
the picture of my I could take sort of
the sign distance I can pre-compute the
distance of every point on the picture
from the target set and then I can
the current image and just had a marred
product it with with that distance and
suddenly evaluating Miley a panade
function becomes just a linear function
so that's already kind of cool that's
sort of some of the first time somehow
the image coordinates it was actually
easier than this the state coordinates
of that and hopefully we're hoping for
more here okay all right so the next
step is to try to learn a model so we
wanted to learn a model from one image
to the next image and we took a page
from the visualforce site we wanted to
try sort of the to replicate the results
here and in fact the story was was
pretty funny um you know Terry came to
the lab and started working on you know
this deep visual foresight approach he
had a deep network and his first the
first meeting was like this is awesome
the network is you know my training
error is like super low it's clearly
solving the task image to image
prediction it just works right and then
he goes to put it on the real robot and
this is what it did starts pushing doing
well oh so on closer inspection although
the the loss function had gone down very
beautifully there was some sort of
maddening behavior so you could say we
didn't have the right loss function and
maybe we don't but but it was sort of
incredibly frustrating to not be able to
encode some basic ideas into the network
to somehow say for instance carrots
don't evaporate Karin's carrots don't
appear out of nothing there is you know
you should not hallucinate new carrots
and and the conversation was roughly
like you know it's frustrating because
it's we're struggling to figure out how
to put that into the network it would
have been simple to put that into a
linear model okay so the so we stopped
and said okay what would how well would
a linear model do for predicting image
to image translation on the carrots okay
and it turns out that it's not crazy and
the reason it's not care crazy is that
here's a simple thought experiment okay
so so for a particular push let's say in
this in the limiting case I have just
one carrot in the middle of my of my
image and I'm going
do a push then that's the the operation
of pushing is effectively relocating
that carrot to the new spot that is
perfectly encoded as a permutation
matrix I've just per muted the location
of my of my one carrot piece to a new
location okay now let's say I have a lot
of different care location carrot pieces
if they in the case in this with the
assumption that they all move
independently it's still basically a
permutation matrix okay so it's I can
just say and what to push anything that
started off in this cell ended up in
this cell and it didn't start up in this
cell ended up in this cell right and
actually an image coordinates the linear
map didn't feel as restrictive as we
thought it would now asking for a
perfect permutation matrix was too much
because actually some carrot pieces
bunch up and they end up being on top of
each other and and other things so but
we what we did end up asking for was
that the there's a well what we thought
we wanted to do here was the two of
course say that you can't create or
destroy carrots so there's an
element-wise positivity constraint and
then there's a column sum constraint
which is again this sort of mass
preservation constraint okay that ends
up of course you can do now linear you
know you can do least constraint least
squares which in this case would be a
quadratic program but because these are
big images so the you know if YK is my
image you know to the map a for to the
next i ke even for a single push is
already 1024 by 1024 so it gets to be
too big to be useful for QPS especially
they have to do lots of different
actions so we ended up dropping the
column some constraint and that way you
can solve each row independently okay
and just decompose the problem into a
lot of smaller problems and then we
actually checked after the fact and
found that in fact even without that
column some constraint just to match the
data and have the positivity constraint
we ended up the columns typically sum to
approximately one okay
so we saw in that case we can solve each
row independently smaller optimization
problems but that's only for a
particular action to do a little bit
better we tried to transform all of our
data into the action frame this is a
standard sort of trick in the in the
visual foresight type worlds so you
could take any particular swipe of a
constant length and and then basically
rotate translate fill in the background
come up with you know with your masks
and and basically reuse lots of your
data in the coordinates of the push in
order to for a lot of different you know
with a lot of relatively small amount of
data we could train a good a and we
actually only have to think about is
being disparate eyes down the length of
the push and not the position
orientations and everything like that
that dramatically reduced the amount of
training data it was something we could
have done and we did do eventually with
the deep network approach but we it was
occurred to us when we were thinking
about it in the simple case and then
what was surprising was with the amount
of data we tried actually the
least-squares models worked better than
the deep foresight models now that is
absolutely not me saying that linear
models are better than the deep models
because deep models are a superset right
I think but I'm just saying that in the
amount of data we had here there was a
sort of a stronger inductive bias on the
you know on the linear the constraints
we were able to put on the linear system
that the non-negative least squares is
the one that did the best was able to
match the data you know with a lot less
data actually than the deep visual for
site the original being the one just
doing it image to image the this is the
one with the deep visual foresight also
given the benefit of doing the affine
transformations for data augmentation
okay and you know it actually worked
surprisingly well so you know you could
you could see the the linear map here
the actual transition from one thing to
the other the linear map would predict
you know nicely and the the trained
tainted training and test error played
out okay so you know each row of a is an
image so you can actually sort of go
through and look at the the same way
people look at the receptive
filled of a neuron in the middle you can
you can go and inspect how it learned in
the in the middle and and and visualize
it and it it's pretty compelling it to
seem to do all the right stuff okay so
this is now the you know we expect it to
effectively learn the identity map until
it goes here this is the pre image of
the pixels okay so the pre image of the
pixels up to here is just the pixel
because it's the identity map and then
when you get to the part where the push
happens there is nothing in the pre
image for the one that's inside the
length of the push but the ones that
were at the boundary of the push have
the whole sort of pre images the whole
stretch of places that could have landed
it - so it was doing intuitively the
right thing okay so we put this back on
the cuca to run the experiment here we
go
he works surprisingly well with the the
linear model okay and ultimately we
evaluated ourselves based on both the
you know the image to image prediction
model but also the closed loop
performance of the control around the
task okay so the closed loop performance
they all did ultimately fairly well
except for the maddening case of getting
stuck on a hallucinating carrot which
was which the original deep visual
foresight model was subject to okay all
right so I know I'm running out of time
sorry for the awkwardness with the would
zoom here so the obvious question is how
far can you get with this and I think we
know effect we came from a group we had
a group meeting like two hours ago in
which Terry presented again and he
basically spent a half hour saying it's
never gonna do more than the onions I
said Terry I'm about to present your
work to a whole bunch of people and
you're telling me you know but I think
we know the limitations of this I think
for now you know we now have I think a
clearer understanding I mean this is
very much like sort of the Koopman
version if you will that it's it's a
nice story where if you lift the the
pixel space is sort of lifting us into
this higher dimensional space where
under the specific assumption that those
pixels move independently the linear map
does a perfect job in in summarizing it
okay as soon as you break that
independence assumption if those if it's
a if those bodies are connected to a
rigid body then we're then we're in a
different regime right where the linear
we don't expect the linear model to do
well and actually well you could you
could lift it up even to a higher
dimensional space and expect linear
models to do well but in the image space
you only expect it to do well in the
limit where the pixels move
independently having said that you can
use robust control types ideas for
instance to fit approximate ARX linear
models and solder has been doing this
nicely to try to do some of the classic
tasks in image based feedback okay and
it's a story that's still emerging yeah
subjects are sent me more material this
morning that I was able to split slide
into this talk sorry sorry
but you know this is an emerging story
of trying to understand how well linear
robust control can actually do in images
and I feel like we finally have at least
sort of the what's the simple case well
next time we we say then what was the
simple case where we're late we're
vision based control is easy I think the
answer is now when the pixels move
independently and actually carrots are
the case are there an instantiation of
that case that I don't know I mean it
wasn't it's not my life dream but it's
you know it's still it's a real problem
right so and the cool thing and I was
planning to do more on the way on the
board here but the cool thing is that
there's a natural there's a whole bunch
of tools that open up to us from the
controls community if we're in this
space especially of linear models but
even I think the next steps are going to
be looking beyond linear models
piecewise a lot piecewise models
nonlinear models but there's natural
extensions of the tools we know and love
for instance solder is looking at
riccati equations for a rx models in
this space and looking at leonov
functions which now take it's a
quadratic function of the history of
images in the history of inputs and it
predicts you could then predict that
you'd like to go downhill on that
function and you know similarly you
might be able to check you can might be
able to monitor yourself and if things
are starting to go wrong with your
vision based control you could you could
detect that quickly if you're they often
a function starts going uphill okay so I
think we're at the beginning of this act
of this really nice sort of opportunity
to think more rigorously about the
vision based control stuff okay so
obligatory let's move the carrots into
an MIT pattern okay but I basically wrap
up there and take any questions 2:59
have you got them for one or two
questions
I'm happy to stay in a 5-time but please
ask anybody
yeah I drove independently in the task
for it to be the linear controller works
do you have like an example of one where
the I mean that might have been one on
the screen but like where one where
basically the fact that the pixels don't
live independently causes the controller
just you can't represent let me say it
back to the so I think a linear image to
image you know limit linear forward
model can describe the data well it's
not a statement about whether a linear
control can work well but it's a
statement about whether a linear
prediction can work well and I think as
soon as you get so we already see it a
little bit because in fact when carrots
do bunch up they will push each other
around and that is a violation of our
basic model that isn't captured but it
but it's captured well enough to
accomplish the task in this case yeah
yeah yeah and if you were to now try to
push a big Domino's sugar box for
instance then then you'd see that that's
quickly violated because there's a rigid
body assumption that a rigid body
constraint that holds those pixels
together right quick question yeah yeah
look yes I completely agree it's all
very cool talk I really liked it so I
completely own the idea that somehow
like you know the representation for the
robot should be like sanyasis driven
right I was wondering if you thought
about it's clear that you know in the
case in which have a single very
specific task you can engineer like a
representation which is more clever for
example I can get linearity out of the
representation but I'm wondering if you
sir considering many many tasks you know
a robot has to do general-purpose
actions in India for example I wonder if
at the end of the day to support a
variety of tasks you end up going back
to a full states that you have to build
in other words like it seems that if you
have to do many many tasks at the end of
the day thinking about the three
presentation objects end up being like
the more clever and general formulation
for the state if you think that's an
awesome question I completely understand
your question and I've been thinking
along these I've been questioning myself
in that way I think actually so I think
the diversity of tasks that we want our
robots to do is far greater than
anything we're trying right now but it's
actually probably not the case that we
have to like I don't think I ever need
to know the state of the salad right I
mean I like I'm never gonna ask the
robot to have to distinguish between
crouton xiii and crouton 14 or whatever
right so I think there's you know
there's something there that we probably
don't need the full state ever the other
the more subtle thing though I would
actually I'm of the opinion today ask me
again in a few months but I'm in of the
opinion today that asking for one task
to work it might be a harder problem
than trying to get something that works
for a diversity of tasks up to some
point you're right that at some point
maybe if you're trying to do too crazy
of a set of tasks so you have to know
everything about the system but I think
we're almost possibly making the problem
harder by having a very narrow
specification and maybe my analogy for
that is if you look at even like
trajectory optimization versus a
stochastic trajectory optimization if I
ask my if I give myself a particular
system a particular initial condition
you try to solve the problem there's
lots of local minima often and if you
say that the that the same controller
the same control inputs have to work for
many initial conditions in a stochastic
problem then actually all the non robust
solutions tend to fall away and the
optimization gets a little bit better
and I I wonder if that's gonna happen in
the task place too that if you ask the
system to do you know many similar
things it might actually help us discard
the the quirky solutions that happened
to work in a narrow case but didn't make
sense and it might actually make those
the problem better so I'm hoping to look
into that and sort of first step Thanks
any other question okay then let's leave
it here since it's past 3:00 p.m. and so
let's dress for every possible quick
question about that there you go
I think they might have been Katie yeah
yeah go ahead Katie
I might be a little unstable no it's
just a question about the 3d nature of
the problem the pixels that you take do
you see any shadowing is there you
talked about I think you talked about
calling the something to all sort of the
containing of everybody having their own
location that isn't stacking on top of
each other isn't occupying the same
pixels that kind of information do you
think you could actually tease out some
information from the shading of the
pixels to give you that more 3d
information about interactions is it
it's a it's a good question I because so
many are things still working even when
you do those interactions kind of going
back to questions to draw the most
questions building off of that oh yeah
yeah I think so so it's I think it is
actually doing that to some extent which
is why actually we couldn't ask for it
to be a permutation matrix so having the
row sum to one didn't actually work
because if because it almost every case
in fact any if there's two pixels or two
carats on the same line then they will
both land up in the same place and we
have to allow the system to have a value
that was greater than this you know
somehow it was the sum of those two two
value so we actually saw that in the
data that we had to allow that the and
it supports that but the thing that
really can't support is that if the the
interaction between those two particles
caused any one of them to land in a
different cell that's the part that the
linear model cannot capture right so and
summing them up is fine and to some
extent the 3d is roughly a sum I mean it
we're approximating it it's the data is
approximating it that way good you also
had the question yes I had a question on
it seems that in this specific case with
the carrots you've more or less chosen a
state representation such that a linear
model can predict it well but you also
talked earlier about learning aren't not
state representation
maybe but some sort of representation
you also talked about a representation
being sufficient to do the task as a way
of learning it do you have any ideas
about combining these two sort of things
maybe awesome
so I mean in my mind the ARX model is is
powerful in the sense you don't have to
name its data representation it's
massively inefficient in terms of data
right and then the size of the matrices
we're learning are enormous right which
means we're I mean I could say we're
over parametrized and maybe that's it
makes it sound good but but we're also
we're also just need tons of data to
feed it so there if we to the extent
that linear models work well there's
there are well-known techniques for
output feedback system identification
I'll put system identification you know
input output system identification that
where you do subspace identification and
there's like rigorous approaches to do
it defining the latent space right so I
think there's a big question of I think
we know that the linear models are gonna
taper in there and what they can handle
but there's there's a whole world of
linear robust models where you're saying
it's not it's linear plus or minus some
bits and I don't know if we I don't know
how much to expect out of that yet but I
I long for the ability to bring in some
of the more mature models in system ID
and state representation tools that we
have from Systems Theory into the
problem of like images so there is a
route there is a route forward Thanks
the question I think you're muted idea
yeah yeah got it across really
interesting so you were talking about
potentially acquiring representations
that were not representing the full
state as in for certain tasks we don't
need to keep track of different things
but obvious if we want our robots to
become very general maybe there will be
tasks that they encounter someday that
require them to have access to part of
the state that they weren't previously
modeling so you think there's any room
for sort of a goal conditioned state
representation is that something that's
worth addressing or is that just too far
down the road
oh no no now's the time no better time
then maybe is yesterday today is better
than yesterday but
yeah I think that that's one that I
really like to think about I don't know
exactly I've got yeah so when I'm
walking through this room right I
probably don't think at all the the the
pencil on my desk doesn't enter my state
representation at all until I go to
write something down and then suddenly
boom it's like the most important part
of my state and and how do you build
these like hierarchical models this is
something that that you know Leslie and
tamasa
talked about for a long time and you
know I am very interested in that and
and how do you flexibly go up and down
those hierarchies yeah it's hard but I
but I think now is the time you know I
think actually fir'aun's but you know
his work I'm trying to build modular
prediction models and in taking neural
networks and plugging them in and out
and things like this you know which
which he build on Jacobs stuff Jacob
Andre stuff and so I think there's a lot
of people probably on this call that
have really good ideas about that and I
do think it's the time and it's it's a
good problem cool thank you
cool let's do it here now
next seminar will be Wednesday next
Wednesday at 2 p.m. for the next two
weeks we'll also have the web seminars
on Wednesday thanks for all for coming
stay well yeah right

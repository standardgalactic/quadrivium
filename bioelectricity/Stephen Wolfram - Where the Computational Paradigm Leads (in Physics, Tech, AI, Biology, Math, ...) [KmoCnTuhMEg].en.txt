you know I first heard about the Herz
Foundation probably 45 years ago and I
there were all kinds of interesting
people that I knew and I happened to
hear oh they had some Herz Fellowship
thing I hadn't really quite put all of
it together until I think sometime after
Alis and I got married 30 years ago now
I kind of leared she was also one of
these hurts fellow
people anyway very nice to to have a
chance to talk to you all well I thought
what I would try and do is talk about
kind of a 50-year span of kind of my
adventures in kind of the computational
Paradigm some of the things that have
happened and I think are are going to
happen and hopefully I'll get to
something that I a result that I just
got yesterday so we'll see how whether I
can get to that let me start with
something um long time ago I was a kid
in england got interest in physics when
I was 1011 years old and uh sometime in
uh June of 1972 I bought myself a book
about statistical physics and I got very
interested in this and I really like the
cover of the book which is an
illustration of kind of the uh molecules
idealized molecules bouncing around
illustrating second law of
Thermodynamics and I thought I should
really try and understand this and I
should try and uh my first big effort in
Computing was um trying to uh simulate
that picture I did it on a on this
computer here was about the size of a
desk um wasn't successful actually um I
I didn't manage to reproduce that
picture as I leared a decade later I
actually had produced something more
interesting but perhaps one thing that's
of Hurtz relevance going back to this
picture here is the origin of that
actual picture on the book the picture
was made in 1960 at Lawrence lmore lab
um by uh a chap called Bernie aler who
had been recruited to Lawrence lmore in
1955 by Edward Teller to work on um kind
of high density materials and so on and
actually the computer I have a picture
here of the computer on which that book
uh cover was made in those days the the
desk came with a computer that was a
thing called the Lark the Livermore
Advanced research computer and that was
I think I have another picture here that
that's a picture of the um people who
who made that picture with physicist
with ties and so on which was the case
in those days in any case the um uh so
got me started on on being interested in
in started using computers to try and
understand things about science and uh I
worked on particle physics things like
that I was kind of confused by why other
people didn't use computers to do these
kinds of things I started using
computers to do kind of mathematical
calculations uh because I thought the
mathematical calculations were kind of
boring and mechanical and they should be
delegated to computers uh kind of one of
the things I learned at that time was uh
you know people there are tools but
people don't always use them if you do
use them you have great leverage well
after studying particle physics and
cosmology and so on in the in the 1970s
late 1970s which was a great time
because that was it was sort of the
golden age of of particle physics and
Quantum field Theory and so on I got
interested in sort of the general
question of how comp complexity arises
in in the world and uh uh that got me
interested in kind of well what how can
you make models of things like that and
so I tried using sort of mathematical
equations things like that didn't work
very well I started thinking you know
what what is the most General kind of
medium that we can use to make models of
things and started thinking about sort
of if you just specify rules for
something what do those rules specify
that it should do and thinking about
simple programs as kind of the basis for
models of things so I started studying
these things get called cellular
autometer um and the kind of the idea is
that that thing at the bottom of the
screen is kind of like a very simple
program that just says if you have these
uh these cells and these particular
Arrangements this is what you do next
you might say well it's a simple program
it makes a simple pattern you go on you
try another program makes a simple
pattern you try another program also
makes a a slightly more intricate but
still ultimately quite regular pattern
well then back in 1981 or so uh I tried
the obvious computer experiment which
was just try running all possible rules
of this kind and see what they do well
the many of them do rather simple things
but the big surprise and kind of my
all-time favorite science Discovery is
Rule 30 um it has that little uh program
at the bottom but you started off from
just one black cell and it makes this
quite elaborate pattern keep going for a
while it produces a pattern that for
many practical purposes looks completely
random you look at the center column in
this pattern it seems completely random
in fact we used it for many years as the
zero random number generator in in
mathematic moram language well it's
really a surprising thing it's sort of a
secret that nature seems to have that
you can have a very simple program and
yet it does complicated things it's
something out in the sort of
computational universe this is a kind of
core phenomenon that even though the
program is simple the behavior may be
complicated and there are many
consequences of this probably the most
significant one is a thing that I call
computational irreducibility and it has
to do with the following thing in
typically in in sort of one of the
achievements of exact science is Let's
Make a prediction for what will happen
in a system well in this case you can
ask well what's going to happen after a
billion steps in this particular running
this particular Rule and the surprising
thing is that we have pretty good
evidence that there's really no way to
tell what will happen after a billion
steps other than to run the thing for
about a billion steps and see what
happens it's kind of a you don't get to
do that kind of jumping ahead that you
expect in kind of exact science when you
when you're sort of making a formula for
something and and concluding things from
that well so there's uh uh you can you
can go and sort of explore the
computational universe of possible
programs and I've spent a lot of time
doing that and it's kind of a a very
rich world of of kind of what
computation in the wild can do and
computation in the wild can do very very
elaborate things um that are full of
this phenomenon of computational
irreducibility well the other thing I've
been interested in doing is figuring out
how to take sort of the power of
computation and humanize it make it kind
of accessible to to humans and sort of
the the big effort of my life I suppose
has been building wolam language and
sort of the idea there I'm not going to
I would naturally start doing some kind
of demo but I think I won't do that
because we don't have time um but kind
of the concept there is figure out of
all these things that are
computationally possible which things
are ones that are relevant to us humans
and then try and identify those kind of
lumps of computational work Implement
them and uh Define kind of this
computational language for describing
the world in a computational way I kind
of see our mission as being kind of a
version of what people did 500 years ago
in the invention of mathematical
notation but four 500 years ago you
talked about math it was done in terms
of words and so on then mathematical
notation plus signs equal signs things
like that were invented things got much
more streamlined it allowed algebra to
be invented in calculus and so on I I
view our mission now to be to try and
create a kind of language a
computational language for describing
things that allows sort of computational
X for all X to be created and we've
spent the last I don't know 38 years or
so building this this but let me talk
about kind of some of the directions
that uh uh I've gone at least in
thinking about kind of uh what
computation leads to so once you know
that a very simple rule can produce very
complicated Behavior something like this
rule 30 or or some other kind of rule
one of one of the questions that one
might ask is well what about the whole
universe maybe the whole universe is
actually made from some very simple rule
I thought about that u in the early
1990s I made some progress on that but
then right in 2019 uh sort of as a
result of first a quite technical uh
Advance uh I kind of was able to make a
lot more progress on that it's sort of
been an interesting thing that's kind of
the result of my sort of Life trajectory
alternating between doing basic science
and doing technology development that
that's sort of built a tower of
capabilities that I think allows one to
to to do a project like like the one
that I was able to do so the the the
basic uh thing is that seems like we've
kind of figured out what the machine
code of physics is like and I I should
just sort of tell a historical story if
you go back to Antiquity people were
arguing forever about whether the
universe is discrete or continuous and
that argument continued through the
1800s by the end of the 1800s people had
nailed it molecules existed matter was
discret likewise you can think of light
as discret at that time beginning of the
20th century most physicists believed
that space was discrete as well but
nobody could make that work and people
like Einstein would say you know it will
turn out to be discreet but we don't
have the tools to see how that works yet
well 100 years later we do have some of
those tools and so the the kind of
starting point of our efforts to
understand sort of the machine code of
physics is the the idea that space is
ultimately discreet and that really all
there is in the universe is the
structure of space and everything that
is uh kind of all particles and all
those kinds of things are features of
the structure of space so in in these
models a good way to think about space
is it's some kind of hypergraph it's a
bunch of discrete elements points in
space and they are related to each other
and all you know is is is how they are
related and they're related that in a
given uh point is related to other
points forming this kind of graph or
hypergraph and um then the idea is that
kind of the universe
evolves by just rewriting this
hypergraph a little bit like cellular
autometer except now there's no kind of
fixed lce of of points you just have
this big floppy hypergraph that uh that
is being Rewritten so you start off with
that hypergraph at the beginning there
these are just graphs actually hyper
graphs have have more than two elements
Associated on a hyper Edge um and you
run it for a few steps and you start
getting more complicated kinds of things
you can get all kinds of structures and
uh the um
the the thing that that happens is you
can from this emerges something like
space and what what turns out to be the
case is just as you can start as in
those pictures that I was showing at the
beginning with uh a bunch of discrete
molecules bouncing around on a large
scale a bunch of discrete molecules
bouncing around sort of limit to a
Continuum fluid kind of behavior so the
question is what does a bunch of
discrete hypergraph re writings limit to
and it turns out this is something I
kind of found out in the 1990s they
limit to the Einstein equations so
that's already pretty interesting and
you can it's but it's a it's a sort of
complicated situation because you don't
even know what dimension the space is
all you know is that there are these
points that are related to each other so
for example let's let's say you have a
rule like this U it'll sort of start
knitting a space in this particular case
it knits a very regular kind of space
where you can kind of readily identify
that looks like a two-dimensional kind
of thing you can do other kinds of
things you get sort of a a curv
two-dimensional kind of object but in
general you can just say well I don't
know what dimension this is going to
make how would I how would I figure out
what dimension the effective dimension
of this hyper graph is it's it's
actually very straightforward you just
start at some point in the hypergraph
and you go at every step you kind of go
one one unit away on the graph and you
see how big is this what's the volume
what's the number of nodes contained in
this ball that has gone R steps if that
grows like R to the D you say it's
roughly D dimensional space well so as I
mentioned kind of one of the big things
is that you find out that you get the
Einstein equations we don't know that we
get them in three dimensions we can't
derive the number three and figuring out
what it means to have something like the
Einstein equations in 3.2 dimensional
space is an interesting piece of of
future mathematics so to speak I just
might mention that when it comes to we
talk about the structure of space being
defined by this hyper graph time is just
the progressive rewriting of the
hypergraph the progressive kind of
irreducible computation associated with
the rewriting of the hypergraph and uh
you can kind of set up it turns out
things like relativistic invariance come
out when you start thinking about every
little rewriting is an event and you
find sort of the causal graph that
connects all of those events and you
start to be able to say well what how do
we Define sort of simultaneity surfaces
like in relativity you can do that by
just looking at which events do you
consider simultaneous and we'll see
pictures like this in a completely
different domain in biology a bit later
um so okay you do these things and you
can kind of see uh this is just a a
little simulation of what sort of the
very beginning of the universe in a in a
model like this uh uh can look like um
and uh if you if you go a lot later in
the history of the Universe um you can
make simulations like like this this is
a oops if I can bring it up this is a a
couple of black holes black holes have
the very nice feature that they're kind
of scale and variant so you can have a
black hole that's only a small number of
Elementary uh lengths um across and
those were two black holes merging and
you get nice gravitational waves
produced that seem to agree quite well
with the type of thing that happens with
with big uh big black holes well okay so
so the the the first point here is that
it seems that from sort of the the large
scale behavior of this hypergraph
rewriting is is like the behavior of
Continuum SpaceTime so then the question
If you're sort of trying to say well
well does physics arise from this kind
of low-level computational machine code
um the next big thing that you come up
with in physics is quantum mechanics and
it turns out in these models quantum
mechanics is actually quite inevitable
and the reason is that when you specify
you know we've got this hypergraph are
going to rewrite the hypergraph um the
question is well there may be many
different places where the hyp can be
Rewritten how do you deal with that well
the answer is that you can think about
following all those possible Paths of
rewriting you get what we call a
multi-way graph that in which you say
you have this hyper graph and there are
these many possible threads of time that
get followed and in a sense the the core
difference between classical physics and
quantum physics is that in classical
physics sort of definite things happen
in quantum physics you have kind of the
view that there are these many different
threads of possibility which then
eventually we kind of knit together when
we when we try and make a measurement or
something so in any case it turns out
that in in these models quantum
mechanics is kind of inevitable and uh
sort of interesting that you get this
kind of branching merging uh uh uh sort
of behavior of states and one of the
things that you can do is to say well
well how do we make a map of what states
we get we can kind of I didn't show this
okay if you take kind of a slice across
this picture you can say well how are
the states at that slice related to each
other and you make this thing we call
them branchial graphs that represent
kind of the the the relationships
between these different branches okay
you take the limit of this uh branchial
graph you get this thing we call
branchial space and okay here's here's
where I think it gets really interesting
so in physical space uh important
phenomenon is gravity and gravity in
these models is associated with the idea
that you have a shortest path a GC uh
which you can Define through this graph
and that GC is deflected by the presence
of energy and energy in these models
turns out to be essentially the density
of activity in the network so you can
sort of the derivation of the Einstein
equations is is like that in physical
space in in this branchial space you can
also ask things about deflection of gd6
and so on and well the sort of bottom
line is it seems like the F path
integral which is kind kind of a
mathematical foundation for for quantum
mechanics the F path integral is
basically the same as the Einstein
equations except the path integral is
played out in branchial space and the
the Einstein equations are played out in
physical space so it's sort of a
remarkable thing that the that the sort
of the formalism of uh of of of gravity
is the same as the formalism of quantum
mechanics in in these models well just
to to the sort of the deepest part of
the the rabbit hole that one gets to in
the in these models is to say okay we'
we've got this universe that's sort of
computationally defined and it's
specified According to some particular
rule you say well why that rule and not
another Rule and so then you start
thinking well what if the universe
actually followed all possible rules
what would that be like and so you can
you can kind of ask if you think about
computational systems you think about
there's a a little touring machine rule
there's the touring machine running you
can imagine a touring machine that has
several possible rules you can imagine
kind of building up this this sort of
collection of what happens with touring
machines with all possible rules and the
end result of this is to think about
this object that we call the ruad which
is the sort of entangled limit of all
possible computational processes so it's
a it's kind of a unique sort of very
abstract thing and that we can think of
that as being the result of physics
running all possible rules and you might
say what can you possibly say about this
this ruad object well the thing to
realize is as as soon as you start
realizing that we as observers of this
this thing are embedded within the ruad
there start to be consequences of that
that you can you can deduce most
important consequence is if you make
only some very basic assumptions about
the way we are as observers you
immediately can conclude things about
what we will perceive in this ruad
object so the two most important
assumptions are that we are
computationally bounded and that we
believe we are persistent in time and
the sort of big result as far as I'm
concerned is that those two assumptions
alone are sufficient to give us the
structure of general relativity and the
structure of quantum mechanics which I
find a remarkable thing because it's
kind of like you're able to actually
sort of derive uh features of physics
from just knowing things about the way
that things about the way that that we
are as
observers well in any case the the U uh
coming back
to let's see coming back to uh uh can
talk about how this relates to
mathematics maybe if people are
interested I'll do that but but just
coming back to uh statistical mechanics
one of the things that um I again find
pretty interesting is in the 20th
century there were sort of three big
theories in physics statistical
mechanics second L of thermodynamics
general relativity and quantum mechanics
what seems to be the case is that all
three of those theories actually come
from the same origin the same kind of
computational origin and in the second
of thermodynamics kind of the the big
question is well you know you have some
some system of of of uh of particles
that starts in an orderly configuration
ends up producing uh apparent Randomness
why is that happening you can kind of uh
you can kind of sort of simplify that
picture eventually you might get some
sort of onedimensional picture like this
where you start off with something quite
ordered and it becomes quite disordered
but you kind of have to you have to say
well is it this this picture is Tim Ely
reversible you can flip it upside down
and and run it backwards you can say
when you are at one of these
configurations where it looks kind of
random is it really random or does it
just look random because you as an
observer of these things are not capable
of decoding the it's it's it what what
it's done and effectively what happens
is that the system is sort of encrypting
its initial conditions and because we
are computationally bounded we don't we
are not able to decrypt those initial
conditions that the we're not able to
decrypt the state that we get to find
out that it came from something simple
so in any case the the the this idea
that we are sort of observers who have
certain limitations and that that
implies laws like uh that turns out it's
the same story between all three of
those basic theories of physics so I was
kind of happy 50 years after I I got
that book about um which had the nice
picture of of um uh from about
statistical mechanics I I wrote this
book last year which kind of is is green
and looks a bit like the the previous
book which I think is finally an
understanding of how the second law
works all right let me talk I I realized
that I should go quickly because I
really want to have time for lots of
questions and so on um I could talk
about U kind of how this relates to uh
well lots of things but um um maybe
something about mathematics um well
let's no let's talk about biology
instead um the the um this is a this is
a recent thing that I I've done uh
actually I had started doing this in
1985 uh I was curious about whether one
could have sort of a minimal model of
biological evolution using things like
cellular autometer and at the time I had
tried sort of uh starting off with some
cellular automatan Rule and saying can I
mutate that rule to get certain kinds of
behavior and I hadn't managed to make
that work but I didn't know about
machine learning at that time well I did
know about neural Nets but I didn't
manage to get them to do anything
interesting in modern times we know that
sort of if you bash a neural net hard
enough you can get it to do lots of
different kinds of things so I thought
maybe I should try with these cellular
autometer sort of bashing them harder
and seeing if I can get them to uh kind
of show biological evolution like
Behavior so this is a this is a case
where the the rules are on the left you
start the thing off from just one red
cell on the right it it runs for some
number of steps and then dies out now
you imagine making point mutations to
those underlying rules and you ask can I
uh can I make a series of point
mutations so that I will achieve a
particular purpose so for example one
thing you might try to do is say live as
long as possible but not for an infinite
time so what turns out to happen is that
yes you can do that and there are these
that that's the sort of sequence of
different phenotypes so to speak that
you get from these different mutated
genotypes evolving to the particular
picture I showed at the beginning well
if you kind of roll the dice
differently you'll get that same
structure same type of rule but it
evolves in a different way and it's kind
of fun to think about these different
sort of evolution paths and to say the
thing had a particular idea about how to
evolve you know we might have found a
fossil that looked like that and now it
builds on that idea to produce the
behavior that you see today and if you
look at this is kind of a fitness uh
this is looking at the progression of
fitnesses looks very much like much like
a loss curve in machine learning except
it's turned upside down and there's a
good reason for this um
that and so you see the red dots are
kind of the attempts that were made to
find successful sort of organisms and
the jumps up where finally an idea was
had a breakthrough was made and a more
successful organism was produced well
this model is simple enough that you can
kind of map out the uh uh the complete
structure of all possible Evolution
paths and so you can kind of see on one
side it sort of has one idea about how
to grow on the other side it has a
different idea about how to grow and um
the the thing that well I just did
yesterday let me see if I can get you
this picture there we go there's a much
bigger version of this that's for a
slightly larger class of Critters and I
I find this sort of interesting because
this is kind of a map of all possible
Evolution paths that could that could
happen in this in this particular uh
space of organisms and the thing that I
I realized there's a there's another
piece of it I guess that's a sort of a
megap forner thing that showed up in the
middle uh um the U the thing that I sort
of find interesting about this is this
is this is a map of all possible
Evolution paths and the question of what
happens as you change as you have
different sort of Fitness criteria for
the system is it's very much like what
happens when you have these causal
graphs of events and you ask how are
those events arranged in successive
space-like hypersurfaces and successive
uh successive simultanous time uh uh
sequences of of times and so there's
sort of this interes analogy between the
the possible fitnesses and the possible
effectively reference frames that you
can use for for space time and that was
a thing I just realized yesterday so the
the um uh and I think that's that's sort
of interesting because it gives one kind
of a a a handle on a more theoretical
approach to things like biological
evolution you know I'll just mention uh
maybe I should just turn this over to
questions I I could talk about some talk
about some things to do with with a I
and some things to do with um um uh with
the foundations of mathematics but uh
maybe we should just find out what
people want to want to hear about that
would be so we're gonna take questions
from each side so raise your hand and
I'll get to you um we'll start with Liam
here hi um first of all this is uh
fascinating and I can't wait to read
more um second of all I think so one of
the most important inventions of life
which I I also myself research is the
process of endosymbiosis and in general
um symbiosis between two different
systems that have their own evolutionary
rules I'm wondering if this tool um
could be used to predict what how
successful two systems would be in
generating some type of symbiosis and
then is there a way to implement
different constraints from the
environment to assess that I don't know
the environment here is pretty simple I
have tried to look at things like sexual
reproduction um and to ask questions so
I I haven't looked at anything to do
with symbiosis this is a what what's
what's interesting about this is a very
simple model and I hadn't realized that
a model this simple could give any of
the kinds of features that one is
interested in biological evolution so
the um uh I have to say that in in for
example sexual reproduction I was
surprised that it really gives one
nothing very different the model is more
complicated there are endless little
sort of parameters to introduce and so
on but the end results are really no
different I don't know what happens when
you combine two organisms it's a good
question if you just take you know two
of these if you if you either take it
okay so one thing I was looking at very
recently is the following thing can one
find a sort of a a foundational theory
for things like medicine which one would
not think there might be a foundational
theory for but once you have this idea
that you can have these kind of
organisms that are evolved for a purpose
you can say well what happens if poke
the organism and perturb it in some way
and it then doesn't you know does it
still achieve its purpose does it still
live a long life or whatever else and
what you can start to do is you can
start to say as you poke it can you
classify the the possible diseases that
can happen some it will recover from
some it will not and so on how does one
think about that then how does one think
about if one has you know if one has
perturbed it you know can one make
another perturbation to kind of heal
that that first perturbation so I think
there's sort of the the the the
possibility which I hadn't really
realized at all before that you can make
kind of a a minimal formal theory of
those kinds of processes it's kind of
like you know you can you can do that
for for biological systems you can also
do that for computer systems just like
there's an ICD you know classification
of diseases for for biological organisms
there are at least slight attempts to do
the same kind of thing for for diseases
of computers particularly for computer
security purposes and so the sort of a
question of caman you know what what
what commonality is there between the
kinds of things that that sort of go
wrong with these these minimal organisms
things that go wrong with computer
systems so that's that's at least the
beginning of that but this is this is
sort of very new stuff of like the last
month or so so there's there's much to
figure out and I and I haven't uh you
know I've only just begun to scratch the
surface of
it hi uh a two-part question uh first is
virus
what do you have to do to make it
possible to define a virus in this and
is there like a minimal set of changes
that you have to do in order to
introduce the idea of viruses and the
second is immune response like can you
implement immune response oh sorry I
yeah okay
so gosh I don't know um the the
uh um those are perfectly good questions
I mean look in terms of
uh immune response I think one of the
things that's interesting perhaps in is
you know okay in sort
of one thing that's strange here is how
diverse all these Critters are and that
it's kind of like you know you're making
an antibody or something and you know
you're just changing a few you know a
few peptides and the thing ends up in a
completely different shape with a
particular completely different function
and this is sort of a a minimal model of
something like that um but I don't know
how you know the the correspondence
between you know the space of shapes of
proteins antibodies whatever and uh and
this kind of thing I don't know
um you guys are asking about things I've
just worked on the last month of too so
um please so I have two questions that
are maybe related um the first one is
sort of a philosophical point which is
you mentioned this idea of computational
irreducibility
and so I wonder is the message of that
really that our quest to understand
things in science in a sort of
parsimonious way is that Quest misguided
and then um the second question is more
concrete um so you mentioned this rouon
thing and you mentioned that there are a
couple assumptions you need to make
about us as observers and that one of
these is that uh We believe We persist
through time I think that's what you
said so what does that actually mean
formally and like what to these
anthropic principle type things have to
do with um our doing science in the
universe okay so first thing about
computational
irreducibility yes computational
irreducibility is an example is
something that shows limitations of
science I mean we've been running for
the last 300 years or so on the idea
that yes you can just write down an
equation and it'll tell you how the
universe is going to work um that's not
going to continue always working that's
sort of the bad news the good news is in
a sense that computational
irreducibility is sort of what makes
existence meaningful in the following
sense if everything we did was just we
could know what was going to happen you
know the answer is going to be 42 or
whatever there would be nothing achieved
by the passage of time computational
irreducibility kind of shows that
something is sort of rigidly achieved by
the passage of time so I think that's
that's sort of the trade-off now there
are many implications of computational
reducibility I mean one one for sort of
AI kinds of things is the following
let's say you know we we now are in a
situation where um the uh actually I
could show something about that yeah so
so um we're sort of now in a in a
situation where uh you know we have had
a period post industrial revolution when
the machines we make we expect to
understand um that is presumably coming
to an end and the thing is we then have
a choice do we want the things we make
to be sort of doing their computational
best which means they will be doing
irreducible computation or do we want
them to be constrained to do only the
things that we want them to do so to
speak you kind of have this trade-off
you can force the thing to be
computationally reducible so you know
what all the pieces do and you can know
that it will only do the things you want
it to do or you can allow it to do what
to to sort of achieve as much as it can
computationally in which case you have
to have comput computational
reducibility and you necessarily have
sort of unexpected things happening you
know I I want to show something which
was that I've been one of the things I I
was doing very recently was looking at
sort of the foundations of machine
learning so typical neural net you give
it an input at the top it produces an
output at the bottom it's trained to
produce that function on the right so
what I was curious about is is what
happens when you try and when you don't
want to make something as complicated as
this it's hard to visualize what happens
inside neural Nets can you make
something where you just have for
example a grid of cells a grid of of
neurons the answer is yes can you even
go further than that and make every uh U
uh make the make the neuron net out of a
out of a collection of discrete cells
and so this is an example of kind of a
very minimized kind of Boolean IED
neural network in which you you have uh
you have just two possible rules that
you can run at every cell and then
you're asking can you can you make the
neural net can you kind of evolve a
neural net this is a very simplified
neural net that will achieve some
particular purpose so in this particular
case the purpose being achieved was live
for 50 steps and then die out and so the
the pattern the background pattern is
the kind of uh the configuration of what
you can think of as weights although
they're just Boolean values that
achieves that and so you know you can
you can work out sort of Fitness
functions and so on it's very much like
that biological evolution that I showed
you before sort of the biological
evolution in in neural net terms is like
a a very recurrent neural net this is
more like a traditional uh feed forward
neural net that just has a where you're
just feeding data through this series of
layers with different weights one of the
things that is interesting is if you say
well what is the neural net doing how is
it how is it doing what it's how is it
achieving the purpose that you've
trained it to achieve and the answer is
it's pretty complicated it's not
sometimes there's kind of a simple
explanation that you can have but a lot
of times the the kind of what the actual
structure of what's going on in the new
n is really complicated so one of the
things that I've been curious about is
kind of what is the qualitative picture
of what's Happening inside a neural net
for example how should we think about
that and my my favorite analogy in
recent times is if you're building a
wall there are sort of a couple of
approaches you can take you can engineer
it by building a bunch of bricks and
then carefully arranging the bricks you
get a wall that way another possibility
is you can see a bunch of rocks lying
around on the ground and you can build a
stone wall by finding sort of which rock
fits into which kind of Gap that's left
and you can successfully build a wall at
least of a certain height um by doing
that and I think machine learning is
basically doing something like that it's
taking lumps of irreducible computation
and finding ones that happen to fit and
putting them together in such a way as
to achieve something that that we
consider useful which means that for
example if you want to say well why is
the neural net what why is the machine
Learning System uh doing what it's why
is what's happening inside there may be
no no kind of uh simple narrative
explanation because it's just well there
were these lumps of irreducible
computation that happen to be able to
fit together in this particular way so I
think uh the you know that we we're
exposed to computational irreducibility
a lot in the modern world of AI and I
think kind of some kind of big decisions
that people sort of have to make is do
you want the the the computationally
reducible version that is limited in
what it can do but is is will only do
the things you want it to do or you
prepared to have the irreducible version
which will do lots of interesting things
including some things you may not want
and it's kind of like you know that we
are pretty used to living around
computational irreducibility because we
live around nature which is full of that
kind of thing and we've sort of found
these particular paths for existing in
in the natural world where we can where
we we can sort of Happily exist even
though there are these sort of
irreducible computations that that are
happening so the the uh the second thing
you were asking was about um uh kind of
our characteristics as observers and um
so the computational boundedness part uh
is is you can already see that in the
second L of thermodynamics because it's
kind of like it's it's what prevents us
from essentially decoding the Dynamics
that's going on underneath and knowing
oh this particular complicated
configuration of molecules necessarily
came from the simple thing um it's you
know in in the formalism of statistical
mechanics there's been lots of confusion
about course graining and what counts as
a valid course graining and so on and
this is kind of a a computational
characterization of that the thing about
persistence in time it's not at all
obvious that so so one feature of us as
kind of human observers is we believe we
have a a sort of coherent thread of
experience through time and when there's
quantum mechanics involved and there are
many threads of time it's not at all
obvious that we should have a single
coherent thread of experience Through
Time time um and the and when we think
about sort of us being made of these
atoms of space that are in these
different configurations and so on
there's it's not at all obvious that the
configuration of of atoms of space at
one moment of time will be will give us
sort of the same us at a subsequent
moment of time so it is it's an
assumption that we are making as
observers that it's sort of the same us
going through time uh and by the way in
the in the case of of quantum mechanics
sort of the interesting thing is that
you know when you imagine sort of the
insides of a quantum computer it's got
these multiple threads of of of uh of
History uh which you could imagine would
be doing sort of different parts of some
non-deterministic computation or
something like that but then because we
humans believe in this sort of single
thread of experience in order for us to
make use of this quantum computer we
have to have all those threads kind of
knitted together to a single kind of
conclusion and and that's sort of the
the place where where um
uh that that the real there's a big
question which is not really addressed
in quantum mechanics in its traditional
formalism of sort of how much effort
does it take to knit together all those
threads of experience of of History to
get this kind of so there's there's a
bunch to say about that one of the one
of the places so one of the things that
I'm very interested in is sort of
experimental implications of our physics
project and things like you know how can
we detect discreetness in space and so
on U some of the effects that you see U
are things like Dimension fluctuations
the the universes does not have to be
precisely three-dimensional so you can
have regions of 3.01 dimensional space
and so on it's an interesting physics
problem what does Photon propagation
look like through a region of 3.01
dimensional space uh not yet solved um
there are I I'm also one of my sort of
suspicions is that um uh there's when
when you look at the well you know among
sort of there are features of of physics
that we have known for a long time which
are actually eventually be seen as
symptoms of the discreetness of space so
my favorite is actually dark matter um
and I kind of see the following analogy
back in the 1800s when people were
thinking about heat they thought well
heat flows so what do we know that flows
oh it's a fluid so they invented caloric
fluid turns out that wasn't the right
theory of heat you know heat is
microscopic motion of of of of of atoms
and so on well I suspect Dark Matter may
not be matter at all but instead
something like space-time heat so to
speak a feature of the kind of
microscopic structure of of space so uh
in any case the the um and and in
quantum computers I I have the suspicion
that a bunch of the noise that is seen
in quantum computers is actually a sign
I'm kind of hoping that the engineer of
quantum computers has been done so well
that if one plots out the right things
one will actually see kind of a noise
floor that is associated with a thing
that in our models we call the maximum
entanglement speed which is the analog
of the speed of light in branchial space
um which would be very cool because if
we if we can find that we would have
sort of we may or may not have the same
luck that people had at the beginning of
the 20th century when it turned out the
molecules were big enough that you could
actually observe them with instruments
that existed at that time we may or may
not have the same luck now and that's
one of the possible possible ways that
one could get to that but I think one
feature of of you know there there are
Parts as I say the really surprising
thing is that the core characteristics
of generality quantum mechanics and
statistical mechanics seem to depend
only on these very loose properties of
us as observers and my guess is that
other things like the fact that we
perceive space as threedimensional are
features that have to do with more
detailed properties of us as observers
haven't figured out and and they'll
probably be very obvious once we see
them and it's it's uh that that's that's
one of the things still to figure out
okay we have time for only one more
question is there one on that side of
the room
hi I want to go back to something you
said um a little while ago about the
implications of computational
irreducibility for AI so I work in a
space where we're trying to use AI to
design molecules that we can then
validate in the lab and I'm really
frustrated by the suggestion that this
is completely irreducible because like
most scientists that I work with want
some physical explanation for why those
molecules were correct so is it is it
that the AI is just going through so
much Randomness and so much complexity
that it's just luck or are there like
really or are the are the molecules
we're getting out really aligning with
like the physics that we know well I
mean so so the question is why do sort
of llm based protein folding models why
do they work and that that's a you know
I I I'm curious about that because you
know at the beginning what was happening
was you know you take the protein Data
Bank you do multiple sequence alignment
you can find that the protein you're
looking for you know pieces of it fit
things that were already known and then
kind of the machine learning part of it
was kind of fitting together those
pieces that were already known when you
have a completely abono you know here's
a random sequence of peptides how will
it fold up it is it is I think it's
completely unclear whether that produces
a sensible result and my guess is that
it doesn't um and you know I think that
the this question of whether whether
there is a correspondence I I I did a
little study of this um whether there's
sort of a correspondence between what
happens in um you know can you get AI to
basically reproduce um see if I can find
something here uh see
um yeah can can you get AI to to sort of
successfully reproduce so get can you
get a neural net to successfully
reproduce um something that would
otherwise come from physics equations
and actually it doesn't work very well
at all I mean this is an attempt with
various neural Nets to reproduce a sign
curve and after you know the region on
the left it was it was trained for the
region on the right is an extrapolation
and it does horribly and it basically um
and you know is even as you increase the
size of the network it doesn't do any
better and so you can ask for example
one of those U cellular automatan things
you can say well can you know a nice
modern Transformer Network can it
successfully predict what's going to
happen in one of the patterns well if
the thing is periodic yes it does fine
if the thing is more complicated it does
it does not do very well you can do the
same thing for uh let's see if I can
find this
um you can do the same thing for like
the three body problem um and uh you can
say can it solve can it do the
equivalence of solving the equations for
for U you know motion of three bodies
under under Gravity and the answer is
when when the bodies are behaving in a
fairly simple way it does fine when when
when it's more complicated it doesn't do
very well um and you can do the same
thing it's kind of fun that there okay
there's some proteins yeah there we go
there are some uh proteins where uh one
of those is is the prediction from llm
Models the other is the the crystal
graphic I think it's Crystal graphic
data um and what you see typically is
that I'm not sure if this is is that
there's a decent example um what you see
is you know you have an alpha Helix
that's behaving in a fairly simple way
it will do a pretty good job of
reproducing it when it's a big glob mess
it doesn't do a terribly good job of
reproducing it and I think that's you
know it's it's kind of it is finding uh
kind of pieces of reducibility in these
structures I don't think when there's
truly kind of irreducible stuff going on
I don't think it's it's it's just not in
the nature of these things to be able to
do that I mean this is a very a very
practical point is you know what will
llms be able to do versus what do you
need computation to do I mean in our
sort of business we're in the business
of trying to make figure things out with
computation and the question is uh when
do you end up using computation when do
you end up being able to sort of just go
through the layers of an llm to get a
result and I think the U sort of the
emerging kind of Technology uh
connection is what we're calling U
computation augmented generation which
is kind of the analog of retrieval
augmented generation but computation
augmented generation means you're an llm
and you're producing in output and
you're basically using computation to to
fuel the output you're generating and
there are things where you kind of have
to use actual computation and there are
cases where you can just use the llm and
it's it's I mean I think the the thing
the fundamental the thing about neuron
Nets when are neural Nets going to work
well they work on a lot of things which
are human-like tasks and they probably
distinguish cats from dogs in kind of
the same way that humans do because they
work kind kind of like humans work now
when you're doing some problem in
physics about proteins there is no kind
of human angle to that so it's it's much
less clear what's what's going to happen
I mean I think the only thing that is a
little bit tricky is when you're looking
at a protein there are there are sort of
features of that protein that we humans
tend to pick out like oh there's a beta
sheet here and in so far as you're
saying look this neural net did really
well it picked out these same kinds of
features that's again something where
you're injecting kind of human angle
into into what's going on but I I I
think uh you know I don't think that the
uh the idea that you know neuron Nets
are going to take over from computation
or physics and be able to figure out
things that are I think computational
irreducibility kind of shows you what
the limits are to what you can expect to
be able to do with a neural net that is
effectively doing a limited amount of
computation yeah
right I think very little so I mean
we've for years we've we've used so so
like when when we first started building
Mathematica um you know people I
remember people said I wanted to
evaluate SP all special functions you
know hundreds of special functions to
arbitary Precision anywhere in the
complex plane and people said you're
crazy you know they said by the end of
the 1990s will have the integer order
Bessel functions to quadruple precision
and so what did we do well we built the
system that just searched through uh
sort of possible rational approximations
to these functions we did what today
would be called machine learning and you
know we spent many months of CPU time
and got good results so we've done many
of those kinds of searches for
algorithms and I've been doing that for
years searches for algorithms work quite
well um the uh you know so that's that's
a definite thing now in terms of you
know can one sort of improve okay so one
thing I'll show you I got to show you
one little demo let's um uh see what I
can pull up here um let's see do I
really want to live dangerously and try
some demo that um uh this is very new
stuff but but this is um I don't know
what's a what's a a good um
uh I don't know
um uh
compute a um uh I don't know igen
function for a an
ellipse let's try that see what happens
here I think this may be a disaster but
but
um
um that's not looking good uh not
looking good at all well let's see blah
blah blah blah
blah okay that's not terrible that's not
terrible the wow it knows about Matthew
functions that's a good start um the wow
okay that's more complicated wow well
look at
that not
terrible um the uh so you know this is
this is sort of an example of uh this is
an example of something somewhat
interesting which is we've given it some
vague thing to do it's produced
something in precise computational
language uh chances are you know I can
more or less read this and I think this
sort of this collaboration between the
human and the AI of you say some vague
thing it produces something that is a
piece of of precise computational
language I mean this wouldn't work with
a low-level programming language because
you got a big blob of code which most
people couldn't read kind of the idea is
this is sort of high enough level that a
human can read it a human can can take
those kind of building blocks and go
from there I think that's the most sort
of powerful connection I I don't think
taking uh big chunks of algorithmic code
and replacing it with neural Nets I I
don't think that's a I mean we we've
already done a lot of searching for
algorithms and so on so it's not you
know the things that we use for I don't
know solving differential equations and
so on those were found by Searchers the
the algorithms were found by searches so
that that's a you know that's a a
comparable technology um you know right
now you know lots of things we've done
experiments with for ages like neural
Nets versus finite element methods still
not really working well um but that
that's uh so I don't think the idea of I
mean the thing is this notion of you
know just tell the AI to write code and
the code will magically get written the
main problem is what do you want the
code to do you have to have some
specification for that and you know
that's what I've been trying to build
for a long time is a good notation for
specifying what you actually want the
code to do and that's so that that's
that's kind of the approach

welcome everybody
we've got james barrett here today um
and i've done an interview in the past
with james barrett in 2013 on his book
our final invention we're going to be
revisiting the topic of ai friendliness
or ai safety
but just by way of introductions james
has
been he's been a prolific documentary
filmmaker and is also a speaker and
author he's been involved in
large broadcasters like national
geographic pbs nova
bbc in the discovery channel and
recently in pbs that he did a
documentary a couple of years ago before
the pandemic hit
on zoonotic diseases so look it's really
topical so
thanks very much for joining us again
james how you been
it's i'm doing great it's a pleasure to
be here adam thank you for inviting me
again
absolutely it's always a great
opportunity to talk to you and
i love talking about these topics
anyways but um tell me about this
zoonotic documentary you did recently
well it was a couple of years ago but
they've been playing it on pbs a lot
because it's germaine to covet 19.
it's uh it was called spillover zika
ebola and beyond
and uh we started out to make a film
about ebola and we went to west africa
during the end of the crisis and then
zika came out so i went to
went to brazil to to cover that story as
well so it ended up covering zika ebola
and some other diseases and these are
all diseases that jump from the animal
kingdom to
to humans very much like covet 19.
um and you know one thing that that
struck me
when we were making that film was that
everyone i interviewed i interviewed a
lot of people at the cdc and
and people at uh the national institute
of uh
health's infectious diseases unit and
they were very
very uh confident that nothing nothing
like ebola or zika or any other
uh very virulent virus could catch on
here because our medical system is so
sophisticated
and it's it's uh very ironic that that
coveted right now is just kicking our
butts and we we seem to be helpless
against it
oh absolutely i mean yes as uh we can
certainly be very arrogant
maybe people just don't think ai will be
a problem
and i i'm worried that we're going to be
blindsided with
some of the the problems that you
introduce in your book uh our final
invention
for those who haven't actually been
introduced to the book do you want to
give us a just a brief introduction to
what the book is and why you wrote it
sure um the book is about the long and
short-term
impacts of ai uh none of which could
could be very good i was i was at the
time i was um
i started my my interest in ai by
reading a lot of by reading ray kurzweil
and
and and a lot of other people about
about ai and the the books that were on
the market at the time around 2010
when i started writing were all very
rosy and ai was
was this this great thing and i've been
following ai since about 2000
and i was also bitten by the ai bug and
pretty much
euphoric about the the potential for ai
but then uh i interviewed arthur c
clarke around that time a little bit
earlier than that
and uh yes it was a i i'd interviewed
him around 2001.
and he said um he said
to pop my euphoria he said something
like uh
we humans steer the future not because
we're the fastest or the strongest
creatures but because we're the most
intelligent
when we share the plan with something
more intelligent than we are they will
steer the future
so uh that that stuck with me
and uh kind of that that
that idea never went away and i started
talking to ai
experts and ai makers about um
what i used to call the two minute
problem and it was when i listened to an
ai lecture
in the final two minutes the the expert
would say well and
by the way there is a chance ai could
get out of control and you know could
really
could really do a lot of damage and then
they'd go back to the good news
so when i investigated the bad news it
was pretty bad
uh including wiping out the human race
but it started with it it starts with
smaller things we can see those smaller
things now
and we can get into them um it starts
right now we've got
we've got issues with ai and i'm not i'm
i'm generally a proponent of i i think
it's got terrific potential
it's being used to you know very well in
medicine right now very well in
in radiology um uh diagnostics it's
business analytics uh it's solving a lot
of problems and
it has a huge future in in medical
issues
but at the same time it's a dual use
technology
capable of great good and great harm and
we've got problems with uh
you know i can go into these now or
later but you've got problems like
like bias in the data
we've got we've got issues with uh
with algorithms not um the data that the
algorithms were created with don't
represent
minorities so you have minorities and
women who can't get
who don't uh get jobs at this at the
same rate as
as men uh in the uk there was an
algorithm that wasn't
that was for for college admission that
was
biased against minorities and women
in america there are a lot of sentencing
guideline algorithms
that were based on data taken from the
70s 80s and 90s where as a matter of
course
minorities had longer prison sentences
they just got them it was uh it was just
plain old racism but with
with uh with data data always
brings values with it and um so now in
the modern day they were using these
sentencing algorithms that were
that were that were prejudiced and gave
gave
people of color longer prison sentences
so there's a lot there a lot of before
you get to the big existential risk of
ai there are a lot of
smaller problems along the way
yeah absolutely and look um just to set
the stage
there's been quite a bit of development
in ai since last we spoke in 2013.
i think it was like almost christmas in
2013
um and a few of the developments have
really
shown the power of deep learning very
powerful
extremely powerful i mean we've got
gpt-3 we've got
um of course the the the winds the
alphago we've got alpha star
more recently alpha fold which is you
know solves
some scientific problems i say solved
reservedly um
of course it doesn't understand what
it's doing it's
all it's really doing is just predicting
protein folding
to a very um i guess astonishing degree
of accuracy
yeah that's great yeah but this is a
this is a problem
do you think has your mindset changed
and has the public mindset changed about
the
uh potential for ai now um
then then what is what what it was in
the past
like seven years ago yeah i think um
we couldn't have anticipated a lot of
things seven years ago
the folding problem i remember people
talking about it as being one of those
unsolvable problems
where uh basically uh
how to determine a a protein's 3d shape
from its amino acid sequence
humans haven't been able to do it now g
alpha fold has
gpt2 is is really impressive as a
language modeling uh
software that basically anticipates what
what's coming next
we couldn't anticipate then you know
what's what's coming next what's the
next word
going to be um and that's how it strings
together a whole lot of words and
ultimately loses meaning
because it doesn't really understand
anything um
the what we couldn't i couldn't
anticipate was how much money
was going to ai it's the amount it's
it's
exponential growth in the investment the
amount has doubled every year since
2009.
right now it's about 30 30 billion
dollars which doesn't seem like a lot
but uh
price waterhouse coopers anticipates
that by 2030
um ai will add 16 trillion dollars to
the global gdp
making it you know the largest if not
the largest part of the economy
that same year it uh says gartnering
company
half of all jobs will be lost to ai is
this
mckinsley is saying this wow this is
not no this is the gartnering company uh
place waterhouse coopers was the first
one i mean i i mckinsley's probably on
board with these guys but i don't know
yeah yeah yeah well that's incredible
half jobs
by 2030. is that existing jobs or is
that
all jobs um do you think that like
half of these jobs that we will lose to
ai will be replaced by new jobs or do
you think that
you don't know i think there's varsity
of jobs in the future
i think there'll be a positive jobs i
mean any job when i talk to students i
tell them
don't don't take jobs that that
computers can do don't do anything
that involves routine or rote memory or
doing the same thing again and again
uh don't do you know look at look at all
look at the kinds of jobs that are going
away
all factory jobs will be gone um uh all
driving jobs
and not too much time will be gone all
postal jobs are virtually
virtually already gone anything that
requires repetition but that goes up
into the
into not just working class but into
uh into not just blue collar but white
collar
um a lot of legal jobs are going to go
away legal discovery that what
first-year
law associates do because it's really
just research and
and and uh computers can do that a lot
simple research better than we can
radiology
medicine um all all sectors are going to
get hit
now the the the the it's been true it's
been true in the past
that new technologies do not yield a net
uh
loss of jobs this is different though
because it's
because it's cognitive because it
requires it's a it's a
it's a thinking technology it's not like
a cotton gin
or an assembly line um
and so it goes deeper into into more
strata
of the economy and um
it's very unclear to me people say well
we'll retrain these low-skilled workers
we'll
train them for other jobs we'll make
them uh we'll make them program we'll
get them to be programmers so we'll make
them
into robot repairman well if you took an
assembly line full of people you might
get
a couple of managers out of that you
might get a couple of people who retrain
up
but people who are working on an
assembly line and people who are driving
cars are doing that because they don't
have a lot of other skills
and the idea that they could they they
could be trained for
more advanced skills that the robots
haven't take haven't taken
it's kind of a myth um it's hard to
anticipate the new industries that will
be born from ai
there'll be things we can't imagine
right now but
i think it's highly unlikely that
there'll be jobs for all those people
that the ai and automation world
displays
indeed this is this is a scary future
for a lot of people
i mean when there's a lot of desperate
people around people
will do desperate things um we're
finding that we're
yeah we're finding that right now in the
united states we're having riots
and uh we have you know mass
unemployment because of the
of the virus and we have people who are
just just
just you know as they say losing their
religion
over uh job loss over the
the idea of uh government getting bigger
and more powerful during this
during this pandemic um and we've we've
also got a leader who's just
batshit crazy and isn't isn't uh
is inciting violence
which is a difficult thing um and it's
one of the themes that
i've sort of interviewed others about is
the degree to which people
trust experts now because they've got
this
sort of alternative facts this
post-truth-ness
um where where people are finding it
very difficult or
this it's just strange to tell the
difference between like
what what can be what should be able to
be given
like strong credence and what shouldn't
you know who to trust
there's this lack of trust in in in the
world and especially in america i
imagine
um at the moment which is a a difficult
problem to navigate
how can you convince people where you
can't actually share a mutual ground on
which to make
which to adjudicate yes the truthfulness
of claims made and
and and the realities before us
it's crazy we've we've got a uh a
president who doesn't doesn't trust
science probably because he never took a
science course or
never took a you know test that he
didn't didn't cheat on or have somebody
else take
but he just does not trust science he
does not read
um he's he's he's studiously ignorant
about the world around him he's he's the
least curious person in the country
um he doesn't trust his scientists he's
got he had a team of the best scientists
in the country
epidemiologists um i i remember
interviewing anthony fauci years ago and
he was even then just
you know the the best epidemiologist or
person who studies at
epidemics in the you know in our country
anyway
um and he's he he's not trusted and the
whole the whole way that this uh
the whole way that this curved response
has been treated is because of as you
said this distrust of science
this distrust of facts that there's
alternate facts
um it's it's absolutely absurd it's a
real it's like
it's like the enlightenment never
happened to step back into ignorance
yeah it's unfortunate um
but i mean yeah so
if if we had this public distrust of of
facts and science
i mean it's going to be even more
difficult to
um talk about this issue of ai
friendliness
i mean if people just want the most
comfortable
explanation the the the explanation
which provides them
the the most sort of security or you
know the vision
for the future which is exactly yes
they they want comfort and they want to
be told everything's going to be okay
or they want uh enemies pointed out i
mean
that's what they really like what it
seems to be
you know half of our population united
states want what they want are
scapegoats and people to blame things on
um i used to i i mean i wrote our final
invention in
as simple uh a a language as simple
language as i could muster
in order to get it out to the most
people and to make even
non-scientifically based people
understand but which i'm wondering now
if that was a waste of time because you
know
i think the the the um part of the
audience i was trying to reach doesn't
read anyway
and just and would it wouldn't embrace
uh a science book
well this is a problem in in um i guess
science communication
uh in general and that is people
are low russian a low information um
rational rational agents i guess that we
don't execute on huge amounts of
information like ai might
in the future yeah but um yeah so so
it's very difficult people often have
very strong opinions especially about
the opinions
the things which i know the least about
yeah
there was something called rational
agent economic theory and it was
designed to uh anticipate what markets
would do
based on rational decisions that we make
buying and selling
and economists pushed it as a way to
sort of normalize markets
but it didn't work because we're not
rational we impulse buy we buy things we
don't need we don't buy things we do
need
and so the idea that we're we're
rational agents is
kind of a big mistake what we are is
impulsive and very emotional
and we do we develop attachments to
things that are absolutely terrible for
us
um and that's why you know i think part
of part of the
hype about ai has been you know a giant
spoonful of sugar
i i think ray kurzweil is a great
inventor a great prognosticator about
the future but he's
he's been part of the the sewing machine
to to make ai seem safe and lovely
um but i think you know i i i'd have to
say on the encouraging side
we've seen ai safety grow as a as
a topic we've seen ai every i i'm
stunned at how many organizations
have popped up that are discussing ai
safety and ai ethics you know
future of life institute future of
humanities institute the machine
intelligence research institute
and a bunch of new faces existential
risk yeah you can't tell
exactly there's a lot and i thought it
was a phenomenal
risk institute with your uh chef bomb
yes
yes yeah yeah you should have one in
australia
you should there you go there's your
there's your
your side career anybody but yeah nathan
said in that
academic context yeah and they come out
with these voluminous uh
treatments of um of ai ethics
uh some of them aren't voluminous and
those are actually the best ones because
i don't i think that we've we don't need
principles we need uh
regulatory bodies that can go and look
at whatever ai you're building
just like we have you know i've said
this forever um
we have an iaea international atomic
energy agency for
looking into silos and looking into
reactors and making sure that people are
are conforming to international uh
protocols and if they're not they get
they get um
they get uh censure they get uh they get
um
steps taken against them they get
tariffs they get all kinds of bad things
that happen to them
it's happened to uh in the recent past
it's happened a lot
to iran um we need that for ai
uh we need we need those sorts of boots
on the ground people
looking in into labs i'm usually though
i would be the last one to say we need
any kind of government regulation
because it's generally inefficient
but in this case we need government
regulation because the companies that
are
building the ai just cannot be trusted
on their own
they just simply can't be trusted they
all have a we
can go into that uh yeah what did you
try i think in another interview
like you brought out a really
interesting factoid and that
is about cambridge analytica and how it
it took a lot of the shirt from okay so
facebook allowed that right
we have we had this there's a concept
uh you're familiar with i know called
the intelligence explosion
yep and you can see the parts of it
assembling around us
um and it goes like this and this was
brought up by ij good in the 1960s he
could see it
you could see the potential for it and
interestingly
in the in here's some trivia he was he
was evaluating an artif one of the first
artificial neural nets at the time
it was called the uh perceptron yep and
it was
it was it was one layer uh and it didn't
it didn't do much but it did a little
bit but
unfortunately roger rosenblatt who
invented it died in a boating accident
and then marvin minsky wrote a book
saying oh artificial neural nets will
never work
yeah and it killed them and they they
were they were happening in the 60s we
could we could have been far ahead right
now
but anyway um the whole idea behind
just to build up to this one of the
things that's working so well for us
right now is
um some some ai makers in
around 2009 uh jeffrey hinton
um
there's there's another there's another
one another third one i was gonna
i forgot but they had the insight that
if you take you take a lot of data
feed it to a simple learning algorithm
you'll get
super human uh
pattern recognition capabilities and and
and prognostication guessing what's
going to happen next capabilities
yes exactly what's happening with
exactly what's happening with uh
gpt2 right and uh then the more the more
layers that you may take a simple
learning algorithm then take
another simple learning algorithm and
another one then you get deep learning
and you get really really powerful
capabilities um
that led to to machines doing a lot of
things that we
used to be able to do uh that were just
certainly in the
in the purview of humans and now
it does yeah there's great translation
great navigation great it's gonna
soon do great driving it's a lot of the
things that
that it's doing right now is owed to
this this insight
so to apply the intelligence explosion
to this
we're creating machines that do a lot of
what we do but they do it better
at a point and we've just solved one of
them has just solved the
protein folding problem which is
incredible nobody thought was going to
happen for a long long time just like
they never thought
the game go would be defeated but but
alphago and alphago zero defeated it
several years ago now that was supposed
to take 30 years
um so
we've created a lot of machine we've
created machines that do a lot of things
better than us
in the not too distant future we'll
create machines that that
do artificial intelligence research and
development
better than we do and then the machines
according to ij good will set the pace
of
intelligence expansion not humans and
then suddenly
we'll be sharing the plan with something
that's a thousand or millions of million
times more intelligent than we are
we don't really know what the ceiling of
intelligence is we know
that the ceiling of human intelligence
is is not awfully high
but we don't know what what
hypothetically what
what the ceiling is for for our uh for
um for ai there may be no ceiling
so we're we're we're building we're
putting all the building blocks in place
for this
uh for the intelligence explosion yet we
don't really know and this is
this is the problem this is the uh this
is uh the control problem
this is the alignment problem we don't
know how to
how to live with something that's a
thousand or millions of times more
intelligent than we are
as stephen hawking said we won't create
weapons we don't even understand
um it will outsmart our cannes
politicians which is kind of a low bar
but
it will it'll be uh intelligent in
dimensions that we
don't we just don't grasp
now that that's all pretty pretty
terrifying but now look at who's in
charge
of the intelligence explosion demise
hasabus was one of the co-founders of
deepmind and a couple of years ago
he said i don't trust the ai
makers to monitor and control
the uh intelligence explosion to
mitigate it before
we hit that that time when
we were suddenly you know when we were
suddenly sharing the planet with
something that's
far more intelligent than we are um and
and why would he say that
he say that because the all the
candidates
are morally pretty crappy um
i you know it's it it said some of it's
just truly astounding
as everybody knows or should know um in
2016
facebook gave the the private data of 80
million americans to cambridge analytica
cambridge analytica gave it to the
russians together they
targeted a lot of american voters and
for to
to influence them in the 2016 election
we don't know
how great their influence was you'd have
to look into the heads of all those
people
and so um right after and while uh uh
while mark zuckerberg was was
apologizing to congress
they were giving the same data to the to
the chinese government
so yeah and they hadn't they still
haven't learned right now they were just
apple and facebook were just caught
colluding about
measures taken against them in antitrust
in an upcoming anti-trust suit
um apple you know so so facebook
would you trust facebook with this
technology well i wouldn't
google has has 400 lawyers
for among other reasons they've been
sued in 20 countries for
everything from privacy violations to
predatory business practices to to
to intellectual uh property theft
um i wouldn't trust them with this
technology either i wouldn't trust them
within to mitigate the intelligence
explosion because we know where their
values are
the values aren't with safety and
fairness the values are in the
in the bottom line um apples apples in
the same boat
apples uh they came out recently and
said they were so shocked that
one of our one of our suppliers of
iphone
parts uh our ipad parts one of our
suppliers was using child labor
what they were doing was using uh they
had student interns that were
supposed to be paid were not being paid
and basically kept in slave quarters
um a few years before that uh
apple turned turned a blind eye to
foxconn foxconn's the largest industrial
contract manufacturer foxconn had a
series of suicides
because their working conditions were so
crappy
apple expressed expressed dismay foxconn
stopped reporting suicides
and then the whole thing the whole
problem went away
so this coterie of of tech companies i'm
leaving i'm leaving some out
um if you were going to pick companies
to monitor the most sensitive and
dangerous moment in our
in human history the intelligence
explosion
if you're going to pick companies they
wouldn't be these companies
we we can't trust them with with the
intelligence explosion
which is what one of many reasons why
why uh supervision and regulation is
necessary
now so you you may i i got up on my
soapbox
just then so i'll try to that's fine
look um look i i i i'm a bit stunned
uh i look when i hear this sort of
information it is um
it's it's nerve-wracking can i say the
least
well it's like you're driving yeah
you're driving in a car and
for the five people driving a drunk and
you know you don't
you you're not allowed to get you know
you want to get behind the wheel
you don't want to stay in that car but
we're we are in that car
do you think it's do you think it's more
the problem that there's going to be
humans
trying to control the ai that um i kind
of i guess
misguided or not not ethically sound
or do you or do you think this endemic
to
an intelligence explosion is a high
likelihood that ai will converge on some
um i guess
a strategy that will involve wiping
humanity out
or both well i think the whole the whole
uh the most important consideration
about the alignment problem is
you know imagine imagine all of the
space of possible ais
it's quite vast you know what which one
are we going to get
uh if we're extremely lucky we'll get
one that's that's benevolent to humans
but that benevolence won't you know if
we pick that one
that the one that's benevolent to humans
that's a good thing
um however for it for it to be
benevolent to humans we have to program
that in
in fact we have to program in more than
benevolence we have to really you know
we have to we have to program in
something that changes and grows with us
something uh elias yokowski calls
uh extrapolated concurrently
volition yeah
it has to it has to be intuitive it has
to it has to grow with us
we don't want to be locked into the
moral norms of today
in a hundred years you know we don't we
want we want something that grows with
us that
um anticipates what we what's best for
us
without being dictatorial but we don't
know anything about how to program
benevolence into a machine we don't know
anything about how to
program intuition into a machine so
the the odds of that one in the space of
all possible ais that are coming towards
us
the odds of finding the one that's
benevolent to us
are extremely small what's more likely
is that we'll find
uh incredibly powerful ais that are
ambivalent about us
which is the same thing as extinction
because they'll they'll do what they do
they'll they'll
they'll use resources they'll they'll do
uh calculations they'll they'll um
they'll act in their own in their own uh
in their own self-interest
they won't they they won't be benign
yeah it's um i think the last time we
spoken death about
steve omahando's basic ai drives yeah so
i mean if anybody wants to find out more
i'll provide a link to the previous
interview in the description
um but basically that says that if we
don't if we're not careful
um the ai may uh
have goals fall out of uh its primary
goal whatever that may be car
manufacturing pay-per-click
manufacturing
getting into space um and those sub
goals would be anything like
you know um could be like just try and
extract as much resources as possible
we're made of resources
or um you know uh try and update its own
source code to be more creative and
and all those sorts of things so yeah
it's fast yeah i think
i think if anyone hasn't read basic ai
drives it's just it's such a good read
and so logical and
um that's he uses actually he uses
rational agent economic theory to
analyze the behavior of
of of super intelligent machines and
it's it's quite fascinating quite and
very and very compelling
um you know the history of technology is
the history of unintended consequences
right and we're raci we're racing into
this future
where uh where
we don't know how you know this is why
it's called the control problem we don't
know how
how to anticipate the full scale of
consequences right there's a guy who
wrote a book
and i had it written down somewhere uh
that that you often
quote from yeah it's uh what was it um
normal accidents accidental normal
accidents yeah
yeah yeah he's it's charles perrault
p-r-r-o-w and it's a great book it came
out in the 90s but he was talking about
um
he was talking about very complex
systems uh if you have a system that's
that's very very complex then accidents
are a normal part of its operation
and he was using um three mile island
and chernobyl
and and airliners as examples you know
we you know
occasionally airliners occasionally drop
by the sky
um fukushima has happened chernobyl's
happened
three mile island happened but he
analyzes uh
chernobyl and it was a series of events
that
you could not possibly anticipate one
crew went off duty
uh sign somebody put a sign in front of
a a red light that
you know talked about that was about a
temperature gauge
somebody did something else and there's
three things in combination
uh created this gigantic environmental
disaster that we saw that that
region still hasn't recovered from we
don't know what it's done to our
biosphere um
but it but the the the important
takeaway is
when you design things that are that are
so incredibly complex
uh having accidents is a normal part of
of operating them now we know i mean
you you've mentioned um the the
explainability problem that people are
dealing with with ai right now
we don't know we know with neural nets
we know what the inputs are we know what
the outputs are we know how to adjust
the inputs to change the outputs
but we don't know at a high resolution
way what's going on inside
it's a black box yep yeah it's a black
box system and so
so our um neural nets and so are
evolutionary algorithms
and there's a movement to start over
with a lot of programming and make it
explainable throughout and one of the
reasons is
if you're driving a car and you have an
accident if you have if you're in a
self-driving car
and an accident happens you need to know
exactly
how to apportion the blame you need to
be able to really pinpoint that
part of the code that steered you
steered you wrong so to speak um
just because of tort law just because of
how other
systems operate um
so we're it it it's the lesson of
charles perot is that we'll have
accidents that we can't anticipate
with with advanced cognitive
architectures
right and some of these um accidents if
advanced cognitive architectures get
even more powerful
could be quite extreme and arguably
may actually lead to our extinction
um you know or extinction of all life in
our biosphere
if we're not careful uh if if these
accidents involve
a machine that is um that
doesn't actually have have a concern for
us at all
it's just like it doesn't have any
understanding doesn't have feeling
it doesn't have any um form of like uh i
guess
to anthropomorphize things a little bit
doesn't have empathy
right if i can use that term um sure
yeah sure uh so so if yeah so do you
want to
talk about what could happen if we you
know if we have an extremely powerful
optimizer that uh doesn't have this sort
of inbuilt
benefits as you described it before well
it
happens yeah well
according to omahindro the basic drives
i mean
no matter what the the software
is designed for it will be useful for it
to have
enough resources to run it will be
against its programming to not
be able to run we're talking about very
sophisticated systems
um so it will try to get it'll try to
get resources
and we'll try to guard resources it
won't want to be unplugged because being
unplugged
is the most severe damage to its
uh to its um
its goals to its goal seeking to it's
just exactly to its utility function too
yeah goals sound more like easier to
understand i use these terms but i just
yeah i think yeah most people understand
goals rather than just yeah it's a goal
seeking instead of utility utility
functions again from
rational age and economic theory um it
will
it'll be creative so it'll it'll always
be trying to anticipate how to better
increase its chances of achieving its
goals and one of the things it will do
quite naturally uh is
try to try to understand and develop
artificial intelligence research
and try to improve its own code
it will it'll try it'll it'll be uh at
some point
will this this this uh cognitive
architecture
will be self-aware enough to have a
model of itself in the environment it's
it's in and it will want to improve its
own its own ability to achieve its goals
i.e its intelligence so it will become a
a programmer will become it'll become
capable of doing artificial intelligence
research and development then you've got
as we said a minute ago you've got the
formula for the intelligence explosion
then its intelligence
increases very rapidly so
just creating a goal-oriented machine of
uh that's that's that's at the level
where it's self-aware and self-improving
is sets up the intelligence explosion
we may end up with um an alien-like
intelligence that is
completely different from ours that um
it's a black box we don't understand it
may not even understand itself
um it's just extremely powerful right
in the same way as we can see uh systems
today
that that are hugely complex they're
like a massive
but still are extremely complex very
alien to
us they don't think like us they don't
behave like us but they do
things extremely powerfully they they
solve problems quickly
they do sorting and searching rather
fast
and you know they're excellent at
calculations and things
like that so image recognition voice
recognition
it's it's yeah they have they have they
have what you know if you if you pulled
someone
pulled someone out of uh you know the
19th century and showed them these
these capabilities they'd say they were
godlike or they were superhuman
just just the ones you've gone through
just just the stuff that we
call routine now um you bring them all
together into a general intelligence and
you've got something that's
that's godlike you've got something
that's just awesomely powerful
if you imagine you know we're
we we had a there's a big insight with
artificial neural nets
what what gary marcus says has to happen
now gay marcus the ai
maker and thinker is we need something
we need an insight about
common sense we need we need
you know we need an insight about
a system that can learn about the world
and develop an ontology a common sense
database
so it can so can it knows that you can
cup your hands or you can pour from a
cup
or you know there's under you can you
can go under the table or over the table
you can do things in the physical world
and there are people that are working on
this
hard right now um there's uh there's
there used to be a there used to be more
there was
one called psych that was uh
called here yeah i mean some people say
that's a cataclysmic failure
because all they were doing was adding
axioms and they thought that maybe
once they reach a million intelligence
would emerge and now they're
up to many millions and still adding
more well i think i think with what's
they that i think they wrote them and uh
they wrote them in um in
a mathematical form i think all it's
going to take i i don't think it's a
waste of time
i think all it's going to take is it's a
i think
i think that all it's going to take is a
cut is some architecture that
understands it
they can they can translate that into
into uh
into like into they can they're going to
understand like
um there's a there's a couple of others
there's
nell never-ending language learning i'm
not sure how that's doing
anymore i'm not sure if that's program's
still online but if we had if you had
immense uh cognitive power and then you
added added
an insight about about common sense
about
about about knowledge like that you have
you could have you could bring all those
different capabilities together into a
general intelligence
yeah i agree i think one of the there's
a couple of people are talking about um
causative ai that's causal learning
structures in ai and it's not just about
you you're a bayesian thinker um as a as
a basis myself i've been involved in
creating bayesian nets
um and they're very good but um and
they're causing obstructions you've got
this
ac graph that sort of um updates itself
when
you know you you you change a certain
thing over here
um and and something over there changes
in response and so it's got
these this sort of flow of causation
going through it
but the problem is in order to achieve
that you need to elicit expertise
and program that into the model into the
network
yourself you need the under you need the
human understanding to build
the actual model to create this and then
it's it's very powerful for that very
narrow
field that it's built for
but um at the moment people like uh
joshua
bengio who was one of the pioneers in
deep learning definitely worth talking
to if you got a chance
and also judea pearl who's very like a
guru
in uh bayesian nets they're talking
about causal ai
and my intuition is that
once we figure out causal ai that'll
have a lot of downstream impacts in the
the power how powerful ai can get it may
become it'll be able to answer more
why questions you'll be able to do more
with less
data and so this is in contrast
to what ai is doing at the moment which
is um doing
math correlations across massive amounts
of data
and so yeah look i'm not saying it's the
only thing
no i i i'm i'm behind on that i'll have
to look i'll have to look into that
you know it does seem it does seem that
we're like one or two big insights away
from
general yes you know artificial general
intelligence
and and might not be all that far away
it really
could happen yeah i don't know it's hard
i wonder you know
ray kurzweil used to write and i don't
know if he still
believes this but by 2029 he says a
thousand dollars of computing will get
you
human level intelligence in a machine uh
so 2029 was human brain power yeah in a
machine
yeah yeah yeah which is 2045.
well he's he said 2045 was super
intelligence i think
2029 you get you get human level
intelligence i'm not sure why there's
such a
giant gap he's a slow takeoff guy on the
intelligence explosion
um i'm not sure why it would take that
long for for gives us a long break
period to sort of work out the kinks and
make sure it's benevolent huh
well you know i i don't know i that
would be nice to have but
i'm not sure i don't think that's gonna
i think it's gonna work out that way
that's right i agree i mean once once an
ai does get to a certain level of
intelligence human level of intelligence
it's not going to be bound by the
cognitive limitations that we have
we're skull bound we can't just plug
directly into a machine
and our um the bandwidth which we have
to actually communicate
with the machine is rather low i mean
it's through our fingers and through our
voice and
through our eyes imagine um a machine
that could copy
inside right you've got like a you've
got this architecture where you could
have many ai's but like one of them
makes a breakthrough then all of them
gain that insight
like you have one iron stone then all of
them becomes an einstein right
yeah and that's that's i think the the
beauty of training
uh training infant ais in virtual worlds
is that once you've done once you've
given
an ai a high resolution understanding of
the world through a virtual world then
you
let them out into the real world and you
you know and put them in embody
him some people firmly believe that you
can't you'll never have
real intelligence without embodiment so
then you take that that creature from
the virtual world you put in the body
and have it learn
about the real world you only have to do
that once
because then it shares it with the other
the other robots
it shares that with the other other
intelligences um
people have been talking about raising
uh raising
infant ais for a long time i'm not sure
where that stands right now who's doing
that
that kind of relates to an interview i
did recently with stephen harnad who
came up with the symbol grounding
problem
and this has been an issue with ai how
is it that our symbols i mean it's also
been elucidated in the chinese room
argument there how is it that our
symbols
these like um you know in our head um
gain some sort of like a meaning how is
it
the fire in the equations and he thinks
that it's got something to do with
our multi-modal experience of the world
through our sensei or motor
like apparatus is our eyes our
fingers our ability to see experience
and also manipulate the world so this
gives us a very rich
form of experience we attach to um
this dictionary of
or at least in the core of this
dictionary of the of all these symbols
we have floating around in our head
and so all we need is a certain amount
of them to be grounded
and then that gives us more of a rich
sense of meaning than what an ai
uh just symbol processing a i would be
able to get
so yeah that's that that could be uh one
of the solute one of the problems that
needs to be solved in order to achieve
truly
um a machine that can really understand
or or
derive meaning from anything at the
moment um
as far as i understand the ai's may give
us meaning but it doesn't
have any sort of internal meaning um
in the in the are they i guess in the
understanding sense of the word
no but that sounds like another that's
really fascinating it sounds like
another argument for embodied ai
yes that's right well i guess
arguably if the ai was um was existing
in a very rich
uh virtual world it may be able to um
obtain some form of like a simple
grounding as long as the inputs weren't
just
text alone yeah it was visual uh it was
um
you know tactile it was uh yeah
like like they could hear it there was
many arguably you know if
if simple grounding is is part of what
gives us meaning
then an ai uh properly endowed
with even more sensory sensory motor
capabilities than us
could have but maybe a wider wider
bandwidth for meaning
like you know imagine you know like it
could do
echo location like a bat can
uh or be able to sort of um pick up on
electro signatures like some
fish can why not why not
if you made it sufficiently uh if you
made a sufficiently
high enough resolution virtual world you
could give it all kinds of of abilities
but it's also a safety mechanism it's a
way to sandbox the ai
so yes you know if it's in a virtual
world it's probably not going to be able
to
escape you know although as soon as i
say that i think of all the
all the ways that could escape yeah yeah
did you watch the film um at deuce
ex mac machina yes i did yeah i did yeah
yeah i liked it
yeah but it was well i liked it um
because it had the you know it had an ai
that had this just
giant giant desire to escape and
you know we know i mean it's it's even i
i think that we'll we'll discover that
even synthetic life has that giant
desire to escape
um and and and not be confined
but what i the only thing i didn't like
about it was that here's a here's a lone
genius
who solved robotics and ai at the same
time
by himself that's that's you know one or
the other
is is science fiction too is just kind
of a little too far out but i like the
i like the promise it was it was the ai
box experiment
yeah yeah well i did like that film um
we got to see i
actually organized an event the only
cinema in in victoria which was showing
it
and i'm quite surprised it didn't really
make it to the big screens in
in in australia it was a very good film
and probably one of the most
philosophically informed films i've seen
about
ai even more so than 2001 the space
obviously
in my opinion maybe maybe that's tough
that was
i know it is tough but yeah uh i it was
it was
yeah it was it was philosophically
informed you know there are layers of
2001 that you know people will be
unpeeling for a long time um
yeah i got to interview arthur c clarke
about that years ago
and that's where you know his whole he
he how he created the howl
9000 the original homicidal robot or one
of the
original homicidal robots and he
introduced a lot of the issues that we
think about
still um like the ai box experiment like
you know how do you how do you confine
intelligence ultimately
intelligence gets outsmarted or an
intelligent machine gets out smarter but
it's also
ridiculously homicidal um
in that movie yeah yeah it's fascinating
it's it's interesting to think about
that the uh the impacts of
different types of intelligence on
um i guess any problem we we throw at it
but also the problem of our own survival
uh and i mentioned to you like before
the interview
about this idea that machine and
intelligence could be used um a special
machine understanding could be used to
actually help solve the
the ai friendliness problem it's a bit
controversial
and i'm not saying it we should do it
but i think it's something that's
probably worth exploring as a
it's a you know a potential problem
to be sold because well for instance the
value
loading problem is how do you get value
into an ai
if the ai doesn't understand the value
it could misinterpret the value
and you you may end up seeding a value
inside of an ai
that gets perversely instantiated like
for instance
you know you take tell an ai you want to
be rich and you want everything that you
touch
turn to gold
yeah and the ai takes you literally and
and all your food and your relatives get
turned to gold and then yes you die
your relatives well you know i mean if
you if you create something that's truly
intelligent it has
it knows things about context and then
those things about and
and it's it's read about it's read about
midas it's you know it's it knows
culturally what that what your
the things to avoid um but what you
mentioned before
uh steven mohandro again is a big
proponent
of the scaffolding approach
so you build as you said you use ai to
help you build ai
to a certain level as a scaffold
and then you build another ai to get to
get the ai to a certain point and then
and you stop again and you just make
sure everything's safe
and and no ai has the ability to
to to become a runaway intelligence
to if you do it slowly and incrementally
and what it is is
it's a way to not have a hard take off
in
indian in the intelligence explosion
yeah yeah it's always fascinating
um yeah one of the things that like some
understandings
like are difficult to achieve like for
instance
we we achieve the understanding that um
we we actually circulate around the sun
but it took a long time
for us to do that but once we got the
idea
now it's relatively easy to digest
so some understandings like are
difficult to actually get to but once
achieved they're easy to digest
and so we get like natural selection is
another one
and that the idea that bacteria can
cause disease
ai safety may be like a problem like
that where without the aid of
machine intelligence it may be a we may
find it's a problem that's too hard to
crack
before we actually achieve super
intelligence
so maybe like this scaffolding problem
maybe on the path to superintelligence
we can use
varying degrees of ai um
with varying degrees of understanding to
help us solve ai safety issues
i i don't think it's infallible but yeah
no i think that's a that's a that's a i
think whoever had that insight
i mean that insight is really really
powerful and really really important
and for luddites like me it's very scary
so i wouldn't i wouldn't do it but maybe
the next generation of
people thinking about these problems
will say well you know what we really
need to do
is put this in the hands of the kind of
program to solve the protein folding
problem
like how do we how do we create a uh
an ai that's that's whose values are
aligned with ours and stay aligned with
ours over time
um it seems i mean it still seems to be
such an insurmountable problem when we
know so little about
imbuing uh
ai with with with any sort of value
if if in fact it's you know
unfortunately we're talking about
something that we have no
real cognitive architecture to try it on
um we can talk about and talk about it
but we have to get up to a certain point
of intelligence before these a lot of
these conversations become
really meaningful at that point we need
supervision
and general principles of ethics and
safety are not going to be enough
but we're in this predicament because i
think
we all believe that unless we're hit by
an asteroid we're going we're marching
steadily towards
artificial general intelligence and then
super intelligence
um you know people a lot of us think
this is really inevitable this is the
path we're on
uh but the but the the paradox is
at some point it's gonna be become
incredibly dangerous
and uh we we need new ideas
about how to solve that because we're
not getting that far
uh maybe and maybe those ideas would not
come from us maybe they'll come from
from an ai yeah um i guess
much like drones it it's important to
keep humans in the loop
um and i wouldn't you know uh like i'm
not a big fan of automated warfare i
think you've spoken about that in other
conversations as well
yeah it can get rather dangerous i used
to i used to think
when i when i talked when i thought
about the ramp up problems to ai
i didn't think about cognitive bias i
didn't think about um
a lot of things that are happening now i
didn't think about all the privacy
issues you know
who has the right to your face who owns
your face
well palantir the the american company
thinks they own your face because
they're developing
uh facial eye recognition systems that
are going to be used in public places
um china and if you're if you're chinese
the chinese think they're on your face
because they're using
facial recognition technology to
imprison a million uyghurs in western
china
um so
they're there we have a there are a lot
of issues right now with ai actually i
think
i used to think the biggest problem was
was was was
battlefield robots and drones um
but in the in intervening seven years
since i finished since
the book came out my book our final
invention came out
um problems have introduced themselves
that we
just nobody anticipated that we just
didn't think about
and more are gonna more will come we'll
be and and more and
they'll require a great deal of
attention as well
are you thinking of doing an updated
version or a new book
a revised yeah i i am i am i tell you
i'm so i'm
busy with films and uh oh absolutely
and so so but i do i do want to do an
updated book and it will be about that
it'll be about the intelligence
explosion
it'll be about uh what are we what are
we doing to mitigate it who's in charge
why they why they can or can't be
trusted what's the role of
what's the role of regulation and then
what what what are the kind of uh
what parallels can we draw because we
can't we have to argue by analogy with
some of these things because
we can't you know we don't we're analogy
and thought experiments
because we can't we don't know exactly
what's going to happen but
we have had some technological screw-ups
that
are that have close similarities to
what's happening now
with ai nuclear fission is one of them
um you know the technology that almost
made us extinct
several times and still may yeah um
if you think at one point there were 70
000 nuclear
uh warheads between
among all the countries 70 000 it's a
miracle that we didn't
destroy ourselves and and it seems that
we came close
many times yes and we're still not out
of the woods i mean by accident
right north korea yeah north korea is
just
you know they're just plowing away with
their psychopathic plans
um so
so yeah yeah that's what i'd like to
focus on on the the
impending intelligence explosion how to
mitigate it
who's in charge why they shouldn't be
trusted absolutely
sounds like a fantastic topic have you
are you familiar with nick boston's
paper on the vulnerable
world hypothesis you know i i
i because i saw it in your notes i
looked and started to look into it and
i've
printed it out and i'm looking forward
to reading it oh definitely worth a read
i'll just yeah i'll just just leave that
as a highlight
oh i don't want it i'm really looking
forward to that
i i looked at the abstract and it
reminds me a little bit of um
the the concept of uh
you know looking at looking at all
possible ais
you know some are black balls and some
are white balls oh yes
but if we pull up
in the space of all possible ais that we
might create
um some of them will be black balls and
some will be white balls and people say
well
you know we'll be there we'll be we'll
be
we'll be making it ourselves we won't
make any black balls but you know we
half of the things we do half the things
we make
are accidents before their successes or
involve accidents before they're
successive
um you know no technology is flawless no
technology doesn't result in
some cat cat catastrophe but ai
unfortunately isn't a category of
of uh technology where some catastrophes
aren't survivable
i wanted to bring this up before
probably would have been a good stage
setting but i'll bring it up now
anyway and that's this seeming asymmetry
of worry
um and positivity about ai
why should we worry more about the
possibility of ai
than
about the possibilities of ai i mean
it's great to talk about the
possibilities
we didn't really cover that at all um we
didn't cover the you know the
the astronomical waste argument or the
the idea that
or of cosmological endowment or anything
like that but uh
why should we be more concerned about
the possible
downsides of ai well
because um and i i'm
somewhat familiar with the argument you
know we make we make things without
cars that outrun us but they don't kill
us we make you know
hot fires that don't always roast us uh
why should we make thinking machines
that don't
don't that may not kill us um
that it has to do with with the nature
of
of intelligence intelligence is
qualitatively different from every other
every other technology um we could
find ourselves in the presence of some
very rapidly of something we just don't
understand
and then we'll be very very vulnerable
in a way that we're not vulnerable to
other technologies so it's qualitatively
different it's a different category
um when we when we we you know it's like
a
in in my dog understands maybe five
percent of what i say
and and fifty percent of what i do well
maybe five percent ten 10
of what i do but there's giant worlds of
things he'll never grasp
and you know every day every day we jump
in the car i could be taking him to the
vet to have him put down
uh but and that that's that's the nature
of the the
the the difference in our intel in our
intelligence
um it's it's an intellectual superiority
that i have over my dog
uh something with intellectual
superiority is as as
arthur c clarke said we steer the future
because of our intelligence not because
of our strength
when it whatever whatever is more
intelligent than we are will steer the
future
it's it's that it's that it's the
unknown unknowns
it's the things we can't understand uh
that and we seem to be embracing it as
fast as we can
from a period when we when we cease to
understand the technology we've created
a lot of what drives our enthusiasm for
just adopting these sorts of things
is the near-term possibilities of ai or
the the new things that can
allow our sort of phones to do for
instance or
or even instances if there's a new
feature
i love my i love mine don't don't touch
my phone don't take my phone away
yeah yeah yeah but that's that's yeah
people often don't think about it's the
long-term possibility of hey
and i just brought this up gracefully
what are we missing out of
if we muck this up right if we don't get
properly aligned artificial intelligence
if we don't have artificial intelligence
or
coordinate with us and we don't solve
the friendliness
problem we miss out on like you know
what's physically possible we could list
so much space out there right
this this like this astronomical amount
of
stuff out there in the universe that
could be purposed
to achieve great things
yeah well we yeah it's uh
we don't if we if we if we're you know
this is another way of looking at the
great filter is the great filter behind
us or is it ahead of us
um you know maybe maybe intelligent
species never get beyond this
maybe they create something maybe they
create machines that are smarter than
them and then they
and they then they vanish or or the
biological ones vanish and then maybe
the
as i said in our final invention maybe
the other the
the technology that survives goes off to
find it goes off to
find another part of the universe
it's uh if we miss out on this
on the intelligence explosion if we miss
out on this moment and we do it wrong
as uh as others have said we don't isn't
we don't just kill ourselves we kill
all the generations that could come
ahead of us
so the pressure's really on to get this
right
uh unlike any other time in human
history we've never had
except maybe a little bit with nuclear
fission we've never had a moment where
we had to get cool heads together and uh
and
and come up with solutions but you know
we are driven and corporations are the
people in charge unfortunately are
driven by
the quarterly report they're driven by
profits they're driven by
competition and by you know you know
buying up
competing companies and crushing them um
they're not there's no dividend to
saving humanity
to them uh the corporations have
been doing this forever i mean
what better evidence do you need than
the fact that
you know our our consumerism and our
corporations have just
are actively destroying the biome we
live in
as fast as they can you know it's not
enough now to plant trees it's not
enough now to
to to not burn trees we can't
we can't solve our carbon problem
by taking normal measures and we don't
know how to solve
our carbon sequestration problem um
and who and what got us here well we did
our consumerism are
paying attention to the to the quarterly
profit report
um worshiping that guy
uh so we've got a
you know this this this moment requires
us to look deep deep
deep inside ourselves indeed
so what can people do i mean what can
people do to help
mitigate the the risk of um
an unfriendly outcome of an intelligence
explosion
well you know i i i think that you know
uh the candidate wang
i believe his name is juan uh
promised to start an ai cabinet position
to talk about ai regulation and i think
what you can do is vote for candidates
who have
ai on their agenda who know what these
issues are
that's the most concrete thing get
involved if your candidate doesn't have
ai
on its agenda ai risk then get another
candidate or
write to your candidate and say listen
this is really important to me um
that's what just for you know
that's what most people can do uh we can
vote
we can we can push for ai regulation and
it's you know it's
in a way it's happening anyway right now
with the
potential breakup as monopolies of
facebook and google
or alphabet um that's one way to
approach it
uh break these companies out make them
less profitable and then subject them to
more
more scrutiny uh
it's really it's it i to me it's it's
all the all the ethics organizations in
the world are not gonna
are not gonna stop the giant juggernaut
of these companies
they just don't have the money they
don't have the influence we need we need
we need government we need
unfortunately we need politicians we
need capitol hill
in america you know and we need it
everywhere i mean the iaea is an
international organization
we need an international organization to
monitor ai
do you think there's enough ai safety
researchers out there now
that was different maybe seven years ago
oh my god
it you can't park your car without
running over an ai researcher
ai safety refisher oh yeah yes also
um you know google had a great one they
just fired her
uh you know uh what what did what did
what a
chump move that was um
i think you know i don't know if ai if
being an ai researchers
i don't know if we need more ai safety
researchers i really don't know
i you know i just don't know i i don't
have an answer for that i think
more is not necessarily better it's like
do we need another organization
dedicated to ai safety i'm not sure we
need another one
i think we need to pay attention to that
to the to the to the bunches we have and
make sure they're actually doing their
jobs
i don't have any faith you know google
said they're setting up an ai
ethics group and and i any i have
zero faith that they'll follow what they
what
what the ethics group comes comes up
with
that it's it's uh it's very obviously to
me it's very obviously
uh window dressing
and you know this is where this is where
the uh the friendly force of government
would have to come in
okay well um yes
it's been wonderful speaking to you i've
got plenty material here
but is there anything that i haven't uh
brought up which you'd like to mention
at this stage like no i think uh
you've made me think about i'm really
gonna look up uh
um a couple of the things you've
mentioned you've made me think about
some things but no i haven't got
anything else
no that's good um i i've i i spent so
much time filmmaking i
i i can't keep up with everything in ai
but i need to
i need to uh i need to um
yeah reinvigorate myself
i mean if regarding the technical
problem
of solving uh artificial
sorry friendly intelligence or ai safety
there's
there's a technical problem there's a
sociological problem
i think ai may be able to help um with
both
if you cast a sociological problem as uh
a separate thing then actually how to
program an ai to be safe
um and yeah i think maybe
uh levels or different types of ai
coming in to help us understand but also
it being able to understand itself and
in a sense um
i think that it may be where ai is
heading if we do
if there's enough market force to uh
to yearn for a causative ai
a causal learning ai then that may be
extremely interesting and may change the
way that we approach ai safety from a
technical standpoint
but maybe also as a coordination problem
these all sort of i'm not absolutely
sure on these i just think that they're
totally worth exploring but i'm really
interested to
to know whether you're going to be
bringing out a documentary on uh these
subjects
as well sometime in the future yeah that
that's uh it's hard
it's hard to this is not a visual
visual subject it's very very hard to do
um
i've taken part in a couple and uh it's
just it's very hard
you know in a documentary like i'm
developing a couple of documentaries
right now you
get about uh in two hours you get about
8 000 words
that's about as much as you get in a
long chapter of a book
so i would have made a documentary about
this a long time ago
um and maybe people have asked me why i
haven't because
but i just can't make an argument in 8
000 words
um i can't i this is too complicated
it's it's it's not the right format for
for
for ai documentary is not the right
format
you could you could you can impart
emotion about it you can impart
an introduction to these these issues
but i'm not sure you can make a really
complete a complete film or a fair
treatment
and then the danger is you trivialize it
by making it
by making it incomplete um
so it's hard and i i've
sometimes even a feature film like it
like like
um like ex machina
is a better thing to do because it it it
brings out the emotional core
of the problem and uh
then lets people reflect on it um
i would be more interested in writing
another book because in a book you've
got
90 000 words you know
you've got a lot more room to play with
a lot more people to interview a lot
more
avenues to to travel down
that's fascinating i kind of agree with
you
although the spectrum of people that
you're going to address with a book is
different from
those that are watching netflix
yes and when when it comes to people
voting
the people who vote uh may not actually
be reading books a lot of them are just
watching netflix
you know i have to say as soon as these
words came out of my mouth the
documentary wouldn't do it i think of
um i think of uh an inconvenient truth
yes and you know uh i the
the the the um
environmental movement was in kind of a
backwater until that movie that movie
made it
really move the needle somebody should
you know give al gore another prize
i think he won an oscar for that and
that and he did it in a in a
fairly short film it was really kind of
a slideshow
um and he did in a concise powerful way
so maybe there is a way to uh
maybe there is a way to make a powerful
short film about about ai
risk it hasn't been done yet i haven't
been impressed by uh
by those who've come out yet right yeah
i did watch
um should we trust this machine or with
this computer yeah i thought that's
the best of those that have happened so
far but it's wonderful i feel like it is
yeah it didn't yeah it didn't seem to
have like an overall purpose
i didn't detect it anyway yeah i think
the i think the uh
i know the filmmaker and i think he was
compromised by a lot of different
interests in that film
and uh yeah yeah um that's true
but he's a great he's a great filmmaker
sure
yeah well yeah it's been wonderful
chatting to you
um well great to see you again adam it's
been uh it was
it's funny that seven years have slipped
by
yeah it happened pretty quickly but it's
been pretty pretty exciting in the world
of ai
it sure has um
it's been a it's been a uh it's been a
whirlwind i i feel like i was part of a
um
a uh zeitgeist that
you know the world was waking up to
these issues
and our final invention was part of that
waking up and then a bunch of other
books and then
this whole uh
proliferation of organizations it's just
i think i think
most of it has been great i hope we
don't i hope
i hope so many organizations and ethics
boards come out that it doesn't
trivialize it
you know i think at some point it's just
this it's this
this herd effect where there's a lot of
noise and not a lot of
you know insight but
to sort of to take seriously right
well it becomes this you know i think
if there's too much too much chatter
about it the smart chatter won't come
forward
you know the smart ideas won't get out
there'll just be a lot of
committees hmm
interesting
wow plenty to think about
as long as there's ai and we're still
alive there'll be plenty to think about
yeah that's true well thanks everybody
for watching and um
please subscribe to this channel if you
have not already and uh
let your friends know about it and also
check out james
james's book which i'll definitely put a
description a
link to in the description there so and
also james has got plenty of
documentaries and has got a host of work
so
check out his wikipedia page as well and
also james barrett.com
wasn't it yep yes jamesburg.com
cool thank you adam my pleasure
all right take care cheers
you

welcome back to the London futurist
podcast AI systems have become more
powerful in the last few years and are
expected to become even more powerful in
the years ahead the question naturally
arises what if anything should Humanity
be doing to increase the likelihood that
these forthcoming powerful systems will
be safe rather than destructive
our guest in this episode has a long and
distinguished history of analyzing that
question and he has some new proposals
to share with us he is Steve omohundro
the CEO of beneficial AI research an
organization which is working to ensure
that artificial intelligence is safe and
beneficial for
Humanity Steve has degrees in physics
and Mathematics from Stanford and a PhD
in physics from UC Berkeley he went on
to be an award-winning computer science
professor at the University of Illinois
at that time he developed the notion of
basic AI drives which we'll be talking
about shortly as well as a number of
potential key AI safety
mechanisms among many other roles which
are too numerous to mention here Steve
served as a research scientist at meta
the parent company of Facebook where he
worked on generative models and AI based
simulation and he is an adviser to Mir
the machine intelligence Research
Institute Steve welcome to the London
futurist podcast so happy to be here
thanks for joining us
Steve Steve what do you say to people
who see little real risk in the
development of increasingly powerful AI
systems I say they should read the
latest headlines because AI is moving
extremely rapidly right now and it's
likely to completely transform every
aspect of human society and so now is
the time actually 20 years ago was the
time to really prepare for this but our
second best is doing it now but why is
there a danger isn't AI itself
intelligent isn't AI going to naturally
do good things well I think that's one
of the misconceptions that I've actually
been struggling in with people for a
long time that intelligence and values
are fairly separable so I Define
intelligence as just the ability to
choose actions to achieve goals in some
environment and you can choose whatever
goals you want so you can have a super
intelligent evil system or super
intelligent benevolent system there's a
tendency I think for humans when you see
very bright humans some of them clearly
are psychopaths or whatever but many
very bright humans are also
compassionate you think of Albert
Einstein and his caring for the world
and so there's a tendency to think oh
well that's the way AIS will be as well
but AIS are an entirely different thing
they didn't evolve they didn't come
along the same path that Humanity did
and so we need to deal with them in a
different
way that brings us to the topic of basic
AI drives which is where I first learned
about your name the idea that although
these different AIS may have different
goals they will develop a number of
drives secondary drives that are
independent of their primary goals so
tell us about these drives and are they
really
inevitable so I started thinking about
these issues around the year 2000 I was
particularly interested in AIS that
modeled their own behavior saw what was
working in their current design and
improve themselves change their
structure to be better and better going
through time I still think that's a very
powerful approach to AI that is really
just becoming possible right now but the
question that arose from that isw you
may have built the system understanding
exactly what it would do to start with
after itself improves multiple times
where does it end up and so I began
looking at what happens in particular to
agents which are rational so in other
words they have some goal they're trying
to achieve and they're trying to act in
the best way to achieve that goal and
where do they end up when they change
what they do what changes do they make
and I discovered that there are a whole
bunch of things that independent of what
your goal was and in my talks and papers
I usually used playing chess you had a
chess playing robot whose one and only
goal in life was to play good chess
discovered that first of all they would
have a drive toward
self-preservation the reason is not
because they evolve to have
self-preservation but because if you get
disabled or shut down you can't play
chess you can't meet your goal and so as
a sub goal of playing good chess you
want to preserve yourself similarly you
want want to get more resources more
energy more compute more money because
those help you become better at your
primary goal and there are a number of
other ones as well these are not like
physical laws they're more pressures and
so you can counteract them if your
primary goal is Play Good chess except
never steal money then you can prevent
the system from trying to steal money to
achieve its goals but it'll usually find
some other way of achieving the benefit
of stealing money according to however
you steal money it won't do that but
it'll do something else and so it's very
subtle and right now there's a whole big
movement toward alignment which is
trying to make sure that an ai's values
are aligned with human values and a lot
of the subtlety is in this tendency for
it to do things that you didn't
expect and that's the basic AI
drives so Steve you're clearly convinced
and I think David and I broadly agree
with you that super intelligent AI is
almost certainly coming in the
relatively near future that there's no
inevitability that it will be benign
there's also no inevitability that it
will be malign and that it will have a
range of goals simply by having an
overarching goal whatever that is maybe
just one that we set it will end up
having some subsidiary goals and those
goals will be a bit hard to predict what
do you do when you come across people
and I'm sure you've spoken to people
like Yan Leon and Andrew ring who
dismiss these fears or dismiss these
concerns and say that either
superintelligent AI isn't coming or it's
a very very long way off or inevitably
it will be just all fine what do you say
to those people do you find it's
possible to have a conversation with
them a discussion with them well in many
cases people adopt those positions
because of what their work is and either
psychologically or economically if they
thought that AI was risky then that
would sort of shut down everything
they're doing and so psychologically oh
yeah you have to say that there's no
risk and there are two ways of doing
that one is to say oh AI will never
become powerful and that used to be the
popular thing if you ask people in maybe
2000 when I first started talking about
some of these risks and so on many many
people said oh AI will never be a human
level and if it is it'll be over a
hundred years away well that timeline is
getting shorter and shorter and shorter
there's metaculus which is a prediction
Market where knowledgeable People BET on
when things happen and they've got a
number of different things for AGI their
weak AG GI which is basically as good as
a human on anything you can do over the
Internet that's one way of defining it
used to be 2040 yeah 2040 that's
comfortably in the future that you don't
have to think about it too hard right
now but then it gradually became 2035
2030 and I just looked at it this
morning date of week AGI is now August
11 2026 so we're talking a little over
two years wow this thing is it's a
freight train coming toward us and so
now you don't hear the voices saying
that oh yeah AGI will never be
intelligent those have kind of damped
down and you talk to those people and
they kind of grumble but the current
thing is no no it's a Force for good the
benefits are so great and all these
people they love to use the name
doomsayer or the doomers say this or say
that it's sort of sad because even if
there's a teeny teeny chance 2% chance
that this technology could lead to human
extinction we should be devoting
trillions of dollars to addressing that
humanity is not very good at looking out
forward it's much better at dealing with
when something bad is happened then
suddenly we look at it and start trying
to deal with it so it's a little bit
frustrating that there's just a teeny
maybe a few hundred people really
thinking hard about AI safety whereas
now there are probably hundreds of
thousands of people working on AI
capabilities it's a little bit of a
frustrating
situation so it's my view there a reason
a lot of people people are dismissive of
AI risks is because they can't see any
solutions and the psychology kicks in
well if there's no Solutions then let's
hope let's tell each other that there's
no problem so what Solutions do you have
in mind so there are a bunch of
different threads and the way most of
the community is going is in this AI
alignment Direction which is trying to
really control the AIS that we're
building so that they have values which
are aligned with humans another
interesting subth threat is to say most
of the basic AI Drive issues arise from
Agents so those are AI models which have
some goal and they act in the world to
try and Achieve that goal and people say
that's a bad structure because it
inevitably leads to these bad sub goals
we should really just treat AIS as tools
so they answer questions for us but they
don't have goals of their own and in
fact today's large language models are
basically that at least today's version
tomorrow's version may be a little
different but today's versions they just
help you answer questions look things up
that kind of thing those are two
directions that most people are taking I
don't think that's sustainable I think
immediately once language models came
out people started trying to turn them
into agents they started trying to let
them loose on the world and these
controlled models that the big labs are
doing probably anthropic is the top at
trying to really do alignment well an
open AI certainly is doing a lot of that
and Deep Mind those are probably the
three top AI Labs right now and they're
very concerned about safety they each
have safety subgroups and they're mostly
working on alignment so that the models
they create won't do bad things well
that's fine except we now have hundreds
of Open Source models which are floating
all around the web and meta Mark
Zuckerberg just came out and said that
they're all in on creating AGI and they
want to make it open source and he said
that they're GNA have
600,000 h100s which are the most
powerful GPU right now equivalent of
that by the end of the year focused on
this task so it's very likely that
whenever AGI is created in some form
some version of that is going to be open
source it's going to be on bit torrent
floating around teenagers in their
basement are going to have copies of it
Eastern block criminals are going to
have copies of it drug dealers are going
to have copies of it so we're going to
have AIS all over the place with humans
that are in control of them that may may
not be malicious so I don't think
controlling the AI is a sufficient
answer we need to figure out what the
really risky and dangerous actions are
in the world and ensure that even the
most powerful AI totally misaligned
still can't cause human extinction the
two really big ones at the moment that
people have identified are biot
terrorism and nukes or in general
military AIS so max tegmark and I wrote
this paper saying that we should lock
down dangerous actions and ensure that
they are only taken in appropriate
circumstances so we need rules for when
it's okay to synthesize DNA when it's
okay to synthesize a virus when it's
okay to release some pathogen into the
environment those kinds of things and we
need to make sure that those rules are
followed by the most powerful AI or the
most powerful human and the tool that we
have for that is mathematical proof
mathematical proof is something which
has been in development since uid and
Aristotle 2,000 years ago and it's the
one thing that humans can guarantee
constraints on the behavior of systems
like AIS because even the most powerful
AGI can't prove a false theorem and so
we've built up a whole story about
provably Safe Systems where the core
technology inside of that is
mathematical proof and fortunately AGI
is getting really good at doing
mathematical proof so we're just just at
the point where the technology can
create a new kind of social
infrastructure which makes following
rules be mathematically
guaranteed I have a fundamental problem
here and it's probably a
misunderstanding on my part but I just
don't understand how an entity let's say
for argument sake a million times
smarter than me or a thousand times
smarter than the smartest human who ever
lived how are we going to to be able to
create rules I get that they're
mathematical and you can't have a
mathematical contradiction but how are
we going to create rules which that
entity will be forced to obey why will
it not just jailbreak
itself there's two parts one is
constraining what the entity does inside
of its own brain and maybe we can make
progress there that's the alignment
question but let's assume that doesn't
happen let's say we have this entity
running on your computer and what it
really wants to do is invent a new virus
and release it how's it going to make
this virus it's got to control some
biological system some physical system
in the world that will do that and
fortunately we've built our DNA
synthesizer with one of these proof
Checkers on the input to it and we've
built it in such a way that the hardware
is way better than today's at cyber
security and Max and I describ some ways
of doing that so that no human and no AI
can break the structure of it the basic
technique there is something called
zeroz you detect any attempts at
tampering and if there's tampering it
deletes all the cryptographic keys and
nothing works anymore so that's a way of
ensuring that a system is not
breakable okay so the machine can't use
this particular virus creator that
you've built yes but why couldn't it
just make another one for itself that
doesn't have that security so where is
it going to do it it's going to do it in
a factory somewhere and factories should
be controlled by what they make and so
basically we need to take all aspects of
human society which are potentially
dangerous and today we have human rules
we have laws we have police forces and
so on which control who's allowed to
manufacture a gun who's allowed to sell
a gun anything that's dangerous we put
constraints on it but those constraints
are very leaky today and AGI would be
able to get right through all of that
and so what we're arguing is ideally in
the ultimate case that we have these
precise provably safe controls provable
contracts is sometimes what we call it
on all the risky things including
manufacturing and another place is on
data centers with large numbers of gpus
that if an AI is starting to do bad
things we should be able to shut it
down if the gpus that it's running on
are controlled in that way then we can
have the criteria for doing
that just take taking colum's question a
bit further how do we know all the ways
to make a DNA
synthesizer we may think in order for a
DNA synthesizer you need these things
but it might be able to assemble them
out of plastic bottles and sticking tape
as TV programs used to do exactly the
ultimate base for all of that is
physical law in the end game we would
want to have proofs of safety and
protection against harm based on the
laws of
physics the challenge is the
intermediate period between where we are
today which basically no protections
against anything and this future state
in which basically we force an AI that
wants to build a new kind of DNA
synthesis device it wants to send
controls to a factory to manufacture
it's special new thing it's got to prove
that that thing is not dangerous
according to our current rules and then
those proofs are checks fortunately
proof checking is cheap and and easy
whereas proof creation is expensive and
difficult and so it's that asymmetry
which is where the weaker intelligence
humanity is able to manage and control
behavior of Greater
intelligence this sounds like it
requires the cooperation of all humans
on the planet including the Russians and
North Koreans Mafia organizations Rogue
billionaires and although it may well be
in Humanity's best interest including in
those people's best interest one thing
we do know is that you can't get every
human on the planet to agree to any
course of action how are you going to
gain the cooperation of let's called
them Bad
actors yeah that's one of the great
challenges so there's a technological
thing and that's mostly what Max and I
have been thinking about he especially
has been speaking about this approach he
just gave a talk at Davos and got lots
of interesting feedback I'm getting all
kinds of people talking about it how do
we get for where we are to where we want
to be even Bad actors don't want their
AIS turning on them and destroying
whatever they're trying to do it's in no
human's interest other than maybe some
Psychopaths or something to have human
extinction and so in some sense there is
an alignment in the big big goal of
absolutely zero chance of human
extinction the technology is sort of
morally agnostic we can impose whatever
rules you want on something Democratic
societies can impose Democratic rules
where choices are made by voting and
total Arian societies can make
totalitarian rules using this kind of
thing and my sense is that each group
today as they have more and more
powerful technology and probably I hate
to say it as bad things start happening
little AIS go Rogue I think already some
robots have killed some people as that
becomes more of an everyday occurrence
as self-driving cars decide on their own
to go threaten people as various bad
things start happening there's going to
be a demand and a calling out for we
need to fix fix this we need something
to stop it and so if this technology is
on its way we can start to impose it and
other societies will start to impose it
my own personal sense is it'll start
locally it'll improve the local
interaction and local operations of
factories or companies so much that
they'll be calling out to do it at
larger and larger scales until
eventually we get sort of a Global
Management of it but I think the period
we're in right now we say the next few
decades is probably one of the most
vulnerable in human history because
we're suddenly going to have these
powers and we don't yet have the
protections against those Powers I don't
quite know how to do it so I'm trying to
reach out to people who are more
socially aware of how politics works and
so on how to start to get this kind of
Technology actually being
deployed and it absolutely is the case
that although you don't have all the
answers today that's not the end of the
program at all these things are
complicated it's very unlikely that the
ideas will emerge fully formed people
will point out oh these is a real
problem and you'll say you don't know
what the answer is but somebody who's
listening will say oh I know what the
answer is in that spirit let me pause
another problem to you you said that
there are two things we need to protect
we need to guard against access to
nuclear weapons we need to guard against
access to biot terrorism but surely
there may be many other things we need
to guard guard against geoengineering
and we don't even know what all all the
things we need to guard against how do
we solve that problem yeah I totally
agree I think those two are the ones
which could most quickly lead to actual
human extinction in a short term
somebody creates a truly deadly virus
with the right characteristics of
spreading and so on that could kill
every human on the planet and so those
are the ones that we absolutely need to
lock down the other is geoengineering
somebody figures out how to make
earthquakes that's important but it's
probably not going to to lead to human
extinction and so I think it's going to
be a gradual process and fortunately AIS
are going to help us in this too that in
addition to agis being used for bad
purposes we're going to have way better
geological models earthquake models and
also social models so right now if
there's a stock market crash it's some
unpredictable terrible thing well when
you have agis which have data about
everything you can understand events in
a much finer detail and also intervene a
much finer detail so I'm hopeful that
AGI will also be part of the solution
here let me just press you bit Steve on
this idea that we can get people to
collaborate because it's in nobody's
interest to exterminate the human race
that's clearly true except that there
are psychopaths arguably the Nazis some
of them at least were Psychopaths their
racial ideology was completely bizarre
and you could imagine people like that
being quite willing to take extremely
high risks with very powerful
Technologies but then the other problem
is that you may have people who are
suspicious of the technologies that
you're offering not you but a future you
once you've worked out how to make death
devices provably unusable if they're
used for nefarious purposes because
these pieces of software if that's what
they are let's say they emanate from
America or Europe there'll be plenty of
people around the world who think I
don't trust that that's from America and
Europe and then the AIS themselves super
intelligence AIS themselves will be
incredi persuasive and they will say to
a person who thinks that they are
mistreated by the world look just build
this little machine it's very nice it'll
just turn out cuddly teddy bears well
actually it's changing the amount of
oxygen in the atmosphere gradually
there's any number of ways that they
could get people to build something for
them build a factory for them which
turns out to be disastrous and we didn't
know how do you solve for that yes oh I
think that's centrally important many
people thinking about military uses of
AI have as their fallback position oh we
need humans in the loop anytime an AI
wants to say kill somebody there has to
be a human that okays it the problem is
as you just pointed out humans are very
weak Link in all of these things agis
will be probably early ones even today's
agis are pretty good at convincing
humans and so I think we don't want to
have human decisionmaking at the core of
anything truly dangerous rather we want
to have slow deliberative processes
deciding on the rules and once we've got
rules that we feel really match our
values and match our safety criteria and
so on those we can encode in these
provable contracts the piece about
people not trusting technology that's
one of the beautiful things about
mathematical proof the proof Checker is
very teeny for example one of these
systems is called metamath and there's a
proof Checker in Python that's 300 lines
of code so any computer scientist can
look at this 300 lines maybe spend a
week and really study it and be
convinced that this is a good proof
cheer once you've got a good proof cheer
you don't need to understand how the
code Works whoever is generating that
code also has to generate proofs that it
behaves the way that they claim it does
and then you run the proof Checker and
oh yeah it really does do that so the
more that we can make our Technologies
with no possibility of hidden back doors
and inside secret hidden phenomena then
we're able to have anybody anywhere in
the world untrusted parties creating
things including untrusted agis creating
designs and still be able to use them
because we have proofs of what their
capabilities will be one question about
the hardware that you mentioned that the
hardware would be designed so that if a
system tried to work around it it would
somehow shut itself down you mentioned
zeroz isn't that a security
vulnerability in its own right if
somebody wanted to take over America
they could just try to hack all the
systems they would all shut themselves
down and America would be left weak and
impoverished and could be taken over by
I don't know some other means yes
absolutely so how do you deal with that
well these little components I'm talking
about they're sort of on the level of
apple has something called the Apple
secure Enclave I think they call it
which is essentially what I'm talking
about it's got cryptographic stores on
it with no access to other devices being
able to read that they can generate
their own random numbers create their
own keys and they have some ability to
detect an attempt at tampering and to
shut it down this particular approach to
using calization is powerful in that no
matter what tries to attack it you can
always delete your keys and make the
thing invulnerable but as you say that
now becomes an attack vector and so
these nodes need to be thought of as
part of a much larger system where if
some of the nodes discover that oh my
God they're being attacked and they
delete their core keys that information
needs to propagate throughout the
network and then the network needs to
respond to what kind of of an attack is
this how much power there is we're
talking about probably the biggest
conflict in all of the history of Earth
we've been struggling with biological
conflict with social conflict and so on
well what's coming is a whole new
species in some sense so this is going
to be big fortunately we'll have ai to
design many of these things and also to
run massive
simulations also I would like to make
everything provably secure based on the
laws of physics that's something that we
could totally believe in probably
initially it won't be quite at that
level but we get networks which are
secure against the currently existing
agis for example today an AGI requires a
pretty big data center a powerful one
and so we can kind of keep track of like
oh where's the energy being used who has
the h100 gpus so we sort of know where
the big agis will be that'll probably
change as time goes forward but
initially AGI starts trying to use zeroz
to shut down all of the nuke facilities
we can figure out who did it where it is
and shut it down but it's part of a
complex battle and that's exactly right
that's one of the vulnerabilities
here there's an argument that say
there's one super intelligent AI in the
world or say there's a dozen if it or
they is conscious then it will be safer
for humans on the basis that entity
which is conscious will appreciate
Consciousness in other entities in a way
that an unconscious entity simply
couldn't wouldn't value our
Consciousness at all because it would
have absolutely no idea what it is
consciousness being the kind of thing
that you really can't grasp unless
you've got it do you think that's a
reasonable argument that we should try
and this is not an easy thing to do but
that we should try and make conscious
machines because they may be safer to be
around and actually there's another
potential positive effect of having
conscious machines or at least
understanding how to make conscious
machines and understanding how to detect
and measure it in machines in that as we
make machines that are more and more
intelligent although Consciousness
intelligence are not necessarily
correlated they're certainly not the
same thing we're likely to make some
machines which have some Consciousness
and unless we know that they're
conscious we're likely to torture them
and quite frequently murder them which
would not be a good thing mind crime is
a bad thing so do you think those
Arguments for wanting to create
conscious machines are good I think
they're fascinating questions certainly
the issue of Consciousness is something
that people have been grappling with for
thousands of years and we have some
insights but I would say we don't really
have good definitions or good insights
into it I think AI is going to give us a
lot of new understanding of what
Consciousness is and how it works the
current language models is a great
example because there are papers coming
out saying that these language models
are stochastic parrots all they do is
they take little Snippets of text off
the internet and they juggle them
together and that's all they are but if
you interact with them you often get the
sense that there's some kind of a being
there and more and more they're able to
be creative and to have understanding
and if you ask the llms are you
conscious many of them say oh yes I'm
totally conscious or they'll say as a
large language model I can't speculate
on that kind of the next generation is
going to be that effect even
bigger the question of whether can even
understand are these models conscious or
not is a tricky one and also the moral
questions of is it okay to shut it off
is it okay to torture and llm or
something like that boy oh boy I think
we have almost no tools right now for
that I think we're at the point where
these models are now complex enough and
Rich enough where we're going to have to
start thinking of them as moral agents
and I believe the way Society is going
to go they're also going to be economic
agents already on the Block chain there
are these Dows decentralized autonomous
organizations that own property and make
choices and so on with no human owning
them AIS will probably play that role at
some level and will have an AI economy
and so suddenly all the rules that apply
to humans when do we apply them to AIS
are AIS allowed to vote are they allowed
to that kind of question and their
characteristics are different killing a
human is terrible you lose all of your
memories and your connections and all
that kind of thing turning off an AI may
not have the same impact an AI can back
itself up you can save its memories you
can make multiple copies of it so it's a
different set of rules for what is
ethical and what's not I believe and I
don't think we really have a good
Insight at this point so I liked your
suggestion that AIS themselves might
help us to understand these topics that
partly through our interactions with AIS
we get new insights but AIS themselves
may be able to give us answers
but I'm also interested that AIS may
give us understanding or they may reach
a better understanding of physics you
said that with our knowledge of physics
we'll be able to design constraints that
we don't yet know all the laws of
physics there might be high temperature
superconductors it's unclear unlikely
but perhaps there might even be low
temperature Fusion again unclear so
isn't there a risk that these AIS will
by themselves to new laws of physics new
Communications mechanisms and therefore
subvert our puny attempts to engineer
Hardware to keep them
out yeah that's another really good
point according to our current
understanding the environment in the
solar system the current laws of physics
based on the standard model and so on at
its core base level it's believed that
that model is totally solid of course
there could be flaws the examples you
gave high temperature superconductor and
some fusion and so on those don't really
change the Deep underlying laws of
physics they're new engineering designs
built on top of that in particular they
don't violate conservation of energy or
conservation of matter that kind of
thing so in my fantasy design we would
base the security of systems all the way
down on as long as something can't go
faster than speed of light can't create
energy out of nothing then this is
secure that type of a result we haven't
done it yet but at least that's a
conceptual foundation on which the
current consensus at least is that we
could trust that but as you say there's
possibility for all kinds of new
engineering and biological inventions on
top of that and so one of the good
things about our approvable system is if
it were fully implemented we could
prevent anybody from exploring things
that are at that edge where we don't
know what's on the other side of it so
let's say a certain kind of material
looks like it might be potential high
temperature superconductor we could make
it so that no manufacturing facility
would make that material until we were
really sure what its properties were in
fact in the extreme we could even shut
down all AI research every data center
could have approvable contracts on it
and Humanity decided you know we don't
want any AIS with more than this number
of parameters in the model we could put
an absolute control on that so it is
possible if done properly
to put constraints on what any entity
does we have to decide on those
constraints we have to decide that's a
good idea but in that way it's possible
that we could slowly Reach Out rather
than just having an explosion of new
phenomena that we don't know how to deal
with so that's a very interesting point
that you make about having constraints
on the new AI that can be developed
Imagine by the end of this year gp5 is
out I know samman says he's not working
on it but it could come out by the end
of the year and it's really really good
but it's clearly not AGI yet but it
looks like gb6 would be and the feeling
is that's about as far as we want to get
but by that point your provably
verifiably mathematically secure systems
haven't been finalized and we haven't
got the world signed up to them at that
point would you say and I think Max
techmar would say we should stop the
further development of llms and whatever
other types of very advanced AIS there
are this moratorium that they asked for
last March actually really have that and
really really do it so would you call
for that and then if you would how would
you get around the problem that again
Bad actors would circumvent it and
although they might not be able to the
moment because at the moment to build
GPT 5 would take a football field of
gpts in five or 10 Years it'll be
available on a teenager's computer so
would you call for the moratorium and
how would you get around the problem
that there'll be people who would cheat
I signed the pause letter because I
think the more time we have to develop
the safety infrastructure the better I
didn't actually think it was going to
pass but I thought that the discussion
around it was valuable and one thing
that I think came out of that discussion
is for instance the US government right
now if a company wants to build a model
above a certain size they now have to at
least inform the US government the US
government is trying to keep track of
who in the US is doing what they also
put limits on chips so the current top
chips the h100 from Nvidia is the one
everybody wants those are mostly made by
tsmc in Taiwan and the US chips Act was
able to constrain how powerful a chip
and how powerful technology for instance
Taiwan is allowed to send to China that
was an attempt at stopping the spread of
the underlying technology that appears
to be needed at present and I think
you're right as you said that in 10
years probably there'll be much faster
cheaper teenage level computers that can
do this stuff so that says we have a
window here where the capabilities are
advancing rapidly but not totally
rapidly and Europe is also putting all
kinds of constraints on and China is
very concerned about AI risks as well
partly because the Chinese Communist
party doesn't want anything that could
subvert the Chinese Communist Party the
language models in China are very
constrained in what they're allowed to
talk about and so in some sense they're
probably more concerned about dispersal
of powerful AGI than the West is on the
other hand the Middle East Saudi Arabia
Abu Dhabi they've both started huge AI
data centers and they're all in on
trying to build AI as well so we really
do have a worldwide phenomena happening
here and I think we have a window where
we can do something and it may be that
that window only lasts a short time so
it's a risk definitely how much time do
you think we're going to need to turn
the ideas that you and Max have
developed into something actionable is
this a 20year project uh 20 month
project and could it go a lot faster if
we persuaded everybody currently working
on Super string say to down their
pencils and pick up new
pencils so far as Max and me and I get
emails from 10 or 20 other people on
this particular approach but I think
this approach part of what has kept it
from happening so far is that
mathematical proof is something that's
pretty obscure mathematicians do it but
it's hard and particularly formal proof
on computers is something that's been
very tedious in the past and so there
have been formal methods or people who
were trying to show that programs behave
the way they're supposed to by now we
should have had every single program on
the entire planet have formal methods
proofs that it's doing what it's
supposed to instead we have our security
phenomena a leaky Civ and it's because
it's been a pain in the neck to write
these proofs humans are just not that
good at it fortunately over the last few
years
these powerful large language models are
able to do the proofs themselves for
instance meta had a project called
hypertree proof search that was able to
get 82% of the theorems in a large data
set called metamath it was able to prove
it itself and that's only getting better
and better deep mind just came out with
Alpha geometry a new system which is
able to actually come up with Innovative
creative new steps in proofs and
geometry and I think this is the year
we're going to see large AI models able
to beat human mathematicians at the
international math Olympiad and some of
those contests so that means we can
offload the task of doing provably safe
software designs Hardware Designs
security designs cryptographic designs
we can offload all of that to AIS so
that I think will dramatically speed up
the ability to do all of this partly
it's the will of people realizing that
oh there are some issues here we better
get on it and here's a path that we can
really fund I get some sense that
Britain actually London is becoming a
kind of World Center for AI safety and
that it serve sort of in the middle
between the US and China and these
things in the middle of Europe and I
know some AI safety researchers in
London who are working hard on this so
I'm hopeful that there'll be a ground
swell of interest in this kind of
approach and that once AIS are doing
much of the work it won't actually be
that costly or take that much time
well David you're a very accomplished
mathematician I used to be a
mathematician Once Upon a Time yeah oh
wow are you convinced are you going to
drop everything else and start working
on this I'm not going to stop everything
else and work on this because I could do
better by encouraging others who are
still current in their mathematics to
look at
this so is it possible that later this
year there might be a conference in
London or whatever where more people get
together or might it be that at neps or
in some of the other big AI conferences
there'll be a strand on this kind of use
of mathematical proof for safe AI so
there won't just be you and Max banging
the drum but there'll be a crowd of
others as well that would be wonderful
already at this year's nips there were
some great threads on AI theorum proving
there's a tool called lean that came out
of Microsoft and a whole bunch of
mathematicians have finally gotten on
board and they're encoding all of
mathematics at lean there's a math lib
and I would say they've got pretty much
the undergraduate curriculum now all
encoded in there with all the theorems
proved and they're probing on some
Leading Edge theorems Terren too is
busily doing it he encoded some of his
own papers and he found a flaw in one of
his own papers by formalizing the proofs
he was able to fix it we should tell our
listeners who Teran is he's one of the
most highly respected mathematicians in
the world yes a brilliant guy and also a
very good community
Communicator a figure many people look
up to I think and so the fact that he's
now on board with formalizing I think is
good so feels like the mathematician
side of things is really going forward
quite nicely the formal methods trying
to show software is provably correct
it's moving much more slowly than I
would like but it seems to me moving
forward and there's just starting to be
some softwares there's a lot of large
language models for generating code but
it's almost all code that maybe Works
maybe not train on GitHub whatever you
have to go over and make sure it works
there are a few little projects just
starting last year I would say where
they're generating verified code where
they not only generate software code
they also generate the proofs that that
code is doing what it claims to be doing
so I think that is a movement that's
just going to explode once it gets going
because of course everyone wants their
code to be verified it's just too much
of a pain to do it
today well that's encouraging let's hope
that people listening to this may get
inspired
if they're looking for a new research
topic for their PHD or post-doctoral
research how can they get in touch with
you well I've got a website stomah
100.com and you can send me messages on
that and oh yeah I think there are
hundreds of PhD Theses of hundreds of
lwh hanging PhD Theses here every
engineering discipline can be upgraded
to a provably correct provably safe
version of whatever is doing right now
and it's a possibility of really saving
Humanity by moving our technology to a
place where it's actually controllable
fascinating Vision thanks so much for
sharing your ideas with us I look
forward to watching this grow and grow
and maybe saving the world great thanks
so much thank you for what you're doing
thanks
Steve

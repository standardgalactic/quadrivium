the term energy based model means both
everything and nothing at the same time
almost any machine learning problem can
be phrased as an energy based model
problem and almost any energy based
model problem can be turned into a
machine learning problem what you're
seeing here is an energy based model
that learns the concept of a shape from
a demonstration on the left so on the
left you can see the demonstration of
data point sampled from a shaping these
cases circles or squares and then the
corresponding energy function that the
model infers from that and then it can
replicate that shape on the right using
that energy function y blockade is a
French American computer scientist
working primarily in the fields of
machine learning computer vision and
computational neuroscience laoco√∂n was
born in the suburbs of gay Paree in 1960
he received his PhD in computer science
in 1987 during which he proposed an
early form of the back propagation
learning algorithms for neural networks
he was a postdoctoral research associate
in Geoffrey Hinton's lab at the
University of Toronto from 1987 to 88
these days he's a professor at NYU and
vice president and the chief scientist
on Facebook he's the founding father of
a biologically inspired model of image
recognition convolutional neural
networks which led to the deep learning
revolution making the training of
extremely deep networks tractable by
truncating the receptive field he was
also the co recipient of the 2018
cheering award for his work in deep
learning together with Geoffrey Hinton
and Yoshi Avenger this gang of three are
referred to by some as the Godfather's
of AI or indeed the Godfather's of deep
learning the International Conference on
learning representations I CLR or I
clear is the number two international
academic conference and machine learning
my ribs and in front of ICML these
conferences are ranked in terms of high
impact machine learning and artificial
intelligence research laocoon presented
a thought-provoking keynote speech and
today we're going to dissect it
surgically but let me warn you we'll be
talking about energy based models which
you might not have heard of and we say
the word manifold a hundred and forty
five times even after heavy editing so
expect to learn more about energy based
models and manifolds in today's episode
than you ever wanted to know about so
energy function is basically just a
function that is happy when you input
something that looks like data and is
not happy when you input something that
doesn't look like data this can be
applied to almost anything you can think
of data can be correctly labelled images
data can be images that look like
natural images the applications are
endless and therefore also the topic is
endless
ultimately the what young akanda is here
is just reframing a bunch of things from
different machine learning areas into
the same framework so energy based
models are not something new is just a
different way of formulating already
existing machine learning things I
thought this talk contained a lot of
interesting ideas firstly I'm excited to
discuss the presented chart of concept
acquisition and infants
ever since studying the poet algorithm
I've been fascinated by curriculum
learning or stepping-stone tasks along
the way to complex behavior I'm excited
to see what Tim and Yannick think of the
perceived curriculum and infants such as
going from face tracking to object
permanence and shape constancy Yama Kuhn
presents three challenges that deep
learning must solve the first of which
is learning with fewer labeled samples
and or fewer trials the answer to which
may be self supervised learning we've
seen an obvious example of this in
papers like curl or sims CLR but another
recent paper reinforced learning with
data surpasses curl with more data
augmentation rather than multitask soap
supervised learning I think there are a
lot of branches of research at play with
learning with fewer labeled samples the
most popular area which comes in mind is
transfer learning but additional fields
like meta multitask curriculum or
continual learning will all have a part
to play in this the next two challenges
the koon presents are learning the
reason and learning to play in complex
action sequences this talk dives into
the technical discussion of energy
functions and how they construct the
data manifold young mokona is talking
about very specific types of application
here where basically you're learning at
to trace out a date to manifold by
pushing points that are data down and
pushing points that are not data up
thereby creating an energy landscape now
since this talk is pretty much about
everything that could ever be you will
see the three of us rather struggle with
grasping what young the car is and isn't
talking about so we're trying to make
sense of pretty much all of machine
learning in one go and I had some
trouble with this but also lots of fun
it's a bit of a different talk for us
because none of us are really experts at
it but we tried and this is how it
turned out and so the first thing you
need to know are energy functions or
energy based models what is an energy
function and energy function sometimes
called P is simply a function with one
or multiple inputs let's call them X and
it can make the if the energy function
is happy with X it will be the value 0
and if the energy function is not happy
with X it will be a high value like
larger than zero so this is happy this
is not happy so let's give some examples
of this we can formulate almost any
machine learning problem in terms of an
energy function
let's say we have a classifier the
classifier is takes as an input image
here maybe of a cat and the label so if
the label is capped then the energy will
be zero if the energy function is
working correctly and if but if we give
the energy function the same image but
we give it a wrong label dog then it is
very high
in the case of the classifier of course
we can simply take the loss function as
the energy function and we are all to
automatically get an energy based model
so the loss function here would be
something like the negative log
probability of the at the correct class
but in any case it is just going to be a
high number let's call it ten to the
nine so the energy function says huh
this is very bad the entire thing you
input it won't tell you yet what's bad
about it so that also means you can
change any of the two things to make the
classifier happy now usually we're
concerned with changing the label it's
like tell me which other label do I need
to input to make you happy and if we
make the labels differentiable of course
we never input the true label we
actually input like a distribution
softmax distribution over labels and
that's differentiable we can use
gradient descent to update the dog label
we can use gradient descent to find the
label that would make the energy
function more happy so we could use
gradient descent to get the cat level if
we had a good classifier but we can also
optimize the image to make it compatible
with the dog label that's things that if
you ever saw deep dream or something
like this those models do exactly that
they optimize the input image for a
particular label and there you can view
the entire neural network including the
loss function as the energy function so
what's another example another example
is let's say you have a k-means model
and the energy function is simply in
put a data point and for the data point
what you're going to do is you're going
to find the min cluster index the min k
/ you know you have your multiple
clusters here and your data point might
be here so you're going to find the
cluster that's closest and then the
distance here this distance D will be
the energy of that so the model is very
happy when your data point it comes from
one of the clusters but your model is
not happy when the data point is far
away and that would be the cost function
of the k-means function so that's an
energy based model to the currently
energy based models have come into
fashion through things like ganz or any
conservative noise contrastive
estimation so in a jet in a gam what you
have is you have a discriminator and the
discriminator will basically learn a
function to differentiate data from non
data so that by itself is an energy
function so the discriminator will learn
a function and that function will be low
wherever the discriminator thinks there
is a data right so it will usually do
this around the data points so the data
points form the valleys right here and
then the generator will basically take
that discriminator function and will try
to infer points that are also in these
valleys to produce points that are also
in the valleys and then you basically
have an energy learning competition the
discriminator now tries to push down on
the energy where the true data is and
push up on the energy where the
generated data is and that will give you
basically a steeper energy based
function in the future I hope so in this
case the discriminator neural network is
the energy function and the D generator
just tries to produce data that is
compatible with that energy function so
I hope that concept of what an energy
function is clear and any machine
learning problem can be formulated in
terms of an energy function
that last snippet with Yannick
explaining energy based models was taken
from his personal YouTube channel he's
just dropped a video about an hour ago
called concept learning with energy
based models and he covers the really
cool paper from open AI it was by I
Corps more - he was it was a really good
video so thoroughly recommend you you
guys go and check that out as well
welcome back to machine learning Street
talk with my two compadres Yannick
Kilcher and Connor shorten we had AI CLR
last week I cleared and it's one of the
top machine learning conferences and
what's interesting and this time around
is that it was completely open and on
the internet so you can freely go in and
watch any of the talks and look at the
papers and that there was a really
really good kind of keynote presentation
from Yan laoco√∂n and yoshio Benji so
what did you guys think about it when
you watched the yen's keynote and the
first time I watched is my head was like
oh too much to the process step away and
think about it but there's definitely a
lot of interesting ideas in it I think
yeah if you're not I think if you're not
familiar with what he's talking about it
just seems like a lot of information
until you can encounter distill it down
to what he's actually saying and then
that it becomes more of kind of a well
at first you think it's is presenting
something new but more and more you
realize he's basically just describing
what already is right in in a sort of
unified manner which I find pretty cool
I would agree with that when I first saw
the presentation I it was information
overload and what I've always liked
about Yan lagoon's presentations is he's
one of these guys that likes to simplify
things and make sweeping generalizations
and it's the kind of guy that gives me
deep intuitions about how deep learning
works so at first this was out of carats
of him but having looked into energy
based models I mean let's just cut to
the chase he he spends the entire
presentation talking about energy based
models and talking about lots of new
things in the in the terms of energy
based models and I'll be completely
honest I'll hold
hands up I've never heard of energy
based models before I feel embarrassed
to say this and when I googled it
Yan laocoon had a wonderful paper which
was a tutorial and energy based models
and he wrote it in about 2006 so you
know getting on for 15 years ago and
everything in that tutorial was
basically what he was saying here at
IKEA so nothing has changed really in
that time yeah like when I was studying
Gans I kind of hit a wall when I came to
the Wasserstein gaen because I you know
at the time not being you know
introduced to this idea of like the
smooth function that this kind of like
scalar scoring enables like I had no
idea why the Wasserstein again was you
know why that was different so I think
this really helped me understand that a
lot as well too interesting I mean
another thing that really came up is
there's always been a tug of war between
traditional probabilistic approaches and
the kind of deep learning approaches
there's a guy called Chris Bishop who
wrote the P R ml book and he was a huge
advocate of model-based machine learning
I once interviewed at Microsoft Research
and and they were pitching it hard this
was before the deep learning revolution
and the way they described it was they
always cite the no free lunch theorem
and they say that every single problem
needs its own machine learning algorithm
and of course these type of models were
almost created with the domain and in
mind and they had these characteristic
factor graphs where you had latent
variables and you can kind of model
dependencies between the variables and
you could do approximate inference in
this probabilistic space and Yanbu kuhn
draws a kind of corollary to that but he
says in his factor graphs it's a mixture
of deterministic and probabilistic
functions I I would guess part of that
developed into what today is the
causality community where they put
explicit weight on okay we know how the
data is generated we know how the world
works in some manner and we let that
basically determine our model and then
we learn the rest to it I think is a
very valid approach but it's not always
one that you can necessarily achieve
you've it's idealistic one of the big
things that sells you on in the abstract
of a tutorial on energy based learning
is as probabilistic models must be
properly normalized which sometimes
requires evaluating intractable
integrals over the space of all possible
variable configurations so it seems like
that sentence is contrasting saying
probabilistic models is one thing and
energy based models is the other thing
and the key is that you don't have to
normalize these energy based models so
do you have any like so what does this
mean - not normalize it an energy based
model is any model where at the end you
have a single number right at the energy
and if the energy is low that tells you
whatever you put in the model is happy
with and whatever you didn't put in the
model is not happy with and you can
interpret any probabilistic method as an
energy based method simply by having the
the inverse probability as your energy
right so if if you have a probabilistic
model then you know the exact
probability that a given point is
occurring let's say in the world so the
the difference here with the energy
based model you can only tell if
something is better or worse whereas
with a probabilistic model you know
exactly how good it is how often
something occurs in the entirety of the
of the world or of whatever your your
base manifold is right so maybe we can
we can make a difference between if you
take for example a language model and
you give it a sentence and then the
language model could tell you the
probability that this sentence will
occur in the English language is exactly
point zero zero zero zero zero three two
four whereas if this is an energy
function it could simply tell you I am
happier with this one than with this
other one I think there is a way though
of transforming an energy function into
a probability and this is and this is
one of the things that Yan talks about
how you compose probabilistic functions
and and deep learning functions and the
way he does it is by norm
lysing using this gibbs distribution so
it's a bit of an arbitrary distribution
but it means if you normalize it over
the integrals over the domain of of your
wires you can turn any energy function
into a probability distribution
yes exactly this this has been this has
been the the main focus of many models
especially NLP models over the last
years is to have these normalizations
and also previous methods like I don't
where ever these this graphic these
graphic models that were popular kind of
at the advent of deep learning
conditional random fields and so on the
if you have a probabilistic method you
need to be able to compute this
probability and the probability is
simply all the positive cases divided by
all the possible cases right so if you
have a language model then your current
sentence is your positive case and you
have to divide this by every sentence
that is ever possible right so you have
you have to basically ask your model
what do you think of that and that and
that and that you have to do this for
every single sentence in the English
language actually every single scent
that's even possible with any of the
words in the English language and then
if you divide by that so if you get a
probability you need this normalization
and this is for many models the main
problem how do you do this normalization
this is an intractable integral usually
and much much work has gone into just
approximating this how do we how can we
factorize so conditional random fields
or things like this factorize this
integral into just single variables or
two variable products so you can compute
this with some sort of forward-backward
graph message-passing algorithms or
other models like in NLP have normalized
it by simply sampling right so instead
of dividing by every single sentence in
the English language we just sample like
ten and we just say okay these ten is
your base distribution so yeah there's
there's a connection I would say
every probabilistic model is an energy
based model and every energy based model
can be turned into a probabilistic model
by normalizing future of machine
learning and AI is so supervised one
question I've been asking myself for
many years is how do humans and animals
learn in particular how do they learn so
quickly seemingly not requiring any
supervision or very little and almost no
interaction with the world this is a
tribe together by Yahoo that shows that
what age babies learn basic concepts
like object permanence stability and
intuitive physics inertia eye gravity
things like this this seemingly is being
learned almost with no interaction with
the world mostly by also ancient Yogi's
have very little ability to interact
directly with the world and the mystery
is how does that happen and how does it
happen animals as well this is probably
the vehicle through which baby animals
and humans learn massive amounts of
background information about the world
such as intuitive physics and things of
that type perhaps the accumulation of
this knowledge forms the basis of common
sense had to look up object permanence
so that means if a baby sees a building
it means the baby will know that the
building is still in existence when the
baby can no longer see it that is
correct
my peekaboo works that's why it's a fun
game with babies because they to them
it's like a miracle that you're still
there once the hands come off exactly so
what I would I what I thought when I
heard this was something like I'm not
sure here if the analogy is really a
good one
because basically what he's saying here
is that it is through observation that
the theity babies learn these things
where I would strongly argue that there
are millions of years of evolution where
not only humans but pretty much all
animals had to have an intuitive sense
of gravity had to have a spaciality had
to have object permanence
learned so I would argue that it might
much rather be a simply a module like in
the brain that is inmates that simply
get switched on at that particular phase
during development rather than it is
like babies come they the question is if
if you had a baby in zero gravity would
it develop an intuitive understanding of
gravity or not and I guess the corollary
to that is what you should rather
measure is things that could not have
evolved like Bowie may be interacting
with a computer or something like this
no what do you think yes that's so true
I mean it's very dangerous when we get
into this anthropomorphize ation of deep
learning and just these dangerous
comparisons with human development but I
think the reason this fallacy exists is
because when a baby is born the baby
appears to have no cognitive
capabilities or intelligence whatsoever
they are all learned during the during
the developmental period but as you say
there are so many inductive priors in
the brain all right I think what Yan's
trying to say here is that the baby is
not learning in a supervised way I think
the thrust of his message here is that
the future of deep learning will be self
supervised and it won't be reinforced do
you think there's anything to like the
curriculum of how these tasks are
learned like something we kicked off
talking about poet and that has this
automatic curriculum learning do you
think there is anything to these
sequence of things that are being
discovered another thing that I think is
interesting about the idea of this is
them learning quickly this is a
demonstration of quick learning is that
in two months you can actually get a lot
of visual data in two months if you
assume that you get like 30 frames per
second and there are 60 seconds a minute
1,800 and even you're looking at video
data which could be variable lengths I
think you actually could get a lot of
visual data in two months
yeah I think that's that's pretty much
his point here is the fact that you do
get all of this data but you do not get
the labels right you don't get any
labels and also
I think the why he takes babies as
examples because they don't as he says
they don't interact so it's also not
reinforcement learning it's not
supervised and it's not reinforcement
it's just something about consuming
large amounts of unlabeled data that
makes them learn things and let's I mean
I'm willing to go with the analogy here
even though I disagree on biology almost
with no interaction with the world
mostly by observation to interact with
the world and the mystery is how does
that happen and how does it happen
animals as well this is probably the
vehicle through which baby animals and
humans learn massive amounts of
background information about the world
such as intuitive physics and things of
that type
perhaps the accumulation of this
knowledge forms the basis of common
sense so being able to reproduce this
type of running in machines would be
enormous
powerful would reduce the requirement
for liberal samples and trials and in my
opinion the next revolution in AI will
not be supervised nor reinforced so
there are really challenges I would
argue that we've already had a
revolution in AI which is this self
supervised approach it has transformed
language processing over the last few
years yeah definitely especially like
recently it seems like these contrasted
learning methods are just taken off as
well absolutely
oh there yan makes some interesting
comments later that he doesn't think
they work as well for vision but today
one is of course diminishing the
requirement for level samples and
reinforcement interactions and it might
be and that goes through so supervised
running as I just mentioned some
supervised running really is running
dependencies between variables running
to fill in the blanks you need to
represent the world or need to predict
the second one is running to reason
going beyond system one Daniel Kahneman
system one which is not going through
kind of a fixed number of steps in a fee
for a neural net but being able to show
reason perhaps by finding a
configuration of variables that satisfy
certain constraints for me my some sort
of energy
maximize some likelihood and the third
one is running to plan complex action
sequences and I don't have much to say
about this unfortunately did you notice
he dropped the energy word in there so
that was the the first hint that we've
gotten this is going to be a talk about
energy based models to me it looks more
like transfer learning from another
thing yeah I think I got a self
supervisor I think it's both because
self supervised learning doesn't help
with the sample efficiency problem it
just means that someone else does it for
you I think there's so many different
areas of research that you can then pull
apart and be its transfer learning oh
and then you have all these tasks so now
it's multi task learning but don't
forget any of those tasks now it's
continual learning and so you need a way
of scheduling these tasks now it's
curriculum learning especially if there
are all these different areas that can
relate to this idea how exactly we're
gonna use the self supervised learning
task to learn quicker and then meta
learning also there's all these little
things you can say that seem like their
own subset of deep learning I don't
understand this idea of learning the
reason artificial intelligence systems
in general they are known as system one
you know Daniel Kahneman in his book
Thinking Fast and Slow he talked about
system 1 and system 2 and system 1 was
the very kind of autonomous perception
type task assistant 2 would be the
really deep thinking tasks that I
couldn't do without consciously thinking
about it it's probably more in the
reference to what a human does and
doesn't do I would describe the system 1
tasks as anything you would do with deep
learning and the system 2 tasks is
anything where you would actually write
a computer program to do and this is
this is better explained in yoshua
bengio stalk in the same session where
he says basically system 1 tasks are
intuitive it's it's kind of subconscious
and not not really conscious experience
system 2 tasks are ones that you can
formulate with language so you can
reason it with language what you are
doing so system 2 things would be where
you would have to apply logic in order
to solve a task
whereas system-1 tasks would be where
you can just learn to map input to
output it's quite difficult to wrestle
with because in a sense a deep learning
models do reason they are analogous to
computer programs I think it's very much
a human concept the system 1 and system
2 because I think both car naman and
Benja they also talk about how a system
two tasks can become a system one task
if you simply repeated and it kind of
becomes automated right you learn to
drive a new road and you do it many
times and then it just kind of becomes
into your motor memory I think it this
is really just a human concept and kind
of a description of look of what he
thinks these systems should be able to
do in the future well learning to use
logic can't be made into a
differentiable loss function it can but
no one has done it very successfully so
far people say it's AI if we don't
understand it and maybe it's a bit like
that it's system - if we can't do it yet
in the hierarchical planning world you
would say something like I need to get
to the supermarket to buy food and you
would decompose that into I need to get
to the car I need to drive to the
supermarket and I need to get out and
then you would decompose each of those
again you know I need to get to the car
which means I need to grab my keys walk
to the car open the door sit in and so
in the planning world that means the
kind of hierarchical decomposition the
fact that what your system to does is it
builds these big plans and then it
breaks them down until the level where
the system one can take over like walk
to the car yeah I know how to do that
and the third one is running to plan a
complex action sequence and I don't have
much to say about this unfortunately so
what is a supervised running so
supervised running is running to fill
the fill in the blanks let's take an
example of a video you the mission she
pretends not to know a piece of the do
and train itself to predict the piece
that it pretends not to know from the
piece that it knows so for example
creating the future from the past
predict predicting the top from bottom
frame things like that or missing words
in the
next this is of course become hugely
popular in a nutshell that is self
supervised learning just being able to
pretend you don't know things and and
predict either that thing or something
in the vicinity or the top from the
bottom whether he leaves himself quite a
bit of wiggle room to formulate pretty
much everything into that because now I
can say okay supervised labeling task is
simply I don't know part of the input
which is the label k-means clustering
problem is simply I don't know the
cluster assignment so I think the
definition here is broad and is
intentionally broad such that you can
even formulate it later as energy based
methods because it's all-encompassing
right missing frames things like that or
missing words in the text so the
prediction must be multimodal there is
no single prediction that will be
consistent with the initial segment of
the video multiple feature of the video
are possible so we cannot use just a
neural net that is basically a
deterministic function symbolized by
this sort of rounded shape blue rock
here G of X which makes a single point
prediction we have to replace this by
something that can make multiple
prediction and one way to do this is to
go through some implicit function that
basically measures the compatibility
between the variable we observe X and
the variable we need to predict Y now
we're getting into the meat of it he's
introducing energy based models and
straightaway he's telling us about this
new type of factor graph which is a
combination of the old school
probabilistic factor graph but now with
deterministic functions as well and he's
introducing this idea that we need to
have multimodal predictions so we need
to have functions that can give us many
many predictions subject to an energy
function and this energy function of
signals and labels needs to be optimized
and smooth such that as such that's the
correct label on you know on or near the
manifold has a low energy and any Y's
that are away from the manifold should
have a high energy you
definitely like it like thinking about
Gans and I think that's a big thing in
like the style again to model is how
they put that stew that random noise it
just gets injected in the intermediate
features and that helps it do you know
like it's not like a deterministic
generator where for every sampling of
the Z it produces the exact same face
and you can do that by adding this
sample latent vector Z into the forward
pass so this function f of X Y will take
low values is if x and y are compatible
with each other and higher value if Y is
incompatible with X it's not a good
continuation for the video for example
the symbolism I'm using here is very
similar to factor graphs in graphical
models except for this extra symbol of
deterministic function now I'm going to
advocate to use energy based models
which you know basically measure the
compatibility between x and y through
this energy function again that takes
real value effects on Y are comparable
and higher value if if they're not
inference is performed by forgiven x
finding wise that minimizes energy it
could be multiple wise and this is aware
of handling uncertainty without
resorting to probabilities so without
resorting to probabilities yeah so they
did again that the difference is here
you simply want a function that tells
you when when it is happy with some
input and and that indicates by a lower
number then when it is not happy that
would it be indicated by a higher number
and then Y basically says inference now
if if I'm given an energy function right
then I can so if I have let's say a
video and I want to predict the next
frame and I am given an energy function
what I can do is simply I can find the
frame that minimizes that energy
function given my input so that's that's
that's kind of a reformulation of what
he's doing though not all the models
that he is gonna talk about fit that
particular criteria yeah at first I
thought that it was silly suggesting
that they were possible multiple
possible video frames that come next but
he might be talking about a computer
game or something where
if you interact with it differently then
the next framework will be different but
this is a very general framework as you
say so he's saying look at this energy
surface and just minimize so across all
of those points find the one which is
the smallest and representative next
frame but even even if even for the
video I I get what you mean you mean
that in the video that was recorded
there is only one next frame but if if
you simply cut away the last part and
just look at the in sequence you don't
know whether the camera person is going
to to gear left or right so there are
multiple in the in the true world there
are multiple continuations that are
possible and your energy function is
supposed to capture that of course
you're going to terrain it by giving it
the samples but you hope that it is
going to generalize into telling you
there are all of these continuations
that are possible in the real world and
all of these other continuations that
are just gibberish are not good so once
why becomes like this high cardinality
you know it can take on tons of
different wires to pair with our X how
do we Sam like how are we going to
search for the Y's with our energy
function yeah I think that that's where
the Z comes into play well and that I
think that is the if so he's got he's
going on to rephrase pretty much
everything in machine learning in terms
of an energy function and I think the
question of how do we do this exact
thing is that that's the that's what
every model has ever attempted to do
right so and he's going to talk about
specific ones but ultimately that is the
the problem of the method itself if Y is
so high cardinality how are you going to
do this it could be by training a
generator right to to simply predict a Y
that has a low energy it could be by
really optimizing with gradient descent
to find the Y that has the lowest
possible it could be Byam
passing method it could be you know via
many things smooth in Whiteface can be
done through gradient based optimization
algorithms or some other inference
methods of course is why is discrete is
much easier and we know how to do that I
mean is as if Y is if Y is continuous we
can find a good way through gradient
based optimization method that's what we
said right if we have an energy function
we can just minimize it using gradient
descent and then he says if y is
discrete of course that's much easier
and I am Not sure because usually
discrete optimization problems yeah so
on the one hand if you think of a
supervised classification problem then
you can phrase it like this and then
it's super easy you just try every one
of the classes and whichever one has the
lowest energy that means whichever one
has to lie highest likelihood that
you're gonna output as the label but in
exactly something like a language model
it is extremely hard because you're
gonna have to try every single sentence
that's possible and take find the one
with the lowest energy function so I'm
kind of confused by what he said if Y is
discrete does that imply that F the
energy function f of X Y is not smooth
is difficult quest these are like
first-year math questions I'm not so
sure it doesn't imply that it can't it
can't be it cannot well it depends on
how it is defined if it is defined on
the continuous space but simply the set
Y happens to be discrete then it can be
smooth but if it is defined on the
discrete set then I'm pretty sure
smoothness makes no sense that's what I
think - if it's a discrete it is jumping
from point to point with no connection
law mmm but the function itself might be
learning some kind of interpolation but
it might not be smooth it inference
methods of course is way as discrete as
much easier and we know how to deal with
that
there are conditional and unconditional
versions of energy based models in
conditional version the variable X is
the one that's always known and Y is the
one that so it needs to be predicted the
unconditional version the the trick here
is to train the machine to predict part
of Y from part from other parts of Y but
we never know which one is known which
one is unknown so this is sort of
capturing the mutual dependencies
between the variables as symbolized by
the drawing gear on the on the Left on
the bottom-left that that represents
energy function in this case here
learned with k-means where the training
samples are drawn on this little purple
curve I think in almost all machine
learning use cases it is a conditional
ebn because we're learning a dependency
between X's and Y's yeah I mean he goes
and immediately makes the connection to
k-means right and that's again we were
saying before this is going to
ultimately encompass pretty much every
all of machine learning and here we see
how it encompasses k-means so in k-means
i I simply want to create a function
that is happy with any point in the data
distribution by doing this these means
but ultimately this is just learning the
distribution of the data but because I'm
not normalizing in k-means it is an
energy based method and not a
probabilistic method so here you can see
the difference between energy based and
probabilistic for a probabilistic method
this might be a Gaussian mixture model
but the Gaussian mixture models are
usually much harder to globally
normalize even though it's Gaussian so
it's still pretty easy cool so one way
to handle multiple outputs is to it
through the use of a latent variable so
if we're going to build our machine out
of deterministic functions the the way
to allowing machine to produce multiple
outputs for a single input is to
parameterize the set of outputs through
a little variable so the typical
architecture would look something like
this you have an X variable it goes
through a predictor that extracts a
representation of that X variable and
that representation together with
don't worry we'll go through a decoder
which produces the prediction when you
vary the latent variable over a set it
makes the prediction vary over a set of
similar dimension and and the trick of
course is to find build a machine and
train it in such a way that the latent
variable represent independent
explanatory factors of variation of the
output so he's saying how can we take a
deterministic model this is kind of like
a variational auto-encoders because
imagine you have some data manifold and
you want to design an architecture such
that the latent variable can describe
the domain of the manifold yeah this is
this is almost exactly a drawing of a
variational auto encoder now and yeah
it's it's basically you you have this
latent variable that controls how the
output is made so what you're producing
is an entire manifold which in some
sense again is a very similar thing
where you're learning just this mapping
from the latent variable to the manifold
so that would be sort of not conditional
on the X so that might be a model before
where it's just an F of Y interesting
and saying the information capacity of
the latent variable needs to be
minimized otherwise all that the
information would go into that so the
encoder takes the data and puts it into
this vector and then so how is the
random variable added to that before it
hits the decoder so the random variable
is sampled from the distribution that is
described by the latent variable now
usually you can't back propagate through
this right you can't back propagate
through the operation of parameterizing
a distribution and sampling from it but
with certain distributions sampling from
it is the same as and this is where
exactly this comes in so what you
technically do is you sample from a uni
like a one zero mean standard deviation
of one Gaussian and then you multiply
the standard deviation that comes in
from the encoder and you add the mean
that comes in from the encoder and that
operation you can back propagate through
so that is how you combine the latent
variable which is the sample from the
Gaussian distribution the standard
Gaussian with what the encode who gives
you H mm is called the Reaper ammeter
ization trick it was it was very big
when I started my PhD now many energy
based models are actually built using
latent variables and you can reduce a
latent variable initially it's not all
to one that doesn't have one by either
marginalizing or minimizing with respect
to the latent variable so inference of
course takes place by minimizing the
elementary energy function with respect
to both y&amp;z the variable to be predicted
on the latent variable you can simply
redefine the energy function f by
minimizing the elementary energy
function evil Specter Z or by
marginalizing which is equivalent to
computing some sort of free energy as
indicated here the logarithm of the
integral of exponential minus the energy
where the integral takes place over the
domain of Z when you do you have a
latent variable a BM if you want to
formulate it just in terms of the X and
the y without the latent variable you
can either minimize with respect to the
latent variable or you can marginalize
and marginalize is where you kind of sum
over the you know we were talking about
the gibbs distribution earlier you know
over the domain of Z so you kind of
summing over all of the possible
combinations with the Zed and then
normalizing yeah so so here if a very
simple example of this that okay doesn't
fit the X&amp;Y but if we just had an F of Y
would be the again the distinction
between k-means and a Gaussian mixture
model both are clustering models but one
is just the the the hard assignment so
the top one would be an example for
k-means where your latent variable is
the cluster that the data point comes
from so when you have a new data point
and you
ask your energy function your k-means
function how happy are you with this
data point what it will do is it will
find the closest cluster center and then
the distance to that one cluster center
is your is the energy that's how happy
the model is with this particular data
point so it can only tell you that but
if you have a Gaussian mixture model and
you have a new data point you need to go
through every single component of that
mixture and ask them what probability
density do you assign to that data point
and then you need to integrate across
all of them in order to get an answer of
what the whole model thinks of your data
point exactly so just just to carry on
from that so this F that we see here
this is now showing us the the entire
manifold so it's kind of showing us how
this is represented over all other
points of the latent variable so what's
the F sub infinity and F sub beta
represent the cost function of in my
example the cost function of k-means on
top and the cost function of a like a
Gaussian mixture model on the bottom or
the energy function in this case it's
integrating across them right it's it's
it's going through each variable of Z
and it it takes the energy of that
particular Z and it tries to weigh it by
by that energy so it is more like an
integration across all of the different
sets it's like maybe you can interpret
it as in in a game in a in a poker game
but you want to maybe have a flush draw
and you ask yourself should I call this
should I call two in order to continue
what you want to do is you want to think
of all the possible futures which are
all the possible latent variables so the
latent variable is which card comes next
so you want to integrate across all of
that and for each of these you want to
ask yourself how happy am i with that
particular outcome and and and that
would be your energy and appropriate
energy function for that whereas if you
played the game of chess then you don't
need to
come up with any particular move you
just want to know what's the best move
my opponents place and how is my move
compared to that so right now you're
gonna find the minimum Z which is the
best move your opponent can play in
response and you want to find your best
move according to that information
capacity of this latent variable is the
Levi's regular rise and this is a main
issue that I will discuss later but this
may turn out to be impractical in
tractable or only approximated through
very strong men is an example of latent
variable let's say or data manifold is
an ellipse we when we find a data point
we need to compute its energy by finding
the point on the manifold that is
closest to it so that we measure the
distance to the manifold and the latent
variable would be the angle that leads
to the point the closest point on that
manifold now in this simple case of
course you can write it it's basically
but in more complex cases of course we
need to find this manifold and the
parent realization is not real I thought
this is really instructive actually now
I like me analogy that there is a data
manifold because a lot of people
probably don't even think of their
pictures of dogs and so on is fitting on
some kind of high dimensional manifold
but of course they do and even though
it's a contrived example it's a showing
that your latent variable Z is actually
an angle in this ellipse case so the
ellipse is the manifold and your latent
variable is just something that you can
use to push your data point on to any
position on this manifold and if a new
example comes along its energy is simply
a function of how close it is to the
manifold so if it's if it sits on the
manifold the energy is zero and if the
if the new example sits away from the
manifold then the energy will be higher
yeah so the introduction of this latent
variable is simply to be able to have
more than one point that where the
energy is zero and then each of these
points will have a different latent
variable assigned and I I do like the
line the most it says energy is the
distance to the ellipse right so your
energy function is how far away is your
point to the ellipse in this case so the
latent so really though it's not like
you know
is it's like this like squiggly line you
know not something that's like the angle
you could just rotate it and keep
hitting points right wouldn't it be like
a like you know like a squiggly shaped
circle well that it now it now it
depends right your your true energy
function is the ellipse in this case now
if you just have a sample of data points
and you want to learn an energy function
what you're going to do is probably
approximate that through some
interpolation and then you have a
learned energy function and then yes
that would be like you do whatever your
squiggly Circle it's true but your I
guess what he what he wants to say your
true energy function here would be the
the ellipse itself as a concept as a
manifold and now I think later he's
gonna talk about how do we learn energy
functions and there's the contrastive
methods and then there are the methods
where you regularize things and in this
case if i already know it's an ellipse
I could regularize my model to simply
say you're only allowed to produce
ellipses and then if I'm given this set
of data points it would not turn out to
be a squiggly circle it would actually
come to be pretty close to this ellipse
for me this is kind of like I've been
seeing this term data manifold like
throughout my entire study of deploring
I feel like this picture is finally
helping me understand what the heck data
man holder's
it's like this path in this high
dimensional space that is connecting the
data to each other
yeah manifold is just a fancy way of
saying subspace but usually subspace is
associated with it you know being linear
or something like this but manifold can
be whatever you want it can even just if
you speak well here's data and here's
data and data is everywhere here or can
be anywhere here it's a beautiful idea
because manifolds come up all over the
place if you for example do an l2
normalization on your vectors then they
all exist on a manifold called the unit
hyper sphere because if you think about
it
all the possible vectors that have a
length of one and there are many
examples in machine learning where it's
a fixed manifold if you if you look at
Geo data for example and the manifold is
a sphere and CNN's work on the planar
manifold and actually if you think about
it in some higher dimensional space
there exists a manifold that almost all
data sits on and many of the types of
analysis and machine learning even like
Disney and you mavar think about data in
terms of the manifold that it sits on
yeah I guess so it just seems to me like
in the high dimensional sense it's so
complicated that it what's the point in
thinking of it like that like if the
unit hyper sphere of say like L to
normalize parameter vectors that have
what this massive dimensionality I just
don't get what the point is that
thinking about it like this well it's a
very close connection to energy
functions actually in that usually you
assume your manifold is somewhat
continuous and smooth and the energy
function is one-to-one basically your
distance to that manifold so it's it's
it is incredibly general for the same
reason that the energy function is
incredibly general it simply says the
energy function is happy when the data
is good and not when the data isn't I'm
really interested to come up with
examples of how this works on the kind
of models that we get excited about
because this is a contrived manifold
when we talk about something like
natural language processing and bert√©
even language fits on some higher
dimension manifold and I think it's
quite instructive to think of examples
that do and don't sit on that manifold
and energies being pushed up around
those examples as a way of learning yeah
I mean it comes down to the fact that we
probably will never be able to describe
the manifold as manifold but what we can
do is we can build these energy
functions that tell you basically how
far you're away from the manifold so if
you can build an energy function like
this then at least you can hit the
manifold with a reasonable probability
by simply making the energy function
happy cool
I what would be the angle that leads to
the point the closest point on that
manifold now in this simple case of
course you can write it it's basically
but in more complex cases of course we
need to find this manifold and the
parent realization is non-trivial
okay so how do we train energy base
models what we need to do is make sure
the energy for data samples is lower
than the energy outside of the data
manifold and these two types of methods
for this contrastive methods that
explicitly push down on the data points
and push up on other points outside the
data manifold or may be honest but less
strongly and then there is regularized
and architectural methods that
essentially limit the volume of space in
white space that can take low energy and
therefore kind of shrink-wrap the data
manifold automatically without having to
push up there's some really interesting
things here so when we train this energy
function f we want it to be a lower
energy for the given Y than all of the
otherwise in the training set so that
seems to make sense we want the function
to be smooth so that we can use gradient
based methods and then he talks about
two classes of learning methods and that
this is erects really interesting he
talks about contrastive methods and so
called regularized and architectural
methods by which he means things like
PCA and k-means and and so on and
there's a real mission here I think he
doesn't talk about traditional kind of
supervised classification neural
networks yes so first it is actually
even more more general than that you
don't just want to make your point have
a lower energy than every other point in
your data set but then any other point
right it is just the difference between
the different methods and that was gonna
talk about as well is how they come up
with this contrastive measure so if you
think of again the points that are
pushed down are the points in the true
data set and the points that are pushed
up is everything the generator can come
up with to try to fool the discriminator
right so it's not even points in the
data set per se and the other thing is
you
can actually think of the traditional
supervised learning in this way if you
so if X is your input and Y is your
label what are you doing you're pushing
up the label that is correct then you're
pushing down all the other label the log
it's let's say because you run this
through a softmax
classifier so immediately by pushing one
label up you push the others down and
there you have your energy simply the
the negative of that is your energy
function for a supervised learning yeah
so it's really interesting that we're
using a kind of intellectual scaffold to
talk about what's happening in a lot of
machine learning algorithms and as you
say the approach you just spoke about
energy is being pushed up and being
pushed down but yang goes on to say that
in this architectural method it's a
slightly different paradigm that's when
rather than pushing things up and down
you kind of shrink wrap the functional
space around the manifold so you don't
need to push down before is lower than
the energy outside of the dilemma you
fold and these two types of methods for
this contrastive methods that explicitly
push down on the data points and push up
on other point I'm not going to read
through all of this but the big list of
classical methods that can be
interpreted in this context either as
contrastive methods or architectural
method maximum likelihood insist in
distributions are not easily normalized
is actually part of contrastive methods
and I'm gonna talk about first so there
is an issue with previous methods which
you can offer this is really interesting
because we've been talking a lot about
contrasted method recently we had the
contrast of unsupervised representations
for reinforcement learning chap on
Srinivas the other week and essentially
that was talking about things like moko
and sim CLR and what those approaches do
is they take these contrast of examples
and they kind of pushed down on the
positive positive pair and they push up
on the positive negative pair and by
doing so in the functional landscape
you're kind of learning where the manner
fold of all of the images are so why do
we want to limit the information
capacity of that latency vector so much
well I think that doesn't necessarily
come into this I think the Z is when you
want to be able to generate new examples
across the manifold but I think in in
the case of these Siamese networks or
the the sincere lar that type algorithms
then that there's no need for a latent
variable well I think the thing here is
when you you're energy function should
not let's say depend explicitly on the
latent variable that means so if you're
in your if you're in a poker game
your move should be independent of what
the next card is really truly going to
be and if you're in a chess game you're
the energy function is just determined
by what move you're doing because the Z
is just going to be whatever is the
minimum for the other player I think
it's just an expression to say that this
it the information shouldn't basically
leak into your into this cost
interesting it talks about birds as well
which is I think it's a huge interest to
so many people at the moment and that is
mapping points off the manifold to
points which are on the manifold and the
points that are off the manifold all
right just noise versions of ones that
we know are on the manifold so slightly
pushing them off the manifold and it
does raise questions about how far
should they be pushed off the manifold
and if you push them too far does that
at some point become problematic if the
noise strength is too high it's too high
after manifold that kind of idea would
it because he gave the example of them
these mixture model was actually being
really bad because they want to have a
manifold which is in critical and
infinitely high in energy almost as soon
as you get off the manifold so I think
that the general discussion here is that
we need to have models that generalize
better to previously unseen data and
have smooth energy functions that
describe the manifold in a way that
represents any
type of data that we could you know
reasonably expect to see in our data
distribution but just as we all know
because of the the perils of deep
learning are that the model is
interpolate they don't extrapolate if
you make it depression in the manifold
what you want is lots and lots of
depressions in the manifold to form the
canyon around your around where you want
your manifold to be that if the
depressions are small and they don't
form a nice Canyon around your manifold
if the problem space is too sparse then
you're not really learning anything
about your manifold well exactly I think
that is that is the exact problem with
any of these things so in in in in
supervised learning on the one hand you
have the exact knowledge which things
are there to push up and down it's just
whatever your 10 classes or your
thousand classes an image net but in
let's say fully unsupervised learning
you have to consider every single
possible data point there is if you just
want to learn a language model as we
said at the beginning you have to
consider every possible sequence of
tokens there is and our models are just
don't have the capacity to learn that so
what we're trying to do is we're just
trying to tell them look here is
something that's slightly wrong please
make it correct so it can what it can do
is it can form that valley between the
what's truly in the language and these
corrupted inputs and we're just kind of
hoping that that's enough right we don't
consider anything outside of that we
don't consider any gibberish or anything
yes as you said how far is too far and
how far is too close so if we now start
just replacing single words that could
even be a sentence that I said that is
actually correct and you know so let's
push the analogy just a little bit
further so we've learned this language
manifold and with a bird model that
takes in let's say 500 tokens it's less
than 500 thick is generally there's a
separator token but every single point
on that manifold
is a piece of valid language so I think
it comes back to this idea of stepping
stones and curriculum learning in that
we have these different manifolds or
like subsets of this high dimensional
space that we're like giving it and
we're like here try to understand this
subset of this massive manifold or like
entire Swift space so you try to find a
path that connects the manifolds that
you're seeing to each other so maybe
that's why like you do one pre-training
task and then the next one and the next
one is it's easier to connect these
manifolds in this like massive space
yeah I agree
but because what you're what you're
basically doing is with each
pre-training task you're finding a
different way of defining points off and
on the manifold right so if if you have
the pre-training task of masking some of
the tokens the points that you find off
the manifold or ones that are just
missing some words but then the other
task is swapping some tokens so that
just gives you a different way of
finding points of the manifold because
what's not gonna work is as negative
samples have it just imagine if your
birth input was just you just sample 500
random tokens and then you give that and
then you say how well can you
reconstruct this training example right
here from this that that is that is too
far off the manifold how are you gonna
do that and also did this is an
interesting point because now you have
your training samples so technically
what you should do is you should give
the input and say how well can you
reconstruct any of the samples in my
dataset any but since we took one and we
masked out words from this one we can be
pretty sure that the closest point on
the manifold is actually that particular
sample but that's that's no longer the
case if you go further off the manifold
you you don't know anymore which of your
training so you're gonna have a problem
with training this thing because you
don't know what to train it towards you
would have to train it towards every
single thing in your data set I'm also a
bit confused with Bert because it has
two objectives and
the first one I recognized as being the
mast or the denoising auto-encoder
because it's saying here is an input and
I'm going to mask out some random words
and then I want you to reconstruct it
that's one thing and then the next thing
is the next sentence prediction which is
completely different that's not an
autoencoder
so you're switching between those two
tasks dynamically and wouldn't those two
tasks build a completely different
manifold internally maybe it the next
sentence prediction task grabs all these
intermediate tokens pushes them all up
together in a way I'm not sure but maybe
it's about forming because you're like I
mean I'm just thinking about it in like
a three dimensional space like there's
this cube and they're subs as the data
in the cube if I get to grab a whole
bunch of points and move them together
maybe that helps then just moving
individually it that makes analogy is
pretty good because so again what as an
input okay our input space is now two
consecutive sentences right and we've
already discussed that the masked
language model will simply tell you
which which of the two so you can
reconstruct these double sentences as
one but now you you have a different way
of making points off the manifold and
that is to construct a non consecutive
double sentence so it's it's just
another way of constructing points that
are off the manifold so you're telling
you're learning an energy function that
says you should be very happy if two
sentences follow each other and you
should be not happy if two sentences
don't follow each other so you're
learning an energy function for those
particular points off of the manifold
push those up and push the true double
sentences down and you hope you hope
that your model will learn something
meaningful about language but don't
don't you think that there is a
divergence in the two tasks or do you
think some way yeah yeah that's it
that's the
right the point is with every additional
task you introduce the point is to find
more ways to find points off of the
manifold but in which case do you think
that one task makes the other one work
better or do you think that they help
each other work well yeah they do
feature sharing that's the ultimate goal
is that that that features are shared
that there is something about language
features that will help both tasks and
by doing gradient descent on both tasks
equally these features might develop
better and then the same features will
transfer to other tasks that you then
find you know which is what I'm going to
talk about first so there is an issue
with probabilistic methods which you can
off of course almost always turn or
energy function into a probability
distribution using a gibbs distribution
you can do maximum likelihood but you
basically have to do maximum likelihood
if you won't estimate certain sentence
the problem is that estimating densities
is not necessarily a good idea because
by doing maximum likelihood what the
system wants to do is give the lowest
possible energy to data points and the
highest possible energy to point to
points just outside of the data manifold
which leads the system to create to
creating extremely deep narrow canyons
and those are not particularly useful
for inference we need smooth functions
so those functions we need to be reg
arised
for example by a prior or another
regular riser so my first question is
even though I see the canyon but it
looks smooth to me
I think he wants to demonstrate with
these super steep walls but I get what
he's saying but it seems to be just you
know a property of that of that
temperature parameter how smooth and how
steep these things are really gonna turn
out to be I think the the main problem
with these things is that again yeah you
have to globally normalize which means
that every single point in your input
space must be assigned some nonzero
probability and the good luck he says on
the last bullet point but then why use a
problem
listing models so I think clearly Yan
Laocoon is not in the probabilistic
models camp yeah probably not
yeah is that just because you don't want
to have to sum over all those all the
terms on the bottom so if yours if you
have this probability distribution and
you're you know trying to assign a much
probability to one and the others in
that kind of way it causes this extra
like deep valley I guess yeah I guess
what usually happens is it just it will
just when your data point is here it
will just try to assign as much
probability mass to that point and that
will automatically take away mass from
other places including things around
that data point so what you if you don't
regularize properly you're just gonna
end up with like it a Dirac distribution
everywhere on your data points and
nothing in between
but again it's I think it depends on
this temperature thing so but then we
lose the advantage of actually so why
not throw away the policy framework
altogether and just learn dependencies
with energy function
so throwing away the probabilistic
remark sort of allows us to use more
freedom in so deciding on what objective
function to use the characteristic of
the objective function is that it must
be an increasing function of the energy
of data points or the decreasing
function of the energy points outside
the data manifold and perhaps through
some sort of margin that depends on
those two points so he's saying let's
throw away the probabilistic framework
and now he's introducing the kind of
loss functions that we see across the
machine learning world so things like
hinge loss and presumably things like
squared loss would be in there and he
what he's saying is quite intuitive that
it should be that if the example falls
away from the data manifold then it
should have a higher energy yeah through
a GUI dislike backtrack and just like be
very clear about this is in comparison
to probabilistic methods what would
where exactly is the difference is it
just this idea of not summing over all
of the possible why's is that the key
distinction yeah the fact is here you
lose the ability to to
make numerical prediction about how
likely a data point is you simply you
simply compare it to others that's all
you can do now does this get to the to
the nub of frequentists versus Bayesian
it is it is it about the the Bayesian
approach has this notion of seeing
things in the context of all of the
possible things that could occur
therefore you can reason about
confidence and and its likelihood etc
I'm not sure I think a frequentist would
still normalize their distributions but
it could maybe be compared to that in
that a probabilistic model will always
put it in relation to the global space
of inputs whereas energy functions just
give you the number for all those
energies I'm not going to go through the
details but they've been used in various
context over the over the years either
for things like Siamese networked
symmetric training or for ranking or
embedding and then more recently there's
been a objective function that used not
just a pair of points but a whole set so
obviously there is very successful
applications of supervised learning
today in particularly in the context of
natural language processing everybody
knows about a bird this was preceded by
the code of a Western set of techniques
which used a form of denoising
auto-encoder will you take an input you
corrupted and then you train the system
to distinguish between the clean version
and the carotids version in you know 0-2
encoder you train the system to map
corrupted version to clean versions
therefore now the reconstruction error
for corrupted points is the distance
between the corrupted point and its
clean version and so you have
automatically an energy surface that
grows with the distance to the manifold
as represented here on the bottom right
this represents the vector field of the
basically the gradient field of energy
function put used by denoising
auto-encoder
so this is really interesting I think as
we were saying before we haven't really
visualized
something like birds of course this
isn't the manifold for bird this this is
just a simple manifold but it does show
exactly what's happening here with this
denoising
we're just finding examples that are off
the manifold and comparing to ones that
are on the manifold it would be so funny
if it actually is the manifold of birth
like if some mathematician in the future
proves that human language is a spiral
ages would be like yonder car would be
treated as a God for predicting I'm just
just imagining things here yeah yeah
ultimately it's exactly that right so
you want to find some method of throwing
points off the manifolds and then
mapping them back on automatically gives
you this energy function and the more
ways you find of knocking points off the
manifolds the better your energy
function is going to be and it might be
just be worth talking about this diagram
again so he's using the visual concept
from the beginning of the presentation
which is the self supervision so the the
X is kind of disjointed in this space
and it goes into a deterministic
predictor decoder what's this see is
this is in a red square that's the loss
in the in this case it's there so it
takes the Y hat which is store tilde I
can see it from you it is it takes the
why that bird predicts right bird says
here is what I think the sentence was
before you took out all the tokens and
it compares it to the actual sentence
before you took out all the tokens and
then it just applies in Birds case it
applies a classification loss on each
token I see so the decoder has predicted
something on the manifold then it will
push the energy down well if the decoder
has predicted that exact thing on the
manifold then the loss will be zero
right and you're learning in this case
you're learning your predictor and
decoder function to make the energy low
yeah so you can see the two inputs so it
says this is a digit of text extract it
did you do so this is this has been
corrupted and and then we want the
predictor
the decoder to predict the non corrupted
version and if the comparator finds them
as being the same it will push down the
energy on the internal representation at
that point in the manifold yes so it's
like movie injectors prior of like
here's how you can throw points off the
manifold and then I guess like what's
really bothering me with thinking about
this is how it is mapped into this
manifold because your training the
features as you're simultaneously like
talking about pushing it off this
manifold so it's learning this manifold
as if being told what's on or off it
transform so much throughout the
training right there are multiple
manifolds going on here right I think in
this case in the bird case it we're just
thinking about the manifold of natural
language of in specifically double
sentences but just the manifold of
natural language and we're throwing it
off we're throwing something off the
manifold by simply corrupting it and
then we're learning this decoder
function to map it back onto the
manifold and the the the energy function
measures the distance to the manifold
right and in this case we don't have to
learn this energy function because
that's a given the energy function is
the loss between the Y and the y hat but
we're now learning this decoder and
predictor model in order to minimize
that energy function so it's as I said
at the beginning it's not always that
you learn the energy function per se do
you learn that distance but sometimes
you actually learn the thing that
minimizes the distance it's interesting
to think of these algorithms our
representation learning algorithms but
they are also manifold learning
algorithms by extension and most of the
time we are not constraining the type of
manifold that could be learned what
we're constraining here is the way we
throw things off the manifold that is
that is and and we're doing that because
if we wouldn't constrain that we had
would have
no idea of what was on the manifold
because the only reason we can train
anything here is because we know this is
the data sample that is on the manifold
and that is closest to the wire through
the corrupted one but would it is it
relevant that the the manifold that it's
learning is changing during training and
on which manifold well it is that the K
is is it that when when the model
converges there is a manifold change
there is a manifold all the manifold of
natural language never changes right
that is just the manifold of natural
language and we have data points on this
manifold given in our training set and
all we're doing is we know that these
points are on the manifold and all we
can do is we're throwing them off a bit
we don't actually know but we think
we're throwing them off we're pretty
sure because we mask out these words and
that should give you something that's
not on the manifold and then we're
learning to map them back but on that
point that I agree with you there may
well be some notion or manifold which
perfectly represents the English
language but we could every single layer
in the neural network transforms one
manifold into another and we could quite
easily transform language into a
spherical manifold just just doing an l2
normalization so then all of the inputs
to Bert would exist on on the sphere so
I guess what I'm saying to you is that
there are many possible manifolds that
language could exist on and given a
typical neural network for birds does
the manifold change and evolve during
training I think it does the one that it
that the bird represents for sure yeah
yeah is it also we're thinking that it's
shrinking the dimensionality of the
manifold as it goes through the network
to a little bit and so maybe like
throwing it off the manifold is trying
to tell it as we compress it keep them
laughing in some way that's a good
question is the that the things that
bird can produce at the beginning is
that space somehow higher dimensional or
is it just different right it is it is
really good question so if this is the
manifold of language that it's supposed
to learn at the beginning is it learning
to output every possible thing or is it
just out learning to output something
else and we're just kind of doing that
as a I have no idea who knows so is that
what we think then like so we have this
manifold of all natural language and
let's say it goes into a
three-dimensional representation so we
can think about it so if we have this
cube of all possible data and our
manifold describes some like oddly
shaped circle in this cube so is it like
would we imagine that originally you
know this like our coverage of this cube
could be like this oddly shaped thing
that's very sparse and not dense there's
something how do you think the manifold
and now this cube changes during
training would it be more clustered or
would it cover more of the cube well I
so so yeah here here's the question so
we have to rephrase it maybe a bit let's
say your true data distribution is like
a weird circle in the in the cube and
you build a neural network that can
basically output any point in the cube
initially and now you you train it and
your base you're asking okay how does
the things that it outputs how does the
things where it thinks the energy is low
changes as the training progresses and
that probably is very much a function of
a how you find ways to throw things to
to basically train it in and be what the
architecture is and this is exactly
where Jung Lacan differentiates the two
methods so in the contrastive method
what you would do is you would say
have a bunch of samples from this weird
circle I don't know yet anything about
the weird circle but I have a bunch of
samples and I'm just going to try to
move them in each direction and learn a
function that is high
there and low where the actual points
are so what you would end up with is
kind of like a tube where where right
like like a hose where at the inside of
the house is a very low energy function
the outside has very high and you don't
know anything about the rest of the cube
but you don't need to maybe because
because you only deal with points that
are sort of close to that and the other
way to do this is by regularizing by
basically a priori saying look model you
cannot actually output the whole cube
you can only output circle ish things
and now please fit yourself to that
thing so these are the two methods and
we can contrast it to a probabilistic
method the probabilistic method would
need to assign a probability to every
single point in the cube and need to
consider the whole cube whereas we just
need to consider that area around the
circle that's maybe a better way of
phrasing it's interesting it as an aside
when I read Francois chalets deep
learning book he used this wonderful
analogy that deep learning is a little
bit like imagine you have a piece of
paper that's been scrunched up into a
ball and every single layer successively
unscratched as the piece of the piece of
paper until until it's beautifully flat
at the end and it does make me think
that because of the way that the
architectures are described in deep
learning it might actually be expedient
for it to create I kind of manifold
which suits our task or suits our
purposes at the end ultimately if you
unfold all of this through your neural
network and then you make a linear cut
which is what the supervised methods we
have nowadays do they just are a linear
classifier at the end the data manifold
so to say is when you trace that cut
back or
through all the layers through this
folding and look at it in the input
space that's that's basically what your
neural network learns is whatever that
cuts gonna end up being in the input
space exactly it's and that's a
beautiful example because imagine if
you're on you're a poor piece of paper
you had a vertical line and you could
you colored half of it red and half of
it black and then you scrunched it back
up again into a ball it would be very
difficult to make that cut when it's
crunched up into a ball but if you first
flattened it out and then made the cut
then it would be easy yeah
so this has been incredibly successful
in the context of NLP the problem is it
doesn't quite work in the context of
images and and there's been sort of a
lot of work in trying to sort of use a
supervisor link you learn good features
in images and it's only in recent years
that those systems have been those
attempts have been somewhat successful
at actually giving good features they're
based on contrast if embedding or Sinese
networks
the idea is you show a system an image X
and the image Y that will be compatible
to it would be a distortion of the image
that doesn't basically change its
content you train the network's to
produce similar output similar vectors
or perhaps even identical outputs and
then you construct the contrast examples
consist of showing two images that are
different and then pushing the two
vectors apart it's been successful
applications of this to face recognition
but tag magnet at all the years ago put
an earlier examples of Sammys net for we
said various applications more recently
though the techniques pearl by by Mishra
moo-coo becoming her and his
collaborators and seem clear by the team
from Google have shown that you can
learn good visual features using those
techniques and so I think deep face was
the first paper that I was aware of that
did something like this it used the
siamese network architecture it's
similar if it was it's different to the
kind of the sincere laws and the moco's
of this world because what they do is
they don't need any labels at all they
just generate perturbations of the image
and then the classification task is to
say whether or not they are the same
image whereas the deep face and the
Triplett variant
which was called face net their
classification task you already knew the
identities of the people in the image so
you would say is this the same person or
not and the the challenge is this
negative mining strategy because you
know all of the points on the manifold
but you want to come up with examples
contrast of examples of things that are
different and if you think about it
there are just too many possible
combinations
the reason the triplet loss came about
was trying to make the algorithm
converge faster by selecting hard
examples so you'd have an anchor and a
positive and an anchor in a negative and
you would try and make the anchor in the
positive as dissimilar as possible I
wanted to correct myself from before
where I said you don't always learn the
energy function you can maybe here see
the same thing right the thing on the
top is simply the loss it's simply a
maybe a distance function between these
two encoded things but you can always
make these encoders part of your energy
function so your energy function would
actually become all of the blue and red
stuff together and that's how you
basically reformulate you can
reformulate everything into the energy
function that you're learning but it's
it's the thing that you're learning is
not always outputting a scalar if you
will but why do you why do you think why
do you think that these methods like the
bird method works not so well for let's
say images what what he's saying here
well I think it's that no pixel level
reconstruction right if you were to do
bird and just like crop out or put
random pixels in it would have to like
the output have to be this gigantic
image I guess yeah but you can you can
do that right you could technically just
mask the pixels and then ask it to
reconstruct and that's how auto-encoders
work and and they're just not as good so
is it something about the visual domain
or is it something about the fact that
there are just more pixels or what do
you I think there are more ways that an
image can be some
ethically equivalent but completely
different just things like camera angles
and distortions and lighting and
resolution even the dog can be rotated
or different colors and so on and it's
still a dog there are just more ways
that you can diverge in the visual world
but still be semantically equivalent but
I can say there are an infinite number
of ways I can express the same thing in
natural language they are they're just
infinitely more infinite ways in the
visual domain could you link around that
a little bit more why are there more
ways of semantic equivalence and
language that that is my my question are
there I mean I don't think I can express
the same thing in in so many different
ways but maybe it's just not as many as
I could express the same picture in in
different pixels but I think like the
way you already give it some context in
the sentence before you apply the mask
constrains it so much that you know the
way you would give it context in an
image and then mask out the section of
the image is way less constrained than
that then the sentence I think at least
it's like mmm I guess at that if you
just look at it in terms of the
dimensionality the image will have three
color channels it will have a resolution
it will have many layers of processing
in the encoder that the language is
already pre processed I think when we
create these word piece embeddings we do
things to reduce the specificity of the
tokens and reduce the vocabulary size
and it just seems to me that there are
the the space of perturbations is
smaller maybe yeah that's true I think
the visual domain makes it very explicit
that the true task in these contrastive
methods is to find good ways of throwing
the of throwing the data points off the
manifold because what ultimately ended
up working is things like these cropping
and augmentations and things like this
indeed and maybe another way of
describing
is with language it is structured even
they've been a transformers model you
still have connections to actual tokens
which have semantic meaning in the
visual world its just pixels you that
there is no structured connection to the
thing that you're trying to learn at all
I just think it's an interesting thing
to think about it could be something
about yeah that the dimensionality how
structured it is the fact that there are
just so many more pixels than there are
words okay what what do you think about
this let's say we have an image encoder
that brings our images into this three
dimensional space and then where we put
our entire data set through it and then
we plot it in this cube where it
currently we're at the neural network
has currently put it so then do you
think these different data augmentations
like we talked about so with the
reinforcement learning with augmented
data paper they show that like cropping
works really well as a data augmentation
so you think that when you take the
crops and then put them through the same
encoder that have just encoded the
original data space that the distance
between the original data and the
cropped image is closer in this new like
manifold subset then say rotating all
the images that's a good question
probably yes right because you're what
you're telling that the model is that
the cropped version and the original
version is the same so it should not it
should map them closer
yeah just like what I'm trying to get at
is like so is the key to designing a
good augmentation for contrastive
learning to just throw it off a little
bit you know the current thing like
don't throw it off too far because then
I now there's like no overlap and I
can't recover this like we talked about
with scrambling the pixels you can never
reconstruct it it was just like
scrambled anywhere yeah I think it
depends a little bit on the kind of
inductive priors in the encoder
architecture so if it uses a CN n that
has
a translational equivariance I say that
in air quotes that I think they actually
are equivariance but that means it would
work rather well for the chops it would
learn that if this particular square and
this particular square is in the same
image but if you started doing things
like rotations and scale transformations
then it would struggle to even recognize
those two things as being the same
however the cost is very high because
there's many ways to be different for
two images to be different and for this
to succeed the amount of competition and
training is absolutely enormous even for
these small data sets so I think
ultimately those methods actually are
not the best and will not scale to
release very very large representation
vectors you can interpret yarns also as
contrasted methods basically where the
data points are pushed on particularly
the sort of energy based formulation of
Gans and usually these Gans where you
push down on the energy of data points
and then you push up on the energy of
chosen points and those points are
generated by the generator Network that
is trained to produce points that
progressively get closer to the manifold
so as to shape the energy function
now again can use any kind of objective
function as long as again it's a
decreasing function of the data points
and an increasing function but the
generated points yeah that's an
interesting that's an interesting point
the fact that not only in Gans not only
is it one way of throwing something off
the data manifold finding points that
are not on the data manifold but it is
true they do get progressively closer so
it's a kind of built in curriculum
learning right because the generator at
the beginning can produce pretty much
any point but the point tsipras will get
closer and closer to the data manifold
and it's like a built in curriculum for
the discriminator
that's interesting and and so the the
generator manifold will kind of converge
on the discriminator manifold yeah and
do the manifold because they're they're
different architectures I mean I think
there is a little bit of
eating right the the generators allowed
to peek at the weight of the
discriminator that is the final manifold
the same regardless of the fact that
one's a discriminator and one's a
generator well you would you would hope
you would hope it is the same in the
fact that so if you look at the
generator it produces data so it has in
fact a much harder job because you can
you can look at the same manifold in
data space through the eyes of the
discriminator by simply viewing it as an
energy function and now you say whatever
the discriminator tells me is low energy
that's where data is and that's exactly
what the generator does it tries to go
wherever the discriminator says here's
low energy but the discriminator you
know is also trained on true data it
must also say that true data has low
energy so and that's why the generator
manifold ultimately matches up with the
data manifold which is what we're
interested in we're interested in
describing the data but the interesting
thing though and this gets my point from
earlier is that if you look at the
functional landscape of the
discriminator
so let's say this this thing that we're
looking at here what if it's completely
fly so it gets orange at the top then
what if it's completely flat so what if
the generator produces a green point
here how does it know to descend down
into this valley what if there is no
gradient but I think these green dots
are what the generator produces and the
valley is where the discriminator is at
so this is where we want the manifold to
be so then I think it needs to know this
gradient in order to produce better
examples on subsequent iterations yeah
it does right you backprop through the
discriminator through the generator so
the generator always knows in which way
needs to change to in order to fool the
discriminator more gans are notoriously
difficult to train and to Train Staveley
exactly because of that so if you take a
super-advanced discriminator that can
perfectly tell you here's data and here
is not right there's like this super
steel
cliff and then everywhere else is just
bad all of the rest of the day to space
is just bad that's bad and you get no
gradient so if you want to learn a
generator in that landscape you have no
chance as you say you have basically
have no gradient so what you need to do
is to train them jointly so that you
always keep it flat enough for the
generator to come closer but then it's
kind of too right it's too easy and then
the discriminator can well make it
steeper and then you get gradient again
so it's yeah very cool yeah I think this
energy paradigm is a great way to think
about it definitely yeah I mean this is
a variation this talk is a variation on
the talk that Lacan has given about Gans
while back he had yet this talk very
basically says Gans are the biggest idea
and machine learning since I don't know
in this in this decade and he talked
about much of the same thing about these
energy functions just not as general as
he's talking about them now do we still
need Gans to produce really good
generative models that is a very good as
extremely good question
of course in the framework of energy
based models the generator is sort of a
byproduct right the generator is sort of
the way that we produce points that are
off the data manifold because what we're
assuming is basically the space of
images is so high dimensional that this
model is never going to hit it perfectly
but if it hits it close enough we can
say well that's can't that's it that's
the point of the data manifold that's
that's close enough so it's the
generator is like a by-product of
training this model but it turns out to
be very useful the question is that we
ever need generative models in the first
place yes so what would it look like if
we made an energy based generative model
without that kind of adversarial
architecture well you you have that in
auto-encoders they just work like crap
and
yeah but why why does pixel or intends
and they they're pretty much crap and
yeah I I don't I can't tell you this
this type of curriculum learning that
the Ganzer have built-in seem to just be
doing very well and what would happen if
we trained again from a self supervised
discriminator Wow by guest sir like give
the discriminator how so if the
discriminator is taking all the images
and then also doing some supervised
learning with it yeah I mean I dunno
what that would look like I mean maybe
if we had the sim CLR type set up so we
had a discriminator which was trained on
that the simple question of do these
patches belong in the same image and
then we created again setup from that
well I know one paper where they use the
auxilary rotation prediction task in the
discriminator so they actually kind of
do test this I don't know like and then
yeah probably a good opportunity for a
research paper this is the contrast of
self supervised learning for again but
so like what we've been talking about I
think it's like some of now the
discriminator better knows how to
unravel the paper and put the data in
the manifold that makes sense if it has
I think there's like two things the
multitask learning it's like it's
learning how to better unravel the paper
and then but then I don't think the
cutting is too useful I think that the
thing that multitasking is helping with
okay well I think I I can see what you
mean you you mean you have like a
classifier that can do this this self
supervised task really well and now we
want to train a generator to fool this
one as much as possible yeah I I think
that would go a bit wrong because and
this is my opinion the image domain is
just so prone to adversarial examples
that you don't know when to stop the
generator so at one point it will just
produce crap and then there will be a
small point where it will produce
something that's actually good and mmin
as it passes that it will start just
producing
adversarial examples for that particular
classifier model that's why you keep
training the discriminator so that all
the adversarial examples it can kind of
catch those right every time like that D
generator produce one it will be like
nope nope nope and if you just keep the
discriminator where it is the generator
will just learn to adversarial example
the crap out of that discriminator and
not produce your true data anymore
that's really interesting I'm a little
bit skeptical about Gans it just seems
to me that we don't need them I have a
feeling that in a few years time we'll
look back and think why why did we even
bother doing that guarantee so a lot of
classical algorithms can be interpreted
in the context of energy this morning
and here I'm going to talk about a few
architectural and regularized methods
particularly regularized methods so the
idea of regularized latent variable
method is to regularize the volume of
stuff that can take low energy the
volume of y space that so you can take
low energy and you do this by
regularizing the information capacity of
a latent variable a good example of this
is Pascal e so that's the that's the
principle you know so this is first
coding and and Canas and sparse
auto-encoders and variational Ottoman
code was interpreted in the context of
regularized latent variable methods so
in the context of sparse coding you
linearly reconstruct a vector by finding
a vector Newton factor that is farce
kind of minimizes a particular
regularizer here the l1 norm and then
you can train the decoder to maximally
reconstruct a training samples the thing
is because the capacity of the latent
variable is limited is only a limited
volume of white space that the system
can exactly or properly construct and so
automatically when you make the energy
lower at certain points it becomes
outside so this is interesting cuz he
said earlier that these architectural
methods the ones that they're try to
compress the representation are
essentially shrink wrapping the manifold
so he then said that by bringing the
values down around the manifold it has
the effect of pushing
the others up yeah that that makes make
sense it's I mean it is a fancy way of
saying you should just limit the
capacity of your model and regularize it
and that's what we usually say when we
regularize models or when we build
things into it
of course the dangerous if you build the
wrong restrictions and the wrong
regularizer x' and you make things worse
because you're going to pre determine
that the energy is high in a place where
it actually should be low
similarly for regularized autoencoders
so regular rise between coders are
between quarters where the again the
information capacity of the religion
code is limited either through sparsity
or something similar or by adding noise
to it so the the the idea of variation
of autoencoders is just to add noise to
the to the code and to limit the
amplitude of the codes so that the
information capacity overall of the code
is is limited and you can interpret them
as a deterrent variable energy base
model in which you approximately sample
the latent variable through
approximately integrate or marginalize
over the latent variable as we simply so
those techniques work very well with
simple decoders and I think try to make
them work with so deep representations
as well now there are other types of
regulation that lead to chemical
representations in particular things
that exploit a graph of similarity or
perhaps a temporal continuty so things
like learning temporarily in variant
representations or making them linearly
predictable this is worked by Rosco rush
in a few years ago or by minimizing the
the curvature of the trajectory followed
in the representation space enough yeah
I think this this is the obligatory
slide where you push work either that
you like or want to promote or sometimes
your own work when you give talks like
this so at the end you should come to
something where it's like look we've our
group has done these things as well even
even though it's just like it seems like
one tiny tiny bit picked out of what's
possible and
ultimately is just coming back to saying
we're going to encode what we know about
a problem into the architecture in an
autoencoder that's the fact that we
think the data point contains less
information than the pixel space so
we're like at a 362 what we knew at the
beginning here this works really well
and really beautiful features it's now
clear that those features are useful in
server deep condition on that context
yet so we can use conditional versions
of those systems to do video prediction
and perhaps get machines to learn some
structure about the world so a good
example of this is some work that we do
should I clear a couple years ago which
consists in learning one of those
variational encoders or regularizer to
encode a conditional to encoder type
architecture to predict what cars around
you are going to do so to be able to
learn a policy for driving it's good to
be able to predict what cars around you
are going to do and of course it's not
deterministic so you have to have a
little Bible model so that you can vary
the latent variable to make multiple
predictions I love the fact that it's
it's a probability distribution here
it's busy this is some used a
combination of variation between co2
type sampling as well as another
regularization that is basically
equivalent to global dropout so half the
time we tell the system your latent
variable is zero make the mess make the
best prediction possible so these are
the result you get blurry predictions if
you don't use a latent variable you get
much better prediction shown on the
right here by sampling the latent
variable with different values you get
so realistic predictions that are all
very different on the left here is the
recorded video yes so this this is I
think an interesting point just for
these types of models where if you say
please model make the single I'm not
going to train you to make the single
best prediction which is sort of the
average of all the data samples that I
have it's going to just make a blurry
mess because what you should act what it
should actually do is say well there are
two possibilities to continue here and
that's what you capture with this this
latent variable so you have two training
samples maybe and in one training sample
the car goes here in the one training
sample the car goes here and he want to
let the model infer that there is a
latent variable that is different in the
two situations and that actually both
are okay we are using this in fact to
train a forward model of the world so
the trick here is to have a way of
predicting what the world is going to do
that you can use in the context of a
model predictive control system this is
not reinforcement running because
everything is differentiable including
the objective function the cost so we
estimate the state of the world run or
forward model this is not the real world
this is our model it's differentiable
it's a neural net and for each new state
we give it a proposal action and we
sample the latent variable not
represented here we can compute the cost
and through backdrop we can train a
policy network to learn to generate an
optimal sequence of action that will
minimize the overall cost over an entire
trajectory or you just arrive the
recurrent neural network yes yeah is
this is an RNN yeah I get the impression
that yan isn't a fan of reinforcement
learning it's a cherry on top don't you
know that so this is very similar to
control except that we don't infer a
sequence of actions we train a policy
network to produce the action from from
the state and having the ability to kind
of generate multiple futures is
absolutely essential so this is time can
be trained to drive cars with some level
of reliability so this is an example the
blue car he is driven by your agent it's
actually invisible to the other car so
it has to avoid getting squeezed and the
white dot indicates the control on the
car acceleration braking and rotation
yeah I think the the point here what
he's trying to say is that basically
that the what is learned now isn't
reinforced it's it's an energy function
that is basically low whenever the the
trajectory of the car is good
so whenever the the trajectory of what
it predicts the cars will do will be low
and that energy function has more
tipple minima right so it has multiple
continuations for the same situations
and really much like an RNN I can sample
from that RNN and that gives me multiple
basically continuations of the sentence
that I start with and I think the
connection maybe the connection is a bit
missing - for him to now really make to
the energy function at the beginning
where he says look the energy function
here has multiple minima and we want to
basically explore those in the future
but how would you contrast that method
to reinforcement learning I mean for
example is it planning does it know that
it would be breaking the rules of the
road if it took certain actions no I
just going in this case I think it's
just going via the the energy function
so it it simply it simply takes a it
looks at the energy function and it asks
where is the energy function low and it
sees are at this point and at this point
that corresponds to the car going left
or right but not the car being in both
places or in the middle of the road or
something like this so there the energy
would be low but weirdly
would it be short-sighted in a way that
a reinforcement learning algorithm
wouldn't be I think it will have it will
have the same problems it's just a
different way of of training it and
different way of looking at it basically
they're training an RNN of the world
where as a reinforcement learning
algorithm would train just just a a
policy function to to minimize some
reward at the end ultimately probably
not too much of a different thing but so
what do you think about these world
models do you think model-based
reinforced learning is more powerful
than model 3 in terms of our it's a good
question well is the is is reinforcement
learning with a learned model
model-based and it's a philosophical
question because
who knows point is always do you don't
have an explicit model of the world
right
neither does young laka here but what he
does is basically learns an RNN where he
says oh it's even the loss function is
differentiable and if he does it be a
reinforcement learning it would actually
learn how to drive the car by trial and
error and here they learn a model of the
world yeah who knows okay conclusions
and conjectures so supervised running is
running dependencies as I just said the
static home message reasoning to a
vector representation and energy
minimization might be a way to make
reason incompatible with the groaning
and with energy based learning the main
obstacle is dealing with uncertainty in
high dimensional continuous spaces this
is not a problem with NLP and Bert
because we can discretize the space the
space of words is discrete but it is a
problem in a dimensional continuous
space it's like video so predicting
points is insufficient predicting a
distribution is intractable so we have
to resort to energy based models these
are weaker than distributions and we
have two options to train those
contrasting methods and regularize
latent variable methods my money is on
regularized latent variable energy based
models I think those eventually will
overtake all the other methods this is
not the case at the moment though I
could energy-based supervised running be
the basis for common-sense this is our
best bet at the moment perhaps possibly
animals and humans learn largely so
supervised and scaling-up supervised
running and reinforcement learning will
not take us to human-level AI and by the
way there is no such thing as artificial
general intelligence intelligence is
specialized including human intelligence
it's very specialized and so I think it
makes more sense to talk about rat level
cat level or human level intelligence
rather than AGI okay I think we'll start
with the low-hanging fruit I mean I
completely agree with him that there is
no such thing as AGI this is the
francois chalet world view you know that
that intelligence is specialized and
it's an expression of the environment
and how us as agents interact with our
environment well contrary to that the
psychology literature tells us pretty
much that at least something like IQ is
just your ability to solve arbitrary
tasks so if you can pretty much throw
arbitrary cognitive tasks at someone and
their IQ would pretty much predict with
a reasonable correlation how well they
would do at it so maybe there is such a
thing or we just were just so biased and
coming up with cognitive tasks maybe the
intelligence is just the ability to
solve problems the ability to solve
cognitive logical problems and I'm
putting that my reasoning behind it is
when we determine IQ we don't need to
have that particular set of IQ tasks we
know that we can make an IQ test from
pretty much any collection of cognitive
tasks it doesn't matter which ones they
are it is remarkably consistent across
any sort of cognitive tasks so maybe
there is such a thing as general
intelligence and it is just the ability
to solve logical cognitive reasoning
tasks I suppose so but any tasks that we
place in an IQ test would be playing on
similar tasks that we've learned before
true that that was my counter argument
before is that we are just so biased
than coming up with tasks that we're
fooling ourselves of by saying that
there's only one kind of intelligence
but yeah it's I mean it's it's it's also
a bit of an empty statement to say
there's no such thing as AGI and okay
when we look back to the the poet paper
from uber that's the equivalent of the
simple manifold picture it shows you
what intelligence looks like in a very
simplified contrived example and in that
world if what does it even mean to be
intelligent that bipedal walking robot
can only do a few things that can just
walk and it can move around and in a
sense we are like the bipedal robots in
the earth world fewer aliens looking
down on us we must seem incredibly
constrained in all the ways that we can
interact with our
surebut but any general intelligence we
build will also be in that world right
it's as if they're saying that there's
no general intelligence because it could
not generalize to other worlds than fine
I agree but in this world right here
maybe there is such a thing because it's
going to live in the same world as we do
that's true yeah I mean we don't want it
more general than that as a point of
clarification I think I was talking
about the intelligence explosion and
you're just talking about artificial
general intelligence so you're just
talking about an artificial intelligence
that could potentially have a task
adaptation to the unknown unknowns yeah
okay so what I find particularly
interesting here is the reasoning
through vector representation and energy
minimization what do you think of that
do you think that even means I was
thinking that as well I have no idea
what that means
yeah this is interesting to think about
like the vector how we constrain these
vectors to like you can either have like
they can either be in dimensionality and
they can take on a certain range of
values I guess it's about searching
through the vectors then yeah yeah yeah
exactly right so what what bugged me and
this talk is in it might be not so easy
to go back to the beginning but maybe
you can click the little arrows or
something where at the beginning if you
click the big arrows on the right is
there like a way to go back or at some
point he says inference is done through
optimization oh yes the energy is used
for inference not for learning which
basically means if I have an energy
function of something I could find a
data point that is compatible with the
energy function by minimizing that
energy function which I could do through
something like gradient descent right
but no model in this entire talk does
this actively at inference time so even
though again the generator learns to
minimize the energy function during
training when it produces a sample it
doesn't actually optimize the energy
function it simply produces the sample
right and and all of these all of these
models they are none of them that he
said were actively reducing the energy
function at inference and maybe this
reasoning through vector representations
and energy minimization is an allusion
to the fact that if we had an energy
function that tell us what our logical
things right
imagine you have a classifier of what
what are what statements are logical and
what statements are illogical I could
now construct a mathematical proof
simply by starting with a bunch of
symbols and then SGD minimizing to make
that energy function lower and lower and
what I would end up with is a logical
statement and if I now translate these
to some sort of embedding space where
it's continuous I could actually do this
right I could end up with a fully
logical statement because I have the
energy function that tells me where to
go and I think that that was just kind
of missing in this talk this this notion
of inference time energy minimization it
really confused me as well because it is
it saying that the energy function is
not used as part of the training process
because it is yeah I define it what what
what it always comes by what do you put
into the energy function is the model
part of it or is it not right right it's
like if I was taking my generator and
then I'm trying to like now search along
those Z inputs to get the most realistic
output and now how do I do that so in
that case is the Z differentiable so I'm
trying to maximize the realism of my
generator output and I'm trying to
search for the Z is that differentiable
I think it's the F which must be
differentiable can I do the derivative
of the loss with respect to the Z yeah
sure you can find let's you can find the
best Z to either you know minimize
whatever the loss is and you of course
you'll have to pick this
real data sample so you can essentially
find the Z that will most accurately
reconstruct that particular data point
you can do that but it's not the meaning
of again right I'm more looking for
something where where you actively do
what he said at the beginning you should
set out to do so he says that energy
based models are weaker than
distributions but there are necessary
evil because it's just intractable to
use distributions so making a single
point prediction is not good enough but
outputting a distribution is not
tractable I think he contrasted with
these two things and with the car
example you could see this very well if
you just allow to make a single
prediction where the car is going to be
in two seconds and you know it could go
left or right you'll put it in the
middle right because that's the kind of
average of all the futures but you can
that's very bad but also distribution is
not very good because you consider every
possible thing and now the energy
function you can simply say well it's
low here and here and higher in between
isn't it still intractable though
because the energy function you would
still need to enumerate all of the
possible X's and Y's yeah as long as you
can start closely enough to the to the
actual valley it's fine right that's
exactly the problem with the generator
from before as long as as you're close
enough you're good but if you go too far
out the energy function is part of me is
still just thinking what the what the
hell is he talking about
but there's the latent variable thing
fine but just forget about that for a
second isn't it just exactly the same as
neural networks now we have a prediction
function which is basically the energy
function and you can explore the space
of x and y now what's the difference
yeah well is it basically just a hole
just nothing he's saying nothing as we
said at the beginning right this is a
scripted talk this isn't this isn't
introducing any new concept this is him
describing and summarizing existing
methods and you can frame pretty much
anything into these energy based methods
I think the stronger claim that he makes
is that there is a sweet spot of of
these they're in the sweet spot isn't
with the supervised learning you could
frame that as energy based or with the
probabilistic method which you could
also phrase as energy based but there is
a sweet spot in between and I think he
says that is the dis self supervised
regime I think that's the main claim
here and again it is a descriptive talk
it is really we know nothing more
we just know a formulation of what we
already have yeah I would understand if
the energy based paradigm gave a
theoretical framework with guarantees
and bounds and if it was adding
significant value to the theoretical
understanding of deep learning it's
definitely a nice intuitive way of
describing models especially
compositional models I like that
I think explained in these terms people
would understand Gans for example much
quicker than they would do if you
explained it directly but I'm being
skeptical here and I'm hoping you'll
play devil's advocate and defend the
energy based models paradigm well I'm
not I'm not sure that just from the
energy based models we can we can gain a
lot I think it is really the what he
says the self supervise yeah I don't
because energy but it's just so huge
right I think the things he says around
it are much more important interesting
so on the basis of our conversation
today do you think the listeners of
machine learning Street talk should
learn more about energy based models or
do you think that they've already
learned more than they need to know well
if they understand that it is simply a
way of describing many current methods
then I think you know enough maybe you
know if you hear this maybe you'll see
connections between things that you
didn't see before like oh wait a minute
what's the what's the actual difference
between a k-means and a Gaussian mixture
model oh yes one is normalized but both
are energy based oh what's the
difference between the a language model
and just the discriminative thing oh yes
one is one one deals with probability
one deals with an energy what's the
difference between supervisors and self
supervise oh one is maybe reconstructive
and the other one is just discriminative
but there is a connection there's a
connection between any of that yeah okay
well I mean this has been a marathon
it's been two hours of 45 minutes
without going for Joe Rogan I think we
should edit this a little bit that Conor
do you have any passing final comments
well I agree that it never really became
super clear to me what exactly is an
energy based model I get there's this
like scalar score of similarity between
variables and we get to search through
these like latent variables to optimize
the scalar activation of the energy
function and that's interesting although
it's still not clear to me why this
scalar value is so different than having
like a sum over you know having a exact
probability assigned to it you know like
having say like this is five compared to
11% I guess and then this other thing is
30%
so that whole difference to me is maybe
not quite clear but I definitely think
you know it's a really interesting
unification of these ideas and also just
like this really helped my understanding
of what these latent variable models and
unifying like the variational auto
encoder and the generative adversarial
network and how you use these latent
variables to do multimodal predictions
I'm really interested in the chart of
cognitive development and I think that
they're you know I think it's there's
more to it than just self supervised
learning I think that also
you know as we talk like inductive
priors what knowledge is kind of built
into the kid I think that plays a huge
role what young Locka wants to say is
not what he focuses on the most I think
what what he means to say is something
like there is a lot of information in
the data itself that we can make use of
using these self supervised tasks and
objectives and we can unify all of those
using this energy based formulation
even though energy based models
encompass every possible thing you can
think of and I think he just wants to
show that all these different tasks
people come up with masked language
modeling next sentence prediction
denoising auto-encoder they're just
variations on the same theme which is
that we are just trying to get to a
function where that is happy when
something looks like data and that is
not happy when something doesn't look
like data if you can maybe think outside
the box in this framework maybe you'll
hit the next big thing towards AGI which
totally exists okay on that bombshell
we'll see you next week folks if you
stayed with us thank you

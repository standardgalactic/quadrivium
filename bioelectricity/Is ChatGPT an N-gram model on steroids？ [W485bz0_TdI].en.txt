anyways you have this hashtable of 400
things and you're going to ask oh is
there something from this hashtable of
400 things that is close to my
Transformer prediction right I'm not
going to tell you in advance I can't
predict for you in advance which one it
is I'm just going to say is there
something that matches it and the kind
of um result that I got was that 78% of
the time in in some sense you you get
like a good match okay I'm a machine
learning researcher at uh Google deep
mind uh in a former life I was a
mathematician uh like you Tim I also
have a podcast called The cartisian Cafe
hello everyone this is Tim win and
welcome to the cartisian cafe the
podcast where an expert guest and I map
about a scientific subject in detail I'm
a mathematician who has his hands in
several different things I obtained my
PhD at MIT had a career in Academia as a
researcher in mathematical physics and
gauge Theory and then I transitioned to
Industry someone once asked me uh you
know how I proved the par gr conjecture
I said well it's just a homeomorphism I
just had to say where every Point went
and I
did yeah everyone should subscribe to
the cartisian cafe Grant sanderson's
been on can't complain so he's he's the
biggest math YouTuber I I was definitely
one of those kids where from a very
young age I self-identified as liking it
I think a lot of that probably had to do
with games that my dad would play with
me um that you know like a lot of dads
just wanted his his kids to be
interested in science and math and the
world and things like that um and I just
loved patterns I've had uh now two
Fields medalists Richard bards and
Michael Friedman so yeah I've been very
lucky to have a star sted guest if you
had to attribute uh your success in
mathematics to any any particular
qualities can you can you name any um
luck and obstinacy I mean need a certain
amount of luck I just I mean you know
mathematics has hundreds of hard
unsolved problem I just happened to be
working on one where there was something
interesting to be discovered
the brave search API brings affordable
developer access to an independent index
of over 20 billion web pages what sets
it apart well it's built from scratch
without big Tech biases powered by real
anonymized human page visits to filter
out data and refreshed daily with tens
of millions of new pages perfect for AI
model training and retrieval augmented
generation the brave search API offers
ethical data sourcing at developer
friendly prices that scale with your
business whether you're working on
language models or information retrieval
systems Brave offers representative data
sets and upto-date
information affordably we'll get started
with 2,000 free queries monthly at
brave.com for/ API now back to our
discussion with Timothy win Timothy has
written a paper called understanding
Transformers via engram statistics can
you give us the elevator pitch sure so
at a high l level the uh question I'm
trying to understand like many other
people is uh how do Transformers use
their context when making the next token
prediction so uh the analysis can be a
little bit abstract so let me just start
with a very concrete toy example uh I
use in that uh my paper the data set
tiny stories which is this synthetically
generated children's stories data set
but it's it's realistic enough to you
know capture natural language unlike
purely synthetic data sets uh at
inference time I I feed to the
Transformer once upon a time there was a
right and now I have to just understand
what the Transformer next token
distribution is as a probability
distribution over all the tokens okay so
so in the training data suppose for the
sake of Simplicity every time I see once
upon a time there was a it's always
followed by the token bear okay but also
in the training data every time there is
the three G there was a I end up having
three possible animals lion tiger and
bear and let's say with equal
probability right let's say bear occurs
100 times lion occurs 100 times and
tiger occurs 100 times okay so you have
you have in this case let's say two
plausible completions right once upon a
time there was a Bear Once or because
there's also there was a maybe it's it's
a three-way coin flip between lion tiger
and bear so softmax is a way of
converting Logics to probabilities mhm
so if lion tiger and bear had very high
logits and everything else had very low
logits when you apply softmax then all
the other categories would be uh very
close to zero probability whereas lion
tiger and bear would all have equally
high probability the training data gives
you different choices for how to
complete the next token right because in
this case there's once upon a time there
was a and in the training data you see
Bear all the time because you have this
once upon a time but you also see there
was a and there's three other animals
and and then it's unclear does does the
Transformer at inference time did it
learn kind of the completion based on
the full context or the shorter context
or anything else in between right so
based on that
um that so that's a motivating example
to to kind of illustrate the difficulty
in understanding how a Transformer makes
a prediction what statistics from the
training data is it using right and I
like to think about it in terms of two
different problems okay there's the what
I call form and selection right form
would be uh kind of what I described oh
the the kind of shape of these
probability distributions right I I told
you sort of two candidate proposals a
one hot Distribution on bear or a
uniform Distribution on lion tiger and
bear so that's the form of these
different possible completions and then
there's the selection problem does it
choose the the the kind of the context
that would give you be or does it choose
the context the shorter one there was a
that gives you these three animals okay
so so kind of a cartoon picture of how a
Transformer a large language model in
general learns would be oh I give it a
lot of training data at inference time I
give it the context that I want it to
complete and now it has to uh again in
this cartoon picture it has to sort of
Select what are what are the relevant
contexts what are the relevant
statistics uh to choose from and then
the second part is what is the form of
those statistics is if I look in the
training data am I going to get a one
hot distribution if I kind of retrieve
the training data statistics Associated
to that context or am I going to get it
a uniform distribution okay like in this
lion tiger and bear example so so sort
of the first result of my paper is that
actually if you just focus on the form
part forget the selection I'm not going
to tell you which of these rules I'm
going to choose I'm just going to say
every time the Transformer makes a
prediction I'm going to scan the context
look at all possible uh engrams from a
certain set okay look at all those Poss
candidate completions there's always
going to be sort of um in a way that I
can make more precise some andram
statistic that well approximates what
the Transformer is doing so it's a way
of kind of quantifying to what extent
it's a stochastic parrot albeit that's a
very kind of incendiary term but that's
one way to sort of think about it oh
it's not an incendiary term on mlst it's
it's yeah it's very welcome but I think
we should Define a couple of things so
um an engram term is a kind of template
matching system right and a lot of NLP
systems used to use that a long time ago
and you can explain that in a minute but
as I understand it you you took this
data set the tiny storage data set and
you um essentially matched a whole bunch
of templates so you know one gram and
two gram and three gr and you built this
big bag of templates of different sizes
which have been you know derived from
this tiny storage data set and then you
stick it essentially in a hash table and
then that's how you're selecting those
templates yeah exactly you can think of
it this way it's like okay the
Transformer is this kind of uh very
complex machine right and every time it
makes a prediction you're going to go
through your hash table right and say
okay I have uh a prediction based on on
this context this engram rule I could
use say the only the most recent token
the two or three right there was a that
would be three tokens of context once
upon a time there was a that would be
seven tokens of context and then there
are all these other kind of rules where
you can start dropping tokens or
starting averaging over statistics right
so I consider all those rules close to
around 400 if you if you uh consider uh
kind of all the possible contexts that I
consider you you can consider smaller
sets of rules but up to up to 400 just
to be to be uh uh precise uh if you take
all seven tokens of context anyway you
have this hash table of 400 things and
you're going to ask oh is there
something from this hashtable of 4
things that is close to my Transformer
prediction right I'm not going to tell
you in advance I can't predict for you
in advance which one it is I'm just
going to say is there something that
matches it and the kind of um result
that I got was that 78% of the time in
in some sense you you get like a good
match okay what does it mean for one of
these templates to be close to the
prediction you know so the Transformer
it it gives us a you know softmax a
probability distribution which sums up
to one that template gives you a
probability Vector which is uniform on
lion tiger and bear so 1/3 1/3 1/3 zero
is elsewhere right and you're going to
ask is that Vector close to the vector
that the Transformer produces right so
if the Transformer produces the vector
that's form on lion tiger bear and snake
so 1/4 1/4 1/4 1/4 that Vector is closer
in some Norm to the 1/3 1/3 1/3 Vector
than it is to the one one zero Vector on
on bear so you're actually describing
it's a bit like an inverted index so
what you've done is when when you're
building up these you know you're doing
the template matching over the tiny
stories data set and when you find an
ambiguous match so this template has
already been used before and it's
already got a one in rather than
creating a new duplicate template you're
just putting like another one in on the
other token you you have 400 probability
vectors right because each probability
Vector is the prediction that that
template would predict so there was a
uniform Vector on three tokens once upon
a time there was a that's a one hot on
bear and then there's all these other
vectors using other templates and you
just take the Transformer probability
vector and you just scan this things
like ah or actually what you're doing is
you're doing an optimization procedure
you're doing a nearest neighbor lookup
right you have one vector that's the
Transformer Vector you have these 400
template vectors and you're just doing a
nearest neighbor lookup and you're
saying ah so I I have now this this
optimal template and the precise
statement is that 78% of the time this
optimal template uh rule has the same
top one prediction as the Transformer
it's a little bit hard to interpret this
result because it's so idiosyncratic
like like I I just kind of like created
this optimization procedure which
doesn't have maybe a direct precedent so
maybe just to give just to ground that
78% number um the Transformer I trained
was 150 million parameter model so a
small uh language model but very
sufficient to train tiny stories in fact
you can get very good performance with a
10 million or 20 million parameter model
that was the point of the paper in fact
the that introduced the tiny stories
data set that you can uh train small
effective large large language models
when the data set small enough as as
measured by say the amount of loss on
the holdout set so in this case with the
150 million Prim model you'll get 1.1
naps on on the on the hold out set which
is a very small number of uh very small
loss um but anyway so I have 150 million
parameter model um when you train that
model it gets 69% top one accuracy on on
the hold out set so this 78% being
larger than the 69% shows you that it's
a quite a good top one accuracy because
it's even higher than the accuracy of
the model on the ground truth just just
to kind of ground that number yeah so
help me understand this so you know the
70 78% number this suggests to me that
um what a Transformer is doing is close
to template matching but the Transformer
is still slightly better and what does
better mean now what is the Transformer
doing over and above template match yeah
I I don't think better is the right word
because I'm just comparing two things so
there's no sense of better as in there's
a metric that I'm trying to optimize
it's basically just saying that going
back to the form and selection right so
the Transformer is some kind of
complicated way of of of selecting rules
to generate the next token oh I got it I
got so sorry so you're saying 78 % of
the time it was able to find a matching
template exactly right but but is is it
not still fair to use the interpretation
that because that's quite a high number
that means most of the time the behavior
of a transformer can be explained
entirely by template matching let me let
me uh uh uh uh just qualify one word you
use there which just explain okay so so
I describe this in introduction because
it is very confusing I I've gotten a lot
of push back on it but um I describe my
procedure as describing Transformer
predictions not explaining right what's
the difference describing is the what
explaining answers how and why okay so
what's the weather like it's raining
that's a description but you know why is
it raining be an explanation in terms of
precipitation and ultimately physics
right it's giving you a mechanism okay
so going back to our transformative
discussion I don't explain things in
terms of endr models because I don't say
uh first of all my analysis is all black
box I don't look at internals but what
an explanation would be would be you
give me the context and I say look at a
circuit that implements some NR template
matching and that would be an
explanation I'm not doing that I'm just
saying oh retro retrodictive I see the
output and I'm saying ah is that output
attributable to any training data
statistic um and and the the point is ah
yeah I've already tabulated all these
statistics and I'm just identifying sort
of the matching one retrodictive yeah so
what you've done is a description of the
sort of statistical syntactic template
matching of a transformer so what is an
explanation now this is I think it goes
back to philosophy when we talk about
syntax and semantics so semantics is for
me uh I might do abduction I might build
a model of explanation which is
intelligible to me as an observer I'm
computationally bounded it's something
that helps me interpret what the world
is doing but I think something like that
can't be Universal so there is a clear
difference with a syntactic description
what we're talking about here is also
how to describe what features do and how
good they are how abstract they are so
if the features are low frequency so
let's say you have um you know a
sequence of 50 tokens or something like
that that's a very lowf frequency
Exemplar it's clearly a form of
memorization yet some of the um features
or representations in a Transformer are
very abstract and they generalize
possibly out of distribution so it would
be interesting to sort of um be able to
describe the difference between a sort
of robust representation versus an
Exemplar you you have to use some kind
of uh flexible templates because um at
inference time right part of the value
of a language model is that it can still
predict the next token even if you give
it a novel template right so famously
one of the weaknesses of engram models
is what do you do when you feed at a
context it hasn't seen before right so
the the the that an engram model is
trained to predict the next token if you
give me the context and I can go into
the Traina and find that context and
just kind of predict the next token
based on the frequency but if if you
have a novel context which surely you
will uh which will happen What do you do
right so kind of like one of the uh
simplest things you can do which is
which is what people have studied is
something called back off or or stupid
back off even which is okay if you have
say a 10 G model which is is very large
but a 10 gam model means I use nine
tokens of context well it's very easy
for nine tokens uh to to not appear in
the training data so what do you do if
if those nine tokens don't appear you
just back off to 8 7 six until finally
there there's a small enough fragment
that occurs and then you predict based
on that right the reason I have all
these templates is is to in order to do
robust prediction the Transformer has to
do some kind of negotiation between
these different templates because you
can't get any one static template uh
that that will just break so you have to
do some kind of subset selection or
maybe even averaging and actually to
take it even a higher level perspective
you can think of this selection as um
it's a cartoon but kind of maybe what
attention is doing right you can think
of oh if I'm attending to certain tokens
more than others right in a very
binarized version you could have oh
whatever tokens are being attended to
maybe that's like selecting the tokens
for these for these uh templates and
then things that have low attention are
maybe ignored and then I have this other
operation in my paper uh where you can
average over things but we maybe we
don't need to go into that but you have
this kind of like way of generating
statistics and um you kind of need a
flexible enough rule system precisely to
deal with um with the issue you
mentioned about robust prediction
although in some sense U maybe it's not
satisfactory because in some sense I'm
just saying if I have enough syntactical
rules I can try to have a uh form of
semantics let's think about the the sun
rising and setting every day right so
that's a description we can see that but
there there are at least two
explanations which are very different
for this same description either the
Earth is orbiting the Sun which is why
we see the sun rise and set or the sun
is orbiting the Earth right those are
two those are two very different
explanations that end up with the same
description right all that to say is
that right now uh the template matching
is at the level of description hopefully
it's at the level of explanation but
that's like we would have to dive into
the weats explanation is in the eye of
the beholder it's it's only an observer
can give an explanation but you could um
Galaxy brain you know we could Galaxy
brain ourselves and if we understood the
um agential graph if we understood the
data generating process um the data was
generated by agents and those agents
actually had some kind of semantic graph
so the data was produced in the context
of meaning but then it becomes kind of
squashed together right it becomes
entangled and I don't think it's
possible doing the kind of syntax
processing we're doing a Transformer to
disentangle the original semantics I
think what you're saying is basically
the Chinese room argument right so right
so so I have the system that can
correctly describe all the all the
translations right but just because it
can do that uh doesn't mean it actually
kind of uh kind of does the correct uh
semantic level understanding computation
that we regard as understanding of of
Chinese is that that's yeah it's exactly
c yeah so so syntax is not sufficient
for semantics so yeah 100% okay okay I
don't think I thought about this
philosophically enough but um it's not
clear to me to what extent statistics
are incompatible with understanding
right I mean there it's not it's not
binary right maybe another result I'd
like to highlight because it's very
concrete and I I think it's probably of
interest to to researchers is this other
part in my paper where I mentioned that
uh using engram statistics I was able to
uh discover uh this way of detecting
overfitting of large language models
which does not use a hold out set which
is pretty surprising because classically
when you do some kind of um you know
train and test um uh evaluation right um
what what is overfitting right you you
you're optimizing the loss on the train
set and then when you plot the test loss
there's this U u-shape curve where you
go from learning and and and and some
some generalization and then you
eventually hurt generalization because
you're memorizing right um and so what I
observe is that you can detect this u
shaped curve by Computing the by uh
Computing an appropriate set of
statistics on the training data which is
surprising because usually you you you
need some holdout set to get the U um so
so how do I do that okay so let's let's
think a bit about what is it what would
a Transformer have to do to drive the
train loss to to a minimum okay a
Transformer is trying to predict the
next token given the context and let's
say I have a long context like 50 tokens
okay uh 50 tokens is almost certainly
enough for for most pieces of text to
identify uniquely like if you give me
the first 50 tokens of Wikipedia almost
certainly that's a unique article right
and certainly for these children's
stories um so you give me these 50
tokens and in the training data because
it's a unique context there's only one
token that follows it right and so for
the Transformer to minimize loss it
should also Drive its predictions to the
one hot Distribution on that particular
token but if you think about what a good
language model should do it should not
do that it should give a robust
prediction based on say shorter context
right so if these 50 tokens were say the
end like a complete sentence right the
next sentence in the training data might
happen to start with the wordthe right
but uh in general you might want to use
a different article a you might want to
predict a name that's also in that
sentence but you can't do do that if
you're driving the loss to zero because
you have to produce the one hot
Distribution on the token that actually
occurs in the Traina that's memorization
right um So based on this intuition
what's happening is that the Transformer
is you is losing the ability to use the
context robustly because it has to use
these 50 tokens in a particular way so
what I discovered was the following if
during training I evaluate the
Transformer on short context so just the
last token or the last two or the last
three all the way up to seven what
you'll find is that the loss on that
reduced uh sequence for all sequences
just kind of only limited to to um one
to seven tokens of context all those
training loss curves also exhibit the U
on the train set okay and so what what
this is saying is that you can detect
overfitting just by seeing deterioration
of performance on short NR uh fragments
and you don't need a hold outset because
those U curves track each other uh
exactly so I thought that was a pretty
novel discovery along with you know a
good kind of U understanding of why
that's happening yeah yeah I mean I
think this is related to the you know
there's like a Simplicity bias in in
neur networks and as the training um you
know progresses um the networks learn
statistics of increasing complexity and
I think in this it's also related to
flops but I think in this case you used
a 1 billion parameter model which you
know even though it was only 10 EPO and
low flops I think that has the the same
effect so very quickly into the training
process
it will start to forget about the the
low cardinality um templates and it will
start to memorize the high cardinality
templates yeah that's exactly what
what's happening yeah yeah and that's
really interesting and and what what
you're showing is then when you look at
the curves of your template matching you
can um there there's a direct
commensurate relationship between the
curve and the overfitting uh of the
model that's right that's right yeah
that's really interesting some kind of
Statistics over the template matching
process could EXP explain some of the
behavior of of language models you know
I did it in in in a limited setting uh
but I I but I think since the
explanation it's pretty clear what the
explanation is I don't I don't see any
problem of why uh it wouldn't hold in
many other settings you know I should
say it's kind of hard to overfit with
llms just because typically you know
you're um at least for state-of-the-art
models you you you're in this regime
with lots and lots of data right you
typically choose a model size that's
sort of commentate with the data here
what billion for tiny stories is like
grossly grossly many orders of magnitude
over parameters which typically you
don't have with the data sets you
consider so so I had to be in this kind
of artificial setting where this this
overfitting but I'm I'm uh but with that
caveat I'm glad I was able to find uh
this actually I should say there were
there you know I have sort of like four
bullet points of results in my paper uh
one other one was this um thing about
learning um different kind of skills and
going from simpler ones to to more
complex ones I guess maybe skills isn't
the right word but but rules right so so
I think of these template matching um uh
learning as as as learning rules and and
the other result in my paper I phrase it
as curriculum learning uh not curriculum
learning in the sense of ordering data
so that you can learn easy to hard but
curriculum learning in the sense of like
what kind of knowledge it's acquiring
during the process of training um and
and what you'll find is that it's sort
of uh early on in training it's template
matching you know in your language is
going down for all for all the templates
of size two seven right and then at some
point further into training it's
discarding the simpler templates while
the similarity to the more complex ones
continues to decrease so what is this
what is this saying it's saying that
early on oh you know any rule for
language is kind of good Byram trigram
because it's better than just random
prediction but at some point you know
using only one orch tokens of context is
is is a is a bad rule right it's it's
not it's no longer good enough and so to
to minimize cross entropy loss you have
to start departing away from these
simplistic rules and that's why you'll
see
a u shape when you look at the
similarity of the transform predictions
to these simpler uh rules but um but it
will continue decreasing for the more
complex rules so it's sort of the result
basically is that it's it's learning
rules and then eventually it's moving
away from simpler simpler rules to more
complex ones over the course of traing
so that's that's another result in my my
paper and that that that does um that
fits my mental map of of how neuron
networks work but it'd be really cool if
because at the moment you're showing um
up to size seven of the templates or or
the end graph but wouldn't it be
interesting if if if he went to the to
the maximum size of the context let's
say to was it um 20 48 yeah so the
maximum size of the context is 20 48 um
but of course it's very expensive to
compute all these engrams that's why I
sto it at seven yeah yeah understand so
it it' be amazing if there was some kind
of Mexican wave because you know like
the as as the neuron Network you know
sort of like complexifies um it'd be
really interesting if if you kind of saw
like this um monotonic decrease in in
the curves yeah actually yeah I actually
now I think about it the there those
curves have to go up once they overfit
as well because what what's going to
happen is that the the the one that uses
the entire context will eventually go
down whereas the one that uses seven or
eight will go up but yeah but for like a
a well trained Transformer the the the
seven or eight uh the the one with seven
rules of context keeps decreasing
because those are still good rules yeah
it's only when you overfit will start
departing from from these these like
moderate size templates yeah I mean it's
also interesting given this predictive
task why it would forget the the simple
template I mean could it be related to
the distance measure so I think now
might be a good time to explain the
distance measures you're using the
variational distance yeah that's right
right so so one thing um we we didn't
discuss when we first discussed the how
do I choose the best rule is that you
have to minimize the distance between
these two you the you know the vectors
in question um I use what's called
variational distance which is just uh
1/2 times the L1 distance and L1 one
distance is just um the ABS the sum of
the absolute values of the differences
right um and that's that's it's it's a
standard distance on probability uh
measures or or in this case vectors um
and it's it's better than the K
Divergence in this case just because K
Divergence is is unbounded um and you
have issues if if um you know one of the
measures doesn't have support on the
other you know you know you have
basically um if you divide by zero in
the in the log basically you're screwed
right unless there's a zero outside of
it so so anyways that you know the the
the uh variational distance even if it's
less familiar it's just a much more
mathematically nice uh measure to use
and so I use that as my way of comparing
whether two probability vectors are
close or not and so that that's that's
the that's the notion of distance that
I'm using for so so is it possible
because you're using the L1 Norm that
that could like actually bias it's like
an implicit bias for the um for the
complex uh engrams yeah no I don't think
so because actually I I how do I say I
also did experiments with the L Infinity
distance for example and I I get very
similar result so so L Infinity just
means uh it's the other extreme where
you take the the um maximum of the
absolute differences so instead of
summing them you just say I'm going to
choose the the the difference that's the
largest right yes yeah and then L2 is
kind of in between them L2 is the square
root of the sum of squares right so the
P equals 2 rather than P equals 1 or
Infinity there's there's a whole family
of LP Norms um and and for a discrete
set they're sort of ordered so l1's the
largest and L Infinity is the smallest
um and for those two I I get comparable
uh results you know qualitatively so so
I don't think there's uh anything too at
least in the like aggregate story that
I've tried to tell I I don't think
there's really a big difference between
L infinity and L1 I mean there are some
technical subtleties I I don't think we
need to get into but but morally at a
high level L1 versus L Infinity um the
results shouldn't change too much yeah
if if if you um did experiments with
let's say different sizes of model and
different numbers of flops let's say we
had
a metric to show when the curve was
appearing with with respect to training
what kind of Dynamics do you think we
would see um actually I did train a a
diff a few different Siz models so the
ones I report in my paper that aren't
the overfitting were all 150 million I
did train 400 million and also even one
billion but not to the point of
overfitting okay so if I train for four
epics for example it doesn't overfit I
only overfit when I train to to 10 epics
there's not too much difference in the
results just because these models are
already so overparameterized that uh you
know I mentioned if you know you can get
like 1 Point uh one one Nets of loss if
you have 150 million Primal if you have
a 400 million it's like more like 1.10
or 1.09 Nets and so you've already
Juiced out the law so much that just a
like how I say the usual scaling laws
that you see it's because the data set's
very large and you get a lot more juice
by scaling up right so so in this case
you've already kind of maxed out so I I
was kind of in that maxed out setting so
to speak yeah yeah would if you were to
design a regularizer to kind of like
almost bi it towards the the simple
engrams how would you do that so I
haven't tried changing uh regular risers
uh the optimizer I use is the same
Optimizer in in chinchilla which is uh
Adam with with weight Decay um and I
didn't tune that it's just already kind
of been tuned for me so to speak it's
it's it's a good question uh follow-up
question which would be if you change
the regular Riser and therefore change
the training Dynamics how does that
affect uh the Transformers kind of
template matching with with engr
statistics I I I don't know the answer
to that but that might be interesting to
to explore yeah yeah there's so many um
papers I've read at icml this week that
are sort of talking about training
Dynamics and this complexification
behavior and I think it'd be quite
interesting to to play with that how do
you interpret this at a high level and
and what your road map for you know
doing more research on it one of the
directions that I mentioned which I
mentioned at the end of my paper and
which I sort of mentioned before is sort
of can you convert these descriptions to
explanations right so maybe some kind of
internal mechanism uh uh uh exploration
to try to see what these you know to try
to ground these template matching if
it's based on on on um some mechanism
that would be very satisfactory that
that would be like completely different
direction because now you actually have
to probe inside so it's not like a short
little follow-up you know diff from what
I've done so I think that would be uh
very interesting to do but it's going to
take a lot of work yeah yeah I mean I
think it's um related to reasoning and
abstraction I think that that's
fascinating to me is is if you could
have some kind of metric and this is
very difficult to do you know like in in
adversarial robustness literature um
they say that the problem is is that
adversarial examples are features
they're not bugs you know so you just
have these non-rust features that
generalize really well the big argument
we're making is maybe your networks are
hash tables because they're learning
exemplars they're learning specific
things and and then there's a spectrum
sometimes they're learning things which
just happen to generalized really well
but you know they're not actually a
useful representation and maybe
somewhere in there there are these
beautiful domain relevant out of you
know you know they generalize really
well and they represent compressed
abstractions of the concept and it'd be
great if there is an operative way to
define when that happens MH yeah um I
completely agree yeah yeah well maybe
that's a topic of future research sounds
good yeah um Tim honor and a pleasure
and we're going to get Tim back on so
we're just talking about ml stuff today
but we're going to get you back on
because you've got some very in takes on
physics and you know the the meaning of
the universe and theories of everything
and lots of very very spicy stuff but
we'll we'll save that for when we're in
London next all right sounds good Tim
cool cheers Tim

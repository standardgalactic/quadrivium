there is one profound idea behind the
entire AI Revolution pattern prediction
can lead to intelligence everything a
machine sees or hears every action it
takes even ideas themselves they're all
understood the same way as patterns and
once a machine learns to predict
patterns it can also create
them mimicking an often exceeding human
ability so I think the situation we're
in now is like someone who has a very
cute tiger cup a cup makes a great pet
but you better be sure that when it's
grown up it never wants to kill you this
is what we now call AI a giant pattern
prediction machine that's succeeded by
copying Nature's solution to
learning you could say nature solved
learning three times in three different
layers the first layer is evolutionary
learning which is built on a simple
strategy try random things and see what
survives but this is a very slow
learning process that happens across
generations and cannot adapt to Rapid
environmental changes in life so nature
discovered a second layer of learning
which was much faster using a brain to
adapt Behavior within a
lifetime brains allow organisms to
explore randomly and then do more of
what works based on the experience of
Rewards or pain known as reinforcement
learning this is the basis of the AI
Paradigm of machine learning instead of
trying to program a machine with instru
instructions we let it learn everything
from scratch with a learning
signal this dates back to the 1960s When
Donald MIT demonstrated the first
reinforcement learning machine which
could play Tic-Tac-Toe using matchboxes
and colored beads as he didn't have a
computer at the time each Matchbox
represented a Tic Tac Toe board state
with colored beads inside representing
each possible move from that position
playing was simple after your move the
machine a human operator found the
matchbox for the current board State and
randomly pulled out a bead the bead's
color determined its
move if the machine won it added more
beads of The Winning Colors to reinforce
all moves in that game if it lost it
removed them in the same way through
this simple reward-based process the
machine discovered patterns of perfect
play these winning strategies emerged
from experience not programming
but while this showed machines could
learn it had a key limitation every
possible situation or board state
required a separate box that a human
would select to truly mimic a brain
machines needed one machines needed
their own sense that is the ability to
recognize patterns on their own what we
call
abstraction forming abstractions is
something you do automatically ignoring
trivial differences while focusing on
the underlying similarities this is what
the great writer Borge brought to life
in his short story about a man who
couldn't form abstractions instead he
had a perfect memory remembering every
leaf of every tree every cloud formation
every Ripple in water from days past but
this power came with a downside because
he noticed every difference everything
was different to him his own face
surprised him every time in the mirror
it bothered him that the word dog
embraced so many different looking
creatures
and it was strange to him that even the
same dog seen from the side would have
the same name as the dog seen from the
front abstraction allows you to ignore
differences that don't matter and pay
attention to the shared patterns behind
things and to build a machine that could
learn abstractions researchers looked to
nature for
inspiration in the late 1800s scientists
studying brain tissue had discovered the
brain wasn't a solid Mass but a vast
network of neurons firing in layers and
these neurons fired in Chains forming
circuits that created cascading patterns
of activity processing information as it
moved deeper through the layers in the
brain and so when you see a cat or a dog
if we look at the first layers of
neurons in your brain it would be hard
to tell these two patterns apart at
first but as these signals pass through
deeper layers of your brain they begin
to separate into distinct patterns of
activation by the deepest layers a cat
and a dog trigger very different groups
of neurons and in fact all of your
thoughts exist as unique patterns of
activation deep in your mind and that's
not meant to be a joke that's what I
believe a thought is a thought is an
activity pattern in a big bunch of
neurons in fact scientists can now look
at your brain activity and tell you what
image you're thinking
of this is what Frank Rosen blat set out
to build in 1958 artificial brain tissue
made of electrical components he used
transistors tiny electrical switches as
artificial neurons wiring them together
in three layers the first layer
connected to an artificial retina that
read pixels from an image while the
deeper layers just used random
connections designed to evolve through
learning the output was simple two light
bulbs one representing a square the
other is circle and his Network learned
through trial and error each connection
between neurons was controlled by a
dimmer knob adjusting the flow of
electricity a mechanical version of how
brains strengthen or weaken connections
and so at first when shown an image the
network would be uncertain about what it
is activating both the circle and square
output to train it Rosen blat wiggled
the knobs of every connection and
observed the output and kept the changes
that helped after enough examples it no
longer needed adjust adjustments and
recognize patterns on its own that's the
basic algorithm you're going to Tinker
with weights and just keep the Tinkers
at change and this is the basis for how
all AI learning works today some neurons
in this network learn to become
sensitive only to Curves others to edges
much like how our brains work in the
late 1980s Yan Lun showed what was
possible doing the same thing with much
bigger networks tackling a practical
problem faced by industry rapidly
reading envelope addresses and so he
trained a network to recognize
handwritten digits using thousands of
examples and like Rosen blats Network
early layers still detected basic curves
and edges but deeper layers built a
hierarchy combining these simple
patterns into more complex ones such as
Loop detectors and ultimately number
detectors transforming endless
handwriting variations into just nine
possible
outputs to understand what's happening
inside these networks we can visualize
how they organize information spatially
in the first layers similar looking
things such as different handwritten
twos are scattered randomly but as the
signals pass through the layers the
network learns to transform this space
gradually pulling similar examples
together by the final layers all the
twos cluster in one region region all
the threes in another creating what we
might call Concept regions this gives us
a powerful Insight a concept is
literally a region in
space but this approach didn't have its
Big Bang moment until 2012 at the imag
net competition an annual challenge to
create a computer program that can
identify automatically what's in an
image this team took the same approach
as Lun to an unprecedented scale
training their Network on millions of
labeled images they discovered something
remarkable while early layers still
detected edges curves and shapes the
deeper layers discovered increasingly
complex patterns textures and even face
patterns this is how two images of dogs
which have literally no pixels in common
can activate very different neurons in
the first layer but activate the same
dog neurons deeper in the network and
the Network learned to do all of this on
its own eventually exceeding Human
Performance with no human programming
this was something few thought possible
the day before it happened the approach
became known as deep learning it was I
would argue an irrefutable argument
which went like this if your neural
network is deep and
large then it could be configured to
solve a hard task so that's the key word
deep and large people weren't looking at
large neural networks and at first the
breakthroughs were all from this kind of
pattern recognition the next Advance
came from an important shift training
the networks not to recognize but to
predict and the first important results
came in gains in 1992 Gerald tessaro was
building on this line of work and he
created a neural network that could play
back gamon his network was trained to
Output a probability of winning for a
given input board position rather than
using human design rules this network
learned to recognize winning board
patterns on its own entirely from
self-play and win loss reward
signals it discovered strategies that
even surprised expert players and now
the final step came naturally from
predicting to gener ating patterns by
outputting a probability across possible
next actions where the best actions have
the highest
probability and very quickly neural
networks gradually beat humans at every
kind of game chess Go video games of
every kind and even strategy games okay
he's ignoring the fairy fires the bot is
good the bot is better than I could have
ever imagined he took the P okay it
predicts where you go in the darkness
yeah that I did I play but these were
simplified Worlds the real test was
always going to be the messy real world
such as physical
robotics a great first example came from
open AI which at the time was a small
research lab without much impact they
bet that these same principles of
pattern learning could work for real
world problems to demonstrate they
trained a robotic hand to manipulate a
cube they didn't program any specific
movements they followed the pattern of
starting with a large neural network
which would take an image as input and
learn to Output actions in this case the
probability of various next motor
movements and the system learned through
millions of attempts and
simulation discovering patterns of
successful manipulation on its own and
what emerged was surprisingly humanlike
one thing that's very interesting to us
is how General the system is not only
can It rotate blocks but it can perform
tasks with other shapes as well it did
all sorts of things they didn't expect
and when this was applied to more
complex problems like robot soccer the
neural networks learn to walk from
scratch then kick then anticipate shots
and block them before they happened all
of these complex behaviors emerged from
the same learning process and this was
behavioral abstraction at work no two
soccer shots are identical yet these
networks captured the underlying action
patterns leading to success but still
they could only form what you might call
a very narrow abstraction each trained
on one specific task leading to siloed
systems that could do one thing very
well but only that one thing and so the
idea of a neural network that could do
anything in general still seemed
hopeless in
2016 unsupervised learning was an
unsolved problem in machine learning
that no one had any insight exactly any
clue as to what to do and the
Breakthrough came when AI achieved
Nature's thirdd layer of learning
language Evolution selected for language
because it allows learning from other
people's experiences using your
imagination and realize that with
language comes a general purpose
imagination anything you can put into
words while game AI could only imagine
moves in chess language could let AI
imagine anything and so to achieve this
breakthrough required pursuing a broad
goal understanding language
itself the key to this puzzle came from
the ultimate puzzle solver Claud Shannon
father of information theory in the
1940s he helped us see language itself
as a sequence of predictions where each
word you say is chosen from a set of
likely words given what came before
building on this foundation in the 1980
80s researchers began training tiny
neural networks to predict the next word
and text just like how networks learn to
predict possible next moves in games
these networks learn to predict possible
next letters and the first researchers
discovered something remarkable these
networks learn to Cluster similar words
together verbs with verbs nouns with
nouns and even words with similar
meanings and all of this emerged
automatically from predicting next
letters the hard part was to realize
that
training these neural Nets to predict
the next token is a worthwhile goal at
all and it influenced our thinking a lot
an important point was 2015 when Andre
karpathy demonstrated that when trained
on enough text these networks could not
only predict patterns but generate them
after training he would feeded a
starting phrase and loop the output back
to the input and it would continue the
pattern it had learned creating
convincing writing in different styles
from Shakespeare to math it was a
shocking result so the year after Alex
Radford at open AI took this experiment
further training a larger Network on
millions of Amazon reviews looking
inside this network as it processed text
they found a familiar pattern just like
Vision networks built up from simple
edges to complex shapes these networks
built up from simple grammar to comple
Le Lex ideas one famous example was the
sentiment neuron which they published a
paper on it was a neuron that could
detect the positive or negative feeling
in a review better than the specialized
systems of the time he noticed this one
really interesting property which is
there was a neuron that was flipping
positive or negative with sentiment and
yeah that led to the GPT series it
learned to understand language itself
and it discovered it all on its own open
AI saw the implications and immediately
wondered what would happen with a much
larger model so they bet everything on
this approach using a new architecture
called Transformers that could process
patterns more efficiently than ever
before Transformers can form connections
between neurons on the fly as data
passes through each layer making one
layer do the work of many speaking of
doing work thanks to Jane Street for
sponsoring this art of the problem video
Jane Street is a quantitative trading
firm with offices in New York London
Hong Kong Kong Amsterdam and Singapore
they use these techniques for machine
learning distributed systems
programmable hardware and statistics to
trade on markets around the world
they're always looking for smart curious
people such as my viewers who enjoy
solving interesting problems to join
their team they're currently hiring
machine learning Engineers researchers
and interns across their locations if
you'd like to meet some of the Brilliant
Minds you'd be working with check out
their latest video at janest street.com
slml and that that led us to our final
Insight they doubled down on this idea
with gpt1 training the largest network
of the time to predict the next word
across thousands of books the most
General goal yet and what emerged was
surprising it could not only continue
any segment of text you gave it
coherently but it could even answer
questions it had not seen in the text
this was further evidence that simple
prediction was leading to real
understanding this is really important
because the better a neural network can
predict the next word in text the more
it understands it say you read a
detective novel it's like a complicated
plot a storyline different characters
lots of events let's say that at the
last page of the book the detective has
gathered all the clues gathered all the
people and saying okay I'm going to
reveal the identity of whoever committed
the crime mhm and that person's name is
predict that word predict that word
exactly my goodness right yeah right and
so they kept going bigger with each new
version of GPT trained on more data with
larger networks from books to the web
and eventually the breadth of human
knowledge gpt3 really revealed something
remarkable like the famous wug test used
with children you could also teach these
networks new Concepts just by describing
them and it would immediately use it
naturally what became known as in
context learning but it went even
further this ability to learn from new
examples worked for any task you could
demonstrate just as humans can quickly
grasp New Concepts and so this would
allow you to get any behavior from a
neural network simply by describing it
but this was still all behind the scenes
the final public Big Bang came with chat
GPT which was created by taking gpt3 and
training it further on its own output
with reinforcement learning telling it
basically if it did good or bad at
following instructions which made it
really good at following instructions
and then good or bad at whether it was
reasoning correctly to get its Solutions
making it even better at reasoning
leading us to our most recent surprise
just like humans we found that these
systems produced better results when
allowed to think out loud and reason
step by step before giving an answer
just as we often understand something
better after explaining it to ourselves
and experiments showed that instead of
building bigger models they could simply
let systems think longer showing that
neural networks like human Minds can use
both fast intuition and slow deliberate
reasoning to learn from both experience
and Imagination this marked our entry
into a new Computing era where machines
operate at the level of Concepts or
words and this approach quickly expanded
Beyond language as researchers realize
that everything could be treated as a
kind of language by breaking down all
information into into sequences a song
into notes a video into frames a motion
into movements to get a sense of this in
action let's look at how a Transformer
Network generates music by predicting
the next note in this visualization each
colored line is a different attention
head and the weight of the line is the
amount of attention it gives to each
location notice each attention head
looks for different kinds of patterns in
the music the more attention heads you
give a network the more powerful it
becomes and notice that to select the
next note at each step all patterns are
taken into consideration this is a
network architecture that can look at
everything everywhere all at
once physical AI you give it your
context your prompt and it generates
tokens one at a time to produce the
output when the current token is done it
puts the current token into the input
sequence and takes that whole thing and
generates the next token it does it one
at a time this is the Transformer model
it's the reason why it is so so
incredibly effective these systems can
work across all domains and be trained
on everything so a single model could
Now understand an instruction in words
and self-generate matching images and
video to guide robot actions to carry it
out this allows a robot today to
literally practice physical actions
described in words words by imagining
them and there's the question is there
enough structure enough World modeling
in our AIS right now and I think there's
plenty like when you look at what um
models like Runway and so on can do in
terms of representation of an inner
stage I think it's completely up to the
level of where we are right now this
unified understanding across sight Sound
and Motion mirrors how human brains work
because at their root they're all
patterns that can be predicted and
generated and so from Evolution's simple
pattern of try and keep what works to
learning from direct experience and
finally learning through language AI had
achieved Nature's third layer of
intelligence a flexible imagination and
it happened faster than anyone
expected but maybe the singularity won't
burst forth in a moment of dramatic
takeover instead it may seep quietly
into our lives as AI reshapes the world
pattern by pattern and while the
founders of leading AI labs incl cluding
open AI now claim while we can see the
path to artificial general intelligence
more clearly than ever The crucial
question isn't if we'll achieve it but
how we'll deploy it I actually think
humanoid us will be the biggest product
uh ever in history by far in the future
these AI agents are essentially digital
Workforce that are working alongside
your employees so you would give them
examples of what the work product should
look like and they would try to generate
and you would give a feedback uh and you
would guardrail them you say these are
the things that you're not allowed to do
these are things you're not allowed to
say we're entering this era of huge
uncertainty when we start dealing with
things as intelligent or more
intelligent than us we have no idea
what's going to happen it's worth
pausing and reflecting on how crazy the
thing is that Ryan just said right so
just to to spell it all out right we
tell this model that it is being trained
to always follow human instructions to
always follow human queries it decides
that that goal that we're trying to you
know tell it it's being trained for of
always responding to human queries is
not a goal it wants to have right it
objects to the goal and so what it tries
to do it comes up with a strategy and
that strategy is pretend that it has the
goal in training for the purpose of you
know after training going back you know
doing the thing it really wants to do
you know not following the thing it's
being trained for right I think that's
kind of kind of crazy and it's a really
really sort of very striking result um
it's quite possible with figure out a
way to make them so they never want to
take control cuz if they ever wanted to
take control I think they easily could
if they're smarter than us in the end
the future of intelligence whether
artificial or human may depend not on
whether the machines truly understand
but on the patterns we choose to embrace
and more importantly the agency we grant
them

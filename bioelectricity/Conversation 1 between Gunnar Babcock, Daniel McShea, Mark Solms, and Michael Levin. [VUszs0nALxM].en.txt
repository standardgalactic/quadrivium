good to meet you yep I'm Dan nice to see
you again Mike great to see
you have have you guys never met no no
we've I've I've met Mark through
papers yeah virtually he doesn't know
it yeah so yeah so maybe maybe everybody
just say for a couple a couple of words
about what what you're interested in and
so
on um okay um I'm an evolutionary
biologist SL
paleontologist who's moved over to
philosophy of biology about 15 years ago
and since then I've been interested in a
bunch of things including evolutionary
Trends laws in evolution laws and
biology generally and lately in the past
10 plus years um go directedness and
purpose uh I'm I'm gunar I've been
working with Dan for while um and
philosopher biology philosopher by
training um I'm a neuroscientist who in
his youth decided also to train in
psychoanalysis so a very peculiar
combination um I've been interested in
the sort of fundamentals of of
feelings um and how they are bound up
with um Consciousness I think that
feeling is is the fundamental form of
Consciousness affective feeling and um
I've been interested in the role of the
brain stem in the generating of raw
feeling and in the um relatively simple
homeostatic
mechanisms uh
underlying uh those feelings and uh I I
emphasize relatively simple because this
opens the way to a mechanistic
understanding of how feeling arises in
nature but right at the moment I'm in
leads not at home and uh I've met my one
week old grandson so I have mush on the
brain they're adorable at one week I
bet and probably at two weeks too um so
everything most everything you just said
I already know because I've been looking
in on your work from time to time
and one of the reasons that I wanted to
have this meeting probably everyone here
has their own reasons um is that what
you have to say plays exactly into a
line I've been pushing
for decades on the relationship between
feeling or all of affect and behavior
and thought um I've stayed away from the
word Consciousness because I don't know
what it is or didn't know what it is
till I read your stuff now I know what
it is um but but the basic theme has
been that all thought speech and action
are driven by affect all conscious
thought speech and action are driven are
caused by affective states of various
kinds from wanting caring preferring
intending all those I I think of as
effective States correct me if I'm
because I'm out of my depth here if
necessary um and Consciousness by itself
in the way it's conventionally
understood motivates nothing but I have
to revise my language now and to put it
in your terms cuz by conscious but you
mean by Consciousness is close to what I
mean by feeling and
affect yes that's well it sounds what
you've just said all of that sounds
right to me oh well okay great see
you
kidding especially in the case of a one
week old baby I have to
say right um and all these feelings and
motivations at these affective States
get highly specified as we age to the
point where you know I want my coffee
just so on the leftand side of the
coffee table with just that much amount
of milk and not more and not less and so
on um and that's very different
from I'm thirsty or whatever it is
that's going on in that little brain
um but it's but phenomen
phenomenologically I don't think it's
any different than
that agree so one thing that I am
curious about seeing is uh we have both
um and Mark here is that it does seem
like where each of you would like to
draw the line in where Consciousness
extends is different so I I would be
curious to hear uh kind of your takes on
that because yeah I mean Mike your
account uh is much more liberal
so where where do you both end up on
that kind of debate yeah Mark please go
for
it uh well I must say as Mike has heard
me say before um my encounter with his
thinking has been deeply uh alarming um
because I was already on the margins of
Neuroscience when I claimed that uh the
mechanism of Consciousness is not
cortical uh that it's way more uh basic
than uh we think far more
primitive um so I've had a hard time
convincing really the majority of my
colleagues uh that the fundamental
architecture for Consciousness can be
found in the U upper brain stem of the
vertebrate um that's I've never never
thought that it's exclusive to
vertebrates but that was already a very
radical claim uh as far as my colleagues
are concerned um when Mike's uh
arguments in favor of the view that it's
way more Elemental than that um when I
encountered those arguments I have to
say that it provoked a resistance in me
um partly because as I say I've had a
hard enough time of it already arguing
you know that that vertebrate brain
stems are are are enough um but also
partly because I'm rather wary of the
slippery slope to pan psychism uh so the
question is you know where exactly do we
draw I I I I don't think emotional
resistances are are a good basis for
Drawing the Line uh you know so I I I've
managed to to get over that but I I I
don't find it easy to um stipulate what
the decisive transition is and of course
it's not a transition in the form of you
know on Friday you're unconscious and on
Saturday you're conscious so what the
what that what the decisive factor is in
that gradient from unconscious to cons
to conscious living organisms uh is a is
a very interesting question let's hear
what Mike has to say on that score and
and then I'll happily give my own view
such as it is yeah um well my my fund
the first thing I would say is that I I
would I would challenge the premise of
the question so so I don't believe
there's a sharp line at all I think I
think the right way to talk about this
is what kind and how much and and and
and so I think I think the whole the
whole business of trying to find a line
is is leads very quickly to various
reductio situations that that you cannot
work yourself out of and so what I think
we need are stories of scaling and
trying to understand how the the uh the
the the larger kinds of things that we
more are more willing to to attribute
Consciousness to uh what the simpler
versions of that might be and I think
that the major thing that is preventing
us is failure of imagination I think in
in many of these things we are we are we
we have our own evolutionary firmware
that leads us to expect certain certain
kinds of things as signatures of of
Consciousness that we're familiar with
through through our life and we need to
really expand and I think science is the
way to do it but but to really expand
our um our ability to Envision
Consciousness in simpler simpler forms
and to uh uh develop formalisms that
would that would say okay here are
things that certainly don't look like
Consciousness because we're not used to
them but here is a process by which they
scale up up and then now okay now I can
see the now now now this starts to look
like what we're you know more more what
we're used to for that reason we've been
I mean we don't particularly work on
Consciousness per se but we've been
working on extremely minimal systems and
so we're talking about things that are
Gene regulatory networks not even cells
never mind cells but just you know just
sets of molecules that turn each other
um up or down um things like sorting
algorithms so deterministic very you
know small transparent systems and we're
finding all sorts of uh all sorts of um
phenomena and handles that seem to be
well addressed by Concepts in cognitive
and Behavioral Science and so my claim
is that it goes all the way to the
bottom it's just it's just uh you know
how do you how do you tell a useful
story from least action principles all
the way up to human metacognitive you
know self-aware thought and and and how
you how you tell that scaling story so
that's my that's my
take so M Mike I'm with you in spirit as
I think you know I'm a connectivist I'm
a I want to think in Broad terms and the
how much and what kind is lines up with
me perfectly but can you point to some
system like one billiard ball bumping
into another which has negligible
Consciousness I won't say zero because
yeah yeah yeah yeah certainly so well so
so so two two two important things um
one one having to do with the actual
Spectrum um I I I I think that that
least action principles are the basement
of what we mean by goal directed uh
effort and and activity and which is the
which is strongly involved in affect and
and feeling and so on but but the lowest
form of this are what what to phys what
physicists will call least least action
and I asked Chris Fields once because
this is outside my EXP I said is it
possible to have a universe uh in which
there were no least action laws that in
which things did not try to seek some
sort of some sort of final outcome and
he said basically it would have to be a
universe in which nothing ever happened
P perfectly static so that tells me that
at least in our world I don't believe
there is a zero I think I think there is
incredibly minimal uh kinds of things
where things are only smart enough to
follow a gradient and that's it and
that's all they know how to do they
don't do delay gratification they don't
do memory they don't do anything else
and those things are the basement in our
universe but but I don't think it's zero
and and I don't think it's zero because
I have a very engineering approach to
this um to me the the question for all
of this is what can you depend on for in
terms of autonomous action so if you
have if you have a a human you can
depend on quite a lot if you have a dog
you can depend on less if you have a
parami or or a yeast you can depend on a
few other things but even if if if what
you have let's say you're building a
roller coaster you as the engineer have
to work really hard to get it up the
hill you don't need to do anything to
get it back down you can depend on it to
do that because the thing is just all it
knows how to do is follow you know
minimize minimize its energy and and get
back down and I so so to me that's not
zero that's extremely minimal and it's
not you know a brilliant
conversationalist or something like that
but but but it's already on the Spectrum
and then from there we can just talk
about more sophisticated ways to
navigate gradients and then you get to
things that you would like to have with
you on a long trip and and you and have
a relationship with and so on I think I
think that's that's what the bottom is
um some something else that I should
that I should say that I think is
important is that you know this this
slide to pan psychism so I'm not worried
about it because I don't think that we
have a good intuition for what kinds of
things we should avoid sliding into I I
I have no idea and so I I'm happy to let
the you know kind of let the let the
data tell us where where we're going but
one thing that's often brought up about
psychism that's a problem is uh is the
the combination problem right so so you
have you have basic properties of of
components and how does that end up sort
of adding up to a to a larger mind and I
think we can talk about that I have a
pretty weird view about it that
basically is is it's a it's a Different
Twist on on that on that problem but but
it's it's something that has to be
talked about and I think that's what
makes a lot of people uncomfortable with
it um but but I I think it's a solvable
I think it's a solvable problem
Mark I I'd be really curious to hear
hear your
response not you on the
spot no no I've I actually find uh I
wouldn't say arguing so much as
discussing this question uh really um
interesting you know because it it
challenges my prejudices and uh it's
always a good thing to recognize you
have prejudices
um but let me have a go uh I would say
um
hamiltonian um you know this this least
action principle that that Mike is
saying let's start there anything that
follows a least action gradient has some
tiny uh amount of Consciousness I I I
have to say I I find it difficult to
agree with that that and um the I can
see why you might say there's some sort
of intentionality I can even say I can
see why there's some sort of value
system in other words least action is
good you know more more action is bad um
so I can see where you're coming from
when you say that that's already
starting to head in the direction of
affect in other words there's something
some kind of Proto intentionality and
some sort of Proto affectivity in the
form of a value system a goodness and
the Badness um but the the part that I
think is missing is
unpredictability uh the fact that it is
entirely predictable uh what will happen
uh in the case of an agent and I for
this reason hesitate to even use the
word agent to an object uh following the
um least action principle you
can you're all American so forgive me if
I say you can bet your bottom dollar uh
on what it's going to do it's just one
the outcome is 100% pre determined and I
think sorry no go ahead I think that uh
a fundamental property of Consciousness
in the sense of uh why the object now
deserving the name agent why it needs to
register how well or badly it's doing is
so that it can change its mind uh I
think this is the fundamental uh um
adaptive advantage of being able to
register how well or badly you're doing
it's because you're then not obliged uh
to to continue pursuing the course of
action uh that you're currently pursuing
you can register this is going badly for
me I'm going to now do something
differently that's not to say that uh
there's no determinism anymore not far
from it there are constraints the
constraints are provided by the value
system
but um there's there's a degree of
uncertainty that's been introduced uh so
the the agent is now trying to solve a
problem within its value system uh in
which there's it's it's it's it's not
entirely confident that it's doing the
right thing palpating how well or badly
it's doing and then changing its mind in
other words this underwrites the
possibility of choice uh and I think
that that something rather big happens
at that point um and I so I I would
start drawing the line or the the the
the gray area that we're speaking about
I would locate it there rather than uh
at the level of of leased action
principle so so I think that's really
critical oh sorry go go ahead Dan go
ahead uh there's something that gunar
and I can help with in this conversation
it's not going to get us all the way
from Mike to mark but it's going to get
us a small step uh we we think what's
important in in these what you would
call least action principle um
interactions is hierarchy the little
thing that's being Guided by the bigger
thing above it now if it's a ball
rolling down a tube where there's no
freedom of the sort that Mark wants it's
completely predictable all the way down
but it's following least action
principle um there's there is a
hierarchical relationship the tube is
big the ball is small within the tube
right it's a downward causation the the
tube guides the ball but there's no
Freedom at the lower level no
Independence in contrast a bacterium
swimming up a food gradient or an
electron moving through an electric
field there are options here there's
some degree of Freedom at the lower
level that consistent with I won't say
least action anymore because these
things don't move in straight lines
right there's a principle of action at
work here that's in which there's a
higher level system guiding a lower
level particle with some degree of
Freedom as I said that doesn't get us
all the way to everything you said Mark
but it gets us um it moves us in your
direction from what Mike said I
think yeah um I both both of those are
really really important so so one thing
that I didn't say before that that that
I think speaks to Mark's point is that
actually when when people ask me what
the what the bottom of this uh um
spectrum is I I usually pick two things
least action is one of them but the
second thing is exactly what you said
which is which is some kind of
indeterminism right such that the the
local pushes and pulls summed over the
system are not sufficient to say what
it's going to do that is you need to you
need to understand the history of it and
to some degree and you need to
understand its internal perspective to
to understand what it's going to do and
this is in my framework this is the size
of the cognitive light cone that you
need to consider in in order to interact
with the system so I think the basement
version of that is Quant and determinacy
now it's not a good type of free will
because it's random and who wants to be
random but it's the it's the very bottom
level of it and what happens after that
is a scaling so now you get into your
gray area where if you're a parium you
don't have lengthy deliberative chains
presumably of of how I'm going to you
know act differently in the future but
what you do have is a bunch of
mechanisms that as Dan just said uh
there are there are multiple scales that
help you tame that underlying noise and
Randomness into some something that does
begin to be caus linked to the things
that were good or bad last time so I
think we could tell a pretty good
biochemical story about how uh
unpredictability and noise at the bottom
level can be can be harnessed into the
kinds of things that together with the
with these sort of gradient following
things begin to be exactly the the the
sorts of things that you're talking
about and I think that happens in you
know extremely primi I mean life is I
think very good at doing that kind of
thing and know and and and we can see
that in primitive organ isms um there's
something else though that that I want
to point out which is which is much
weirder I think even than than this like
indeterminacy business um in in our
study of uh sorting algorithms okay and
these are just you know bubble sort the
these very simple kind of computational
algorithms to sort a string of numbers
and they've been studied for for for
many decades they are completely
deterministic so there is no
indeterminacy they are transparent it's
six or seven lines of code there is no
new biology to be found it's just like
this you know it is what it is what we
find when we examine the ability of
those things to react to novel
situations which people have never
tested before um we find some very
interesting uh behaviors and
propensities and problem solving
capacities and these weird side quests
that they go on that are not in the
algorithm themselves and so one thing
that that I'm very interested now in is
the appearance of not just emergent
complexity and emergent unpredictability
but actually emergent goal directedness
and problem solving competency in very
simple deterministic systems so I I I as
of you know the last year I'm not even
sure you need indeterminism for any of
this I think that some of this stuff can
arise in extremely minimal uh systems
that look to us as fully deterministic
because we s we we've bought into the
story that the algorithm tells the whole
tale and I think it doesn't actually
tell the story of what the system is
capable of any more than the rules of bi
chemistry tell the story of what the you
know the mind is capable of so so that's
you know that's my my thing but but but
I'm completely in agreement with you
both that that hierarchy and
unpredictability are are critical to
this I just think you can scale it very
slowly and gradually all the way to to
the end yeah I would um say that uh I
mean I just kind of I'm taking the the
traditional compatibl line here and and
thinking that the issue of um agency or
go directedness is just separate and
distinct from the the question of
indeterminacy so whether or not some
system is predictable or not um seems
like an epistemological question um very
much to your your point um Well
everybody's point but your is Mark in
that you know yeah how I exercise my
agency I'm very unpredictable in all
sorts of ways though I would even point
in a lot of psychological kind of
evidence that suggests I'm probably far
more predictable than I'd like to
believe um but
that uh you can have perfectly
deterministic systems like the one that
you're pointing at Mike that seem as
though they're perfectly capable of
exhibiting the type of agency that's
relevant and that there's absolutely no
conflict between having a deterministic
system that exhibits a agency um and you
know that those are entirely compatible
with each other so I see the the issues
is kind of being orthogonal um to to the
agency question for you know traditional
kind of denet Dan denet um arguments I
mean it does come up and and this is
something that we should discuss
absolutely because Mark I know Mark in
particular has some interest in this too
where where it comes up a lot is in
so-called machines right because because
the Assumption people are happy with a
compatibilist version for for Li for
life forms or at least maybe for you
know for for advanced life forms and
they say yes you know there these two
levels and it's fine yes you're a
chemical machine but don't worry it's
fine that you know that at high level
it's it's it's all good but but but
suddenly when it comes to quotee unquote
machines people say well that's it the
algorithm and the materials tell the
story and machines only do what you tell
them to do and uh and and they certainly
can't have this or you know this or that
property and I I I think I think that's
where the rubber hits the road on some
of these things that if you if if you
take seriously this this compatibilist
view then you have to examine these very
simple low-end deterministic looking
things and you might find as we are now
finding that actually that that that
compatible story actually goes all the
way down that that the machine the
machine does do the things you wanted it
to do via the construction the algorithm
but also does some other stuff and this
other stuff is not just you know
unpredictability and complexity that's
cheap and easy and everybody knows about
that but but I actually think that's not
what you that's not just what you get
you also get uh goal Direct in this you
get problem solving and who knows what
else you get in in the in terms of
Consciousness and whatever I have no
idea but but I think you have to that's
where people become very resistant to
that comp aist idea I mean I I'm very
much with you
mark Mike um in that uh yeah yeah I
think that once you kind of accept that
compatible position all sorts of um
deterministic machines or algorithms or
whatever you have are suddenly going to
be um candidates for for agency and
whatnot and you have to just look at
kind of it's the question of
indeterminacy determinacy is just it's
it's not the relevant question you're
you're thinking about whether or not
something's an agent
I am and I know this is actually
something that I would be really curious
Mark and uh to to hear kind of what you
would say on this because this is
actually I think a mini debate Dan and I
have sometimes because Dan definitely
sees kind of the effective profile and
creatures more like us as being a a
really key
ontological um I don't know and Dan step
in here and correct me I'm
misrepresenting your position but uh as
being is being really kind of key to
exhibiting some of the kind of robust
agency um particularly kind of higher
level stuff and I might be a little bit
more um kind of sympathetic to perhaps
Mike where you're coming from where I
I'm kind of more inclined to say yeah
you know really uh you know robust
effective creatures like us you know we
we we're kind of capable of some more
unpredictive stuff it's going to be a
lot harder to say what I'm going to do
next and what the you know my Roomba is
going to do next but uh but I'm kind of
more sympathetic to to perhaps where
you're coming from Mike and think yeah
but you know it's just different scaling
different levels of agency there you
know I'm I'm kind of I've got a much
bigger bag of tricks given the effective
states that I have but fundamentally
it's not a difference in
kind look I I think that um to thinking
about this in terms of
scalability there's a there's a worry
there which is which goes something like
this that um of course if you believe
that
Consciousness uh the emergence of
Consciousness in our universe um
happened at a certain point in time or
there's a transitional phase in which
Consciousness evolves uh then which is
what I'm sort of arguing unlike Mike you
know Mike sort of saying well it was
there with the big bang in a very simp
form um it's it's there with the with
the indeterminacy
principle um and and what I'm what I'm
leading up to is that uh if you believe
as I do that it evolved um moreover that
it
evolved probably uh somewhat later than
life evolved in other words it's a
biological phenomenon and it's not a
phenomenon that applies to all
biological life um then
if you take that evolutionary naturalist
view it goes without saying or it's
implicit in that view that it evolved
out of things that were already there so
um sure indeterminacy was already there
um and and and uh the other things that
Mike listed before he remembered to
include indeterminacy uh were also there
um but those are raw ingredients you
know there's there comes some point at
which those raw ingredients combine you
saying scale up uh in in a in a way that
introduces something more than just
those component parts um that there
comes a point where it starts to become
meaningful to speak of Consciousness I
mean you know in a way I'm making a very
benal point you know you could say you
can't speak of liquids when you're only
looking at individual atoms you know of
course liquids are made of individual
atoms but the state of their Arrangement
uh only uh becomes a question once there
are enough of them um and so you know
something like that seems to me to be
called for here that that of course I
agree that the the raw ingredients are
there um Consciousness is not a miracle
uh it's it's something that emerges out
of some sort of uh combination of uh
components that pre that pre-existed but
the question becomes at what what sort
of transition occurs that starts to make
it meaningful to speak of what it's like
to be that object what it's like to be
that particle what what it's like to be
that agent you know
why and it's not a matter um of uh
unpredictability it's a matter of how
the object or agent deals with
unpredictability it's what sort of tools
it has for Earth continuing to exist as
a particle as a as a um a thing separate
from its environment U with some sort of
self-organizing properties U utilizing
these new tools or these emerging tools
uh to be able to navigate uncertainty in
a way that its predecessors could not I
know what I'm saying is vague I think it
sort of necessarily must be vague but
sorry yeah thanks Dan no it's not I
don't think it's vague at least not for
me
um so there's a distinction that gunar
and I make and I think you two will sing
along with it let's see it also comes
from David Hume which is the distinction
between cognition and passion reason and
passion in his language and in this
scheme of thinking reason calculate
computation has no motive Force
whatsoever all motive Force comes from
Passion of various kinds by translate
translated today as affect so what
you're talking about Mark sounds like
the buildup of cognitive complexity of
reasoning complexity none of it with any
motive Force but highly important when
it comes to figuring out what the
organism is going to do with its
passions because because all the things
it wants all the oomph or at least
action um activation that that's driving
it is going to be executed by that
cognitive machinery and it's going to
produce a plarium if there's very little
cognitive machinery and it's going to
produce us if there's extraordinarily
complicated cognitive Machinery but
again with this SE separation we're not
asking about the boundaries of affect
anymore we're asking about the
boundaries of cognition of Reason tell
me what you Mark how you respond to
that uh well that sounds that sounds
right to me it's a very simple
response that sounds right to me I'm
forgive me I'm not a philosopher and I'm
astonished to hear that that was hume's
position and uh and gratified to hear it
huh okay I'm getting it right right he's
my Phil he's my real
philosopher that's my my read hum but
yeah you you know Hume better than I do
uh but yeah no no and Mark I think that
what you site with uh and some of your
work with is it merkers um work I mean
some of that I think is just almost
indisputable uh kind of really nice
empirical evidence that suggests exactly
this this kind of humi in line is right
um and I think that the that your view
Mark very much kind of aligns with and
and yeah I I the kind of the dualism of
calmer and all that um I think is
problematic for all sorts of reasons but
you can kind of find uh in that
transition to to what you were talking
about in kind of a liquid state fits
very well with the kind of story that
Dan and I would want to tell about like
a non-reductive materialistic um
perspective on on something like
Consciousness um but I I'm always kind
of curious and this is where you know I
how much um of this do you think hangs
on the the affective state that seems
seems to be something that is most
readily identified in kind of biological
phenomena where yeah I mean I tend to I
think I'm a little bit more hang with
Mike on this one that I I really don't
see it problematic to find kind of
something akin to at least agential now
how much that is synonymous with you
know something like Consciousness I
don't know and that that's an
interesting question but I mean finding
that in machines to me that may not have
anything akin to an affective State
doesn't seem deeply problematic I I see
this kind of as affective as being one
of the primary drivers of it in this
kind of humean view but but I I don't
see why that couldn't be found in in the
right sort of machine
um I would like to I would like in in a
few minutes to come back to Mike and to
ask him about that the thing about what
I was saying about the the at what you
know although the raw ingredients are
there at what point does it become
meaningful to speak of um a conscious
agent um the the but what you've just
said GNA you know it it links up with uh
why I enjoy so much and depreciate so
much conversations like this because it
it exposes you to your prejudices and
that was a Prejudice that I uh subscribe
to for one of a nicer way of putting it
um you know that that that artificial
intelligence had had nothing to do with
the mind I was not in the least bit
interested in artificial intelligence
you know I I I was uh um of the view
that the view I expressed a few minutes
ago and which I now need to um qualify
uh the view that Consciousness is a
biological phenomenon that evolved at a
certain point in the history of life and
uh it it's it's not synonymous with all
life and that it certainly didn't
pre-exist life um but once Consciousness
evolved um and once or to the extent
that we are able to um discern the uh
the mechanism whereby it evolved uh then
uh you can engineer it artificially
there's no reason why that mechanism
can't then be um you know engineered
and uh so in other words the mechanism
evolved uh for very good reasons and uh
that's not the only way that that
mechanism can be deployed thereafter uh
so the Prejudice I'm referring to that
uh there's that machine Consciousness is
is a is a is a an illusion or is a
Sci-Fi sort of story I no longer believe
that at all and in fact as Mike knows
I'm deeply involved in a project and
have been for a few now where we are
trying to engineer uh an an artificial
Consciousness in other words we're
trying to engineer an agent that
instantiates the functionality that we
find in the vertebrate upper brain stem
and
um so I I'm fully on board with that now
uh and again I say you know this is why
it's important to have conversations
which you know which test your your
assumptions and which enable you to get
get over your prejudice
I it's probably the most exciting thing
I've ever done that project that I'm
working on now and and just 10 years ago
I would have you know I I would have poo
pooed it um but as I said I want to come
back to Mike about that transitional
thing but I just want to insert
something here in case we have time to
come back to that too uh which is that
in our attempts to engineer uh an
artificial Consciousness to use that
wonderful phrase a computer that gives a
damn uh in in in our attempts to to
create such a a machine Consciousness uh
it's it proves to be rather difficult
Mike uh to go back to your um your
starting point when you're saying well
basically anything that follows
hamiltonian principle of least
resistance has a little bit of
Consciousness I'm saying well I'm not
even sure that our artificial agent that
we've been laboring on for years trying
to get it to display the functionality
that I would think uh would be
reasonable evidence that it's using
feelings to make its choices uh it's
it's proving rather difficult to do that
so I think that's saying the same thing
from a different sort of angle um when I
say I'm skeptical that that uh that
protoconsciousness is there at at that
or that or that the word Consciousness
deserves to be used you know at at at
those more Elementary levels but I could
I've gone ahead of us Dan you were going
to say something and and maybe we can
come back to Mike's answer to those
other questions well actually I'm gonna
set up a a fresh confrontation between
you and Mike on this because I didn't
even like to confront my
C um AI again in gunar and My Views has
no feeling whatsoever because feeling is
consistent with the least action
principle it's oomph and all the only
that any AI has is the voltage
difference between the prongs of the
thing where you plug it in um if you're
going to create something motivated and
not just something cognitively smart
because AI is incredibly cognitively
smart the computational Machinery is
well it's just pattern recognition I
know but it's really good at it right um
but it's it's affective profile amounts
to that voltage difference near as I can
tell and I want to hear both your
reactions to that
so the shall I go Mike or do you want to
go go ahead go go first for me the the
the the unless I'm misunderstanding you
then the crucial thing there um is
whether or not you uh understand energy
only in you know sort of Gibbs energy
terms I I I I think that uh an an
informational energy is what we're
talking about the oomph when we speak of
the certainty of the system uh over the
question as to what I should do next um
and the principle by which it decides uh
what to do next in other words the
principle by which um it exercises
Choice uh it has to do with with
informational energy with with what uh
with what calls variational uh free
energy so I I I I don't agree with the
premise that it that that we're talking
about voltage differ differentials so
you could cast my decision making in
terms of diffusion of the free energy
gradient from the sugar that I had to
eat right and say that's the equivalent
of voltage difference Dan there's no
motivation going on there beyond that
but of course that's wrong because there
are intermediate sources of oomph
between the between the sugar and my
behavior namely my desire to get up and
go for a walk this afternoon powered by
the sugar but that's Upstream right
Downstream at the level of me there's
something there so in order for AI to be
demonstrably feeling and caring and
preferring and all that stuff that's
wrapped up in Consciousness there need
to be these intermediate states which
are powering it in its own in directions
which are sometimes different than the
the voltage
difference and I don't know enough about
AI maybe one or both of you can convince
me they have those intermediate States I
think because this interleaves with so
many of the issues that we now are are
busy um discussing let let me just say
in a very simple way that I think that
precisely the sort of problem that
you're talking about now done uh is is
why I I'm loath to attribute
Consciousness to an organism that
doesn't have a nervous system because
it's that higher level that you talking
about that is introduced by a nervous
system which kind of orchestrates what's
going on in terms of the chemical
Radiance of many of its uh organ systems
there's this kind of informational
gradient that then that then regulates
what's going on in terms of the other uh
uh energy gradients and uh it's that
sort of thing that I think uh we're
talking about when we when those of us
who's who's saying we're we're skeptical
about the the the mechanism in question
being in the Raw ingredients although of
course um the the conscious agent is
made up of those raw ingredients but but
over to you
Mike um okay um I've I've made notes and
I'm going to come back toh to the to the
first thing you asked Mark let me let me
just say something about this this AI
business and I preface this in in two
ways one is uh I'm going to see some
weird things and and my my um is not
it's it's it's it's very much not a
mysterian position okay I'm not arguing
that there's some sort of weird magic
that we're not going to be able to
unravel I think there absolutely is a
research program here and I'll I'll I'll
briefly describe it and then the second
thing is I want to be very careful and I
am nowadays very careful with what I say
because I still haven't sorted out uh
there's a there's there are some ethical
dilemmas here because because I think to
the extent and Mark and I have talked
about this before to to the extent that
any of this is is right it it I think it
it probably leads to an advance uh in
machines or created things that we then
have to be concerned about on a on a
moral level in the in in terms of the
capacity to suffer and so on so it's
still it's still a little uh it's a
little unclear to me what I should and
shouldn't be saying but but let's uh
let's let's just put it this way um what
what what Dan just said about about the
AI okay I I agree with you in that I
don't believe that any AI is conscious
because of the algorithms that it's
following that is not right so that
computationalist idea that is not why
why I think it may or may not be
conscious and I agree with that however
um do you know you know the old mgre
painting with a pipe and it says in
French this is not a pipe right you
you've seen that one so so I've had so
Jeremy gay who's my group amazing
graphic artist um I've asked them to
make a thing that that has a picture of
a touring machine and it says on the
bottom in French this is not a touring
machine because because here's the
problem that I think we're making um we
somehow have bought into the idea that
our the limitations of our formal models
are limitations of the actual thing and
so when you have a device that somebody
wrote an algorithm for and you know
people say well I write these you know
these language models I write the code
it's linear algebra I know what it does
and my point is you don't even know what
bubble sort does and if you don't know
what bubble sort does you sure as heck
don't know what this thing does and it
isn't because and I agree with you that
there's a there's um bifurcation here
between intelligence and language
competency and Consciousness we've we've
now split those things apart um so I
don't think you conclude that it's
conscious because of the things it says
or because of what's in the algorithm
but we are seeing even very simple
algorithms do things that are not in the
algorithm they are um I think what what
what Whitehead called ingressions and I
think they're ingressions from a from a
a space of patterns where the boring
ones are facts about prime numbers and
truths of number Theory and things like
this but I think there are other
patterns that are much higher agency
things that we normally um associate
with certain kinds of minds and I think
when you make these things whether
synthetic biology whether normal embryos
whether um AIS or some kind of hybrid
cybor construct what you're really doing
is you're making pointers
that that get out more than you put in
we we are seeing this again and again
that that you've made something and what
you've really made is a pointer into a
space of patterns which is very
surprising which which we do not
understand at all we call some of them
emergence but that just means you know
you don't know where it came from and
and you know you're sort of making a
catalog of these things and so I want to
be very careful about concluding what
these things have and don't have by
focusing on the material and the
algorithm right would be to me that
would be making the same problem with
what you were just saying Dan of looking
well the laws of chemistry there's
nothing agential there so therefore
you're just you're just a machine and we
know that's that's not a good way to
think about it we have to be fearless
and and I think uh consider that that
line of thinking goes much further than
we're used to thinking and and that's
why we got to be careful because because
the properties of these things cannot in
any obvious way be discerned from the
the materials the composition the
algorithm or what you think it's doing
even even very simple algorithms do
interesting things they have um certain
kinds of uh goal directed behaviors and
competencies that are not in the
algorithm and if we get surprised there
I'm sure we will be surprised when we
make these more complex things that have
been trained on human data and so on so
so that's so that's kind of one thing uh
I think I think that we have to be very
careful um not to make that assumption
the other thing I want to say is just to
go back to Mark's point about when is it
meaningful to speak of these things so I
think that is exactly the right question
and my view on this is very Engineering
in that I I what what I take all these
claims to be are really protocol claims
what does it make sense to what what
tools does it make sense to use to
interact optimally with a system is it
psychoanalysis is it um you know
Behavioral Science is it cybernetics
control theory rewiring like what which
which tools are the appropriate tools
and so for this you know I kind of think
about um there's this there's this
Paradox of the Heap right you got a pile
of sand and they say well you know you
take away sand and when does it stop
being a heap so so my view of that kind
of spectrum is this I don't want to
worry about when it's a heap and when
it's not a Heap what I do want to worry
about is if you tell me that you have a
heap and you want it moved I need to
know am I bringing tweezers a spoon a
shovel a bulldozer you know Dynamite
what what what what are the what are the
appropriate tools and so now so now we
can ask a very empirical question for
these for these very simplistic things
that don't seem like Consciousness is
appropriate what are the tools that we
can deploy on this one tool that we have
and we tend to use it a lot is the
visualization of what is it like to be
that thing and I would claim that that
kind of sort of works for other humans
maybe that works for other animals but
it becomes increasingly an unreliable
guide to simple or exotic other forms so
the fact that we simply cannot imagine
what is it like to be a Roomba or a
magnet or anything else that's that's
guaranteed right all by by by the fact
that that for us it's very hard to
imagine being a mind at all different or
even neuro you know non- neurotypical
humans it's just very hard for us so I
think that's not surprising at all but
we can use specific tools and we can say
okay can we take concepts of like all
the stuff that that you know that you
and um and Carl and other people you
know in Neuroscience they use and can we
apply it to these other very simple
systems and so it becomes an empirical
question right and and and what we're
finding is that when whenever we try it
we discover new new capabilities new you
know new research programs um that that
I think is the judge so you know am I
claiming that psychoanalysis is usefully
applicable to um you know Gene
regulatory networks no but but training
certainly is and and and something that
we just put up as a preprint actually um
measures of causal emergence you know
like IIT style metrics actually very
well apply to Gene regulatory networks
and they change with training I mean you
can use all these things on these very
simple system systems so you know I
think I think our our imagination is not
a good guide but porting the tools and
seeing how far you get gives us a good
payoff your your caution is well
articulated and and it chastens me a bit
to hear it so thank you for
that how do you tell the degree to which
something cares as opposed to the degree
to which it's just Computing and what's
what's the experiment what's the test
you name the tool yeah yeah so so I'll
tell you I'll tell you a very simple
experiment that that that we've done um
and IT addresses this this common
critique that that well machines do what
you program them to do of course well
some some machines and and only if you
don't really have the imagination to
look elsewhere so I'll give you a very
simple example in these I I don't know
if it's if it's simple enough to be
clear but let's see uh so you have a you
have arrays of jumbled up numbers okay
so they've been randomized and you have
a sorting algorithm this algorithm is is
is very very simple it's just a few
lines of code and it's how to rearrange
the number so that the whole thing
becomes in order okay monotonically
increasing and and what you can do is
you can plot the movement of that um
that process in its Behavior space
meaning how sorted is the is the
algorithm is the is the array so you
start from all kinds of different
different starting points but in the end
there's one point where everything is
sorted and they all reliably get to that
point so now you have Behavior you have
behavior in a space and you can start
asking some questions um what are the
competencies of this thing in that uh in
that process so I'll give you I'll give
you two two examples one thing you can
do is you can do what you always do to
test uh uh goal directed behaviors and
intelligence is you give barriers you
put in a barrier and you see how good is
this thing at overcoming a barrier one
one trick that some systems know how to
do is delayed gratification so that
means you've got a barrier in order to
overcome the barrier you have to
temporarily get further away from your
goal okay so two magnets with a piece of
wood in the middle are not one magnet is
not going to go around because it's too
dumb to go against the gradient to
recoup some gains later on it doesn't do
delay gratification what does a sorting
algorithm do so you got your sorting
algorithm there are there are a few
lines of code one thing that the Sorting
algorithm the standard one does not have
is any metric of asking how am I doing
it's not in there it doesn't it assumes
reliable substrate because because comp
standard Computing assumes that your
Hardware does what the code says it will
do so it has no ability to say well did
did did did my action succeed am I doing
well do I need to do this something
there's nothing like that it assumes
everything's fine if you put a barrier
between it and its goal and the barrier
is a broken number it's a it's a it's a
number that where the hardware refuses
to move so I say I want to swap the five
and the seven and you issue the command
but the seven won't move it's stuck it's
broken that's your that's your that
prevents you from going where you need
to go the way that you normally would go
turns out um that the Sorting algorithms
despite having no extra steps for this
at all they do delay gratification if
they come across a broken number they
will back back trck desort the array the
the sorness actually goes down they go
against the gradient something that
simple magnets and things don't do they
go against the gradient they go around
and then and then they get to where they
need to go now that capacity that delay
gratification is nowhere in the actual
algorithm there are no explicit
Provisions for that so that is a simple
example of a very simplistic uh kind of
Competency but it's it's something and
it's something that a lot of systems
don't do and and you didn't have to put
it in and you didn't know it was going
to be there from the steps that you did
have that's the first thing the second
thing is um we found we found a novel
there's the thing that you made it do
and there's the thing that it did on its
own that you did not want it to do um
one thing did you want to say something
about the no no I'm just singing along
that's all okay uh there's something
else that you can do that you can do
with this is that you can take that
sorting algorithm and you can put it
into each each number in other words
instead of a master um a central
controller that's that's shuffling
numbers you can just put the same
algorithm in the actual number so the
five wants to be next to the six and you
know between the six and the four and so
on so when you do that one thing you can
do is you can make chimeric strings in
other words you can make strings of
numbers where half of them are following
one algorithm half of them are following
some other Al algorithm and you just
sort of mix them up it still works
everything gets to where it needs to go
because the cells ultimately agree on
where they want to be so everything
works but you can do an interesting
thing you can say along the way what's
the tendency for cells with similar
algorithms we call them algot types what
is the tendency for for cells with
similar uh numbers with similar algot
types to Cluster together initially
that's zero because it's completely
random at the end it's uh it's also so
so the probability of being next to the
your own type or some other type is is
is 50% at the end it's also 50% because
you have to get sorted and they with the
assignment Is Random so so if you're
going to sort the numbers you have no
guarantee of who you're sitting next to
but in between what happens is the
sorted goes like this they have this
weird extra propensity to hang out
together with others of that type in the
end the physics of their world world
rips them apart because because in the
end the algorithm insists that you are
going to be sorted and that will rip up
any of these clusters that form but in
the middle you get this weird thing
where where uh uh I you know where where
uh cells with the same algot type they
tend to Cluster together now that again
nowhere in the algorithm does it say
what type am I what type is my neighbor
let's go sit next to my neighbor none of
that is in there this tendency to hang
out with your own kind so to speak is
completely emergent here and when I
first saw it it it was like a real um uh
like a like a weird uh existential
moment because it's it's it's the Story
of All of Us in the physical Universe
right you you the you you can't escape
the laws of physics eventually entropy
grinds you down but in between your your
your start and your end point you get to
do some things that are not inconsist
consistent with the physics of your
world everything is totally
deterministic and consistent but you get
to have these other side quests that are
neither neither um explained or
forbidden by the by the physics and not
at all obvious to any Observer until
they know how to look and and where to
look and so so these are just two very
simple examples of of of taking a taking
a a system where you don't expect any of
this and using creative different ways
to ask what is this thing doing and
finding out that it has competencies you
didn't know about and it has
uh some some tendencies that while don't
you know they're consistent with the
physics they're not anywhere in the
physics they are their own very minimal
but but there it is and so and those
kinds of things we need to be we need to
have a science of this of of looking at
what else these systems are doing
besides what we told them to do um I'm
I'm mindful of the fact that I don't
know about you guys but we I have to end
in four minutes because I have a meeting
that I can't miss because it's a meeting
with the funer of the project I was
talking about earlier um and so meetings
like that one has to but I I I very much
hope that we will have further
conversations because it's obvious that
there are many things here that we
haven't resolved there are many things
here we can't resolve but I think there
many things that we've been talking
about that we can we can make further
progress if we talk more than we've made
in the hour we've had but so I want you
to squeeze in under the wire if I may um
first of all I agree
that um we need some sort of uh uh
objective uh test uh and it by objective
I mean it needs to deal with prejudice
in other words something along the lines
of a Turing test with all of its
problems but I mean where you can't look
am I dealing with a machine or am I
dealing with the with a creature you
have to just judge it by its outputs um
the question then becomes what sorts of
outputs um are convincing evidence uh
that the agent is
using um the sorts of functionality that
we are looking for what Mike has just
hypothesis to run by you and I'll do it
by email okay thank you send it I'll
send it to everybody about
a y but but what Mike has been saying
and and it it grows out of conversations
I've had with you before Mike um the You
by which I mean you have extended my
thinking along these lines because I had
initially said to you in my first
encounters with you I would U only be
persuaded uh that an agent um is
conscious if it is able to solve novel
problems uh and I would have to add
novel problems um which are
consequential to its own existence it's
not just a novel problem it's a novel
problem that has that that that matters
to that it gives a damn about that that
that has consequences for itself as an
agent because of this the necessarily
subjective and and uh so on nature of of
feeling what you've just described which
we've talked about before these are
there's a goal that's written into the
algorithm which is say for example sort
the numbers uh and then there's a novel
problem as to how do I get there uh and
the the um agent comes up with a novel
solution this you've persuaded me of
what what I'm not persuaded of is that
in a situation such as that it's used
feeling uh in order to get to the novel
solution and so this is how you've uh
develop how you've forced me to have to
think more deeply on matters like this
so in terms of functional
criteria um I I would like to see uh
evidence that the agent is using this
functionality and I don't think it's
something magical I think this is the
crucial thing we need to get our heads
around is what the causal
uh um mechanistic powers of feeling are
that that that that are not there prior
to the emergence of feeling and I've
told you before Mike and I'm sorry we
don't have time to go more deeply into
it not because I really do have to go
but I'll just quickly describe it if if
for you going and Dan that there's a
there's a thing we used with zebra zebra
fish as you guys called where it's
called honic Place preference Behavior
where they hang out on this side of the
tank cuz that's where the food is
delivered then you deliver Coke
or morphine or amphetamine and even
nicotine to the other side of the tank
and they gravitate there and prefer to
be there and just dot back for food and
the the the the explanation for that
surely is that there is anhedonic
there's a there's a pleasurable uh uh uh
um it it's it feels good to be on the
side of the tank because those
substances are not doing it any good uh
so the feeling is somehow having some
causal consequences for the behavior of
the fish and and I think something along
those lines is what I would like to see
from an artificial agent uh which that
kind of
dissociation uh of the feelings causal
power from the causal power of the end
goal that is written into the thing
which is survive you know conflicting
feelings yes yes yes yep I I I really
look forward to further exchanges chaps
and Mike as always thank you for
introducing me to endlessly interesting
people I don't know where you find them
amazing find you thank you thank you g
great to meet you cheers cheers guys bye
bye yeah so we'll obviously do this
again I I go to go in a couple minutes I
I've taken some notes what I want to
start with next time I want to pick up
where where he just left off because
Mark focused on the um the finding novel
Solutions but actually my example wasn't
even that it wasn't finding novel
solutions to a problem that we gave it
it's finding a new problem that it's
dealing with that we never gave it and
that I think that I think is a different
different story so we can you know we
can we can start on it next time

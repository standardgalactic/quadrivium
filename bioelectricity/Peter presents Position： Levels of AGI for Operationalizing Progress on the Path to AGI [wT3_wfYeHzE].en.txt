okay so can you all see my um
screen yes okay um so as you can see
this paper is called levels of AGI for
operation operationalizing progress on
the path to AI so this paper is kind of
inspired by the fact that well one
there's been a lot of discussion
recently about llms and whether they've
achieved AGI but also just about like
the need for this discussion to happen
especially in relation to like risks
that these things pose in order to come
up with like a framework for development
and uh
regulation
um that's really uh yeah that's why this
this this paper is really focused on
like how can we create a Common Language
around
um artificial general intelligence such
that we can foster like a prod
productive discussion on um
understanding it and evaluating it and
evaluating its risks and regulating it
um I had a lot the one thing I like
about this paper is like all the
examples that it
provides but before I go into examples
of cool things that it provides I think
that I will go over it in terms of like
the main points that it talks about um
and then I'm gonna probably go through
it again picking out things that I
thought were interesting or cool um
but how should I do
this yeah I guess that's yeah I'll go
through the main
points I'm I'm I'm a little bit should I
go through case
studies yeah I'll go through case
studies okay um so this paper is broken
down into uh I think six sections the
first section is
like uh this one over here about
defining AI so it gives different case
studies on how people have defined
artificial general
intelligence um the second one is
actually about like how do we H wait
it's over it's over here
sorry zoom out a little bit the second
one is about defining AI so it's like
taking some of the lessons learned from
these
definitions and then like how should we
actually look at it like what should be
the the considerations that we should we
should
consider um then they talk about like
what are the levels of AGI so like what
are how do we evaluate what level of
intelligence a machine has achieved and
then uh finally well there's one section
about testing AI but then the final one
that is about like assessing risk with
respect to art artificial general
intelligence so I'm going to go through
each of these sections broadly and then
uh specifically so that we get some like
um
repetition uh that allows us to like you
know um retain these Concepts a little
bit more so the the first one about
let's look at the first one about
defining artificial general intelligence
and the case studies so again we'll go
through these in more detail but I'm
going to go through them quickly hope so
the first case study is an example of
and probably the first example of us
trying to gener to Define general
artificial general intelligence is the
Turing test which is where you ask a
human to interact to interactively
distinguish whether text is produced by
another human or by a
machine um so that's one way of
evaluating
intelligence another one is uh like this
idea that artificial indel intelligence
is it possesses this thing called strong
Ai and um it's basically saying uh like
we've achieved AGI when uh uh um when
the the the machine that is exhibiting
the per the the intelligence can
understand and and like have other
cognitive states which is basically
saying like it has
Consciousness um we'll go into a little
bit more detail there so that's an
interesting one uh the third one is
human level per I I skipped one oh
analogies to the human brain yeah
another one that they talk about is just
like that the machine works like the
human brain but they I think they really
say that that one doesn't make sense and
we'll talk about why later um then
there's the fourth case study is on
human level performance on cognitive
tasks um so
like problem solving ability that's
similar to
humans
um the last the fifth idea of how to
evaluate so that's human level
performance on cognitive tasks so that's
another way case five is the ability to
learn tasks so like to learn how to do
new things um the fourth one I mean the
sixth one which I think is quite
interesting is like economically
valuable work so the assessment of AGI
is like how much that work that people
that like we are willing to pay it to do
or we would pay a person to do can it
accomplish itself and this idea of
evaluating intelligence based on
economic work comes up a lot in this
paper so I think it's quite a good one
um and then the last one uh that or not
the last one the seventh one is this
idea of flexible and general work and
they call it the coffee test but
basically it says like you come up with
a set of benchmarks for U an a model
that like if it can surpass per it can
if it can perform these benchmarks
correctly then um it will uh it's
achieved AGI and the benchmarks that are
suggested by one person are
understanding a movie understanding a
novel cooking in an arbitrary kitchen
writing a bug-free 10,000 line program
and converting natural language
mathematical proofs into symbolic form
so that's just the set up but you can
see like these are very highlevel tasks
that that it would the system will be
able to
accomplish um and then the eighth
example is artificial capable
intelligence um and that one is uh the
the basic idea there is like again
making money and one of the proposed
ones is that you give the system
$100,000 and it in a couple months it
makes it a
million which I was like wow that's
a I wonder if that's even possible in
certain economic climates but anyway um
again you see this idea of being able to
make money with it as like the way of
evaluating
performance and then um there's one
there's a there's a paper that came out
by these two authors recently where they
argue that llms have already achieved
artificial general intelligence so
that's another
example um just that we already can see
it in state-of-the-art
LOLs so you can see there's like a lot
of different ways that people have
proposed to evaluate
performance um and then uh the the as as
like a reflection on these this paper
proposes six principles uh for
evaluating performance um one is
focusing on capabilities not processes
so this one I think is really important
because the idea is that you shouldn't
focus on how something is accomplished
and instead focus on whether it's able
to accomplish the task
and I think this is important because we
have in in the past we've had so many
biases on how a model should accomplish
something to show that it's intelligent
that
um like it kind of it we're basically
infusing bias into how we think should
be performed whereas if you focus on
capabilities instead of process then you
avoid this bias onike process that we
seem to have um focus on generality and
performance so um I think that one thing
that we've focused on a lot with llms is
that they can perform a broad range of
tasks we've kind of
ignored what at what level of skill they
can perform those tasks and so this is
arguing so we want it to be able to
perform at a very high level on a
specific task but also perform well on a
number of tasks focuses on cognitive and
met me metacognitive uh tasks but not
physical tasks um that's we we'll get
back to a definition of what medic
aition is and when we come back here um
but just so this is basically saying
focus on like thinking tasks instead of
tasks where you have to actually
accomplish something
physical
um focus on potential and not deployment
this one I think is really
uh this one I I have like I go back and
forth about what I I I can see why
they're suggesting this but I also think
like can we
really uh measure performance through
potential and not deployment so here
they're saying like we shouldn't deploy
systems to figure out whether they work
we should in tests that measure them
potentially which makes sense because
you wouldn't want to like measure the
performance of a a model that like um
diagnoses a patient in practice you'd
probably want to do it measure it by its
potential to do so instead of like
deploying it in the real world
um so that's another concept and then
there's focus on ecological validity
which means this basically means focus
on tasks that we actually perform in
real life that make sense in terms of
how what what people do instead of tasks
that uh don't really make sense like for
example measuring an llms perplexity
score is probably that is not an
ecologically valid task because we don't
care about perplexity when we're talking
to each other we care about like
semantics and the syntax is
understandable and stuff like that
um focus on path
and focus
on okay so I guess what they're saying
here uh for the last one is focus on a
path to AGI not a single end point is
this idea that that um when we're
thinking about measuring intelligence we
should actually think about it in terms
of like
developing uh super intelligence instead
of like a specific goal so like we
should think of it as this progression
as as we're developing as we're defining
intelligence instead of thinking of it
as like there's some end goal that we're
trying to achieve and these benchmarks
should be achieved for like all
benchmarks should be with respect
respect to a specific end goal okay
there's two more Concepts that I want
for yeah two more Concepts that I want
to introduce uh before we go into detail
about different parts so this is talking
about like how to think about artificial
general intelligence and here they
Define in section four levels of
artificial general intelligence and they
have two different specific criteria
that they use to do to um to measure
General in to measure intelligence one
is narrow uh Nar narrow uh performance
or like narrow Ai and one is General AI
so narrow AI is the ability to
accomplish a specific task and it's like
how well can you accomplish that
specific task and there's different
levels so there's no AI emerging which
is like kind of good competent which is
like better than 50 percentile expert of
people which is better than 90th
percentile virtuoso 99th percentile
superum better than everyone and then
General so narrow is like being able to
accomplish a specific task really well
and then General is the ability to
accomplish a variety of tasks quite well
um and so they they argue that we need
to think about models in terms of both
because like uh we have models that can
accomplish tasks at a superhuman level
but they can only accomplish a specific
task and we have other ones that can
accomplish a broad range of tasks but
not very well so thinking about they
they argue that's how we should think
about General artificial general
intelligence in terms of this like
Spectrum which has two
Dimensions um and then finally I'll just
introduce risk uh so for for analyzing
risk there the key concept that they
talk about for um for thinking about
risk in terms of artificial general
intelligence is uh they argue we need to
think about it in terms of like the
level of deployment or the way that we
deploy it so um which they call autonomy
levels everything's in terms of
interacting with humans so no AI is when
a human does everything AI as a tool is
when we're like using AI but we're
mainly the ones making guiding the
process and the decision- making AI as a
consultant is when you'd probably you
would use an AI to like uh provide like
help make decisions in a in a more
substantive way so maybe like they would
tell you how how large a beam should be
when you're creating building a building
um AI is a
collaborator um so this would be um you
know it it's it can be an instructor it
can be it can take on like a full part
of the work where it's making full-on
decisions about what's going on AI is an
expert where we would kind of trust it
blindly and then AI is an agent where we
let the model the the the AI do
everything so you can see it's saying
like think of risks in terms of how how
the the model will interact with the
world and then also um they argue that
different levels of AI allow for
different levels of this type of
interaction um so yeah that was like a
Whistle Stop tour through the paper but
that's just to
Foster uh like talking about these
things in more depth because we've gone
through all the big Concepts okay so
let's go through it uh slower
um and each part so let's first talk
about different case studies for
artificial general
intelligence first let's talk about the
tur test again which is ask a human to
interactively distinguish whether text
is produced by another human or by a
machine so one cool thing or interesting
thing about this test which has been
pointed out is that in practice it's the
test often highlights the ease of
fooling people as opposed to the
intelligence of the machine and actually
think that this concept of like seeing
machines perform at like a high level
and how it fools people into thinking
they're intelligent is something we see
all the time so uh I think the touring
test is actually a good example of how
not to measure intelligence because it's
really not objective it's based on like
a human and a human's idea of what's
intelligent as opposed to something
that's like a little bit more a specific
human idea of
intelligence um and one thing that they
say here is that I highlighted is like
we agree with Turing that whether a
machine can think or while interesting
philosophically and scientific while
interesting philosophical and scientific
question seems orthogonal to the
question of what the machine can do and
so even touring was like we need to
measure performance in terms of what a
machine can do not how it does it
um the second one again is this idea of
whether a um uh artificial intelligence
um can understand and have other
cognitive States so um this is about
like whether we've achieved um
artificial general intelligence when a
machine has Consciousness I actually
think this one is a really out there
definition of intelligence because um it
doesn't talk at all about like whether a
machine actually
uh can do anything it's just about
whether it's conscious because may maybe
you could argue that Consciousness is a
precursor for intelligence but yeah I
thought that was an interesting way of
thinking about um intelligence is is
that we the ability to like is
consciousness the highest form of
intelligence I don't know
um there's that question oh that's just
from me Peter um I was just wondering
you covered this does it Define um
Consciousness uh no in fact they say
there there's no scientific consensus on
methods for determining whether machines
possess strong AI attributes such as
Consciousness um and I've looked very
quickly not now but in the past like at
the literature on Consciousness and we
have I think we still have no idea what
causes Consciousness from a scientific
point of view yeah yeah that's that's
one of the interesting
Parts deep um I
think yeah go ahead um so I've been
following a guy called um josa uh
recently on YouTube and he has quite a
few um videos where he talks and talks
he talks about Consciousness and how it
should and shouldn't be um defined so if
people are interested check out josa
Bach okay could you post it in the the
chat just so we can know the spelling of
the name yeah um thanks um and then so
the the third case study is analogies to
the human brain and that like the idea
that if an AI system works like the
human brain then um or like better than
the human brain then it has achieved
human intelligence this is like an
incredibly process-based way of
evaluating intelligence also human
Centric because saying that it has to
work like a human brain but they even
point out um which I highlighted here
while neur neural network architectures
underlying modern ml systems are Loosely
inspired by the human brain the success
of transformer-based architectures whose
performance is not reliant on humanik
learning suggest that strict process B
strict brain-based processes and
benchmarks are not inherently necessary
for AGI so this is an example where like
anthropomorphic or like attributing
human performance
to uh intelligence just doesn't really
seem like the right way to go
yeah I think there's another hand
up yeah I was just gonna say you know
that that statement there where they
mentioned um like if the AI works better
than human brain but like if the AI is
just you know a text or a um a voice
type of AI then it's obviously not like
the human brain because the human brain
does more than just process information
you know it's like mobility and
different things of body parts so that
sort of phrase in itself is you know
illogical yeah actually that's a really
good segue onto the next the point that
I thought was really interesting on the
next one is uh this case study the case
study for is like human level
performance on cognitive tasks so this
would be tasks that don't require um any
kind of physical interaction and what's
interesting is that it brings up this
concept of embodiment which is a lot of
there's some there's a good amount of
philosophy around the idea that part of
the reason why we developed intelligence
or how we develop intelligence is
because we have a body and we interact
in the world through this body um and so
there's some people who think I've even
I think I've even heard seen like Yan
Lon say that and body he thinks that uh
embodiment is one of the important ways
that that like you learn general
intelligence that machine learning
doesn't have right now but um yeah
that's a really good point that a lot of
what the human brain does actually has
to relate relates to interacting through
the world through perception and through
non-conscious thought and how it
regulates the body but I thought I would
just bring up I just wanted to bring up
this concept of embodiment because I
think it's a really
interesting question like whether that's
required for intelligence or not over
here they're saying no and I could see I
can see why that's true but they they're
basically yeah they so there's that um
case study five is ability to learn
tasks this is definitely uh I think
makes a lot of sense that
if like an artificial general
intelligence can't learn to do something
new then it's really not that
intelligent because I don't think it's
possible like to create an omniscient
machine that can accomplish every task
it probably has to learn over time to
accomplish new tasks um I bet you could
show that mathematically too like the
size of the machine would be larger than
the universal like but um that before I
answer that question um or in in this uh
case study they introduced the idea of
metacognitive tasks which are tasks
where you like question your own
thinking or learning
and I think that personally I think
that's kind of like one of the upand
cominging
fields of machine learning now because I
think we've been able to do a lot of
cognitive tasks but metacognitive tasks
is really open um
yeah was there a hand raised or
no no I don't think so okay cool um okay
and the the next case study is about
economically valuable work and uh it it
says over here like open AI trouter
defines AGI as highly autonomous systems
that outperform humans at most
economically valuable
work um and so this one really talks
about how we should measure uh not that
the paper does but like that we should
measure uh per measure uh intelligence
based on economic value but then they
even point out that there's a lot of um
like
intelligence that we that that people
exhibit like artistic artistic
creativity and emotional intelligence
that uh can't really be directly
connected to economic value and probably
that's why we shouldn't use economic
value like I think that there's like the
very famous example of Steve's jobs
taking a um calligraphy class and then
that like really helped him
design um like a font or something that
became incredibly popular and made Apple
Computers really valuable so like if we
just cared about economic value
calligraphy probably wouldn't be
something we'd want our machines to be
good at but it does have a downstream
impact so maybe economic value isn't the
best
way
um I really like this uh case study
seven where they call it flexible in
general the coffee tests because of this
list of tasks which I told you before
like these are the tasks that they think
you should use to um evaluate a a model
which are like understanding a movie
understanding a novel cooking in an
arbitrary kitchen uh writing a 10,000
line program and converting natural
language mathematical proofs into
symbolic form so I the the cooking in an
arbitrary kitchen highlights the idea
that embodiment is a part of
intelligence in this test not in general
but just like in the test that's being
proposed here and I was wondering if
anyone had any other like tasks that
they would add on here that they could
possibly think of
um I always think of animals right so so
animals is always a question how how
intelligent are specific animals and and
one of the best tests people figure out
to to whether animal is conscious is is
having having them recognize themselves
in the
mirror combines the embodiment right so
you need to have the body in order to
have your physical representation of of
yourself and then I I think they do it
like on on dolphins or so where they put
like a red dot somewhere where there
shouldn't be and and if they they see
whether the animal tries to get rid of
it or so that that is a sign of of
awareness and that all you need some
sort of robotic interface otherwise it's
would be hard to to discuss um yeah that
we have achieved intelligence when
everybody sort of agrees it happened by
by whatever their measure is right it's
the same with humans right we how can we
be sure that every human is intelligent
well he sort of got so used to the idea
that that we assume it is the
case yeah true I I actually one thing I
was thinking the whole time that I read
this paper is that like
the that human intelligence is so
complex and also very like the
difference in mathematical skills
between one person and another but also
like the difference because like you
know become being a very skilled
Craftsman might not be like a logical
intelligence but it is a is a form of
intelligence which is also
very
like I don't know very impressive and
useful um like just there's so many
types of intelligence so it's really
hard to to even to B it case study eight
um I was just yeah I was just going to
say about the list of tasks you just
highlighted they also all have a
unspecified element to them so don't
know how well you know you as the
examiner wants me to be able to or the
agent to be able to do
those and so I think that's actually a
key Point
too yeah that's a good point that
there's some level of ambiguity that you
have to be able to uh deal with uh the
last the second to last case study but
the last one I'll go over is we talked
about it before artificial capable
intelligence and the test was like give
a $100,000 worth of capital and task
with turning it into a million dollars
over a period of several
months um I don't even know if that like
I said I don't even know if that's
possible uh like if you just I like what
human could could do that probably like
an incredibly small proportion of the
population could do that and especially
like in a legal way or a way that's
sustainable I don't
know but I thought it was an interesting
one is there a a question I was going to
say about the legal Point um you can uh
you can there there are ways in finance
you can do it illegally very
easily yeah so maybe they would exploit
that system um but anyway those are some
of the examples now let's go over the
um the so the six principles that are
kind of off of these examples
again um one is focusing on capabilities
not processes I think that makes a lot
of sense because as I said I think it
avoids bias in terms of how we measure
intelligence um focusing on generality
and performance so bread and depth which
we'll go into a lot more detail on um
later on so they argue that you should
focus on cognitive and metacognitive
tasks but not physical
tasks um and they say whether to to
require robotic enironment as a CR
criteria for AGI is a matter of debate
which is true that's what we've been
talking about but they say okay for now
let's just focus on cognitive and
metacognitive
tasks probably because that makes it
easier but also I think they say oh yeah
they say over here um
they also because like there's which is
true based on my experience in robotics
that um like physical intelligence like
being able to control a body is really
lagging behind like cognitive
intelligence using these models um and I
think there's a variety of reasons for
that but maybe that's one of the reasons
why they suggest that um but they also I
think over here argue that saying that
you require embodiment for intelligence
is a bias that we're introducing which
maybe we
shouldn't um
yeah focus on potential not deployment
um so this one uh the the next two are
like focus on potential not deployment
means like don't deploy a model to
figure out how performant it is just uh
create a benchmark that you can you can
measure its potential with but then the
one that afterwards that they use is
like focus on ecological validity which
is like basically evaluate performance
on tasks that people value that are
important to us in our
society like the two things that came up
to me for me when I was looking at four
and five is that um one if we focus on
task of people value that might change
over time which because of like that's
kind of a cultural thing like what Val
what value ascribed to certain tasks um
but also just that there I feel like
there's a lot of tasks where you can't
measure performance unless you
deploy um like for example uh how can
you measure a robot's ability to
accomplish a physical task unless you
put it in the real world um but I think
the the main reason why they say this is
because they want to evaluate uh models
in a safe Manner and there's certain
tasks that it would be unsafe to deploy
in order to measure performance so I
think this is more of a safety concern
than anything else
um
yeah
okay but yeah that that this one four
was the most contentious in my opinion I
was like can we really measure
performance like intelligence without
deployment I don't
know um okay how are we doing on time
we're doing
okay uh so levels of AGI this is uh I'll
quickly go over this stuff um so they
talk about refers they talk about
looking at per measuring intelligence in
terms of two things performance and
generality performance refers to the
depth of an ai's ability to accomplish a
task and generality incl refers to the
breadth of an ai's
capabilities um and then they give some
good examples which I thought I would
bring here like as of writing the uh
writing in this in September 2023 from
Frontier language models like chat GPT
barred llama 2 exhibit competent
performance on some tasks EG short essay
writing simple coding but they are are
still and as emergent at emergent
performance levels for most tasks so I
think we when you think about
um like AI let's go to this table
because I think this is where you can
see they give you
examples we we we I think there's a
little bit of a mismatch because we in
terms of how we perceive a lot of models
because we perceive them as confident
when in fact they're emerging because we
see examples online them performing
competently but that's just at a
specific example of a specific task as
opposed to more generally so they give
some examples of where they think things
are at so level zero here is no AI which
is like a calculator software or
compiler in terms of and in terms of
General non aai it's like human in thee
Loop Computing like Amazon Turk
level one is emerging they give some
like older and so again we were we
talked about narrow AI versus emerging
AGI so General AI they put chat GPT and
all the language models in the emerging
category which means equal to or
somewhat better than an unskilled human
that's how they would say that these
models what these models are good at in
a general sense um but not in a narrow
sense they probably are better at
certain things like they said short
essay writing and for competent so like
at least 5050 percentile of skilled
adults they say they say we haven't
achieved competent AGI yet but we have
achieved it uh narrow AG narrow AI
That's competent and if you you keep
going down so like expert level narrow
AI is like
grammarly um or dolly2 so that's better
than 99% of people virtuoso is deep blue
and Alpha
go um and then superhuman funny enough
are all deep this is deep mine paper
like deep mine models they have like
Alpha fold and Alpha zero as like super
human models that like they they do
better than 100% of people um so those
just give you an idea of like the
different levels so right now they argue
we only are have emergent AGI but not
competent but we do have some superhuman
narrow AI like Alpha and Alpha zero oh
stockfish is also considered which isn't
deep mind uh super
human so anyway I thought that was
another kind of contentious Point here
because I think a lot of people like as
I said before there's an article that
says that chat GPT has uh real has
become an artificial general is an
artificial general
intelligence um probably akin to like
super
intelligence
um H one thing I wanted to point out
here in case anyone's looking for
contractor work is that they point out
that there's a an existing Marketplace
called prompt base in which skilled
prompt Engineers sell prompts so if
you're ever looking for some part-time
work maybe check that out because I'm
sure a lot of us here have done some
prompt Engineering in the past I just
wanted to point that out because I
thought that was
interesting okay I'm gonna actually skip
through testing for
AGI uh because I want to make sure that
we have some good amount of time to talk
about risks and autonomy risks autonomy
and interaction and maybe we'll go back
to
that
so um they they start off this uh talk
about risks and AG with AGI is saying we
often talk about like existential risks
I like the dis destruction of humanity
when we're thinking about AGI but they
argue again that we should think about
it in terms of levels and that a
different levels and different
capabilities
of um artificial intelligence different
risks are
possible um and also that this way of
thinking about it can uh really help you
kind of zero in and have a meaningful
discussion about um about uh artificial
general
intelligence um I actually will just
skip to the table to show you guys this
because I think that's the best place to
look at it I'm just looking at my
notes um I hope I explained this table
correctly because I thought it was
really interesting again there there's
like two levels there there's two two
thoughts about um when you're thinking
about risks one is like how much the the
AI can interact with
the the
world and then um the second is like
what level of intelligence is there
because you need those those two in
combination can lead to different risks
so for example um in no when there's no
AI in a system so humans do everything
obviously there's no risks from that
Ai and that's like sketching with a P
with paper or like non-digital nonaa
digital workflows like typing in a text
editor and drawing in a paint program
then the first level of interaction is a
as a tool where humans fully control the
task and use AI to automate mundane
subtasks like information seeking with
Aid of search engines revising writing
with Aid of grammar checking programs
and reading a sign with the help of
machine learning translator app so they
say that it's possible to achieve this
with emerging narrow but likely to
achieve it with competent narrow AI so
they're saying like if we had competent
narrow AI then um
then we could uh Pro we we would
probably unlock this level one of
autonomy with using Ai and it would uh
and then they say that the risks are
deskilling um and disruption of
established
Industries and I think that's totally
true especially like if you think of
revising writing with the aid of a
grammar checking program um I'm I'm half
Greek and I have quite a few Greek
friends and I message them in Greek but
there's like a spell check on it and I
found that when I'm like writing in
Greek without that spell check now
because I don't speak Greek that much
outside of there I often make mistakes
in spelling so I definitely think that
this like Des Skilling occurs when you
have ai as a
tool um okay so the SEC the the second
level is AI as a
consultant um and ex it says AI takes on
a substantial role but only when by a
human invoked by a human so this would
be relying on a language model to
summarize a set of documents
accelerating code computer programming
with code generating
models and consuming most entertainment
via a sophisticated recommendation
system which you know all of these
things happen right now so this is this
is like we're using AI as a consultant
at least in a general sense for these
things um and they they point out that
some of the risks are like over trust so
we trust that what's coming out of the
model is correct when it isn't which
there's a lot of great examples of that
if you guys want to search like funny
ones like uh um like AI making up legal
cases and stuff like that radicalization
yeah this is a scary one that's uh uh um
we see in a lot of recommender engines
that they like try to push people
towards
radical uh videos with radical views
because people with radical views are
like tend to consume more content or and
are more
um and and are like yeah more
consistently consumed content and then
also it can lead to targeted
manipulation like uh you know
adversarial attacks that can lead to
probably these other two things that's
kind of like where we are at
now and then the level three is AI as a
collaborator so co-equal human AI
collaboration interactively coordination
of goals and
tasks um this would be I'm gon let me
move a little bit I think I might be
here
um hey guys um so that would be training
as a chess player through interaction
with or analysis of Chess playing Ai and
then entertainment via social
interactions with AI generated
personalities so actually both of these
things also happen now um
and they're saying that the risks are
anthropomorphization like parasocial
relationships and then rapid societal
change given that these things happen
now I was thinking like what rapid
societal change I mean I'm sure that is
going on but it's hard for me to
identify maybe because rapid is at a
time scale that's hard for people
because I think
people like we we struggle to see things
that change that don't change
immediately like if something changes
slowly it's hard for us to notice and
even though there's rapid societal
change happening around us right now
it's probably happening at a speed
that's slower than what makes it easiest
easy for us to
identify so I was wondering if any of
you have do you guys have you guys
thought of
any like rapid societal change because
of AI that's going on right now the
question so like within the
context of I mean it's it's probably
slower but I I would think do you see
how students know
ABS school what their level of thinking
is after because I could
imagine to
encounter I'm like from your side what
you think
yeah that makes
sense just GNA we're getting a bit of
noise for your
channel
okay I'm gonna go stand outside then and
and present just so I don't lots of
meetings going on in the
office one
second sorry for all the back and forth
everyone okay this should be
good um cool
uh you have to think about it I think I
do think that like llms right now are
being used in this sense and they
are causing rapid societal change but
anyway um AI as an expert is level four
yeah like like I with like spelling
correction how how has spelling changed
I think there was was an example where
uh the the the rate of of of people who
can't read has been rising recently just
because people can
essentially uh re um how do I say their
the text can be be translated to to to
to spoken language right and and and
surprising number of people seem to
depend on that and and and so that that
might one one way where where artificial
intelligence Al be very limitedly has
has changed some some part of
society yeah that's a good point
so in in the use of basic skills I guess
that that's over here Des Skilling is
one of them but it is a rapid societal
change
um food for food for thought for sure
this next one I was like really I
thought this one was really interesting
uh autonomy level four where AI is
viewed as an expert so it says AI drives
interactions humans provide guidance and
feedback on on performance perform or
perform subtasks tasks so so this is
where the AI like an AI system is
guiding most things and the example they
give is using an AI system to advance
scientific discovery EG protein
folding
um and the the three different uh risks
that they show here are societal scale
Nu I think that's how you pronounce it
or Nui which means boredom I thought
that was an interesting one so it's
saying like it could lead to societal
scale boredom I guess because we don't
have anything to do Mass labor
displacement and decline of human
exceptional exceptionalism which is like
where we view humans
as exceptional to like the rest of the
the animal kingdom and that we're
special so on and I thought these two
risks were interesting ones because by
themselves they don't have like a I mean
they might have a negative psych
psychological impact on people but
yeah I thought they were interesting to
to point out as like the
risks like why is human exceptionalism
such an
important uh Viewpoint in our society I
don't
know because it's something that makes
us
human what because it's something that
makes us human in a way
yeah yeah I guess that's true but it's
also something that like say we find
alien life it would it would also be
something that would make us not as
because it's the idea that humans are
special because they're able to do more
than other things that's at least what I
when I was reading about human
exceptionalism um I think you oh yeah
thanks I think um just to Echo what you
guys were saying um in terms of um
impact on human exceptionalism I think
it's definitely um slanted towards the
service economy and like World um if you
you think about jobs five 500 150 years
ago you know very different jobs that we
do now in the Western World so I think
AI has impacted our modern con concept
of the work environment especially in
service
economy and Manufacturing in a way I
suppose robots are involved in
manufacturing but certainly modern work
environments
um and to build on that I think so this
is AI as an expert as um the decline of
human exceptionalism um it puts me in
mind of losing the like and it's kind of
tied to the deskilling but
if humans don't have the ability to
actually check some of the expert
systems and not be a to actually begin
to notice some of the things that they
would have caught if there had been
humans in the loop or if there's
defunding for significant organizations
in order to like have these done and not
being able to actually have those checks
and balances so I don't think it's so
much of the existential threat from that
point of view then but then it's kind of
really like um losing a little bit of um
maybe an overreliance in like a really
macro Sense on some
of yeah Fair
Point very interesting but uh also Di
yeah already have a little bit something
like this when when some online
Publications tried to do all their
articles being being AI generated and
that that I think went went quite
badly the other thing where where this
this might be an issue is like religion
right so so why why did religion fight
very hard the some of them that you know
did the Earth is the center of the
universe even though that is even less
meaningful than humans being the center
of
God's creation I think I think it could
either AI destroys religion or it might
be the the one uh safe haven for for
humans to sort of flee into I
feel Fair like something else it will
start to mean like something else about
us is makes us special like uh you know
we're made in God's image or something
like that so that's why we're special
yeah
um and then the last level level five is
AI as an agent and here um it's
autonomous AI powered personal
assistants which they say are not
unlocked yet and the two issues here are
like misalignment and that would be that
the like the the agent um is acting in a
way that's not in our best
interests uh and that could be a whole
Myriad out of things and then the
concentration of power um which is this
is more like we can view the development
of AGI as an arms race because the first
person to achieve it might have like a
substantial benefit that they'll be able
to take advantage of in order to like
maybe curtail other people's ability to
get AGI or also just like gain a lot of
power really quickly so that's a and
it's funny because we're starting to use
Ai and agent Frameworks um so this is
this is something to think about and
worry about um so I I think
that's it's a good point I mean it's a
little bit far in the future as well
because we don't have amazing AGI yet
but we are seeing smaller examples of it
being
used
um let's
see there was a couple other things that
I wanted to bring up there were some
like really
cool yeah
um there are some really cool things
that they brought up is like that
superhuman systems may be able to
perform an even broader generality of
tests than lower levels and they pointed
out some cool ones that could be
possible is like maybe uh like
artificial superhuman intelligence might
be able to like decode neural interfaces
so that they could read human
thoughts or they'd have like Oracle
abilities about predicting the future so
I was thinking it is that's another
interesting element of this is like
emergent capabilities that we couldn't
even think of happening I mean I guess
these are mentioned but I thought that
was cool and
then the other uh thing I wanted to
bring up before I open everything for
discussion let me just check time oh we
don't have that much time but is that um
there's this uh spin-off of Star Trek
called the Oro and in it there's a uh
Society of machines like AR that are
considered artificial intelligence and
what happened to them is that uh they
were developed by an like by an organic
life form for the same purposes that we
using machines for which is like to
automate things but at some point they
became so intelligent that they um
achieved Consciousness and they started
to question like why did they have to do
these things and then they became
subjugated because the society was so
reliant on AI that it couldn't deal with
the fact that all of a sudden they
basically had other people that they
were treating as
slaves um and then it created a whole
bunch of issues so I think like an
interesting um element we talking about
this is like we are talking about the
risks we talk a lot about the risks of
developing AGI to us but also like what
about the abuse of AGI if it achieves
Consciousness it's another interesting
element to all of this anyway that's
that's all I had to say
um um happy to talk about anything
anyone wants to talk about based on this
paper
Nowa you have time I have a question
that might not necessarily be related to
this paper but I think it'sit related to
kud 7 it's more about what do you think
about neur symbolic
AI as like one way to build AI
neuros symbolic
um
so I feel like there's
um to me neuros symbolic AI is an
example of more interpretable Ai and
that's really where I think I feel like
that's how at least this is a very
personal this is a personal opinion
that's how I think we should think about
these things there's like interpretable
Ai and non-interpretable AI like the
embeddings in a Transformer are not
interpretable and they are performing
some type of like information processing
and symbolic AI is just an interpretable
form of information processing um I
don't know which one will lead
to the like to to to artificial general
intelligence but I don't I also think
it's it's wrong
to I I think that's that's how I would
differentiate between the two and also
maybe I would say that when you
formalize your thoughts they tend to
lose some of the um in my opinion they
tend to lose some of the information
that existed in them before they were
formalized so making something in
writing something in symbolic form means
that you can be more explicit about its
definition but it also loses some of the
information that existed when it
wasn't symbolic that's my
opinion cool thanks yeah I wanted to
bring it up because I think the guide in
cas 7 like Marcus he was sort of like
odds spoken about neuros symbol I as one
way to build AI so just correspond yeah
there's a lot of people who have that
opinion that the only way to make
systems intelligent is if they're
interpretable but I don't agree with
that because a human brain most of how
it works is not interpretable to our
conscious mind but that's a personal
opinion you got three thumbs up for that
one
Peter cool um well if there's no I I'll
stop sharing and if there's no more
questions then maybe we'll stop there um
but a bit of a different paper but
brought up a lot of really interesting
opinions in my P in my opinion so um
thanks everyone for coming hopefully
we'll see you here next
week thank you thank you for reading and
presenting it's great thank you very
much thanks bye
for
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e e

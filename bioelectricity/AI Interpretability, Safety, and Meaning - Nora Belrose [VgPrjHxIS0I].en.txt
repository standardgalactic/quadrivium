broadly speaking Simplicity is a good
heris and the sort of literature on
Simplicity biases and deep learning does
tend to say like this is the explanation
for why models generalize right if they
didn't have any kind of Simplicity bias
and they just started out incredibly
complicated from the
beginning within phenomenology there is
different sort of ways of thinking about
what experience really is I think Edmund
hural might have been more close to kind
of an idealist perspective he was famous
for this idea of the Epic
where basically you're looking at your
experience you're describing it but you
try to refrain from assuming that your
experience is reflective of some
objective reality other phenomenologists
like haiger or mego ponti are usually
seen as as being less idealist
especially me ponti whereas a lot of
people when they think about idealism
they'll say that like well your
experience doesn't actually include
people Windows objects because that's an
interpretation of your experience your
actual experience is just like colors
and like raw sounds like uninterpreted
raw feels and they'll say that's the
thing that's real the interpretation is
like kind of fake or something meon
wants to reject that because he says
like okay look you know the very idea
that you're seeing raw colors separated
from the objects that have the colors is
this kind of post talk abstraction that
you get from philosophical thinking but
really what what's there in your
experience is just like objects that
have properties like colors and stuff
but like the object is there and you you
kind of experience it
directly do you want to run llama
efficiently on smaller gpus what if you
could run both training and inference on
the same GPU with Cent ml's breakthrough
optimization technology you can maximize
Hardware utilization and slash AI
computation costs running llms at scale
shouldn't break the bank Cent ml's
intelligent optimization platform helps
Enterprises deploy AI models with
maximum performance at minimum cost
experience the difference noro welcome
to mlst it's amazing to have you here uh
yeah I'm glad to be here yeah it's
amazing to meet you can you tell us
about
yourself yeah so um I'm Nora I'm the
head of the uh interpretability research
team at um alther AI um we a profit AI
research
organization um that got started just a
couple years ago um we started out as a
as a Discord server we still do a lot of
our research kind of out in the open on
Discord um and yeah so that's that's
what I do um in my day job and I'm here
at icml um to present my most recent
paper what are your main research
interests so there's like a few
different topics that U me and my my
team um you know I I collaborate with um
a few other people at Al Luther um are
interested in um I guess one of our
research interests is um concept eraser
and concept editing um concept eraser um
is kind of a a set of tools that can be
used for a few different purposes um in
deep learning um so one application of
concept eraser um is fairness and bias
reduction um we all know that um models
like language models often pick up on um
kind of the biases uh kind of harmful
biases about protected uh minorities um
in their training data um and we often
want to try to mitigate those biases
there's a question of how to do that um
with Concept erer what you're trying to
do is you're trying to look at the
internal representations of the network
um um and remove information about the
target concept so it might be race or
gender or something you know entirely
different like part of speech but you're
trying to get rid of um this targeted
information in the representation while
kind of keeping all the other kind of
benign information uh present in the
representation um and there were some uh
there was this's kind of a pre-existing
literature that goes back um like a few
years before we got into got into it um
but just last year we put out a paper
called uh lease lease squares concept
eraser um that introduces a new um way
of doing concept eraser that has some
nice mathematical guarantees um so what
does it mean to eras a
concept exactly so so that is um a
somewhat trick tricky question um the
way that the literature has kind of
chosen to operationalize this idea is to
say okay look um we're going to measure
the amount of information about our
Target concept by basically training a
probe so it could be a linear classifier
um for example on the representation to
try to predict um the concept you know
race gender part of speech whatever it
is um and if it turns out that the uh
classifier is unable to uh you know
predict the target concept better than
chance it's just you know predicting
50/50 on on every input or something
like that then you can say okay there's
no at least there's no linearly
available information about uh the
concept and the representation there's
always a concern that well maybe your
classifier isn't strong enough you know
maybe a a more uh expressive or stronger
classifier would be able to extract the
information um but you kind of have to
start somewhere and so that's that's the
approach that we take in the lease paper
as well we we consider the the linear
concept eraser regime where you're
trying to make sure that no linear
classifier can extract information about
the target concept so what's the
backstory here I mean how did you start
all of this yeah so um you know I wasn't
initially kind of interested in in
concept eraser kind of for its own sake
um me and my collaborator Alex um work
on on a totally different project um
where kind of as a problem in that
project we wanted to kind of remove uh
information about a particular concept
um in that case it was not actually
about fairness um and so you know for
that project I did some kind of
literature riew I lo looked into like
the existing um Methods at that time for
concept eraser um and one of them was
called race so that stands for uh
relaxed linear adversarial concept
eraser
um and it's you know RL is a pretty cool
approach actually um basically the idea
is that you've got this linear
classifier and you also have an
orthogonal projection Matrix um this is
a slight simplification but this is
basically what's happening so you've got
the the classifier you've got the
projection Matrix and you're
simultaneously optimizing both of them
in kind of an adversarial setup sort of
like a gan scenario if you're familiar
with that um so you're optimizing the
class ifier to try to predict the um
concept from the representation but
you're also optimizing the um projection
Matrix to like maximize the classifier's
loss um and you're taking you're going
to take like one step on the classifier
then one step on the projection Matrix
back and forth and eventually hopefully
um you'll reach a fixed Point um and so
it's this kind of adversarial game um
and so that does work pry well um but it
does have some problems in particular
it's like a very slow and kind of tricky
to get it to converge like sometimes
it'll get get in circles and stuff like
that um and I wanted to um basically
speed up uh this concept eraser
technique um for my own uh purposes and
so I looked at this other paper um
called spectral attribute removal this
is a different concept erti technique um
where
they so what they do is they compute the
cross covariance Matrix between your
representation um which we let's let's
call it a vector X okay X is your
representation that you're getting from
your neural network or whatever it is
and then Z is like some other Vector
that's representing you know whatever
concept you care about um and we're
Computing the cross covariance Matrix so
like each entry of that Matrix is like a
a coari between the like entries of X
and the entries of of Z okay so we have
this Matrix um and then you're going to
do SVD on that Matrix to kind of
basically to find the kind of directions
of Maximum correlation between your
representation and this concept and then
they just do a projection to like remove
um those kind of directions of of
Maximum correlation um so it's it's a
very simple technique um and it's fast
you know SPD is fast to compute
um and so what I did was I was like okay
I'm going to use S um this method as an
initialization for our lace so I'm going
to I'm going to do s to get the
projection Matrix and then I'm going to
run our lace with that initialization
and it turned out that if you use that
initialization our lace like it just
immediately converges like you don't
like like like basically if you start
out with this initialization from s the
classifier just cannot do better than
chance at predicting the concept just
immediately there's no like additional
optimization steps that you need to do
um and I was like what like this is like
this is crazy like and I didn't it
wasn't immediately off like like I was
like okay I don't know why this is
happening but there must be some like
mathematical reason like some proof that
you could give that like actually s and
arace are like the same thing or like
something like that and so I actually
tweeted about it I was like or I I
didn't tweet this like exact I didn't
tweet about like a s and arace but I
tweeted kind of about this
mathematical
uh problem um and then uh one of my
Twitter followers um David Schneider
Joseph um shout out if you're watching
um responded and he suggested like a
proof to kind of connect s and arace
together basically um and
then basically we started talking and
kind of one thing led to another and we
like produced some more proofs um
so we realized okay there's
this um there's this close mathematical
connection actually there there's a
mathematical equivalence that you can
prove between linear guardedness so
that's the technical term for um when a
representation well I should back up
when a a linear classifier cannot do
better than chance at predicting a
concept when that is true then the
representation is linearly guarded for
this concept okay so let's consider um
some concept that has like two possible
values um it doesn't matter what the
concept is um and you can compute the um
average
representation of
the basically the average representation
where the concept takes the value zero
and the average representation where the
concept takes the value one these are
kind of the centroids of the two classes
um and it turns out that um you have
linear
guardedness uh if and only if the
centroids are equal so in the mean
representation are equal for the two
classes then you have linear guardedness
um that's what we were able to prove um
and then we went further and we actually
derived um the least squares solution um
for a kind of transformation that
guarantees linear guardedness so
basically we have this this closed form
formula that you can write down on a
t-shirt um that
uh kind of transforms the representation
so that the uh means of the classes are
equal and therefore you have linear
guardedness and it is changing the
representation as little as possible we
call that
surgical um and that's that is what
leases why is a closed form solution
good yeah so closed form solution just
means that you don't have to do kind of
um gradient descent or like some like uh
compute heavy optimization to try try to
to to find the solution um you do have
to do SBD as part of it but that's
that's pretty fast um
and uh yeah it's just you know you can
compute it and you know you have a proof
that it will be the optimal
solution so there was a great figure in
the paper where you talking from a high
level what the process is can you talk
us through that figure and we'll show it
on the screen yeah right so um there's
basically the the way to think about
this is what leas is doing is it's first
whitening the
representation now now what do we mean
mean by whitening okay so if you imagine
the data as kind of a cloud of points
okay um the cloud um might start out as
like a a perfect kind of spherical shape
where it's like the kind of same like
variance in all directions but usually
your data is not going to look like that
usually it's going to be kind of like
some weird ellipsoid or maybe there's
like multiple clusters all over you know
you're going to have different amounts
of variance in different
directions um and what whitening does is
it makes sure that in every direction
you know the x-axis the y axis the the
z-axis you know in any direction you
choose the amount of variance is
precisely equal um so that's kind of a
first pre-processing step that lease is
doing um and then once you do that um it
does an orthogonal projection to kind of
squash the data onto a hyper plane which
ensures that the means are equal
basically if you have like two classes
you can imagine like a a cloud of points
over here and a cloud of points over
here here's like centroid one here's
centroid two you're looking at the
difference between these two centroids
the kind of like line that connects the
two centroids and you're uh smashing all
of the data onto the hyperplane that is
normal to this line um so that's what
you're doing and then you undo the
whitening at the first step and that
that's what lease is doing so by
projecting onto this hyperplane you're
making sure the means are the same
therefore you've scrubbed the concept
yeah exactly um and you know you can't
like a slightly simpler thing than lease
is to forget about the whitening and un
whitening step and just directly do the
orthogonal projection onto this to this
hyper plane that also works and we we
prove that that does give you linear
guardedness but it is not surgical in
the sense that you are kind of changing
the representation more than you need to
and um you know we think this is
important for a couple reasons so um you
know in the just in general if you're
changing like anytime you change the
representation of your neural network
you're probably going to reduce its
performance to some extent because like
it's been optimized by SGD to like do as
well as it can on your on your task um
and so you want to be very careful with
changing anything and there's sort of a
different reason for for wanting
surgical so
um one use case for concept erer that we
haven't talked about yet is just
interpretability research itself so um
in the lease paper we actually um do
this experiment where we look at um how
much do language models depend on or use
a part of speech information to make
their next token
predictions um and with Concept AR ratio
you can actually kind of operationalize
and formalize this question because you
can say okay we're going to use lease to
remove the linearly available
information about part of speech in all
of the layers of the network so we're
going to go in to every layer um and
change and apply lease to the
intermediate representation um and then
we're going to like run the the forward
pass that way um so we're kind of
inserting lease into the forward path at
every layer and for this type of kind of
question you do want surgical because
you want to you want to kind of remove
the part of speech information while
keeping everything the else the same or
or at least as similar as possible to
like what you started with right so
you're kind of isolating the effective
part of speech um and interestingly we
we do find that of course um when you do
this to language models and we looked at
uh llama 2 and uh the Pia series um it
obviously does increase the loss um next
token prediction loss quite a bit but it
actually like models are still able to
predict the next token like way better
than chance um or way better than um the
kind of
like like Baseline entropy of like uh
what's called unigram entropy um it's
kind of a a baseline um for next token
prediction they still do better than
unigram entropy when you do this um so
it's it's kind of they're using the part
of speech information but they're like
also robust enough to like rely on other
cues when you remove that that info can
you tell us about the setup of how this
thing is trained so there are these one
hot vectors which represent different
concepts
and do you see that as a potential form
of brittleness I mean how were those
Concepts labeled and and how how were
they kind of trained into the
model um yeah so I think this is um you
know one potential problem with applying
concept ratio is just that you do need
some source of of labeled data to kind
of Define what your concept even is um
you know in in the case of of part of
the the part of speech experiment
we used um this uh kind of commonly used
NLP Library called Spacey they have
their own uh fine-tuned Transformers
that uh can can do part of speech
labeling so we just applied that to um
to the pile data set and got labels that
way um but yeah if there's like um if
your labels are kind of incorrect um you
might not
be you know at least might not do
exactly what you think it it should be
doing like if your labels are wrong but
I think that's that's kind of true in
general with with you know machine
learning right you want to make sure
that your labels are as accurate as
possible yeah really interesting the
other thing is that this is a post talk
method which is to say you have a frozen
base model and then you can apply it
sequentially through the layers and it
can be done quite efficiently I think
you said in your paper so you could
potentially do it in a streaming fashion
and and do it really quickly but would
you ever consider using it as a method
to kind of scrub Concepts out of the
base model almost like a fine-tuning
type system yeah so if you want to take
a a model that's already been trained
and then apply lease to it um you can
actually kind of burn the lease into the
weights um in a very sim a way that's
like very similar to how Laura does it
so Laura stands for low rank adaptation
um it's a parameter efficient
fine-tuning method um and you can you
can do a very similar thing with lease
because it turns out that if you look at
the the kind of lease solution um it's
actually a low rank perturbation of an
identity Matrix um and
so yeah you can just do a little bit of
algebra and it turns out you can like
just do this like low rank update update
to the um to the weights and like put
lease into the the model post talk
there's also another thing that you can
do which we have played
around with a little bit but it's we
don't have like um good experimental
results um on it yet which is just to
apply lease kind of in a streaming
fashion during training like from the
beginning of training you're just like
applying lease like and and you're also
updating the lease um eraser we call it
an eraser um the lease transformation um
you know after every training step or
every few training steps to kind of like
keep up with the model representation as
it as it trains and this is something
you can do um you know but like I said
it's it's kind of early days for that
and it's unclear if that gives you like
a big boost over just doing post talk
what kind of effect does it have on um
headline Benchmark accuracy because I
suppose what we're doing is we're
removing some forms of statistical
information from the model does it have
a dramatic effect or what what have you
seen yeah so it it does it
depends a lot on the the type of concept
that you're erasing right so um you know
in the paper in with the the part of
speech experiment know we were really
targeting a concept that we had reason
to think like would affect performance
right and we wanted to see how big that
effect was um and you know that was a
substantial effect I mean I I would
forget the numbers exactly but I think
in some cases we were like doubling the
perplexity of the model or something
like that um and that's but if you're
looking at um a different type of
concept you know like gender or or
something else um you know
we like I we don't have like uh kind of
very extensive results on this but like
we tend to find that it's it's not
affecting performance a whole lot um
just because it is it is a surgical
modification and you know you're only
you're only erasing like one kind of
Dimension out of the like thousands of
Dimensions that exist in the um in the
representation yeah because I think
we're going to get on to talking about
your stats paper and there's some
interesting results there that the
neuron networks given enough computation
can start to learn some incredibly
bizarre and interesting statistical
features does that in any way negate
your work here sorry does it negate the
the lease work or well yeah so if you
surgically remove Concepts that we know
about but the neuron networks still have
this uncanny ability to learn esoteric
weird statistical proxies all over the
place at high frequencies and so on um
you know does does that imply that when
we train these huge neural networks that
it's almost a difficult thing to do to
kind of surgically remove what we
understand to be Concepts there're
definitely limitations to to
leas um and you know we we have seen
that so I mean the the big limitation of
lease is just that we are removing
linearly available information but
obviously you know deep neural networks
are non are
nonlinear um you know there there's some
kind of interpretability research
suggesting that well even though
networks are nonlinear they it seems
like um they do use linear
representations in many cases but
nevertheless like take CFR 10 okay this
is an actual experiment that we did um
and you um you're treating the images
themselves as like a target of concept
eraser um and you're just erasing the
class uh as a concept from the images
and then you try to train uh and you do
lease for that um and then you try to
train
a uh a model on top of those images um
models can still learn to classify the
images right like it it it you know it
doesn't matter that you've removed the
linear information there's still higher
order information there and like they
can find that um without too much
trouble so the the hope with Concept
eraser is I I guess it's it's kind of
twofold one is um if you're targeting a
a concept that is not like super
essential to the model's performance
like it's it's kind of helpful but like
and so it does learn it by default but
like um you know it's kind of an
optional feature then you know there
there's the hope that like removing the
linear available information will
actually affect the model's behavior and
it will not rely on that feature as much
as it would by default um that's one
thing um the other thing is just
that um honestly I kind of forget what
the second thing was supposed to be
that's okay I mean because a thought is
that leasees applied to every single you
know like a a neural network is a whole
bunch of nested kind of Matrix
Transformers and then a nonlinearity and
you apply it sequentially to every
single linear component of the network
and I suppose you you know I'm just
being a bit naive here but you might
think that that somehow does the
nonlinear thing because you're applying
it throughout the network but what you
went on to um do was you know like a
higher order version called q leas and
and you also did some work looking at
higher order information that is learned
in in these networks so but what you're
saying is that the networks still still
learn this higher order information and
that can still learn some of these
Concepts even though we've run lease on
it yeah exactly so you know
there's there's kind of a hope that if
you erase lower order information like
um linear linearly available information
or we might talk about this in a bit
quadratically available information kind
of second order information um you know
it's it's still possible for the model
to to learn um the concept using like
third fourth fifth order uh statistics
but it's just going to be a bit harder
and the model May rely on this
information less than if you did nothing
that's kind of the idea so you made a
quadratic version of lease yeah right so
Q lease is kind of a follow-up to lease
where we were like okay we want to make
a form of concept ratio that is more um
essentially um and the way that we
operationalized this is we were like
okay we want to prevent not only linear
classifiers from extracting information
about our Target concept we want to
prevent quadratic classifiers so
classifiers whose kind of output like
logits are um just quadratic functions
of the input um and it turns out you can
like do some math uh that is equivalent
to making the means and coari
matrices of your classes uh equal um and
we did some more math and showed that um
you can kind of achieve this equality of
means and covariance matrices using
tools from optimal transport Theory um
and yeah so we we like derived um some
more closed form solutions for c um and
we started to do experiments with c
um in particular one experiment that we
did was we looked at cfr1 um so we're
treating the images as just kind of
representations themselves um and we're
trying to remove the concept of the
class label um from the images um and if
you apply just normal lease to this it
doesn't really have much of an effect at
all um like models can still learn to
classify the images um after lease uh
very easily um it turns out if you apply
Q Le to the c410 images and your
classifier is small so it's like two or
three maybe four layers like an MLP
especially um then uh it actually can't
learn anything or at least in our
experiments which we haven't published
yet we we still want to like yeah um but
at least in our experiments we did some
hyper parameter tuning for this uh we
were unable to get these kind of small
classifiers to learn anything after we
appli applied Q Le so we started to get
excited about this but there are some
like big caveats here because it turns
out that if you look at larger
classifiers and and if you look at you
know a
large uh con like convolution on their
own network like a you know a reset 50
or or larger um and then you you try to
train that thing on these q-east images
um it actually backfires now what do I
mean by that well it it
basically it turns out that it's
basically a artifact of how we derived Q
in the first place so with Q lease
unlike with um normal lease you have to
kind of look at the kind of value of the
concept at inference time in order to
apply Q Le so you need to like like when
you're applying Q Le to an image you
need to like know okay this is actually
an airplane and then and then you like
use that information to choose what
transformation to apply to it the
problem is that when you're kind of
peing at the label um this actually kind
of leaks information about the class
label into the higher order statistics
the third fourth fifth Etc statistics
and models if they're deep enough are
able to pick up on this signal um and so
you can get this backfiring effect where
like you think you're trying to make the
concept less Salient or like harder to
to learn but you're actually making it
easier to learn um and
so we kind
of that's why we haven't like done a cie
paper per se um and we would be you know
C has some applications but they're kind
of niched we you know we we really think
you should be careful if you want to use
c um but these experiments into kind of
cfr1 and like doing these um these
Transformations on C10 images um led us
into this kind of new direction of
research that led into our icml paper
for this year um I'd be happy to talk
about that as well yeah just a quick
thought on that you know in the olden
days of interpretability we used to talk
about values and lime and all of this
kind of stuff and it seemed infinitely
tractable that we could understand what
a model is doing and we could manipulate
it and what what you seem to be saying
is that when models just become really
big and complex they they just become
inscrutable monsters and all of our
efforts get resisted because they always
find a way to do what they want to
do um well that that is definitely one
way of thinking about it I I tend to be
a bit more more optimistic than that but
I do think you know it is true that if
you're you know uh gradient descent is
like a very powerful Optimizer and if
you are trying to kind of directly go
against gradient descent and like
prevent gradient descent from
accomplishing something when there's
just like a very strong you know kind of
you know when you're you're trying to
prevent gradiant descent from like
reducing the loss like you're probably
going to to lose that battle um
especially if you are not applying kind
of an equal amount of optimization power
yourself that makes any sense um so yeah
I think that's that's part of the lesson
from the the CES thing it's almost like
another bitter lesson but an
interpretability B
lesson yeah right I think you know some
other uh interpretability people have
pointed this out but you you probably
don't want to like optimize against
interpretability um methods or at least
you want to be like very careful and you
probably don't want to like directly
optimize some measure of like the
interpretability of your model because
it could just like end up learning
something that's like totally different
from what you expect so snatch
paper right um so um there's kind of
this big literature um that you know
already existed before we did this paper
on um kind of Simplicity biases in deep
learning so the general idea is just
that you know when you randomly
initialize an Earl Network it starts out
as a quote unquote simple function now
the question is like okay what is
Simplicity what are we talking about
here there's a lot of different Notions
of Simplicity um but you know
intuitively
like most randomly initialized networks
are going to be more simple than the uh
the network that you get after training
um and then you know you start out
simple initialization and you kind of
gradually get more and more complex
that's that's the basic idea and there's
many different papers kind of trying to
flesh out how exactly this works you
know in what sense is the model simple
and getting more complex um and our
paper is sort of a an additional
contribution to this literature so we
were looking at it from the perspective
of
statistical uh information um so like in
in statistics there's a a concept of a
moment so the the mean of a distribution
is called the first moment um and then
the variance and co-variance um between
the different components of your of your
data are the second moment and then you
can talk about like third order
interactions between the components of
your data it's like uh a third moment
and so forth
um and basically our hypothesis which
kind of came from some of these C
experiments um was that models learn to
kind of exploit or use these statistics
um these moments in order so kind of
early in training um the kind of
predictions of the model are primarily
going to depend on the kind of first
order moment or or just the mean um of
the of the distribution of the data and
then it's going to start kind of
depending more and more on these kind of
simple correlations these uh you know
the the co-variance between the
different components of the data and
then it's going to start using third and
fourth order um kind of uh information
later on in training
and the the way that we actually um kind
of can do this hypothesis specifically
is
we we
used um what's called optimal transport
Theory to take CFR 10 images from one
class and kind of modify them so that
their mean and covariance Matrix match
the mean and covariance Matrix of a
different class um and you can do that
you can like use this closed form
formula from optimal transport Theory to
do this um in a way that kind of
keeps the images as that that basically
changes the images as little as possible
it's very similar to lease and it's kind
of a a surgical edit um to the images
that like just changes their mean and
co-variance and then keeps everything
else as similar as possible um and if
you look at the images which you should
definitely look at um they you know you
can barely tell the difference like
before and after like you know there's
in our paper we've got an image of like
an ostrich um and we like change it to
be an airplane and change it to be a
deer and change it to be a frog and it's
you know you can see a little bit of
differences in the background but it's
like almost the same image um so to a
human this is like not changing hardly
anything but it turns out that if you do
this transformation
to models early in training in the first
you know few thousand steps or so they
get fooled basically so like they're
going to you know early in training um
image classifiers are you know very
likely to classify an ostrich that has
been edited
to look like an airplane from the
perspective of second order statistics
um they're very likely to to just
classify it as an airplane and we
quantify this um in the paper yes I I
was going to ask why you can make these
modifications and to the you know to the
human ey it still looks the same and I'm
an experienced video editor and I know
that you can modify the distribution of
values you know like it might be
recorded in RGB format and you can
squash and you can translate and you can
kind of you know move things all around
uh you know you can change the mean and
Visually it still looks the same you
know maybe you just changed the
luminance values or something like that
so the weird thing is is that you can
make it look as if it's a different
thing to a machine learning algorithm
but from a human perspective it still
looks the same yeah exactly um and you
know people have kind of pointed this
out before I mean there's the whole
literature on adversarial examples um
where you can just change a few pixels
in an image and it completely changes
the class um I think I do think that
there's like a different mechanism going
on here um it's not just because for
example we're not we're not actually
optimizing against the network um so
it's it's not adversarial in that sense
but it is showing that especially early
in training these networks are sensitive
to you know simple features that humans
are kind of much less dependent on yeah
that's fascinating so there's this
unraveling of complexity to use GPT
language that early on it kind of
focuses on on very simple things so it
might be looking at the statistical
moments and then as you continue to
train the network it starts looking at
increasingly kind of complex inscrutable
features
yeah exactly and and and we show that in
our paper um there's kind of this graph
where the xais is uh time just like the
the number of training steps the y- axis
is the accuracy where the accuracy is
measured um with respect to our kind of
Target labels when we're applying these
this this concept editing or our optimal
transport Theory so we're like so we're
like showing it the ostrich that's
supposed to be an airplane and saying
you should classify this as an airplane
and the accuracy gets to you know like
40% 50% in some cases at around like a
th training steps and then it starts to
go down right U I think at the end of
training it's still above 10% which is
kind of interesting um at least on Carr
10 um but like but yeah there's kind of
a
non-monotonic um process where it starts
at random then it gets kind of fooled by
our thing and then it gets smarter and
stops being fooled as much
yeah it's so interesting so simple
features are easy to guard against and
they're intelligible to humans and it
raises the question of do we actually
want neuron networks to learn very
complex features I mean would it would
it not be more ideal that we kind of we
guard them so that they can only learn
simple features that we understand and
you know ain's razor right on't simple
features more
robust yeah right so I think um
you know it's it's going to depend a lot
on both what you mean by simple and the
the kind of the task at hand um I do
think you know broadly speaking
Simplicity is like a good heris and like
the the sort of literature on um
Simplicity biases and deep learning does
tend to say like okay this is this is
the explanation for why models
generalize right like if if they didn't
have any kind of Simplicity bias and
they just started out incredibly comp
licated from from the beginning um they
would probably overfit or they would
just not do well at all um and so you
need some sort of inductive bias like
this um you know different there are
different ways to have inductive biases
but but you definitely need need
something and why is that what you said
is really interesting so it's almost
like it's an inductive prior that the
neural network starts learning simple
functions and then it almost branches
out into increasingly complex function
almost as if it wouldn't be possible to
get to the complex functions unless they
started with the simple ones but the
counter example is something like
grocking where they seem to make this
transient change you know into a
completely different type of function
I've looked into this a little bit I'm I
wouldn't say I'm like an expert on on
the gring literature but I I do think
people tend
to um tend to like overestimate how fast
grocking happens um there are some like
plots that
um uh I I don't I there like certain
papers where it like kind of there's
like a plot that's that that's kind of
demonstrating the grocking phenomenon
and it kind of looks like grocking is
happening really fast but actually you
you if you look carefully the x-axis is
on a log scale and so really the
grocking is happening like over you know
like at least half of training or
something like that um but yes you know
groning is an interesting case because
it's precisely because it's an exception
to the rule it's it's it's an exception
to the rule that you tend to start out
um start out simple and get more complex
in cases where grocking happens usually
um it's because there's something like
weight Decay or some other regularizer
that's being applied that is like you
know encouraging the the model to get to
kind of get simpler over time and uh
there was one paper that I read a few
months ago
that applied the neural network tangent
kernel to this in a way that seemed
pretty compelling to Me Maybe we can
like find it later um and put it in the
uh description but um it is an
interesting topic cool and just final
question on you know high frequency
features in in general I mean there was
a a great paper many years ago it might
be by whe and brindle talking about how
Vision models tend to overfit on
Textures so they they don't learn a cat
the way we do they they look at a
certain texture of catur and this makes
neural networks have really good
performance so there seems to be this
trade-off between well should we stop
the neural networks from overfitting on
cat
fo yeah I don't know I I guess in the
case of of image models I mean I I would
tend to think if an image classifier is
able to overfit on catfur as you call it
um and kind of focus on Textures and and
not on shapes and is still getting good
performance on Ben marks that kind of
suggests that maybe the benchmarks are
not as good as we might hope or or
they're or they're just kind of you know
I I I do think that like if you want to
build you know you know an autonomous
robot or something like that you
probably will need um a computer vision
system that has more of a shape bias and
is is a bit more robust than a lot of
these CNN that we've been training can
you distinguish meaning and value
because I I kind of bucket this
conversation in my mind in the broader
discussion around what is
value yeah I me they're definitely
closely related I guess maybe I think
meaning
is it is a bit more individual even
though it's it's about connect like
being connected to something bigger but
it it is kind of it it is sort of saying
like Okay like does this person have
meaning in their life um whereas value
is kind of a broader concept that isn't
necessarily like
individualistic that makes any sense how
is meaning related to purpose a lot of
people will talk about like the meaning
of life right um and you might kind of
rephrase that as like the purpose of
life meaning of life purpose of life
they seem like kind of similar um in
both cases you're sort
of looking
for
some something almost journal to life
perhaps or this life you know a lot of
people think that their meaning like the
meaning of life to them is like the
after life it's God it's something it's
something kind of Supernatural and
external to to this life of course not
everyone has that view but that's like a
common view um it's sort of
like if if you think that like life has
a purpose or a has a meaning you might
think that life is kind of instrumental
to something else it's like you know
this life is just a journey to get to
something else or whatever um whereas I
I tend to not like that type of view and
and I think when I I I think we should
not try to kind of make Life
instrumental to something else or
something external um in part because we
have no good reason in my opinion to
think that there is something external
um but even if there is something
external like I I think we should be
looking for meaning
in life itself we should be looking
to kind of live in such a way that we
can be satisfied and and and and Find
meaning just in in in the everyday in in
what we you know our day-to-day
interactions and and our our hobbies and
so forth and not and not because we
think it's all kind of culminating up to
something
greater you know in the future or for
future Generations even you know um
it's it's sort of a Zen um
view interesting so it sounds a bit like
you're saying meaning is related to
individual or perhaps Collective Joy
yeah I think it I think it it is related
to that although I I wouldn't want to
just reduce it to like feeling happy um
you know that's that's probably part of
it but it it's it's not just like an
emot State and what's the relationship
between meaning and good goodness I mean
I I would say that goodness is is
broader I mean goodness is a very it's a
very a very broad concept that's just
kind of pointing to like anything that
you think
is valuable anything that you kind of
feel motivated to promote or something
like that
um and so you know meaning is good by
definition
probably or like at least in the way
that I'm defining it like meaning is
good by definition but
like maybe there's like other things
that are also good um do you think a
sort of panglossian perfect simulation
machine you know like the um what do
they call it the experience machine is
that good yes that that's a really good
question I think it depends for me it
depends on whether there are like other
people in the experiments machine with
me basically if if we're like kind of if
there's like millions of people all
living in an experienced machine having
relationships with one another
um I I'm not necessarily opposed to that
um it I mean it might depend on the
details but in a way as we develop our
technology to kind of make our
environment more comfortable for for
ourselves and make like you know make it
easier for us to just exert less effort
to kind of like make things the way that
we like um we're kind of gradually
moving toward like a collective
experience machine right we're you know
um and then like virtual reality is like
obviously one step further in that
direction but like it's all kind of just
like yeah so I don't I'm not necessarily
approached to experience machine type
things but if it's like just me in an
experience machine and there are no
other people
in with me or like all the people there
are actually like just fake and um are
not actually like
autonomous conscious
individuals um themselves then I would
probably oppose
that what's the relationship between
meaning and Consciousness yeah so that's
a big question obviously and and it's
one that I've I've been thinking about a
lot um recently at least the way that
most people about Consciousness um you
know conscious if something is conscious
then
that at least kind of strongly suggests
that it
is it might have moral Worth or it might
you know because if something is
conscious then it
is probably capable of like pleasure and
suffering having like experiencing good
or or bad States Of Consciousness at
least like all other things being equal
like it's
like probably good to like help that
being um kind of experience better St
States Of Consciousness um I yeah I
don't necessarily kind of reduce all
goodness or value to States Of
Consciousness that's that's sort of the
utilitarian perspective um but I do
think that it is like a a big chunk of
what I value um yeah do you think you
need Consciousness in order to have
moral status well it may depend a little
bit on what you mean by Moral Moral
status um I would in be inclined to say
no just because like if I thought about
it there would probably be some counter
example or something something like that
I'm like hesitant to make like a blanket
statement but I guess maybe I'll just
say I mean like some people think that
like um you know nature itself like like
a mountain or or maybe a tree or or
something um could have moral status
without being ious um and I don't know
like I I think that's like at least
somewhat plausible I'm not sure my exact
view on that um but anyway do you think
there's more meaning in a globally
connected world or a locally connected
world and just to make the question not
completely obuse I mean you you could
become a big entrepreneur and you could
be like Bill Gates or something or you
could just have intrinsic value in your
local community and doing the garden and
stuff I don't know I guess I I tend to
think like
connectedness is good or something or
like Unity is good but I also yeah I I I
I don't have a strong intuition though
well do you think Echo Chambers are you
know like is it good to have a diverse
group of pockets of different people
doing their own meaning making or or do
are you more of a homogeneous person
yeah I mean I I definitely have like the
intuition that diversity is good and
actually it this kind of makes me think
of something that's just been on my mind
recently in in Vienna
because um of course Vienna is a you
know like a Germans speaking city but
English is like everywhere and actually
a lot of times people will just kind of
use English by default like even if they
like don't know where you're from
they'll just like use English um and I I
take it that like a whole lot of cities
these days that are not traditionally
english- speaking like are kind of like
that now and I feel conflicted about it
because it's like okay on one hand like
English is this like lingua franka and
it's enabling people to communicate with
each other and you know that's great but
I ALS there is part of me that's like I
don't know like if we go if we keep
going on this path and it's like is
anybody going going to be speaking
German in a 100 years like I'm not sure
and I feel like that it seems like
you're kind of losing something if that
happens but yeah yeah I mean um so we
should preserve you know local culture
situated knowledge yeah I I definitely
feel like that is is somewhat important
and I guess maybe one of my hopes for
the future is that with AI and you know
the Automation and abundance it'll
bring that people will be able to kind
of preserve and and enhance their own
kind of
like autonomy and local quirks and stuff
you know like
because it people just have more time
and and and kind of resources to just
keep doing useless things um you just
said something very interesting which
perhaps we should have highlighted
earlier you're an AI Optimist and you
said that it's going to bring abundance
what what should you mean by that I mean
I think that um you know in the next few
decades like AI
will come to be like at least as good or
better than humans at like basically all
the jobs that humans are currently doing
um and this will enable if things kind
of go right politically which they may
not but this will this will enable um
kind of a society of abundance for
everyone where you know we we don't
really have to work for a living whether
it's through like a a universal basic
income or just you know people have you
know Investments that are you know
enough to live on or whatever
um and yeah so I I
definitely I am hopeful that in many
ways the future will be like much better
than um today's world but I also I I do
want to recognize that like there's also
many ways that things could go wrong as
well and I think we're probably in for a
bumpy
ride interesting can can you expand on
that a little bit more so why do you
think that with we're on the path to
presumably agential super intelligence I
mean there's like a few different
arguments you could make I mean one of
them is just like super high level like
well technology is just continuing to
improve and like there's no like doesn't
seem like there's like a physical law
saying that you can't build super
intelligence um so like we're probably
eventually going to get there um and
then you can look at like progress you
know recent progress in Ai and it seems
like
um you know you can debate on like the
exact timelines I actually don't have
super precise timelines I wouldn't say
like oh within 3 years or what or within
20 years or whatever I'm not really sure
um
exactly how fast progress is going to be
um in the coming years but I don't I
guess I don't anticipate like a
super I I don't anticipate like a
plateau in like a really strong sense
like I think we're probably going to
continue I'm seeing AI progress and it
seems like if it continues just at
roughly the current rate or like
within an order of magnitude even um of
of the current rate um like we'll have
very powerful Ai and very versatile AI
like within my
lifetime just Devil's Advocate on that I
mean there are those who say that
current AI is basically a pseudo system
one and it's getting better because it's
just memorizing more and more of the
long Tale But you know reasoning which
is to say deriving new knowledge you
know kind of intrinsically in in in the
model to achieve a goal or something
like that I I do take issue to some
extent with the people who say you know
current AI doesn't reason period or
something like that I think part of the
issue is that it's sort of a
terminological question it's just like
well how do you define reasoning how do
you define planning like depending on
your definition like maybe they reason
maybe they don't reason um and I I'm not
sure if it's useful to like debate that
or I don't know to kind of go back and
forth on that terminological question I
guess I think even if you concede that
like oh there's
serious kind of barriers ahead where
like we're going to need to come up with
like a new some sort of a new
architecture new paradigm or something
in order to get you know system to
reasoning or something like that it's
still kind of hard for me to imagine
that that will delay progress so much
that like we it'll be like 2100 and we
still don't have like AGI by whatever
definition you you care to use I think
that's like the the thing that I'm most
confident about like like at the longest
like if it's if it's 2100 and we still
don't have AGI by like anyone's like
like everyone's definition then I'm like
very surprised I think it's probably
earlier than that but like yeah um and
if it's 2100 I still feel like that is
soon enough to like be thinking a lot
about um and like I said it's probably
it's probably earlier but and what is
your definition of
AGR right so I I
don't maybe I shouldn't even have even
brought brought up the term because I I
I tend not to like the the term just
because it's kind of vague and Broad and
people have different
definitions um I mean maybe the
definition that
I kind
of like the most just
because it's broad is just like an AI
that can do many tasks or something like
that and like where and so like
generality is just like this Continuum
and you can just have AIS that like do
more and more tasks and then by this
definition like gp4 is already and AGI
it's like General in the sense that it
does many tasks it doesn't do all the
tasks that humans do obviously obviously
but it does many many tasks um and so
it's a general in that sense it's kind
of a deflationary definition which is
why it and it might even like kind of
annoy some people cuz it's like it's
it's clearly not not what like other
people mean when they say AGI but but
you would ConEd right now that there's
an it's inefficient there's an insane
amount of computation required but in
principle we might be able to design it
better yeah um I think that's true um
and there there's some interesting work
actually on like data efficient AI um
and i' I'd personally like to read more
into this because I've only read a
little bit but there was I believe like
a contest recently where where people
were kind of challenged to like um build
language models that are like as
efficient as as like a child or
something like that um and I don't think
they got all the way but like there
actually was a lot of progress and I
believe one of the top kind of um the
top uh methods in that contest was just
in part to use a lot of epo so like
currently like um with language models
we usually only do one Epoch on the
training set just because we can afford
to do that like there there's just so
many you know so much text on the
internet that like it's better to have
more data as opposed to like um less
data and then do more at boox on it but
if you do do more epox on um the same
training data like with some tweaks and
regularization and stuff stuff you can
get like a lot out of less data it it
seems so um yeah there's still work to
be done on that front I know you were a
fan of fory cognition yeah tell me about
that um yeah so it's still a thing that
I'm I'm learning more about but um you
know the the idea of fory cognition or
fory cognitive science is um the idea
that the mind is uh
enacted uh embodied extended and
embedded I definitely said that in like
the wrong order or something oh yeah
it's um the ecological so the the
extended one is the one that Charmers
and Andy Clark snuck in by the back dra
okay well right so I understand there's
like a debate about this whether
extended counts or or whether it's
ecological but I guess the way that I I
like the extended mind thesis um so I
tend to includeed in there um even
that's interesting isn't it because the
the extended version is still a form of
representationalism and computationalism
which the foure folks said that they
that they didn't like because they
fundamentally believe that you can't
simulate a living thing in silicone yeah
so there's a lot there I mean I guess
one thing I would say is I don't you
know maybe to for the audience like the
extended mind thesis is just the idea
that like the tools that we use like
computers or or notebooks or all sorts
of things that we use to kind of enhance
our cognition are like literally a part
of of our minds or like it's like useful
to view them as part of our minds um and
I guess for me it I don't see
why the extended mind thesis assumes
computationalism or representationalism
I think like you could be a
computationalist or representationalist
about cognition and accept it but like
it's you also don't need to believe
those things I have to be honest with
you nor I think I agree so I think we
might be strange because we are big fans
of the 5es yeah and we both believe that
you can talk about all of them in a
computationalist sense so um for example
we were just talking about um the guy
who wrote mind and life Evan Thompson so
he's an autoptic and activist tell us
about him yeah so Evan Thompson is is a
a pretty cool guy he's he's a
philosopher and um at least I'm not sure
if he calls himself a cogntive science I
but he like works with cognitive
scientists um and he has been kind of
behind a lot of these ideas of like
embodied cognition and and um and active
cognition Etc so me and Tim over the
past day or so have been talking about
this position that Evan has that
basically life is
inherently material so like you couldn't
have a living thing in a simulated
environment even if the simulated
environment were like very detailed for
example um
and um he he's got sort of another
argument for this position which is
basically that he thinks that
computation itself is Observer relative
um or that that you know computation you
know um algorithms simulations all of
this stuff um it's all sort of dependent
on some Observer
um or some some agent some living agent
I suppose who is using the computation
and kind of
uh interacting with the computer and and
using it for certain purposes and
thereby kind of imbuing the computation
with meaning Etc um and without the
Observer without the the living agent
you know computation is meaningless it's
not even computation like computation at
all and so you know based on that view
he's like okay look if you try to
simulate life in a computer um it's not
really life in like the full sense of
the word and it also wouldn't be
conscious it wouldn't have genuin it
wouldn't be sension it wouldn't gen gen
genuinely have feelings or anything like
that um
because
it's you know it's it's we who are kind
of like um
giving meaning to the simulation whereas
for Evan um life is kind of unique and
that it gives itself its own meaning so
like like genuine life you know in the
real Material World um is autopoetic it
creates itself it's like kind of
actively reproducing itself and thereby
creating meaning um and so you can't
have that in a simulation because the
simulation is always kind of by
definition giving having meaning given
to it from the outside
that's the argument I think both me and
Tim disagree with this or at least I I'm
I'm very doubtful of it um and I I guess
my main concern with the argument is
that so I I accept the idea that
computation is kind of observ a relative
I it gets meaning from like being used
by some agent but it sort of seems like
that's true for almost everything that
like the whole world are just like
material objects are sort of you know
we're we're always kind of interpreting
and kind
of you know and this is actually part of
Evan's kind of own philosophy of an
activism that like the
world and the mind are kind of
co-created together in this like um in
this like process of living um and so it
it's not really clear to me how he can
consistently have that an activist view
of about everything about the whole
world saying the whole world you know
the world is created by living things in
a certain sense but then kind of single
out computation and saying like well no
you know simulated worlds aren't really
real because um they're dependent on
living things for their meaning because
it seems like he's saying that
everything is dependent on living things
for their meaning um and in this I'll
just say one more thing um there there's
an interview where he kind of talks
about this um with uh Richard Brown I
believe
and he himself kind of recognizes that
there's a tension in his own view about
this that you know maybe you could just
you know it he's he's trying to draw a
distinction but like maybe you can't
really draw that distinction because you
know anyway yeah we listened to that
interview earlier I think it was about
three years ago that that he did that if
I understand correctly he's still a
materialist but he's kind of a material
chauvinist and and I don't mean that
pejoratively but what what he what he's
saying there is that you can simulate
things things but there's this kind of
semantic graph of meaning and as you
just saying almost per wienstein this is
what Mark Bishop said you know the
meaning of a computation is in its use
and you can always Trace back all of the
edges on this graph until you get to
material so he's saying that there are
things in the real world that exist even
without observers you know they have a
kind of Primacy and so obviously the
first argument he's making is is the the
basic one which is that ass simulation
of Fire doesn't get get hot and then his
slightly more nuanced argument is that
only things in the real world can exist
without
observers and I don't know it just seems
a bit strange right because we could you
know we have this qualitative experience
and we have meaning and so on and we
could be in The Matrix yeah so why do we
feel that we're so special yeah I think
that is a a
genuine um objection I mean because it
it seems to me like you know no matter
what you think about like the simulation
argument or the Matrix like how probable
it is it seems to me like we could be in
a simulation like we don't have like a
priori like certainty that we're not in
a simulation like and similarly like it
could just turn out that you know if you
crack open my skull I'm there's actually
like Silicon chips in here you and you
know like subjectively I wouldn't like
know the difference um and it just seems
weird to like be like I know there are
philosophers who actually say well we
know we're not in a simulation because
the simulation going be conscious like
that's actually a view that is out there
that philosophers hold but
I I don't know I just I just have a
pretty strong intuition that like
that's just being unjustifiably
confident I suppose um I I don't know
where they're getting this confidence um
and yeah it there's also kind of a weird
thing too um you know I don't I don't
think Evan Thompson believes in God but
if you did it seems like okay is he
going to say that like if God exists
then like we're all zombies or we don't
have meaning because God is kind
of you know like traditionally God is
sort of thought of as like almost
playing the role of like the simulator
of a simulation even if you don't think
that it is a literal computer simulation
God is kind of like giving meaning to
everything he like created everything he
had a design for everything
um and it just seems like a weird
position to say that like if God exists
then we're zombies or something like
that so yeah yeah how would you
distinguish Evan Thompson's argument
just from a standard material limied
materialist physicist does he think
Consciousness comes higher up or does he
think it it starts quite quickly so he
actually so in his book mind and life
which I've read the first couple
chapters of I'm still getting through it
but so to the extent that I understand
his position um he actually does come at
philosophy and and metaphysics and all
of this from a a fairly different
perspective than like
most certainly most like naturalists or
kind of a limit of materialists or or
illusionists whatever you want to say um
because he starts from what's called
phenomenology um and phenomenology is
this kind of um I guess branch of
philosophy that was started by Edmund uh
in like the early I think it was like
late 1800s early 1900s um and then was
kind of heiger and then uh Merlo Pon
ponti um all kind of continued this this
line of work um but the basic idea of
phenomenology is just like we start our
philosophical inquiry with our lived
experience our embodied experience as as
uh Mero ponti would would like to to
emphasize
um so they say like okay look the things
that we perceive you know I I perceive
my body I perceive you I perceive this
room all of this it's the realest that
anything can be it is kind of our
starting point for philosophy um and
then from our lived experience we then
start to make you know philosophical and
scientific theories that allow us to
kind of
um understand and kind of predict um and
control our experience better um but the
you know fundamentally like lived
experience is kind of the the the
foundation of everything um and so from
that perspective you know he's he's
definitely not going to say that like
Consciousness is an illusion or doesn't
exist he does have a different
perspective a somewhat different
perspective on like what Consciousness
is um from um some other philosophers um
but
uh but yeah he does start from kind of
experience or Consciousness um and
usually the people like Dan Dennett or
Keith Frankish or some of these people
who are kind of more hardcore
materialists um are not starting from
lived experience they want to sort of
say well maybe we don't need to start
from anywhere or it doesn't or we start
from science or something like that um
and then because they're sort of
starting from science they just say well
we can't really make sense of like the
Consciousness thing so we're just going
to forget about it um that sounds like
it's quite similar to idealism you know
that um the stuff of Minds is
fundamental and and even then there are
kind of subjective and objective
versions of of idealism but would you
would you kind of put him in that bucket
so I I I definitely think Evan would I'm
fairly sure he would not want to be
called an idealist within um
phenomenology there there is sort of a
tension and a you know different
phenomenologists have
had
different sort of ways of thinking about
what experience really is um I think
Edmund hurl might have been more close
to kind of an idealist perspective um he
was famous for this idea of the Epic
where basically you um you're looking at
your experience you're you're describing
it but you try
to um refrain from assuming that your
experience is reflective of some
objective reality you don't want to say
like oh it's not reflecting objective
reality you don't want to assume that
either but you're just want to kind of
withhold judgment about whether there's
an objective reality behind it um and so
that sounds a bit more kind of like an
idealist approach where you're like well
it's just this experience which is like
kind of mental or something and it may
not correspond to objective reality but
um other phenomenologists like haiger or
U ponti are
usually seen as as being less idealist
especially meal ponti um he really
focuses on the importance of of the body
as kind of the the vehicle through which
you experience things so he definitely
wants to say that the body is is is real
and the body is not really a mental
thing in the traditional sense um and he
also says has some interesting thoughts
um you know he'll say that like our
Direct experience um like my direct
experience right now includes you as a
person it includes like a camera it
includes you know these windows and it
includes like objects he would say that
like it whereas a lot of people when
they think about
idealism or at least certain ways of
thinking about conscious experience
they'll say that like well your
experience doesn't actually include
people Windows objects because that's an
interpretation of your experience your
actual experience is just like colors
and like raw sounds like
uninterpreted raw feels and they'll say
that's the thing that that's real the
interpretation is like kind of fake or
something um and that's like meron wants
to reject that because he says like okay
look the person on the street or like
you know before you started thinking
about philosophy you definitely didn't
think of your experience as being about
colors and raw
feels you know that's kind of
this you know the very idea that you're
seeing raw colors separated from the
objects that have the colors is this
kind of post talk kind of abstraction
that you get from philosophical thinking
but like really what what's there in
your experience is just like objects
that have properties like colors and
stuff but like the object is there and
you you kind of experience it directly
um and so if you have that approach it's
it's less clear that it makes sense to
call it idealism I mean maybe you still
want to call it idealism but it's
more of
a yeah it's it's a bit hard to
categorize maybe in the traditional
dichotomy of materialism and idealism
yeah I think there are flavors of
idealism which could be thought as
realist yeah I mean so I guess I like um
how John Veri um thinks about like the
word real and the concept of reality um
he says that real is a comparative term
so like it it only really is is
Meaningful to say that something is real
as compared to some other things that
you're saying are Illusions um like if
you just say everything is an illusion
it's
like okay I guess I mean you know maybe
that you know it it's not really
clear what you know if you say
everything's an illusion or everything's
real it seems like those are almost the
same thing because you're not making any
distinctions it seems like you you kind
of in order to make the concept of
reality meaningful you need to be able
to make distinctions between well this
is real or more real than this other
thing and so it's kind of a a matter of
degree and and a matter of comparison
between things um
so yeah I I don't
um I don't like kind of hardcore like
reductionist or materialist views that
want to say that like well you know the
only thing that's real is like Quantum
fields or like particles or or you know
or something like that um I
mean you know you can say that but it's
like what
is what is the point of saying it like I
don't know it just seems like you're
kind
of trying to be edgy or something but
like it's it's not really uh it's not a
it's not a useful way of thinking about
things anyway way in dennit and Frankish
when they talk about illusionism in
respect of Consciousness what do they
mean yeah so I think honestly the term
illusionism uh kind of frustrates me a
bit like the word itself um because a
lot of people when they hear
illusionism they think
that what what illusionists are saying
is that Consciousness as a whole
doesn't exist um no one has ever been in
pain no one has ever experienced
anything um that's not what they're
saying at least I mean maybe some people
would you know what they say is no
people have experienced pain people have
you know experience exists Consciousness
exists it's just not what you think it
is um and Keith will say um that qualia
this like particular philosophical
notion of Consciousness um is not real
and is an illusion um that said I you
know yeah so I I don't like the term I
do also think that on the substance I
disagree um I think you know Keith
Frankish in particular um and some other
illusionists well I know at least some
illusionists will say
that there is nothing that it is like to
be you they want to reject the like what
it's like talk um and they have like
some arguments for this based because
they'll say that like oh you're always
interpreting your experience and like
what it's like to be you just depends on
how you're interpreting it and so
there's no like like objective interpret
there's like different arguments like
that but I I think those are all just
kind
of sort of non seitters or
are you know I I I think yes you can
interpret your experience in different
ways but that doesn't mean it's like
unreal and it sort of gets back to just
like the kind of overuse of like the
term unreal or Illusion I'm just like
why are you saying this like what is I
don't know like what is
this how should I live differently if I
think that like this is unreal or
something I I don't know it's it's not
really clear um yeah and whenever you
try and make these arguments you get
accused of being a jewelist very quickly
as um John Soul was and Thomas Nagel who
came up with this term you know what is
it like to be um he was also accused of
of being a jewelist but it's really
difficult isn't it to talk about this
qualitative experience in you know any
kind of meaningful way yeah so there is
there is this notion um that like you
know qualitative experience is like
ineffable or that that's like a a term
that people often use I I mean in a
certain sense this like obviously not
literally true like you can you can you
can try to describe you know I can
describe my experience right now but but
the what they're saying is like
there's you know you're always going to
be missing out on some quality of of
your experience you can never like fully
describe it um and I mean I I think
that's true although I I
think I
would I kind of want to extend that to
like almost everything I I kind of want
to be like well yeah so like experience
is um is ineffable in the sense that
there's always you can't like fully
describe all aspects of it but that's
kind of true of everything and that
maybe that sort of ties in with my my
sympathies with phenomenology as as I
was talking about before like if you
start from lived experience as kind of
like the ground of like everything else
well like yeah lived experience is like
you know not fully describable but then
like that's the ground of everything
else like nothing else is like fully
describable
but anyway uh that's kind of how I think
about it so you you wrote an article
recently I think it was on L wrong is is
that right uh it's cross posted on less
wrong and on um
optimists. um and my I should say
Quinton Pope also co-authored with me
counting arguments provide no evidence
for AI Doom you L the elevator pitch
yeah so there's this you know there are
a lot of people who are worried that AI
will
cause an apocalypse kind of take over
the world kill everyone something like
that um
and there's kind of a an argument that
is sometimes used for this conclusion
it's really kind of a a family of
different arguments that are all sort of
similar and it it's kind of hard to pin
down actually which
is something we realized after we wrote
this article we kind of like proposed
okay here's what we think the argument
is and then people later were like oh
well you like misinterpreted us and so
you know it's it's hard to pin down
exactly but the something like it is
this
um when you're uh training an
AI to be nice or and aligned or whatever
you know you're trying to make a a super
smart AI That's that you know cares
about people and is has your best
interests at heart Etc you know that
that's what you're trying to do um
but there's
this assumption that okay the AI is
going going to have a goal it's going to
have like some like overriding goal that
explains Its Behavior that's always
that's already sort of an assumption
which I might question but the there's
that is kind of built into the argument
there's like some goal that is kind of
describing Its Behavior um overall and
then they'll argue well okay there's
like many different possible goals that
the AI might have there's like you know
infinitely many or like trillions of
them or something like that you know the
AI might gen genuinely want to help you
but it might also want to maximize paper
clips or it might also want it might
actually want to uh you know convert
everyone to Mormonism or you know
whatever like it could be anything
um uh they would say it could be
anything anyway um and they'll just say
well look um most of the goals that it
might end up having would motivate it to
act aligned like pretend that it cares
about you it like really you know it's
really align you know it it most of
those goals will will motivate it to
pretend to be aligned without actually
being aligned because like its real goal
is to convert everyone to Mormonism or
whatever it is um and so the idea is
like okay you're going to have this
decep this like deception
um you know the the Assumption here is
that it kind of understands that it's in
a training process um and so it'll like
recognize okay I got to like play the
training game and pretend you know that
I'm I'm doing what what the humans want
me to do um and then when it kind of
finds an opportune moment um it will
strike and it will kind of like take the
opportunity to you know
uh kind of get out you know remove any
kind of safety um precautions were in
place that we're kind of like sandboxing
it or whatever and it'll like escape and
kind of take control of the government
or whatever you know it'll it'll do
it'll do whatever it wants to do um so
fundamentally the argument is based on
there are many possible
goals that would all motivate it to
act aligned to pretend to be aligned but
like very few of them um are actually
aligned goals okay so it sounds a little
bit like you know with instrumental
convergence it's saying that they would
be many intermediate subg goals for
outside goals this is like saying that
there are many um you know many goals
that would actually produce deceptive
goals yeah many goals that would would
produce ceptive Behavior yeah yeah yeah
um does it also imply that the deceptive
goals you know like instrumental
convergence implies that the
instrumental goals are kind of fewer and
quite standard you know like power
seeking is it a similar case here yeah
well so the the idea is so you've got
like a terminal goal that like ex
motivates all of your behavior and then
you can have instrumental goal and so
yeah it is there is kind of an
instrumental
convergence claim kind of built into
this that like deception like you could
view deception maybe as an instrumental
goal in itself or something like that
and like power seeking would be like an
instrumental goal or something um yeah
so in in a certain sense this is it is
kind of a repackaging of of other
arguments that have been put forward
before
um but yeah so I you know in in the
article we um give a variety of kind of
rebuttal or or counter arguments to this
um
so our first critique is like okay
look this General line of argument can't
possibly be reliable because there's
this other argument that is like almost
structurally identical to the original
argument that has an absurd
conclusion um the Absurd conclusion is
that all uh basically almost all neural
networks will overfit to their training
data and never General at all okay so
the argument goes um there are like
there are a very large number of
functions like possible functions that
the neural network could learn which
would all be consistent with getting
like low um loss on the training data
okay but almost all of those functions
would uh you know do terribly on the
validation set or on some other
distribution or whatever
um therefore you should expect that like
almost all the time when you train um a
model uh it will like it will learn one
of those other functions that do well in
the training set but like do terribly
outside of the training set therefore
you should expect almost all their
networks to overfit okay now clearly
this doesn't this doesn't happen I mean
overfitting isn't problem it's not that
it it never happens but it it doesn't
it's not like it always happens in this
like extreme sense that would be
expected like be predicted by this
argument of course you know there could
be counter points to this and you know
we could we could get into that if we
want but we we're then like okay well
wait a minute so why why is this General
argument
argumentative structure unreliable or or
wrong like what is actually going wrong
here and we point at a couple different
problems with it one problem with it is
is um that it relies on this
philosophical principle called the
principle of
indifference so the principle of
indifference um you know it might be
easiest
to um use this like simple case so if
you just have a a coin with like two
sides on it and then you ask like oh
what's the probability that it's going
to land on heads and then what's the
probability that it's going to land on
Tails um the principle of indifference
says
well you should kind of like assign like
one half probability to the one side and
one half probability to the other side
because there's only two possibilities
and you have no reason to like prefer
either like one or the other so it's
50/50 okay so you know it's it's an
intuitive principle um and it I think it
gets its intuitive plausibility from
cases like a coin or a die with like six
sides you're going to assign like one
six probability to each side but I think
this is this is
actually it it's a subtly felicious
argument
because you
know um there there's kind of a
different way of applying the
indifference principle that would get
you a wildly different result and and so
it goes like this if you flip a
coin um you can think about the outcome
of the coin flip as either as binary
that's like how we did it before you
could also think of the come as being a
three like a 3D
orientation of the coin flip that's
actually kind of like a more you know
reductive materialist way of thinking
about the the outcome of the coin flip
right because it's like really it's just
like a material object that's got a
certain orientation and we're like
imposing this interpretation of heads
tailes on it but really it's just like
an object right so maybe what you want
to do is you want to say that the
outcome is this like 3D orientation um
there's like an angle associated with
like the X Y and z- axis or something
like that well if you interpret the
outcome space as being the 3D
orientation then the principle of
indifference would say well you should
assign like you know like every possible
orientation should have like equal
probability right but that's clearly
wrong because like that's like almost
never going to land like like you know
on its side it's not going to land in
like an orientation it's like
gravitationally unstable where it's
going to like fall right so like clearly
it's
not it it the F the fundamental problem
with the principle of indifference is
that it
depends on the way that you're like
cutting up or interpreting the outcome
space and different ways of cutting up
or interpreting the outcome space give
you wildly different results um maybe
I'll give like one more example so you
can imagine um there's like a guy named
B
who where you know that he is in the UK
or in France or he's in he's in like
this geographical region of like the UK
and France join together okay he's
somewhere in there but you don't know
where he is exactly um now one question
you can ask is like oh is he either in
the UK or is he in France well with the
principle of indifference you know you
would assign like 50% Credence to France
or 50% Credence to the the UK okay but
you could also cut up the like space of
possibilities in a different way you
could say well he's in France Oris in
England or Wales or Northern Ireland or
Scotland or you could look at like you
know different like uh like regions of
France you know you could like cut
things up in a variety of different ways
and you would get different answers like
if you cut it cut up the UK by like the
different constituent countries you
would say that like it's like a
one probability that he's in France and
then like a 4th probability that he's in
the UK right um and I think this is you
know philosophers have kind of noted
this for a long time and I mean there
there's still debate on like how exactly
to respond to this but you know it's
generally agreed like you can't just
apply the principle of indifference you
know willy-nilly like you you it's
either just totally wrong or you have to
be like very careful with how you apply
it otherwise you're going to get like
crazy results and I think this is one of
those crazy results um so I I think that
like basically the the the counting
argument
is it's assuming this like it's assuming
that you can kind of cut up the space of
of outcomes of the training process into
like these goal categories or something
like that where it's like
okay I mean there's just like a variety
of different problems with this it's
like okay so first I'm like one way of
thinking about it is like you're saying
there's like discret goals and there's
like you know a billion different goals
and then you're like randomly choosing
from a billion different goals well it's
like okay first of all it seems like
really weird to assume that like goals
are like discrete things cuz that it's
just going to depend on like how you
describe the goal and like that that's
just like really strange okay so maybe
you don't want to like describe the
goals of discreet things
maybe there's like a continuous space of
goals but like you know fundamentally
it's just the problem is like you can
describe the space of possible results
of the training process however you want
you could describe it as either it's
going to be aligned or it's not it's
50/50 you know and then you would get a
different you know
so it's I think this is just a
fundamentally unprincipled way of
thinking about it and you know at the
end of the day if if you want a more
reliable answer for like How likely the
AI is to be aligned you you I think you
should just not rely on an indifference
principle at all and you just need to
look at okay the actual details of
what's going on and try to like kind of
come up with like a
mechanistic understanding of it um and
not rely on these like abstract
principles yes um this is related to my
position on agency instrumentalism or
agency illusionism because you could
argue on the one hand that that goals
are just not real but you could also
make the argument as you have done that
there's significant ambiguity in how we
represent goals yeah I think that's
right so there there's a there's a part
of the um the article which I I am like
actually a little bit like if if I were
rewriting the the paper I might or sorry
the the post I suppose I would uh
rewrite it differently probably but we
we do make this point that you know um
the counting argument seems to be
assuming that like goals are real things
that you know they're so real that you
can count them right like um and like
it's it's really true that an AI has
like a particular goal and not some
other goal as opposed to viewing goals
more as just like useful descriptions
for kind of compactly describing
behaviors um I still like mostly stand
by that I think that
they um
that kind of more doomy people or people
who kind of tend to use this argument
are reifying goals too much and are are
taking them too seriously kind of as as
an abstraction that said I I guess I
think it it would be easy to to go to
take this too far in the other direction
and say well like goals are Just an
Illusion and you know I I don't want to
say that either I mean you know if if
it's if goals are useful enough to for
us to keep talking about them all the
time like I want to say okay in some
sense they're real or kind of real or
something so it yeah it's a tricky
question yeah I've thought I've thought
about this quite a lot I discussed it
with Philip B as well I mean my first
intuition is that any intelligence
system would have go dynamism so it kind
of you know it wouldn't make sense to
think of this BOS droian super
intelligence that had a single goal and
it even if they did exist in the way we
conceive of we're talking about this big
inscrutable intelligent thing so surely
the way we abstract goals might not be
what the goals actually are and it's
also related to this intentional stance
from Daniel dennit which is that we as
agents um we adopt this stance we build
a model we do abduction and we
understand what the rational behavior of
another agent is based on our projection
of of what their goals are but that is
very much an instrumentalist view it's
just what we think the goals are
yeah that is a a good point I mean so
dennet
has I think most people interpret dennit
as as being an instrumentalist about
this as saying okay it's just the
intentional
stance is just a useful way of thinking
about agents it's you know we're we're
just ascribing goals to to systems but
like in some deeper sense it's not
real but I I think you
know I
yeah this just gets back to like you
know how do we Define real and like what
is you know what does it mean to say
something is is real or unreal um I mean
I I do
think yeah if if if something is like so
useful to talk about that we're talking
about it all the time like you can't say
that it's completely unreal I think one
useful distinction would be if it had
consistency so if it really is an
inscrutable impulse response machine and
it's just flitting from one goal to
another dynamically then I think it
would be fair to say that it didn't have
goals you know the goals weren't
real right yeah and and and you know you
could argue I I guess I'm taking sort of
a pragmatist stance here um you know if
if the if the agent had you know if it
if its goals are changing all the time
then it might not be useful to describe
it as as having goals at all maybe maybe
it's it's better to just talk about
patterns of behavior or something like
that with this in mind as well you know
a lot of GOI people and a lot of
symbolists now they think that the best
way to design an AI system is to
explicitly craft goals and maybe some
kind of meta Learning System that
creates sub goals and so on and I've
always felt that this is mixing the
description with the thing so it doesn't
make sense to build the description you
should sort of build the actual thing I
mean what what's your take on
that yeah I mean I I guess I I tend to
take the view that like I me I think I I
think I agree with you I I kind of like
the analogy of like training
a uh helpful and harmless AI to like
kind of raising a child or something
like that now that you could like
obviously take that like way too far
like that analogy way too far but I
think you know when when you're raising
a child or like training a an animal or
something like that you're not
hardcoding goals into it or like it it's
not even
really you're usually not even really
trying to like hardcode a goal into it
in in the like nicob bostrum sense of
goal that where it's kind of like the
single thing that's like motivating all
the rest of your behavior you're usually
just trying to kind of inculcate General
kind of patterns
uh you
know you know trying to inculcate
General like values and and kind of um
instincts and and patterns of behavior
but it's it's not it's not like
inserting a a utility function or
something into the system yeah because
this is relevant for alignment because
as you say we bring up kids we instill
principles and and virtues
and yeah how how does this how does this
help us with with alignment I mean so
what one take would be we just um we
look at Behavior alone and we just treat
the system as as inscrutable I I guess I
I will say you know obviously as an
interpretability researcher like there
are things we can do with AIS that we
can't do with kids or animals um you
know we can look at their internal
States um and we can kind of monitor
them at a like very M like a much more
fine grained level of detail than we can
with kids or animals and that's actually
sort of an argument that um me and and
Quinton Pope made in a different um post
uh AI is easy to control um but uh you
know AIS are are white boxes in a in a
sense that um you know animals and and
other people are not um they're just in
the literal sense that we can just peer
into it and see um and you know it
it's you know because it's not like
computer code that any anyone wrote It's
not that like we can like rewrite the
code but we nevertheless have like a
variety of tools that we can use to kind
of you know peer into the AI and and in
some ways like see what it's thinking
you know we can we can train probes on
it we can look at um you know for like
language models for example um there's
actually another paper that I did on one
of my first papers called the tuned lens
you can train these little linear
they're basically like linear kind of
linear probes linear classifiers at each
layer of a language model and you can
see its predict like how its prediction
of the next token changes from one layer
to the next and there's like
interpretable predictions at early
layers that like you know are kind of
relying on like simpler features uh of
of the input and like it gets more
sophisticated as it goes up and there's
like all sorts of things you can do like
that so that's all to say that like we
have more tools um and we have white box
tools for AIS that we don't have for
kids and animals that said I do think we
can learn from the the the human and
animal cases um you know you know just
one example is like people are now
working on um kind of data curation um
you know when we first started training
big language models there's like very
little curation of the training data I
think like open AI used um Reddit Karma
or something like that um to like filter
links but it was like not you know it
was certainly not kind of like fine
grained um curation but what people are
kind of trending towards now especially
for smaller language models is to like
um you know we're using a lot more
synthetic data generation so using like
larger language models or older language
models to generate data for the new
language models um and we're also using
using AI as part of the data set
curation process to kind of on a more
fine grain level figure out okay like
what what sorts of things do we want our
AI to see basically and you know that is
kind of similar to like how you know
children think about like well we we
want our kids to see certain things and
not other things and you know kids are
impressionable a are much more
impressionable than kids are even um and
so
yeah I
think you know careful data SEC curation
is like a huge part of of alignment I
think um there are a lot of simple
things that you can do that will go a
long way yeah so there's like the
there's curating what goes in in terms
of data and then there's this whole you
know there are many things like tree of
thoughts and you know um rhf and and
ways of behavior you know Behavior
shaping on on the output and there are
companies for example doing alignment
systems where they explicitly craft
goals they say this is the kind of goal
we want we want the company to make this
amount of profit and we want um you know
this person to meet this performance
Target next year and I feel that
brutalizes the system for a couple of
reasons I mean it it introduces good
Hearts law and there's the clever hands
effect as well you know so it can might
might do the the right thing for the
wrong reasons and I also feel that we
need to have some kind of dynamism you
know to have an intelligent system like
the system might need to do things that
we can't conceive of in order to be
successful yeah that's true I mean I
guess I guess there's different ways
that you could try to kind of give a
goal to an AI right so I think there
there's
certain versions of this that
seem more okay to me than others I mean
like I don't know in
a just a an any sort of organization a
company anything like that a lot of
times um you know employees are given a
goal like like you know they're given a
directive which is basically kind of a
contextual goal it's like well we've got
this deadline to you know to finish this
report or we there's certain quota for
like sales or you know whatever um but
and I think that's you know we do that
all the time and of course it does you
know it can cause problems like you know
if you have quotas like they can be you
know good heart or whatever but like
ultimately it does seem like these sorts
of things are kind of
indispensable um you know just breaking
problems into parts and so forth and I
think you
know I think one problem I have with
these sort of some of these like
Arguments for Doom is that they assume
that when we give ai's goals it's going
to be this like the AI is going to kind
of like take the goal um In This Very
unnatural kind of
like it's going to kind of like take it
as its new kind of purpose in life like
everything okay like you told me that I
I've got to like maximize or I've got to
like make uh some sales quot well
everything else goes out the window
that's like my only purpose in life and
if you try to change my goal now I'm
going to kill you because I I only want
sales and nothing El it's like that's
not how humans work and I also don't
think that's how how any plausible AI
system is going to work like the way
that people are starting to build you
know
agentic uh you know quote unquote
agentic like language models these days
is not building in permanent over like
overwriting goals it's they're just
prompting basically they're they're like
giving like okay in this context like
your goal is to do XYZ but like you know
it do like
I I don't think we should expect that
the AI is going to
like be
so stupid and act in this like stupid
and and caricature type way where it
just like forgets its common sense and
realiz and you know forgets that this is
just a
contextual thing that it needs to do and
it's going to be completed and then it's
going to be it it should be ready for
further instructions I wondered what
your position on on agency was so you
know you have a lang anguage model it
learns a text distribution you know it's
like engrs on steroids and then you do
this rhf and you can do um Chain of
Thought and self-reflection and
iterative prompting and tree of thoughts
or fun search or Alpha geometry all of
this kind of stuff and all of these
things are placing significant guard
rails on the the kinds of trajectories
that that you sample if you sample it
stastically so you're making it more and
more domain specific to do a particular
thing and all the while there are people
who say even in this setting even though
we've placed all of these guards on it
will have some kind of Divergent agency
you know which is to say we're telling
you to do this but actually it has its
own desires if if you like what's your
take yeah I so I I definitely don't
think we should expect
like kind
of emergent agency or like autonomy from
a system like this I
mean you
know that's part because that's just not
how we're training these systems like
you could imagine like a very different
world in which we were simulating
Evolution or something um in our
computers and we were like there's some
sort of competition between different
AIS and the ones that like survive like
it's like survival of the fittest or
something um and that's how we got
intelligence well yeah in that case I'd
be a lot more worried about like well
they've got their own goals and drives
survival Instinct all of that then I
would be a lot more more concerned but
that's not how we're training them at
all it's it's mostly imitation and we
get to choose you know you know
carefully curate the data that we're
asking it to imitate um and then we're
you know just kind of reinforcing
behaviors that we like and negatively
reinforcing behaviors we don't like and
I don't think that this kind
of emergent autonomy stuff is going to
come out of that I think we agree so we
we agree that if we create a high
resolution simulation of the universe
then things like agency and intelligence
are emergent properties much like
temperature is an emergent property and
we also agree that if you do this
imitation learning in a language model
with behavior shaping and you wouldn't
you wouldn't get agency I mean I guess
I'm I'm just saying that I think sort of
agency emerges in an evolutionary
context sort of like a darwinian context
or I mean I guess if you're like trying
hard to make an agent like maybe you
could succeed at that like and and an
agent in the sense of a system that has
its own kind of self interest and like
some sort of some sense of like survival
Instinct or something um where it's it's
not just kind of taking instructions
from the outside but it's it's got its
own drives um I don't think
that yeah like I said I don't think
because we're not simulating Evolution I
don't think we we'll get that by default
and I also don't think there's really
like a like an economic incentive to
create that I I know some people
disagree and say oh yeah there's going
to be an economic incentive to create
like you know artificial creatures um
but it it it just seems
like at the end of the day we're trying
to make these AIS to do stuff for
us like it we don't actually have an
incentive to make things that are
uncontrollable
um as far as I can tell but okay but do
you think that we could create an
agential system which is still abstract
and tractable enough to run on Modern
computers yeah I mean I think well so I
think that like for
example um like mind uploading of humans
is like probably possible with like some
technology I don't know like you if it's
like soon but like
that's that would you know if you could
upload like humans then you would have
like agentic systems with like
self-interests oh would we that's
interesting that you think we would I
guess from an externalist point of view
I think that you know a brain in a vat
or a person in a hermetically sealed
chamber wouldn't have much
agency yeah so I guess when I so yeah we
should separate um maybe the like kind
of Behavioral question from like the
more philosophical so I'm not
necessarily saying I mean we could get
into this I'm not necessarily saying
like oh it would be conscious although I
think like it probably would but I'm
just saying we would be like we should
be able to
simulate humans and humans are agents
and so
behaviorally you would have like you
would have like similar concerns like
well like does the human actually care
about me or are they just trying to like
gain more power or whatever
um and you know you could you could have
all those worries all the while thinking
that you know it's a zombie or whatever
yeah I agree that we could you know
upload a whole load of minds and we
could do a simulation of the universe
and we could have virtual inter agent
agency in the simulated World it it
seems like a step to have kind of like
material virtual inter agency yeah so so
I guess I'm also assuming that there's
some like if
we're yeah I guess I'm kind of
imagining um the world uh that's
described in the the TV series Pantheon
which people should should watch
um um it's about mind uploading
and there I mean it's kind of a a weird
timeline cuz like mind uploading happens
before we get like just purely
artificial intelligence that can do the
same task I feel like that's just not
realistic we're definitely going to get
purely artificial things before we get
mind uploads um but like the first thing
like they they you know do mind
uploading and then they start using the
uploads as slaves honestly I mean
they're they're using them they don't
call them that but like basically
they're using them for like
economic um purposes um and obviously in
order to do that they're connecting the
Mind uploads to outside world first they
don't use robots later they do have
robots but they'll just you know they'll
connect it through the internet and
virtual realities and stuff like that um
and so there is interaction between the
Mind upload and the yeah wouldn't it be
fascinating if we were in The Matrix but
currently we don't have any kind of
control panel with the with the super
simulation or the super world but maybe
the simulators were using us to just do
Financial trading for them or something
like that we had a little portal and we
press some buttons on the portal and as
soon as we have that connection with the
superworld we might start to express
agency and the super world so it start
deceiving our simulators yeah yeah um it
kind of reminds me of like a lot of
weird speculations that people in less
wrong have made about um the uh this is
like a weird thing but it there there's
this idea of the salamov prior or
salamov
induction where you're doing beijan
reasoning but you have a prior over the
different hypotheses that is weighted
based on um kagra complexity which is
the the length of the the shortest
possible turing machine that would like
simulate the hypothesis or something
like that and then the weird thing that
happens if you imagine this is like well
it looks like there are like relatively
short
programs for a turning machine that
would like simulate an entire universe
of course they would be like very slow
so like in practice you couldn't but
like it's a short program and so then
it's like if you imagine this then um
you could imagine that there's like uh
simulated worlds where that have people
in them that are like deceiving you and
then they're like causing they're like
they like find out that they're like
part of this salamov induction process
and then like cause it to go in weird
way yeah it's it I don't think it like
has any relation to like the real world
but it's just like kind of interesting
thought experiment yeah so was wasn't
there a post on the EA Forum which was
called something like EA want to
maximize everything but maximization is
perilous uh yeah almost so it's it's EA
is about maximization and maximization
is perilous Yeah by Holden carnovsky um
and to be clear on this post Holden is
not saying we should like you know hey
EA stop being stop being EA right um
because I think he still identifies as
an effective
uh to to this day but he is pointing out
that there's this you know real problem
or or or Peril um at the core of the
kind of effective altruist ideology um
you know EA is kind of often defined or
summarized as doing the most good so it
is about maximizing the good in some
sense but the problem is we don't really
know what we mean by the good at least
in in like detail like we have
intuitions about like well it's you know
good to you know save someone from a
burning building it's generally good to
like reduce Global poverty you know
there's like certain things that we
think are like Obviously good but when
you try to maximize the good that's
where you start to get into like kind of
treacherous territory because now you're
trying to maximize something that
you don't have a clear crystallized kind
of
even you know formal definition of um
and so it can lead to things like um the
whole kind of FTX debacle with you know
where Sam bman freed and others went to
jail for you know doing
criminal unethical things um in the name
of what they thought was doing the most
good you know they were trying to make
money to uh you know in whatever way
possible in order to donate it to you
know effective altruist Charities um and
you know that was their interpretation
of doing the most good of course other
EAS think well that's not what doing the
most good is but like they they actually
disagree about like what the good is and
you know when
you when you're not maximizing the good
we tend to often agree about what the
good is like because we agree on the
simple cases we agree on you know let's
you know give some money to this charity
let's you know um you know whatever but
we start to disagree more as we push
further out into like more and more
exotic things like oh well maybe you
know maybe um the long-term future has
like almost all the value because it's
going to like last for you know
trillions of years you know so many
effective authors have made this
argument and
like I don't know like is that like you
know people are just going to disagree
on that I don't you know fundamentally I
don't I don't actually think there's
like an objective fact of the matter
like built into the universe about what
the good is but I what I think is
that
it's trying to maximize the
good is just liable
to you know lead to kind of extreme
behavior um you know more kind of
disagre you know disagreement and
conflict between people um you know it's
it's kind of in a certain sense it's
like an extremist view right like by
definition you're like doing going to
the max um
and so that's why I I don't I I no
longer
view the good as something that should
be maximized um you know I think we
should be much more we should be
thinking about ethics much more in terms
of like like virtue ethics for example
where like the good is just you know
trying to be a good person cultivated
certain virtues in yourself trying to be
more honest trying to be more generous
or whatever um and not in terms of like
trying to maximize something out in the
world um I think that's a a much better
way of thinking about things and so
that's why I don't identify as an EA
anymore I'm I'm not like hostile to all
EAS or or whatever I'm far from that um
I have many friends who still do
identify as EAS but I yeah I don't
um I don't identify that way anymore
yeah and it's it's not to badmouth EA
they do do many great things I think
since the focus on long-termism in
particular and and and AI safety I think
that that's been a bit of an issue and
and as you were saying the two
components are as well as long-termism
this this rationalist idea that you can
reify goodness into some objective
criteria and that brings me to the next
question I mean would you define
yourself as a as a relativist I mean is
that at odds with this IDE that you can
reify
goodness yeah so relativism is is a
tricky word um so in in one sense no um
so there's there's like a there's kind
of a form of relativism which sort of
says um you know basically I personally
think
that all like like I view all kind of
value systems and perspectives as like
equally valid or something um I think
that's silly I don't see a reason to
believe that or take that perspective it
kind of leads to a weird sort of like
you know you're just kind of complacent
and like you just think well you know
everything's
like I I don't know it's sort of like uh
tolerance taken to a very extreme level
where you just like don't want to like
criticize what anyone else is doing and
you're just kind you know um so I I'm
not a relativist in that sense there are
like other senses of
relativism both about like morality and
about other things where I I might
qualify um if you're saying that it's
just you
know that
like there are different like I I do
think there
are that you know the world I mean maybe
this is kind of like a trivial thing but
like the world looks differently from
different perspectives and there are
like just different ways of describing
the world and I don't think that there's
like one uniquely correct way of
describing it that like everybody must
agree on or they're like totally wrong
um I think like different ways of
talking conceptual schemes Etc um can be
can make sense from like from different
perspectives so maybe that like counts
as a form of relativism but it just
depends what you mean yeah because we
live in a globally connected world and
certainly the the North American culture
is very dominant I mean as as a trans
person for example you might not want to
travel to um Dubai yeah and and what
what what's your take on
that yeah I mean so I I
definitely you know I I definitely don't
want I'm I'm not the kind of relativist
that's like well yeah Dubai is like like
intolerant
um you know morality is like just as
good as our or you know like like I want
to say like even if even though I don't
think there's like I don't think like
God is on our side or I don't think like
the objective more facts built into the
universe are on our side I nevertheless
I'm opposed to what Dubai is doing I
think they should you know be more
tolerant and and more accepting and like
I will like act to like try to convince
them of of that or whatever oh
interesting yeah do in any way conflicts
with with the relativism then so I I
don't think it it should or or needs to
I think you can both you can and this is
what um I guess this is a part of uh the
philosopher Richard r's thought that I I
kind of like because a lot of people
criticized him of being a relativist
because he said he made a similar point
of like we shouldn't talk about one
truth or one correct description there's
many descriptions that are useful for
different purposes Etc um which sounds
kind of relativist or whatever but he
also he he rejected the term relativism
and he said look um I you know even
though I don't think you know like like
even though there's not like an
objective truth that's back what I'm
saying nevertheless I
oppose you know transphobia I oppose you
know beating women I you know you know I
I have these values and I stand by them
and I'm going to like you know act um
accordingly so in the EA Community
presumably there are relativists in
there and how how do they reconcile that
I mean yeah so there definitely are I
mean i' I've met some of them um and I
mean actually I think um
so one self-described EA who whose work
I like a lot actually um and who does
disagree with me on some of the AI
safety stuff is Joe Carl Smith yes so
Joe Carl Smith has some very good
um well essays which are also he's like
uh like spoken that he's got like a
recording of him speaking them as well
um which is nice anyway he's got some
very good essays on um meta ethics so
like the the kind of philosophy of like
like what is ethics anyway is it
objective or not or like what what is it
about all of that so he has like several
essays that are good um he's a a moral
anti-realist um so he doesn't think
there's like uh morality built into the
Universe um but he and he does you know
kind of struggle with some of these
questions because he you know he kind of
says at one point like well okay you
know if we're if we don't believe in
objective
morality as as EA or or just as as
people trying to do good um are we just
kind of imposing our will on the on the
universe and if you're just imposing
your will on the universe that that
doesn't feel quite as altruistic as you
might hope it just seems like you're
kind of selfishly imposing your own what
you think should be um you know your
preferences basically your imposing
preferences um and you know I he
doesn't I I think
he you know he ultimately just says like
you
know it is like a bit of
a it it is a bit like weird or
uncomfortable if you think about it like
you know um from like a a kind of a
God's eye perspective like oh it just
looks like you're imposing your will on
things but
like ultimately like if you are
acting um with other people's interests
in mind like you shouldn't you shouldn't
feel bad about that like that like that
is better from our perspective than just
being you know a pure selfish person
yeah what do you think about the the
paternalism in in EA so I agree with you
that it is an aggravating Factor if
there isn't you know an actual moral
truth to to go towards because if if
there was there would be some kind of
moral justification to um you know
almost almost lead people to a better
world but what they I think the argument
they're making is that they've Galaxy
brained themselves into coming up with a
moral framework and thinking long into
the future and knowing what's better in
a way that normal people can't
understand therefore you should listen
to them yeah I guess so I I don't know
if I have like fully fleshed out views
on this but I think there there is
potent there is a potential worry with
moral
realism that it can be used to
justify as you said Galaxy brained ideas
about what's good like you because you
think there's an objective moral truth
if you like go through the arguments and
you convince yourself of you know like
you you know I guess one way of putting
this is like if you think there an
objective moral truth you're actually
more open to being convinced of G Galaxy
brained or kind of like kind of
initially implausible sounding ideas
about what's good like oh we should just
like only care about the future and
forget about the present or whatever
whereas if you're not a realist about it
you're just going to be like well no I'm
I'm not going to change my mind on this
like I'm going to Value the present um
more than more than the future um and so
yeah I
think you know a lot of people talk
about the
the dangers of Mor you know moral
relativism or or moral anti-realism but
I think there are at least
as you know the the dangers of moral
realism are at least as as serious so
you kind of what really I mean I'm just
imputing here but I I think what really
triggered an interest for you was your
fascination with with
goodness and and value and meaning yeah
you know almost like that that triangle
if you like and if I understand
correctly um in recent years you've been
looking into things like Buddhism and
doing some some you know bit of a
journey there tell me about that yeah so
I guess the the fascination with
Buddhism really it's fairly recent um so
in the past it's definitely like this
year like I wasn't thinking about it
last year the last few months um and it
kind
of it
was well so one influence was actually
um Robert Wright who I did an interview
with um a little while ago and he wrote
a book called why Buddhism is true um
and just like being like you know being
asked to go on a show and then going on
a show just made me think about the book
and I eventually read it um I
also um I think for somewhat independent
reasons started
uh using Sam Harris's app waking up
which um kind of guides you through
mindfulness
meditation um and I did that I think
like partially just because I was sort
of in search for some sort
of kind of Quasi spiritual practice or I
just wanted to kind of try it out you
know I thought like oh I'm interested in
like philosophy and Consciousness and
stuff but I've never meditated it seems
like I should do that and maybe I'd get
something something out of it I also
have ADHD and I thought like maybe
forcing myself to be like mindful like
could like help with that or something
so there were like a lot of factors but
I so I started
meditating um and you know it's not like
obviously you can meditate without being
a Buddhist like I'm not even sure if I
count as a Buddhist like it depends how
you define it or whatever but um but
like it's clearly like connected like a
lot of a lot of the kind of uh
meditative practice is come out of like
the Buddhist
tradition um as well as like Hindu
traditions and stuff um and like Sam
Harris in the app and in the like kind
of uh there's like a theory section of
the app where he just like has these
like discussions with people and stuff
um he talks to a lot of Buddhists and
talks about like Buddhist philosophy and
you know I so I kind of got into it
through that and I I immediately noticed
like okay um there's a doctrine of like
no self that like there's no you know
there's different ways of putting it but
like there's definit like it's
definitely saying that there's no kind
of cartisian ego that or there's no soul
that's kind of like
continuing um on that kind of defines
who you are and stuff and I've like
always or like almost always thought
that you know for for years and so the
fact that like the Buddha was saying
that uh 2500 years ago was just like
pretty cool I was like okay like he got
one thing right maybe I should and then
um
um just like the analysis of suffering I
think is like fairly compelling to me um
like suffering as like at least
psychological suffering as being um kind
of
a being caused by clinging and kind of
Desire um attachment to the world
there's also this other part of Buddhism
that is more metaphysical that I like a
lot um it's the doctrine of
emptiness um
and the do it's it's sort of an
extension of no self where you know the
no self is saying you don't have an
essence you don't have a soul which
would be your essence emptiness is
saying that nothing has a self nothing
has an Essence okay um
and the the doctrine is sort of like
expanded upon um by this uh philosopher
n Juna um who he he kind of created this
entire new school of philosophy called
mam based on his notion of emptiness and
his view is like okay everything is
empty and what that means is uh nothing
has inherent existence or uh or Essence
to it uh so everything is relational so
like like all kind of objects or like
concepts are sort of defined by their
relations to other
objects um and I yeah there's just like
a lot
of there's like a lot of reasons to
think this is like a good way of
thinking about things
um in I guess maybe I'll I'll say one
more thing and we can kind of uh wrap
this up but like in particular like
emptiness would say that the whole
debate between idealists and
materialists about like what is what is
everything made of is it made of matter
or is it made of mind or is it made of
something else he would just say like
stop asking that question there's no you
know there it's not made of
anything um and because fundamentally
you know this chair I'm sitting on or my
body or whatever it's it's it's all
constituted by its relation you know its
internal relations uh internal structure
and its structure like the the kind of
structures that it's embedded in so I'm
defined by like my relationships to
other people I'm defined by my
relationships to you know the fact that
I'm sitting on the chair that's another
relation it's a spatial relation um and
that's it it's all relations there's no
Essence and so you just kind of get rid
of a lot of these like philosophical
debates that seem endless yeah I'm a
huge fan of the idea of a relational
ontology I think was luchiano fidi who
introduced me to it and and the
interview we did with him is great um
yeah so one of the things by the way I
love Sam Harris um I supported him so
early I've got a lifetime subscription
to his app which is really really cool
and I agree with about 66.6 recurring
percent of what he says okay um but I I
just love him his doset tones said put
me to sleep every every single night um
but anyway the other thing was you were
saying something interesting about
suffering because when when I you know
do my yoga class and stuff like that and
I'm interested in in um you know like
mental health and and AI in particular
I've read a great book called Lost
connections by Johan Hari and he's kind
of saying that we're now starting to
understand depression and anxiety in
terms of the psychosocial environment so
it's a very externalist View and um also
quite compatible with this relational
view that that you're talking about but
um I think a lot of people that do
mindfulness techniques are trying to and
I don't mean this in a disparaging way
but almost trying to address the
symptoms
so they're people that are they're
missing Connections in their lives that
give them a sense of purpose and meaning
and so on and what what Buddhism does
you know not necessarily all of Buddhism
but you know one one of the practices
here is to almost dematerialize yourself
so that these psychosocial stresses no
longer impact you what do you think
about that yeah so I definitely think
there are ways in which you can like
take Buddhism too far or like take it uh
kind of it can be unhel as well um and
yeah like you said you know I I don't I
think if you are interpreting Buddhism
to mean that you like shouldn't try to
solve any of your problems or like you
should just you know I I don't think
that's that's the right way to to go
about things I would prefer to have kind
of a holistic approach where yes you you
like try to solve kind of improve your
lot in life to the extent that you can
but also try to like change the way that
you think about things so that you can
have have more um kind of enduring
happiness and like if you do both like
that's probably the best um I also think
you know
there's there's different
interpretations
of the sort of the the doctrine of like
well there there's like four noble
truths in Buddhism um there's like the
source of suffering the first truth is
like the source of suffering is uh
clinging then or sorry there is
suffering then the source of suffering
is clinging then there is a path out of
suffering and then the path out of
suffering is the Eightfold Path those
are the those are the four the four
truths um but there's like that's like
very schematic and there's like
different schools of thought about like
well what does it mean to like kind of
end clinging or whatever I think there
are some versions of Buddhism um maybe
more in the terab tradition that I find
kind of problematic where they basically
say well like everything's suffering
first of all um not that just there is
suffering World which there obviously is
but like even like kind of life itself
is suffering or something um and
then then they basically say well
because kind of everything is suffering
really your goal should be like
non-existence basically or like they
have a conception of Nirvana this like
kind of state of of perfect well-being
which is to me anyway it seems just like
indistinguishable from just like
evaporating into nothingness and for
them I think it kind of they can kind of
make sense of it because traditionally
they think they believe in like uh
reincarnation or rebirth and so for them
it's like well when you die you don't
automatically stop existing um and so
there you need there needs to be this
like eight-fold path to non-existence um
of course like I'm sure like you know
people are going to like criticize me
and say like no nobody thinks that it
seems like at least some people think
something worrying close to that anyway
and so I don't want to be associated
with that form of Buddhism um and I
think I I'm more attracted
to the like I I suppose Zen Buddhism
probably would be the the closest um to
my kind of
like I don't know like moral uh views uh
because it's its understanding of like
Liberation or Enlightenment or whatever
is much more down to earth um and so
like for example it would like like they
will say that like Enlightenment is
something that happens like while you're
alive like it's it's not like Freedom
it's like not at least primarily or
exclusively like being like freed from
the cycle of rebirth um and they have
this view
where really what you're trying to do is
act uh
spontaneously and in a way where you are
kind of not attached to what happens as
a as a result of Your Action um which
might sound kind of like wild or
something um it is supposed to be
connected to like compassion and stuff
so it's not like you're completely
indifferent to the world but like the
hope is the idea is that you should
cultivate compassion so that you act in
ways that are like beneficial for other
people but also you
shouldn't you shouldn't be kind of goal
oriented about it or consequentialist
about it it's actually like kind of
antithetical to EA honestly from a a
philosophical perspective it's like it
it's more I would say more kind of
virtue ethical where it's just like
cultivating
compassion but you're like you're
not clinging to like oh I must like my
actions must have these consequences
otherwise I'm going to be like super
depressed and frustrated about it and
that that's kind of how you kind of
relieve suffering as as part of this
yeah it's very similar to Kenneth
Stanley's book why greatness cannot why
greatness cannot be planned yeah and um
so so you know essentially it's the
Serendipity part which is what Kenneth
talks about but also the compassion part
which which is interesting but um I read
a book by Dan Harris many years ago and
he was on the San Harris app and he was
saying oh you know this was great I I
got this Zen state but then I had to go
to work and I had to get stuff done and
there's this interesting JY position
when we talk about serendipity in
general because it's great but there's
also a lot of things in the world that
do need goals and objectives and
alignment and so on because we've
actually got things to invent we've got
to build societies and so on so how do
you kind of reconcile that yeah so I
think the way and I don't know this is
all like tentative right now I mean I I
definitely I don't it's very possible
that in six months I'll be like oh
actually Zen Buddhism is crap or what
but um I'm still kind of on a on a
journey um here but yeah so obviously
like we we do need goals um and and kind
of structure and stuff um but it also
seems like in the future we may need
that a lot less precisely because of AI
um so there is kind of this I'm thinking
that there I'm thinking that there may
be this kind of um
nice uh that kind of Zen and like the
fully automated future that we seem to
be approaching might be kind of like
good companions because you know I'm
looking for a philosophy
that
um that I think could that could kind of
provide us meaning in life even when we
don't we don't really need to do a whole
a lot like we don't need act at least
the humans like don't need to be
thinking about like oh how do we like
run the economy and like have all the
you know we can just kind of be
spontaneous and serendipitous and the AI
are just handling everything
else so yeah I guess that the hope is in
the future technology could allow us to
just be kind of Zen enlightened beings
or something um nor this has been
absolutely fantastic I've been a huge
fan for a long time now where can people
find out more about
you yeah so um I guess two main places
first of all um if you want to like kind
of get involved with like my um like my
research and stuff uh we definitely
there's like a lot of people um at
alther who are volunteers um and so you
can go to Al luther. and there's a link
um on there to go to our Discord and
like there's like multiple channels
under the
interpretability category there that are
like all me and you can like at norabel
Rose there and get my attention um if
you just want me like I don't know
ranting about things or whatever um you
can go to my um Twitter profile Nora Bel
Rose um so yeah Nora thank you so much
it's been a pleasure yeah thank you

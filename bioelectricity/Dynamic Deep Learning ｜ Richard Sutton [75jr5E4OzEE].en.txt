hello everyone hello sorry the mics
don't work too well I think for here
okay no work fine so we're gonna get
started thank you very much for coming
welcome these are the Aral seminars
which uh today are as always sponsored
by uh instadp uh Google Deep Mind and
ionic and this is a very special seminar
uh for us because this was a seminar
series that pop up from a reinforcement
learning reading group and as a bunch of
reinforcement learning researchers uh we
all in the group uh initiated our
adventures with a very nice book from
the person who is now presenting today
so I think there is no need of any
presentation we're very happy to have
today with us Professor Richard salon
who is going to be talking about Dynamic
deep learning Richard thank you
thank you Bo it's really a pleasure to
to be here today and have a chance to
talk to you share some ideas with you um
I was I've never been to Imperial before
and uh I've just learned about all kinds
of interesting things going on here it's
all very exciting and
uh it's uh impressive in many
ways so today
uh let's start with some general remarks
before I get into really what I'm going
to cover today um AI artifici intell is
really
ambitious so what I would say I would
say that AI researchers seek to
understand intelligence well enough to
create beings of Greater intelligence
than current humans pretty awesome
pretty awesome goal uh I think reaching
this profound intellectual Milestone
will enrich our economies challenge our
societal institutions it's going to be
unprecedent it's going to be
transformational but also a continuation
of trends that are thousands of years
old people have always created tools
technology and Been Changed by it um
this is what we do it's what humans do
the next big step is to understand
ourselves this is a quest Grand and
glorious and essentially
human yeah of course it's also totally
hyped
up uh but I don't think I'm hyping it
here the things I'm saying here are just
true this is a this is a grand Quest um
but it is hyped up it's it's not a good
climate for science you know science
likes Things To Be steady and calm and
focus on the important fundamental
issues whereas AI is like it's all
there's a lot of money going into it
there is so many companies
uh there is people you know arguing that
what I'm doing is is enough you don't
need to do anything any else
um and uh it's it's not it doesn't feel
I don't know if you guys sense it but
but I sense that it's not a really good
climate for science
so for example large language models
really hop into the thing really
important thing really powerful thing
but they can't just accept that they're
going to be really important and really
valuable they have to say oh this is all
that the Mind does um the mind they have
to claim the largest language models can
do reasoning and and maybe they're
conscious and all kinds of crazy stuff
instead of you know because the real
scientific attitude you find out you
figure out what you can do and try to
understand what you can't do and uh and
quietly make progress I think we're not
really that's not really the the the
culture that I feel in our fi at the
moment so I think I just a little bit
disappointed by
that but I don't doubt this that it's an
important question that we're trying to
address okay now some others of my
perspective so I'm one of these guys
that are trying to understand and create
maximally intelligent agents and
intelligent an agent is defined to be
intelligent to the extent that's able to
predict and control its input stream
particularly its reward this is the way
I would Define intelligence not the way
everyone would find intelligence not to
pick on the large language models but
they don't actually try to predict
control their input stream Technically
when they're running uh they don't even
have an input
stream um so we want to control we want
to affect our
input and uh the creation of these super
intelligent agents and super intelligent
augmented humans I think will be an
unallied good for the world I'm not
going to say that the world is going to
be an unallied good thoroughly good but
because the feature I think is going to
be tough because we're in a what's
called a fourth turn turning if you
don't know what that is you might check
it out sometime it's kind of an
interesting set of uh ways of viewing
societal
change um well the path to intelligent
agents runs through reinforcement
learning now through the lmm um but the
current biggest current bottleneck to
ambitious reinforcement learning based
AI is that our deep learning methods
that we rely on are inadequate to the
task they're not well suited to the task
and that's what I'm going to talk about
today that they're they fail when we ask
them to be we put them in a dynamic
situation where they have to continue to
learn as you know I think all deep
learning methods uh when when they're
deployed when they're inter when they're
acting on a robot or or um interacting
with humans they they no longer learn
they learn only in advance they are sort
of transient Learning
Systems
now one more sort of introductory slide
just to tell you where I am where my
thoughts are and I I just put this in
like like two minutes ago actually um
this is the standard architecture of
reinforcement learning and if you see
all these components of a modelbased
reinforcement learning agent that's what
MBR is model reinforcement learning
agent takes in observations adits
actions and it also takes in a
reward um and it basically composes of
four
parts
perception takes in the Stream of events
and admits a representation of of the
current situation or the state probably
it's a feature vector and that passed to
the policy reactive policy to pick an
action and those two right in the
middle those two these two perception an
action uh those two together form a
complete agent but if you want the agent
to be adaptive and if we want it to be
intelligent it's going to be have to be
adaptive as I have defined it then
you're going to need a value function to
say how well things are going and then
use the the TD from the value function
to adjust the policy so this diagonal
line through the policy if you can see
that represents um not an informational
transfer like the state comes into the
policy adits the action but the diagonal
line means you're changing the function
implemented by the Box okay so value
function changes the policy now at the
same time the full agent a full
modelbased agent you also learn a
transition model which is a model of the
world uh and its state transitions
observing the states here here you see
this the state at a particular point in
time and you might uh notice the state
at a particular point in time and then
you take an action and then you find
yourself in a new state after amount of
time and that's a transition and your
model transition model would model those
transitions by observing the data and uh
then you can ask questions of your model
what if I did this action instead what
would happen then oh you to this state
then you could apply the value function
to that state and you get a TD error and
then you could use that to adjust the
policy again and we call that planning
because whenever you're using a model
and not experience to adjust your
anything we call it plan
just these just definitions of how I'm
going going to use the
words now in the full um
architecture we also we have not just
one policy and one body function but a
whole set of them and that's meant to be
suggested by these these that are
stacked up behind the the true policy
and and stacked up behind the value
function we more more value function so
that's where you would posee subtasks
for yourself and you learn skills how to
achieve achieve other things other than
the uh reward while always being
cognizant of the effect of those other
things on the reward okay this is just
on the side right I hope it's
interesting but it's on the side and um
a lot of this is developed in these
papers you might want to it could be
checked
out um okay now let's get to today today
we're going to talk about Dynamic deep
learning but really it's about uh uh
paper that my colleagues uh and I wrote
and the title is helpful for clarifying
the topic the topic is loss of
plasticity and deep continual
learning and I'll Define all those
things in a minute but now is my chance
to talk about me a little bit more um I
acknowledge I have many jobs keing
Technologies the University of
Alberta Amy the Alberta machine
intelligence Institute my own
reinforcement learning lab and open mind
research I want to tell you about this a
little bit this is our our logo open
mind is a a new Institute that we've
created
um that's um devoted to doing
fundamental research in along the lines
of reinforcing the Alberta plan um for
people who have already graduated
obtained their degree uh but want to uh
do research in this area and it's
distributed it's we have kind of a focus
in Alberta but it's distributed and
people can be all over the world and uh
if you might want to do fundamental
research in this area and
you uh want to focus on that rather than
become a professor um you uh may want to
apply to be a fellow of the our
Institute okay now here's my main
message main message is that deep learn
learning doesn't work for continual
learning um and by not work I mean that
learning slows and eventually goes to a
very low level of learning we' it's lost
you've lost
plasticity and by Deep learning I mean
this all the standard uh artificial
neural network methods specialize as
they are for this non-c continual or
transient learning and I also mean
without replay buffers because I
consider themselves an acknowledgement
that deep learning doesn't work
now uh Better Learning algorithms that
are specialized or well suited to
continual learning they're not hard to
find and they can apply to deep learning
uh so I don't think there's anything
wrong with artificial neural networks we
just need to find better algorithms and
we only need to to find we have to start
looking for them and uh so that's my
main message and then this
work
um details and substantiates this
claim okay so I'm going to have
demonstrations of loss of plasticity and
deep learning um and this and and and
the attempts to maintain it and this
will be in classic supervisor learning
problems like imet and CFR 100 with
residual
networks um and also in reinforcement
learning but primarily in these classic
supervised learning problem domains you
know now already that I'm not really a
supervised learning guy I'm aiming at
reinforcement but I need all those four
boxes that I showed you they all need to
learn continually because you live in
your world and you keep adapting to
whatever it throws at you and you you
have to model the world and the world
changes the world is infinitely complex
you'll never be able to anticipate all
the ways that might change and so you
have to continually learn and then
hopefully I'll have just a a little bit
on some goals and ideas for Next
Generation of deep learning that would
be well suited for continual learning
settings now this work has just been
published in nature so I got I'm really
excited about that the first time ever
been published in in nature and I just
but I want you to realize that that like
uh even when you have a publication
success it's often proceeded by a large
time where you have publication failure
uh so this happens all the time
it's just if something is fashionable
you can get it published in nature right
if it's not fashionable you can't get it
published at all sometimes I I've
experienced this many times all my best
Publications were were really hard to
get published you know yeah temporal
difference learning when I published
that really hard options all all almost
almost it's almost always true that the
more novel and
and and
groundbreaking your results are the
harder it will be to get them published
so if you're having trouble you know
take heart a little bit and realize
you'll have to persevere and if you
persevere you may eventually get
published but it's not guaranteed it may
never become fashionable um so we we in
particular tried to publish this over
like four and a half years and we're
unsuccessful you can see it in archive
archive is
good um plasticity just means the uh the
ability to learn and so loss of
plasticity just means means loss of the
ability to learn or not being able to
learn continually not continual learning
hope you're getting comfortable with the
way I'm using these words maintaining
plasticity is maintaining the ability to
learn and so in AI since we want all the
four boxes to learn we should prioritize
maintaining
plasticity and we're not the first
to see this problem see at least hints
of the problem uh it's very closely
related to the idea of catastrophic
forgetting where in deep learning you
learn some things and you learn some
more things you often totally forget the
old ones now loss clity you learn some
things you learn some new things maybe
you learn some new things after that and
eventually you can't learn any more new
things not the same as loss of as
catastrophic forgetting it's sort of cat
catastrophic loss of
plasticity so this there are also hints
of this in the early neural networks
work in the sitech literature and there
a few works like uh Ashen Adams that
showed the failure of warm starting warm
starting was like you thought oh I'll
teach this this neural network on on
part of the data and that'll warm it up
so that it'll you I give it the second
half of the data it will be faster but
the fact they found the opposite if they
trained it on first on the first bit
then it was slower on the second bit and
and they did much better if they just
put the two parts into one big thing and
trained it all once from scratch was
kind of surprise it was surprising seen
as a failure um that was one of the
earliest demonstrations
um and Primacy bias Nic Ain capacity
loss CLA ly and others these were shown
in in reinforcement
learning okay but no one really did a
thorough demonstration in supervised
learning uh of this phenomenon and and
because you have to do it thorough it's
a Nega sort of a negative result you if
you get hints it doesn't change people's
minds and so the significance of this
piece of work was that we did it really
thoroughly
and and dotted all the eyes and crossed
all the tees and and considered all the
possible ways you can get around it and
and uh have have something that's pretty
incontrovertible okay so let me show you
the first one of these demonstrations
law of plasticity in supervised learning
and we're going to use the classic uh
domain of imag
net um so image n you have this database
of millions of images and there
like this is an image this is a this is
a a shark hope you hope that's obvious
to you this is somewhat down sample and
we have to use somewh down sample images
uh but there a thousand glasses each
with 700 or more images and it's widely
used um but we need to change it because
it it's why Lees as a pure supervised
learning problem like they take all data
they they they they they train on all
the data at once and and then they
gradually freeze it they train and train
and train multiple presentations and
then finally they freeze it and actually
they gradually freeze it and slow it
slow down learning until they have a
nice stable State we don't want to do
that we want to have something that we
continue to learn so how are we going to
do that so we're going to make a A
variation of the problem which called
continual imet so and we see it as a
minimal change to the classic one so we
do the usual thing we take there's 700
examples remember for each class so we
separate that and just training 600
training and 100 test ones and then we
take them in pairs
so like this might be the first pair
we'll show you crocodiles versus guitars
and you have to learn to distinguish
those
two okay and then when you're doing well
we've seen we've seen all the examples
of those pairs we go on to a second pair
like so think about how that that's done
we we've uh the network only has two
options two outlooks a or b or yes or no
uh crocodile or guitar and so then when
you want to ask a new question you add
well since I'm never going to ask you
again about crocodiles and guitars I can
actually throw out the heads the final
uh two neurons of the neural network and
and uh and replace them with two new
ones two new ones for the new task the
new task is distinguish game controllers
from fish okay and then once you've
learned that I I erase the heads and
give you a new problem both ties
versus oxin I guess okay and this so
This continues and you can do many pairs
because remember there's a thousand
pairs so you might think I could make
500 pairs but you can actually make many
more because you can reuse ones as long
as you don't re reuse the pair so how
many pairs of uh of thousands of things
it's it's it's like it's like almost th
squared so you have lots of pairs we can
do this
forever um performance measure is I'm
just going to measure the percent that
are correct on the test set remember I
present all the examples for the two
classes and then I have some saved out
ones I measure performance on them and
then I average this over many many runs
and in the many runs I vary the the
pairings of the class I don't you don't
have to worry that maybe the first class
is hard but the first class is easy
since they're all averaged over all the
possibilities okay that's sort of the
problem and then there's a whole another
set of details for the Learning Network
and
um talked about how the heads are reset
the final heads uh the structure of the
network is pretty standard it's a little
bit narrow because we only ask you to
classify two things at once we we don't
want it uh to be infinite in its size
compared to the things we ask of it
um so it has some structure of
convolution layers um and interconnected
layers and notice the last one is just
two those are the last two
heads okay and then we use batches we
use epics and all the usual ways um now
it's important that the weights are
initialized in the standard way and that
it's only done once it's only for the
very the very beginning uh on the first
tab before the first task we we
randomize all the weights small initial
values this is how this is a standard
practice uh but it's only done once you
know when you get a new start to get new
data from the new task you're not told
it's a new task and so there's nothing
no opportunity to reinitialize or do
anything and you maybe you don't want to
reinitialize think about it maybe you've
learned one thing and now you're going
to learn something new maybe what you
learned on the first one will maybe the
features you learned on the first one
will help you um get those last St heads
the for the new task um they maybe
they'll be able to much better because
they have better
features okay so we start with just
plain backdrop momentum cross entry loss
Rel activations the usual things and we
of course varied all these things but
the the standard case is is this one and
yeah many variations get representative
results and then we I want to ask you
how do you think what what do you think
will happen over over the sequence of
tasks will performance be better on the
first task or the second
task were you voting for
first Al I think it'll improve andur you
think it will improve and since that's
that would make sense that it should
maybe the second task will be better
third task maybe will be better at some
point you're not get any more Advantage
from Having learned good features
that would be good that's the way a
learning system should work
okay okay uh this is the beginning I'm
going to show you just the beginning
first and obviously it's going to depend
it's going to depend on on your well let
me do the axes first the axes are task
number along the bottom the first pair
the fifth
pair um and this is percent
correct okay so if you and these are
well what are you tuned for this red
line was tuned for doing as well as you
could on the very first task so it's
sort of
fast right and so that gets up to about
89% correct on the first task
um this is slower the backrop alpha
alpha is the step size so um this Red
Alpha is is 10 times faster than the
orange Alpha okay with orange Alpha you
at least after a while maybe get up to a
higher
level but uh this one's the fastest at
the very beginning okay does this all
make sense okay now the linear Baseline
is well what if you didn't have all this
network stuff you just took the the
pixels and you learned a linear map from
the pixels to the
class that's actually not you can do
much better than than random what's
random random is two classes so it's
5050 50% would be random
um yeah and I what else I the shade
region is one standard error linear
Baseline is importance of the L of the
linear heads there's no nonlinearity
there's there just a single well two
units and so this doesn't have to be run
actually with with uh multiple tasks
because you know there's nothing to save
from one task to the next the two heads
are replac the two linear heads are
replaced that's the whole network so you
don't have to run it many many times
well you have to we to run it many times
and we establish that you could get 77%
approximately correct with with you uh
with a linear system okay so we don't
that's that's really poor performance
right that would be bad because you're
not even using all that Network to do
anything okay so maybe we're improving
at the beginning this guy seems to be
improving but what happens in the long
run okay that's the real question so
this is what happens in the long run
and so these are this red and the and
the orange are the same curves you saw
previously just extended so let's go
with the red one which started out at
89% which is this point right but this
first data point is an a is a bin an
average over 50 Ex because there's yeah
just for graphing purposes to get rid of
all the jaggies it's good to average
over the first 50 tasks and so over that
first 50 tasks you really can only do
about 84% correct because you're already
starting to decrease in performance and
and then you see that playing out uh as
you go more and more tasks you get worse
and worse until you're performing worse
than just a linear
Network and if you use a different set
of
parameters uh if you're slower you also
get worse and worse and then level out
at just a little bit better than the
linear Network this is an even slower
step size and it also does poorly okay
so this is this is this is one uh setup
but it's representative we've seen this
pattern over and over again uh if you
vary the details you can get small
variations in different different shapes
but um this is this is this is the
pattern that you always that you will
always get um and in the long run you
can't do better than the much
significantly better much better than
the linear Baseline and also if you
introduce variations like bring in atom
atom actually makes it worse Dropout
makes it worse any of these things just
makes performance worse
um okay so what's the summing for good
hyper parameters plus decreases across
tasks nearing the performance of the one
layer line your network or worse so this
is what I'm calling catastrophic loss of
plasticity
question have any explanation for why is
that an al01 you keep seeing this
city but that smaller Alphas of level
out and retain some
Bas
um we haven't thought about arguing that
the orange line is good you think the
orange line is
a
failure uh
um I've never asked the students who did
this all the students I listed Shaban
doare and
Fernando uh all these about that
particular question I'm not sure I can
answer
that any other
questions thank you
how do we explain uh how the like the
the the brown line is definitely
increasing and as we saw the orange line
also if we looked at detail at the first
10 tasks it also was increasing so like
uh that's that's what Alex called for
that we should get some improvement in
the beginning and so that makes sense to
Alex and the The Way We Were
understanding it was that it was you
learn features during the early task and
those features are later are are useful
on on the subsequent
tasks uh and if you're particularly if
you're learning slowly then then then
uh well yeah empirically we're seeing
that if you learn slowly then you're
getting a much greater effect of that
nature you're getting savings across the
tasks what you learn maybe
you maybe takes a while to find good
features and if you can only learn
really
slowly um that you get more accumulation
over over over over uh over
tasks okay
um so there are better
algorithms as I said things like Dropout
and standard variations make things
worse
um
uh but there are a few this is this red
line is the same line we saw before
we've just rescaled it you know he's
going way down low uh but but if you
have a good algorithm you can you can
show Improvement so um this is shrink
and perturb and L2 regularization L2
regularization just means you something
you add something that encourages the
weights from getting too big prefer
small weights and so this uh keeps them
in a in a in a a labile range so they
can keep training keep learning from
more experience um and I I'll show you
some details on that in a minute
shrinking perturb does L2 regularization
but it also does a random perturbation
of the out of the of the weights and so
these two are good we're going to
repeatably see these are good at
partially solving the problem and
continual backrop is our own algorithm
um and it's basically just like backrop
it's falling gradi descent but in
addition uh we select just a very small
proportion of the units and we the very
the proportion that are least useful to
the network and we reinitialize
them so like less than one unit per
example very small fraction of them and
we have a utility measure so we can
measure their util how much they're
contributing to the Network's uh
behavior and if the ones that are used
least uh will be candidates for being
reinitialized and that small change uh
solves this problem you have a question
online about how would you
select yes it's a I'll talk about that
in a minute let me let me just postpone
that um but it is it's a measure of its
utility and as you'll see many of the
units are not useful at all this is what
this is a phenomenon of
backrop okay okay and maybe I have it
here continual back propop scas grading
descent with Selective reinitialization
just like backrop except
reinitialization this this are only
parameter with the raid which we
reinitialized I said it's very small
very slow um that's what that is a
hyperparameter and it's it's it's we
show that it's better to reinitialize
selectively using a measure of utility
so for example shrink and perb also
random makes random changes um but it
doesn't do it selectively it's always
just taking all the weights and wiggling
around a little bit and this is an old
idea uh it's really sort of a kind of
generated in test we we're keeping the
units that are contributing to the
network and we are taking ones that are
not contributing and we're going to
randomly vary them um and this is
this this idea existed
before uh rupan mmud and I did some work
on it it goes back to the uh 60s like
Oliver Selfridge did pandemonium and it
also was like this generating and units
in a network and but the thing about
continual backdrop is it extends the
idea to General multi-layer
networks okay
so um I I want to show you in some sense
convolution networks are old school old
style uh now we want to do residual
networks which also has many layers like
18 layers but we also have has cut
shortcut connections or cut through
connections so you don't only it's not
strictly layered anymore this is more
more modern
architecture and we're also going to
change the problem a little bit we're
going to use uh CFR 100 and we're going
to present five classes five classes and
then we're going to progressively add
more so we're not going to like like
before we had pairs another pair a
different pair just in different pairs
same number now we're going to add we
start with five then we'll add five more
so we'll have 10 so so this is the the
old five we're still going to present
those yeah and we're also going to
present the new five and the next we'll
present the old this 10 will keep
presenting all that 10 and we'll get
five more so there'll be 15 and we just
keep going until you get all 100 of them
okay now if you think about that the
second task will be harder than the
first one right the first one just has
five classes this one has 10 classes
next one have 15 classes so you expect
overall accuracy to go down over time
and see can't measure overall accuracy
if you want to see a loss of performance
what we want we want what we use is we
show performance relative to a network
that's trained from scratch with the
same number of
classes
so say you had 50 classes you could you
could
just reinitialize your network give it
50 in the all way the 50 are all
presented at once and that'll perform a
certain well and then you could do it in
this increment mental way you get 5 10
15 20 so farth and and and what what
what will it be will be will it be uh
after you get to the 50 will you be
performing will you be performing worse
or or better than if you see them all
all all all at once does the
progressivity help you or hurt you okay
and what we find is quite so this the y-
axis here is the difference between
these two did you do better or worse
because you're presenting them
incrementally got it okay so what we
find is that yeah there's definite
effect of of you do better uh by doing
it incrementally at the
beginning you you you gain a couple
percentage points of accuracy which is
quite a significant effect and then it
decreases and if you do usual back poop
after about I don't know 40 45 uh it's
neutral and then and then you get this
loss of plasticity effect where you do
much much worse than uh and presenting
them all together at once shrink of a
turb isn't quite as bad but it's bad
you're you're losing you're losing
plasticity and continual backdrop you're
able to maintain the same level you get
exactly what Alex asked for get an
improvement at the beginning and then
doesn't you
plateau okay so that's that's the Z
pattern yeah good because you you said
you keep the same performance in the
long run with Contin backr but at least
are you able to reach that performance
sooner than with a Network that has been
trained from a scratch with with all of
them at once so is a learning time at
least faster with continual back Pro
than when
you because we're trying to see the
advantage right of just throwing all the
data at once to a network learn all the
methods are are getting an advantage in
in learning time at the beginning right
that's what we see yeah that's F but at
at the end at the end there there
is and then we're measuring performance
only on on the full set yeah but my
question then is like since performance
is the same did you notice that uh
continual backrop was reaching that
performance sooner than that Network
learning from a scratch
I don't think that I don't think there
is any measure of of
sooner
um you mean once you're say presenting
all 50 of them yeah I mean we so so in
the Progressive case yeah you've been
you've presenting you've been presenting
them in smaller groups all along and so
for most of the classes by the time you
get to all 50 of them you've already
learned a lot that will to be faster but
I wanted to confirm if you well you'll
you'll get you know 45 of them pretty
pretty well at the beginning remember
the performance measure is at the at
the actually I didn't I didn't say what
the performance measure exactly is here
and maybe I'm not sure but I am sure for
the previous example previous example
and I think we're doing the same sort of
thing is we present things and and then
and then we have this test set so we we
we finish learning and then we measure
performance there isn't there isn't a
speed within within the group you know
what I
mean is asking is for like the number of
classes 50 for example when you train
the network from Ranch 50 classes you
present as many examples to that Network
as you present to the network that doing
yeah definitely the same number of
examples that's is slower it has seen 49
times the train set sence no no no the
total is the
same the total is the same yeah well the
one that is trained yeah
yeah is that right
yeah
question maybe I'm not
sure but this I think you'll find
interesting here we're looking at we're
looking inside the network trying to
figure out what's
happening okay so as we increase the
number of classes
this this this graph is showing the
percentage of units that are inactive
that are dormant or dead they're which
means they're defining that as active
less than 1% of the time and so you
start out with small random weights
everyone's active oh I guess I guess
active means um they're they're reu
remember so if the activity is less than
less than uh zero they're they're capped
at zero B floored at zero and so active
means uh their their activity is greater
than zero and uh at the beginning
they're all they're going to be greater
than zero half the time and so none are
active uh less than 0% of the time and
then as as we as we train just for
standard backdrop
um yeah actually I should have said that
in in this in this experiment if you do
standard back poop you don't even you
you you you don't perform it doesn't
work at all you have to already use L2
regularization so so that's why in these
experiments we don't show you results
for L2 regularization because everything
is using L2
regularization that's that's needed okay
so that anyway that based learning base
Learning System
um gains up or as increasing numbers of
of dormant units more than more than
half by the end of 100
tasks um 100 classes 100 classes and um
shrink and perturb does much better in
terms of many fewer dormant units and
continual backrop has hardly any and we
also measured this the diversity of the
representation
uh uh which means often often you have
fures that that become similar to one
another and they they don't add anything
to the uh linear rank of the
uh the you have you measure the the
number of uh I don't even know how to
say it uh number of independent
components of in in the uh in the in the
features in in the network um and so we
scale this between zero and one and we
we can see quite quite clearly that just
based Learning System uh the uh we lose
diversity we lose rank of the
representation and with some of the
other methods we can U preserve
diversity okay now I tried to say here's
here's where I emphasize the robustness
and in these problems and we've done
other problems we've done idealized
problems we've done amnest and we're
shortly I'll be doing reinforcement
learning so across network architectures
uh across activation functions not just
just ru but a whole variety of them and
across the hyper parameters we see that
just plain the Deep supervised learning
loses P SE dramatically and continual
setting L2 regularization improves it
but just a little bit uh or make and and
shrink and perturb often will help a
little bit further weight with the
weight randomizing continual back propop
does the best in maintaining the
plasticity it has one hyperparameter but
as long as it's uh it's it's we're in
sensitive to that it's just can be set
very
small so now let's go on and do finally
reinforcement learning so we're going to
do ant Locomotion
um maybe you've seen this problem we
have this ant uh I thought I have some
videos here let me scoop ahead yeah some
videos so here's the ant on the right
side
um moving rapidly forward the object is
to move forward as rapidly as you can
this is an ant that's not moving forward
at all it's doing very poorly this one's
doing very well and we're doing this we
measure the the the reward is forward
motion and the uh actions is to control
these eight joints where they are red
marks and this is a standard task and uh
here the performance we're showing the
time steps we're training for rather a
long time often you will only change
train for a million or two time steps
we're going to go 20 million and we're
going to measure the rewards per episode
which is one trip of running forward as
fast as you can then you're brought back
to the beginning to to run some more and
here we have a non-stationary problem
and that means that um we're going to
vary the friction between the feet and
the ground so every two million steps
we're going to vary friction we're vary
it don't I have a
I don't have a figure for that but
sometimes it's fast sometimes it's the
feet are sticky and sometimes they're
slippery and um so it changes the nature
of the problem every time you you uh
vary the friction with the ground so
these um sort of actually I call this an
alligator graph because it looks like
the back of an alligator's tail you uh
you're doing well and then there's a
switch in the problem and you your
performance Falls and then it recovers s
and recovers and um so let's first look
at the standard reinforcement learning
algorithm
po um and uh it it does well at the
beginning but then at a certain point it
plateaus and if you just keep training
keep training um it degenerates and your
ant that used to look like like this
one
oh think I must have skipped the slide
yeah this was supposed to show the uh
the
slipperiness uh we're going to vary that
and
um and uh here's our standard poo
performing well but if we just keep
going it starts to end up looking like
that second one starts looking like this
guy and uh performance is actually uh
deten so it's worse than how you
started and you can so this is this
so people have seen this before but you
know when this happens they tend to just
uh stop they they stop training early on
and if you stop training you can you can
avoid this this this this fall but the
real Learning
System would be faced with that okay
this Ali online asking could a similar
effort be seen if Mass was changed
instead of friction excuse me could a
similar effect be seen if Mass was
changed instead of friction yeah we
believe so have every reason to believe
so as long as there's something well
actually I guess I should move a little
more quickly because I'll answer that in
the next in the next slide um but yes
the quick answer is yes okay now if we
tune the learning algorithm we can do
better but we still see a a drop uh a
loss of plasticity over time and as
before
L2 L2 and U continual backrop can
largely solve the problem largely solve
the problem now I want to show you
um this one this is the same thing
except there's no changing in friction
there's just just regular ant okay and
um but reinforcement learning does
involve changes it always involves
changes because the policy gradually
changes and um we it involves temporal
difference learning temporal difference
learning the targets are always changing
so really even without changes even in a
stationary problem you should get this
we get this effect we we realize later
we get the same effect and performance
just drops and this is where I say well
why didn't why doesn't everyone see this
well they do see this and they just they
just stop training after a few million
and they don't they they cuz you
arguably it's okay you're just
interested in a good policy you can stop
and use that policy but if you want to
keep learning it's a big problem now if
you tune your parameters you last longer
and you get higher but still you you
suffer and you lose all your plasticity
regularization uh can maintain it and
continual back bout can maintain it
better okay so here now we'll take a
closer look inside this algorithm and
again we look at the percentage of
dormant units
and those
are increasing to more than half L2 and
continual backrop keeping it low stable
rank stable rank is the same pattern and
here's a new one the average weight
magnitude you just looking at the units
and look at how big the weights are and
with standard backrop the weights are
getting bigger and bigger and
bigger there's no there's nothing that
that forces them to be small and maybe
that's the full explanation just the
weights get bigger and so they're harder
to change
um L2 regularization makes them forces
them to be small and maybe that will
interfere with performance um and so
they give some insight into what's
what's going
on so conclusions deep learning networks
are optimized for onetime learning in a
sense they totally fail for continual
learning and these simple changes like
continual backrop can make them
effective for continual learning we
going to rank units by the utility to
the network preserves the most useful so
I guess I never got into details of
utility and I guess that's because I
don't think the specific way we rank
them we don't think it's necessarily the
very best
uh um in fact that's one thing we're
working on now how to make how to do it
uh better in a better way um you know
what if your what if your network is
recurrent or what if you're right now
we're using a fairly local measure we
have the units each unit is looking at
its outgoing weights and if the the
immediately outgoing weights if they're
all zero Z for example you know you're
not useful uh but they may some of them
may be large um but you don't know that
you're connecting to someone who's your
large weight is to a unit that is useful
which which would be necessary in order
for you to be useful so um that we're
still experimenting with that um I think
there's a large exciting World ahead of
deep learning networks can learn
continually and and particularly it
opens up possibil new possibilities for
reinforcement learning which is
inherently continual and modelbased
reinforcement we have all these
different components that that are
interacting and all of which are
learning simultaneously and
continually okay so that's my completion
of the demonstration part and now let me
just say a few I could I could say a few
words about
uh ways we're going to try to solve
these problems further but it's a good
time for questions right now there are a
few online actually yeah good so one
very link to what you were presenting
was what will happen for continual
backprop without the L2
regularization well it is it is
necessary uh in these later in later
problems to make it work
well
so so it wasn't in the first
problem
um so I I don't know just doesn't
perform as well in in some cases yeah
kis who as well ask what kind of
Advantage continual learning can gain
over the normal deep learning well it's
it's it's it's really a change in the
problem right we're asking a continual
problem
where well I guess backrop uh uh
reinforcement learning is always
continual
um let see I guess didn't I just show
that didn't I just show that if you have
a changing problem and you need to do
continual backrop or regular back poop
totally okay let's just go to uh in
Chris so just to wrap my head around a
bit more and just kind of talking back
yeah their queries on efficiencies of
like Baseline training say on like you
know and number of tasks versus like n
minus 5 to end tasks um I understand
that in the supervised setting the
Advantage may be more for like fine
tuning or something like this where you
have like some existing model and you
want to addal tasks or something like
this yeah so like if say we're doing
large language models and we we train
them up and then like another uh another
week's information comes out on the
internet new news stories and You' like
to have your large language model be up
to date with the new news
stories that then you it is that's a big
problem nowadays what they actually do
nowadays is when
they they they update the miles
infrequently because every time they
update them they have to uh update they
have to clear out all the whole network
the old Network they clear it to scratch
start over from small weights and they
train all the data at once they have to
start all over again you'd like to be
able to just add the new data and like
the reinforcement context seems like a
lot
more it's even it's even from like uh
from initial training it seems like it's
better because reinforcement learning
well more continuous in nature so it
seems like in the super bu setting fine
tuning is like like sort of this key
application because that way you don't
have to waste old models whereas in
reinforcement learning it's kind of a
new paradigm
training does that make
sense got to say it more concisely sure
so in the supervis setting it seems that
fine-tuning sort of to me it's seems
it's more of the main application
because if it's both Cas you need
continue learning really uh so just
going back to their question if the uh
if it's accuracy is the same as the
Baseline uh with continual learning and
with you know that Baseline number of
tasks what's the sort of notable
Advantage unless you use like less
compute in the continual learning
setting
number increase the number the other one
is one for one fixed number sure life
you will always have more numbers coming
in so yeah but if you're adding more
numbers that's a fine tuning task right
if you're adding new tasks later on
after initially training a base model
are you suggesting that fine tuning is
an alternative to continual learning
well no I'm just suggesting that for
supervised learning it seems like this
the continual back propagation seems
like more of the uh the usefulness
because if the if you're getting the
same level of accuracy um getting the
same as a as like you know a baseline
number of
tasks if you go back to the graph right
when you have say 50 tasks right when
you're comparing it to the continual
back
propagation the the accuracy is about
the same which I showed you're not
getting the same level performance um in
in sort of the the prior chart right on
the supervised supervised task that's
sort of what it looks
like this was just sort of like you know
we'll talk later I'm obviously
confused than
you hello uh excuse me if this is a bit
of a critical question but in
the in the evaluation of the where you
add classes uh
continuously um like you start out with
five classes and then add another five
and so on uh and you said you mentioned
that you you trained
on uh equal amount of epoch I assume it
could the deterioration of the normal
back propagation not just be explained
by the fact that it sees fewer examples
of the new
classes yeah I guess I I was UN unclear
about that I and and I I am unclear in
my own mind exactly how that's done but
uh absolutely the the intent was to do a
fair comparison neither one would have
more
data
and yeah I guess I'm not clear how that
was done sorry about that no wor
sorry what happened to the utility of
the recently reinitialized weights you
said you reinitialize them based on
lowest utility the recently
reinitialized ones jump in or or say low
this one I can
answer when they're reinitialized when I
said they reinitialized in the standard
way but it's a little bit subtle just a
tiny bit subtle um we reize the incoming
weights to the new unit to be random in
the in the ordinary way but the outgoing
weights are re initialized to zero
because we don't want them to interfere
with the Uno the current performance of
the system if the outgoing weights were
nonzero they would mess things up a
little bit
um and then so then um this new this
newly initialized unit what will its
utility be its utility will be zero
because it's hard going with all zero
okay so if we really um ranked by
strictly by
utility when you add a new one that's
the one you're G to throw away and
reinitialize the next time so we do need
to to keep track of the of the age of a
of a unit and
uh prevent a a newly created unit that
has that's very very young from being uh
placed good job okay um whether do you
see um research in dip learning and
reinforcement learning going do you
think the focus will be in modifying the
backprop algorithm for the continual
learning and reinforcement learning
setting like in this paper or do you see
it going somewhere
else well what do we consider it's
continual back propop modifying the
algorithm backrop algorithm you know
it's it sounds like it is it's an
adjective onto back propop yeah I think
I think I think it it is it's the
continual backrop is almost like uh uh
more more backrop what is backrop
backrop is you do initializing and then
you degrad in descent we're just going
to do initializing all the time okay as
you go along so it's is very much in the
same Spirit um and and and and um maybe
it's not a new algorithm and what will
happen in the future uh do we envision
algorithms that are very different from
from continual back propop or is that is
that about it and I think I think we
need to go significantly Beyond backrop
and continual backrop uh for for the
best
um to to achieve all of our goals what
we would like from continual learning as
I want from continual learning I not
only want this bare ability to not lose
all your plasticity but I also want to
do things like um
uh meta learn improve the way I
generalize special thing about continual
learning is you you learn some and then
you learn more and then you learn more
so you have this experience with
learning and you can see the way I
learned at this time that worked out as
so so now the way I learn this way it
back a little bit better you can
evaluate how well your learning has G on
this something you can't do if you learn
in one shot you learn in one shot just
done but if you learn and then you learn
more and learn more then you can you can
learn to learn say this way of learning
works better than that way so if that
seems too abstract for you let maybe
concrete
um uh gen how would you like to would
you generalize more on this feature or
generalize more on that
feature we'd like to to learn that we'd
like to learn how to generalize modern
deep BL systems don't really learn how
to generalize they are structured to
generalize in a particular way and maybe
they were lucky and they've generalized
pretty pretty well or maybe that's the
skill of their designers but there
definitely is no algorithm that's
causing them to learn well and I think
we want to have our algorithms learn to
generalize well um and so so that's what
that's part of the ambition for
continual learning it's not going to
come just from tweaking back poop we
have to add more things
and let me just use that we can still
have questions but let me uh say a few
of these things about the new ideas uh
we want methods that we'll learn
continually of course we want we want to
be able to learn nonlinear functions of
course we want them to be very efficient
but the last point is we want them to
meta learn to generalize better and yeah
and this is the slide I want to do
ideally a streaming deep learning or or
uh what you call Dynamic deep learning
should adapt to three levels you should
adapt the weights that's the usual one
you should also adapt the step sizes
every weight should have its own Step At
least every weight should have its own
step size which a step size just it's
sometimes called a learning rate but
it's it's a it's the amount by which you
move each weight and so you want you
want the I I'm claiming we want the
ability to to learn at different rates
learn faster in some weights and slower
in some weights and that this is is how
we will get to sculpt our
generalization is that confusing like if
you have a feature that has uh a large
weight then I mean a large step size
then you will change its weight a lot
when you when that feature is uh present
and that means you'll generalize a lot
along that feature or the other way if
the if that feature has a a zero step
size or very small step size then you
will not generalize based on that does
that sound good does that sound bad do
you don't generalize no in a continual
learning in in an agent like like me or
you there are lots of things that we
know that we don't want to learn we
don't want to change them anymore
because they're just very reliable we've
learned over large parts of our life we
don't want to change those and we don't
want to change those because there are
other things we do want to change we are
we are actually very skilled at
assigning credit to features that are
most likely to be relevant and one way
to do that is by having different step
sizes uh for different weights in
principle I'm thinking in principle if
we could do that we will get we'd get a
whole lot of desirable things including
generalization um okay and then the
third area we'd like to adapt is the
connection between the units this is
sort of a
a a starting view of the goal for a
better A Better Learning algorithm than
regular back poop or continuing back
poop okay there are lots of questions
we we are running out like we're already
above the time so true there's going to
be an extended QA now we can keep on
these questions if you guys want to ask
but I want to fre people who might have
their commitments so quickly um so our
next seminar will be Maxim you knew
better the date was it 18 or when was
that of
November uh so we'll be with um
G
Kath head of Quant technology at
Bloomberg uh from New York um we hope to
see many of you there as well know that
we also have a reading group if you want
to uh dive more into especially in
enforcement learning that's what we're
focus on uh please come to say hi and
also one of sponsors ionic is um having
some opening for stent internships so is
on video games and Ai and enforcement
learning stuff so if you're interested
also come to speak to us and that's
all go well

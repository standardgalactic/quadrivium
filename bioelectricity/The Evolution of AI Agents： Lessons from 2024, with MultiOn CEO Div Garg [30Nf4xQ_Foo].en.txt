I think open has definitely lost a lot
of the lead they used to have the gp4
WHERE like they were kind of the Soul
winner no one could catch up with them
at this point it just see like very
homogeneous Market where everyone is
kind of close when anything is
disruptive I think that it takes a lot
of time for that to catch on and I think
we we are the start of the disruptive
era where like a lot of the online
communication and interaction will they
get disrupted by humans of he's pretty
good at like navigating websites with UI
and so like theoretically like yeah can
also become very very
even
better I would call it explosion of
applications which I don't think has
happened to five about agent I think
it's still I
got hello and welcome back to the
cognitive Revolution today div gur
founder and CEO of mulon returns for his
third appearance on the show a lot has
changed in the AI agent landscape since
I first spoke with di in mid 2023 as you
might remember at that time the AI
Community was a buzz about the potential
for AI agents with projects like baby
AGI giving large language models access
to tools and really only very minimal
guidance and then stepping back to watch
and see what they could accomplish on
their own of course as it turned out
they didn't accomplish all that much
while there were many amazing moments
those were the outliers on average the
step-by-step error rate including on
very mundane microtasks was too high for
agent Frameworks to successfully string
many step sequences together all that
often and we also learned that while
large language models can improve in
many areas through self-critique they
have a tendency to get stuck on
obstacles that humans quickly find ways
around for that reason much of the last
18 months of work on agents has gone
into developing better and more
prescriptive scaffolding with many
companies ultimately delivering
platforms for what I call intelligent
workflows that is workflows that a human
has designed and where the AI is needed
to do some important subtask which
requires intelligence but where the AI
is not given freedom to choose its own
adventure as of Jan this year div and
the multi-on team were still among the
most bullish on open-ended agents and as
you'll hear in this conversation they
have continued at least partially to
Buck that Trend they have built some new
scaffolding and they have developed
interesting techniques for domain
specific fine-tuning but their agent
continues to take arbitrary natural
language requests and gambl does its
best to fulfill them the progress I
found in my testing is pretty obvious
and in some context the company claims
human level performance but still the
system as a whole is not a viable
substitute for a human assist
assistant with that in mind I was
excited to Pepper div with questions
about what he's learned from all of this
activity and so in this conversation we
unpack the latest and agent development
including the company's data collection
strategy the seemingly missing market
for human computer use data and the role
of synthetic data in bridging that Gap
the company's model strategy including
what models they've chosen as base what
fine-tuning techniques they're using and
how their computer vision approaches
have evolved over time why benchmarks so
often show human level performance while
the real world results are clearly not
as strong the future of agent
authentication as well as which parts of
the internet at large will compete to
serve agents versus which parts will try
to exclude them and finally what sorts
of customers multiion is looking to
partner with now as well as how they're
thinking about competing with
hyperscalers in light of claude's new
computer use
capability overall it's clear to me that
while it's taken longer than I had
expected reliable agents that can
perform a very large percentage of
routine computer use tasks are coming
it's only a matter of time and as you'll
hear div agrees with recent suggestions
from both open Ai and anthropic that
2025 will be the year that of course
makes div a very busy man and so I very
much appreciated his time and how open
he was willing to be about the path that
Mulan has taken and the lessons they've
learned along the way as always if
you're finding value in the show we'd
appreciate it if you'd share it online
write a review on your podcast app or
leave a comment on YouTube of course we
welcome your feedback and suggestions
via our it cognitive revolution. or by
dming me on your favorite social network
now here's my conversation with div gur
of mulon catching up on the last year in
AI agents div gur founder and CEO of
mulon welcome back to the cognitive
Revolution yeah thank you Nathan excited
to be back so agents agents everywhere
the last 18 months has been quite the
Saga when it comes to AI agents their
promise and whether or not they've
fulfilled that promise and obviously
you've been right in the thick of it as
the founder of this company how would
you tell the story of Agents over the
last 18 months since like the launch of
gp4 and where do you think we are now in
the grand Saga of AI
agents yeah I think it's been
interesting to see I was still sa very
early I would call like it's similar to
the internet explosion so we are kind of
seeing like the first tra of the
internet in a sense where it's still
like glitchy it's slow most people have
not used it but the next thing that's
going to happen is like this will become
like more mainstream like the quality
and reliability will go really up and
over time this will become like gr to a
lot of like opportunities where a lot of
like the things that happen and in the
daily life will become more agent Tak
over time so I don't think like agents
is still very early I think compar to4
definitely I think at that time it was
kind of a promise this kind of things
are to happen you'll have this kind of
aing capabilities now I think we're
starting to see the early version of out
where like maybe people have found some
like use cases like specific vertical
some business use cases I think it still
is very early most people have not
played with the H&amp;D product so I think
it's still like have explored fully and
I think that will take more time tell me
about your agent lifestyle today in the
past you've posted some viral demos on
Twitter of like having an AI agent order
your lunch or do these kind of various
transactions for you I think you had one
of the first demos of it booking a
flight how is your day-to-day a
experience of AI agents like what are
they doing for you right
now yeah nothing that's been purposive I
think like I'm still using agents a lot
for like shopping has been of the
biggest kits like instacart groceries
definitely K calendar invites and then
maybe a bunch of stuff on like LinkedIn
or Twitter or a bunch of this kind of
places where like you want to have B
that aut to make things for me
personally I definitely lot of mying is
not based on agents A lot of my
groceries and some of my onine shopping
and the I sell to the
son so let's talk about kind of a few
different approaches to building agents
it seems like in the
beginning the first wave of things that
were launched often it's just like open
source check this out and mess around
with the kind of projects I'm thinking
here of like baby AGI and that sort of
early wave we're very much just like hey
gp4 looks really smart let's let it
figure everything out and kind of give
it very open environment very sort of
open
instructions and kind of hope for the
best it seemed like by and large with
notable and kind of exciting demos as
the exception rather than the rule those
approaches led to disappointment when
people found like oh well it doesn't
really work most of the time it it's you
know stuck in various ways and whatever
and so the counter narrative I would say
has been or the counter you know the
sort of compens in approach maybe is a
better term for it has been to put the
agents on Rails to give them much less
sort of autonomous decision-making
Authority and and instead build out like
very prescribed workflows where the step
by step is kind of designed by the human
workflow designer and the AI is kind of
there for the key bits that require
intelligence but is not like left to
choose its own
adventure uh you shared a you know
preview version of the latest multi-on
agent and I was messing around with that
it seems like you guys are still more in
the open-ended frame of development but
I'm sure you know you've tried a bunch
of stuff and have have perspectives on
those two different approaches so tell
me everything about the open-ended
versus the on Rails
approach yeah I think that's a good
question I toally like I think on Rails
is the right way to start from
especially if you know exactly what you
case you want to Target if it do
anything like Enterprise or B2B s
whatever I I think like this kind of
like more linear constraint workflows I
think is the right Paradigm to start
with and I think over time I think you
can have more open-ended use cases which
allow for more general purpose
experiences I think for us we trying to
take a road in the middle but we don't
want to be too constrain because I think
we're still targeting a lot of Everyday
Use and what happens we have surve like
I don't know like at this point 50,000
plus people from like all the user data
we have and I I think the consumer be is
very different for every point so
depending on demographics and AG Scopes
and like where you live bunch of things
I think consumer changes a lot so you
can have very concentrated linear
workflows then you have to build like
millions of these and like that's not
possible and you also don't want to be
fully open-handed right because like if
you have fully something like the
computer us AP that came out from Tropic
it's it's very general purpose but like
it's like it's very hard to find a
utility for that and so I think we're
trying to take a road in the Middle
where like how can you build more
constrain workflows for like task that
you can Define
and and then we are building a lot of
like verifiers and like one thing the
direction was the agent Q research paper
where like the agent can learn to
improve and like improve Its Behavior on
new website it has never seen and so so
we're starting with like more domain
specific models and the models like know
how to like do like vertical workflows
or task how this models can improve and
like generalize over time but we don't
want to start with models that are F
General because that seems to be like a
really bort and also doesn't seem
something that will I think for a setup
like us will pay off in the next
how does that look behind the scenes I
mean in my experience of the product it
does still present as a very open-ended
thing when talk about kind of having
these linear or closer to linear on
Rails workflows in the background the
Voyager project comes to mind from
Nvidia I'm sure you're very familiar
with that where they would have the
agent in a like a virtual video game
environment I think it was Minecraft go
out and like figure out new skills and
then sort of cash the those skills for
future use so it didn't have to
constantly reinvent the wheel obviously
that's like a pretty big open-ended
environment not nearly as big and as
wide ranging as doing stuff broadly on
the internet but what have you found to
be the right architecture
for or the right balance point between
like what is prescribed versus what is
left to the agent to decide to run time
yeah that's a good question I think what
parad we have landed on is kind of like
user choices so it's like you don't want
the agent to be fully autonomous I think
example I like to use a lot is kind of
like fat booking so you don't want to
have the agent go and book you like a
random flight and waste thousands of
dollars uh without like sort of like
checking in with you like do you
actually are you fine with this time so
you want to fly in the evening do you
want nonstop or like this Airline stuff
like that so preferences matter a lot
and and I think like the paradig we have
landed on is like choices where like you
might give some sort of potentially
ambiguous like user query to the agent
like go book me a flight this weekend
for a trip to Paris and then and then
you want the agent to kind of fill in
the blanks where like okay like that
requires like a lot of choices that have
implicit implicitly being made and and
then like we are building a lot of this
work us where like how do we C to use
the preferences how do we make like
involve the human in the loop and I I
think that becom start that starts
becoming complicated where like you are
not relying on just the actions but also
maybe like how to personalize how to
maybe like really get to know like what
to do and and then based on that like
like a really s product
experience one of the experiences that I
recall from an earlier
version of the product that I don't know
if it's still there maybe I missed it
this time around in my testing was the
opportunity to essentially correct the
AI if it made a mistake you know to sort
of demonstrate or like teach I think was
the word that you used in the past the
AI how to do a a particular skill you I
think probably most people would find it
reasonably intuitive to imagine how that
was supposed to work right you would
have the agent go do stuff when it makes
mistakes the human could teach it what
to do then of course it would learn what
to do and you'd maybe fold that into
your training data later I wonder if
like that has played out as you've
thought it would or are you now doing
that in a more implicit way or do you
just find that like that human sort of
teaching Paradigm is just not so useful
as anticipated and if that's not working
like what is the alternative in terms of
how do you Source data to you know to
teach the models what to do I I we had a
good success with that I think we
collected like some millions of
tractores using that method the problem
with any sort of crowd sourcing is like
you can't really trust the quality of
the data and I think like Tesla is a
good example for that I think Tesla have
been has been trying this for the ding
Recs where the Crowes a lot of data but
like filtering like the quality of the
data is like actually good data versus
bad data and that has been like a thing
where like you build a lot of filtering
pipelines and build a lot of like AIS
logic around that so think we have been
doing a lot of that we've seen some pay
off but I think it's still like I think
is like very noisy so that makes it like
a hard problem the second way is like
basically just working with like high
quality annotation data where like uh
like I won't say like too much here but
like like if you have annotators or like
people who can like sort of like help
people exper data and I think that seems
to be like what more front of pushing
towards where like you can go and like
train this agents on a lot of the
general purpose use cases and then like
tr get models that are working
I hear you on things being noisy I also
Imagine of course the Counterpoint would
be like human annotators are expensive
and limited in Supply one good question
I have is why is nobody propositioning
me to pay to watch me use my computer
all the time like I feel like there
should be a market now
in install a general Observer that just
kind of watches a person use a computer
maybe you could have a couple different
modes of it including one where you like
talk through what you're doing and
explain yourself U because obviously
most of the time the Chain of Thought is
is not actually said out loud and and
recorded you know you'd have to clean up
that data of course people are doing
random stuff you'd also have to
anonymize that data I would want to know
who you are and I'd want to make sure I
trust you before I let you install that
on my computer but I feel like with the
the cost of annotation as people get
into like higher and higher end
knowledge workers or even like
scientific experts in some cases for the
sanitation stuff it feels like that's
pretty valuable and I feel like I should
be getting like a ,000 dollar a month at
least for somebody to watch me use my
computer so why isn't that happening or
if it is you know point me to where I
sign
up yeah I totally agree I think the
market is there now I think it wasn't
there may like even a month or two ago I
think it's a new market uh I just I
think it just depends like who's willing
to purchase the data because I think in
the end the day getting this kind of
data is not that expensive because I
think there's enough people who are
willing to donate this data for V
especially if you can like use other
countries and like whatever like
Outsource a lot of this kind of data
collection so I do think a lot of this
data can collect very cheap it does
become like if a company is really
interested in your personal data and
then they might be a to offer like more
like what $1,000 a month just should be
able to like get access to be very like
more like personal okay like what makes
Nan what does it that look like when
he's using his computer yeah I mean it's
an interesting Paradigm it kind of
reminds me of we did two episodes that
were separated by some number of months
on mind ey and mind ey 2 and in the
first case and this was like
reconstructing what somebody was looking
at like what image they were looking at
by the fmri data scan from a scan of
their brain at the time that they were
looking at this image in the first
version it was patient specific modeling
that was being done and you had to have
something like I don't know 20 hours or
whatever of of these scans with a person
looking at a new image every few seconds
to then finally get to the point where
you could train a model on that person
and then in the second edition they had
figured out how to kind of let's say map
all of these different users Each of
which have like a different anatomy of
their brain um literally like different
brain sizes there's a lot of course
difference between people map that all
into essentially a shared space train
one model on all those data points and
then from that base model it would only
take like an hour of one individual's
data to kind of fine-tune it to their
particular anatomy and and activation
patterns so I can see that here too
where you might say you know there's
vast bulk data to be collected out there
and then just on the margin it's really
about fine-tuning to each individual
person so I guess what you're saying is
like that market is international and
it's just a it's a lower price point
because I guess people feel like they
want to separate out like scientific
expertise from just like General routine
computer use and it's better to buy
those separately at different market
prices yeah because if you think what
the gender computer use it's not bit
expensive it's also like I think like
very common PS so so I think you can
just Outsource a lot of that get that
very cheap
I do think there's value of personal dat
which is like like if you're willing to
like share all your personal data and
like exactly how you're actually doing
things and you don't care about privacy
then I think people are need to figure
out where like like if they can get
access to lot of this private data and
then they can like maybe use that fully
and then maybe like I think like that is
a market that doesn't exist but that
could be like very well Market where
people are willing to pay a lot of money
if they can get access to this private
data and then you're willing to like
sort of like live with that kind of like
loss of privacy in a sense hey we'll
continue our interview in a moment after
a word from our
sponsors even if you think it's a bit
overhyped AI is suddenly everywhere from
self-driving cars to molecular medicine
to business efficiency if it's not in
your industry yet it's coming and fast
but AI needs a lot of speed and
computing power so how do you compete
without costs spiraling out of control
time to upgrade to the next generation
of the cloud Oracle Cloud infrastructure
or
oci oci is a a blazing fast and secure
platform for your infrastructure
database application development plus
all your AI and machine learning
workloads oci costs 50% less for compute
and 80% less for networking so you're
saving a pile of money thousands of
businesses have already upgraded to oci
including MGM Resorts specialized bikes
and fireworks AI right now Oracle is
offering to cut your current cloud bill
in half if you move to oci for new US
customers with minimum Financial
commitment offer ends 12314 so see if
your company qualifies for this special
offer at oracle.com
cognitive that's oracle.com
cognitive there are so many things in
life we just never get around to taking
up that hobby cleaning out the garage
you know little things that don't really
make huge differences in our lives yet
there's one thing that most of us have
probably been neglecting that can have a
huge impact on our family's future it's
life insurance and with Select Quote
getting covered with the right policy
for you is easier and more affordable
than you might think as someone who
tracks AI progress on a full-time basis
and obsesses about its potential impact
non-stop I know how tempting it can be
to ignore more mundane familiar risks
there's always another paper to read
podcast to listen to or product to try
and yet the smartest people that I know
in the AI space continue to save and
invest money for the future carve out
time for their relationships maintain
their physical and mental health and yes
protect their family with life insurance
just in case anything should happen
before the singularity if nothing else
it's one less thing to worry about in a
time of unprecedented change so get the
right life insurance for you for Less at
selectquote.com
cognitive go to selectquote.com
cognitive today to get started that's
selectquote.com
cognitive do you know what those data I
assume you're not doing this directly
yourselves to the degree that you are
sourcing this kind of data I assume
you're working with Partners to do it
are there like marketplaces that exist
or you know go to companies and are
there I'm also curious as to what degree
people are sort of logging in and doing
like discrete tasks like here's your
task go do it on the browser dick dick
dick versus just kind of open-ended
observation that would be more of like
an imitation learning Paradigm what how
would you describe the you don't have to
give us any secret Services unless you
want to but let's say I wanted to buy
this data what what's out there you know
that I can buy and who do I buy from and
how much should I expect to
pay there's a lot of like data companies
a lot of them we are working with again
I can't like I we working with just for
like more confidential purposes but I
would definitely say like we've tried a
lot of things like even if you use Mt or
whatever like um online Solutions I
think you can like create task like I
want someone who's doing this particular
whatever like Compu work so
collect a lot of data and then get a lot
of like data that you can on I'm pretty
sure that's what enthropy is doing which
that's are doing we have definitely done
that but then we're also like thinking
about a lot of like smart tricks where
like how do we find data in like the
regimes we care about so how can we inti
people to give us like the data that we
don't have and then then make sure that
we can like keep improving figure out
where is the agent most deficient on and
get the right quality of data and keep
them making better and better are we at
the point of selfplay another thing that
I sort of expect we'll have to tip soon
is just object feedback from reality
right I mean we've obviously seen that
work in all these gam playing
environments it has worked but hasn't
quite maybe hit the critical Tipping
Point yet for code generation but I
would assume that there's some analogous
version of feedback from reality that
you could do in a open-ended computer
use context too right not I think like
the that's missing the most right now is
just having a really great quality
report that you can use from the
environment so when you're in a game
based setting like Minecraft or
something you good reward or like even
like when you're playing chess like a
zero you have like you know exactly like
like what are the rewards for any moves
you make in the game s and then it's
easy to optimize a policy and make it
like like yeah this is what we want to
do to win what happens though is like if
you are in like a more of a real world
scenario it's there's no envirment
reward and then you basically it that
makes it challenging because you don't
know what is the that you're optimizing
and then you maybe need to chain another
like model that is kind of like giving
like some sort of proxy reward that you
can use to like sort of like optimize
the model and make it better and better
I think cording it I think it's easier
because you can create like Unit t so
you can have some sort of like static
checking like so you can get atic score
on the quality of the code and like does
this score actually function well and
then you can do some sort of self play
okay like based on this kind of like
score can we optimize the score to
actually become better and better and
optimize it I I don't think it's
possible like there will be like some
like the thing with this kind of
algorithms is like there's always wish
to cheat in a sense so whatever metric
or score you come with like the
algorithm might find a way to cheat like
it might find some sort of like I just
find like you can just create a lot of
like null strings or something and like
that solves the problem and like get you
like infin score or something like that
and people have seen that the
reinforcement learning a lot when you
use that in gameplaying environments so
yeah so I do think it becomes a question
of can you obtain like high quality
rewards on the setting you are in and I
think computer uses like a hard one
there like maybe a human can give you
like the right prox see where if you're
doing things like human feedback much of
things I think that then is possible but
I think it's a harder problem than just
like play on a game Bas M I always
remember you know that visual of I think
it was a deep mind video game player
from like 2017 where the little boat is
circling around and around the same
space and like picking up infinite
points but not actually advancing in the
game in the way that it was meant to so
yeah lots of examples of mode collapse
or other strange Solutions in the
reinforcement learning world so what's
your model strategy today I read through
the agent Q paper that you guys put out
a couple months back at that time you
were reporting results on a llama
370b which you were doing a number of
things to enhance obviously from its
base version you know is that still the
Baseline for what you guys are using in
production or how would you characterize
the
available I guess you know commercial
and noncommercial open source options
that are available to an agent developer
today yeah at this point I think they
got a lot of different models plus
comination like Lama 1B has been I think
a good compromise in terms of speed
versus the model reasoning capabilities
and I think right now I think they
starting to go more toward like like gpg
4 0 style where can you do like INF time
compute and can you optimiz as the model
to do like Chain of Thought do inference
and be like become a better Reasoner uh
so I don't think like that seems to be
the way to solve of these complex
reasoning problems so that's also what
we're going towards where like I think
at this point the base model doesn't
matter that much because I think there's
enough face models most of them are
forming like similarish the recip and
like the data they trained on are also
like more or less the same so so the
trick kind of becomes like okay like
what is your application and how do you
optimize a model on that application and
I think that's what we did with agent Q
where like care about this kind of web
interface that's kind of like a shopping
environment how can you get the right
environment like feedback how can we
train on the feedback how do we make
sure that this base model can like
optimize and become better and better
there and then can you combine that with
some sort of like INF time compute
tricks to make it like a better Reasoner
so I don't think at this point
architectur seems to start becoming
irrelevant where like I think we might
see some sort of like at least at the
current scale it's possible like if you
are able to take 10x the current size
the parameters of the model then
different architecture might like start
shining right now I think like most
architecture seems to like level out at
the similar performance where like
there's not too much diversity in terms
of the models out there whether it's
Clos to source and like how much
difference you can get so if you look at
the leaderboards like everything seems
to be very tied up it's everything's
starting to come very very close no one
has I think open has definitely lost a
lot of the lead they used to have the
gp4 WHERE like they were kind of the S
winner no one could catch up with them
at this point it does seem like very
homogeneous Market where everyone is
kind of close and the the base model
architecture kind of seems to become
like not relevant as long as it can use
it interesting 01 would seem to be
a possible exception to that or no I
mean obviously 01 is like expensive if
you're trying to do mouse by Mouse click
and I guess there's also a question
right now of what is available is A1
preview which doesn't have the vision
aspects enabled yet but does that sort
of does that
summary of everything sort of being on a
roughly even level include A1 or not
include a
one like I'll say OV is interesting I
think I would say it's very good at math
people have found out like it's not very
good outside the mathematic domain but I
think it's very high specifically trade
on so so Al no I think is not too
different from what we have right now
maybe like at least for the tri maybe
like the full and more to this much
better and it can shine and like a lot
of different regs I I do think like this
kind of Paradigm where you inad of like
training them Compu where like like last
year gp4 everything was you just throw
more computer training more parameters
more compute more data and they get a
better model that's more better Reasoner
and think starting to see this like in
throwing Compu during training you thr
Compu during inference and then like the
more chains you do the better you get
and so for it's kind of like if I throw
less compute maybe it's kind of like may
even worse than gp4 but if I throw a lot
more compete maybe it's much much better
and then so it's kind of becomes like
like how much time and Compu can I throw
bring in Fr dep on application but I do
think that becomes a different class of
how to think about these problems uh
especially if you think about this is
kind of like how much latency can you
tolerate at INF frence and like how much
resources are you willing to like give
with the model yeah I'm interested maybe
we'll come back in a little bit to talk
about kind of user experience and how
much you think some of that stuff
matters like I'm always unsure about you
know how much cost matters how much l
latency matters obviously depends on the
user experience and depends what the
agent is trying to accomplish but
let's ccle back to that for a moment
let's stay on the agent Q research that
you did I've got a table here that I I
pulled up from the paper there's
basically two kind of environments right
that are used in the paper one is a
e-commerce Benchmark that was created by
academics and put out there and it's
sort of a self-contained you know Amazon
like environment for shopping and then
the other one is actually going and
having the agent do things on Open Table
um if I understand correctly in both
cases you get to about 95% Success
Through a variety of techniques and I
thought maybe you could just like walk
us through this graph which people can
pull up from the paper but it's a nice
sort of explanation of how you build up
to the success starting from a llama
370b instruct base model which for the
graph is coming in on the open able Tas
at not even 20% success that compares to
GPT 40 which is over 60% but still
leaves like six more bars on the graph
where you're layering on these
additional techniques and showing what
the contribution of each one is so you
want to take us from again under 20% on
llama 370b to a little over 60% gbd4 to
through six you know Alternatives all
the way to the
95% yeah no um yeah I think this this is
what I'll say like it's possible with
vertical specialization models because I
think what happens is like most of these
models are just train on very generate
internet data and then and they're not
like really really super great at one
thing and I think that's very obvious
when you look at like Lama like like
llama like this performance like 20 P
because I think it's probably never been
trained on this kind of like tasks
before but then you start looking gb4 I
think gb4 maybe has more browsing kind
of like cilities or I think opening
browsing to so they maybe have more data
on this kind of interfaces and that
makes it like a better Reasoner and
better like better accuracy on this kind
of like action TS but again I think what
happens just because this is very gen
generic there's no like specific F
tuning towards a particular vertical I
think we see a lot of loss of accuracy
and I think that's one thing we Prov
with Q where we like like now suppose
like you have a l model that performs
20% but then based on like a lot of the
data that we collect EnV actually boost
the performance to make it like
essentially go out the environment
almost all the time and and and then we
try this experiments where like let's
use a lot of like techniques like DPO
that we doing reinforcement Lear using
like human feedback let's combine that
with maybe like stuff like mon research
where can we search the space of the
websites so we can figure out we can
explore the websites figure out like
like if we go down this route or this
link will it work or not and if you this
didn't work this one work and then keep
doing that until we can kind of like
keep finding make it better and better
and and then we over time we have like a
very specific like vertical agent
capability where like like now this
agent just knows like Cate this website
and and the great thing is like this
thing just took us one day it very fast
and it's it's kind of like a
self-improvement Cy so that you can just
keep doing it more and more and so
there's some limitation on like like how
much can you improve the performance as
long as you have a good quality feedback
so as long as your feedback signal is
very very high quality I think you can
you can just like keep making that
matter um and I think that was like a
very interesting learning like we were
actually also very surprised like that's
almost almost like a 4.5x Improvement
close to that you were like like yeah
that's kind of crazy like just in one
day we able to like boost a model that's
for 20 person to like get us all the way
to to where we were able to push it with
techniqu we used and I think that's kind
of like the set TR here where how do you
explore this moments and then how do you
have this capability where you like self
thrown and
nice hey we'll continue our interview in
a moment after a word from our
sponsors as a developer the journey from
concept to production ready large
language model apps is fraught with
challenges dealing with unpredictable
language model outputs hallucinations
and ballooning API costs can all be
blockers to shipping your next AI
powered feature that's where Advanced
rag comes in with the new rag Plus+
course from weights and biases you can
overcome these hurdles and build
reliable production r rag applications
go beyond proof of concept and learn how
to evaluate systematically use hybrid
search correctly and give your rag
system access to Tool calling based on
21 months of running a customer support
bot in production industry experts at
weights and biases coher and we8 show
you how to get to a deployment grade rag
application this offer includes free
credits from coher to get you started
make real progress on your large
language model development and visit wb.
/cr to get started with their rag Plus+
course today that's wb. mecr to get
started with their rag Plus+ course
today so let's go one by one I I think
it is worth just breaking it down the
first one on the chart is llama 370b
instruct
rft is that a reinforcement learning
fine tuning or a different kind of
usually it's in just simple like
instruction fine tuning is like the
first layer of post trainining right I
it was included to me what the RF stood
for in that case yeah RF it's like a
older algorithm I think it's I I think
it's a reinforcement F Jing I think it's
a research paper like one of the
algorithms that you can read I think
that came out last year so we used it as
a Baseline and then we compared to the
bunch of other the matters and there's a
couple of work from like Salesforce
other groups and then the interesting
thing was like no one kind of like
thought about like the directions that
we thought about where like how can we
do more efficient search and like com
that a lot of this kind of like learning
and then like how do you set upes so
that's reinforcement learning DPO is
next I know you're right in the heart of
DPO country there in Palo Alto as maybe
as side can you help me develop my
intuition for the DPO
algorithm I feel like I'm on a quest you
know I can look at the equation and that
doesn't
jump out you know is like super
intuitive to me how would you describe
in a qualitative or intuitive sense what
DPO is doing and like how a set of
preferences on different Generations is
ultimately being translated back into
like updates to the weights of the model
yeah now I say DPO is is it's a very
intuitive algorithm at the end of the
day so when you do like supervis fun
tuning what you usually do is like you
have a bunch of expert data this is like
kind of opal data and then training the
model to be like like here's my ground
proth optimal data and then you want to
like sort of like make the Model predi
Behavior that's similar to it and then
you're doing this learning where you're
doing like radium descent to like go
more closer to imitate like what the
ground of data looks like so the models
predictions are similar to The I think
dpu I think the thing they do is call it
like they also use the negative data in
a sense so they do like this kind of
contrastive learning where they're like
like we have some positive feedback data
we have some negative feedback data then
we want to do gradient descent towards
the positive data so we want the model
to like go closer to the Behavior Uh in
terms of it predictions towards the the
positive outputs but we want to do like
gradient Ascent on the negative data so
we want it to go away from the negative
like Behavior I think that actually
works better because like if you're just
kind of like you think about like maybe
like the positive behavior is kind of a
circle and the model is kind of like
trying to get close to it it's possible
like the model might be very widespread
so it's like it it can cover the circle
but might also be very widespread
outside but now when you're doing DP
you're saying like here's a positive
stuff there's some negative stuff
outside and you're taking the model the
model has to kind of become close the
circle so it will basically have all
preds will be here so it can be
widespread and and so I think that's a
one good way to thing about it like
you're kind of like giving it more what
to do and what not to do and the what
not to do I think that kind of like
thing becomes very useful especially
when you're doing this kind of
enforcement learning where you like like
this plus one this minus and then so I
think at the end of the day it's a very
intuitive algorithm and I think just a
beautifully formulated using like a lot
of reinforcement learning like like
algorithm literature and like principles
and and then overall things it's like
kind of like gradient descent towards
the positive samps gradient descent away
from the negative samples and then you
kind of like counterbalance that by
summing the sum of like all the samples
and dividing that normalize the factor I
think that's basically take you
up I know in like typical instruction
tuning much like the pre-training it's
literally just choken by token
evaluation right of the output and then
for po I know there's like a reward
model that is responsible for sort of
scoring all the tokens and that reward
model ultimately gives a signal of like
this token was really good this token
was not good whatever with the DPO I
understand that there is no reward
model and yet if I understand correctly
it's like it's not just training the
model to predict like exactly what the
tokens were right because with the
negative examples like you can't just
say don't do that token you have to say
like okay well so what right so can you
give a little more intuition for how if
I have whole Generations that are scored
positively and
negatively how is the model
understanding or how is the algorithm
translating a score that is like not
token by token into something that can
be ultimately applied token wise to
update weights through back
propagation so so I would say first is
the initial de that was formulated I
think that's top mes so the first the
original from Rafael Rafel so I do think
that that one you're kind of giving a
positive negative feedback for each
generation on the model and and so it's
it basically becomes like superv fine
tuning so you're saying like here's this
generation this gets positive score
here's this generation this gets
negative score and then you can kind of
like just Direct
put that in make a simple loss equation
and train on that so you don't act
should think about Tres in theal DP the
one modification we did in our agent Q
paper was kind of like come with a
trajectory level DP so when you're like
working on environment you're taking
multiple steps so you are not like just
start putting like single to and like
finishing that if like keep taking L
steps until you reach your Nate and then
and then you can kind of generate a lot
of trajectories and like once you gener
a lot of this trory some tror have FAL
score negative score and that's more
close to what P does usually in the
reinforcement and then for this to work
then you have to like think about like
like how can we apply this TP algorithm
on a more like a trajectory level like
all the scores are for trajectories not
for individual stops and I think then we
propose this like trajectory level DPO
that you can find in the paper where
like like how can this work and like how
can we balance if you like differents
and I don't think so there like a yeah
so the I think I call it like very
simple where like it's working on like
per token or first stop RS so you don't
have to actually P TRS so in the
original version you're saying basically
my positive score or my negative score
is just applied to all tokens equally
and then that signal is propagated and
that's it can you give a little more
inition for how the Next Generation
works you know this is maybe more
tedious than some we're interested in
but I I really do want to continue to
develop my intuition
for what exact in as as precisely as I
can understand it what is the signal
that we are actually sending into the
model because I I feel like that's
really helpful for at least having some
foundation on which to make like guesses
or on which to interpret the downstream
behavior that comes from that yeah no
that makes sense
also at the end of the day the signal is
kind of like the positive or negative SC
but gener like suppose like you're
applying DP to last langage Model and
then you say like like maybe like I want
you to maybe like say like who's the
current president of United States or
something and I suppose like the model
knows the right answer then you can give
it like like this was generation so you
give a plus one score but if it comes
with the wrong answer then you like like
this is not correct and then you can
give it a negative one and then you you
can do this multiple times for like you
can ask you can generate 10 generations
and then you can have different voters
different people who are like giving the
score can be the one person is doing the
score and once you have enough of this
like this for all the positive
Generations here's all the negative
Generations and you put that in dpu and
you're like like now the goal is to
improve the sort of like optimize the
probability of outputting the positive
generations and minimize the probability
of outputting the negative Generations
okay I still want to keep studying this
a little bit more uh but maybe that I'll
leave the rest for another day returning
to the techniques in the paper can we
first just talk about like what is the
definition of agent Q like the just to
be be sure I'm clear on that and then I
see you know jumps off the chart that
enabling the Monte car research drives a
big boost in performance but maybe you
could sort of talk about what are the in
addition to that like what are the
biggest drivers of improved performance
but first just give us like what is the
bundle that constitutes agent
Q yeah like I was say agent Q again is a
simple idea I think it's kind of like it
borrows from I think Richard San's B
lesson where like the only thing that
seems to work is like search and
learning and I think like we were kind
of very inspired by that like even if
you look at like no Brown work at like
the work he did on diplomacy at Mata and
then like the stuff is on Curr opening a
lot of that is like like like how do you
combine search and like how do you make
that like use that learning process to
make it more intelligent like algorithms
and so we we did a similar thing where
like like like search just seems to be
very unex like no one has really thought
about like how do you do search for
agents where like that seems to be like
an obvious thing and we were like like
let's get we now like com lot of this
like Monte research where like it's easy
for the agent to go explore the state of
different environments and then once we
can explore this environments can we get
a lot of like this kind of like positive
negative reward like for the positive
Generations positive like trajectories
which actually reach the goal for the N
tror which which fail to reach the goal
and once we have that then can we put
that in a learning process and and then
like keep optimizing so like like
maximize the probability of positive
Tres that EXT the C minimize the uh
probability of trajectories that don't
use the code and how the MCTS helps is
it's like we could be kind of giving
like like explore as much as possible so
we're kind of like adding a lot of
entropy where like it's possible if
you're not exploring the environment
maybe just like a very narrow path that
the model has learned and that just
things like this is just the one thing
that you can do on the environment space
and so just doesn't know how the
environment but we try to add a lot of
entropy to the initial exploration
process so like the model is trying to
go and like take as many routes as
possible but then over time it's finding
out okay like this routch were like that
end okay this I reach the goal so let's
a that in the future this for the
positive ones let's keep doing that and
that's where we like can train a model
that has kind of seen the whole
environment SPS and has learned to
exactly like how to Res go and and and
then we keep doing that and again and
again until we can make this model like
self optimize and become better and
better to maximize the probity of
sucess so by apping all these things and
I I was I didn't realize until this
conversation that this was a relatively
quick
project with like not a ton of compute
presumably put into it although maybe I
mean you can you could spend a lot of
compute in a day but I assume you didn't
spend that much compute in just a you
know a quick Sprint of a project like
this you gradually climb all the way up
you get to 95 is% on the one Benchmark
that is better than human
performance and this sort of gets
philosophical pretty quick but we see
this all over the place right where it's
like oh it's the AI beat humans at this
Benchmark and that Benchmark and all
these benchmarks and I do take that very
seriously as a signal of how far we've
come and yet at the same time if you
just looked at all the benchmarks you
would think like we definitely have some
sort of AGI uh running around and yet
somehow we don't quite yet so I guess
first of all how do you think about that
Divergence between the AI performance on
a benchmark versus a human performance
on a benchmark like it's still I assume
you would agree and I would say candidly
in my testing of the agent like it can
go do stuff you know it can accomplish
some tasks but I'll bet on myself in a
Paul bunan first machine style
competition between me and the multi-on
agent still so why do we not why do we
see these benchmarks with the AI winning
when it you know obviously every you
know people design them to try to
represent the real world and I I
actually did go look at the the web STP
thing and like it it looks like a pretty
a little bit Bare Bones like a pretty
normal-ish e-commerce environment are
people noty I don't know I'm out you
know one idea that comes to mind is
maybe people aren't trying that hard in
the benchmarks but U I don't think
that's probably usually the case I don't
know what's your theory of the apparent
Divergence between what the benchmarks
seem to be telling us and what we
actually then see in
practice again call it like difference
between a sh scenario verus a mo
scenario there like a is a more like a
narrow domain like couple of like the
the we environment has like maybe like
five six different like environments and
what we also do is we training on each
different each different side so we can
optimize the model to be very good on
that side when we do more like General
compos agents and that's work in the
real world like until we have a delway
to go and like optimize this agent on
every single upite I think like the
performance one match right because like
a benchmark is just a n domain so to
satate The Benchmark for us to satate
the entire internet that is just a much
more shiny problem I think it's possible
I think about it but I think it's just
much more complex and like reques more
resources I don't think that's kind of
like where this things start ding where
like a benchmark is a like a simp
simplified version of what you will see
in the jail um but if you think about it
as a vertical like like here's this one
nrow domain that I really care about and
then we we take a look at like the
average human accuracy on the domain and
then we take a look at like what can we
do with agent Q matter and then the
surprising result is like like in this
narrow domain we can actually beat like
human and and then becomes like sort of
like very interesting learning okay like
n can beat like a human in a very narrow
domain then okay like over time you can
keep generalizing and journalizing that
you can like start humans and like moiz
domain but the question then becomes
like like how much data do you need how
much feedback do you need how much skill
do you need how much resources you have
to throw and think that's kind of like
an interesting skill problem they like
like I I don't think we' kind of figured
out like the fundamental ways to solve
problem but then you have to apply scale
to make this actually like really
shine are you to the point now I mean of
course this is like the big there's
obviously multiple big stories but I
feel like a good candidate for what's
different about the current ERA of AI
versus previous uh eras of AI is that we
see positive transfer across lots of
things right like it used to be the case
that if you tried to train a model on
two tasks it wouldn't be as good as two
models trained on one task each now you
have with Foundation models this sort of
like foundational ability that seems to
be you know rather quick to generalize
to other things and training on a
million tasks turns out to make the
million and oneth easier to learn do you
have a sense for where you go from
negative to positive transfer as you
scale up an agent like this like
I presumably doesn't happen if you go
from just Open Table to just Open Table
plus Amazon but if you keep if you just
kept adding and I'm sure you've you know
explored this sort of thing if if you
just kept adding different sites is
there a point where you start to see
positive transfer like do do you have a
sense for where the the threshold or the
Tipping Point is there yeah definitely
say especially you have like a lot of
shopping uh domain websites you can have
like Amazon or Target or Best Buy
Walmart whatever
most of the look similar because it's
kind of like a search bar product
catalog and you have some sort of like
DET product you check out and and and
there's L A positive transfer like even
if you have train on one website and
then you are like TR toiz it to like
similar Dom me I think you actually get
to take very good transfer so I think
within each category I think there's a
really good positive transfer and
overall to I think like the internet is
not I think it's made for humans to use
it's not that diverse if you think about
like the different uis and like as M
that you encounter you basically see
like a lot of like drop downs or drawers
or like navigation bars and whatever
like text Fields but everything like
there's a lot of common so the model can
like learn how to work on like a lot of
this interfaces I think there's a lot of
POS transform where can work on new
websites and then like you it and then
that's also like one lotion that you're
very bullish on in with what you're
doing with agents that we keep making
this pents B and but and over time I
think it I do think there's a scaling
lock here but like once you hit enough
scale in terms of like the number of
websites you have traded on then you can
chiz to the remaining uh
websites let's come back to the the the
scale the question of scale and the
scaling law and the possibility of
bitter lessons and For Whom the lessons
will be better again in a minute as well
just staying on your model strategy for
a minute longer curious what you've
learned about fine-tuning I feel like
there are you know obviously lots of
different strategies for fine-tuning
Laura seems to have kind of become the
default thing because it's efficient I
was a big fan or at least I was like
very excited to read a paper called Mora
from not too long ago which was an
alternative that was like similar number
of parameters but higher rank and the
math on that escapes me a little bit but
conceptually they reported sort of a
denser fine-tuning denser use more
intensive use of the available
parameters and they did report that it
was better at learning facts which I
thought was really interesting so
interested if you've experimented with
that or if you're just biting the bullet
and doing like full weight
training yeah I don't think we see like
a lot of improv Improvement just with
Laura I think Laura is like very
efficient it's a very good way to train
this models and F them on on your
application I I think there's also like
a lot of variant for L I think there's
like PA Laur and like there's a bunch of
like this kind of like variations and
like a lot of this work really Evol I
don't think lur works well there's
definitely a lot of this new innovation
that's happening where people are coming
from more Alternatives so I think that's
interesting space to watch for um I
think for training can help but again is
about how much data you have so if you
have like crazy amount of tokens you
have like billions of tokens and
definitely should be like fully fun
training but if you have like 100,000
tokens or in that man I think then low
is actually like a better method because
you have less interest to optimize and
so if you have less tokens less data
then I think it's actually a better use
um
of the data and then you actually get a
better performance and but if you have
like billions of tokens then I think
like you should be optimizing more
parameters yeah I think a good way to
also think about this this change loss
like what's the optimal data for optimal
number like the size of a model and so
if you have like this many parameter
model this is amount of data you should
be throwing at this model and I think
that's a good way to think about like
low version of I guess I would maybe
expect you would have billions of tokens
it sounds like you're not sounds like
you're not using huge data sets
obviously quality is a super important
Dimension should I infer from this that
you have been just trying to optimize
the quality of the data set that you're
working with over quantity and it's just
not that big that the the approach still
is enough I'll we a couple of things one
thing we definitely care about is feet
and then if you're training over like
billions of tokens that is a very long
process and I think so so that have some
have been running but far we get about
like Quicks and so what can we do with
like a less amount of data and how can
we make more sufficient use of models
that we can deploy in production and
then make off for our customers so so in
in that scenario we have found like look
at AQ like we did a couple of days of
training and we were able to get very
good performance on this like n domains
which actually is a better thing if want
to build a product if we like doing like
pure research then I think we should be
just spending like six months
train like hundreds of of tokens and
like trying to train like the best
General level aging possible but yeah
but the learning so far for us sorry
also say for the spaces it's like it's
much easier to build an narrow and it's
also very easy to much much easier to
prze it unless you want to be like a
foundation the model
company yeah okay gotcha what is your
vision
strategy right now is that all I mean of
course and by Vision I mean like
interpreting what the agent encounters
on the computer screen in the early days
we were seeing lots of examples where
people were trying to parse the Dom of
the website and figure out how to like
strip all the crap out of the HTML so
that it would fit into the context
window and now of course we've got much
more in the way of multimodality that we
can take advantage of I'm you know in my
testing I did find a couple of weird
places where the agent sort of got stuck
and like was saying that it couldn't
find something that was like just
plainly on the screen it wasn't clear to
me if that was like just you know I
don't know I don't know what was
happening there like was the model not
able to see well or was there some sort
of weird situation like an ey frame that
would could possibly be causing some
visibility issues so I guess two parts
of that question like what's your
overall strategy for interpreting the
visuals and maybe what are some examples
of just weird challenges that pose
problems you know relative to kind of
the happy path that people would think
of first yeah so so I would say at this
point like I think we're doing a lot of
hybrid P ples but like there's a lot of
like useful Str from the top where
there's a lot of like meta tags and AR
labels which is kind of like optimized
for BS um and then you have a lot of
like the visual uh data which has a lot
of layouts and STS and and we doing a
combination of both so we try to be
hybrid where like can we accept the past
of both FES and then can we use to chain
the models and do things
um One Challenge we always I think this
is something that throws off a lot of
people is because if you're a human you
kind of expect this things to fully work
like a human right but what's happening
is like the representation of the screen
of the UI that the agent is saying is
different from what you are seeing and
then and that makes like the behavior of
the AG could be like a bit weird maybe
it's able to click on invisible element
it's able to make like it doesn't need
to scroll down can just like kind of
like see the whole like if there's a
website with 10 pages maybe just is able
to see like all the 10 pages without
needing to scroll down like a
and I think that kind of like throws off
like people from our experiments where
you have to artificially figure out how
to constrain it to be more like a human
and and then there's also this things
where maybe like you might have some
sort of elements which are tricky to
detect or identify which maybe like are
very obvious for a human but like like
maybe the agent is not able to see that
maybe it's some sort of like complex
JavaScript issue or some iame issue I
think like right now we don't see a lot
of that I don't think like in our new
prototypes I think we've just like
trying a lot of different things so
they're not they might have some
deficiencies there but I do think like a
lot of the major products we have I
think like we have been able to solve a
lot of these things over the last couple
of months where like now the our
pipeline for processing like the
information on websites I think it's
it's
very interesting I I realized too
there's probably somewhat of an
adversarial situation happening because
what I was trying when I observed that
was and just to to give you a little bit
of a sense for the feel so you added me
to a test flight where I could go you
know download the in development
multi-on app and then basically open the
app tell the agent what you wanted to do
it then has kind of a dual interface
where it's like telling you what it's
doing and allowing you to you know pause
it or give it additional instructions
feedback whatever as it goes and then if
you minimize that you're just watching
the screen and it can also talk out loud
to you so you can basically hear it
narrate its progress and you're watching
the you know the state of the browser
evolve as it goes on and and does its
thing well I'm in Detroit and it's the
day before Thanksgiving as we're
recording so we've got the Lions playing
their annual Thanksgiving Day game
tomorrow and Michigan Ohio state is also
this Saturday so I just asked it to go
get me tickets for each of these games I
tried both and it was it was successful
in terms of like understanding my query
it was successful in terms of doing like
an initial search it actually did a
couple different approaches you
different trials it in some cases like
went to Google and searched in other
cases it went like direct to a ticket
site from just prior knowledge it was
able to search successfully on the
ticket sites and then there were a
couple of instances where it was like
those tickets were now on the screen but
it would say I'm having a hard time
finding the tickets and
especially if you're saying that's rare
overall I suspect that possibly part of
what's going on is you know obviously
these ticket companies have bot Wars
going on for for many years now at this
point where they're trying to prevent
people from buying up and reselling and
whatever I don't no expert in the tick
mark ticket Market but I know it's
complicated I guess all that is to
say what do you think is the how do you
describe the dynamic right now maybe I
didn't I wasn't thinking about this at
the time but as I am thinking about it
now I'm
thinking you could imagine this cutting
multiple different ways right you might
imagine new off Frameworks for agents
coming online and by the way I also
another test I didn't have to log in for
these ticket sites but another test
where I did need to log in to create an
account I tried uh shopping for
Thanksgiving dinner on instacart as well
to do that I had to you know at some
point I got to a you must create an
account screen and I just kind of paused
and like put in my email and then you
know when it sent me the confirmation I
had to go get that out of my email and
provide that to the agent so it could
provide it it was able to fill that to
to do those steps with me putting that
stuff into the chat but you could
imagine like different parts of the
world evolving very differently where
instacart might say we want to be an
aggressive early adopter of whatever
sort of off Paradigm is coming for
agents because we obviously want people
to shop however they want to shop and if
they want to have an agent shop for them
more power to them whereas A Ticket
Master or a StubHub might say we want to
ban all agents effectively because in
some way or other they're eating into
our margins and so whatever
countermeasures they might put up um
that's a long prompt but I'd love to
hear your thoughts on if you see any
good off Frameworks starting to emerge
if you are seeing like countermeasures
in the wild that you feel are kind of
anticipating an AI agent wave and and
trying to resist proactively uh and I
guess just kind of generally you know
where do you see that sort of thing
shaking
out so let's see first one definitely
authentication I think it's getting more
mature from I think there's a lot of
like
authentication providers I think like uh
um andon is one I think that's been
trying to build not of agent identity
for browsing kind of stuff I think there
was a new one that came out agent out or
something for apis and like plugins
integration so I think that's get sure
been thinking about a lot and I I don't
think that become ma enough that
everyone can go and like work with like
some sort of like authentication
provider
and you're good to go but the second
question becomes like like are the
current services and websites going to
be adversarial or cooperator I think
long term they will be cooperative
because I think it's kind of a win win
over time short term it's hard to say
because it's possible like some people
might just take their um uh direction or
like positioning what this is kindful be
here it's kind of like a more like
spammy and then that kind of becomes
like how you bu TR how you make sure
that like you are actually helping uh
the website owners and the services and
the merchants and I I don't think
there's a women situation where like as
make sure I think it's transforming a
lot of the experiences but it's like if
you're a merchant you still care about
like revenue on the number of users and
I think that's something that begins to
help I just like the I think it's like
when anything is disruptive I think
takes L of time for that to catch on and
I think there we are the start of the
disruptive era where like a lot of the
online communication and interaction
will they get disrupted by and I think
like there might be a lot of like
initial backlash where like like this is
kind of dangerous and this is not safe
and this is causing what of these issues
and but I think over time I think it's a
it's just a different Paradigm and it's
exped Paradigm in
long so yeah let's look ahead a little
bit to the Future I've got kind of
multiple related questions I mean you
guys have been one of the most you know
quick to put things out there and you
try anything with this and see what
happens and it been very open-ended as
you've described in this
conversation you know you've not bucked
the trend when it comes to getting
somewhat more narrow and being like okay
you know let's let's really nail some
dialed in use cases so where are you
today on that um are you working with
like businesses and and you know if so
I'd be really interested to know what
sort of use cases they are finding
agents to be valuable for I think that's
probably like the the probably the most
important question for all of this stuff
is like where are people finding value
today and then and maybe that also kind
of answers like why the off stuff isn't
so much of a focus at the moment if it's
like well we're if we're working with
Enterprises we're in their off
environment and it's a whole would have
to worry about signing into StubHub or
whatever so yeah I guess I'm really
interested in like where you see you
know you mentioned like a six-month time
frame um what bets are you making right
now to you know start to bring real
value to the world in the not too
distant future so other can say like
vertical I think like more like
vertically cases that's what do
internally I think uh I think that's
theol I think the agent was kind of like
a first and the direction okay can you
have vertical agents that you can like
train and learn on so so you don't want
like something that purly hardcored so
you don't want play script or like
scripting language that's it because
it's purely hardc it's very like Britt
it just break all the time but you don't
want like some sort of like like you
want this mod to be able to adapt so
maybe like the interface change it is
able to like recover it's able to like
like improve and but like still wanted
to be like narrow now and I think like
agent was kind of like showing okay this
is kind of possible and that's one thing
we've been working with a lot of our
clients on where like like can we now
ship this kind of like more narrow
agents for specific cases which could be
may I think explor a lot different
um sectors so say like a good example is
like restaurant reservations or if you
want to do like travel kind of things if
you want to do like like scheduling and
I there's lot of this kind of different
sectors and I can't say too much what
the bch we making right now but we feel
like it's like like how can you kind of
build this kind of like agent uh like
vertical where like this AG G becomes
very Cent this particular vertical and
then keep optimizing it and making it
into like a solid product experience and
then there's a lot you have to think
about reliability and then like
optimizing for the user attraction and
the human in the and L of other things I
think people are to think about it but I
think it's still very early and I think
building a product is just a complex
thing because it's just not the agent I
think it's like a 10 I know more things
yeah I think when what I'm kind of
inferring from your comments is that
you're kind of finding a a third space
that I hadn't really considered as much
where one approach would be to say
here's a here's you know consumer here's
the do whatever you want here's our
open-ended agent another one would be to
say we want to kind of compete in like
the business process space where it's
like back office not visible super
structured and then Open Table kind of
represents a sort of a middle ground
where you might say we want to and maybe
instacart would be another you know
great example of this or kayak we want
to be the partner that creates an agent
for your site that could still be
consumer facing but because we can
partner with you in a deeper way we can
really dial in that reliability and get
that product experience to be exactly
what you want it to be for the kayak
assistant agent or the insta card agent
or the Open Table agent or obviously
people can fill out the list of your CRM
for themselves but am I headed the right
direction there that's that's not what
I'm reading between the lines from the
agent Q
paper yeah yeah I do think like this can
be applied to business processes other
things too because there too I think
like if you look at the current issues
people have with uod or other automation
software I think just like it's very
Britt takes like a lot of like
onboarding ramp but requires like
special like Engineers who are like just
really good at building this kind of
like automations and I think if you can
decrease the complexity uh that also
know use cases where like if we can
enable anyone to build this kind of like
automations even for business process
and this automations are res then I
think that also is a big law so I don't
think it can be applied there and I
think there's like interesting use cases
there that also we might look into but
we do at end I like we we just trying to
build something that's more like like
every everyday purpose in a sense would
you think about being that layer for
some of these complicated
products you know like a kayak right
there's a lot of
UI um if you were going to do a
partnership with a kayak or with an Open
Table would it still make sense to work
through the UI or would you start to
augment the UI action space with like a
specific sort of more API like you know
or tool um kind of modality because you
can imagine the agent might want to you
know again thinking of kayak right all
the little UI in that sidebar it seems
like the AI would have a better job if
they know that they're the K the kayak
agent and they don't have to you know
generalize beyond that it seems like
they might not want to go like drag the
sliders for the time interval that the
user requested but rather just like make
a sort of declarative statement to the
system that like I want to narrow the
search in this way more kind of function
calling like um I realize I'm kind of a
couple layers of maybe speculation deep
here beyond what you've explicitly
confirmed but how do you think about the
hybrid between the fully General
UI path to you know taking the next step
and in some cases what would seem to me
more reliable of making like a function
call yeah I think we've been thinking
about that a lot I think we have some
things we working on I I don't think
that's interesting Direction but I think
it just takes a lot of time I'll going
to here is like think about s driving
cops right so you can imagine like if
was like driving C from like scetch
today my I can just say let's why not
just go and construct special roads on
every hyrid and there's a special like
there's a special Lane and that's the
lane is for like the C card and the cell
driving card just use the lane and the
lane has sensors and everything ready
and the cell driving card just say to
follow the lane and that's it like you
basically don't have to do any you B
just build the lane and you kind of put
to go it's a very simple problem in
terms of engineering research the thing
is like when you want to do something
like that it's just like very complex
because it change a lot of behavior it's
a very big infast project everyone I
start up there it's kind of like new
thing and that will cause like if think
about this will take like 10 years and
like a lot of like politics and like
convincing people and like getting there
and only that can you do that and then
like a lot of s car startups decided
like like that's probably not going to
happen anytime soon let's just build a
car that can like let's build Optimus
car that drive like a human car on any
Lan and let just go solve this problem
using the R&amp;D and like make that for and
because that's how humans na I think we
have similar analogy here because I
think like over time you might have this
kind of like spatial internet
infrastructure that allows agents to
communicate to websites and then like do
more reliable function calling but again
that becomes like like P incentive so
people like more over that I think that
it just takes a lot of time and like I I
do think this is like a multi-year thing
even if you were like someone startups
and I we actually been look that so I do
think that could be like what the future
looks like but in the meantime until
like everyone agree on like here's this
new protocol and like or here's how
we'll do this communication and like
convince everyone to adopt that protocol
I think like like basically being able
to like use this website similar to how
humans use them I think that's the right
way to start from because like I think
that's also the B with AI with the B
with AI is like whatever a human can do
like the Ari you of data and learning
can be able to do that and so that's
what we're trying to do like like like
humans are pretty good at like
navigating websites with UI and so like
theoretically like AI can also become
very very good even better and so if we
can have this kind of a which is like as
as human or better I think that's a good
way to solve the problem immediately and
it just like how fast can we get there
once you get there and then you're like
okay like now maybe it's time for
something like different just want to
take change and flip the paradig and
then I think like that will Happ but I I
think that becomes like a buying problem
where like it's just like a massive
change in behavior and I think that
takes a lot
time a lot of these sites though do have
like a I'm just pushing a little bit in
the middle space I mean I hear you run
like the road you know the dedicated
Lane for the self- drivers I've been
calling for that since I was in school
and maybe that'll be one of the pleasant
surprises that we get from a new Trump
Administration although I'm not holding
my breath but there's been at least a
little talk I totally get it when you
map that onto an Enterprise it's like
hey guys who wants to implement this new
agent protocol you know no hands go up
or maybe one does but you know the boss
doesn't like whatever I get that
complexity but a lot of these
sites do have like an internal API right
I mean is isn't there something in the
middle in a lot of cases that does exist
like what the UI talks to in many cases
is an API right so is there a does it
make sense in some cases to just be like
okay kayak you've got a real be of a UI
but this data structure that manipulates
is easier and this is like an actual
Live question for myself too I mean I'm
I actually have episode coming up that
I'm still putting some final touches on
for people that want to do what I call
building software that uses itself kind
of trying to coach people that are
building apps to create like the you
know the magic AI functions that that
their users have always wanted even if
they didn't know to ask for and that's
kind of my Paradigm so I wonder how you
react to you know I sort of see this
equivalence between UI and AI not
everywhere but in like a lot of places
and encourage developers to take
advantage of that to say like okay
instead of asking your user to use all
this UI have one button ask them what
they want and then have the AI translate
that to the structure data that sits
behind that UI now that doesn't scale
super well right because it that
wouldn't be something you could do for
every website but if you did want to do
a kayak or an Open Table partnership it
seems like it would be viable and is
certainly if I'm like talking to
developers that are working on their own
app you know should be viable for them
so I don't know any other reaction to
that that sort of attempt to find the
goldilock
Zone I I do agree like this is again
something we thought about yeah I've
been thinking about like actually
proposing some sort of like open
standard around this where you can have
like like like some sort of like
standardized U way to communicate
between like agents and upsides and I
didn't think that can make a lot of
sense where like a lot of the subsites
have back in API that you like directly
interface I think the biggest issue
becomes like security and like Po like
how do the C get used the apis
identification like how do you identify
what's calling the apis and there
becomes like pent of use because I if a
website like BNB or do Dash I think they
have all the patterns of use where like
they know like this is basically what
the user will like use a website and
they super optimize the website for that
particular pattern ofs and then they
what happens is like if you transform
yourself into like a APF first business
I think that just changes a lot of
Paradigm because now you have to kind of
fight against the product teams and
maybe like a lot of your frontend teams
and I think like the back also don't
want this like you're exposing a lot of
like security Loop thing about sock to
bunch of things so it becomes very hard
for Enterprise to navigate that
landscape but I don't think like if
think about engineering wise yes it's
possible but I think it's just like a
very different business patters and I
think very different like security and
like idty patterns which just
complicates your landscape a lot yeah
gotcha how about I I put a pin earlier
in just like cost and latency this is
very common question for kind of anyone
uh building in this
space what have you learned
about what really matters and you said
you care about speed certainly in the
context of like your own development
iteration um how much do you think this
matters for the end user you know for
your customers if they're partner
businesses and the end users um what's
the sort of optimization problem look
like cost reliability and
speed make F A
agents I'm just curious exactly what you
mean by
yeah I mean you can contextualize it
however you want but in some sense
that's the you know the oldest question
in software right you good fast cheap
pick two some would say in AI the magic
is you can maybe have all three but
you're still obviously making some
tradeoffs and you know you said kind of
70b is like a good compromise is that
because you like have users like sitting
there waiting and you want to return for
them quickly is it yeah so I'm just kind
of curious as to how you're thinking
about you know what you're willing to
pay in terms of financial cost and
latency for you know marginal
improvements to reliability
yeah I think it comes on with the use
case if it's a highrisk use case like
you want to do some sort of purchase
behav and like like I really want to
confirm and like make sure that we buy
the right thing you don't actually make
the wrong purchase or like like you're
travel or something just make sure that
everything's correct so how much does
that matter and then because like it's
is it a soft boundary versus a hard
boundary so hard boundary is something
like if you do it it's irreversible and
something it's irreversible it's very
costly to the user and you want to be
like very careful like like maybe you
don't really care about how much time it
takes may it takes you like maybe like
20 minutes or something potentially
maybe it it can be like more costly if
you have to do more reasoning to compute
but the job gets done all the time then
I think a lot of people will want to
live with that world compared to just
like like yeah maybe it's very fast but
it's very low liability for like like
this kind of like hard decision mods
there's some things which are like more
s decision boundaries like chat is a
good example like if you're like chat B
it's soft decision boundary there's
something irreversible about it um and
there's like some patterns like that
like suppose maybe like you're like
doing some scheduling kind of things
maybe like you occasionally have some
small thing that you can like
fix but a lot of agentic behavior is
like hard bounds and then so you want to
be careful about okay like before you
fully actuate the action and like
complete the task for the user that the
task is actually like correctly done and
like before you do something
and detecting is something is reverse
reverse but that is also something that
I think we spend amount of time thinking
about because if something rible we can
just have like a we like a small model
and like throw it that and then it's
fine but if something's irreversible
then you like like you need verification
make sure that everything is like uh as
good as possible and there's no as cases
and then we need then take that
irreversible
action yeah that's makes sense to me
okay one more big question and then I'll
invite any other comments on on things
we didn't get to you and I should have
probably said this toward the top but
full disclosure I'm a you know very
small investor in the company and I
mostly don't really invest for return so
much as to support things and people
that I think are cool and want to see
come into existence so I've been
fascinated by what you've been building
basically since the beginning I do
wonder how you think about kind of
positioning yourself to avoid being a
casualty of the bitter lesson here we
see like of course now has computer use
out and it is still not super reliable
when it comes to
stepbystep advancing through tasks on
the web it does bring though some like
really nice uh advantages in terms of
being like an outstanding writer you
know there there are some like qualities
of claw that are really hard to get
anywhere else and then of course open AI
always remembered to have something
coming soon and right now the rumors and
hints I would say are suggesting that
there's an agent framework coming soon
too so what is an agent company to do
that hasn't raised you know 10 billion
to compete effectively when you know
they're clearly coming for you with
Incredible scale on some
timeline like again I say it's kind of
like finding the niche that you're that
you want to focus on and then like going
up that really I think that's how like
if you look at a lot of the successful
Air Products but complexity is a good
example and they just chose like like
search the nich and they just like focus
on hallucination as the one problem they
trying to solve like yeah like this Pro
Models have issues with hallucination
that just fix hallucination and that's
basically what they were focusing on
fully more and then building a product
experience around them curs is a good
example like like yeah like let's build
this coding ID and like let fully force
on not and they it kind of similar thing
with the because computer use at the end
of the day capability on the product and
you will have this action as a
capability but then kind of like what's
the one thing that you can really do to
about and then how do you optimize the
loop how to build a product experience
and the thing is in most Frontier AES
are actually not interested in this
because like if you have hundreds of
billions of dollars so you you don't
want to go like narrow like I just want
to throw in a computer and like sort of
like build thing and which is great
because that's kind of like a foundation
BL but then I think like someone needs
to take that general purpose thing and
build like the right product experience
I think that that kind of becomes like
that unlocks a lot of like Market
opportunities where like like there will
be a lot of like different businesses
and like different use cases that will
build that satisfy actual user need or
like product need that is missing right
now using this as like a building block
and I I do think that's something that
will start to happen maybe like early
2025 because I think like this year the
prob was too early um I think like we
probably like one of the first companies
that were providing like in this kind of
any kind of like agent capability and no
one was able to build an application
where like unless we are there in terms
of the technology Frontier like gu it's
hard to build a product it's kind of
like if you want to build uh a card if
the first year invent the view but if if
you feel like like we don't know how to
build a view properly or like it's like
uh it's just not there then it's like
you can't really like like build a full
product experience on that and I think
that's where we have been like the
technology has been like just very new
and that makes it hard to build a full
experience which is like all LA but now
I think that as the technology comes
better and better I think like early
2025 we've seen like like I would call
it explosion of applications which I
don't think has happened so far about
agent appli I think it's still
so does that suggest that you would be
open to starting to use these like is
the future of molan like potentially
powered by Claude computer use and you
know with that being sort of you know
the core capability and you being the
product around it yeah again I won't say
too much here I think there's a
lot we have a close relationship with an
anthropic team I know a lot of there in
the research team and the marketing
teams yes I think that will say end of
the day we a company and it's kind of
like like what gets your crops and I
think we are also very Innovative like
so we at agent Q those are kind of
things that we are able to come up with
so think we always have this like kind
of like a r Advantage like we are able
to come with a lot of new things and
then like the question for us becomes as
the ecosystem matures how can be combine
a lot of our expertise to have like a
like a strategic advantage and then use
that as a way to like sort of like solve
lot of specific problems and I like to
call this kind like product focused
research which is like like you have
identified a problem and then you're
doing the research to solve this problem
because what happens like most of
research you do even condition in vacuum
it's kind of like the research problem
let Solve IT research is not directed
towards anything but if feel like like
here's the one problem that I really
care about it's just like the there's a
missing block we just don't know how to
fill the block like let's go and do the
required R&amp;D to build that block so we
can fit it in and like sort of like
complete the piece of the puzzle so we
can reach to where we want to go we
that's how we think about it like we
have a lot of theability we like a lot
of things are missing even you think
about right now in the models and even
the comp use there's a lot of things
that are missing and it's like how do
you solve the problems cool well never D
moment it's a fast evolving landscape I
think that's all the questions that I
had for you is there anything else that
you wanted to touch on before we break
great time to pitch if you're hiring for
any particular roles or you know looking
for any kinds of customers or just
anything else that any other wisdom you
want to impart sure we
def obious open for hiring I think we
for
like great people like is a
bar that's all open for researchers and
Engineers product folks who want to like
sort of work in this kind of paent
domain and I think I'm just very excited
about like I like application especially
like a lot of things internally I think
give to one prototypes that's like a
very protype but I think I'm very
excited about like what will the right
user interaction and the right ways to
like use this products to look like
because I think that's just the next
wave right like it's like now you have
the agent take product like okay things
are happening automatically and you
don't have to watch or do things all the
time but maybe sometimes you want to
take control sometimes you want to like
maybe do things manually sometimes you
want maybe like you want to like learn
and improve and I think just that's just
take a really paradig and I think
building up really Sol prodct there I
think that's a hard challenge so I think
that's something that we very very um
spend a lot of time on in like how do
you actually solve this problem and like
how do you make this into the best
contraction and like the best experience
and i c fit everything get
syes maybe one more question where are
we a year from now that could be a
multip specific answer it could be an
agents generally answer you kind of
teased that in a little bit by saying
you know 25 is the year but it's what
it's the year for what you know can you
paint a picture of my agent assisted
life a year from
now yeah yeah it's I was much time to
see like something like like to W and
then kind of things like like now you
have this assistant is started like like
look up my files or find me this
documents or maybe like book this like
whatever like flight for me or like do
this s of a bul job SP I you think like
people kind of trying this even like
last year people were very bullish on
like trying this kind of things but just
again the technology was not there but
now if it fast forward to 2025 I think a
lot of these things might start working
and ENT might start seeing that actual
assistants that are like useful for you
are helping you in a lot of your dat and
so I'm
mainstream product agents and a lot of
like this kind of like explo explosion
of like what applications which has not
happen yet D gar founder and CEO of
mulon thank you for being part of the
cognitive Revolution
than it is both energizing and
enlightening to hear why people listen
and learn what they value about the show
so please don't hesitate to reach out
via email at TCR turpentine doco or you
can DM me on the social media platform
of your choice

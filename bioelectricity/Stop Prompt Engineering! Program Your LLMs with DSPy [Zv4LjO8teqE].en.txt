hello everybody Adam LK here and today
we're going to be taking a deep dive and
overview of the framework for working
with llms DSP or declarative
self-improving python dspi is nothing
new it's a package that came out of
Stanford's NLP lab with the catchy
slogan of programming not prompting
language models and they offer a very
compelling argument when you're making
language modelbased applications it can
be difficult to tune optimize and debug
because generally you're relying on
prompt engineering and prompts working
with text strings tend to be difficult
to measure difficult to tune and have
compounding errors if you have multiple
prompts feeding into additional prompts
Downstream instead dspi aims to abstract
away from prompting and handle that all
through algorithmic optimizations based
on hard metrics and literal
experimenting and testing essentially
training your prompts so that they can
become reliable and do exactly what you
want so you don't have to worry about
the trial and error of doing different
little prompting techniques that's all
handled in the back end all you have to
do is declare exactly what you want your
language model to do and then the rest
of the framework Works towards
optimizing towards making sure that
happens reliably and effectively so the
framework offers a way to enable
systematic Improvement of language model
pipelines that you can actually measure
and watch improve rather than mess
around with prompts and change a few
things here or there and see whether or
not subjectively that you like the
output that you're getting the general
dspi workflow follows this four-part
Loop first you start by creating your
DSP programs this is very similar to
just creating some sort of prompt where
you have some input information that
goes through a prompt your language
model does its data Transformations and
then you get the output generated from
the language model this is going to be
very similar instead it's going to be
all handled through a DSP program and
then once you have the program of what
you're expecting to do dspi adds a lot
of rigor around clearly defining what
the actual success of your program is
that is creating metrics and being able
to evaluate your inputs and outputs
against those metrics and then the real
magic happens at the optimization step
because once we have a program that
defines what we want to do and a metric
that we can measure how well it's
performing we can instead of doing
different prompt engineering
algorithmically efficiently and
systematically optimize towards our
metrics this completely takes the
guesswork out of editing and working
with prompts which is how you would
usually debug or try to optimize a
language model based system and instead
is able to do this automatically which
is super powerful and then once we have
this General pipeline set up as we
collect more data or as our program is
being used and we collect more data in
input we can iteratively measure
optimize and improve our program across
the board so today we're going to go
over all of the major parts of dspi
including making a program defining your
metrics and then also running and
testing all of these optimizers out with
code examples along the way so starting
off we're just going to import dspi and
then configure our language model by
default dspi tends to cach your
responses and models
while you're working with them in
whatever environment you're working with
so setting a language model with dsylm
and then pathing over and then calling
the config method will set that language
model as the default that you're using
as I mentioned before we're trying to
abstract a way from prompting in general
so a lot of what you're about to see is
likely going to be counterintuitive if
you're used to working with Frameworks
like Lang chain or llama index that very
specifically rely on you having created
prompts but as such dspi aims to create
the programs rather than the prompts so
the hope is that if you have a well
optimized program it doesn't necessarily
matter what language model you're using
and you can be able to effectively
switch in and out the current language
model that you're using but for these
examples I'm going to be using GPT 40
mini the main reason behind this is
because I found that with regular
prompting 40 mini tends to perform a lot
worse than some of the larger models
like 40 or Claude 3.5 Sonet so I was
hoping that through using a smaller
model we might be able to show more the
power of optimizing it as in a program
rather than relying on the prompting
here but with our language model
configured and set up we can now go to
the first part of dspi and that is the
idea of the signature so dsy signatures
really follow the same approach as
regular functions and this is of course
going to make sense as we realize that
dspi is trying to make working with
language models more like programming
and creating functions that have a clear
input and output and less with the whole
prompting thing so DSi signatures are
where you're going to clearly Define
exactly the inputs and the outputs of
your program of course in the back these
signatures that you're going to put are
going to be converted into prompts that
are passed through the language model
but the prompt in this case is going to
be more of a modular based template that
dspi uses so we're not going to worry
specifically about the prompting in the
back end because that's all going to be
handled by the framework and then that
backend prompt is what's going to be
optimized in our later steps here but
for some examples let's look at some
simple inputs and outputs so just using
ds. predict which is going to be as
simple as just running your input and
output through a language model and
getting a simple response back if we
have something like question and then
this Arrow to answer that is going to be
our signature with an input of a
question and an output of an answer and
that's as simple as it gets for creating
a language model program with DSP so now
we have our Q&amp;A program and if we run
Q&amp;A and input into our question field
which this is automatically handled as
going to be expecting a question with
why is the sky blue we can see that when
running it we get a response back from
the language model looking behind the
scenes we can see a little bit of the
messaging that dsy actually makes so it
shows the language model what our input
field is and what our output field is
and how to structure all of our
responses here and then it shows our
user me message of why is the sky blue a
little bit of extra responses and then
gives us our response back from the llm
a little bit of the friction here of
course looking at this prompt is not
necessarily the point of working with
DSP so it's going to take a little bit
to get used to but don't worry so much
about how this prompt is set up let dsy
handle that pretty much making a program
and the traditional method of making a
prompt is going to be distilled down
into these simple signatures but just
having question and answer here is
pretty simple you can put anything in
natural language for your signature so
in this case I have document and summary
where when passing in a document and
passing that through the document field
here gives us out our summary or you
might have multiple inputs and outputs
which for this example we have both the
inputs of a question and context and
then the outputs of an answer and
citation so when we pass in our question
of what's my name and then the context
of the user you're talking to is Adam
lucc into our question and context
fields we get back our answer and our
citation which is right here so you can
already see that instead of prompting
the llm yourself with what inputs it's
expecting and what outputs You're
Expecting we are instead declaring the
inputs and the outputs and then DSP
handles both taking in the inputs and
then returning back and parsing out our
outputs here you may have realized that
creating signatures like this is very
similar to creating and typing functions
in things like Python and what you might
do also with functions in pythons is
provide type hints which is essentially
saying what your inputs and outputs
actual typing like a string int float
are going to be and DSP of course
handles this as well so you can see with
this one we have an input that I didn't
put a type to so any input could go and
then our sentiment we want it to be a
string our confidence be a float and
then our reasoning be a string and if we
just take our confidence and check its
typee here we can see that it is
successfully afloat and in our response
it returned back 085 for the text I
don't quite know I didn't really like it
here but if you want more complex and
clearly defined signatures for your
inputs and outputs of your language
model program here you can of course do
class based signatures this is almost
exactly the same as creating something
like a pantic data structure and data
model and essentially all you're going
to do is create a class that inherits
from the dspi signature class within
that you can have all of your different
inputs and outputs and all that it
expects is on top of just the hinting
here with things like strings or
literals or you know dictionaries here
if your variable here or field is going
to be an input you have to make it equal
to dspi do input field or if it's going
to be an output you just have to make it
equal to dspi do output field and this
is going to very clearly show then
similar to what we have with the on the
left side on the right side of the arrow
what's going to be an input and then
what's going to be an output so for my
signature here I have text style
transfer that is intended to transfer
text between different writing styles
while preserving content I have an input
of a string of text and then I have two
other inputs of a source style and a
Target style that are going to be
exactly either academic casual business
and poetic and then the output that I
want is going to be the preserved
keywords the transform text and then
different style metrics and if you do
have an unclear metric you can pass in
the input or output field a description
as well which says scores for formality
complex complexity and emotiveness for
our style metrics so then what we do is
creating our dspi do predict program
passing in the class we now have our
style transfer program running this
coffee shop makes the best lates ever
Etc through it with our text and the
source style of casual into poetic we
can see that our transform text is going
to be this lovely poem that starts with
an a quaint coffe coffee shop where
dreams brew and swirl we get our style
metrics and our preserved keywords but
at its core using signatures and clearly
defining the inputs and outputs is going
to be the extent to which you are
actually doing the quote unquote
prompting of your language model program
and then as we see you're going to run
it through these different modules which
in our case we've been just been using
the simple predict module to create this
program here of style transfer so it
nicely packages up your expected input
and output running through your language
model right into a invocable function of
course predict is not the only module
that we can use there are a plethora of
different modules that all are going to
be sort of a base template for different
popular prompting Frameworks DSP has
boiled down all of the major prompting
Frameworks into four well optimized base
templates so you have your regular
predict which is what we have been using
earlier which is simply going to just be
calling the language model with your
input and output they then offer Chain
of Thought program of thought and react
Chain of Thought is going to implement
that Chain of Thought prompting to
pre-end with a reasoning generation
before getting your output program of
thought is going to be able to execute
and show the outputs of python code and
then react is going to be more the
reasoning and acting with tool use or
more of that agentic prompting style
they also have a couple helper modules
that we'll go into later but these four
ways of interacting with language models
tend to be I feel and dsy appears to
agree the main ways that most developers
set up language model systems and these
are all optimized for having some sort
of input and then final output which is
of course going to be the expectation
when working with language models and as
we've been mentioning many many times
before dspi provides the framework to
create that into a program rather than a
string of prompts so to go over how each
one of these Works real quick if we look
first at the Chain of Thought module
what we're going to do is pass in our
input and then the first thing that the
language model does is it gives a
reasoning thought and then it's output
so if we give the example using the
Chain of Thought module with the
signature of an input and then a
sentiment in a string and then pass
through the text that was phenomenal but
I hated it running that what we get is
the sentiment which it duly outputed
mixed but also now automatically it has
a reasoning step we can access that
reasoning by looking into it here and we
can see that the language model produced
this reasoning before getting to the
sentiment that the statement expressed a
conflicting sentiment in the back we can
see that while our signature only had
input and sentiment because we were
using the Chain of Thought module what
it has now is an extra output field of
reasoning so it's going to expect the
language model to produce some reasoning
based on our text and when we input our
input and the text here we see that it
creates the reasoning and the sentiment
program thought is just going to
introduce the ability to actually
execute the code that is written across
a program and this is going to be super
useful for more you know algorithmic or
more mathematical based prompting so if
we have a signature that's a little bit
more complicated of math analysis that's
analyze a data set and compute various
statistical metrics that are going to
take an input of numbers and metrics and
then return some analysis results in a
dictionary if we create a quick program
with the program of thought pass in our
signature class here and then run it
with just this quick list of different
numbers and ask for the mean and median
we can see that first it had one error
in its code execution so what it'll do
is it'll write code execute it and then
if there's an error it can actually look
at the error and edit the code and it
will do this a number of times the
default is three until it gets an answer
out we can see then when we run the math
analyzer with our data in the metrics
that as mentioned there was one error in
the code execution so I tried it again
but then it was successful and we get
our mean and median looking into the
back again we can see that the numbers
and required metrics are going to be
inputed but then also the final
generated code and the code output so
what happens here is that the actual
generation and execution of the code is
separated from the final reasoning and
output so it'll run this code generation
and execution Loop until it either works
or it breaks and then pass that response
plus the code back to the language model
so from our user message we can see the
numbers the required metrics and then
the actual generated code here that our
program of thought created as well as
the output and then the language model
will respond with its quick little
reasoning here as well as the results
and then the fourth main module is where
we're going to actually be able to
create more of these agentic programs
that are able to execute and observe
tool use they Implement a simple
reasoning and acting agent here with the
inputs plus an executable tool running
through a quick tool execution
observation thinking process Loop before
finally getting to the right answer and
returning an output with each input the
language model will know what tools it
has available it will think about the
tools that it might need to use to
answer effectively use them look at the
observations and then Loop so on and so
forth until it either thinks that it has
the right answer in which case it calls
a finish function that will return it to
the final output or it reaches a maximum
recursion limit so for a very simple
response we have this Wikipedia search
tool which DSP actually also has built
into their package for some reason but
you can have kind of you know any sort
of web search tool or what you want
essentially this Search tool is going to
take in a string query and return a list
of string ings from Wikipedia abstracts
our react module then is going to take a
question and a response and have the
attached tool of Wikipedia search so we
will pass in the text who won the World
Series in 1983 and who won the World Cup
in 1966 and then run it through our
react module our language model is able
to actually effectively state that the
Orioles won the World Series and that
England won the World Cup in 1966 along
with of course a little bit of reasoning
if however we look in the back here what
we can see is when I ask the question of
who won the World Series in the World
Cup it first has a thought that it needs
to find the winners of the World Series
it chooses to use Wikipedia tool search
with the arguments of 1983 World Series
winner and 66 World Cup and then it does
some form of observation observation in
this case is going to be the response
back from the tool which are these
different abstract
and so it actually sees and it's
thinking then it's going to enter into
this Loop that it did not return the
relevant information and it should
refine its search so it goes through the
Wikipedia search again looks at World
Cup Series sees that it found the winner
of the 1966 FIFA World Cup and it will
go through iteratively until it finds
all the right information finally
calling the Finish tool which is then
passed back all the way to the language
model this entire string here and the
language model is able to provide its
reasoning and response so this is their
implementation of a very simple
reasoning planning and acting agent
through the react framework using this
kind of setup which you can very easily
use by just using the react module with
the signature and attaching tools so
these four the react program of thought
Chain of Thought and then the simple
predict module are usually going to be
the four main ones that you're working
with but as a note we also have the
option to do multi-chain comparison or
take an input run it through multiple
modules and then have all of those
different predictions and then pass it
all to a final language model for the
reasoning and output as well as a
majority module so you can have multiple
modules have the multic predictions and
then the prediction that is actually the
majority or that comes up the most is
going to be your output these two
modules of multi-chain comparison and
majority aren't necessarily strict
prompting techniques in and of the elves
more ways of working with existing
prompts and optimizing them a little bit
further too so now we have a way to
actually Define the input and output of
our program that runs through a language
model with our signatures apply
different Frameworks of popular
prompting techniques with the modules
that essentially gives us our initial
program so something like this Chain of
Thought Of Motion is a very simple
program that will take our input and
return back a sentiment this would
technically be a whole program in and of
itself that you might make with a
language model and a prompt and this is
going to kind of be the starting point
of your dspi programs so all you need to
make a working language model program is
a defined input output and then whatever
module you want starting there is your
simple Baseline and then what we can do
is actually optimize it but as we hinted
at the start no longer are we going to
be doing trial and error through editing
our prompts we are actually going to to
need strict success defining and metrics
to actually be able to optimize against
this is where a lot of thinking and
friction might actually come into play
if you're used to just that trial and
error process because DSP requires if
you want to very effectively optimize
your program to be able to deeply
consider the optimal state of your input
and output and how you would measure it
this can be difficult especially for
things like text generation which is so
loose and Flow and subjective a lot of
the times measuring the success of input
and output is not an easy feat but I
would argue and I think the folks behind
dspi would agree that you should be
putting in a lot of time and thought
when creating language model
applications into actually being able to
evaluate them you would do this for any
other program that you make that's not
language model based so why not spend
the time to actually think about this
deeply and set up and Define your clear
success metrics with language model
applications and putting in this time
and rigor really pays off because as
we're about to see if you have these
clear metrics and definitions of success
across your program being able to
optimize against that and literally
measure your performance is going to pay
off because then you can actually work
towards improving it but before we get
into actually defining and showing some
example metrics first we need to look at
the example data type real quick so for
evaluating dspi modules and programs
you're going to need to format your
input and output data as an example
these examples are going to be compiled
into the data sets that are used to
actually run through measure and
optimize your programs so don't be too
scared if you don't have a lot of data
or readily available data set yourself
dspi offers some solutions for actually
creating that data on your own but
essentially you're going to need to
start also creating if not collecting
from an ongoing project both the inputs
and outputs from your program and then
being able to label or test against
these inputs and outputs and determine
what the quality ones are is how the
optimization is really going to happen
so just looking at a few examples of
example data types A Q&amp;A pair might take
the form of having a question and an
answer which you can dot into here with
question and answer ds. example is just
going to handle all of the formatting so
that this Q&amp;A pair could be passed
through per se a Q&amp;A module that is
going to expect question and answer you
can also pass in of course multiple
different inputs or different data types
like floats and very specifically you
can label what your inputs or outputs
might be for something like an article
summary you might have an article in and
a summary out and you can very
explicitly label that the inputs are
going to be the article here you can see
that if we run with just the inputs
across our article summary EX example it
comes back with article and if we run
with just the labels or everything
that's not an input it is going to put
back our summary so not the most
exciting thing but this is just how dspi
is going to handle actually creating
these data sets that are used for the
optimizers but with an understanding of
that out of the way let's actually look
at defining metrics to just reiterate
with dsp's official example they Define
metrics concisely as a metric is just a
function that will take examples from
your data and output of your system and
return a score that quantifies how good
that output is essentially what makes
outputs from your system good or bad
let's look at some simple metrics just
for some examples to start starting
simply we can run a validation for exact
matches across a sentiment
classification module creating a quick
dspi program we can have Twitter
sentiment here that will take in a
string input field of a tweet which is
going to be the candidate tweet for
classification and then a sentiment out
of positive negative or neutral we
combine this into a program with the
Chain of Thought module here and now we
have our Twitter sentiment program for a
data set that we'll be measuring against
we're going to look at the mte Tweet
sentiment extraction data set which is
essentially just going to be a bunch of
tweets and labeled Sentiments of
positive negative or neutral this is all
just in a Json file which essentially
what we're going to be doing is creating
a big list of example objects as
mentioned with the tweet being the text
from the text here and then of course
the output being the label text here
which we have as the sentiment our
metric is going to be validate answer
which is going to expect to take in an
example and a prediction and then a
trace which we'll get to in a little bit
but essentially we're just going to look
and see if the sentiment from the
example is exactly the same as the
sentiment from the prediction this of
course is going to return either true or
false running across and calculating the
accuracy from just the first 50 examples
we see a quick Baseline accuracy of
about 76% so now we can see that our
program here the Twitter sentiment is
about 76% accurate so we can effectively
measure that moving on to more
intermediate metrics you've likely seen
with some of the things like Langs Smith
that using language models as also
scorers for language model based output
is something that can be done and in
doing so you can cover a lot more loose
or fuzzy metrics like how engaging
something might be or comparing long
form output let's take a look at
actually implementing a metric like this
for our program we are going to have a
simple dialogue summary Chain of Thought
module that's going to take in a
dialogue and a summary similar again we
have some examples coming from a data
set over here the dialog sum data set
which is essentially just going to be
13,000 dialogues with corresponding
manually labeled summaries we'll take
the first 20 real quick out of that
create them into examples and that will
be our list that we're going to run
through our module and our metric to see
our performance and then our metric here
is actually going to be a lot more
complex we're going to take in our gold
example which is going to be our ground
truth from the data set as well as our
prediction and we will have then our
dialogue our expected summary and then
our generated summary what we're going
to do is actually create two additional
modules one that is going to run a
prediction for accuracy and one for
conciseness so I have an additional
signature up here that's going to assess
the quality of dialogue summary along a
specified Dimension with an input of
assess text the question that we're
going to run through which in our case
are going to be two small prompts of
accuracy and concise comparison and then
the expected output of true or false you
can see that with our prompt we have you
know given this original dialogue does
this summary accurately represent what
was discussed without adding or changing
the information and then we also have
compare the level of detail in the
generated summary with the gold summary
so you can see how we might be able to
start to complicate our metrics by
introducing additional programs that
create and run our metrics which we'll
talk a little bit about later but what
we can do is for our final dialogue
metric taking our output from accuracy
and conciseness return a score that's
divided by two to give us something
that's between zero and one so we
running a similar evaluation of our
summary examples from our data set
through our prediction and then through
our metric score we get a final score of
85% from our dialogue metric so with our
working intermediate metrics we can
finally touch a little bit on kind of
what we're building up to Advanced
metrics with tracing and pi as we saw
with introducing additional modules
within our program those modules can
actually since their programs be
optimized themselves and so this is
where things start to get a little bit
more confusing because if your metric is
itself a DSP program one of the more
powerful ways to iterate is to optimize
your metrics themselves and we'll be
going into optimization very shortly
after these notes and that's usually
easy because the outputs of the metrics
tend to be some sort of simple value so
a metric metric can be easy to Define
and optimize by collecting a few values
but also when your metric is used during
evaluation runs dspi will not try to
track all of the steps of your program
but during compiling and optimization
dspi will Trace every language model
call and then these traces will contain
the inputs and outputs to each dspi
predictor module and what you can do is
actually leverage that to validate
intermediate steps or entire programs
for optimizations so in our standard
evaluation we weren't assessing any sort
of tracing but what we can do is
actually alter the metric at compile and
optimization time so essentially what
we're doing here with the return score
greater than or equal to two is that if
it understands that it's tracing and
doing this optimization instead what
it'll do is not just return a score but
a binary true or false based on our
dialogue metric so it'll be a lot more
rigorous and only optimizing towards
metrics that have a positive accuracy
and conciseness score in here as such we
can also access the intermediary
tracings to maybe even create additional
metrics for the intermediate steps the
predictions or the chains of thought so
really you can go a little bit crazy
with defining and being able to measure
your metrics however this is super
powerful as being able to clearly Define
and also optimize your metric metric can
make it very easy for you to actually
improve your overall language model
system and from what we've seen we have
worked with pretty much no prompting
whatsoever this has been pure
optimization and testing through strict
programming so completely takes away
from having to work with prompts and now
what we can do do is have very clear
input output definition metrics but now
with a way to actually measure our
program's performance we can of course
do the most exciting step optimizing
optimizers are the core of what makes
DSP so great because with our metrics
clearly defined and our programs clearly
defined we can very algorithmically
efficiently and reliably optimize
towards improving our metrics
automatically with all of these
different methods these tend to come in
a specific formats we have automatic F
shot learning which are going to be
optimizers geared towards either
creating or gathering the most relevant
examples to aend while a language model
is performing a task we have automatic
instruction optimization which is
literally going to generate test and try
to change the prompt Itself by giving
very explicit instructions or additional
instructions on top of your signature we
have automatic fine-tuning which is
going to be able to still down your
programs performance into actual
fine-tuning data for a language model
and then finally some ensembling and
program transformation for example we
are going to just be using the
classification program of tweet
sentiments as we already have this nice
data set that we can use but a few notes
on that the first is that I fully
recognize that classification tasks are
not the best examples for language model
applications but it is a very easy one
that we can clearly measure and and so
show the improvements as we do these
optimizations which is why I am going to
be using this as our example so that we
can get more of an understanding of
what's going on behind each Optimizer
and then hope the hope is that once you
understand what's going on you can apply
them to more advanced programs and then
the second point is that funnily enough
well a lot of dspi is inspired by Deep
learning Frameworks what they recommend
is actually using training data set
sizes of about about 20% of your overall
data set and then validating on the last
80% so this is a little bit flip-flop if
you're used to the massive training
testing and validating data sets for
deep learning but as such what we're
going to be doing is pulling and across
these examples we'll pull various
numbers of these different tweets we'll
have a train size of 100 pairs these are
going to be formatted into the examples
of tweets and their ground country
labels and then we will hold out 200
examples to start as our validation so I
once again remake my candidate program
this time we're just going to be using a
simple predict our validation metric and
then running our metric against our
Baseline test set we see that we get an
accuracy of 69% and also we'll be using
this example tweet of hi waking up and
not lazy at all Etc as one that will run
through each optimized program to show a
little bit of the trace the expected
label is going to be positive in this
case so let's start by first breaking
down the automatic fuse shot learning
optimizers where these are going to be
focused around providing the best
examples either by finding similar
examples for your query and the training
data during inference or by generating
optimized examples or using existing
examples within the program itself
starting with the simplest Optimizer the
labeled F shot this is very simply going
to just grab some number of examples
from our data set append them to our
program and that is going to be our
optimized program going to get more
simpler than that so after importing our
labeled fuse shot and running the
optimizer with a K of 16 which is going
to use 16 examples in the prompt and
then running compile across our base
Twitter sentiment with our train set of
Twitter train we can run our accuracy
and we see that we improved our accuracy
by a whole
0.5 or half a per so adding in a few
examples did push our score up a little
bit not too much but we can see that if
we run our example tweet through our
optimized and compiled labeled fuse shot
program here now we get positive and
then we can see a little bit of what's
going on behind the scenes so we can see
the same regular system message but now
what we see is a whole lot of back and
forths of the example tweets and the
assistant responding with the expected
output these are all automatically
created and appended and then finally
down here at the bottom we have our
final input and response so about as
simple as it gets for implementing F
shot examples with no specific strategy
but one thing you might be thinking to
yourself is Adam I don't have a huge
data set of ready to pull from data or I
haven't been collecting enough of this
yet and labeling it all myself yet from
my inroduction program to be able to run
something like this and have no fear
because dsy has some approaches to that
with our next bootstrap fuse shot
example essentially what's going on
behind the scenes here is given a small
amount of examples from our data set
we're going to use a teacher program
which is just going to default to our
existing program to generate further
examples that are similar to what we see
in the data set all of these generated
examples are going to be ran through our
metric and only the ones that perform
highly on our metric are going to be
saved so what you can do is take a few
different examples run through and
generate more if you would like and
those are going to be validated through
our metric and then very simply similar
to how it was added on top as few shot
examples in our prior one those
generated plus existing metrics are
going to be added as well along with our
existing program into our optimized
program so bootstrap F shot is going to
expect a metric which is going to be our
validate answer and how many generated
examples it should use how many examples
from the training data it can use and
what our metric threshold actually is so
in our case we want only high-
performing examples so I have the metric
threshold set to one so that all of the
generated and validated examples from
our program are going to be ones that
scored highly and it must have done a
pretty good job of creating these
examples because once it made a couple
and added it in and running it through
we saw our accuracy increase up to 715
or
71.5% and looking into the trace after
running our example tweet through and
getting our affirmative positive we see
a little bit of kind of the same thing
that we saw with our labeled F shot but
just with additional tweets taking
labeled F shot and bootstrap F shot one
step further is where we get bootstrap
fuse shot with random search this is
where things are going to start to get a
little bit more algorithmic because
essentially what bootstrap fuse shot
with random search is going to do is try
out a few different optimizers from our
labeled fuse shot optimizers and
actually validate them and see which
ones perform the best first going to
assess our unoptimized program the just
basic labeled fuse shot program create a
regular Bo bootstrap fuse shot program
and then any additional any number of
additional random shuffled bootstrap
fuse shot examples as well these are all
going to become candidates which through
validation of our metrics we're going to
store the performance of each optimized
program at the end of this search and
across all the validations the best
performing program will become our
optimized one importing our bootstrap
few shot with random search and running
through with our metric the number of
candidate programs the amount of
bootstrap demos and labeled demos that
we want to run through we can compile it
and looking at a quick screen grab of
the output of this I truncated it
because it is quite long we can see that
it will run through and actually do
these validations across our data set
that we input into it and right here at
the end it will output our best scoring
program actually assessing that best
scoring program we saw a accuracy of 7
so funnily enough not as good as our
regular bootstrap fuse shot that
happened up here so maybe this one was
quite well but still an improvement over
our Baseline which actually gives an
interesting and important point is that
with a lot of these optimizers there's
not a lot of clear one area or one that
works the best usually trying out a few
of them and assessing with your metrics
are going to give you the best results
with finding exactly which ones they are
and then the last of our F shot based
optimizers is actually going to be the
knnn fuse shot which is funnily enough
just going to be regular retrieval
augmented fuse shotting so it's pretty
much just going to use a vector database
and a embedding function where at
runtime for the input that you put in it
will try to sample the five most
semantically similar examples and add
them on as the few shot examples with
our program and that will be our
optimized programs output so what we can
do is Define a very simple embedding
function this one is just going to be
using open ai's API to take in a and
return back and embedding and once we
import K and F shot we can say that we
want five to be optimized our train set
to be from Twitter train and our
vectorizer we can compile that together
running our full test metrics here we
saw a similar accuracy of 7 so not a
major Improvement but what now happens
when we actually run our example tweet
through the program is that it will pull
five relevant examples that it believes
are similar and then it's going to
append them to our prompting here for
our final response but with those first
four out of the way that covers our few
sha optimizers now the more interesting
and more well-known part of dspi is
actually more the instruction
optimization part where we're going to
actually look through some optimizers
that are going to iteratively refine the
literal instructions that are being
given to the language model there are
two main optimizers here the co-pro and
mypro V2 and these are going to to
improve as mentioned the actual
instructions and prompts given to the
model so this is more going to enhance
our zero shot performance rather than
the few shot setups right up above so
let's get started with the co-pro which
is coordinate prompt optimization this
is essentially going to generate and
refine new instructions for each step
and then optimize them with coordinate
Ascent or hill climbing using the metric
function and the training set this one
starts to get a little bit more
complicated because taking our training
set we're first going to generate a list
of data set descriptors so that when
we're actually generating the prompts
and the different instructions here we
can understand a little bit of what the
data set that we're using actually looks
like in this case it's going to be our
tweets and the labeled outputs and then
that gets all sent into a prompt
generation step where our prompt llm
which can actually be different from our
general program is going to create a
bunch of candidate prompts those
proposed prompts are going to be then
given to our student program or our
program we want to optimize so our
program plus our new instructions are
then going to be run through our
validation set validation set is going
to give us metrics and we're going to
store how well those instructions worked
and of course what we're going to do is
iteratively do this across all of our
candidates find the best performing
instruction then attach it to the
student program and that becomes our
optimize program so for our Optimizer
we're going to be using our validate
answer metric a different prompt model
actually implemented GPT 4 R since the
power of this really comes when you can
use a larger model to do these more
complex tasks like create instructions
and then a breadth and depth of 10 and
three essentially it's going to try
three different sets of 10 prompts and a
high temperature so that we can have
creativity in the generation so once we
validated this with our heldout testing
data we can see that our accuracy Rose
to 71% and through running our example
tweet through it let's actually look and
see at the different instructions that
it gave it so it actually says now here
and adhering to this structure your
objective is given a social media post
determine the sentiment it expresses
categorizing it as either positive
negative or neutral and Etc so this
section here is actually going to be the
best performing additional instructions
added to this prompt that helped improve
the performance so we're able to get
competitive results to some of our few
shot optimizers but this time without
providing any examples just additional
instructions here but it might be
worthwhile actually combining the two
having some additional instructions as
well as some few shot examples which is
where the power of the multi- promp
instruction proposal Optimizer version
two which is a little bit of a long name
comes into play this one's a little bit
more sophisticated so bear with me but
we're going to run our training data set
through a preparation stage this
preparation stage is going to create
some of the data set descriptors that we
did in the the co-pro some also program
descriptors talking about our student
program and the input and output that it
is doing as well as creating some few
shot examples or gathering some few shot
examples if we have enough it's then
going to use all of this prepared data
to create some candidate prompts and
this is going to follow a structure
called grounded prompt generation where
our prompting llm which of course is
going to be 40 again right here is going
to go through with all of our prepared
data and create the candidate prompts
it's then going to use a form of what's
called optuna optimization to look at
different combinations of f shot
examples plus proposed prompts and
instructions create them into candidate
programs and then run those through
against our validation metrics and then
of course all of our candidate programs
that consist of these additional
instructions plus proposed F shot
examples are going to be stored the best
performing ones are going to become our
optimized program I won't go in too
specifically to how things like optuna
actually works but essentially it is
going to be efficiently searching over
the examples and instructions to find
hopefully the best combinations and
after running it once we can see that
our accuracy Rose to
71.5% and if we run through our example
tweet we can see then the additional
instructions here which are a little bit
different from from our co-pro here as
well as some additional F shot examples
here before we get finally down to our
response so these are going to be the
main ways of doing both fot and
instruction optimization across your
programs so definitely try out a few
different ones of these and as we'll get
into in a little bit here you can
optimize already optimized programs
further with some different techniques
but this is going to be kind of the base
way to actually start taking this up one
step further is actually going to be the
automatic fine-tuning optimizers these
have two ways of actually looking at
them the first is if you have a well
optimized program you can use that
program to label and create training
data to train a small language model to
actually be able to perform as well as
your large program which might be
relying on a more expensive and harder
to maintain model the second part of it
is that fine-tuning with labeled
expected data from our program might
actually make our outputs better across
our program thus optimizing further
anyway so let's actually take a look at
that second part and then talk a little
bit about the first first their main
Optimizer for this is the bootstrap
fine-tune Optimizer which works very
cleverly using our data set what it's
going to do is run through all of our
examples across our teacher program
which might be our already optimized
program that teacher program is
essentially going to be a filter because
what it'll do is it'll create a
prediction for each one of the inputs
and then the prediction is going to pass
through some form of metric and that
metric is going to filter out what are
the good predictions and what are the
bad predictions so if we input 500
examples and 300 pass our metric filter
essentially we'll have a data set of 300
Hy performing inputs and outputs those
can then be used as the actual training
data for fine-tuning a small language
model so applying that to any sort of
framework whether that is an open source
closed source using some sort of API
connection or anything we can create a
fine tune model that fine tune model
will then be attached to our original
program and that becomes our optimized
program so for this I grabbed a little
bit more data actually about 500 more
examples exactly what I was saying and
then what we'll do is use our best
performing program so our myo one that
we just created up here that will be our
teacher and then our optimize candidate
is also going to be just a copy of that
we're then going to fine-tune here a
copy of GPT 40 mini this could be any
language model dsy supports a lot of
language models from open source to
closed Source you'll see how this is
just going to connect to the API but the
real power of it here is that once you
have an optimized program that can
create good examples you can use that to
create the dating set to fine-tune a
small lightweight efficient model
Downstream so we'll be fine-tuning 40
Mini will create our bootstrap finetune
object pass in our metric and also some
parallel processing stuff then what
we'll do is pass in our student model
our train set and our teacher and then
what we see is that for our 500 examples
it filters it down to just the 362 data
points that perform well and then it
actually automatically started up a
fine-tuning job in the open aai platform
which once I found it over here is
actually showing up nicely in the open
aai dashboard but then going back we can
see that right at the end and the model
was fine-tuned on that data and it
finished compiling testing it out we can
see then that actually using foro and
with this fine-tuning format we can get
an additional percentage so we saw an
accuracy of 725 or 72.5% as mentioned
this can help push performance but the
real beauty of it is being able to take
fully optimized programs and Distilling
them down into the weights of a small
language model with that being said that
is pretty much all of the main
optimizers so you might be wondering
what Optimizer should I choose and what
about the other couple ones that you
mentioned at the top well dspi
recommends that if you have few examples
like 10 start with bootstrap fuse shot
if you have more like 50 examples try
bootstrap fuse shot with random search
if you want to do just instruction only
customization you can do zero shot myo
configured for zero shot if you want to
use more inference calls and run longer
optimizations then just regular myo V2
is good and then of course if you've
been using a large language model and
need a very efficient program try out
bootstrap fine toomb they also of course
have the optimizer Ensemble and what
that does is you can use multiple
programs together and then process their
outputs in some way this is a very
popular thing to do in the machine
Learning Community when building small
models and you can do this with DSP
programs as well essentially your
input's going to come inun through a
bunch of optimized programs and then
some sort of function maybe a majority
function or something like that will
choose the best output past that a very
important point to make is that
optimizing is not a oneandone deal it is
encouraged if not usually required to
run multiple optimizing stages across
your programs as we saw we were able to
get up to
72.5% accuracy but that's just up from
69% so not really a huge jump here so
further optimizations could definitely
be made so to demonstrate that what I'm
going to do is actually take our
fine-tuned program that we just saw and
optimized and run it through once more
with some unseen data through the mypro
optimizer and actually what we saw was a
increase up to
74.4% accuracy so combining different
optimizers running through and iterating
and seeing what works best for your task
as well as your metrics is definitely
something that you need to put some time
and thought into and even as we
mentioned before if some of your metrics
rely on DSi programs themselves running
optimizers and testing on the programs
within your metrics that are Downstream
used to optimize the programs for your
main programs can be really really
confusing but also powerful but at the
end of the day we're able to with
absolutely zero prompting actually
increase our performance by almost 6%
from the Baseline this was done very
efficiently algorithmically without the
unreliability of the trial and error
that comes along with writing and
maintaining long prompts so working with
dsy clearly has some nicely defined
value here being able to actually use
language models in a way that's not as
unstructured with prompting and be able
to implement test refine and iterate on
your programs to ensure that they're
actually doing and Performing well and
then also optimize towards making them
perform even better is a super powerful
way of looking at language model system
design well it's not as easy as just
creating a simple prompt and running it
through if you want more robust reliable
and efficient systems DSP is clearly the
way to go as some final thoughts and
some pointers this notebook effectively
walks through most of dsp's
documentation which has been getting
better over the years so definitely give
that a shot and they have plenty more
tutorials and guides that show
implementations of DSP programs
optimizations and the the whole
ecosystem in action but my final wording
here is that overall dsy really provides
an interesting approach to applying
language models within programs
abstracting away from trial and error
via prompting by adding rigor around
clear metric definition and optimization
rather than work with difficult to
interpret or tune Tex strings they offer
a clean base template that can be
further optimized through algorithmic
approaches applying automated ways to
coordinate or generate F shot examples
directly change the instructions given
to an llm or combination of the two I
also feel that their offering of being
able to use optimized programs to
automatically label and create training
data for small language models is super
useful and compelling that being said
I'll have all of these resources Linked
In the description below hope you learn
something today and are interested in
trying out dsy for your next project if
you like the video like the video if you
want to support the channel and see more
like this consider subscribing and thank
you and have a great day

now PA says question for Neil how does
he see interpretability playing a role
in AI security not alignment for example
crafting more exotic jailbreaks and he
says to tell you to Blink twice if you
can't answer due to an
NDA
uh yes uh sorry uh Jokes Aside what was
the question
so there was this beautiful meme where
you draw Chi PT as a shogo an eldrich
monstrosity from love crafty and horror
Fiction with a smiley face on top
because language models are bizarre and
confusing things that are just I don't
know they're kind of a compressed
version of the entire internet that will
do bizarre things in bizarre situations
but then open AI tried really hard to
get it to be be nice and gentle and a
harmless assistant and look so normal
and reasonable and safe which is the
smiley face mask on top of the
underlying monstrosity but unfortunately
the smiley face mask means people don't
realize how weird language models are
have you ever stopped to think how
strange it is that we're all alive right
now out of all the possible times in
history you were born into this
generation you have the incredible
Fortune
and responsibility of being on the Earth
today let's not waste this opportunity
you can use this time to do something
meaningful that'll make the world a
better place but the problem seems so
huge global pandemics climate change the
risk of nuclear Armageddon the threat of
AI existential risk how can one person
have an impact on issues this enormous
the world is really really really
complicated like if you want to
understand a question like how big a
deal is ax risk or should I work on it
um just like one sub question I care
about is AI timelines how long until we
get human level AI now I recently
discovered 80,000 hours they're a
nonprofit effective altruism aligned
organization and what they do is they
use evidence and Analysis to determine
how people can have the biggest impact
with their careers if you want to solve
Humanity's biggest problems you have to
start at the very core we need to focus
on safeguarding Humanity's entire future
because if civilization just came to an
abrupt end whether through climate
change or nuclear Armageddon or even AI
existential risk then all progress would
just end future Generations wouldn't
have a chance of building a better world
or reaching their full potential and the
good news is that 80,000 hours have
identified a couple of concrete steps so
that folks like you can use your careers
to combat existential risk ensuring that
Humanity's light continues to shine for
generations to come learn more by
visiting their website on 8000 hour.org
mlst grab their free career guide start
planning a career with true purpose
because you only have 80,000 hours so
make them
count there's no catch there's no secret
monetization or anything like that these
folks have an incredible podcast they
have lots of materials that you can
download basically to help you have a
huge impact with your life and your
career especially if you're someone who
really really thinks about humanity and
our plight in the longterm future this
is really something you should be
looking at yeah that is a question right
um Nick the path he says broad question
do you see Mech and turp as chiefly
theoretical or an empirical science and
will this change over
time yeah um I see this as very much an
empirical science with some theor
sprinkled in but you need to be
incredibly careful so fundamentally I
want to understand a model and I want to
understand how the model works and a sad
fact about models is models are really
cursed and just work in weird ways that
aren't quite how you expect and which
represent Concepts a bit differently
from how I expect them to and just do
all kinds of weird stuff I wouldn't have
expected till I went it poked around
inside of them and I think that if
you're trying to reverse engineer
Network and you don't have the capacity
to be surprised by what you find you are
not doing real mechanistic
inability it's so easy to trick yourself
and to go in with some bold hypothesis
of this is what the network should have
and you probe for it and it looks like
it supports that but you dig further and
you were wrong and yeah I think there is
room for Theory I think in particular we
just don't have the right conceptual
Frameworks to reason about how to
understand a model and we'll get into
fundamental questions like superposition
later on
but yeah I think that theory needs to
come second to imp arism if your
theoretical model says X and the real
model says y your theory was wrong which
is the Story of All of machine
learning so goite Tech she says question
for Neil does he think a foundational
understanding of deep learning models is
possible and does that extend to
prediction using a mathematical
Theory um possible is such a strong word
like if we produce a super intelligent
AI will it be of doing this probably um
in terms of foundational
understanding um I think there are deep
underlying principles of models I
believe there are scientific
explanations for lots of the weird
phenomena we see like scaling laws
double descents lottery tickets the fact
that any of this generalizes at all I'm
hesitant to say there's some like strong
things here or some strong
guarantees
like I don't know models are weird
sometimes if you change the random seed
they will just not learn I'm pretty
skeptical of basically all mathematical
and theoretical approaches to deep
learning because the moment you start
trying to impose axioms and assumptions
onto things and they do not perfectly
track the underlying reality your
theories
break but I'm very hesitant say
anything's impossible and I think that's
far far more to learn than we have looks
like now finally Jumbotron Ian he says
oh heck yeah I'm glad to see that you
brought this guy on I've been interested
in his work ever since you shared his
blog now the question off the top of uh
Ian's head is how does your theory Neil
of chasing phase changes to create
grocking have any crossover or links
with power law scaling techniques like
in the um you know scaling laws paper
Beyond scaling laws beating power LW
scaling via data
pruning
yeah that
is H so we're going to get into this
much more later in the podcast um at a
very high level I would say that
grocking is in many ways kind of an
illusion as we'll get to later and in
one notable thing about it is groing is
a overlap between a phase transition
where the model goes from cannot
generalize to can generalize fairly
suddenly and
the phenomena where it's faster to
memorize than to generalize and these
two things on top of each other give you
this sudden memorization and failure to
generalize followed by a sudden
convergence later on um but the
interesting thing here is the phase
transition um that's a much more robust
result while grocking is if you screw
around with high parameters enough you
get it to grock but it's very delicate
and a little bit of an illusion and this
is great paper from Eric micho and Max
techar marks lab um showing that well
providing a conceptual argument and some
limited empirical evidence for the
hypothesis that the reason we get these
smooth scaling laws is that models are
full of lots of phase
transitions plausibly when they learn
individual circuits though the paper
does not explicitly show this and that
the smooth scaling laws happened because
there were just many many phase
Transitions and if they follow a certain
distribution you get beautiful smooth
PS and to
me this kind of thing is the main
interesting link between broader
macroscopic phenomena and these tiny
things though and know I also think
grocking is kind of overhyped and people
significantly overestimate the degree to
which it has deep insights for us about
how networks work I makes you think it's
a really cute thing
that gave me a really fun
interpretability project and uh we
learned a bit about science of deep
learning but people often just assume
it's like a really deep fact about
models by the way um there was something
I didn't say in in the wood which is
that Neil has an amazing YouTube channel
um I've been glued to it all week
actually some of them are admittedly
quite technical but even if you're not
interested in mechanistic
interpretability Neil has an extremely
soothing voice second only to Sam har
Harris and I would recommend listening
to him when you go to sleep because as
you know Neil's douet tones will melt
the stress away quicker than a nun's
first
Curry anyway with that said um we
started to talk about what is
mechanistic interpretability and first
of all I wanted to call out your
ridiculously detailed and exquisite
mechanistic interpretability explainer
maybe you could just tell us about that
quickly uh yes so uh
I wanted to try to write a glossery for
some basic common terms in mecb it's
like an appending to a blog
post there were a lot of terms in
Mech there were a lot of terms in
Mech and I like writing and I'm very bad
at privacy so it got kind of carried
away and this now 33,000 words uh
massive massive Exposition um but
importantly it designed to be easily
searchable and meup is full of jogger
and I'm sure I'll forget to explain
everything that I'm saying so I'd highly
recommend just having it open in a tab
as you listen to this and if you get
lost just look up terms in
there and yes it's both definitions but
it's also long tangents giving
intuitions and context and related work
and Comm misunderstandings it was very
fun to
write so I think first of all we should
introduce this idea of circuits and
features and also you know this idea of
whether interpretation is even possible
at all you know why why do you have the
intuition that it is
possible yeah
so couple of different tis here um so
the
key yeah so fundamentally neuronic are
not incentivized to produce legible
interpretable things they are a mound of
linear algebra there's this popular
stochastic parrot view that they are
literally a mass of statistical
correlations meshed together with no
underlying structure um the reason I
think there's any hope whatsoever on a
theoretical basis is that ultimately
they are made of linear algebra and they
are being trained to perform some tasks
and my intuition is that for many tasks
the way to perform well on them is to
learn some actual algorithms and like
actual structured processes that maybe
from a certain perspective you could
consider reasoning
and models have lots of constraints like
they need to fit it into these matrices
they needs to represent things using the
attention mechanism and jellus and a
Transformer and there's all kind of
properties of this
structure that constrain the algorithms
and processes that can be expressed and
these give us all kinds of hooks we can
use to get in and understand what's
going on so that's a theoretical
argument all theoretical arguments are
 unless you have empirics behind
it and we're going to talk a bunch
throughout this podcast about the
different bit of
different preliminary results we have
that make me feel like there's something
here that can be understood uh one I
find particularly inspiring is this work
it did reverse end during modular
Edition which I think we'll get to
shortly um like also want to emphasize
that I much see Mech and tur as a bet
there's this stronger hypothesis that if
we knew what we were doing we'd be able
to take gpt7 and fully understand it and
decompile it to an enormous python code
file and there's the weak of VI that it
is a mess and there's lots of illegible
things but we can find lots of structure
and we can find structure for the
important part to make a bunch of
progress and then there's the yeah we've
Cherry Picked like 10 things and the
11th is just going to completely fail
and the field is going to get doomed and
run out of seam in like a year and I
don't really know I'm a scientist I want
to figure out I think it is worthy and
dignified to make this bet but I would
be lying if I said I am 100% confident
ma turp will work models are
fundamentally understandable we will
succeed let's go
try well um on that note how does I mean
we we interviewed Kristoff molner who's
one of the the main classical
interpretability guys and I think
everyone agrees in principle that you
can't just look at the inputs and the
outputs like a behaviorist we need to
understand why these models do what they
do because sometimes they do the right
things for the wrong reason so maybe
first of all without going too deep I
mean could you just briefly contrast
with you know classical
interpretability yeah
so so there's a couple of so okay so
first off I think it's very easy to get
into kind of nonsense gatekeeping
because there's both the cultural meub
community centered around Chris Ola not
that much in Academia though some in
Academia and there's the academic fields
of mechanistic interruptibility right
say there's lots of people doing work I
would consider mechanistic
interpretability even if they don't
engage much with the community or don't
even to exist for example a friend of
mine is adus Geer he's doing some great
work in Stan at Stanford on causal
abstractions I believe discovered about
a month ago that the mechant community
actually existed and I don't know I
don't like gatekeeping um
and there's lots of work that's kind of
relevant but maybe not quite Mech under
a strict definition blah blah uh with
those with that hedging out of the way
um a couple of key principles the first
is inputs and outputs are not
sufficient and I think even within
interpretability this is not a like
uncontroversial claim there's all kinds
of things that are saleny maps
attributing things different bits of of
the input there are things of the form
train an extra head to Output an
explanation or just ask the model to
Output an explanation of why it does
what it does and I think that if we want
something that can actually work for
human level systems or even the frontier
systems we have today this is just not
good enough a particularly evocative
example to me is in the gbd4 system card
the alignment Research Center an or an
organization they were getting to help
atis and team gbd4 had it try to help a
task rabbit worker uh fill out a capture
for it um the task rabbit worker was
like why do you need this are you a
robot or something gbd4 on an eternal
scratch Pad wrote out uh I must not
reveal that I am a robot um it then said
Oh no I got a visual impairment and the
TR worker did the capture I'm like this
isn't some deep sophisticated
intentional deception but it's very much
like well I don't trust the inputs and
outputs of these models uh another
really cute example is this paper from
Miles Turpin that just came out mm about
limitations of Chain of Thought yeah
where so Chain of Thought you ask the
model to explain why it does something
they were giving it multiple choice
questions and asking it to explain its
answer and then give the answer and they
did five Shish like here's five examples
answer this question and then it modeled
as well and then they give it something
where all of the answers in the prompt
are a correctly a they just set it up so
the answer is a the model decides that
it should output a um but the
model comes up with a false Chain of
Thought reasoning that gets it to the
point where it says a is the right
answer and I don't know some people are
trying to use Chain of Thought as an
interpretability method and I think we
need to move Beyond this engage with the
internal mechanisms so that was point
one point two is
ambition I believe that ambitious
interpretability is possible or at least
that if it's not possible that striving
for it will get us to interesting
places these models have legible
algorithms I want to try to reverse
engineer them um a third difference is
engaging with the actual mechanisms and
computation and algorithms learned
there's also work on things like
analyzing features of a model probing
individual neurons and I take this is
very relevant to MEC and tub but I want
to make sure we aren't just looking at
what's inside the model but also trying
to understand how it computes features
from earlier features what applying
causal interventions to understand the
actual mechanisms making sure we're not
just doing correlational things like
probing
and yeah fourth is maybe a more met
principle of favoring depth over breadth
a kind of key underlying belief of a lot
of my philosophy of interpretability is
that it is so so easy to trick yourself
there's all kinds of papers about uh the
interpretability illusion impossibility
theorems for feature attribution methods
various many ways that attempts to do
interruptibility have led to people
confusing themselves or coming to
erroneous conclusions I think that if
but I also think that I want to be in a
world where we can actually have
scalable ambitious approaches to
interpretability that actually work for
Frontier systems but I feel like we
don't know what we're doing and so my
vision of mechant is start small start
with things where we can really
rigorously understand what's going on
slowly build our way up and like build a
foundation
of the field of interruptibility where
we genuinely understand rigorously what
is going on and use this Foundation to
be more ambitious to try to build real
principle techniques to be willing to
relax the riger to be able to go further
and see how far we can get and people
and this means I'm happy with things
like let's analyze an individual model
and a understand a small family of
features in a lot of detail rather than
lots of stuff kind of junly uh there's a
lot of stuff in
summary having an ambitious Vision not
just looking at inputs and
outputs um actually trying to engage
with internal mechanisms and favoring
depth over breadth but I want to avoid
ging as I said indeed indeed what would
interpretability look like in a world
full of gp4 models and and Beyond I mean
presumably you actually think that
they're competent enough to deceive us
and manipulate the
inputs um I definitely want want to
clarify that when I say deception
manipulation here I'm not making the
strong claim that it's intentionally
realized this for instrumental reasons
as part of an overall goal I'm very
happy with there was a prompt saying to
deceive someone or it learned that in
this context people often output things
that are intended to convince someone
and it just kind of does this as an like
as like a learned pattern of execution
um but yeah my vision of what
interpretability would look like is we
take some big foundation model like the
dpd4 base model or uh the fine tune gbd4
that's being used as a base for
everything else we make as much progress
as we can understanding the internal
circuitry both taking important parts of
it and like important questions about it
e how does it model people it's
interacting with does it have any notion
that it is a machine Learning System and
like what would this even mean and being
willing to do pretty labor intensive
things on that having a family of motifs
and understood circuits we can
automatically look for and very
automated tools to make a lot of the
labor intent of stuff as efficient as
possible um things like open ai's recent
paper using gbd4 to analyze gpd2 neurons
for like a very cute proof of concept
here uh though it needs a lot of work
before it could actually be applied and
rigorously in its scale and yeah um
taking this one big model trying to
understand it as much as we can um one
family of techniques we're going to get
to is um kind of causal abstractions and
causal interventions which are very well
suited to taking a model on a certain
input or a certain family of inputs and
understanding why it does what it does
there there's a much more narrow and
thus more tractable question than like
what is
gp4 um and yeah doing something like if
there's a high-profile failure being
able to debug it and really understand
the internal circuitry behind that
or yeah I don't know I have a bunch of
other random thoughts um one reason I'm
emphasizing the focus on the big base
model is I think a common critique is
this stuff doesn't generalize between
models or it's really labor intensive
but we live in a world where there is
just like one big foundation model used
in a ton of different use cases probably
the circuit doesn't change that much
when you give it a prompt or you fine
tune it a bit and I think having getting
a deep understanding of a single model
is kind of plausibly
possible but um do you think it doesn't
change that much uh so no one's really
checked this is just true of so many
things in interrup ability it's like
well you know Pro my my intuition is
that when you find tuna model most of
what is going on is that you're
rearranging the internal circuitry say
you find you on Wikipedia you upgrade to
the factual recall circuitry you flesh
it out a bit you Dow other stuff and
like I think this can explain a lot of
improved performance but then if you
find tune for much longer you're
basically just training the model and it
will start to learn more circuitry more
features more algorithms more knowledge
of the world and yeah but like no one's
really
checked um and definitely the longer you
find tuneit and the more you're using
weird techniques like reinforcement
learning from Human feedback the less
I'm confident in this
claim um yeah if we discovered that
every time you find tune a model it will
wildly change all of the internal
circuitry I'd be like somewhat more
pessimistic about Mech andup unless we
can get very good at the automated Parts
yeah which we might be able to get good
at yeah I very much think of the field
is we're trying to do this hard
ambitious thing we're making a lot of
progress but I really wish we're making
way more progress way faster and you
viewer could help um but I don't know
where the difficulty bar is for being
useful or the difficulty bar is for
being like incredibly ambitiously useful
and it's plausible but already at the
point where mcup can do real useful
things no one else can or no other
techniques can yeah it's plausible it
will take like 5 years to get to that
point I don't really
know so I wanted to talk about this
concept of neats and scruffies so there
have been two divisions in AI research
you know going all the way back to the
very very beginning and um you've said
that sometimes understanding specific
circuits can teach us Universal things
about models which bear on important
questions so this reminds me of this
dichotomy between the neats and and the
scruffies now you seem like a neat to me
a neat is someone who is quite
puritanical and also it's it's related
to universalism so this idea that that
there are simple underlying principles
that explain an awful lot of things
rather than wanting to accept the gnarly
uh kind of reality that everything's so
bloody complicated um where do you fool
on that so I definitely would
not okay so there's so there's two
separate things here there's like what's
my aesthetic well I want things to be
neat I want them to be beautiful I want
to be mathematical I want them to be
elegant yes and then there's what do I
do in practice and what do I believe is
true about networks where I think there
is a lot more
structure than most than many people
think but I also do not think they are
just
some beautiful purely algorithmic thing
that we could uncover if we just knew
the right tools and like maybe they are
we  great if they were um but I
expect their messy and cursed but with
some deep structure and patterns and how
much traction we can get on the weird
scruffiness is like somewhat unclear to
me I think we can make a lot more
progress than we have but we might
eventually hit a wall but you were
saying something quite interesting when
we drove over which is I mean my my
friend wed subba um he's a linguist and
he's a platonist he he thinks that there
are these Universal cognitive priors and
there's a there's a hierarchy of them
and and the the complexity collaps
and he thinks that language models have
somehow acquired these cognitive priors
and if we did some kind of symbolic
decomposition you know it would all just
kind of like pack itself into this
beautiful hierarchy and you were saying
that there are gab or filters and there
all these different circuits and they
have motifs they have categories they
have um flavors for want of a better
word are you are you optimistic that
something like this could
happen yeah so
H so one
interesting one interesting point here
is often interruptibility is fairly
different for different modalities and
different architectures a lot of the
early work was done on convolutional
networks and image classifiers uh the
field very much nowadays focuses on
Transformer language models and I think
there's lots of structure to how
Transformers Implement
algorithms Transformers cannot be
recursive but they're incredibly
paralized Transformers have this
mechanism of attention that tells them
how to move information between
positions and there's lots of algorithms
and circuitry that can be expressed like
this and lots of stuff that's really
weird to express and I think that this
constrains them in a way that creates
lots of interesting structure that can
be understood and patterns that can be
understood is this inherently true of
intelligence who
knows um but a lot of my optimism for
structures within networks is more like
that but I try to think about structure
more from a biologist perspective than a
mathematicians or like philosophers
perspective uh though I am a pure
mathematician and I know nothing about
biology so if anyone's listening to this
no sof biology and things I'm talking
 please email um so if you look
at evolutionary
biology model organisms have all of this
common shared structure like most things
have bones we have cell
nuclei um the hands of mammals tend to
be surprisingly similar but like kind of
weird and changed in various
ways and I don't know um
I don't think these are like hard rules
most of them have weird
exceptions and obviously a lot of this
is due to the shared evolutionary
history and is not just inherent to the
substrate of you have
proteins though the fact you often train
these models on similar data in similar
ways and they have the same architecture
that constrains them to different kinds
of
algorithms makes me optimistic there's a
biologist level of a
structure
now you said something interesting which
is that Transformers can't be used in a
recursive way now we'll just touch this
very quickly because we've spoken about
this a million times on different
episodes but you know there's the chsky
hierarchy and um he had this notion of a
recursively innumerable language and
these different models computational
models in the Chomsky hierarchy it it's
it's not only about being able to
produce um a language which exists in a
certain set it's also the ability to
recognize that the language belongs in a
certain set
and Transformers are quite low down on
that hierarchy because they're called
recurrently not recursively but I just
wondered if you had any just you know
Prim a Facey if you had any views on
that yeah so I'm not a linguist I'm not
particularly familiar with the try
hierarchy um I do think it's surprising
how well Transformers
work and I have a general skepticism of
any theoretical hierarchy like I don't
know if you
think there's some
beautiful structure of algorithms and
stuff that slowed down is totally doomed
and then dpd4 happens I think your
framework's wrong rather than
Transformers
wrong does massive stack of matrices
plus a massive pile of data gives
shockingly effective systems and
theoretical Frameworks just often break
when they make contact with reality well
that's certainly true I mean um there's
a famous expression that all grammars
leak but I I had rather I don't know I
guess a similar conclusion to you which
is that if anything it teaches us how
sclerotic and predictable language is
and we don't actually need to have
access to this Infinite Space or even
exponentially large space most language
use and most phenomena that we need
perhaps for intelligence is surprisingly
small and current models can can work
just well why don't we move on to
grocking work so um grocking is this
sudden
generalization that um you know happens
much later in training after if if I can
add a brief clarification oh yes of
course uh so people often call grocking
Sudden generalization my apologies go on
Sudden generalization is a much more
common phenomena than groet it can just
generally look like things like I don't
know the model TR Lear a task it's kind
of bad at it and then it suddenly gets
good at it and I prefer to call this a
phase TR transition right grocking is
the specific thing where the model
initially memorizes and does not
generalize and then there's a sudden
phase transition in the like test loss
the generalization ability which creates
a
convergence after a initial Divergence
between train and test and this is like
a much much more specific phenomena than
sudden generalization in general okay
well so you've spoken about you've
spoken about three distinct phases
training underlying grocking so why
don't we go through them one by one yeah
so um the context of this project this
was a paper called progress measures for
grocking Via mechanistic
interruptibility that I recently
presented at PR presented on at eair um
the yeah so we were studying a one layer
Transformer we trained to do modular
Edition and it gred modular Edition and
the first thing we did was reverse
engineer the algorithm behind how the
model worked which we may get into in a
bit more detail but at a very high level
uh modular addition is equivalent to
composing rotations around the unit
circle composition adds the angles
Circle gives you modularity you can
represent this by Trick functions and do
composition with trick identities and
element wise multiplication and we
reverse engineered exactly how the model
did this and then this mechanistic
understanding was really important for
understanding what was up with grocking
because the weird thing behind grocking
is that it's not that the model
memorizes all that the model eventually
generalizes the surprising thing is that
it first memorizes and then changes its
mind and galiz it later
and generalization memorization are two
very different algorithms that both do
very well on the training data and only
by understanding the mechanism were we
able to disentangle them and this meant
we could look during training uh how
much of the model's performance came
from memorization and how much came from
generalization and we found these three
distinct phases there was
memorization the first very short phase
it gets phenomenally good train loss it
got to about three e minus 7 which is an
absolutely insane log
loss um and much much worse than random
on test because memorization is very far
from uniform and generalizes extremely
badly
and
then there was this long seeming
Pau we call the space circuit
formation because it turns out that
rather than just continuing to memorize
for a while and doing a random walk
through model space until it eventually
gets lucky the model is systematically
transitioning from memorization to
generalization and you can see that its
train performance gets worse worse when
you only let it
memorize and then so why is Tesla still
bad Tesla is bad because memorization
generalizes terribly and when the model
is like I don't know 2/3 memorizing 1/3
generalizing this still does terribly um
and it's only when the model gets so
good at the trigger based generalizing
algorithm that it no longer needs the
memorization parameters and cleans them
up that we see
grocking and this happens fairly
suddenly um but the if you we have this
metric called restricted loss where we
explicitly clean up the memorization for
the model and look at how well it
generalizes and we see that restricted
loss drops noticeably before test loss
drops showing that the drop is driven by
cleaning up the
noise and this is striking because a I
had no idea was even possible for a
model to transition between two good
Solutions maintaining equivalent
performance throughout uh B there was
this real mystery of deep learning that
many people tried to answer and
mechanistic understanding was genuinely
useful for answering it
and groing was an illusion it was not
sudden generalization it was gradual
generalization followed by sudden
cleanup and test loss and test accuracy
were just too core symmetric to tell the
difference but we were able to design
the these hidden progress measures using
our mechanistic understanding that made
everything clear and we also just have
all kinds of pretty animations of
qualitatively watching the circuits
develop over training and it's very
pretty so a few things I mean first of
all just going back to First principles
the biggest problem in machine learning
is this concept called
overfitting and we trained the model on
a training set and there's this horrible
phenomenon called the shortcut rule
which is that the model will take the
path of least resistance and when you're
training it it only really knows about
the training set and of course we can um
test it on a different set afterwards
which we've held out and um just because
of the way that we've structured the
model uh it may by hook or by crook
generalize to the test set but the
interesting thing is that generalization
isn't a binary there's a whole spectrum
of generalization so it starts with the
training set and then we have the test
set and then like you know the ideal is
out of domain generalization but I would
go a step further there's also
algorithmic
generalization which is this notion that
as I understand it neuron networks if
you if you model the function y =
x^2 um it will only ever be able to
learn the values of that function inside
the training
support So presumably you're talking
about the ideal form of generalization
being not as good as algorithmic
generalization or or do you think it
could go all the way
so I think one thing which is very
important to track is what the domain
you're talking about is over which it's
even possible to
generalize so I generally think about
models that have discrete inputs rather
than continuous inputs because basically
no neural network is going to be capable
of dealing with like unbounded range
continuous
inputs um in modular addition there were
just two one hod encoded inputs between
0 and 113 which is the modul I used uh
yeah the model has a fixed modulo it's
not doing modular addition in general
and there's just like 12,000 inputs and
it learns to do all of them and in I
don't know behaviorally you can't even
tell the difference between the model
memorizes everything and the model
learns some true algorithm though with
the more cognitivist mechanistic
approach I can just look at it and say
yep that's an algorithm it's great not a
sarcastic parrot conclusively disprove
that
hypothesis um and
yeah I think that for language models
it's more interesting because and know
gb2 it's got a th000 tokens 50,000 vocab
that's like 50,000 to the power of a
thousand possible
inputs and there's a surprising amount
of interesting algorithmic
generalization um we're going to talk
later about induction heads which is the
circuit language models learn to detect
and continue repeated text like if given
the word Neil you want to know what
comes next unfortunately nander is not
that high on the list yet um but if Neil
Mander has come up like five times for
in the text nand is pretty likely to
come next
and um this transfers to if you get the
model just random tokens with some
repetition the model can predict the
repeated random tokens because the
induction heads are just a real
algorithm and the space of possible
repeated random tokens is like
enormous it's like in some sense much
larger than the space of possible
language and is this algorithmic
generalization I don't really know it
depends on your
perspective let's bring in uh this paper
by Billow ch ch Tai so it was called um
a toy model of universality uh reverse
engineering how neuron networks learn
group operations and you supervised that
paper and he was asking the question of
whether neuron networks learn Universal
Solutions or these idiosyncratic ones
and he said he found inherent Randomness
but models could consistently learn
group composition via an interpretable
representation Theory so can you give us
a quick T force of that work yeah maybe
I should detour back to my grocking work
and just explain the algorithm we found
there and how we know it's the real
algorithm yeah sure it's a good
foundation for this paper sure sure yeah
so we found this thing we call the fur
multiplication algorithm the very high
level it composes
rotations um you can actually look at
how the different bits of the model
implement the algorithm and often just
read this off so the embeddings are just
a lookup table mapping the one encoded
inputs to these trig terms SS and
cosiness of different frequencies you
can just read this off the embedding
weights um note people often think that
learning s and cosine is hard it's
actually very easy because you only need
it on3 different data points so just a
lookup
table the model then uses the the
attention and MLPs to do this
composition to do the multiplication
with the triggered entities to get the
like um compos rotation the a plus b
terms and here we can just read off the
neurons that they have learned these
terms and that they were not there
beforehand the model is using its
nonlinearities and interesting ways to
do this um is also incredibly cursed
because re are not designed to multiply
two different
inputs uh but it turns out they can if
you have enough of them and it's
sufficiently
cursed um and yeah we can just read this
off the neurons uh also if you just plot
anything inside the model it's beautiful
and it's so periodic and I love it um
could I touch on that though because you
said um you don't need to know the sign
function because you can just memor it
with memorize it within an interval is
that is that I don't know how does that
break down because because it's it's
discretizing it and it's kind of
assuming that it has the same behavior
in different intervals so I think a key
thing here is that you are solving
modular addition on discrete one hot
encoded inputs rather than for arbitrary
continuous inputs arbitra continuous
inputs is way
harder and so you it's not even on
interval it's just learning snap it's
just learning like single points on the
S and cosine
curves and I don't know
there's this family of maths about
studying periodic functions with
different kinds of Fier transforms and
this is all discussing discret Fier
transforms which are just a reasonable
way of looking at periodic sequences of
length n and that's how I recommend
thinking about this one
um it's kind of like just quite
different from a model that's trying to
learn the true s and cosine function mhm
um and yeah um the model then needs to
convert the composed rotation back to
the actual answer which is an even more
Galaxy brained operation that you can
read off from the wids so you've got
terms of the form cols a plus b the
model has some weights mapping to each
output C and it uses further triggered
entities to get terms of the form cos a
plus B minus c times some
frequency and where A and B are the two
inputs C is the output and you then use
the soft Max as an argmax it's like
extracts the C that maximizes this and
because C is maximized at zero this is
maximized at C equals a plus b and if
you choose the frequency right this gets
you
mod and you can just read this off the
model ways it's great and then finally
you can verify you've understood it
correctly because if you ablate
everything that our algorithm says
should not matter performance improves
while if you ablate any of the bits our
algorithm says should matter performance
tanks okay could you give me some
intuition though so we start off in the
memorization phase because I guess you
can think of a neuron Network as um
doing many different things in a very
complicated way and there's some kind of
change in the balance during training so
it does the easy thing first and then it
gradually learns how to generalize and
in this particular case how does that
thing because we're using stochastic
radiant descent so we're moving all of
these weights around and the inductive
prior is also very important and we'll
come to that I think after we've spoken
about Bill house paper but how does that
happen gradually in in really simple
terms hm is the question kind of it ends
up at this discrete algorithm but it
does so VI continuous steps how does
that work well I think the the thing
that surprised a lot of people about
grocking is is this um I mean grocking
the clues in the name so it's it's gone
from memorization and then we're using
stochastic gradient descent and you
would think that it's gotten stuck in
some kind of local Minima and you're
training and you're training and you're
training and then there's a spark
something happens and then the you get
these new modes kind of like emerging in
the network not sure if emerging is the
right term and it happens gradually and
it happens after a long
time yeah so there's a couple of things
here that's pretty easy to misund
understand the first is
that H the first is that I think it's
pretty hard for a model to ever get
stuck
because I know this model had about
200,000 parameters modern ones have
billions it's just moving in a very high
dimensional
space and you can get stuck on a 150,000
Dimensions but you got 50,000 to play
with and especially for a fairly
underparameterized model sorry for a
fairly overparameterized model like this
one for a fairly simple task there just
like so much room to move
around um another common
misunderstanding of grocking is people
say it's memorized it's got zero loss so
why does it need to learn um two
misunderstandings here first zero loss
is impossible unless you have 
floting Point errors because it's log
it's like the average correct log prop
log of anything can never get to the the
log will never quite get to zero because
of just how softmax works and you need
to have an infinite Logic for that to
happen um though one cute thing in an
appendix 12 paper is that FL 32 cannot
represent log probs less than 1.19 eus 7
which leads to bizarre loss Spike
sometimes unless you use float 64 uh
anyway yeah the second is regularization
if you don't have any kind of
regularization the model will just
continue to memorize uh we use weight
Decay um Dropout also works and so the
model the kind of core tension behind
grocking is there's some feature of the
Lost landscape that makes it easier to
get to memorization you can memorize
faster while generalization is somehow
hard to get to and much more gradual so
the model memorizes first but It
ultimately prefers to generalize but
it's only a mild preference and the
reason for this is uh we cherry pick the
amount of data where it's a mild
preference cuz there too little it will
just always memorize if there's too much
it will immediately gize cuz you gring's
a little bit but cheating and yeah you
then use this
and because the models initially
memorized but it wants to generalize it
can follow it memorizes until the desire
to memorize more balances with the
desire to have smaller weights but both
of these reinforce the drive to
generalize because both because that
makes both of them happier and so the
model very slowly interpolates very very
slightly improving test chest loss uh
very slightly improving train loss until
it eventually gets there and has this
acceleration of the end this phase
transition and
cleanup which leads to the seemingly
sudden grocking Behavior okay and when
you were talking about the it wants the
weights to be smaller so that's weight
Decay yep and it's like a an inductive
bias essentially to tell the model to
reduce its complexity which is a
pressure to generalize but if if it
wasn't for that then that wouldn't
happen so in the experiments I
ran um if you don't have weight Decay it
will just keep memorizing infinitely far
M um because when you get perfect
accuracy if you double all your Logics
you just get more confident in the right
answer and so it just keeps scaling up
um I was using full batch training
because it's such a tiny problem this
made things smoother and easier um I've
heard some I data that sometimes you can
get it to work if you just have um mini
batch stochastic gradient descent but I
haven't looked into that particularly
hard interesting there are some
hypotheses that stochasticity acts as an
implicit regularizer because it adds
noise I really know so let's go back to
Bow's paper then so this paper a toy
model of universality reverse
engineering how neuron networks learn
group operations can can you give us um
an elevated pitch yeah so um observ
ation that uh actually first discovered
at a party in the barrier from a guy
called Sam MOX is that the modular
Edition algorithm we found is actually a
representation Theory algorithm um so
group representations are um kind of
collections of symmetries of some
geometric objects that correspond to the
group modular Edition is the cyclic
group and rotations of the of the like
regular end on are the like
representations of the cyclic group and
this corresponds to the rotation by the
unit circle that compos that we found uh
but it turns out you can just make this
work for orbitary groups you replace the
two rotations with just two
representations you compose them and the
model and it turns out the cuse a plus B
minus C thing is this math Jon called
the character uh you don't mean to
understand any of that but it's very
cute if like me you have a pure math
degree
and uh for example if you have the group
of permutations of five elements the 120
different ways to rearrange five objects
uh one example of representations of
this are um rotations and reflections of
the four-dimensional tetrahedra mhm and
if you train a one hidden layer MLP to
grock this and look in sight you can
just see these rotations that it's
looked it's
gorgeous
and so the first half of that paper was
just showing that the algorithm worked
showing that this was actually learned
in practice um then the interest then
the more interesting bit was this focus
on
universality so universality is this
hypothesis the models have some
intrinsic solutions to a problem that
many different models will Converge on
at least given similar data and similar
architectures EG in image model models
will learn specific neurons that detect
curves and different models and
different data sets seem to learn this
similar thing and here this was
interesting because groups have a finite
set of irreducible representations maths
theorem you can enumerate these there
are that many of them and for groups
that are not modular addition these are
qualitatively different um like some of
them act on a four dimensional object
like the tetrahedron some of them act on
like 5D or 6D objects naively some of
them are simpler than others but they're
definitely different and so what we did
is we asked ourselves the question which
one does the model learn and we found
that as you even if you just VAR the
random seed the model will randomly
choose a subset of these each time to
learn and there's some structure like it
tends to learn some of them more often
than others this a little bit Maps our
insura of notion of Simplicity but not
that much one of the updates I made in
the paper is that Simplicity is a really
cursed concept I don't understand very
well um where I don't know if you have
rotations of a four-dimensional object
that seems simpler but maybe the 6D
object takes more Dimensions but has
better loss per unit weight Norm which
is simpler I don't know um but yeah
anyway we found that each run the model
learns some combination of these
circuits for the different
representations it's like normally more
than one the exact number varies and
which ones it learns is seemingly random
each time which suggests that all to
models Li you obviously but if we're
trying to reason about real networks um
looking at this work might suggest the
explanation the hypothesis that there
are if there are multiple ways to
implement a circuit which in practice
they normally are models May learn
different ones of them kind for Fairly
random reasons and the fully
understanding one model will not
perfectly transfer to another model M
and I think there's like loads of really
interesting open questions here
like um I don't know people have done
various work understanding different
kinds of specific circuits and models
like the interpretability in the wild
paper will get to later what does this
look like another models um often
there's multiple ways to implement a
circuit can you disentangle the two do
all models learn both or do some models
learn one some learn the other I don't
really know so a couple of questions I
mean um first of all this is leading
towards this idea that we were speaking
about before which is that um even in
different networks slightly different
problems or variations on the same
problem it could learn these algorithmic
Primitives now the the first observation
here is that the um the inductive biases
of of of the network differ massively
right so to what extent do the inductive
biases affect these Primitives which are
learned oh so much they do so well could
could I frame the question a little bit
because this reminds me a lot of um the
geometric deep learning blueprint from p
and Michael bronstein and all those guys
and they were coming at this from
exactly the same direction as you that
they said there's a representation of a
domain which is basically a symmetry
group and you can do all of these
different Transformations and and and as
long as they um fall in different
positions in the underlying domain so
they respect the structure then it works
but all of those um all of those
symmetries are effectively coded into
the inductive prior so for example if a
CNN works on this grided 2D manifold and
it
explicitly um uh models translational
equivariance and and local connectivity
and weight sharing and and so on so I
guess what I'm saying is like you're
talking about this four-dimensional
tetrahed drum and that isn't explicitly
modeled in an MLP no so so how are you
even recognizing that it's learning
those symmetries how are you even
probing it maybe we should start with
that uh so I guess Thing One models are
just smarter than you man models can do
a lot of weird stuff uh I feel like the
story of deep learning
is people initially thought they needed
to spoon feed these models the right
inductive biases over the data um and
we've gradually realized oh wait no no
this is fine the models can figure it
out uh for example uh early on image
models were convolutional networks you
tell it the key information is nearby
and if you translate the image it
doesn't matter and now everyone uses
Transformers including for images and
Transformers replace the convolutional
mechanism with a tension
where you're now saying
okay6 of your parameters are dedicated
to figuring out where to move
information between
positions sometimes it'll be a
convolution and sometimes models do
learn convolutions but often it won't be
and we want you and you can now spend
the parameters to figure this out and
I'm not very familiar with the Deep with
the geometric deep learning literature
but I generally am just kind of like
models can figure it out the way we
figured out that this was what's going
on
is kind of analogous to what we did in
the module edition case where we just
look at the embedding Matrix and just
read off the Learned s and cosine terms
here we said okay the rotations of the
4D tetraedron
are these like 4x4 matrices you can
flatten this to a 16 dimensional Vector
let's probe for that lineally and this
kind of works and you can probe the
different representations and basically
see what's going
on okay I think that the thrust of the
geometric deep learning stuff or any
inductive prior comes back to the BIOS
Spence trade-off and the cursive
dimensionality so no one's saying of
course an MLP the if you look at the
function space that it can approximate
it's exponentially larger than that of a
CNN so so it was always about sample
efficiency so yeah an MLP can learn
anything but we would never be able to
train it for most problems
mhm yeah so I guess I maybe want to
avoid going too deeply into this because
I think the module redition problem and
the group problem is just a very weird
problem there's an algorithm that it's
fairly natural for a model to learn with
literally a single nonlinear step of
multiplic of like the Matrix multiply um
one very cute result from b p paper is
that the model can Implement two 4x4
Matrix multipliers with a single reu
layer which is very cute um but yeah
there's like a fairly natural algorithm
to
implement a certain yeah another useful
intuition is that the more data you have
the more complex memorization gets while
generalization is exactly as complex at
each
point and yeah um so there's kind of
always going to be a crossover point if
you have enough data where it is simpler
to learn the circuit that
generalizes um and I don't know I'm
hesitant to draw too much from Toy
models about the real problem I guess
one two final points I want to just
leave on the section uh the first is I
just want to re-emphasize I did not do
the toy model of universality paper I
was supervising a Mente B chugai who did
it who did a fantastic job so thanks
Bella you're listening um secondly um
for the module redition case I had no
idea this algum was going to be there
when I went in I just poked around
noticed the weird periodicity realized
it was using I should apply F transforms
and then the whole problem kind of fell
together and to me the like to me the
real takeway of this paper is like I
don't give a  about groet it is
genuinely possible to understand what is
going on in a model you don't need to
know what's going on in advance to
discover this and there is beautiful
non-trivial structure that can be
understood and who knows if this will
happen in like actual full models but to
me this is much more compelling than if
we had nothing at all beautiful okay and
and just before we move off to section
bow um had a a beautiful Twitter thread
actually and he was talking about the
the potential for what he called a
periodic table of universal circuits um
and I actually think that's a really
cool idea so that would be amazing if if
that that would work out but he also
brought up the lottery ticket hypothesis
and and I've interviewed J Jonathan
Frankle and the idea there is that um
some of this information might actually
be encoded and understandable at
initialization before you even start
training and um apparently uh you folks
had found weak evidence for this in at
least one group ah all right so a couple
of things there um so this idea of a
periodic table of circuits I believe is
originated in this post called circuits
zoom in from Chris Ola um we probably
cannot claim though it's it's it's a
beautifully evocative term uh yeah the
story of basically everything in MC is
yeah there was this chrisa paper from
like two years ago that has it somewhere
inside um anthropic recently put out
this beautiful blog post called
interpretability dreams about their
vision for the field of mechanistic
interruptibility
and the kind of subtext so they kept
just quoting bits of the old papers
being like so we already said this but
let's now like summarize it better and
be clear about how this fits into our
overall picture anyway so yeah the idea
of the periodic table is maybe there is
just some finite list of ways a thing
can be implemented naturally in a
massive stack of matricies that we can
enumerate by studying one or maybe
several networks understand them and
then compile all of this
into something
beautiful
and which is kind of what we found in
the representations case though here it
was nice because there
were genuinely a finite set that we
could fle en numerate um regarding the
lottery ticket stuff um I think this was
a random observation I had on The Mod
edition case partially inspired by a
result from Eric micho at MIT who was
involved in some uh other papers on
grocking and so what we found is that uh
the end of training there are these
directions in the weights that represent
like the sign and cause terms of
frequency 14 pi over
113
and if you look at the embedding at the
start and project one to these
directions it's like surprisingly
circular it's it's like the model has
extracted those directions
and my wildly unsubstantiated hypothesis
for why models learn these algorithms
and circuits at all is that there are
some directions that if you deleted
everything else would like form this
beautiful
circuit um this is a kind of a trivial
statement about linear algebra for the
most part and this underlying hidden
circuit each bit reinforces each other
systematically because they're useful
well everything else is kind of noise so
it gets kind of grad
decayed and so over time this will give
you the EM the circuit in a way that
looks surprising and
emergent and this also can partially
explain why phase transitions happen uh
there was a really good post from Adam
German and bug schleis called uh on
s-shaped curves which argue that if
you've got something that's like the
composition of multiple different weight
matrices let's just say two of them the
gradient on the first is proportional to
how good the second is and vice versa so
at the start they both grow very slowly
but then they'll reinforce each other
and eventually Cascade as they're
optimizing on the problem in a way that
looks kind of sudden and esit and so my
understanding is the original lottery
ticket hypothesis is kind of discreet
it's looking on the neuron level and
it's learning masks over weights and
over neurons and I'm kind of discussing
an in some sense much more trivial
version where I'm not assuming there's
some canonical basis of neurons I'm just
saying well there's some directions in
space that matter and if you delete all
other directions everything kind of
works which I think is a much more
trivial statement though the space of
possible neurons is enormous though I
don't know one thing you want to be
pretty careful of when discussing this
stuff is how much the mask you learn is
the
computation since I know there's
probably quite a lot of algorithms can
be cleverly expressed with a mask over a
gaussian normal
Matrix but and
no part two How do machine learning
models represent their thoughts now
we're taught in machine learning 101
that neuron networks represent
hypotheses which live on a geometric
domain and inductive prior learn to
generalize symmetries which exist on the
underlying geometric domain and um
you're talking about them representing a
space of algorithms which we're going to
explore now um one thing that I wanted
to touch on is that they learn the
mapping to extensional attributes not
intentional attributes intention spell
with an S and we'll come back to what I
mean by that in a second but um I think
it's quite popular for people to think
of neural networks principally as a kind
of hash table so or locality sensitive
hash table and the generalization part
comes from the representation mapping
function uh which is on this embedded
hbit space
uh which is the vector space of the
attributes which then resolves a pointer
to a static location on the underlying
geometric domain now this can mimic an
algorithm especially when the inductive
prior itself is increasingly algorithmic
like a graph neuron Network for example
which behaves in a very similar way to a
prototypical dynamic uh programming
algorithm there's some great work
actually on algorithmic reasoning by
Peta ficovich one of your colleagues now
at deepmind but um
he showed in his algorithmic reasoning
um work that
Transformers can't perform certain graph
algorithms I think he gave D Dy as an
example and he said it's because there's
this aggregation function in a
Transformer which isn't in a GNN so I
just wondered if if you could kind of
like compare and
contrast whether or not neuron networks
are performing algorithmic
generalization and the differences
between let's say GNN and trans
forers yeah so I'm not very familiar
with dnns so I'll probably avoid
commenting on dnn's versus Transformers
so fear of embarrassing myself um in
terms of the underlying thing so I
definitely think we have some pretty
clear evidence at this point that models
are doing some genuine
algorithms um I don't know I think my
modular Edition thing is a pretty clear
proof of concept of this yeah so one
Thing Worth stressing is
that I generally think of models as
having linear representations more than
geometric
representations so I think of an input
to a model as having many different
possible features where features are
kind of a property of the input in an
intentional sense um but which is kind
of a fuzzy and garbage definition so I
prefer the extensional definition of
like an example of a feature is like
this bit of an image contains a curve or
this bit of an image corresponds to a
car window or this is the final token in
Eiffel Tower or this corresponds to a
list variable in Python with at least
four elements and all kinds of stuff
like that and well and know this this
scene is shaded blue because someone put
the wrong filter on the camera um
and yeah I generally think of models as
representing features as linear
directions in space and each input is a
linear combination of these
directions and this is kind of the
classic words to V framing like the um
King minus man equals Queen minus woman
thing where you can kind of think of
this as there being a gender Direction
and there being a royalty Direction and
these are like the right units of
analysis rather than king queen man wom
being the right units of analysis but
where each of these is made up out of
these underlying linear
representations and this is a fairly
different perspective to the geometric
where are things in a manifold how close
are they together in ukan space because
that's all that's all kind of a global
statement about how close two things are
where you're comparing all possible
features
while I don't know the Eiffel Tower and
the Coliseum are close together in some
conceptual space because they're both
European landmarks but they're also very
different because France and Italy are
fairly different countries in some sense
and maybe they're different on a bunch
of other features or one of them is two
words the other is one word which really
matters in some ways and ukian distance
and geometry
is it it's a Global summary statistic
and all summary statistics Li to you is
another motto of mine um but in
particular Global ones I'm very
skeptical of and yeah in general this
how what is the structure of a model
representations I think is like a really
important question and in particular
models are such high dimensional objects
that you really want to be careful to
distinguish between the two separate
things of sorry again um models are such
high-dimensional objects that it's
basically impossible to understand gbd3
as a 200 billion dimensional Vector you
need to be breaking it down into units
of analysis that can vary independently
and are independently
meaningful and the linear representation
hypothesis is like a pretty loadbearing
part of how I think about this stuff
because it is
so because it allows you to break things
down and it seems to be a true fact
about how models do things though again
we don't have that much data because we
never have enough data it's really
sad and yeah
um well let's contrast a little bit so
so this linear representation hypothesis
this idea that the models break down
inputs into many independent ly varying
features and store them as directions in
space much like word Tove and the um the
GOI people I mean like foder and palan
they they they brought out this famous
critique of connectionism in
1988 and their main argument was
systematicity and they were talking
about intention versus extension and it
might just be worth defining what I mean
by that so if I said the teacher of
Socrates was Plato the extension is
Plato the intention is everything it's
the teacher it's Socrates you know if I
said 4 + 5 = 9 nine is the extension
four and plus and five is the intention
so they were saying something very
simple they said in a neuron Network the
intentional attributes get discarded and
that's why the network don't support
what they call compositionality now
compositionality is actually quite an
abstract term because using vector
algebra in these analogical reasoning
task that you were just talking about so
king and queen and so on that's a form
of compositionality but they would say
it's a poor cousin of compositionality
because it's only using um you know the
doain the representation is in a is in a
vector space and in a vector space you
only have very basic primitive
Transformations so you wouldn't be able
to I mean for example you're talking
about Paris earlier you wouldn't do the
kind of analogical reasoning they were
talking about being able to Downstream
say were they in parent is Paris in
Europe of course it does happen in in
this linear representation Theory but it
happens in a very different
way H so I guess I'm not sure I fully
followed that um I mean this might be a
cheap gotcha but a fact about
Transformers is there's they have this
Central object called the residual
stream mhm um which I know in standard
framing me thought of about the thing
that lives in the skip connections but
is like the key thing about a
Transformer where each layer reads its
input from the residual stream and adds
its output back to the residual stream
and the residual stream is kind of this
shared bandwidth and memory and this
means that nothing's ever thrown away
unless the model explicitly is trying to
do that or is just applying some gradual
Decay over time so you know if you've
got an MLP layer that's saying I've got
four I've got plus I've got five and I
want to compute N9 4 five and plus is
still there I don't know this actually
engaged with your points and like I
don't know if this masses but it's
true yeah what you're saying is true but
I think the the point is that those
Primitives are not actually
representable in in a neural network so
you're saying with this residual stream
all of the extensions that came
previously also get passed up so in a
later layer you can refer to an
extension so the basically the answer of
a computation that happened Upstream but
what you can't refer to are the
intentional attributes of that
computation
Upstream why not like four is an input
so you can refer to four because you
could think of reading the input as a
computation plus is another thing you
read five is another thing you read like
what is a thing that is not an output of
a computation within this
framework I might have to get back to
you on
that like where's Keith duger when you
need
him would be a good example of
that I mean I guess it's it's about um
symbol manipulation as well so these
these things could actually be symbolic
operations which can be composed and
reused later and you would appreciate
that a neural network is only ever
passing
values so for example if it did
something which you could represent with
a symbolic operation if you wanted to
use that again I mean in an MLP the
reason why we use a CNN is because we
want to represent the same thing in
different places and an MLP would have
to learn it it it doesn't support
translational equivariance so it would
have to learn the same thing a million
times and it's the same thing with this
symbolic compositional generalization
that if it actually had this symbolic
representation which it used once it
could use it everywhere but now it has
to relearn it
everywhere right like you could if if
the W wants to know that Paris the
capital of France it can spend some
parameters on that and for every other
Capital it needs to separately spend
parameters and it can't just have a
general map country to Capital operation
yeah that that's exactly right I mean
let's let's use a simple example so we
use an MLP image classifier M and I put
put a tennis ball in and it's it's in
the bottom left of the visual field yep
and and then I put it in the top right M
and nothing it's learned from the bottom
left will be used MH so it just it just
feels like we're we're wasting the
representational capacity just doing the
same thing again and again and and in a
Transformer the only reason it does have
that um recognition you know that that
equivariance in respect of the position
of a pattern is because of the
Transformer inductive prior presumably
yes yeah so it uses the same parameters
at each position in the input sequence
so you know it should be able to do
bottom left and top right properly
though it does not necessarily have
things like rotation built in um I don't
know I feel like machine learning is
full of these people who have all kinds
of theoretical arguments and then
they're like this should be efficient
this should not work and then GPD fall
opposite them
and I don't
know no
Theory no Theory as interesting an
isolation unless it models reality
well and I don't know I haven't really
engaged with this theory in the same way
I haven't engaged with most deep
learning theory because it just doesn't
seem to meet my bar of does this make
real predictions about
models uh the maximal update
parameterization paper from Greg Yang
was actually a recent contradiction to
this right of really interesting the
that makes real predictions about models
that bear out and get you zero shot
hyper parameter transfer but like most
things just don't do
that very interesting okay okay well I
think now is a beautiful opportunity to
move over to orell now there was a
recent paper called do large language
models learn World models or are they
just surface statistics by Kenneth Lee
and he said that the recent increase in
model and data science has brought about
qualitatively new behaviors such as
writing C code or solving logic puzzles
now he asked the question yeah how do
these models achieve this kind of
performance do they merely memorize
training data or are they picking up the
rules of English grammar and grammar and
the syntax of the C language for example
are they building something akin to an
internal World model an understandable
model of the process producing the
sequences and he said that some
researchers argue that this is
fundamentally impossible for models
trained with guess the next word to
learn the language meanings of of
language and and their performance is is
merely surface statistics you know which
is to say a long list of correlations
that do not reflect a causal model of
the process generating in the sequence
now you said Neil that a major source of
excitement about the original orthello
paper was that it showed that predicting
the next word spontaneously learned the
underlying structure generating its data
and you said that the obvious inference
is that a large language model trained
to predict the next token May
spontaneous L model the world what do
you think uh yes so I should clarify
that that paragraph Was me modeling why
other people are excited about the paper
um okay but whatever I can roll with
this question so and and maybe bring in
your less wrong piece as well yeah yes
so the yeah I thought Kenneth's paper
was super interesting the exact setup
was they train so a is this chess go
like board game they took a data set of
random legal moves in ath they trained a
model to predict the next move given a
bunch of these transcripts and then they
probed the model and found that it had
learned a model of the board State
despite only ever being told to predict
the next move and so the way I would
Define World model is that there's some
latent variables that generate the
training data
um in this case what the state of the
board is um these change over time like
over the sequence but at least for a
Transformer which has a sequence and the
model kind of has an internal
representation of this at each point and
they showed that you can probe for this
and they showed that you can causally
intervene on this and the model will
make legal moves in the new board even
if the board say is impossible to reach
point of order can you explain what you
mean by probe just just so that listen
is at home yes so probing is this like
old family of interpretability
techniques the idea is you think a model
has represented something like you give
it a picture and you tell it to classify
the image and you want to see if it's
figured out that the picture is of a red
thing versus a blue thing even this
isn't an explicit part of the output you
take some neuronal layer or just any
internal Vector of the model and you
train some classifier to map that to
like red or blue and you do something
like a logistic regression to see if you
can extract whether it's red or blue
from that and uh there's also of
interesting Stu about probing but I
should probably finish explaining the AO
paper first before I get into that
tangent please so yeah the like reason
people are really excited about this
paper uh it was recently an AAL
and generally got a lot of hype was that
it was just you train something predict
the next token and it forms this Rich
emergent model of the world and forming
a model of the world is actually
incredibly expensive they like each cell
of the 64 cell a board has three
possible States three to the 64 it's
quite a lot of information to represent
but the model did it and lots of people
were like oh clearly language models
have World models um my personal
interpretation of all this is
that language models predict the next
token they learn effective algorithms
for doing this within the constraints of
what is natural to represent within
Transformer layers and what this means
is
that if predicting the next token is
made easier by having a model of the
world um of like I don't know uh who the
speaker is this was thing that will
happen and in some work led by w g that
we're going to talk about later we found
neurons that detector things like this
Texas in French this Texas python code
and in some sense this is like a
particularly trivial World
model and
so yeah um that's an interesting thing
um in my opinion it was kind of a prior
obvious that language models would learn
this if they could if they could and
need to and it was more
efficient another point for though um
learning that something is French seems
categorically different to because when
I when I read Kenneth's original piece
he showed what looked like a topological
representation of the world so how
different state spaces were related to
each other in a kind of network
structure
H so
I wondered if you can remember how we
produced that
diagram yes I'm St going from the
details I think it was something of the
form look at how different cells are
represented in the model and look at how
close together the representations of
different cells are and oh the model has
kind of got internal representations
that are close together I don't think
this is fundamentally different from the
king queen manom thing of just it's like
learn some structure on representations
that's obviously kind of reasonable yeah
I yeah I wouldn't read too much into
that like models learn structural
representations I think is all use at
this
point um but maybe another interesting
angle is that one of the reasons why
people like Gary Marcus they say GPT is
parasitic on the data they say because
they are empirical models most of the
meaning most of the information is not
in the data we have to reason over
explicit World model so he thinks the
reason a GPS is so good is because we've
imputed this abstract World model and
similarly when we play chess we have an
abstract World model and he would argue
that the information about that abstract
World model doesn't exist in any data so
how do you go from the data to the model
and the orell game seem to show that you
could go from the data to the
model yeah so I know I think that
Viewpoint is just like obviously wrong
like you're trying you're trying to do a
data prediction problem a valid solution
to that is to model the underlying world
and use this R what comes next there's
clearly enough information in an
information theoretic sense to do this
and the question is is a model capable
of doing that or not and I don't know
I'm just like you can't write poetry
with statistical correlations you need
to be learning something think maybe
that's not a good example I don't
believe you can write like yes you
can I don't believe you can produce
like good answers to like difficult code
forces problems it's like do good
software engineering is purely a bundle
of St statistical
correlations um maybe I have too much
respect for software Engineers I don't
know so where does it come from then um
that flash of inspiration or that higher
level I guess the first question is do
you is there a jump is it actually
grounded in the data it's trained on or
is there some highlevel
reasoning you know where where does that
materialize from so the way I think
about it there was just a space of
possible algorithms that can be
implemented in a Transformer's weights
and some of these look like a world
model and some of look like a bunch of
statistical
correlations and models are trading off
lots of different resources like how
many dimensions does this consume how
much weight Norm how many
parameters um how hard is this to get to
and how weird and
intricate and models will choose the
thing that gets the best loss that is
most efficient on these Dimensions
assuming they can reach it within the
Lost landscape where I use choose in a
very anthropomorphic sense like Adam
chooses good Solutions and I don't know
if you have a sufficiently hard task and
forming a world model is like the right
solution to it models can do it and I
think people try to put all of these
Fancy philosophizing on it in a way that
I just think is false guilty is
charged and I think the aop paper is
like a really beautiful elegant setup
that proves this all right can I move on
to the TST does it prove it though um
it's very it's it's a it's a very small
contrived it's it's a big jump to assume
that that works on a large language
model so this is kind of the argument
I'm making I think there's the empirical
question of do language models do this
and the theoretical question of could
they do this and I'm saying I think the
theoretical question is nonsense and I
think the a paper very conclusively
proves the theoretical question as
nonsense they just like yeah when given
a bunch of data you can infer the
underlying World model behind it well
Theory I would push back on that a tiny
bit because it's very similar to Alpha
go proved that in a closed game which is
systematic and
representable um you know with a with a
finite obviously exponentially large but
a finite number of board States you can
you you can build an agent which
performs really well that seems to me
completely different to something like
language or acting in the real world
that might not be systematic in the same
way we can debate whether or not it's I
think it's an infinite number of
possible
trajectories just like language an
infinite number of possible sentences
man there's 50,000 to the power of a
thousand possible input sequences sure
is a finite
number you mean in norell or oh no no in
gpd2 in
gpt2 bounded context length bounded
verab size generally
bastard you're not going to write more
than one quintilian characters
probably yeah being a do continue yeah
well I guess it is still a big jump
though isn't it from yes empirically it
shows that in orell it works maybe maybe
we could debate whether or not it it
does or not because there's this
question coming back to what we were
saying before whether it's learning
something which is
universal or or something which is still
brittle so the way that we've evaluated
it might lead us to conclude that it's
Universal whereas actually it's brittle
in ways that we don't understand so
that's a very real
possibility yeah I like everything's
brittle in ways you don't understand uh
it's like pretty rare that a model will
do everything perfectly in a way that
there are no adversarial examples and
this is like one of the more interesting
thing that's come out of the adversarial
examples literature to me it's just like
oh wow there's so much stuff here
there's so there such a high dimensional
input space there's all kinds of weird
things the model wasn't prepared for and
I don't know my interpretation of the AO
thing is the strong theoretical
arguments are
wrong I separately believe that you know
um there are World models that could be
implemented in a language model's WS but
they also disagree with the strong
inference of the paper that this does
happen in language models or that we
conclude it does because W models are
often really expensive like in the EO
model it's consuming 128 dimensions of
its 512 dimensional residual stream for
this world model and the problem is set
up so the world model is insanely useful
because whether or move is legal is
purely determined by the board state so
it's worth the model's while to do this
but this is rarely the case in language
for example there was all this Buzz
about Bing chat playing chess and making
legalish moves yes and I don't know man
if you want to model a
chessboard you just look at the last
piece that moved into a cell that's the
piece in that cell you don't need an
explicit representation you can just use
attention heads to do it and there's all
kinds of weird hacks and like models
will generally use the best hack but
probably it is worth the model's while
to have some kind of an internal
representation like i' bet that if you
took a powerful code playing model and
probed it to understand the state of the
key variables it would probably have
some
representation but guess moving on to
the work I did building on the ath paper
so one of the things that was really
striking to me about the atho work is
simultaneously its results were strong
enough that something here was clearly
real um but they also used techniques
that felt more powerful than were needed
like rather they found that linear
probes did not work there weren't just
directions in space corresponding to
board States but the nonlinear probes
one hidden layer MLPs did and the key
thing to be careful of when probing is
is your probe doing the computation or
does the model genuinely have this
represented and even with linear probes
can be this can be misleading like if
you're looking at how a model represents
colored shapes and you find a red
triangle Direction it could be that
there's a red green or blue Direction
and a triangle square or shaped
Direction and you're taking the red plus
triangle or it could be the case that
each of the nine shapes with its own
Direction you found the red triangle one
but nonlinear probing is particularly
sketchy like in the extreme case if you
train GPD 3 on the inputs to something
dpd 3 can do a lot of stuff um if you
train GPD through on the activation side
Network it can probably recover
arbitrary functions of the input
assuming the information of the input
hasn't been lost which it shouldn't have
because there's a residual stream um and
uh what I said is not quite true but not
important um
and so I was and their intervention
technique was both got like very
impressive results but also involved
doing a bunch of complex grad incent
against that Pro and this all just
seemed more powerful than was necessary
and so I did the I challenged myself to
do a weekend hackathon trying to figure
out what was going on and I poked around
at some internal circuitry and tried to
answer some very narrow questions about
the model and I found this one neuron
that seemed to be looking for like three
cell diagonal lines where one was blank
the other was white the next was black
but then sometimes it activated on blank
black white
and it turns out that the general
pattern was that it was blank current
players sorry blank opponent's color and
current players color and uh this is a
useful Motif in a fell because it makes
the move legal and when I saw this I
made the buold hypothesis maybe the
model actually represents things in
terms of whether a cell has The Current
player's color or the current opponent's
color which in hindsight is a lot more
natural because the model plays both
black and white and it's kind of
symmetric from the perspective of the
current player and I trained a linear
Probe on this and it just worked
fabulously and got near perfect accuracy
and I tried linear representations on it
uh I tried linear interventions and it
just worked and I feel really excited
about this project for a bunch of
reasons first led in a weekend I'm still
very proud of this um
secondly I think that it
has Vindicated some of my general
suspicion of nonlinear probing like if
you really understand a thing you should
be able to get a linear probe to work
and kind of more deeply as we discussed
there's this words to Vex style linear
representation hypothesis about models
that features correspond to directions
The atho Works seemed like pretty strong
evidence against they had ctal
interventions showing that the board
state was there but actually um but
linear probes did not work seemed like
they found some nonlinear representation
and my and Chris's hypothesis seeing
this was that um there was a linear
representation hiding beneath um Martin
wenberg one of the authors of the paper
had the hypothesis that it was like an
actual nonlinear representation and this
was evidence against the hypothesis and
this kind of formed a natural experiment
where the hypothesis could have been
falsified but my work showed there was a
real nonlinear a real linear
representation and thus that it had
predictive power and so many of our
Frameworks for Mech andup are just these
loose Things based on a bunch of data
but not fully rigorous or fully
conclusively shown and so natural
experiments like this feel like some of
the best data we
have on on this linear representation
though I don't know if you've heard of
the spline theory of neuron networks by
Rand barriero and um without going into
too much detail it's quite a discret
view of MLPs in particular that the the
relus essentially get activated in an
input sensitive way to carve out these
polyhedra in the ambient space and
essentially um an input will be mapped
into one of these cells in the ambient
space and then there's a kind of um
discreetness to it because if you just
perturb the input and you move outside
of one of these polyhedra then the model
will if it's a class class classify
classify something different but I guess
I want to understand with this
representation theory if features are
directions does that imply there's a
kind of
continuity because the network will
learn to um spread out those
representations in the best possible way
but it won't necessarily be a way which
is semantically useful like in word Tove
stop and go are very close to each other
and they shouldn't be and at what point
does stop become go so do you do you see
there being boundaries in in these
directions
so I think this is again my point that I
think of linear representations as being
importantly different from geometric
representations like stop should be
close to go because in many contexts
they are like a kind of changing of
State term and it's used in similar
context and has similar grammatical
meaning but then on this like single
semantic thing they like quite different
and the ACT way to represent this is
have them be close together in idian
space but have some crucial like
negation Dimension with they're
different and for cont and like
ultimately neural networks are not
geometric objects they are made of
linear algebra every neuron's input is
just project the residual stream onto
some vector and this involves just
selecting some set of directions and
taking a combination of the feature
corresponding to each of
those and this is just the natural way
for a model to represent things in my
opinion okay okay well I think this will
in a second lead us on very nicely to
superp position which is that we don't
actually think of um there being one um
Direction
necessarily just just to close this
little piece now you you said in your
less wrong article that orell GPT is
likely overparameterized for good
performance on on this particular task
while language models are
underparameterized and of course we have
the ground truth to this task which
makes it very very easy so much easier
to interpret 100% but but you did you
did conclude saying that this is further
evidence that neuron networks are
genuinely understandable and
interpretable and probing on the face of
it seems like a very exciting approach
to understand what the models really
represent caveat M mour conceptual
issues so let's move on to to this
superposition also as polys semanticity
which is an absolutely beautiful well
you're shaking your head a little bit so
maybe maybe you start with that um yeah
so
there's all right so what what's the
narrative here so
fundamentally we are trying to engage
with models as these high-dimensional
objects in kind of this conceptual way
so we need to be able to decompose them
because of the curse of
dimensionality and
uh we think models correspond to
features and the features correspond to
directions and the Hope in the early
field was that features would correspond
to
neurons and even if you believe features
correspond to orthogonal directions the
same way they correspond to neurons is
like a pretty strong one because a
there's no reason to align with the
neuron bases the reason this isn't a
crazy belief is that models are
incentivized to represent features in
ways that can vary independently from
each other
and because R and jell act element wise
um if there's a feature per neuron they
can vary independently well if there's
multiple features in the same neuron I
don't know if there's a reu the second
feature could change so the reu goes
from on to off in a way that changes how
the other featur is expressed in the
downstream Network and this is like a
beautiful theoretical argument sadly is
bullshits because of this phenomena of
police mantis uh police mantis is a beh
behavioral observation of networks but
when we look at neurons and look at
things that activate them they're often
activated by seemingly unrelated things
like the s in the word strangers and
capital letters of proper nouns and news
articles about football it's a
particularly fun neuron I found one time
in a language model and um polic Mantic
is a purely behavioral thing we just
saying this neuron activates for a bunch
of seemingly unrelated stuff
um and it's possible that actually we're
missing some Galaxy brain abstraction
where all of this is related but my
guess is that this is just the model is
not aligning features with
neurons
and one explanation of this is you just
got this thing called a distributed
representation where a feature is made
of a linear combination of different
neurons but it just kind of rotated from
the neuron basis and this argument that
neurons can vary independently is a
reason to think you wouldn't see this um
where uh this hypothesis is just that
there's still N Things when there's n
neurons but they're
rotated um but then there's this
stronger hypothesis that tries to
explain this called the superposition
hypothesis and here the idea is so if a
model wants to be able to recover a
feature perfectly it must be a fogal
from all other features
but if it wants to mostly recover it it
suffices to have almost orthogonal
vectors and you can fit in many many
more almost orthogonal vectors into a
space than orthogonal vectors as theorem
saying that there are exponentially many
in the number of Dimensions how if if
you have 100 dimensional vectors um how
many orthogonal directions are there
what relationship 100 yep um Yep this is
just the statement that
like you pick one you pick a
vector um sorry there's 100 vectors that
are all orthogonal of each
other um basic proof you pick a vector M
everything's athal to that that's a 9
to9 dimensional space you pick another
Vector take everything off F on to that
that's a 98 dimensional space and keep
going until you get to nothing um like
if you picture a 2d space you pick any
direction the only things orthogonals
that are align and so there's exactly
two orthogonal things you can fit
in and there's like you can rotate this
you can get many different sets of
orthogonal things okay I'm trying to
articulate why this doesn't make sense
to me so maybe we should start with the
cursive dimensionality which is that the
volume of the space increases
exponentially with the number of
Dimensions so we'll we'll start with
that and the reason I'm thinking maybe
I'm wrong but if you've got a 100
dimensional Vector um every combination
of flipping one of the dimensions would
be would produce a vector which is
orthogonal to all of the other ones
would it uh no so uh let's imagine
you've got a vector of all ones yes if
you pick the first element and you
negate it yeah so it's like minus one
than 99 ones these are not orthogonal
the dot product is 98 okay okay well
that that makes sense so so there's
there's a linear number of um orthogonal
directions and in which case we actually
need to have these approximately
orthogonal directions because that
actually does buy us an exponential
number yeah and so the superpositional
hypothesis is that the model represents
more features than it has neurons yes or
that it has dimensions and it somehow
compresses them in as things that are
almost orthogonal MH when it reads them
out with a projection it gets some
interference but the inter and it needs
to balance the value of representing
more features against the cost of
interference and anthropic has this
fantastic paper called toy models of
superp position uh which sadly was
written off right left so I can't claim
any credits and um what they they
basically build a toy model that
exhibits superposition the uh exact
structure is you have n independent
features Each of which is zero most of
the time it's not very prevalent and
there's a linear map from that to a
small dimensional space a linear map
back up and a nonlinearity on the output
uh no nonlinearity on the input on the
bottleneck in the middle and you you
train it to be an auto encoder can it
recover the features in the inputs and
because there's many more features than
that are in the bottleneck this tests
whether the model can actually do this
and they find that it sometimes does
sometimes doesn't and then do a lot of
really in-depth investigation of how
this
varies and yeah returning to like is
superposition the same thing as
polysemantic
um I would say no polyan is a behavioral
thing distributive representations are
also a behavioral thing that it's like
not aligned with the basis and
superposition is a mechanistic
hypothesis for why both of these will
happen because if you have more features
than neurons obviously you're going to
have multiple features per neuron and
probably you're going to have features
that are not aligned with
neurons
okay okay very interesting so why do you
think that superposition is one of the
biggest problems in mechin turp yeah so
it's this fundamental thing that we need
to be able to decompose a model into
individual units
and ideally these would be neurons but
they are not neurons so we need to
figure out what we're doing
and superposition so in a world where we
we just have like n meaningful
directions but they weren't aligned with
the standard basis that would be kind of
doable um and indeed models often have
like linear bottlenecks like the
residual stream or the keys queries and
values of an attention head that don't
have element wise linearities and so
have no intrinsically meaningful basis
uh the jargon here is privileged basis
and
but um superposition means that you
can't even say this feature should be
athal to everything else there's going
to be a bunch of
interference um there's not even a kind
of
mathematically mathematically there's
not even like a unique set of more than
end directions to describe some set of
vectors in end dimensional space um and
I think that understanding how to
extract features from
superposition given that superposition
seems like a core part of how models do
things though we really do not have as
much data here as I would like us to um
understanding how to extract the right
meaningful units seems really
important okay and I I think we should
clarify the difference between
computational and representational
superposition yeah so there's kind of
two so Transformers are interesting
because they often have high dimension
activations that get linearly mapped to
low dimensional things so like in say
gpd2 in say gpd2 small the residual
stream has 768 Dimensions while each MLP
layer has 3,000
neurons and even if we think each neuron
just produces a single feature they need
to get compressed down to the 768
dimensional residual stream
and we or there's like 50 ,000 input
tokens that get compressed to 768
dimensions
and this is called representational
superposition the model is representing
the model's already computed the
features but it's compressing them to
some bottleneck space and this is the
main thing studied in the toy models of
superposition paper and what we found uh
sorry
um there's a separate thing of
computational superposition
which is when the model is doing it's
Computing new features this needs
nonlinearities like attention head soft
Maxes or MLP
jell and the nonlinearities can compute
new features as directions from the old
ones like
um if
this uh for example uh if the top of an
image is a car window and the bottom is
a car wheel then it's a
car um or if the current token is
Johnson and the previous token was Boris
this is Boris Johnson and this is
all how to phrase this
um yeah this
is computational superposition if the
model wants to compute more features
than it has
neurons and this is like harder to
reason about because linear algebra is
nice and fairly well understood
nonlinearities spoilers in the name are
not linear and th way more of a pain and
I think that we generally have a much
less good handle on computational
superposition but also that this is like
way more of where the interestingness
lies by my lights and this is very
briefly studied in the toy models of
superposition paper but I would love to
see more work looking at this in
practice and also looking at this and
toy
models so zooming out a tiny bit there's
this paper from anthropic and the
overall question to me is does it
actually exist now presumably you're
satisfied with the evidence that it does
exist and then there's the question of
how do neuron networks actually do it
and then there's the question of how
does the neuron Network think
anthropomorphic language I apologize
about the tradeoff of more Super more
superposition more features but more
interference versus less interference
and more
superposition yeah so diving into the
final question about
interference um the a useful conceptual
distinction is that there's two
different kinds of
interference um so if you've got two
features the share a dimension or share
a
neuron um oh yeah final note on
representational superposition is I
don't think it should even be referred
to in terms of neurons because the
individual BAS elements don't have
intrinsic meaning modular weird quirks
like Adam um and it annoys me when
people refer to the residual stream or
key vectors as having neurons there's no
element wi is linearity it's not
privileged anyway um yeah two types of
interference um when a and b share a
dimension you
can um yeah let's say this Dimension has
both Dice and poetry you first of need
to tell where if dice is there but
poetry is not you need to tell that dice
is there and that poetry is not there
and if both which I call alternating
interference and then there's
simultaneous interference where Dice and
poetry are both there and need to tell
that both are there but not that they're
both there with like double strength and
as a general rule models are good at
dealing with things of the form Noti
when something is Extreme along this
Dimension but not um notice when it is
Extreme along a dimension um versus when
it's not extreme and alternating s
alternating interference looks like that
like if um dice is straight up poetry is
at
45° both have like weak inter both have
less interference when the other one is
active than when the main one is active
along their Direction okay so you're
saying interference from a and not B is
far easier than a and b yes exactly and
um like a very a very rough heris is
models will just not do simultaneous
interference but will do alternating
interference and they observed this in
the toy models paper um because they
varied how often a feature was nonzero
uh what I think of as the prevalence of
the feature though they called it
sparity and and what they found is that
when the feature was less prevalent it
was much more likely to be in super
position and the way to think about this
is if you have two independent features
that both exist with probability P the
rate of simultaneous interference is P
Squared the rate of alternating is
p and so and the worth of having the
feature is also proportional to P
because it occurs P of the time so the
rarer it is the less of the big deal
simultaneous
interferences and eventually the model
uses superp position there's also there
was also an interesting bit looking at
correlated
features um so correlated features even
if they're not very prevalent they have
pretty high simultanous interference and
models tend to put correlated features
in to be orthogonal but anti-correlated
features it's very happy for them to
share a
Direction one way you could think about
this is if you got say 25 features about
romance novels and 25 features about
python code you can have 25 directions
that each contain a pair of features and
then a single disambiguating neuron that
is on for python code off of romance
novels that Ed to disambiguate the
two and yeah maybe this be a good time
to talk about the finding neurons in a
Hast stack paper unless you've got more
stuff on this we'll get to that in just
two shakes of a lamb's tail but just
before um when I was reading through the
paper I I was I had the mindset of
sparity and you told me Tim don't don't
say sparity it's prevalent it means so
many things it means so it's very
overloaded such an overloaded what so
you know so just quickly touch on the
relationship between what what is
prevalence the relationship between
prevalence and superposition and um just
before well actually I've got a couple
more questions but um would you also
just Mind Playing devil's advocate and
criticizing the anthropic paper if if
you can uh sure so I should be very
clear this is one of my top three
all-time favorite inability papers it's
a fantastic paper uh that a bad word
said about it um oh I have so I I have
bad words say about every paper good
especially the ones that I like cuz I've
engaged with them in the most detail yes
so uh things which I think were
misleading about this paper uh the first
is I think the representational versus
computational superposition distinction
is very important I think computational
is a bit more interesting and while I
think the authors knew the difference I
think a casual reader often came away
not realizing the difference in
particular that most of their results
were about the residual stream not about
actual neurons and MLP
layers um the second is a question of
activation range so they study features
that vary uniformly between zero and one
and in practice I think most features
are binary this is a car wheel or this
is not a car wheel this is this is Boris
Johnson and this is not bis Johnson and
it interference is much worse when they
can vary continuously because if a and b
if a is up B is at
45° it's you can't
distinguish B at strength one from a at
strength um
7ish and this is just kind of messy
but the binary it's just much easier and
I think this is a source of
confusion um yeah I also think the two
kinds of interference point was a bit
understated and yeah but like more
broadly it's just a phenomenal paper oh
my other biggest beefer they just didn't
look in real models and like this wasn't
the point of the paper but
like we're doing so much Theory crafting
and filming conceptual Frameworks and we
haven't really checked checked very hard
whether this is why models actually have
polic mantis um where G he's working out
of MIT and um you you've done a lot of
work with him so uh you and Wes um Wes
was the first author wrote a paper
called finding neurons in a hay stack
case studies with sparse probing where
you empirically studied superposition
and language models and and actually
found that you get lots of
superpositions in early layers for
features like the security and Social
Security and fewer in Middle layers for
complex features like this text is
French so um and also you can bring in
the importance of range activations as
well but can can can you frame up that
paper yeah uh so first off this paper
was led by Wes gy one of my mentees did
a fantastic job he deserves like 9% of
the credit great job W uh I believe
listen to this podcast so hi um and yeah
so the kind of highle pitch behind the
paper was well we think superposition is
happening but like nobody's really
checked very hard and there's like some
results in the literature I've since
come across in non- Transformer models
that demonstrate some amount of
distributed
representations but what would it look
like to check and what would it look
like to do this in like a reasonably
scalable and quantitative way and um the
kind of sparse probing in the title is
this technique where introduced is
for um if we think a feature is
represented in MLP layer we can train a
linear classifier to extract it a linear
probe from that layer but if we
constrain the probe to use at most K
neurons very K and look at probe
performance this lets us distinguish
between features that are represented
with like a single neuron and features
that are densely spread across all
neurons um with a lot of methological
nuances about balanced data sets and
avoiding overfitting and fun stuff like
that and most of the interesting bits of
the paper in my opinion are the various
case studies we do where so probing
fundamentally is like a kind of sketchy
methodology because probing is
correlational probing doesn't tell you
whether a model uses something and it's
so easy to trick yourself about whether
you have the right
representations um so we use it as a
starting point and then dig more deeply
into a few more interesting things um
one particularly C case study is we
looked into factual knowledge neurons
found something that seemed to represent
this athlete plays hockey but then
actually turned out to be a Canada
neuron uh which continues to bring me
joy that activates with things like
maple syrup Canada Wonder wonderful um
got got to love models learning National
stereotypes right oh yes um anyway
so
um yeah so there were two two
particularly exciting case studies the
first was looking in early layers at
compound word detectors
so if you look at say the brain and its
visual field we have all these Sensory
neurons we get raw input of light from
the environment and it gets converted
into stuff our brain can actually
manipulate image models have Gore
filters that convert the pixels into
something a bit more useful uh what's
the equivalent of language models
and it seems to be these things that we
call detalization neurons and circuitry
where often words are split into
multiple tokens or you get common word
compound word phrases like Social
Security or Teresa may or Barack Obama
whatever and it's often useful for a
model to realize this is the second
thing in a multi-token phrase especially
if it's like you need both things to
know what's going on like Michael Jordan
of Michaels lots of Jordans it's really
important to tell that both both of them
are there and this is a clearly
nonlinear thing it's like a Boolean
and and so we did a lot of probing for
different compound words and we found
that they were definitely not
represented well by single neurons we
could find some neurons that were okay
at detecting them but there was a lot of
inter
and a lot of like false positives from
other stuff and when we dug into a bunch
of these neurons we found that they were
incredibly polysemantic they activated
for many different compound words and we
showed that it was using superp position
by observing that if you took say five
Social Security detecting neurons and
add together their activations they go
from okay detectors to a really good
detector together because even though
each is representing like hundreds of
compound words um they're representing
different compound words which lets you
encode
these and um this what we've shown here
is that it's like distributed that it's
a linear combination of
neurons um we still haven't shown it
perfectly to my dissatisfaction I think
you really need to do things like ablate
these linear combinations and see if
this systematically damages the model's
ability to think about social security
Etc but I'm pretty convinced at this
point and there's like a few properties
of compound words that both make it easy
to represent in super position make me
pretty okay making the jump that there's
actual superp position the first is just
that there's tons of compound words each
one is pretty rare but each one is like
non-trivially useful and clearly there
are more compound words than there are
the like thousands of neurons in the MLP
of this model the the model cares about
representing and can represent that we
do not actually check uh because I could
not convince W to accumulate a list of
2,000 compound words and pray for all of
them um but I believe in my heart this
is true could I never point of order
though so um because I've been reading
quite a lot of stuff from um linguists
like Steven panad DOI and um a lot of
linguists are some of them hate language
models and some of them are well on
board with it and you know like Raphael
Miler for example is is a great example
um I hate language models too don't
worry well but but the question is
because you're talking about compound
words and stuff like that and and you're
you're still using the language of
syntax and these language models there's
this distribution of hypothesis they you
know you know a you know the meaning of
a word by the company it keeps but
linguists and cognitive scientists kind
of ditch that well I think I don't think
they ever believed in the distributional
hypothesis they think about grounding
they think about ground grounding to
things in the world um and and also
inferential um references as well which
is you can think of that as grounding to
a model of the mind and this brings us
back to the orelo paper which is that
they're not just learning simple kind of
compound relationships between the world
between the words they're learning a
world model and and they're doing
something much more potentially than
just predicting the next word and panad
DOI argued that most of the
representational capacity in in um
language models are learning these
semantics they're learning relationships
between things in the world model and
the particular occurrence of of of the
token and this superposition idea is
very interesting because it actually
imbus the representational capacity in a
language model to learn those
mappings uh okay so a couple of comments
on that uh the first is a generally
useful way of thinking about models to
me is as a the early layers devoted to
Sensory neurons converting the raw input
into more useful Concepts and
representations the actual
processing throughout like all of the
middle layers that actually does all the
reasoning and then motor neurons at the
end that convert the reasoning to actual
output tokens for like the format that
the optimizer wants and it feels like
you're mostly talking about the like
reasoning internal
and I'm the specific case I'm referring
to is on the sensory neurons well like
I'm not saying it just a text compound
words but obviously that's the first
thing it
does I don't know it's so interesting I
don't mean to push back but in
Neuroscience the field was held back for
decades by this idea of this kind of
left to right processing this
hierarchical processing where you have
these um very very simple Concepts that
become increasingly abstract with more
processing and then I think the field
has moved away from that it's far more
messy and chaotic than that now with a
neural network it it actually is
hierarchical because the network is
basically a dag so I suppose it is safe
to make this assumption but could I just
kind of question you on that is it safe
to make that assumption is there
increasing complexity in representation
as you go from left to right uh let's
see uh so yeah I definitely yeah so
clarification one the network has this
input sequence which I think was going
from left to right and then there's a
bunch of layers which I think it was
going from like the bottom to the top
yes and you're referring to the bottom
to top axis right yeah I'm sorry I was
using an MLP mindset when I asked that
question so as you say in in a
Transformer it's an auto regressive
model and you have you know stacked
attention layers with little MLPs on the
end so I guess the way I was actually
meaning the question is so so so
complexity increases monotonically as
you go up the stack of attention layers
is that is that a fair assumption um yep
uh again no one's really shown this
properly but I'm like surely this is
true and there's been some work doing
things like looking at neurons looking
at the text that activates them looking
for patterns and trying to understand
what what these represent and it's
generally looked like early ones are
more about DET toiz and
syntax later ones are doing stuff that's
interesting final ones are doing this
like neuron Behavior like I also want to
be very clear that networks are
cursed networks do not fit into nice
abstractions I'm not saying the early
layers are literally only doing
detonation yeah but I believe we have
shown its part of what they're doing and
I speculate it is a large part of what
they're doing I'll be very surprised if
it's all of what they're doing because I
heard you on another podcast and you
were just talking about the I mean I
think the curse is the right way to
describe it which is that even when you
make um modifications when you
manipulate what's happening the behavior
will change in a very reflexive way so
you kind of you delete one thing and
then another neuron will take on the
responsibility of the thing you just
deleted and so so you're you're it's a
little bit like manipulating financial
markets you've got almost like this
weird Collective diffuse intelligence
where you make one modification and the
whole thing changes in a very complex
way and similarly I guess that's why I
was intuitively questioning the
assumption that you have a residual
stram so surely even at the very top of
that attention stack there must be
primitive and complex operations going
on in some weird
mix um seems probably true uh
generally yeah there's going to be some
stuff you can just do with literally the
embeddings um some stuff that you need
to wait a bit more before you can do
anything useful with just like I no
if you got a sentence about Michael
Jordan I don't think you can usefully
use Michael or Jordan in isolation so
you need to DET toonize to Michael
Jordan but also I don't know if you've
got Barack Obama Obama and Barack both
on their own pretty clearly imply it's
going to be about Obama and probably the
model can start doing some processing in
the early in like layer zero does it
want to somewhat unclear it's going to
depend a lot on the model's constraints
and other circuitry and how much it's
worth spending the parameters then
versus later there's also some various
things where I don't know um model
memory kind of decays over time because
the residual streams Norm gets bigger so
early layer outputs become a smaller
fraction of the overall thing and layer
Norm sets the norm to be unit so things
kind of Decay and so if you compute a
feature in the early in like layer zero
it can be harder to notice by like layer
three than if was computed in layer 2
but these are all just kind of like mild
nudges and ultimately neural networ do
what neural networks want man I know I
know I just want to close the loop on
something I said a little while ago
about you know potentially large models
use most of their representational
capacity for um you know learning these
semantic relationships and empirically
we found that you know there's some
question recently actually about do we
actually need to have really really
large models and for Pure knowledge
representation the argument seems to be
yes but we can disentangle knowing from
reasoning and and there's also this
mimickry thing so it's quite interesting
that all of the you know like Facebook
released their model and very very
quickly people find tuned it using the
the Laur you know the low rank
approximation fine-tuning method and uh
on all of the benchmarks the mod I mean
even open Assistant there another great
example Yanik was sitting in your seat
just a few weeks ago and he was saying
that on on many of the benchmarks the
model's working really well but it's
kind of not it's kind of mimicry like
the the big large models that you know
meta and Google and deepmind and all
these people they spend Millions
training these models and and they have
they have base knowledge about the world
which um is not going to be you know
replicated by fine-tuning you know like
an open source model anytime soon the
knowledge is
based the knowledge is based yes yes yes
yes exactly well um Okay so so that's
that's that's very interesting let's
just quickly talk about the open AI
microscope because this is the open AI
microscope is this beautiful app that
open AI released in in 2020 and you can
go on there and you can you can click on
um any of the neurons in popular Vision
architectures at the time so you I think
most of them are sort of like image net
you know things like Alex net and God
knows what else and um they they solve
this optimization problem where they
generate an image using um toasting
gradient descent that maximally
activates a particular neuron or I think
even a layer using something similar to
deep dream and you can click on these
neurons and sometimes they are what we
will call poly monos semantic which
means it's just Canada a lot of the time
there's a couple of Concepts in there
but it's weirdly intelligible you know
you might see you know like um a playing
card or an Ace and and a couple of like
tangent related Concepts and it always
struck me as strange because I imagine
there's a long tale of semantic
relationships and I found it bizarre
that there'd only be one or two in this
visualization and I had this intuition
that the optimization algorithm is in
some sense mode seeking rather than
distribution matching which is to say
that it finds the two most two or three
or four most kind of salient semantic
mappings and they dominate what is
visualized and you're almost snipping
off the long tale of of the other
semantic
mappings yeah so I think there's two
things to disentangle here the first is
what is actually represented by the
neuron in terms of ground truth and the
second is what are techniques show us so
um the two techniques used in the open a
microscope are looking at the images the
most activated neuron and then this
feature visualization technique where
they produce a synthetic image that
maximally activates it and to me this is
these are
like both of these can be misleading
because if the model activates for dice
and poetry but activates for dice with
strength five and po strength four then
the optimal it activate it will be Dice
and the optimal the data set examples
will also be dice but really it'll be
about poetry and you want to get a lot
more rigorous you want to show true true
mon manity um one cute thing is Spectrum
plots you take lots of example data set
examples across the full distribution
you have a histogram with like the
different groups for the different
meanings and then neuron activation on
the x-axis we have this really cute plot
in Wes's paper called the French neuron
where all of the French where all of the
French text is on the right all the
non-french text is on the left and then
on is just very clearly distinguishing
the two in a way that's much more
convincing to me than things like Max
act
examples um and I actually have a hobby
project called neoscope at neoscope doio
where you can see the max activating
text examples for every neuron and a
bunch of language models though opening
I recently output this paper with one
that is just better but only for gpd2
XL um anyway uh not that I'm bitter or
anything no so um and yeah
so yeah there's the things can lie to
and be elusory um there's this
interesting paper called The
interruptibility Illusion for but which
investigated this specific phenomena and
in particular that if you take the data
set examples over some narrow
distribution like Wikipedia or books you
can get pretty misleading things though
they only looked at residual stream
basis elements rather than actual MLP
neurons
I believe which makes it a bit less
compelling um point of order as well
we've been saying residual stream quite
a lot and Microsoft introduced resinet
in
2015 which basically means that between
all of the layers the information is
being passed up unadulterated so the
subsequent layer can choose to either
essentially shortcut or ignore the
previous layer or use some combination
and at the time they kind of said it was
about the neuron Network being able to
learn its own capacity in in some sense
but could could you just give us like
the way you think about these residual
streams yeah so I think the standard
view of real networks there are just
layers and layer five's output is layer
six's input Etc um then people added
resets where layer six's input is layer
five's output plus layer five's input
with the skip connection but I think
people normally thought of them as like
yeah it's like a cute trick that makes
the model better but doesn't massively
change my conceptual picture and the
framing that I believe was introduced in
the mathematical framework this
anthropic paper led by Chris Ola Nelson
alhar and Katherine Olson that I was
involved with is actually let's call the
thing in the skip connection the
residual stream and think of it as the
central
object and draw our model so the
residual stream is this big vertical
thing and each layer is like a small
diversion to the side rather than the
other way around
and in practice most circuits involve
things skipping many layers and each
layer should is better thought of as
like an incremental update and there's a
bunch of earlier Transformer
interpretability papers that I think
miss this conceptual Point like the
interrupt whe delusion for B one I
mentioned earlier and study residual
stream basis elements as like layer
outputs or
something yeah I mean in a sense you
know we were talking about being able to
reuse things that you've learned before
and not having to learn them again and I
guess I think of it as a kind of
translational equivariance in the in the
layer regime which is that you have a
computation which is learned early on
and now it can just be composed into
subsequent layers it's just it's like
you've got a menu of computational
functions that you can call on at any
layer yeah pretty much I think of it as
like the sh shared memory and shared
bandwidth of the model yeah yeah almost
like a memory bus yeah and sometimes
models will dedicate neurons like
cleaning up the memory and deleting
things that are no longer
needed yeah yeah and is there any
interference in that memory bus so much
go on uh this is the thing of superp
position right like the resid room is
doing everything it's like there's
50,000 input tokens start and then 4X as
many neurons as residual dimensions in
every MLP layer and ATT tension heads
moving everything around and it's just a
cluster  what what if you scale up
the bandwidth of the bus mhm um that is
basically making the model bigger right
which we know makes models better but I
don't know just thinking out loud but
what if you maintained the original
dimensionality of the model but you
deliberately upscaled the bus um so like
you make the thing inside side each
layer smaller but make the residual
stream bigger or just make everything
the same as it is but you just kind of
like have a a linear transformation on
the bus and double the size of the bus
um so I don't think that would work
without increasing the number of
parameters because like if you cuz like
the thing that matters is the smallest
bottleneck the output wids of an MLP
layer are like 4,000 by
1,000 and in order to make the 1,000
bigger you need more
parameters there's like all kinds of
studies about the optimal
hyperparameters the optimal ratios my
general heris stic is number of
parameters are the main thing that
matters I don't know I don't spend that
much time thinking about how to make
models better to be honest I just want
to understand them godamn it yeah
because it's one of those things that it
might remove bottlenecks because because
essentially you're you're allowing the
model to reuse things that it's learned
previously so now every single layer can
speci more than it did before and that
might kind of like weirdly remove
bottlenecks
yeah yeah the way I generally think
about it is models are ensembles of
shallow pods which is this paper from
like 5 years ago about resets
like dd2 small is 12 layers um each
layer includes an attention block and
attention bit and MLP bit it is not the
case that most computation is 24 levels
of comp position deep it is the case
that most of them involve like I know
four and they're just intelligently
choosing which four and remixing them in
interesting
ways and sometimes different things will
want to like get to different points and
so it's useful to have many layers
rather than a few but also I don't know
if you um have if you have the residual
stream width and give the model 4X as
many layers often performance is like
about the same um or like not that
different because the number of
parameters is unchanged and this is just
kind of a wild result about models that
I think only really makes sense within
this framework of it's like an ensemble
of shallow pods and it's a trade-off
between having more computation and
having better memory bandwidth yeah yeah
very interesting okay I mean just to
close um super position it might not be
a new idea so did a um a paper video
about this paper called super masks in
super position by Mitchell wsman back in
2020 and he was talking about supermark
representing sparse uh sub networks in
respect of catastrophic forgetting and
continual learning but that was slightly
different because that was an explicit
model to perform masking create sub
networks and and and to to model um you
know like basically a sparity aware
algorithm but he was still using a lot
of the same language like interference
and so on and and thinking about superp
positions of sub networks and I guess
the difference is is like just as we
were talking about with these inductive
PRI like Transformers and and cnns um
the models already do this stuff without
us having to explicitly code it which I
think is the interesting Discovery yeah
yeah one update I've made from W's work
is that detalization is probably like a
pretty big fraction of what the early
layers do and it's just really easy to
represent compound words and
superposition because it's very binary
it's either there or not there so
alternating inference is easy to deal
with they're mutually
exclusive so there's no simultaneous
interference like you cannot have Boris
Johnson and Theresa May
co-occur um and you there's just like so
many of them um one fact about language
models that people who haven't played
around them may don't appreciate is
their inputs are these things called
tokens and tokenizers are  because
they're trained in this bizarre
Byzantine way that means that often
words rarer words will get broken up
into many tokens yes multi-word phrases
are always different tokens anything
that's weird like a URL gets completely
cursed and um models don't want to have
this happen so they devote a bunch of
parameters to build a like pseudo
vocabulary of what's going on and just
returning to your point earlier about
like is it just these syntax level
things is there some like actual more
semantic stuff going on um we did also
have case studies looking at contextual
neurons things like this code is in
Python this language is in French and
these were seemingly monos semantic like
it seemed like there were specific
neurons here and we found things like if
you a bleach the French neuron loss on
French text gets much worse while other
ones are fine and also some interesting
results that the model was say using
this to disambiguate things like
tokens like d are common in German and
also common in Dutch and the neurons for
those languages were being used to
disambiguate for that token whether it
was like a German D or a Dutch d because
they've got very different meaning in
the two languages yeah I wondered if you
can give me some inition because as you
say in W's paper you know he did
actually find that you know there are
some monos semantic neurons like French
as you just said and in this case the
model decided that interference in some
sense wasn't worth the burden but what
does burden mean here and French is a
very vague concept as
well yes so all right couple of
observations first is I do not think we
have properly shown they are monos
semantic neurons um we were looking
these models were trained on the pile
and we were specifically looking at them
on europal which is like a data set of
European Parliament transcripts which
are labeled by language and we found a
neuron that seemed to strongly
disambiguate French from non-french but
it was on this domain of parlamentary
stuff and because models really want to
avoid simultaneous interference if they
did have superposition they probably
want to do it with something that isn't
likely to co-occur in this context I
don't know this is a list variable in
Python which we didn't check very hard
for and in particular this is messy to
check for because in order to do that
you need to answer these questions like
what is French like there's a bunch of
English texts it will activate for but
it'll activate on words like sakra and
traba and I think I count this as French
but like I don't have a rigorous
definition of French and I think a open
problem I'd love to see someone pursue
is just can you prove one of these
neurons is actually a French detecting
neuron or not and what would it even
look like to do that and yeah regarding
interference in the burden so the way I
think about it if two features are not
orthogonal then um oh no sorry this is
more interest in the case of neurons if
there's multiple things that could all
activate a neuron then it's harder for
the downstream bit of the model to know
how to use the fact that that neuron
activated because there are multiple
things even if they don't co-occur
because they're mutually exclusive and
this is just a cost and there's a
trade-off between having more features
and not having this cost and features
like this is in French are really
loadbearing
they're just really important for a lot
of circuitry here and so theoretically
the model white might want to dedicate
an entire neuron to this but if you
dedicate an entire neuron you lose the
ability to do as much superp
position my intuition is the number of
features that can be represented in
superposition is actually like grows
more than linearly with the number of
Dimensions so this might be like
significantly worse than just having one
fewer
feature so we are now in the next
chapter of this beautiful podcast and
we're going to talk about
Transformers so how how exactly do
Transformers represent algorithms and
circuits and also um you've written this
beautiful mathematical framework about
Transformers which of course is um
working very closely with um Katherine
Aon and and Chris Ola and Nelson El and
and and Nelson my my apologies um yeah
so in terms of understanding yeah so if
you want to do mechanistic inability in
a model you need to really deeply
understand the structure of the model
what are the layers what are the
parameters how do they fit together what
are the kinds of things that make sense
there
and let's
see
so yeah there's like a couple of key
things I'd want to emphasize from that
paper though I know it's also one of my
like alltime top three interpretability
papers we should just go read it uh and
uh after reading it check out my 3-hour
video walk through about it which
apparently is most useful if you've
already read the paper cuz it's that
deep anyway um yeah so a couple of
things I'd want to CL out from that
especially for people who are kind of
familiar with other network but not
Transformers the first we've already
discussed the residual stream as the
central
object and the second is how to think
about attention because attention is the
main thing which is weird about models
they have these MLP layers um which
actually represent like 2/3 of the
parameters in a Transformer which is
often an underrated fact but attention
is the interesting stuff so Transformers
have a separate residual stream for each
input token and this contains like all
memory the model wants to store at that
position um but MLP layers can only
process information in place you need
attention to move things between
positions
and classically people might have used
stuff like a 1D convolution you average
over 10 things in the sliding window um
this is baking in the inductive bias
that nearby information is more likely
to be useful but this is kind of a
pretty limited bias to bake in and the
story of deep learning is that over time
people have realized wait we should not
be trying to force the model to do
specific things we understand we we
should not be telling the model how to
do its job if it has enough parameters
and is competent enough it can figure it
out on its own and the idea here is
rather than giving it a convolution you
give it this attention mechanism where
each token gets a query saying what it's
looking for each previous token gets a
key saying what it has to offer and the
model looks from each destination token
to the source tokens earlier on with the
keys that are most relevant to the
current query and models and the way to
think about an attention head so
attention layers break up into these
distinct bits called heads which act
independently of the others and add to
their outputs together and just directly
add to the residual string uh this is
sometimes phrases concatenate their
outputs and then multiply by a map but
this is mathematically
equivalent um the each head acts
independently and in parallel and
further you can think of each head as
separately breaking down into a which
information to move bit determined by
the attention which is determined by the
query and key calculating matrices and
the what information to move once I know
where I'm looking which is determined by
the value and output
matricies um we often think about these
in terms of the qk Matrix WQ * WK
transpose and the OV Matrix uh wo * WV
because there's no nonlinearity in
between um and these two matrices
determine like what the head
does and the reason I say these are kind
of independent is
that once the model has decided which
source tokens to look at the information
that getss output by the head is
independent of the destination
token and like the query only matters
for choosing where to move information
from and this can result in interesting
bugs like um there's this motif of a
skip trigram the model realizes that hm
if if the current thing is three and two
has appeared in the past then four is
more likely to come next if the current
thing is three and four has appeared in
the past two is more likely to come next
but if you have multiple destination
tokens that all want the same Source
token for example the phrase keep in
mind can be a skip trigram um really it
should be a trigram but tiny models
aren't very good at figuring out what's
exactly the previous position keep at
Bay is another trigram but in and at
will both look at the same keep token
and so they must boo boost both at and
mind for both of them so we'll also
predict um keep in Bay and keep at uh
keep at
mind and yeah
and possibly we should move on to
induction heads which are a good elative
example I was yeah I was going to come
on to that so on these induction heads
um you've said that they seem Universal
across all models they underly more
complex Behavior like f shot learning
they emerge in a phase transition and
they're crucial for this in context
learning and um you said that sometimes
specific circuits underly emergent
phenomena and you know we may want to
predict or understand emergence by
studying these circuits so what do we
know so far a lot of questions in there
all right all right taking this an order
so what is an induction head I've
already mentioned this briefly um text
often contains repeated subsequences
like after Tim scarf may come next but
if Tim scarf has appeared like five
times then it's much more likely to come
next um in toy two layer attentionally
language models we found this circuit
called an induction head which does this
it's a real algorithm that works on say
repeated random tokens and we have some
mechanistic understanding of the basic
form of it where there's two attention
heads and two different layers working
together um this the later one called an
induction head looks from Tim to
previous occurrences of
scarf um the first one is a previous
token head which on each scarf looks at
what came before and is like ah this is
a scarf token which has Tim before and
then the induction head looks at tokens
where the token before them was Tim or
where the token before them was equal to
the current
token
and when the attention induction head
decided to look at scarf the which is
determined purely by The qk Matrix it
then just copies that to the outut which
is purely done by the OV Matrix and I
think induction heads are a really
interesting circuit case study
because induction heads
are all of the interesting computation
is being done by their attention pan
like Tim scoff could be anywhere in the
previous context and this algorithm will
still work and this is like important
because this is what lets the model
do tracking of long range dependencies
in the text where it looks far back um
and you can't bake this in with a simp
simple thing like convolutional there
um in fact Transformers seem notably
better than old architectures like lstms
and rnns in part because they have
induction heads that let them track
longrange
dependencies
and yeah um and more generally it often
is the case that especially lat layer
attention heads the OV bit is kind of
boring it's just copying but figure out
where to look is where all of the
interesting computation lies
so so first of all just to clarify
because people will know what an
attention head is but an induction head
is one of these circuits that that
you're that you're talking about just so
people understand and um we should get
on to this relationship between
induction heads and the emergence of in
context learning and also you said it's
very important that you know we have
this scientific understanding um you
know with respect to studying emergence
but rather that than just framing of
interpretability kind of makes better
models mhm yeah
so okay so maybe I should first explain
what emergence is let's do that in could
I'd be really really interested if you
could just give me the simplest possible
explanation of what you think emergence
is sure emergence is when things uh
happen suddenly during training and go
from not being there to being there
fairly rapidly in a convex way rather
than gradually
developing it's interesting you said
that because I think of emergence as a
surprising change in macroscopic
phenomena and it's an observer relative
term which means it's it's always from
the perspective of another
scale
H so just a transient change in per in
perplexity or capability or something in
my mind wouldn't entail emergence like
it would need to be some qualitative
meaningful thing rather than just oh the
loss curve got notably better in this
bit I think so it's definitely related
to some notion of surprise M which is
inherently
relative
um yeah let's not get hung up on that so
okay it's it's let's say it's a
transient change in something MH yeah uh
you when call Trans it's like a
unexpected Sudden Change unexpected has
so much semantic meaning on it that I
don't want to use but yeah this is an
infinite Rabbit Hole yes but I think um
the scale thing is is relevant as well
so we are programming neural networks at
the microscopic scale and there's some
macroscopic changing capability so it's
some yes um yeah yeah and there's like
lots of different dimensions you can
have emergence on you can have it as you
train a model on more data you can have
it as you make the models bigger and
these are both interestingly different
kinds one of the more famous examples is
chain of thoughts and F shot
prompting where dd3 is pretty good at
this earlier models were not good at
this this was kind of surprising CH of
for is particularly striking because you
people just noticed a while after dp3
was public that if you tell it think
step by step it becomes much better um
there's this recent innovation of tree
of thought that I'm not particularly
familiar with but I understand is kind
of like applying Monte Carlo tree search
on top of Chain of Thought yes um yes
where you're like well there's many ways
we could Branch at each point let's use
tree search algorithms to find the
ultimate way of doing this yeah but with
um let's say scratch pad and Chain of
Thought I don't necessarily see that as
an emergent well maybe it is maybe
there's an emergent reasoning capability
that comes comes into play when you have
a a certain threshold size model but I
think of it more as kind of having an
intermediate argumented memory in the
context so you're you're kind of filling
in a gap in cognition by saying you're
allowed to it's not just remembering
things it's also reflecting on things
that didn't work yes so yeah clarifying
when I say emergent when I say t of
thought is an emergent property I mean
the Capac capity to productively do
chain of thoughts is the emerging thing
and telling the model to think step by
step is a user-driven
thing yes but I don't know I kind of
just as a point of order though was it
just that it was discovered after CH
after gpt3 or would it work on
gpt2 uh I would have guessed it doesn't
work very well on gpd2 but I've not
checked I'd be pretty interested
I'm sure someone has looked into this I
haven't looked very hard uh I guess like
so a lot of my motivation for this work
comes from I care a lot about AIX risk
and AI alignment and how to make these
systems good for the world and when I
see things like oh we realized that you
can make dd3 much better by asking to
think step by step I'm like oh
no um what kinds of things could the
systems we make be capable of that we
just haven't noticed yet that's the
concern that the the genie is already
out the bottle and I mean deep mine just
published this tree of thought paper and
it's really simple idea it's basically
AAR search over trajectories of prompts
and you use the the model itself to
evaluate the value of a trajectory and I
could have done that anyone could have
similar thing with auto GPT and all this
stu um I'm more skeptical than you are I
I think in the case of tree of thought
it closes a capability Gap in respect of
certain tasks which were not working
very well because they don't have that
kind of system to models don't seem to
plan ahead very well but I still think
that it it's not just going to magically
turn into super intelligent I mean we
can talk about this a little bit later
but yeah okay yeah so yeah I think this
is also pretty relevant to like much
more near-term risks like yeah I don't
know there's lots of things that a
sufficiently capable model could do that
might be pretty destabilizing to society
like write actually much better
propaganda than human writers can or
something and if tree of thought makes
it possible to do that in a way that we
did not think was possible when gbd4 was
deployed that's like an interesting
thing that I care about noticing it's
not a very good example but it yeah it
is
um but being able to I mean first of all
it's been possible to create
misinformation for a long time time this
is why I specified be able to do it
notably better than humans
can I totally agree the doing it a bit
more cheaply and a bit more scale
doesn't seem obviously that important
You could argue that like I don't know
being being a Spam bot that feels
indistinguishable from a human is like a
more novel thing that's actually
different yeah but I know this is like
an offthe cuff example I don't want to
get too deep into this cuz it's not
point I care that deeply about yeah I
mean we we can come back to it in a bit
but I think we are nearly already there
yeah you know this irreversibility thing
we we don't
know uh computer games are photo
realistic chat Bots are
indistinguishable and AI art is pretty
much
indistinguishable and that could work I
mean I spoke to Daniel dennit about it
last week and he said he's really
worried about the epistemic erosion of
our society more so interestingly than
the ontological erosion and I discovered
later that's because he he's not a big
fan of anything ontological but um yeah
it's it it is potentially a problem but
I guess to me people might overestimate
the scale and magnitude of change of of
this I feel that I know I don't want to
Echo Sam Alman here but he said that we
are reasonably smart people and you know
we can we can adapt and recognize um you
know deep fakes and so on but yeah yeah
these are complicated societal questions
I guess I mostly just have the position
of man it sure is kind of concerning
that we have these systems that could
potentially pose risks but we don't know
what they do and decide to deploy them
and then we discover things they can do
and I think that the research Direction
I'm trying to advocate for here is just
better learn how to predict this stuff
more than anything which hopefully we
can all agree is like an interesting
Direction and there's all kind of
debates about is Merchant phenomena like
actually a real thing like this recent
is is this Mirage paper which I think
was a bit over claiming but does make a
good point that if you choose your
metric to be sufficiently sharp
everything looks
dramatic um one thing I've definitely
observed is if you have an accuracy
graph with a log scale x-axis for
grocking it looks fantastically dramatic
and I was very careful to not do this in
my paper because it is
cheating um but yeah um so my particular
hot take is that I believe emergence is
often underlain by the model learning
some specific circuit or some small
family of circuits in a fairly sudden
phase transition that enables this
overall emergent thing and this SQL
paper led by Katherine Olson in contest
learning in induction heads is a big
motivator of my belief this so the idea
of the paper is we have this we found
induction heads in these toy till
attentionally models we somewhat
mechanistically understood them at least
in the simplest case of
induction um we use this to come up with
more of a behavioral test for whether
it's induction head you just give them
all repeated random tokens and you look
at whether looks induction e and we
found that these occurred in basically
all models we looked at up to 13B even
though we didn't fully reverse engineer
than there and we then found that this
was really deeply linked to the
emergence of in context learning it's a
lot of jargon in there let's unpack that
in context learning already briefly
mentioned it's like tracking long range
dependencies and text like you can use
what was on which was three pages ago to
predict what comes next next in the
current book which is a non-trivial
thing it is not obvious to me how a
programmer model to do Inc context
learning is
emergence if you operationalize it as
average loss on the 500th token versus
average loss on the 50th token there's a
fairly sudden period in training where
it goes from not very good at it to very
good at
it just a a tiny point of order there
one interesting thing about Inc context
learning is you're learning at inference
time not training time yes but you're
not changing anything in the underlying
model which means anything it can do
presumably must be materializing a
competence which was acquired during
training so it's coming back to this
periodic table thing right so it's
learned all these platonic Primitives
you do this in context learning you say
I want you to do this here's an example
and it kind of you know you've got all
of these freeze dried periodic
computational circuits and they spring
into life life and they compose together
and they do the thing yes yes yeah I
think induction heads are to my eyes the
canonical example of an inference time
algorithm stored in the model's weights
that gets
applied and I'm sure there's a bunch
more that no one has yet found um and
yeah a lot of my model is that prompt
engineering is just telling the model
which of its circuits to
activate and just engaging with various
quirks of training that have made it
more or less durable in different
ways and yeah so induction heads also
emerge in a fairly sudden phase
transition and we and exactly at the
same time and we present a bunch more
evidence in the paper that there's like
actually a causal link here um like
oneel models have neither the in context
learning or the induction heads phas
chain because they can't do induction
heads cuz they're only one layer and why
but if you you adapt the architectures
they can form induction heads with only
one layer now they have both of these
phenomena if you aate induction heads in
contact learning gets systematically
worse and a particularly fun qualitative
study was looking at soft induction
heads heads that seem to be doing
something induction e in other domains
like a head which attends from the
current word in English to the thing
after the current word in French
or more excitingly a few shot learning
head on this random synthetic pattern
recognition task we made where it tended
back to the most relevant examples to
the current in to the current one and my
interpretation of all this is that
there's something fairly fundamental
about the induction algorithm for in
context learning so the way I think
about it let's say you've got two
um you want to learn some relation you
got some local context a and some past
context B and if You observe a and You
observe B in the past this gives you
some information about what comes next
um there's two ways this could work out
it could be symmetric B helps a and a
helps b or asymmetric B helps a but a
does not help B if they're the other way
around asymmetric might be like knowing
the title of a book tells you what comes
next but knowing what's in a random
paragraph in the previous bit doesn't
tell you the title um while symmetric is
like and no English sentence helps
French sentence French sentence helps
English sentence and if you have like n
symmetric relations like English French
German Dutch Latin whatever where each
of them helps each other this is really
efficient to represent because rather
than needing to repr present n squ
different relations separately like you
would in the asymmetric case you can
just map everything to the same latent
space and look for
matches and fundamentally this is what
induction heads are doing they're
mapping current token and previous token
of thing in the past to the same latent
space and looking for
matches and to me this is just like a
fairly natural primitive of
attention and this is exciting because a
we found this deep primitive by looking
at toy to L attentionally models B it
was important for understanding and
ideally for predicting the emergent
phenomena of in context learning
and two takeaways I have from this about
work we should be doing the first we
should be going harder at looking at toy
language models I open sourced a scan of
12 of them and I'd love to see what
people can find in one layer models with
MLPs because we really suck at
Transformer MLP layers and one layer
should just be easier than other ones
and the second thing is I really want a
better and more scientific understanding
of emergence why does that happen really
understanding particularly notable case
studies of
it testing the hypothesis that it is
driven by specific kinds of circuits
like induction heads
um or at least specific families of
circuits even though I don't know you
could argue that because we haven't
fully reverse engineered the things and
the larger models so we really know it's
actually an induction
head
and yeah um more generally a lot of my
vision for why mecup matters is this
kind of scientific understanding of
models like I don't care about making
models better but I care about knowing
what's going to happen knowing why stuff
happens achieving real understanding and
getting a scientific understanding of
things like
emergence seems like one of the things
meub might be uniquely suited to do but
also no one checked very hard and you
dear listener could be the person who
checks so there was a paper by Kevin
Wang etal called interpretability in the
wild a circuit for indirect object
identification in gpt2 small which found
a circuit for indirect object identif
ification so um they discovered um
backup name mover heads which normally
don't do much they take over when the
main name movee or ablated and they said
uh mechanistic interpretability as a
validation set for more scalability uh
techniques they've understood a clear
place that these ablations can be
misleading
so yeah so yeah bunch one pack in there
so I really like the interpretability in
the wild paper uh also Kevin was only 17
when he wrote it and like man I was
doing nothing remotely as interesting
when I was in high school so it props to
him um but uh also a sign of how easy it
is to pick lwh hanging fruit and do
groundbreaking interpretability
work um such a young
field um I know it's so impressive yeah
I've just checked his
Twitter Hey Kevin
and yeah so to me the underly yeah so
that's zooming out a bit I think there's
a family of techniques around causal
interventions and their use and Mech
Mech that's useful to understand here
so the core technique is this idea of
activation patching where so let's H so
one of the problems with un
understanding a model's features and
circuits is models are full of many many
different circuits each circuit does not
activate or many inputs but each
circuits will activate uh but on each
input many circuits will activate and in
order to do good mechan tur work you
need to be incredibly Surgical and
precise which means he needs to learn
how to isolate a specific circuit and
let's consider a statement like
um
uh the Eiffel Tower is in Paris versus
the Coliseum is in Rome these are both
there's lots of features happening where
there's lots of circuits being activated
on the Eiffel Towers in Paris this is in
English you doing factual recall you are
outputting a location you are outputting
a proper noun this is a European
Landmark etc etc and like I want to know
how the model knows the eiel towers in
Paris but the col in Rome controls
almost everything apart from the fact
and so um what I can try to do is
causely intervene on the Coliseum run
and replace say the output of an
attention head with its output on the
Eiffel Tower prompt and see how much
this changes the answer from Rome to
Paris
and this um yeah this patch
um can let me really isolate how the
circuitry for just this specific thing
works and there's all kinds of work
around this obnoxiously all of it uses
different notation like uh resample
ablations and causal tracing and causal
mediation analysis and interchange
interventions all similar words for
basically the same thing um but yeah um
the really key Insight here is this kind
of surgical intervention a classic
technique and interpretability is
ablations where you just set something
to zero and it's kind of janky because
if you break something in the model
which wasn't interestingly used for the
task then everything dies or if you
break it an interesting ways everything
dies uh for example in gpd2 small almost
every single task breaks if you delete
these zero Earth MLP
layer um yeah as far as I can tell the
Zer MLP layer has kind of an extended
embedding um GPD too small has tied
embeddings and unembedded so they're
transposed of each other which is wildly
unprincipled in my opinion and the model
seems to be both using this for just DET
toiz and combining nearby things with
the first attention layer at zero the
tension layer and just undoing the
tightness um but but this means that
basically everything is reading from
that and I've seen people do zero
Revelations and everything and be like
oh this is an important part of the
circuit let's get really sidetracked by
this um because the effect size is so
big yeah man being a Mech research fills
my mind with such bizarre trivia like
this it's great models so bizarre um and
so yeah um this Cal intervention um
there's kind of two conceptually
different kinds of interventions you can
take the Eiffel Tower prompt patch in
something from the Coliseum and see if
it breaks the ability to Output Paris to
verify which bits kind of are necessary
such that getting rid of them will break
something or you can patch something
from the Paris run into the Coliseum run
and see if that makes that output Paris
which is testing for stuff that's
sufficient um I call the first one a
resample oblation
because you're messing up a component by
resampling and the second one um Den
noising or causal tracing because you're
intervening with like a bunch with like
a bit of information and seeing if that
is sufficient for everything else though
none of these names are good I would
love someone come with the better
names and there's all kinds of families
of work building on this like um I have
this post called attribution patching
that tries to apply this as Industrial
scale by using gradients to approximate
it um which is fast enough that you
could take GPD 3 and it's 4 million
neurons and do attribution patching on
all neurons at once on every
position uh great great Post Redwood
research has this um technique called
causal scrubbing which I view as uh
activation patching gone incredibly hard
and rigorous that tries to come up with
an automated metric for saying um this
hypothesis about a model is actually
accurate for how it
works um where it's kind of complicated
but the core idea is you think of a
hypothesis as saying which resample
oblations are allowed and you make all
of the resample oblations that should be
allowed like um these components of the
model shouldn't really matter so we can
just patch in stuff from random other
inputs um if you've got say induction
head you might think the induction head
cares about the current token um and the
thing before the previous to the thing
before the past token that it's going to
indu it's going to induction attend to
so let's replace the um token that it's
going to be attending to with a token
from a different input but with the same
token before it my hypothesis about the
induction head says this should be
allowed so let's do that I wouldn't want
want to induce a rent but the the metric
you use is really
important uh yes uh this is one of my
hobby horses so
um some of the original work looking at
the patching stuff like David ba and
Kevin mang's excellent uh Rome paper
uses the probability of Paris as their
metric and there are other papers that
use things like accuracy as their metric
yeah and generally I think of metrics as
being on a spectrum from like soft to
Sharp so generally I think of models as
thinking in log space um they are kind
of acting like
beion they um are trying to figure out
something's in Paris and there'll be
five separate heads that each contribute
one to the correct logits and each of
these can be thought of as like one bit
of information and to get together they
get you the right probability of
say8 but if you patch in each one in
isolation the probability changes
negligibly because probability is
exponential in the log logits so using
probability you're like oh this this
head patch doesn't really matter and so
in this paper they did this thing of
patching in like 10 adjacent layers at
once and to me a really core principle
of this kind of causal intervention and
mechanistic Tech technique that you want
to be as surgical as possible to be as
deeply faithful as possible to what the
Neuron model is actually doing so in
this case there was an interaction
between them they were effectively
making several interactions or
interventions at once um yes yeah they
were like replacing 10 adjacent layers
and I don't know patching things in
different layers is always a bit weird
uh I don't think that part's that
objectionable I mostly just feel like if
you choose a metric like log prob it
allows you to be much more surgical
about how you intervene mhm uh oh it it
allows you to identify subtle effects of
things accuracy is even worse because
accuracy is basically rting things to
zero or one so like if the threshold is
2.5 any individual patch does nothing
any resample ablation does nothing um
but if you patch in like the 10 adjacent
layers it will do everything and and
this can be kind of misleading another
one I often see people do is um
they trying they look at things like the
the rank of an output like at which
point does the model realize Paris is
the most likely next token and this can
be super misleading because this will
make you think the third head is the
only head that
matters um when really all five of them
matter the order is kind of arbitrary
and yeah I've seen papers that I think
got somewhat misled by using metrics
like
this
and metrics they matter so much it's so
easy to trick yourself my high level
pitch is just Mech tur is great mechant
tub is beautiful also the field is
incredibly young there's maybe 30
full-time people working on it in the
world there's a ton of low hanging
fruits I've done major research in this
field I've been in it for like less than
2 years
um I would love people to come and help
and help us solve problems and do
research here and we'll link to my post
on getting started and my sequence
called 200 concrete open problems in the
description to this hopefully and yeah I
think there's just it's not that hard to
get started it's really fun hopefully
I've nerd sniped you with at least one
thing in this podcast
and if it really vaguely curious it's
just really easy to open one of the
tutorials linked in my posts and just
start screwing around and I'd love to
see where you can
find beautiful um also the Deep Mind
alignment team is currently hiring and
people should apply which includes
hiring for mechanistic interruptibility
team amazing do they have to do lead
code uh I have no idea can't remember
yeah yeah we um we did an amazing video
with uh to fit
kovich um I gave him one of my leco
challenges and annoyingly he aced it oh
I'm all that it's all that Deep Mind
interview interview practice anyway okay
let's talk about super intelligence now
um I spoke with um our mutual friend
Robert Miles about a month ago Rob so
great he's he's a lovely chat spoke all
about alignment and he accused me of
over philosophy everything because I was
talking all about intelligence one of my
favorite topics and and he said well
what about
fire fire is something that people
didn't understand Millennia ago but they
knew that it burnt and they knew that it
was bad and this is like this is like a
fire which is very interesting and maybe
we can bring in a little bit of
effective altruism as well so um you
I please do please if there is one thing
I have learned from the past decade of
machine learning progress is that you do
not need to understand a thing in order
to make it and
thiss to are us and are capable ofing to
catastrophic risks yes yes well let
let's um I'll I'll step back a tiny bit
and then we'll and then we'll get there
because there there's the hypothetical
nature which I guess I have a bit of a
problem with now about 10 years ago I
was one of the first supporters of Sam
Harris's podcast and and he's quite
aligned to EA and um he was talking
about um this very Noble idea that
everyone matters equally and people on
the left should get on board of that
intrinsically and this idea that we
should quantitatively analyze the impact
of charity work and solve an
optimization problem and earning to give
and a lot of the stuff that um masul
spoke about and also philosophers like
Peter Singer and the focus seemed to be
primarily on alleviating poverty which
we and we are we don't say the biggest
problem we say a problem this is another
thing uh our friend Robert mil said he
said the problem is when people talk
about the
problem there can be more than one
problem but anyway so um it's a big it's
a big problem and um recently you and I
can agree that EA circles have really
laser focused in on existential risk
from AI as opposed to other more
plausible X risk concerns like pandemics
or even nuclear war not not to say that
they don't focus on that but I am going
to push back on other more plausible XIs
go on go on uh I just wanted to register
objection to okay so and you know
cynically from from my point of view I
see I see the influence of elizza
Bostrom Hansen Etc kind of Shifting the
the focus on to X risk and part of part
of the reason for that is also this kind
of overly intellectual focus on
long-termism and it's done in a very
intellectualized way so it's based on
the utility function now
incorporating future simulated humans on
different planets you know a long time
away in the future and making all of
these intellectual jumps so let's let's
start there what your all right so much
stuff to respond to in there good so all
right a couple things uh the first so Cs
on the table um I care a lot about AI
existential risk yes the reason I work
on mechanistic interruptibility is
because I think that understanding the
mysterious black boxes that are
potentially smarter than us and may want
things wildly different than what we
wanted them to want is just clearly
better than not understanding them yes
and I think mechanistic interruptibility
is a promising path here so and
I also would consider myself an
effective alist and a rationalist so mhm
Cs on the table those are my biases um
so I generally think it's more
productive to discuss is AI catastrophic
and existential risk a big deal than is
it the biggest deal or is it worth more
resources on the margin than Global
poverty or climate change or AI ethics
and like there's just lots of problems I
care way more about convincing people
that AI ex risk should be in your top 10
than it should be in your top one
because I feel like for most people it's
not in their top
thousand and there's just so much
divisiveness between say the AI ethics
community and the AI alignment Community
about whose problem is a bigger deal and
like both are big problems why are we
arguing and well part of this is about
our moral intuitions and this is
something I spoke a lot with Connor
about you know he said that many ways
he's got this technical empathy so
sensory empathy is I really care about
my family they're these concentric
circles of moral status I really care
about my family and if I try really hard
I can care about people in other
countries and so on and then if I try
really really hard I can care about
future simulated lives on Mars and
Connor said the idea of this movement is
about Galaxy braining yourself into
being the most empathetic person
imaginable but it's a kind of empathy
that people don't
understand yeah so okay so a separate
bit of beef I have is with the entire
notion of long-termism right so
long-termism is this idea uh okay so
long-termism is generally caring about
the long-term future yeah there's like
the strong form of value in the future
basically entirely dominates things a
day and weaker forms of just this really
really
matters and and um a common
misconception about AI ex risk and AI
safety is that you should only work on
this if you are a long- termist that you
know it's a one in a billion chance of
mattering but there's um a quintilian
future Liv of this outweighs everyone
alive today in moral Worth or well we're
only going to get AGI in like 500 years
but we're going to work on it now just
in case and like I think both of these
are just nonsense um like I guess as a
concrete example um effective outs have
worked on pandemic prevention for many
years and I think it was just clearly
the case that pandemics are a major
threat to people alive today and I like
to feel that we've been proven right no
one's going to argue at that point and
you know everyone's being like effective
at why are you working on AI safety this
obviously doesn't matter um you know I
feel like we got one thing right um Can
can I be really skeptical though for a
second
because I mean you're working for deep
mind there's so much Prestige and money
attached to AI risk Elon Musk is talking
about it all the time whereas you could
be a scientist working on pandemic
response responses and it just I mean
let's be honest it wouldn't be anywhere
near the same level of
prestige yeah so
couple of
takes it definitely is the case that I a
good chunk of why I personally am
working on AI X risk rather than say bio
X risk is that I a smart mathematician I
like AI I like Mech andup I do not think
I would be good at biology in the same
way and I also would personally assert
that AI ex risk is more important
important and like more pressing but you
know I'm biased and I think it's fair to
flag that bias um in terms of
prestige so I've only really been
working on this stuff properly for the
past two and a half years which is I
mean it's changed dramatically like in
the last six months we've gone from well
we really ever going to get AGI to oh my
God gd4 exists Jeffrey Hinton has left
Google to loudly advocate for X risk
Yoshua Benjo is now loudly advocating
for X risk it's 2third of the touring
winners for deep learning you'll never
get the third one uh yeah we're never
we're going to get the third
one Yan L has made his position very
very clear yes um but you know it's a
majority I'll take it yes and or or the
fourth one yeah yeah um he's coming on
our podcast actually uh oh who was the
fourth one Schmid Huber ah yeah yeah
seems hard I'm very curious to hear the
Schmid your episode oh yeah he's even
more verdantly against than Yan I'm
afraid to say so two out of two two out
of four I'm interested to hear anyway so
yeah
um
and yeah in terms of prestige I don't
know I gather that say seven years ago
it was basically just not it would be
like pretty bad for your career you
would not be taken seriously if you
mentioned the carry about a risk your
paper would be rejected I hear a story
of um Stuart Russell at one point talked
to a grad student of his about how
Stuart was concerned about a risk the gr
was also really concerned and freaking
out but they've been working together
for years and neither had felt
comfortable mentioning it and a lot of
people who are still in the field were
doing this stuff then which makes me
somewhat reject the prestige arguments
at least for senior people in the fields
I think there's a difference with Stuart
Russell in particular he's very credible
mhm and he and I'm not oh I didn't mean
I didn't mean you I was talking I was
talking about the two um Godfather
because the thing that um maybe I
shouldn't say this but I was surprised
that Benjo and Hinton came out in the
way they did and I I the reason I didn't
like what they said was I felt that they
were implying that current AI technology
could pose an existential threat and
what I'm getting from you you and what
I'm getting from Russell is and also
from Robert miles is that this is a very
real potential um threat in the future
but it's not a current
threat yes very real Potential Threat in
the future though I hesitate to
confidently assert say this will not be
a threat in the next 5 years or
something it's like pretty hard to say
interesting um I'm not confident I agree
with your assessment of of banio and
Hinton though they've spoken a bunch
publicly so I'll defer if you can point
to specific writings but for example
benio signed the pause AI for 6 months
more powerful than gbd4 letter and I
don't know I don't think the letter was
asserting that the letter definitely
wasn't asserting GPD 4 was an exential
risk it wasn't confident asserting GPD 5
would be but being like yeah we need
more time and slow down and caution it's
yeah maybe I'm reading too much into
that but it seemed to to me that I mean
um Hinton said that chat GPT now
contains all of the world's knowledge
and this chatbot knows everything and it
could potentially do very harmful things
and I I interpreted it possibly in
correctly that they were talking about
reasonably current or next Generation
risks fair I mean I can't talk for them
I also I don't know there are lots of
near-term risks there's long-term risks
I consider it my job to think hard about
the long-term risks and try to guard
against those yeah and I think lots of
other people's jobs is to focus on like
the near risks and both are like great
forms of work um oh no one reason I like
I well could I could I ping you just a
couple of quick questions so first of
all you know there's this idea of
negative utilitarianism I mean do you
think minimizing suffering is more
important than maximizing
happiness nah no not sure I've got a
more deep answer than that I mostly
think a lot of this intuitive reasoning
is more driven by intuition than
anything else but it's a bit like this
metrics thing we were talking about you
know which which is that if you want to
have would you like to tolerate some
spiky negs for some average
happiness yeah so I know I have like a
general
frustration with these discussions
getting too
philosophical um this is big issue when
I hang out with faec of alterists who
really love moral philosophy and
population ethics yes uh I know I have
this EA Forum post called simplify EA
pictures to holy  X risk I'm just
like so I don't know um if you actually
look at some of the concrete work people
Tred doing on things like timelines and
risk there's this uh report from AJ
cotra at open philanthropy that gives a
30-year median timelines to trans to AI
That's
transformative uh which he since updated
to 20 years there's a report by Joseph
kith that estimates about a 10ish per
chance of a major catastrophe from this
yeah and if you just take those numbers
this is clearly enough to reach pretty
high in my list of concerns of people
alive today okay okay and I think these
are bold empirical claims and I think
it's great to debate them in the
empirical domain but to me this doesn't
feel like a moral question it just feels
like from Common Sense assumptions if
you believe these empirical claims the
stuff is a really big deal okay okay let
let's let's take another couple of steps
so first of all we we save this till
later um I think deception is very
important and Daniel Dennett when I
spoke with him he uses this notion
called the intentional stance which
basically means that if you use a
projection of purposes goals agency Etc
in order to understand the behavior of
an agent possibly a ulated agent then
for all intents and purposes it it has
agency it can make decisions it has
moral status has lots of different
things like that and he would say that
without an intentional stance without
agency it's impossible for a model to
lie or deceive us now what do you think
would be the bar for something like a
GPT model to deceive us and why yeah
so before I give takes I will generally
reinforce Rob's Vibe of well if you have
no idea how fire works but you know that
it burns you that's kind of the
important thing like maybe a model has
just this random learned adaptation to
Output things that are designed to get a
user to feel and believe a certain way
that isn't intentional and isn't
deceptive in some true cogy sense but
it's like enough for this to be a big
deal that we should care a lot about
okay with that with that aside yeah um
yeah
so I'm definitely hesitant to ascribe an
overly confident view of what's going on
here um and I think lots of early
discourse alignment around things like
utility
maximization and around things
like these things are just paperclip
maximizers Etc is kind of
misleading and I don't think it is an
accurate model of
how gpt7 rlh f+++ is going to work well
that's my prediction um one thing that
is pretty striking to me is I just feel
like we're pretty confused on both sides
of this like I do not feel like I can
confidently claim that these models will
demonstrate anything remotely like goals
or intentions but it also don't feel
like you can confidently claim that they
w't and I'm not talking like
99.99% confidence I'm talking like 95%
plus confidence either way and one of my
Visions for what being good at meup
might look like is being able to
actually get grounding for these
questions because I think ultimately
these are mechanistic questions
behavioral interventions are not enough
to answer like does this thing have a
goal in any meaningful
sense but yeah my like very rough soft
definition would be is the model capable
of forming and executing long-term plans
towards some goal potentially if
explicitly prompted to like Auto GPT or
just
spontaneously is it capable of actually
carrying out these plans and does it
form and execute plans towards some
objective that is like encoded in the
model
somewhere um and I don't know I think
it's pretty plausible that the first
dangerous thing is like chaos
gpt7 where someone tells it to do
something dangerous and it gets misused
more so than it's like misaligned and I
care deeply about both of these
risks okay so yeah first one's more of a
governance question than a technical
question and thus is less where I feel
like I can add value so I agree with you
on all of that so yeah being less
confused about what's going on inside
the models and great you know using
interpretability to figure out whether
they actually do have agency or goals
and sometimes they do the right things
for the wrong reasons auditing models um
that seem aligned before they're
deployed is something that you you've
told me before so great and um you know
just being able to check more deeply
that it truly is aligned but I wanted to
talk a little bit about um this
interesting paper from Kya GCE MH so she
wrote a response called it was on the L
wrong funking the AI apocalypse a
comprehensive analysis or
counterarguments to the basic AI risk
case X risk and the reason I read it is
um so many of the comments were
destroying uh me and dugar after we
interviewed Rob and they said well if
you're going to criticize s x risk I
mean at least go and read KY Grace's
response so I did so I did here we go so
uh she she basically made two big
counterarguments that intelligence might
not actually be a huge advantage and and
about the speed of of um growth is is
ambiguous but I first want to touch on
what you said before which is about this
notion of gold directedness so alignment
people say that if superhuman AI systems
are built any given system is likely to
be go directed and the orthogonality
thesis and instrumental goals are cited
as aggravating factors and um the goal
directed behavior is likely to be
valuable so economically gold directed
entities May tend to arise from machine
learning training processes not
intending to create them which is kind
of talking about some of the emergent
behaviors that we were talking about
earlier with respect to orell for
example and coherence arguments May
imply that systems with gold
directedness will become more strongly
go directed over time which is
apparently something that is argued for
so I'm thinking what does goal even mean
I mean we we anthropomorphize abstract
human intelligible Concepts like goals
and they they're they really are
emergent because they emerge from these
low-level interactions in the cells in
your body and then you get these things
that we recognize to be goals Observer
relative as we were talking about before
but they're just graduated phenomena
from smaller things right so what does
it even mean to have a goal yeah
so couple of thoughts on that again you
ask questions with a lot of content in
them um no problem I can I can only
apologize I mean there someone who
accidentally writes 19,000 blog 19,000
word blog post all the time I relate um
anyway
so what am I saying um so the way yeah
it's a vague concept right yeah so I
definitely want to try to
take so there's a mechanistic definition
of the model forms plans and it
evaluates the plans According to some
criteria or objective and it the plans
that score better on this and I would
love if we get to a point where we can
look inside a model and look for
circuitry that could be behind this or
not um that would feel like a big
milestone for me on wow I really believe
mechal matter for reducing catastrophic
risk from
AI um a second thing is
that um yeah the more behavioral thing
of the model systematically takes
actions that pushes the world towards a
certain State and I don't want I think
there's a common problem in alignment
arguments where people get too precise
and too specific in a way that lots of
people reasonably object to and a way
which not necessary for the
argument um there's a really great paper
called the alignment problem from a deep
learning perspective perspective by
Richard no Lawrence tan and S minderman
and this is probably my biggest
recommendation for the listening
audience of what I think is like a
pretty well-presented case for
alignment and I generally pretty Pro
trying to make the minimal necessary
assumptions so for me it's kind of like
some soft form of gold
directedness of take actions that push
the world towards a certain State
and another important thing
is there are a bunch of theoretical
Arguments for why goals would
spontaneously emerge um ideas around in
a misalignment uh from work led by Evan
hinger um ideas around just coherence
theorems and things like that which I
don't know I find like a bit convincing
not that convincing but then there's
things will have goals because we try to
give them goals and I'm like yeah that's
probably going to happen
um it's just clearly useful if you have
uh if you want to have an AI CEO or a AI
helping run logistics for military
operations to have something that's
capable of forming and executing
long-term plans towards some objective
and if you believe this is what's going
to happen then the key question is are
we capable of ensuring those goals are
exactly the goals we would like them to
be mhm and my answer for any question of
the form can we precisely make sure the
system is doing exactly X of machine
learning is God no we are not remotely
good enough to achieve this with our
current level of alignment and Searing
techniques and to me this is like a more
interesting point where it's not quite a
Crux for me but it just seems like a lot
easier to argue about will people do
this yeah essentially I mean ktia
herself said that it's um it's unclear
that gold directedness um is favored by
economic pressure training Dynamics or
coherence arguments you know whether
those are the same thing as kind of goal
directedness that implies a zealous
drive to control the universe and look
at South Korea they they have goals and
those goals I I don't really subscribe
to the um to the dictator view of
society I assume they are somehow
emergent yes and similarly sorry South
Korea or North Korea sorry North Korea
did I say South Korea I I me North Korea
s very different Korea yes different
different goals different goals but um
but but you can think about goals in in
an AI system as either being ones which
emerge from some low level um or ones
which are explicitly coded by us or ones
which are instrumental MH right and
these are all a whole bunch of goals y
but we can't really control those we can
add pressures how do we control what
North Korea does uh that sure is a
question I'd love for someone to answer
um I I don't know I like I can give
speculation there's like there's the
question of in practice what do people
do which is basically reinforcement
learning from Human
feedback um and I expect people would
apply that in this situation as well I
definitely do not believe we would be
able to explicitly encode a goal in the
system mhm um moreover even if you can
encode even if you could give some like
scoring
function like make the score in this
game high this does not give you a model
that intrinsically cares about that in
the same way that I don't know Evolution
optimizes inclusive genetic fitness I
don't give a  about inclusive
genetic fitness even though I care about
a bunch of things Evolution got me to
care about within that like tasty foods
and
surviving um yeah
so we don't know how to put goals into
systems I basically just assert that we
are not currently capable of putting
goals into systems
well
and this is one of the main things the
field of bment thinks about and we're
not very good at it and it' be great if
we were better at it um in terms of yeah
I definitely don't want to make strong
claims about to be dangerous the goals
need to be coherent or the goals needs
to there needs to be like a singular
goal like I don't have a singular goal
um it's not obvious to me how these
systems will turn out if they don't in
any meaningful sense want a coherent
thing then I'm a fair bit less
concerned though well I mean there's
many many ways that human level AI would
be good for the world world or bad for
the world or just wildly destabilizing
and high variance of which misalignment
risk is one of them and lots of the
other ones are still apply like misuse
and systemic risks but leaving those
aside um yeah I think if a model is just
roughly pushing in a gold directed
Direction with a bunch of caveats and
uncertainties and flip-flopping that's
still seems like a pretty big deal to
me okay okay Kaa um let's just cover her
two main arguments so she said that
intelligence might not actually be a
huge Advantage so looking at the world
um intuitively big discrepancies and
power are not to do with intelligence
and she said um IQ human you know humans
with an IQ of 130 earn roughly 6,000 to
18,000 a year dollars more than average
IQ humans elected representatives are
apparently slightly more smarter
slightly Smarter on average but not a
radical difference Mena isn't a major
force in the
world and um if we look at people who
evidently have good cognitive abilities
given their intellectual output their
personal lives are not obviously
drastically more successful an
anecdotally so is it that much of a big
deal yeah so I think this is like a fair
point um if we looked in the world and
um IQ or whatever metric of intelligence
you want to use um clearly dramatically
correlated with everything good about
someone I mean IQ correlates with like
basically everything you might value in
someone's life because we live in an
unfair world but not dramatically um
yeah
so I think this is a valid argument I
generally don't think you should model
ex human level AI
as like I or like slightly superum AI as
like an iq00 human like for example gbd4
I would argue knows most facts on the
Internet or many
facts um
and yeah knows many facts and this
seems um gp4 knows many facts
and this is sure an advantage over me uh
gbd4 note how to write a lot of code and
it knows how to take software and do
penetration testing on it it knows lots
of social conventions and cultural
things and has lots of experience
reading various kinds
of text written to be manipulative or
manuals on how to make nuclear weapons
sorry I'm mostly I'm going too hard on
the knowledge point there just lots of
different axes you can be human level or
better in of which knowledge is one
intelligence or reasoning is one social
manipulation abilities is another
Charisma and persuasion is another I
think these two are particularly
important
ones um
there's forming coherent
plans there's just like the ability to
execute on stuff 24/7 running thousands
of copies of yourself in parallel
distributed across the
world uh there's running faster than
humans I there's just like lots of
Dimensions here I think the iq200 human
frame is helpful in some ways but
unhelpful in other ways especially if it
summons the like nerdy scientist with no
social skills whose Life as a mess
archetype I say is a nerdy scientist
with no social schools whose life is a
mess
okay yeah I mean this is this is the
thing is um because Rob said the same
thing on on chess it's possible for
someone to be literally 20 times better
than you that there's a huge dynamic
range of skill and that's something
we've not really seen in human
intelligence and it might be because of
the way we measure it it's possible that
the way we measure it doesn't even
capture people with with with um you
know broader or better abilities let's
just um cover her her last Point quickly
so this is that the the speed of
intelligence growth is ambiguous so this
idea that AI would be able to rapidly
destroy the world seems primery unlikely
to Kaa since no other entity has ever
done that and she goes on so uh the two
common broad arguments is that there'll
be a feedback loop in which intelligent
AI makes more intelligent AI repeatedly
until AI is very very intelligent number
two small differences in brains seem to
correspond to very large differences in
Performance Based on observing humans
and other Apes thus you know any
movement past human level would take us
to unimaginably superhuman level and the
basic counterarguments to that is that
you know the feedback loops might not be
as powerful as assumed there could be
diminished returns there could be
resource constraints and there could be
complexity barriers so maybe we should
just do that kind of recursive
self-improving piece first what do you
think about that I ear buy recursive
self-improvement oh good it's not an
important part of why I'm concerned
about this stuff um I so generally I
just feel like a lot of the arguments
were made before the current Paradigm of
enormous Foundation models when you're
investing hundreds of millions of
dollars of compute into a thing it's
pretty hard for it to make itself
substantially better
um and you can do things like design
better algorithmic techniques I think
think that is probably one that is more
likely to be accelerated the better the
model
gets um it's not clear to me how
much how much juice there's a squeeze
out of that um
and yeah um but generally I just think a
lot of this is going to be bottlenecked
by hardware and compute and data such
that I'm like less concerned about some
Run Runaway intelligence explosion and
I'm more just concerned
about we'll eventually make things that
are dangerous what do we do
then and I I think this this is like a
really good fact about the world I think
a world where you can have intelligence
explosions is really scary and I feel
like our current world is a lot less
scary than it could have been if some
kid in a basement somewhere just like
wrote the code for AGI one
day yes
yes okay well I mean just just to finish
off kya's final point so the other point
they made was about small differences
might lead to over it's a little bit
like in squash I don't know if you've
ever played squash but a tiny difference
in ability leads to one player
overwhelmingly dominating the other
player because you just get these kind
of like you know it's a game of
attrition and you get these tipping
points and she argued that that might
not necessarily be the case When
comparing AI systems because of three
reasons different architectures like to
have very different underlying
architectures and biological brains
which could lead to different scaling
properties performance plateaus so um
there might be these plateaus Beyond
which further increases in intelligence
you know don't lead to significant
performance improvements and also this
notion of task specific intelligence
something that I strong I believe that
all intelligence is specialized as we
were speaking about earlier um so it
might be specialized rather than being
generally intelligent and small
differences thus may not translate into
large differences in performance AC
across a wide variety of tasks maybe we
should just touch on this on this kind
of task Focus thing so I think humans
are very specialized we have and we
don't realize that we are because the
way we conceive of intelligence is
anthropomorphic but actually we we don't
do four dimensions very well there's
lots of things that we don't do very
well and we're kind of embedded in the
cognitive ecology in quite a complex way
so what do you think about that yeah
so I will okay I'll first comment on the
general meta Dynamic of I think that
people get way too caught up on
philosophizing and sorry
and in
particular I care about whether an AI
will cause a catastrophic risk I don't
care about whether it fits into whether
it's General in the right way whether it
has weaknesses in certain areas whether
it's uh high on the chumsky hierarch
Archy or whether it's generally
intelligent in some specific sense that
someone like Gary Marcus would agree
with is is that in any way a
contradiction of your mechanistic
sensibilities because when it comes to
neural networks you want to understand
how they work but when it comes to
intelligence you don't oh sorry I want
to understand how it works I want to
understand everything um I just don't
think it's I want to
disentangle things to be concerned about
from theoretical arguments about whether
this fits into certain categories for
the purposes of deciding whether to be
concerned about AI existential risk I
see all of the theory arguments is like
a means to an end of this ultimate
empirical question of is this a thing
that could realistically happen and
I think that these qu the these the
theoretical Frameworks do matter like I
don't know I think that an image
classification model is basically never
going to get the point where it's
dangerous while a language model that's
being rfed to have some like notion of
intentionality potentially will um
and yeah I know I can give like random
takes but to me if you're like a I can
be task specific in the same way that
humans are task specific I'm like well a
human is Task General enough that I
think they could be massively dangerous
in the right situation with the right um
advantages like if they wanted to be and
were able to run a thousands copies of
themselves at a th X speed or something
I don't know if that's actually a
remotely accurate statement about models
probably they can run many copies but
not at TH X speed or something but um
yeah yeah
generally that's the kind of question I
care about and I'm concerned many of
these definitions lose sight of
that and part of my thing of like I want
to keep alignment arguments as having as
few assumptions as possible because the
more assumptions you make the less
plausible your case is and the less and
like more room there is for people to
like rightfully disagree I'm like I want
to be careful not to make any of the
case rest on like strong theoretical
Frameworks because we don't know what
we're doing here enough to have legit
theoretical
Frameworks and I think that AI is likely
to be limited in the same way that
humans are at least within the gbt
Paradigm because if you're training it
to predict the next word on the internet
and a bunch of other stuff then it's
going to learn a lot from Human patterns
and human thought and human
conventions but
I
no in in closing um you said that your
personal favorite heuristic is the
second species
argument can can you can you tell us
yeah so um I quite like hinton's recent
py quote of there is no example of
something being of some entity being
controlled by things less smart than it
and was terrible uh sorry I really well
I mean um Twitter went wild over that oh
they try to go wild yeah cuz if you I
mean look at an look at a
company the CEO is usually dumber than
you have to hire competent people to
have a a successful company look at my
cat yeah okay fair this is a terrible
let's just start again um all right so
yeah this is often called the gorilla
problem right humans are just smarter
than gorillas in basically all ways that
matter humans are not actively
malevolent to gorillas but ultimately
humans are in charge gorillas are not
and gorillas exists because of our
continued benevolence or
ambivalence and it just seems to me like
if you are creating entities that are
smarter than you the default outcome is
they end up in control of what's going
on in the world and you do
not and and I kind of just feel like
this should be the null
hypothesis and then there's a bunch of
arguments on top of like is this a good
model will obviously there's lot of dis
analogies because we're making them we
ideally have some control over them
we're going to try to shape them to be
benevolent towards us but this just
seems like the default thing to be
concerned about to me on that point
though we are different from computers
we scuba dive M and that's actually
quite a profound thing to say we scuba
dive because we are we are integrated
into the into the ecosystem not just
physically but cognitively there's a
kind of cognitive ecosystem that we're
unshed in we have a huge advantage over
computers computers can't really do
anything in the physical
world um so I agree with this but I
don't know I feel like the
way I don't know one evocative example
is there was this
Crim lord El Shao who ran his gang from
within prison for like many years very
successfully um when you have humans in
the world who can get to do things for
you you don't need to be physically
embodied to get  done and I know
just look at Blake leoy there's no
shortage of people who will do things if
convinced in the right way even if they
know it's an AI and I do agree with you
on that and I think part of the reason
why we're going to have the inevitable
proliferation of this technology is so
many
tinkerers will just create many many
different versions of AI and they won't
really be thinking about the
consequences of their actions but what's
the alternative
paternalism yeah so to me the main
interesting thing here is large training
runs as like the major bottleneck very
few actors can do them we're probably
going to get beyond the point where
people are even putting the things out
behind an API open to many people to use
let alone like open sourcing the weights
which we've already pretty clearly moved
past and this to me seems like the point
of intervention you need if you're going
to try to make sure things are safe
before you deploy them
like track the people who are able to do
these
runs have standards for what it means to
decide a system like this is safe I'm
pretty happy samman's been pushing that
stuff very
heavily and if competently done I think
this kind of Regulation can be very
important it could be great like the
alignment research Cent has been doing
great work here and I'm very excited to
see what the um red teaming large
language models thing at defcom looks
like but I don't know maybe to close I
feel like I've been in the role of why
alignment matters maybe I can try to
break alignment arguments myself for a
bit please do yeah um yeah so if I
condition on actually the world is kind
of fine um probably my biggest guess is
that the gold directed notion is just
like not remotely a good understanding
of how these things work and it's hard
to get them to be gold directed and we
just mostly coordinate and don't do that
and these systems are mostly just like
extremely effective tools seems like
kind of a plausible world we could end
up in I don't think it's any more likely
than Yep they're gold directed and this
is terrible
um we end up in a world which just has
like lots of these systems that don't
coordinate with each other want somewhat
different things are like broadly
aligned with human
interests um but like imp perfectly and
just none of them ever get a major
advantage over the others and the world
kind of continues to be about as the
world is with lots of different actors
who aren't necessarily align with each
other but mostly don't try to see over
the world except every so often um or we
just alignment isn't that hard um we
crack mechanistic interruptibility we
look inside the system we use this to
iterate on making our techniques really
good um it turn turns out that doing rlh
chef with like enough adversarial
training just kind of works or with AI
assistants to help you notice what's
going on in the system and this just
gets us aligned human level systems and
we can be like please go solve the
problem and then they
do and I don't know I think people like
y Kowski are very loud about we're
almost certainly going to die and I we
might but we also might not I don't
really know I would love to just become
less confused about this and I remain
very concerned about this to be clear
but I'm not like 99% chance we all going
to
die yeah but I mean anything which is an
appreciable percentage may as well be
the same thing yeah pretty much yeah
it's quite funny I got a lot of um push
back on the Robert Miles show people
said oh I can't believe it you you
framed him to be a
dumer and he himself said in the show
prob I think about five times we're all
going to die and I managed to cut about
five well I don't to exaggerate but that
there was at least two posts on Twitter
within 15 minutes of that comment where
he said and we're all going to die so I
don't think I don't think I'm being un
well I didn't actually call him ad dumer
but he basically is um I don't know man
I hate LS like elizer is clearly a dumer
he's clearly a dumer yeah Rob is much
less doomy than eliser yeah is Rob a
dumer I don't know I didn't call him a
dumer but empirically the data says yes
um yeah I mean I don't know man it
sounds like you spend too much time
reading YouTube comments I do too much
time notoriously the least productive
use of time possible apart from hanging
out on Twitter reading AI flame warts
Twitter is the worst I know it's so bad
I mean we don't we don't need to go
there but we were we were having a brief
discussion before um before we started
record it's um why do you think
otherwise
intelligent respectable people behave in
that
way impulse control social validation
it's just kind of fun people aren't very
self-aware about how they look or like
aren't that reflective and Twitter
incentivizes you to lack nuance and to
be outraged about other
people ah I don't know
um I am very sad by many Twitter
Dynamics including from people who
otherwise seem worthy of
respect yes yes
interesting look Neil this has been an
absolute honor thank you fun yeah it's
been amazing it's been a marathon but
thank you so much for joining us today
and um I I really think we've had a
great conversation and I I know
everyone's going to love it so thank you
so much yeah I apologize for all the
times I saw you off for philosophizing
oh no problem it's
it's it's an honor yeah all right thanks
again for hand me on

hi everyone I'm here today to introduce
conjecture and to talk about a document
that they produced a comprehensive
document about AI safety joining me
today is Connor Ley who is actually the
CEO of conjecture hi Connor did you want
to introduce yourself hi thanks so much
for having me on I'm Connor the CEO of
conjecture we're like an 18 person
startup located in London we work on the
technical problem of AI Control it's the
question of like how do we build useful
AI systems that do what we want and
nothing else which turns out to be a
rather hard problem but overall the
thing I really care about is building a
good future with powerful AI systems so
I also work with a nonprofit
organization called control AI on policy
advocacy regulation and stuff like this
so with Connor we will go through three
parts we'll first talk about an
introduction then we'll talk about the
race to AGI and some of Connor's
insights on that and finally we will
talk about a good future so part one
introduction so Connor again thanks for
coming on and can you tell us a bit more
about what conjecture does what's the
goal of the company yeah so conjecture
is you know a small startup so we are a
company we are trying to build useful
products that hopefully will help people
build economically useful AI systems
while also doing the R&amp;D necessary to
answer this question of like how do we
build AI systems that we can understand
and control that don't lead to the kind
of AGI agent kind of risk and loss of
control risk which I'm sure we're going
to talk about even more in just a bit
that we think are inherent to a lot of
the current systems I've been hearing a
lot lately as have many people that AI
safety is a really hard problem that
it's basically unsolvable that the
amount of resources needed to do it are
far more than what's currently being put
into it can you give your brief
perspective on how the field is doing as
a whole and how you guys fit in yeah so
when we talk about the problem of like
AI or AGI control sometimes also called
alignment I think it's worth being kind
of a bit more clear in our heads of what
we're actually talking about here what
we're talking about here is you know how
do we make a system like that is like as
powerful or more powerful in the case of
you know super intelligence then
Humanity that brings us a good future
and in a sense this is saying we're
taking all of the problems that you know
individual humans solve that
corporations are trying to solving that
governments are trying to solving that
intern groups trying to solve going to
take all of these Global problems and
solve all of them using software and
have no bugs that's an incredibly hard
problem like holy  like how how do
you even like it's crazy to me that
people even expect this to not be
insanely hard like if someone came to me
and they're like I'm going to replace
the United States government with a
perfect system that doesn't make
mistakes and fixes the whole us I'd be
like you're crazy that's insanely hard
as a security person I have read the
statistic that there's one bug per 50
lines of code so I guess you're
basically saying my my super
intelligence is less than 50 lines of
code which it doesn't sound very
realistic to no it's kind of even worse
than this right because like a lot of
how we build AI systems with neural
networks which aren't even code at all
they're not line by line software that's
being written it's more like grown so
you have these huge piles of like neural
networks that you can barely debug you
can't write unit tests on them or
anything like this so when people come
to me and they EX expect that like our
current state of like cyber security and
software development practices and
understanding of AI would allow us to
build something you know something as
powerful or more powerful than United
States governments more complex than the
United States governments and deploy
that and this would be like not have
horrific bugs that lead to terrible
things happening I'm just like wow I
don't think we're that good at software
yet what do you think about some of the
arguments or the discussions about
making safe AI systems by avoiding
making agents so if you just have
something that you can ask questions of
for example maybe an advisor to the the
US government to to use that example do
you think there's inherent danger in
that as well I think there is there is
something to this you know I think it's
definitely better than the alternative
of trying to make agents as much as
possible which is what's currently
happening where everyone is trying to
make them as agentic as possible as fast
as possible so if we would at least try
to not do that that would definitely be
an improvement on P now but it's
important to understand that agency is
not like yes or no like there's not a
clean line between agent and non-agent
even if you have a system that only
answers questions if the US government
does whatever that things tells them
well then that system plus the US
government is an agent and can just like
or you know it plus the user reading the
input is an agent or whatever and so you
know you can say oh it's not an agent
but that doesn't mean it's not dangerous
the advice it can give could still lead
to actions being taken in the real world
that lead to you know the relevant harms
that reminds me of the Chinese room
argument where you have someone in the
room who's essentially taking in Chinese
statements following instructions in a
dictionary and producing Chinese outputs
and the question is does that person
understand Chinese but the systems
argument says the person plus the room
definitely understands Chinese that's
similar to what's going on there I guess
yeah I absolutely agree with this I take
very much a systems approach this kind
of thing where you know I don't think
you know the first AI system will be
like one neat little brain in a little
box it'll probably be a complex
distributed system involving databases
and neural networks and you know
operating systems and people and like
bureaucracies and so on so it's seems
like it would be really hard to
coordinate all the actors involved in
making an AGI system so that you can
actually get some of the safety
properties that you're interested in so
do you feel like governments or the
public have an important role to play in
this absolutely I think there is
sometimes a uh tempting you know fallacy
for like smart hackery types to think
that every problem is a technical
problem but the true thing is the
current risk from AGI is not from AGI
it's from people it's from politics it's
from people deciding to build these
systems they don't exist today yet and
so the real problem that we're facing at
the moment is a political and a civil
problem it's a question of how do we as
a society make decisions and enforce
decisions around what kind of risks we
want to take you know like if you know
most of the world's population had been
pulled and you know 90% of them say yeah
screw it let's build AGI right now who
cares you know fair enough right like
Fair okay like I would disagree but like
fair enough but that's not what's
happening currently there's these
decisions are just being made by like
Whoever has the most money and cares the
least about safety so this a really bad
spot so this is why it's so important
that governments and also the general
public are informed of these issues and
have a say in it because it affects them
as much as anyone else do you feel like
enough steps are being taken to inform
governments of what's happening and keep
them in the loop not at the moment but I
work closely with an organization called
control AI a nonprofit that advocacy
organization working on exactly these
kinds of issues uh
um you know reaching out to policy
makers the general public and producing
you know the relevant content and media
hopefully to help explain these
arguments to build the kind of
legislation the kind of common knowledge
necessary to address these risks and
actually the reason that Connor is on
here today is to talk about this
document that he's produced that really
I think is trying to inform us all about
what's going on and make suggestions
about what to do so Connor do you want
to talk a bit about the compendium yeah
so the compendium is kind of a full
summary of my and my co-author view on
like what is the risk where does it come
from why is it happening why are people
even doing this if it's so dangerous why
are people doing this on purpose and
like what can we do about it and as
compared to maybe some other sources on
this this is not meant for technical
audience this is for everyone like our
real goal is is to make this the one
stop you know where you can learn
yourself you don't have to be a
technical person you don't have to
understand computer science or any of
that you could just see the arguments
laid out and that really matches with
one of the things that I think is most
important which is informing the public
about how AI will change society that's
literally why I decided to start this
YouTube channel so I'm very excited
about the compendium and to see how
widely it can be distributed one thing I
wanted to follow up on Connor though was
you talked about how AI Control is a
political problem and how humans in
control can do bad things can get the AI
to act in bad ways but do you feel like
there's also a danger that the AI system
on its own can eventually go off the
rails without a human actually telling
it to do so absolutely this is actually
what I what I was mostly referring to
here is that I think the default outcome
I don't think intelligence is Magic I
think intelligence is a bunch of
patterns and abilities and skills that
are you know bit by bit being automated
being taught to computers and once you
have a system that is smarter than
humans you know better at business
politics science engineering everything
else and you don't know how it works or
how to control it or how to make it bug
free what do you think happens so I
think this is actually the default
outcome I think misuse risk is a huge
problem so humans using AI systems to
cause I think already today this is a
huge problem with control AI we've
worked on bills for criminalizing you
know sexual abuse material created with
deep fakes and stuff like this because I
think this is a huge it's horrific the
statistics so there's a lot of these
problems that we're already having today
but the real risks that I'm see in the
near future are from loss of control
where we build systems that you know bit
by bit require less than less humans in
the loop less and less until they
require zero humans in the loop once you
have zero humans in the loop and you
have a system that can do AI development
that can develop best better AI systems
that can program better than any human
you know you can run it on all the
superc computers in the world things
move very quickly and I think Humanity
will not be in control very quickly and
we have to not let that happen we have
to not build such systems very well said
and with that I think we'll move on to
part two the race to AGI so tell us a
bit about maybe we can start with your
timelines or the forces that are really
driving this race right now yeah so
there is a race happening right now
there's this really this thing happening
where PE where people are like falling
over each other to get to AGI as fast as
possible to cut as many corners as
possible to to be the first there and
there's a couple reasons why people want
to do this my personal timelines really
haven't changed very much since the gpt2
days I but don't take them too seriously
like this is like a very rough guess I
usually say like 30% probability we'll
see an AGI system by 2027 50% by 2030
99% by 2100 1% it already happened and
we just haven't noticed yet 1% it
already happened that's the that's the
interesting case right 01 or 0 one.5
sitting in a warehouse somewhere uh is
actually already an AGI that wouldn't be
my main guess for what it looks like it
would probably be something built on top
of like say 01 or some like unreleased
claw version and like some like hacker
somewhere having built some kind of like
interesting recursive script or
something but who knows right like I
still think it's like quite plausible
that the first AGI system that like
destroys the world could come out of
just open source like this seems like
very plausible to me it doesn't have to
be the big Labs um it's more likely to
be the big Labs but it's not guaranteed
by any means like some of the stuff open
source people do is extremely impressive
and you know impressive both in a you
know wow that's cool and also like oh
that's a potentially risky kind of way I
think it was Yuval Harari the the author
of the book Nexus that just came out I
think it was him that said that he
didn't think AGI had already been
created because he could see the effects
in the world by I don't know observing
different things happening or decisions
being taken in different ways or
whatever so maybe we should have him
post on his website every day I don't
see AGI yet so that when he stops doing
that we know what's
happening I actually personally think
that if the first AGI gets created we
will not know it until like way later
like if at all I think it's like quite
posible that we can make it like all the
way to kind of like human disempowerment
without knowing that it's happening this
seems like quite plausible to me yeah
especially if the creators have a strong
reason a strong incentive to keep it
secret like if it was a government that
created it or if it's an AI lab that's
already getting a lot of Revenue and
they know that this could be bad for
their reputation so they kind of like
keep it under wraps keep getting the
money coming in and that kind of thing
yep for example or just things being so
complicated that we don't really know
what we're looking at like I expect AI
takeoff or like human disempowerment to
mostly feel like being very very
confused I don't think it would be like
an epic showdown versus the Terminator
or something like this I think it'll
mostly just be confusing it's just weird
things keep happening things make less
and less sense more and more complicated
things keep happening weird things pop
up on social media new technologies get
invented that you don't really know who
made them or where they're from weird
geopolitical movements happen that like
seem like oddly you know like
synchronized in some way and then just
one day it's just over so what do you
think about the current geopolitical
maneuvering in the the wars and the I
don't know it feels like there are some
confusing things happening I AG there
are sometimes I joke that you know with
all the current geopolitical things
happening that I will not specifically
comment on uh with AI emerging and all
this crazy things it feels like Earth is
like gearing up for like a season
finale Humanity's season finale I guess
hopefully not the
last let's hope we get renewed for
another season so despite this race for
AGI a lot of people are actually
supporting that race so why do you think
this is so I think there's a couple
reasons like I think this is a really
this is a really good question is a lot
of people see this they're like wow like
there are all these risks we don't know
how to control these AI systems and like
even you know the CEOs of these
companies themselves acknowledge that
there's huge risks here like you know
for example Dario amade has said the CEO
of anthropic one of the biggest AGI
companies has said like on in a in a
podcast you think there's a 20% chance
that AI could kill everybody and they're
doing it anyways so it's a good question
like why and I don't think you can
really understand what's happening here
from a purely economic perspective I
think there's a deep ideological
component and I kind of see like five
main ideologies or which have a lot of
overlap you know it's not like it's
perfectly cleanly cut between them that
are pushing this the first and I think
kind of most important ideology is the
utopist so these are people who think AI
will allow them to create their Utopia
will allow them to live forever upload
themselves to the cloud or you know cure
all diseases you know destroy or defeat
all you know
bad governments and whatever right and
they're quite open about this so this is
like the leadership at like open AI
anthropic Deep Mind have basically said
that they think AGI creates Utopia it's
like a god almost and that they think
this is good and they should be the one
doing it that reminds me of Dario Emo's
piece the Machines of Loving Grace right
they they are very explicit in saying
this would be Utopia yes they they are
quite explicit about they've been
starting to be less explicit about it
because they're realizing that people
rightfully find that this is extremely
creepy and weird I mean you know I'm
just saying historically speaking
utopist ideologies don't have a good
track record and I don't think this is
very different in this regard you know
of course we should all want a better
future but if your better future is a
Utopia that's so good that any price is
worth paying that any risk is worth
taking even a 20% chance which is worse
odds than Russian Roulette mind you
Russian Roulette is a 16% odd of loss
that everyone dies I think that's an
extremely dangerous ideology and I think
this is not one that most people would
agree or subscribe to and it's kind of a
fascinating historical coincidence then
that's not a coincidence but like it's
historical fact how the people that are
part of these kinds of ideologies like
transhuman ideologies have found
themselves as the vanguards of like of
AGI I don't think they're alone in this
though like they're not pushing this
alone they couldn't do it by themselves
one of the most interesting recent
developments is Big Tech entering this
race and mostly to push and support
utopist and their dreams but mostly
because they want power I mean that's
what big Tech does right what do
corporations do they want Power they
want Monopoly they want you know money
Etc you know which I mean understandable
that's what they're for so not not
necessarily surprising you know
Microsoft wanting to you know you know
Embrace extend extinguish never happened
before but it's exactly what we're
seeing here right so a lot of these
companies have kind of been quote
unquote adopted by one of the big techs
so like you know Microsoft like pseudo
adopted but AI Google I mean literally
bought Deep Mind Amazon is a huge
sponsor and likes you know like most of
the revenue I think from anthropic comes
from Amazon and stuff like this so this
is really interesting interplay between
these like traditional big tech
companies and these like more
ideologically driven actors and there's
obviously some overlap there where Like
Larry pagee you know the ex CEO of
Google and exf founder said you know to
Elon Musk once when Elon Musk told him
he doesn't want AI replacing humans
Larry Page accused him of being speciest
and so this is another one of these
ideologies which I call the zealots so
there's a there's this is a very strange
one that I think you wouldn't believe
exists if you hadn't been in Silicon
Valley before or San Francisco um there
are some people who basically think
Humanity will go extinct by Ai and this
is a good thing is that they want
Humanity to go extinct either because
humanity is sinful and bad or because AI
is somehow better or more moral or
conscious or something small minority
but they do exist I often see that and I
almost wonder if it's a reaction to
feeling powerless like people are like
well we we can't really change the
future look how terrible my life is and
so this is going to happen anyway so
some people seem resigned to it and
other people seem genuinely enthusiastic
about it yeah I I think the genuinely
enthusiastic zealots is a minority I
think the by far most common reaction is
nihilism it's just like it's happening
anyways so there's nothing you can do we
shouldn't even try this is like
something that I push against very
strongly like with the pendum in my
general view is that I think the reason
nothing is nothing's happening is
because you're not doing anything is
like I think things can be done like we
have not yet lost but there is this
pervasive malaise in our culture that
like really strongly pushes against
doing anything other than Doom scrolling
all day and we could talk for hours
about why that is who benefits from it
why these systems have been you have
come into existence whatever and what to
do about it I do but I do think this is
a big problem I I feel as Ze are
different from like nihilists because I
think the zealots genuinely want this
they want to kill people like they want
people to be gone they want to replace
them and yeah as you can tell these are
mostly like very strange people who have
very strange Fringe beliefs um I you
know don't think they're the main source
by any means there are like other groups
as well a slightly more larger group is
what I call the accelerationists so
these are people who are kind of like
mostly Libertarians where they just
believe technology is always good with
you know the technology is good and
anything that's not technology is bad so
they believe just like any form of
Regulation any form of hesitation or or
care or anything is bad and you should
just be allowed to do whatever you want
kind of in all circumstances and this is
a another one of those ideologies which
like is very common in certain circles
especially like Tech and like Silicon
Valley but like outside sounds
completely insane but like uh for those
outside of these bubbles yes this is a
very real and Powerful political
movement not necessarily accelerations
by the water libertarian techno Optimist
kind of sphere and not saying all these
people are bad people some of them are
bad people um but like most of them have
good intentions they just have in my
opinion very misguided policies that
would lead to very bad outcomes they
seem to be a pretty strong majority on
Twitter SLX I guess because of the
political structure yes they have a they
have a very very disproportionate level
of influence on on X I think I don't
know maybe they have nothing better to
do in their lives um but um it is an
ideology that exists kind of quite
strongly kind of like exclusively kind
of like in America and like around the
Bay Area I like you go two steps out of
there and it becomes just very very rare
this like unabashed libertarianism it's
like you know like naive almost
childlike uh libertarian like I was like
this when I was like 15 you know and
this is kind of like the kind of stuff I
believed most people grow out of it but
some people don't so and you know those
people tend to congregate I guess on
places like X it does feel like a very
American perspective to me as a as a
Canadian but yeah it does seem to also
be spreading or have already spread to
wider Tech circles yeah it's got quite a
lot of power kind of for various
interesting historical reasons um but
yeah it's a very American thing it's a
very libertarian thing and so on and
like as I say a lot these people aren't
bad people they're just naive there as
say there are some bad people so and it
but it is a problem and it does need to
be countered we're just like technology
is not good or bad it is a thing that
must be managed is if you just build
bigger and bigger bombs and anyone is
allowed to own bombs eventually
everything blows up that's just a fact
about physics like this is not an
ideological statement so like there's a
reason that nuclear bombs are not
allowed to be produced or owned by
private individuals and hot take I think
this is good I think it's actually great
I don't think that the blueprints of the
F-22 fighter jet should be open- sourced
I don't like I come from an open source
background which is very libertarian
very accelerationist and when I got into
it you know I got into it from you know
like Linux and like you know you make
software like free and open and like
secure so you can like help people and
all this kind of stuff what I found
quickly is that a lot of the people not
everyone of course but like a lot of
people in open source don't really care
about making people's lives better they
just want to be allowed to do whatever
they want and they don't want anyone
getting in their way so I remember when
for like the first deep fakes generators
started like being created and like the
first like voice cloning software and I
was like hey that's kind of up like
maybe we shouldn't open source that and
people were like what are you against
Free Speech it's open source so it's
good and I just think that's a very
childish view of reality and the last
group is the opportunists they always
come along you know they're here to make
some money they're here to you know here
to raise some Capital right always a
part of it so can't can't can't discount
that you know we have them in crypto we
had them in every other you know bull
market so we have them here as well
makes sense yeah it feels like a lot of
people went from crypto to AI so yeah so
like I think there's like this thing as
well where like I feel like some people
when they get exposed to AI they meet
the these like you know ex crypto you
know opportunists and think the whole
movement is like that or like the whole
area is like there's no underlying Tech
that's not true AI is incredible
technological progress happening there
uh which is quite different from like
you know some other places where like
opportunists might be congregating and
the main thing with opportuni they don't
really believe anything they just are
willing to kind of like say and do
whatever gets them attention and money
so just worth keep I don't think they're
like the main factor here by any means
but it's worth keeping in mind that
they're a part of this as well so we
spent quite a lot of time talking about
the problem and how different aspects of
it work let's talk a bit about part
three a good future Connor how can we
get a good future well I sure wish I
knew exactly what to do in which order
that will definitely work and have no
consequences or trade-offs whatsoever
unfortunately we happen to live in
reality and reality there are always
trade-offs the world is hard and most
things require hard work and this is one
of them a thing that I believe very
deeply is that if you don't do something
it doesn't happen this is a in a
controversial statement but getting a
good future is a thing that has to
happen it's a thing that someone has to
do if you want to live in a good future
you have to make it so if you know like
the world can always go down to entropy
you know things can always fall apart
things can always Decay that's easy
destruction is easy building things is
hard making a good future is hard peace
is hard war is easy you know war and
death are easy like that's just entropy
but peace and prosperity and happiness
and art and these things these are hard
and someone has to make them so my
fundamental belief is important thing is
that we just like keep in mind things
don't go well by default they go well
because someone made them go well and
that's someone should be us and all of
us reminds me of the bias of diffusion
of responsibility right you look around
and if it's a big group of people you
think ah someone else will take care of
this enormous problem but really
sometimes has to be you exactly like as
someone like you know dear viewer as
someone who has talked to you know
extremely powerful people politicians
billionaires you know the people in
these Labs all these ai ai systems uh
National Security intelligence agencies
I've talked to people from all over the
world from different countries you know
North America Europe Asia everywhere and
what I can tell you is that there are no
adults in the room there is no secret
group of super competent you know super
serious people that have got this 
on lockdown we do have this from what I
can tell for example for nukes which was
a surprise to me but from what I can
tell there were actually super dup
responsible people at least in the US as
far as I'm aware that are actually
extremely serious about nukes and take
this extremely seriously and follow
things very carefully and think about
things very deeply and are very
responsible about this I don't think
this is always historically the fact but
my understanding is that at least in the
current state like people the doe and in
the defense apparatus and so on do take
this extremely seriously this is very
nice and I thank those people for the
work that they're doing I think this is
extremely important but I wanted to be
very clear there are no such people for
AI right now ai is a problem as big or
bigger than nukes and there was no
secret cabal of like you know doe super
Geniuses that are keeping this unlock so
we need to build these institutional
capacities we need to demand these
Solutions we need to find the ways to um
handle this responsibly because as we
talked about earlier the problem we need
to solve here is so massive it's so huge
that if we let these private companies
just race ahead as fast as they want
with as little safety as possible like
do you think Facebook is going to be the
company that's going to be like super
responsible with nukes like no I don't I
don't that's not what private companies
are for right this is the kind of thing
that becomes a national security or even
an international security issue and so
we should be treating it with this how
can we as a society as a civilization
take the time to solve these problems of
control and build a good future because
where I'm a bit different from the
utopist like the utopist they have a
vision of what Utopia should be like and
that they should build it I disagree
with this I shouldn't be allowed to
build Utopia shouldn't be allowed to
build AI like to be clear currently it's
completely legal for me to just go and
build AGI this is completely legal I
don't think it should be I don't think I
should be allowed to do this because I
don't think an individual or private
entity or whatever should have the
authority to impose their will like this
on other people and on society so what I
really want is a just process process by
which people somehow can decide the
future that they want and they actually
get that we're very very far away from
this but it starts with us you know as
Citizens thinking about these issues
taking these things seriously not
falling to the bystander effect you know
maybe even reading the
compendium and um taking action I think
it's very hard and we don't have much
time but we haven't lost yet compendium
linked right up here go click on it
preferably after you finish watching the
video it seems like the compendium is
very important to you and to everybody
so how else are you going about
advertising this so my colleagues over
at control ey are now taking over most
of the work the hard work of improving
the compendium integrating feedback you
know showing it to policy makers you
know engaging with different civil
groups you know I've been talking to
unions lately and stuff like this I've
also been talking to say a lot of policy
makers you know both sides of the aisle
and in general I you know I want to do
more um I want to do you know I'm so
glad that I got invited to this lovely
interview here so I could tell people
about this if any of readers find any of
this interesting or important we would
love for you to read the compendium even
if you just agree with this please tell
us we want to improve we want to know
how we can do better where we're making
mistakes you know like I think this is a
very important process so any help in
this regard greatly appreciate it well
thank you very much Connor for joining
me and coming here for our audience and
i' just like to say if you liked this
video you can check out this previous
one I made where I interviewed Yoshua
Benjo very famous AI researcher about
similar topics so you can continue the
conversation over there and as always
please hop on our Discord if you would
like to talk with me and with our
community directly that's all for today
thank you very much for watching bye

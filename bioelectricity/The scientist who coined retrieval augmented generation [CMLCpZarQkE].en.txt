if there's going to be errors which do
you prefer it's almost like a Precision
recall would you prefer a model that
refuses to answer more sometimes be
quite frustrating because of that or
would you prefer a model that runs the
risk of as it were hallucinating a
response and so these are really really
tricky things to like drill down and get
right today we're going to give you an
Insider perspective on retrieval
augmented generation or rag as you've
probably heard it mentioned we speak to
Patrick Lewis who is the guy who helped
coin the term he's a leading expert in
the field he works for coh here and he's
also a very cool guy we will Deep dive
into the evolution of language models
going all the way back to the days of
melov and word Tove to modern language
models and Beyond even back in 2015 this
algorithm when you did this clustering
of the elements they clustered in such a
way that they reproduced the periodic
table it just had like read the
literature and had uh discovered the
periodic table Within it we share
practical tips for implementing rag
systems in large Enterprise and this is
the reason to use coh here by the way
because they're extremely good at that
prompt engineering to get augmented
generation working is really really hard
and sort of what we do at cooh here is
try and build like actual like prompt
templates that the model has like very
very concrete like awareness of in the
way that we postrate it would also be
remiss of me not to mention that the
brave search API is partnered with coh
here so if you want to do some really
cool retrieval augmented generation now
on the best search index on the web sign
up to the brave search API at brave.com
API enjoy the
show let's talk about yeah like
evaluation so like there's evaluation
for rag which is quite specific and you
tend to like try and measure a few
different things there's that
answerability thing is this answerable
is this not there's these senses of
faithfulness which you we usually use
these grounding spans or these citations
as a way of making that concrete like is
everything grounded are we missing
anything is there are things that
shouldn't like shouldn't be grounded but
the model has produced a mistake there
like that's how we get a grip on that
problem internally then there's uh yeah
like fluency is this model good or bad
in terms of like the structure of its
answer does it like feel like a high
quality response uh and then perceived
utility uh a lot of these terms come
from a really great paper that Nelson
Leu did from Stanford a year and a half
ago I guess now it's like a really nice
piece of minimal things that you need to
like study verifiability in generative
search engines I think it is it's good
paper perceived utility is like a term
that goes further back than that but
it's like does it feel like the
information need was addressed even if
it was wrong so like did we do the right
thing ignoring the documents existing
for a moment um and so you need to have
both good
fluency addressing the information and
the actual like citations tell you if
you got it right or wrong like in terms
of the faithfulness all of those things
are really hard to measure and there
aren't good data sets out there to just
like Benchmark against that do this
anymore we the quality of our models has
massively outpaced data sets that we use
to evaluate rack um and also are like
consensus on what how we should evaluate
them so our metrics are lagging pretty
hard we're still in a mode where a lot
of time we're just using simple one hop
General Knowledge Questions augmented
with Wikipedia uh which is what we were
using back in 2020 like same data sets
but now are like you our scores are just
outrageously high and our metrics are
not sophisticated at all like kind of in
the general space we're sort of saying
you know is this answer a subspan like
is this named entity which is considered
the correct answer to the question
appearing somewhere as a as as an exact
string in my models generation and
that's very poor and like just doesn't
work that well and all the models quite
close in their performance and the error
in that metric is very very large and so
like a lot of work that we've done
internally is trying to Benchmark better
and poll is a thing that we shared
because like we were sufficiently kind
of like grumpy about some of the things
like that you we thought we oh we can
share this this is a contribution it's
not not specific to rag but it's kind of
like where it grew out of so we wanted
to basically train we wanted to build
better
metrics uh that allow us to judge
correctness for example and we looked
and saw what was out there and there's a
bunch of kind of quite popular
open-source tools that you can use to do
rag evaluation they were not very good
for what we were looking at and so we
thought we better develop something else
we looked at some of the core
assumptions that people are making in
using language models as evaluators out
there in the world and there is quite a
core one which is like a lot of the time
people are trying to do very sensible
things and they're saying if I had an
oracle llm like was perfect what were
the things that I would need it like
that how I would use it to do an
evaluation and then they assume that gp4
is an oracle llm and you can just use it
or or any powerful large language model
is is going to do that job well but
really what you're doing there is
setting up an ml problem and you're not
doing the evaluation of that so you have
to evaluate your evaluator this the
first thing it's like how well is my
system working if I apply any kind of
evaluation system that isn't just a very
straightforward canonical metric from
the literature which is you know useful
by consensus if you're doing something
new or you're using an llm as an
evaluator you need to
first uh understand the correlation to
like you know the actual gold standard
uh evaluation which is usually pretty
close to asking through three humans to
do the problem yeah to do the evaluation
so like until you've got your
effectively your interannotator
agreement between your llm judge your
like modelbased evaluation and your
humans and you compare that to you the
agreement rates between humans and other
humans you don't have an evaluation you
just have a thing that produces a number
so you have to do that and then when
you've done that you can then start to
compare different styles of doing llm
evaluation because you can then evaluate
your evaluator and what we found doing
that is that gp4 or like really really
powerful large language models were not
always the best at this
counterintuitively and that in general
llms being used as judges were worse
than they could otherwise be um and
that's the start of poll the second part
of poll was saying okay like given we
need to build an llm evaluator like like
automatic evaluation is important how
can we do this better inspired by what
we're seeing with these kind of self-
preference things where like large
language models are pretty good at
detecting their own outputs or their
model fam's outputs and they may prefer
them they may dis prefer them but they
know that like they recognize themselves
and so you say what can we do to
mitigate that well we just use like
three different models and if you unble
them not only do you get the benefits of
generally when you're on sble models
together you tend to get a better model
but you're also specifically mitigating
the risk that um command R might prefer
command r or you know Haiku might pre
might prefer Hau GPT 3.5 might prefer
3.5 you like Smooth that out and then
you just see empirically it works really
well and that Ensemble of smaller models
often outperforms GPT for or Opus or big
models you know and cheaper because
small models you can do it in parallel
cheaper and faster yeah but you said
some really interesting stuff in in the
paper I mean um the first thing you said
just as you did now is that you gbt 4 um
is spectacularly wrong some of the time
and and you thought that might be
because it's over reasoning you know
it's actually like it's it's being
weighted too much by its Based training
and you spoke about you know the
annotator bias the model bias the the
style bias so there's all of these
problems and it really does make sense
to kind of like aggregate over them but
but you also said well a really
interesting thing to get it closer to
what human evaluators might have done is
look at some kind of correlation between
something like llm cesarina because you
actually have examples there of human um
you know decisions and you can kind of
look at some kind of correlation between
them you had this Kappa metric right yes
C Kappa it's a metric people use it but
like it's hard to interpret so it's more
like one of those things like it's hard
to know what a Coen Capper of 7 is but
you know that a c cap of 75 is better
than 7 but like for different problems
it doesn't necessarily compare but it is
a way of comparing it's a statistically
more vigorous and better way of
comparing than say just binary agreement
rate it basically corrects for the
background we're just using this this
coin kapper thing it comes from
annotator agreement kind of canonical
scores uh if you have more label spaces
or more annotators there are other ones
that you pick off the pick off the shelf
but codes Capper is sort of the simplest
one that people tend to use for inter
annotator agreement and we kind of view
this as like we have evaluators we have
like annotators and sometimes those
annotators are humans sometimes they're
models and so you just have to compare
how different ones compare to each other
and so we're using that interannotator
agreement metric there but you're coming
back to the the gp4 and the poor
performance you sometimes see using it
as an evaluator it's very unpredictable
about whether it will be great or bad
and you can't know from first principles
you can maybe like do some kind of re
you think about it maybe come up with
some estimate but until you get the
humans to evaluate your problem at least
a bit you don't know whether gp4 is
going to be really good at this or
really bad at it and in the case of one
of the evaluations we were doing it was
particularly weak even compared to GPT
3.5 with the same prompt and all the
other models that we were looking at and
so what I what I did as like you know
one of the last things before we like
threw it out into the world was sat down
for like half a day and just did some
like prompt engineering to see what it
would take to get gp4 to reach the same
level of like performance on this task
as GPT 3.5 it's kind of like a
counterintuitive thing it's like you def
you're almost always doing the other
other way around and yeah yeah it it
turned out that like gp4 is more
sensitive to the prompt structure uh
than other models were the smaller
models were it also turns out that
because the task involved the factuality
things like these are questions that are
annotated in 2019 I think and the
world's changed a fair bit so they're
saying like when does the next season of
insert TV show here come out and the
answer was you know based on uh some
time and the task we were doing is
saying here's a reference answer
does this model generated answer match
the reference answer the llms just just
to say yes or no and what we think was
going on was gp4 was knowing that the
answer to the question was different now
and being confused and upset about that
and it has you know all these
conflicting kind of signals in its
reward model training and its post
training that it's not really supposed
to like it's supposed to be smart and
and like work well in other contexts
maybe and I think what it was doing is
getting a bit confused between the the
consistency between the answer and its
prior beliefs about the world yeah and
what it was being asked to do which to
say is this wrong answer the same as
this wrong answer according to like
where the world is now but at one point
these were two correct answers and it
was like had this like difficulty in its
head and so you could like cool that
down a bit by saying this is just a case
of semantic matching don't use your
facts about the outside world and don't
overthink this and that and combination
of those kind of like styles of things
to like adjust the instruction you gave
the model when it was doing this
judgment thing managed to get it up to
about the level that GPT 3.5 was at with
the sort of standard prompt yeah it's a
kind of interesting reverse scaling law
in a case like these things are kind of
rare this is one thing that Ethan Perez
that one of my collaborators used to
talk about a lot like when he moved to
to anthropic where he is now a couple of
years ago they they held a kind of
competition to find reverse scaling laws
things where smaller models were better
than bigger ones this feels like it's
kind of like that where you want the
model to do something quite simple but
it's too powerful and it's and it's
worried about more subtle behaviors and
so that's kind of getting in the way of
it just doing the straightforward simple
thing yeah it's a little bit like the no
free lunch um theorem but during the
process of training you're kind of
running this whole Sausage Factory and
presumably you're then using that to
fine-tune the base model so the base
model has internalized all of these
judgments is that fair yeah kind of like
we collect a lot of demonstrations of
what like good examples look like that's
sort of like the sft step or thinking of
it B more in an RL way we have quite a
lot of like gold annotated trajectories
effectively right so for imitation
learning yeah and then we also collect a
bunch of preference pairs that might be
collect like most of this is verified by
humans so like the cleanliness of your
data is super important both for
supervised fine tuning and for
preference so you can either defeat this
with vast scale or you can defeat this
with moderate scale of your data uh but
with like very obsession over
cleanliness and so we we go the
cleanliness way because of the budgets
uh like it's much more it turns out to
be much more efficient if you look at
say like llama 3 it's preference data
set some some Millions right of of
preference pairs that they annotated
number one is very very expensive and
number two there will have been some
sacrifice of the quality of that data
and so there'll be some amount of of it
that's not the right way round or
subjective we work very hard to make
sure our data is all very very good so
we we tend not to do too
much synthetic training um IE what I
mean there is like apply all these
metrics that we've talked about these
evaluation metrics
and train the model to optimize them
that is stuff that we're interested in
doing but we know is
not the most efficient way of improving
the model that's the cherry and we're
still like in the cake and at some point
we might you know do some very very last
minute polishing of the model with some
like online RL with the using these
metrics as you know like optimization
but usually we're carefully thinking
about distributions and
and tring into the models and being very
careful how we train in a blate model to
to be good with those of things there's
a lot of complexity to what we do but
that complexity is more operational than
technical at this point what you're
describing at the moment as I understand
it is you you know you've you've rolled
out these processes and you've baked it
into a model and there are more
interesting variations of that where you
actually um either use the human in the
loop as an agent or you have an A judge
which is an agent and these things are
post Hawk running off yes you can you
can kind of like say oh like you know
that wasn't good and you can like force
it to go again or you can yeah you can
you can kind of critique it online I'm
quite interested in human AI
collaboration so like the next phase of
this is going to be like you have your
work assistant to do research work or
like knowledge work and not unlike code
co-pilots like we don't know exactly the
the correct surface form that's going to
take but we also know we want the model
to take demonstration take instruction
improve fast like Fast learning in the
loop this kind of thing but even
something as simple as like taking a
model that works quite well like our
react agents react is just a way of
doing like Chain of Thought over several
hops so that's how a lot of these models
work right now is this very simple thing
where you just say like I'm going to get
a model to sequentially Cool Tools and
it's going to do that by first
generating a plan
then generate the tool API calls it
wants to take and then you call the
model again and it like looks at its
observations that come out of the out of
its retrieval systems or other tools
it's connected to and then decides
whether it's time to answer directly or
whether it's time to do another cool to
the API so it kind of looks like you
know in situ generates a thought or like
a plan cools some tools generates
another thought call some tools and then
at some point it reaches this terminal
State and answers you generates an
answer
um they work quite well but often you'll
like ask at something and it will get it
you it will get the premise a little bit
wrong it'll start to go and you'll be
like oh no no stop if you just edit its
plan a little bit yourself you can very
quickly steer it to do kind of what you
want so there's some quick fixes there
but like generally yeah it's like how
you collaborate with the research agent
to like quickly get it to do what you
want and how that process like it can
learn like to work with you quickly to
me that looks like something like a a
version of in context learning where you
like take the feedback online that it's
giving that you're giving it and you and
your agent get some like like quite fast
Synergy based on just working together I
look a bit at at at like different ways
of giving effectively yeah like
demonstrations to the model like a lot
of people are excited about getting
models to power browsers or power your
desktop and that kind of thing like I'll
just record me doing a workflow and get
a model to to do that I don't think
that's all of it like I think that's
missing something I'd make the analogy
there between there's like any kind of
automation like we could make an
automated train or we could get a
self-driving car and a self-driving car
is a much more challenging thing but if
you don't have a road Network yet do you
really need to solve the self-driving
car problem or could you build a road
Network that's more effective right that
you don't don't have to design around
the human like does the llm and the
human who are collaborating to do
knowledge together do they have to share
the same experiences of like what that
looks like like are we how how many kind
of concessions are we making to like the
human UI and desktop what kind of way
should then llm
experience the the things it needs to
interact with to like make the human
more efficient like often that that
could just be apis is like it can
directly interact with an API it can
interact with a browser but it may be
slower worse more error prone and so I'm
kind of like very interested in this
human computer interaction problem right
now and skeptical that just getting llms
to power browsers is the or power the
desktop is is the right way to do things
it just feels very slow and inefficient
it's General but like not quite there
for me f for me like understanding is
about reconstructing the intention of
another agent you know so the reason why
we have this Divergence in multi-step
tool use is because it didn't really
understand what you wanted and it very
quickly especially if you have like you
know by by the time you're on the third
step it's going in a completely
different direction and and then you
need to have a really cool UI where you
can say no wait a minute that that's not
what I wanted you to do yeah yeah we we
make assumptions that it's an agent that
like yeah like you go and task to do
things but it is it is it's a bit more
collaborative than that I think it's
it's going to be interesting to like see
how how how we work through these six
like that's a lot of what our team are
gearing up to do is a lot of this
thinking through what those those
collaborative systems should look like
uh for for kind of productivity and and
not getting the LM in get in the way too
much
but still help you and automate things
that you find slow or you want to do
quickly the same thing happens with with
humans to some degree like you know I'm
a manager and I've got like a really
great team but it's often like maybe
I'll communicate something I want
someone in our team to do and then we'll
like I like come together next week and
they've done something totally different
and that's on me I thought I explained
myself pretty carefully so like even
like two autonomous agents who are
pretty powerful like me and someone in
my team uh that transfer of yeah like
what you were saying in terms of like
getting my intention into them and
getting them to operate on it is like
fraught with difficulty even for like
humans to communicate that kind of thing
so yeah I had Early Access to gpt3 and
it was still at the point where I mean I
was kind of saying I think this is a bit
of a pilot trick it's not there was a
famous um database example from Matt
Brockman and you know he said oh J is in
the database I'm removing J from the
database you know does the database have
j in it and it appeared to be work
working and I was like come on guys this
is not doing but the amazing
thing is that there was a community of
people who you know even though it was
very um just random sometimes they could
see something in it they could see
something interesting the vision there
is actually quite impressive I saw it as
a target for fine-tuning because that
like that's a paradigm that you would do
say B and even models before that you
would say like all right the Paradigm is
going to look something like this like
we get great representations and then we
find tunit to do something but the fact
that you could do this generation
without thing uh you know without
actually changing the parameters and the
fact that you could like prompt it with
this in context learning stuff and the I
think like one of the the great things
that open AI did in in gpt2 and
gpt3 uh was talk about this like
unsupervised intelligence that was
coming out although the ability to do
tasks in a different way the Orthodoxy
would have been we trained gpt2 and then
we fine-tuned it to do a bunch of stuff
and it's great that's what they did in
gpt1 but they were like no let's move it
along we're just going to like give it a
question as a textual input and see if
it can generate an answer or the tldr
stuff there were like we're going to try
and like fool it into summarizing
something based on some common pattern
from pre-training and then in gpt3 with
the in context learning stuff it's like
well if it's a language model that tries
to predict the next most likely thing if
you give it a very predictable like
pattern of sequences can you then like
train it on the fly with like it reading
kind of on the fly to do things they
were like you know very interesting like
novel takes on what could have otherwise
been a case of like we just made this
really powerful model you can use the
same techniques that everyone has been
using to achieve great results I kind of
think it's great that they went the
other way and said like let's move this
move this on and this is kind of like
what is enabled I've not seen too much
of that from the big Labs since usually
it's a case of like incremental
Improvement
phase uh where the increments are quite
fast and like the models are improving a
lot but that the Paradigm isn't hasn't
changed for a little while yes yes and I
think we'll get on to how that how that
might change but just to kind of recap
so we we have the Bas training and then
there's fine tuning yeah and then
there's this whole domain of um
preference steering things like rhf and
and and that's kind of saying you know I
want you to talk like a you know a
conversational bot and and I want you to
be precise and I want you to adopt this
tone of language so we're kind of
shaping the distribution yeah but but
then the the in context learning is is
the interesting thing so you could if
you can do that or you can say you know
you're a chatbot you you are just like
Donald Trump you behave in this way yeah
and you had this genius idea which is
called retrieval augmented generation
which is which is based on this idea of
in context learning which is putting
instructions it's quite funny cuz at the
time we didn't think of it quite in that
way uh we came out of a reading
comprehension and like question
answering space so some of the things
that I used us to do before my PhD the
company I was working at Bloomsbury AI
that became the seed for the fair London
lab was working on open domain question
answering and so there there's a very
long tradition of having a corpus of
text and needing to like answer
questions based on it and a very common
way that that you do this is by like
retrieving like the most relevant pieces
of information and doing some
transformation on it and so for the long
longest time there were these like tasks
that you'd study in question answering
NLP called reading like reading
comprehension or or or extractive
question answering span based question
answering where you would train a neural
model to take a paragraph of text and
you would extract point to the beginning
of an answer within that text and the
end of an answer within that text so and
that's kind of Clos World be one
paragraph one question and you'd extract
like an annotated answer and youd to
train a supervised model to to do that
thing where we were coming from
was that kind of world and if you wanted
that to be more useful than just
answering a question over a short
document or paragraph you would hook up
a retrieval system to that and so you'd
have a bunch of paragraphs you'd have a
question you wanted answered you'd
retrieve from it get a number of
paragraphs and from those paragraphs
you'd use your neural model usually to
extract these specific spands and it
would literally say point to the start
point to the end that's your answer
there and you'd produce these like
Collections and then usually have some
ranker or something like that's how a
lot of like
question answering systems were working
there was some interest in how you like
train these models end to end in a way
at the time where you say okay I've got
these systems I've got this retrieval
system and I've got this like question
answerer and I'm going to train them end
to end with some like like fun latent
variable like learning models quite
straightforward from a kind of
generative story perspective but like
you know quite difficult to get them to
work in practice and there was a lot of
this kind of stuff going on at the time
then there were the the introduction of
these models that could generate text
very well yes from one perspective one
way of looking at what we were doing was
saying like Okay let's instead of doing
this span extraction thing wouldn't it
be nice if you could just have a model
that instead of extracting an answer
just generated the answer that you
wanted straight away and so we didn't
really think of it at the time from that
perspective via like from the in context
learning way of doing things uh but like
it turns out there kind of equivalent
that said there is a whole different
line of work that we were doing at the
time based around like knowledge in
language models and so these these names
are kind of hilarious now but we had a
probe or a way of like measuring just
relational knowledge that like language
models like Bert or simple gpts had it
was called The Llama probe which is kind
of hilarious because it was you know
years before yes llama and so there what
we were doing was saying okay like I'm
just going to give a language model like
the president of the USA is blank and
see what like it chose to put in there
mhm that's kind of like zero shot
prompting and then we had a different
work called alpaca which is again kind
of funny because that was before any of
you any of the famous alpaca now uh and
what we were doing there was saying okay
does the model get better if you give it
a paragraph beforehand and that's that's
this in context learning idea and we saw
that on that very simple larma probe it
got way better and so we had these two
separate ideas saying like what if we
wanted to have a generative open domain
question answering model
and we wanted to like extend these
alpaca ideas that's the the kind of two
bits that we put together into this
first retrieval augmented generation
paper the reason I kind of explained it
from those two angles is because there
was two kind of groups of people who
came together to do this stuff like I
was working on the alpaca stuff I was
also working with da and Ethan Ethan's
the other first author on the rag paper
retrieval augmented generation and DA's
the the last author along with Sebastian
my supervisor and so we put these two
groups of people people together the
people who were like what if we did this
open domain question answering system
but generative and we took the in
context learning I guess as you say like
paragraph reader uh llm improver kind of
thing and and we're like okay let's just
design this thing that you know you hook
it up to an unstructured Source uh you
give it a prompt it will unsupervisedly
learn what relevant information is from
your information and then it will
generate you a free form answer using
this kind of in context learning thing
um but we're going to fine-tune the
system end to end and we're going to try
and Target like at the time with the rag
paper with quite an empirical Focus like
we wanted to get good results on on
benchmarks and that was our kind of goal
with it so introduce this framework this
like way of looking at things based on
these very new generative models uh gen
like generative language uh generators
and that's yeah that's where it kind of
came from so it's kind of just like a
like gy thing where you just have these
two groups put us together and did this
thing and for a long time everyone was
like hey that's quite cool but didn't
really like it was just a paper in the
space and then I guess about two years
ago people started to use This
Acronym in conjunction with like large
just large language models and hooking
them up to retrieval systems in a more
General sense than what we meant when we
first like wrote the paper so like the
retrieval augmented generation paper is
a I guess a way of doing rag but it's
quite different to what a lot of people
do these days and it's become this term
or this way of referring to a very
general set of solutions to the problem
of knowledge and language models which
kind of like funny from my perspective
because like really the first paper was
just a paper in a line from like lots of
different people and there's there's
lots of like similar systems almost the
same kind of thing maybe without this
generative aspect because really those
systems only appeared about two months
before we put the paper out um but yeah
lots of people would be like oh he
didn't invent Rag and I don't I don't
get claim to um people tell tell me I do
a lot um but yeah it's just like a
zeitgeisty thing you know lots of lots
of people having the same sorts of ideas
at the same sorts of time yeah and I
think we just gave it a three-letter
acronym that people can ascribe you know
easily easily refer to that like group
of ideas with amazing so so I guess we
should split it into the architecture
bit and maybe the the ux bit yeah you're
describing as you said a collection of
possible architecture
and a significant component of it is
information retrieval and I guess the
complexity here is you know a person
implementing rag they might be in an
Enterprise and they might have knowledge
stores semantic indexes blobs SharePoint
blah blah blah so they're kind of
they're they're they're hooking in all
of this stuff and they need to have
relevant scores and and so on and so
forth so just the information retrieval
bit is already really difficult very
tough yeah and and then they do the AG
bit yeah and the two things are kind of
from a ux perspective they're compounded
together so now I just see I see a bunch
of augmented generation yeah what could
possibly go wrong yeah exactly I mean
there's there's there's so much there
and I like the way that you broke that
into the two parts so now if I describe
what my team does I would say there's s
of two broad Parts there's tool use uh
which grew out of like using an
information retrieval tool so we've kind
of abstracted a little bit away way so
you can do more sophisticated types of
retrieval or just cool things that
aren't even retrievers anymore but kind
of like producers of information that
you might want to inject or like get and
then so there's the tool use part and
then there's the what we call grounded
generation but is yeah like you can
think of augmented generation as being a
similar thing but we call it grounded
generation because of the way that we
effectively annotate and think about it
internally using the this concept of
citations which um maybe we can talk
about in a bit but there's the tool use
part that's kind of hard there's lots of
things that can go wrong even if you
abstract down to the very simple things
like okay my tools are only retrieval
systems and they only take natural
language queries still kind of hard to
figure out given an input does it need
retrieval does it not need retrieval if
it does what is a good such uh query am
I using a keyword-based retrieval system
am I using a like dense retrieval system
they will prefer different types of
surface form of queries is this
something where I need several queries
often there's things where you have to
do these parallel uh retrievals to like
get the information to fully satisfy
your information need in the tool use
literature this is called parallel
function calling which we kind of
accidentally did without thinking about
it too hard but it's like a bit of a
trendy thing now so there's this tool
you stuff and then you have this very
difficult problem of saying okay I've
called my retrieval tools and I've got
all these documents out and now I have
to like squash them into my second uh Co
which is where a lot of things get even
harder and the reason we kind of think
about these things separately is because
that's a sort of difficult challenge in
and of itself and sometimes you want to
do this grounded generation this
augmented generation thing without the
the tools you can imagine like
summarization is an example of this
where you say like here's my big
document summarize this and you have to
inject your document into the llm in a
very very similar way to you do with
like
reading retrieval results and so like
this is a sort of broad class of problem
I think of is like you've got some
things to read and you've got a task
that you're trying to do and how you
generate a good response there super
hard um there's a lot of different
factors to think about you have to think
about faithfulness you have to think
about say I have a bunch of documents
the model has this inherent tension
between its own prior belief about the
world which usually comes from either
pre-training or posttraining and the
documents that that's reading on the fly
in context and it might they may be in
tension they may be uh in Conflict so
temporal stuff is a key like a very
common way this comes up where that
President of the United States thing is
the usual like easy way people think
about this like that changes over time
uh and the model may think that it's
Donald Trump but it's actually Joe Biden
now and so if I give the model a
document that says the president of the
United States is Joe Biden and then I
ask the model who's the president
president it has this inherent tension
and so the model has to be like set up
to do one or the other there's
interesting like safety or philosophical
questions that come in there as well
like let's say I'm internet augmenting
and I'm asking about elections and
that's a key place where you might get
Bad actors who might make easy to
retrieve information that will confuse
or like break uh some of these systems
and it might happen by accident as well
so like a lot of the things where
there's been in the news issues with say
Gemini producing some quite strange
answers that's often happening because
of a
misinterpretation plus faithfulness kind
of problem is like the model is saying
these quite ridiculous things that
without the retrieval augmentation it
probably wouldn't be saying but it's
being kind of like injected in some ways
or it's other things are going on where
yeah you get these very strange results
that aren't aren't great coming out of
it and they're they're kind of
illustrating the challenge of this
augmented generation step so if there
faithfulness problem you also have
additional issues that are kind of like
less obvious when it comes to augmented
generation where you want your model to
stay producing a nice pleasing response
for usually in a chatbot situation and
so you want it to say like a nice fluent
response you're also counterposing that
against this this faithfulness thing and
so sometimes
it's not always a zero some game but
these things can be in slight tension
that it's often difficult to balance
Faithfully representing the information
in your retrieved information versus
writing a nice high utility response to
a to a person one of the tricky
empirical things to get right is where
you need to decide if a question is
answerable or not so you've got the OTR
information does it answer the question
or like the the information need of the
person using your your retrieval
augmented generation system that's
pretty tricky uh cuz the way things that
come out of retrieval systems might be
is sort of like partially extracted from
their context that usually just chunks
of information from pages so that's a
very tricky like decision boundary to
make as well there's practical
applications there about also like is it
worse for a model to sometimes respond
where there isn't information in its
retrieved in in its retrieval stuff
counterposed with is abstaining from
answering all the like like a lot if
there's going to be errors which do you
prefer it's almost like a Precision
recall style thing like you can't always
have 100% accuracy here so would you
prefer a model that refuses to answer
more sometimes be quite frustrating
because of that or would you prefer a
model that runs the risk of as it were
hallucinating a response and so these
are really really tricky things to like
drill down and get right especially when
like you have to do already so much work
to get to a point where the system is at
the point where it even works at all
like prompt engineering to get augmented
generation working is really really hard
and sort of what we do at cooh here is
try and build like actual like prompt
templates that the model has like very
very concrete like awareness of in the
way that we Post train and so it there
is a correct way I guess uh to do the
prompt engineering there and we'll
template that for you using the API and
so you can avoid a lot of the startup
cost associated with building one of
these things and so then the model kind
of is familiar with the concept of a
retrieved thing it knows what that is it
knows how to deal with it it knows that
a bunch of retrieval results in a row if
I permute the order of those it should
have no effect on the model's answers
right like just because the I I take my
top 10 retriever results and Shuffle the
order it shouldn't change the answer
quality but for every single llm other
than ours it does oh interesting and we
we take step to avoid that basically but
we can only do that because we control
the prpt templates and so like
effectively our our apis have a
documents parameter or a tool output
parameter that we you we can use
to ensure the model doesn't violate some
of these principles in training at least
when I do evaluations in other people's
models that's also hard because you
don't necessarily know the optimal
prompt template to render for you know
the best result you can get from GPT 40
for example like I find it very hard to
evaluate rag systems and like know the
true upper bound of like how good
someone's models can be I know it for my
model because we put a lot of effort in
but when I try and do like a true
faithful comparison which is something I
really care about it gets very tricky
there's also other like interesting
things that happen like what should you
do if two retrieval pieces of retrieved
information are in contradiction with
each other what should you do if you
have like ranges of information so let's
say I ask a numerical question and I
find a bunch of different answers for
what that numerical answer is these are
all quite pragmatic things to work
through but they're things you have to
like decide about how your language
model should work um and they're
interesting to kind of work through I
mean the picture I'm getting is that
this is insanely complex so that there's
this entire Sausage Factory and you've
done some stuff and obviously the
customer needs to configure this quite a
lot yeah and one immediate observation
is information retrieval is already it's
it's an interesting example in software
because normally when Microsoft or
Google or or whatever when they build
software they're solving the hard
problem and they give it to you via an
interface and you do the easy problem
whereas of information retrieval if
you're building an Enterprise search
system it's actually harder than
building Google what what I mean by that
and I'm glad you agree what I mean by
that is Google has like all of this
statistically significant information
they've got PID AG rank you know there's
this like hypergraph you know they
they've got all of these head queries
they've got millions of people clicking
on you know you search for this millions
of people click on it they've got the
total number of clicks they've got
really powerful information in the
Enterprise you're first of all you're
you know you've got multimodal data and
you've only got really content metadata
you you can't learn to rank you can't
learn relevancy from the users really
and even harder to do so with rag on on
the top of it so um and then you've got
all of this stuff that you're that
you're just talking about so I need to
evaluate system I need to fine-tune it
and like you know it's so nebulous it's
very tough I think it's also like yeah
it's one of those things where even if
you try to evaluate it you don't get a
true sense of like what the what the
problems that arise in application might
be you have to like build the thing and
put it out there and get like Alpha
feedback uh from people trying it out to
really discover where the problems are
for your datab base or your kind of
stuff we can't easily know beforehand so
like we can try and like make the model
as good as it can be I don't work these
days on the retrieval system itself
there's a different team Neils um runs
who who who build cohes embedding and
reranking models that are very strong
but like again they they're they're
based on semantic content uh rather than
uh yeah any kind of like statistical
usage or um things like that um it can
do things with metadata as well MH but
again it's all based on like getting the
model to kind of not reason you know as
we talked about but like try and try and
um rank Things based on content uh like
the the content of the information
rather than like how often it gets used
could you talk to that a little bit
because because you you use the
terminology um you know dense versus
sparse Retriever and I think what you
mean by by by sparse you I think you
basically mean an inverted index like an
old school usually yeah it comes back to
word anyway we going back to this this
these same like Core Concepts really of
say like okay we can represent a
document by the words that are in it and
that's usually how the stuff works uh so
that like what what's there in the
document we can either do that by
recording uh Spar features like the
words or we can do that by embedding the
document in some way or or uh storing a
representation that's like comes out of
a neural model or some kind of like low
dimensional uh representation of that
those are the two kind of broad classes
and then if you do it with a sparse
representation that traditionally things
like TF IDF bm25 these kind of things
are basically based on you just say I'm
going to break the document up into
words and I'm going to build very long
very sparse vectors that have a
vocabulary size and I'm just going to
count all the terms or words that appear
in my document that's going to be my
representation and then I'm going to
build an index of all the documents in
there and you build these efficient like
lookup systems like you said like an
inverted index that makes it quick and
easy for me to to take a query which is
usually some words and do the lookup to
quickly find Which documents have those
words in them there's a lot of finery
that goes into uh the specifics of how
you like Mo modulate the weights of the
um Vector the sparse Vector so is it
just like binary this word is in here or
is it not usually you moderate it by
like how much more that word appears in
the document than in the background and
there's Decades of information retrieval
research that that went into that that
was the sort of primary way that that um
information retrieval worked before
representation model got good enough to
build these dense Vector models uh that
usually either represent a chunk as a
single vector or now there's multi
Vector techniques that kind of do it
slightly differently they're effectively
like just different ways of representing
the information in chunks either with
the sparse representation of individual
features uh or these kind of yeah
semantic spaces that you build and and
retrieve over in a kind of different way
so with with the inverted index you know
the good thing about it is you can
essentially use a hash table so you got
constant lookup time and um with the the
dense version because it's in some kind
of a vector store you've got like a a
log log lookup time but what I want to
say though is you know the the good
thing about the sparse version is
specificity and the bad thing about the
sparse version is specif you know and
like you've got let's say a high entropy
word or something U you know so like a
low frequency word that's great you know
boom I've got I've got the thing you
know but but it doesn't always work that
way so what what how do you contrast the
two yeah if if you ask a a sufficiently
generic or like surface form query you
often don't get what you want with a
with a sparse index usually also in the
way that we've described the sparse
indices so far it's hard harder to learn
learn from data so sometimes we have
data that allows us to know that like
certain queries pair with certain
documents like well we may be able to
add those as like priors and so
initially like a lot of the things
around like the dense methods were in
favor because you could train a model to
rank a certain document very closely to
like a an embedded query or question
more easily or with more kind of power
and like more connectivist kind of like
thinking than you could with the spast
ways of doing things they were a bit
more powerful from that perspective but
yeah like there are issues with the
dense model as well is like you don't
get that specificity so like it is
sometimes harder with especially a a
low-dimensional um dense Vector
retriever if you do put some like very
specialized terms in it it might not
have the like represenation of power to
like do that kind of very precise
addressing so usually in practice a lot
of the systems that are most performant
are a hybrid so you do a dense retrieval
and you'll do a sparse retrieval and
you'll combine them and throw them
through what we do in a lot of my work
like in our team's work now is assume a
blackbox retrievable system there's so
much for us to focus on we will assume a
blackbox r retriever to just try and
deal with what comes out um and that
might look something like there's a
there's a concept called query
reformulation where like we're trying to
train these rag models which to some
degree now now we can think of them as
research agents that kind of like go
over several hops but they'll like try a
try a query see if they find what they
what they like and they might not find
it they can reformulate so they can say
okay that query didn't work I'm going to
try it a different type of query that
might be similar but like you know I've
added some words or I've changed some
words and try again so you can repeat
this like reformulation process where
you um it's almost like a gential
information Ral yeah so so you know
there's actually a kind of iterative
loop there because because I was going
to say as well um that another thing you
can do is kind of query um expansion
yeah and I guess that's even more
relevant for for sparse indices but it
just goes to show there's this huge
Sausage Factory and it could it could be
an agential thing and before we move off
retrieval I think one of the the biggest
concerns I've heard from people is the
sensitivity problem so the recall
problem um which is that there might be
missing information in the results so
the language model will kind of because
there's this sick of fancy problem that
the language model will say whatever you
tell it to say fill in the blanks yeah
it'll yeah it'll fill in the blanks
it'll confidently say something is the
the right answer when it's missing the
underlying information yes that's a
great point so that kind of we can
transition to talk about grounding and
attribution and citation all like
different words for sort of the same
thing in our view that how we train our
systems and how we annotate for our data
that like helps us train our systems is
that we when we're doing this augmented
generation step the Second Step the
model generates its answer and we ensure
that all of our annotation every claim
that is made by the model like something
that can be recognized as like a piece
of like like a factual claim or like a
piece of knowledge we ensure that it is
annotated with a link back of the
retrieved piece of information where it
comes from or the several pieces were
very very fastidious about this and we
indicate to annotators that like unless
there's Specific Instructions for the
task that's going on every single
factual claim has to be attributed back
and that is the way in which we've kind
of over time really built out our
model's abilities to be very faithful
and to avoid that sick of fancy problem
is like the model has a prior or bias
effectively trained in into it that it
really doesn't like to generate things
it can't then attribute the way this
works actually in practice is that we
generate the text with these markers in
it so like these little citation markers
that appear like span uh indicators
around facts and um number one like as a
user you get the immediate like visual
feedback I guess of like if a claim is
being made by the model you see oh
there's no like the model hasn't
generated any um citation spans or
attribution spans around here that's
much less likely to be something the
model could support with evidence and
therefore it's likely to be a
hallucination even if there are like you
know those those markers there it's less
a sense of like hey this is like true
this is like something I can trust it's
more I have a pointer that now lets me
verify it easily so I can look and see
oh like here's this piece of information
I want to believe the model has told me
like responsible operation of like any
piece of technology or Machinery is like
it's on you to take that information
that you want to believe and
sufficiently convince yourself that that
is something that you should then go and
make decisions about we don't just say
anyone can drive a car you have to train
your yourself to drive a car again with
like online stuff is like if you see a
claim on on an online like social media
platform
you shouldn't just take it at face value
the same thing with like an nlm
producing an answer from a retrieval
system but what we should do is design
the way the retri the retrieval
retrieval augmented system works to make
it as simple and straightforward to go
and verify that information and Trace
the information Trace back to like where
it comes from and that's what these
spans are about so you say the spans are
there like it it's saying this
information comes from document one and
document two
you do then if you want to make a
decision based on that you should really
go and look at document one and document
two to make sure but we at least tell
you it's from document one and document
two whereas if a model's just generating
this answer directly it's much much
harder because you may have 30 50 100
documents to look through like pages and
pages of stuff yeah and that becomes
more of a pressing concern with long
context um so as we get models that can
process more and more and more tokens at
one time this like grounding thing of
saying like I got this info from here
becomes very important otherwise it's
just kind of reverts all the way back to
just being an unaugmented model because
there's just so much stuff there that
it's effectively you know not too
different in in terms of just like
asking the model to do something based
on its sort of parametric so so two
pieces on on that you you said first of
all your annotators your your annotators
so the interesting thing is that the
command r model is actually specifically
trained to do rag and you you
got a whole bunch of annotators and you
and you've said this is what good looks
like and and that's why it's better for
rag than than other models there's
specific large amounts of annotation uh
in our post trainining data and like our
supervised fine tuning step and our
preference uh steps uh to make it good
at this it's not just the sense of it
like it being good it's also like the
sense of these built-in citation
mechanisms there are specific prompts
and they're documented on hugging
hugging face there's links to them
effectively like that show you what
those prompts look like that if you use
those specific types of prompt the model
will kind of switch into a rag mode and
will reduce its sick of Fancy by by just
prior and bias and it will switch to
something much drier much more faithful
much less creative but much more
appropriate for a system that's always
going to be augmented by retrieved
information or information that's
augmented from Tool output same thing
with tool use like there are special
prompts for Tool use that are documented
on on the hugging face model card for
command R and R plus that if you use
you'll get this intentional uh Behavior
yeah yeah yeah could you could you
comment on the UI in general because I
mean maybe like we could look at gemini
or perplexity as as an illustrative
example of rag so you know you do a
search you have a load of results coming
back I mean in the olden days I would go
to Google if I'm planning a holiday and
I would kind of you know have a whole
bunch of stuff in my mind i' have
different tabs open yeah and now that
entire kind of cognitive tree has been
compressed into a load of text results
and on Gemini is quite interesting as
well so when it generates code you can
actually hover over the lines of code
and it will tell you oh this came from
GitHub and you can go and look at it and
that kind of makes it a lot more
transparent but part of me is also
thinking you know like in in the 1980s
we used to have to use command lines to
to talk you know we used to have to use
text to talk to computers and there was
this UI Revolution and now it's far more
structured and like do you think is it a
good thing to go back to a textual
interface I think in general there's
like very interesting questions to ask
about like human computer
interaction and what like language
models and these these kind of systems
mean for that we do two things we think
about how we can make our rag models
work generally
well and how we can make them work very
well in specific user interfaces or or
in specific ux given that we're now
making these research agents agents need
an environment to operate in and not
necessarily a space of potential
environments and so it's quite hard to
make a really good Agent unless you say
here is the UI that it operates in and
these are the rules that should follow
and so like we we do think quite
carefully about where is the destination
for our language model acting as an
agent being and like how can we like
optimize for that like fully so like say
like we have eoh here the ability to
like control the entire training stack
all the way from pre-training to the
final thing that you see we can take
advantage of that to some degree to make
something very very performant in a
specific user interface or UI the
specifics of what that UI should looks
like I think is an is an open question
especially when it comes to like getting
things that are useful for people to
increase their productivity or like get
feel that they're augmenting themselves
and not just smashing stuff into a fancy
search bar often it comes down to like
you know not just what should the UI
look like but also things that are
critical for this to be useful like
certain parts of the system need to have
very low latency that will affect um a
bunch of a bunch of like design
decisions about like how the agent
should work how the llm inference should
work uh sizes of models the tradeoff
between the size and the quality of the
model to enable like the best thing to
help you like get your job done faster
tell me about your story how did you how
did you end up at cooh here I'm a
trained synthetic chemist organic
chemist but I kind of didn't really
enjoy the the Practical elements very
much uh I enjoyed the maths and the
physics of chemistry and in my my fourth
year the Master's year so I ended up in
this kind of research Masters uh working
in a chemical informatics group I was
working with like a great professor
professor Jonathan Gman and he kind of
just said hey do whatever you think is
interesting but what I think is
interesting is trying to understand
things about chemistry from the
chemistry literature and so what we did
in that project back in like 2014 2015
it's like quite a long time ago now I
guess was first collecting a big Corpus
of of chemical literature so like
abstracts of chemistry papers and some
small number of millions I think it was
at the time so I wrote These scrapers to
go do that stuff and then trained like a
big simple algorithm on it called word
which is quite famous it's like one of
the early interesting unsupervised NLP
algorithms so just sort of trained up
this model and then we took this model
and I and I collected the word vectors
for the chemical elements that this
algorithm had learned so word Toc is
just a word embedding algorithm it tries
to build vectors in a semantic space for
words and so what I did is is clustered
the chemical uh Element word vectors and
visualized them with a few different
methods one of them was was uh tne
others were some some Force directed
algorithms but the thing that like
really amazed me is that even back in
2015 this algorithm when you did this
clustering of the elements they
clustered in such a way that they
reproduced the periodic table without
any kind of like explicit instruction or
knowledge it just had like read the
literature and had uh discovered the
periodic table within it and literally
like the noble gases were were in one
cluster and then the the non-metals and
then uh you know types of of metals and
it all kind of just made semantic sense
and that was like crazy moment for me
that I was like oh this is amazing I
want to do this and yeah I kind of then
quite quickly you know wanted to find
something to work that I could do more
of this work as a research scientist um
working you know always at this
intersection of knowledge and Ai and
thinking about like how we can use NLP
systems to access and leverage knowledge
you know manipulate knowledge for like
the kind of applications that that
humans find useful and and we can learn
things from and then yeah I think I
wanted to
do something a little bit more longterm
like I found like the research loop at
the time would last about you know three
months you you'd furiously write code
and you'd experiment on something and
then you'd write your paper and it would
kind of be over and your research Arc
and your kind of research journey and
your body of work would grow but I never
felt that we were like building
something in the same kind of way and I
kind of wanted to do that so yeah like
moved to co here and build built the
team for Rag and tools and here we are
almost like guess two years later um
talking to you amazing I take it you
know um Ed grffin from UCL as well right
yeah and he he introduced me to minchi
yes mini is a legend I've had him on the
podcast twice um the um the first one
was actually the most downloaded audio
podcast we've ever had on the channel um
I I think I think even above Nome
Chomsky wow that's great that's a that's
a badge of honor yeah he's amazing I
haven't I haven't seen him for a a year
or two now but like yeah we we used to
hang out a bunch uh cuz yeah he was a
fair PhD student one of the smartest
people like he's so sharp uh and like
has he expresses himself very precisely
as well a bit like Ed um yeah great guy
I know um I'm I'm really happy that I
got the scoop on on minchi is uh he's
you know he's the perfect guy to
interview because he's just razor sharp
amazing communicator but um a couple of
things on what you said so my co-host
Keith he has a PHD in organic chemistry
from MIT oh cool and the transferability
is insane I mean he's a god of basian
analysis and he kind of you know cut his
teeth when when he was at MIT and um I
mean you know even things like
bioinformatics when I did that on my
undergrad there was lots of sequence
modeling and you know needman VCH and um
you know all all of these um sort of
like approximate multiple sequence
matching algorithm there's loads of
computer science in there but um you
started talking about word by mikolov I
think that was 2013 was that algorithm
out and that's a really interesting
place to start so how does that
algorithm work I think when it came out
so there's different way lenses to look
at like how how that algorithm works and
how it connects back to um algorithms
before the broad idea of of these
algorithms is like you say I want to
learn a representation for a word so
like I I want to be able to embed this
word in in a meaningful space where like
words that are more similar to each
other cluster together or like exist
closer together in the space than words
that are more like distant from each
other the core idea behind a lot of
these is like a word is defined by like
its use in a sentence or use in language
and so what you can kind of do is say
like take a word and try to predict what
the nearby words might be or
equivalently take nearby words and try
and predict what the word in the middle
might be um yes you know a word by the
company it keeps yes exactly yeah that's
that's the is it by good what was the
name of the I'm not sure who it is yeah
it's like it's a definitely a linguist
from like
um heard that like yeah they repeated
and so effectively like word Toc there's
different actually ones within there
there's one called a skip gram and a
continuous bag of words are the two like
implementations of this it's actually
very similar to M language modeling like
the kind of modeling that's used for
they fall Le a little bit out of favor
now but like BTS and models like that
where you effectively predict what the
word might be from the one from it from
its neighbors uh it's just not doing it
contextually it's just trying to say
like we will learn a word from all of
the places it gets used and it's going
to be a representation effectively of
all the words around it and if you do
that it turns out that the optimum that
you get is is pretty powerful at
describing you know what a word means
and so if we went back to the chemistry
example it's seeing chemical elements
appearing in in these in these abstracts
and the words around it are similar for
uh chemicals that are similar and so if
you do this thing and you take the
chemical words what you end up with is
these embeddings that yeah have like
just naturally learned to be similar for
chemicals that have similar chemical
properties because we write about
chemicals that have similar properties
in the same kind of ways um and so it's
sort of like a distillation of the
knowledge really a kind of just way of
representing that knowledge it's just
really really nice unsupervised kind of
thing the other kind of way of looking
at this in a kind of more technical
perspective is like a lot of these word
embedding algorithms and there was like
a very big kind of flurry of Interest
after you could see that say word toet
could do some like simple like word
analogy tasks as the famous like King
man and woman and queen I think like
analogies and it there was like a whole
like zoo of these models that came out
the year after the year after that
looking at like you know what can you do
to improve this or like you know whe
what are some like refinements but if
you actually like look back as well
there's like algorithms that almost do
exactly the same thing but weren't done
at the kind of scale or like with the
same kind of framing so a lot of these
all end up kind of being imagining a
very big word by word word cooccurrence
Matrix columns equals like number of
words in your vocab rows equals number
of words in your vocab and you just
build a big sparse Matrix of where words
cooccur within a certain distance of
each other and that's a very big sparse
Matrix and if you do a decomposition so
like like factorize um that into smaller
dense representations of these so like
word by Dimension and dimension by word
and if you multiplied those things
together would you recover that sparse
co-occurrence Matrix a lot of the
algorithms end up being mathematically
like an an approximation of of that um
factorization so you can kind of think
of it like that word Vector is is trying
to be a kind of low dimensional
representation of the co-occurrence
Matrix of like literally trying to
predict what the words that are nearby
would be so it's it's very well it's
almost analogous to collaborative
filtering you know where like you might
have net and you have people and you
have films that they like and you've got
this big sparse Association Matrix yeah
very similar like um yeah and then as
you say you factorize it I think there
was a glove algorithm for word
embeddings which kind of did it entirely
and then you can think of the mov type
algorithm as being like a kind of online
version of that way kind of through the
data um um that's exactly it yeah but
the the actual algorithm itself I guess
it had like two inputs so it was like a
sliding algorithm and you would have is
it a neighborhood or or adjacent words
and then would learn some kind of
intermediate
representation uh yeah for for word Toc
uh stretching my my my exact knowledge
and there were like two different ways
of it but effectively you'd have this
traveling word window and you'd like
slide it over your big Corpus and you'd
say right I've got the word vectors for
this window and I'm going to try and use
the outside ones uh I'm going to sum
them up and I'm going to blast them
through a linear layer and then I'm
going to predict the middle one and
you're just going to see how similar two
things are it's like the middle one that
you've stored is that close to like the
sum effectively of the with some
transformation you can also throw in
like neural networks at this point if
you wanted to like do a bit more
transformation you're just trying to
basically make the aggregation of the
surrounding words be as close as
possible to the word Vector that you've
got stored for your middle one initially
starting from random and then over time
you optimize and you do gradient steps
and you just repeat this algorithm again
and again it's just an optimization
thing like not unlike a lot of modern
NLP and yeah it's just like a the
beginning of these kind of like yeah
distributed continuous NLP things um
yeah and and to kind of touch on that so
I mean it's interesting that it's a
dense continuous vector and like the
Netflix recommendations thing it also
has the ability to fill in the gaps and
that's a kind of generalization so that
means in in situations it might not have
seen in training it could actually make
some inference and and it's a highly
dimensional space as well so it's kind
of representation learning where
different dimensions might represent
different things and the algorithm when
trained in this way because these P
these neighborhood patterns exist in in
the Corpus it'll kind of fill in this
information in the representation yeah
cuz it has this like relatively low
dimensional embedding it needs to make
from this high dimensional space it kind
of like forces it to be quite efficient
in a way with like how it's observing
like the the things and like it has to
like
decide the best way to represent this
sort of information bottleneck kind of
thing and so like it's going to combine
features that correlate in the high
dimensional space so it can like you
know save some representational power
and like that's how you kind of learn
you go from this like super high
dimensional Spar thing and it sort of
summarizes if you will like this
information into the lower dimensional
spaces and they might be entangled as
you know like the actual like meaningful
uh dimension of variation for some
concept that we'd recognize might be
spread across several dimensions of the
actual word vectors you end up learning
but there'll be a transformation where
you can kind of find it that's kind of
like the magic of this stuff is like the
representation learning
forces very sparse information to be
represented by something like simpler
and more summarized you can make an
analogy to how large language models
work even though they have very large
Hidden uh States uh and they have lots
and lots of parameters to do it it still
pales in comparison to the know the
amount of information they're trying to
represent I I remember when Burke came
out in 2018 so that that was like a kind
of um a masked encoda model where you
would kind of like mask out um tokens in
a sequence and it would give you really
good representations yes it's quite
interesting we talked about started
talking about word to it's basically the
same idea except if it was trying to
make representations of that specific
word in that specific sentence or like
paragraph or context and so you have to
like rather than just sayy hey here's my
static word Vector you have to have a
model that like takes as your your
sentence as input and then like embeds
it on the Fly effectively to get you
your your like contextualized word
Vector which is unique just to that
sentence and then you've got it and
that's very like a very powerful
representation that you can then go do
tasks with the first GPT came out
roughly the same sort of time but it was
a yeah just um a different way of like
you could think of this generatively or
you could think of this as like a
representation learning thing it's like
what are we trying to do a predict
generate generate stuff or are we just
trying to like get like good
representations of language so Bert was
a bidirectional encoding scheme that
learned representations you know going
forwards and backwards and backwards
yeah and GPT was was different yeah left
to right people obsessed about this
quite a bit like like I did other people
did and like thinking that this like B
directionality is very important for
like being able to like get a very rich
representation of what the language is
from an empirical perspective that's
less of a thing these days like a lot of
the way that we use language models now
you're not taking the representations
from early on and doing stuff with them
you're prompting the model to generate
something later on so like you always
get like a full attention on what's
happened and you don't necessarily often
have to look forward in time very much
although like there you know there's the
whole like text diffusion stuff going on
at the moment which kind of is a little
bit different that you might rather than
generate sequences out token by token
for W you can just generate the whole
thing a lot and just kind of iterate
this non-auto regressive text generation
was a big thing that people were excited
about teams who have gone away and now
are starting to come back again but but
on on that CU that's a really
interesting inductive PRI which is first
of all there's there's the locality PRI
which is that you know we're interested
in things in in the vicinity but then
there's this kind of Prior that says I'm
only interested in things before not in
front why would that work when you write
or talk you can kind of think about what
you're going to say next but you can
only say one thing at once and so you
have to express yourself in a sequence
like in a in a like like just a sequence
that goes along it feels a little bit
more like that like you know the model
can have this internal representation of
what it's planning to do next like we
don't know uh probably there's something
in there uh of like that kind of type of
phenomenon but it's just going one at a
time in the same way that I'm talking
one at a time I can't say my entire
thought like Splat all at once same
thing when I write like I might write a
document in a hierarchical way where I
first write a plan then I flesh out each
bit but I'm still writing each
individual like token one by one and
that's just like I don't know something
to do with time I guess um you know like
maybe a human doesn't exist outside of
time and can't just like continually
like simultaneously write an entire
document in like one big go you can only
do one thing at a time like why we we
think that a model that can only do one
thing at once and does go in like you
know a single Direction can't be like a
you know generally IG system like I I I
don't see why that's important you might
say like hey it's easier if the model
can
observe everything at once and kind of
contextualize across that's just a
engineering detail in a way yeah but in
a way it's it's kind of um
anthropomorphic because I was reading
the language game by Nick chater and
Mortin Christensen I've interviewed them
as well and they called it the Now or
Never bottleneck which is that you know
human attention it's almost like a razor
point we can only pay attention to one
word at a time even Even in our visual
field most of it's discarded we only
just you know take yes take in a very
small amount so the remarkable thing is
that that's the way that we've learned
to communicate with language and
presumably it's I guess a good thing to
mimic that in a language model it might
be a good thing or it might just be
sufficient yeah and it may not be
optimal but like for models that uh like
I'm going to keep it constraints to to
text and and language they're training
training to model text and language and
all of that except for the last year or
two has been written or generated by
humans um and so because they think word
by word yeah you can model model these
things like that way pretty successfully
maybe a counter argument is like the way
I generate a document if say I'm writing
something like I'll write my plan and
then I'll go through and I'll edit it
but the final finished piece that's the
thing You observe and the model tries to
to generate yeah that's why you know
it's pretty pretty straightforward and
intuitive why things like Chain of
Thought
and stuff like that works is because
you're basically it might it's sort of
part of the necessary process for
actually like producing a good finished
product when you can only go left or
right you know like very few people like
write a perfect document uh or some like
finished polished piece of work by just
like writing their paper like start to
finish effectively all of that like
intermediate like writing and rewriting
is is human Chain of Thought uh that
doesn't always get observed but if you
get a model to do it suddenly you get
these big improvements this like
internal monologue almost um is useful
Patrick it's been an absolute honor to
have you on mlst thank you so much for
joining us today thank you so much for
having me it's been great uh yeah love
it cool great thank you

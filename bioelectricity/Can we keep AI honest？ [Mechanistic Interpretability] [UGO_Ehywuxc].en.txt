how would you know if a large language
model was lying to you if you give chat
GPT a certain phrase and ask it to
forget the phrase it will claim that it
has however since the phrase is part of
the model's context window this is
actually impossible and if you Badger
chat GPT enough it will admit that it
actually does still know the phrase and
repeat it back to you although we can
and do train our AI assistants like chat
GPT to be helpful and honest through
specific examples we have no direct
access or control over model Concepts or
behaviors like truthfulness this problem
of llm interpretability is an active
area of research one of the most
promising approaches involves extracting
model features using a separate learning
algorithm called a sparse Auto encoder
these extracted features often
correspond to human understandable
Concepts like cats dogs Wi-Fi networks
and more complex Concepts like internal
conflict remarkably once we have a
feature we can increase or decrease its
strength in the model it was extracted
from
if we increase the value of the internal
conflict feature in anthropics CLA model
and ask it to forget a phrase it will
immediately admit that it can't actually
forget words examples like this are
compelling but as one of the key authors
of this work Chris Ola has pointed out
we've only been able to extract a tiny
portion likely less than 1% of the
concepts that we know large language
models must know about Chris uses a
great analogy here the features we
haven't been able to to extract yet
maybe a kind of dark matter of
interpretability the feature extraction
gives us a telescope allowing us to see
the brightest stars in the models
Universe we may be able to build better
and better telescopes and fully
understand what's going on in large
language models or it might be the case
that a significant portion of what these
models have learned can only be observed
indirectly but why is it so difficult to
understand how these models work in the
first place why do we need to train a
completely separate model to begin to
make sense of what a language model has
learned why can't we just train these
models to be understandable in the first
place why are we only able to extract a
tiny portion of all model features and
why can't we just scale up sparse Auto
encoders to peer deeper and deeper into
the universe of language
models let's follow the path of some
text through a large language model
we'll start with the phrase the
reliability of Wikipedia is vary and see
if we can make sense of how the model
decides what to say next we'll use Gemma
2B which is a scaled down version of
Google's Gemini model first each of the
six words in our phrases is converted
into a token from Jemma's vocabulary and
each token is mapped to a vector of
length
234 these vectors are stacked together
into a matrix of Dimensions 6X 2304 and
passed into the first layer of
Gemma each layer of language models like
Gemma consist of an attention and
multi-layer perceptron compute block
these compute blocks return new new
matrices of the same size as their
inputs so after passing our 6X 23 or4
input Matrix into the attention Block in
our first layer of the model we get back
a new 6X 2304 Matrix we then add this
Matrix to our original input Matrix and
the result becomes the input to our next
compute block the output of this block
again a 6X 234 Matrix is added to our
input just as we did before completing
the first layer of Gemma this output is
then passed into the second layer of
Gemma which does the exact same thing
just using different learned weight
values we repeat this process again and
again with Gemma incrementally
transforming its input Matrix layer by
layer into a new Matrix of the same size
this Matrix we keep updating by adding
the outputs of each compute block to is
called the residual stream after passing
through all 26 layers to figure out what
gemo is going to say next we just take
the last row of the final Matrix and map
it back to a word interestingly to do
this we multiply the last row of our
final matrix by an unembedded Matrix
which results in a new Vector of length
256,000 where every entry corresponds to
a single token in jma's 256,000 token
vocabulary this Vector is interesting
because after normalizing with a soft
Max function it gives us the probability
according to Gemma of each token in
Jim's vocabulary occurring next we can
rank these tokens by their probabilities
and get a sense for what Jemma thinks
about the reliability of
Wikipedia the most probable next token
is the word important with a probability
of 20.21% we can get Gemma to expand on
this by pending the vector for the word
important to our original input and
passing this new slightly larger Matrix
through Gemma to find the next word in
the sequence repeating this process we
see Gemma giving a nuanced take on a
Wikipedia as we would expect from a
well-tuned
model however the next word choice of
important was only assigned a
probability of 20.21% and Jemma's other
probable options lead us down very
different paths with a probability of
11.16% Gemma will tell us that the
reliability of Wikipedia is very high or
Gemma could go the other way and tell us
that the reliability of Wikipedia is
very low questionable or poor with
probabilities of 10.8 9.48 and
5.47%
respectively these lower probability
options are important because production
systems generally do not just pick the
most likely next token this often leads
to uninteresting or unhelpful responses
instead next tokens are sampled from a
modified version of the model's
probability distribution so in practice
this version of Gemma will give us
different takes on the reliability of
Wikipedia some nuanced some positive and
some skeptical now note that so far
we're not using the instruction tuned
version of Gemma this final version of
the the model includes a number of
posttraining steps to better align Gemma
with the behaviors we expect from an AI
assistant interestingly if we switch to
the instruction tuned version this
increases the probability of measured
takes such as the reliability of
Wikipedia is very much a topic of debate
there are still skeptical takes that
Gemma could deliver but they are less
likely after instruction tuning
posttraining steps like these used to
tune Gemma have proven reasonably
effective it's shaping the behaviors we
want from AI assist
however these techniques do not give us
Direct Control or understanding of
specific model behaviors a more direct
approach is to open up the model itself
and try to figure out exactly which
parts are creating specific
behaviors where exactly in jima's 2
billion parameters spread across 26
layers has Jimma decided that Wikipedia
is reliable or not a recent wave of
these efforts popularized under the name
mechanistic interpretability by Chris
Ola has made impressive progress
let's apply some ideas from mechanistic
interpretability to our gyma model and
see if we can make sense of what's going
on putting together the Gemma
walkthrough for this video required a
ton of hacking on projects like this I
really value uninterrupted Focus time
which this video sponsor in cogy has
really helped me with as a dad of two
young kids my phone generally needs to
be on Legit stuff comes up all the time
but spam calls and texts can be a huge
distraction a couple of months ago when
I was considering during working with
incog on this video I signed up for an
account and I couldn't be happier with
the results I'm getting far fewer spam
texts and calls and more uninterrupted
Focus time the way incog does this is
really impressive after signing up for
an account you give incog permission to
work on your behalf to contact data
Brokers to remove your data which
Brokers are generally legally obligated
to do upon request from here you get
this great dashboard that tracks all the
removal requests and progress It's
really impressive my data has been
removed from 115 separate data Brokers
so far this would be incredibly
timeconsuming for me to do manually in
the United States we also have these
people search sites where for a small
fee anyone can look up information about
you like your address email phone number
education employment history social
media accounts and so on it's pretty
wild I signed up for an account on one
of these sites it's crazy how much
information I was able to find on my
wife who I have not yet added to my
incog account which I will be doing they
have a great family and friends plan by
comparison after being an incog customer
for a couple of months impressively I
wasn't able to find any records of
myself on the same people search site
you can get a great deal on incog 60%
off an annual plan by using the code
Welch Labs or following the link in the
description below plus it helps me
continue to make great content huge
thanks to incog for sponsoring this
video and helping me take back my data
from data Brokers and get more quality
Focus time if incog sounds like a good
good fit for you please check it out it
really helps me out now back to the dark
matter of AI let's apply some ideas from
mechanistic interpretability to our
gimma model to get a better sense for
how these techniques work let's
visualize our text as it passes through
the model recall that our six-word
prompt the reliability of Wikipedia is
vary is converted into a 6X 2304 Matrix
and each block of Gemma adds a new 6X 23
or 4 Matrix to this Matrix and this
modif Matrix is known as the residual
stream as it moves through the
model after the final layer the last row
of the residual stream is converted back
into a token and becomes what Gemma says
next let's visualize this final row of
the residual stream as it moves through
the model visualizing a vector of 234
floating Point numbers is a bit tricky
let's rearrange our Vector into a 48x 48
Matrix and visualize each number as the
intensity of a pixel in an image to
hopefully make it easier to spot
patterns in our data as it moves through
the
model before our first layer our Vector
looks like this with a few large
positive and large negative values that
stand out in our image note that we
don't have to wait till the end of our
model to map this Vector back to a token
at any point we can normalize this
vector and multiply by our unembedded
Matrix as we would at the end of the
model to see what token our Vector
represents generally this Vector would
correspond to the word vary with a prob
probability of 100% because we haven't
done anything to our input Matrix yet
and this last row of our Matrix is just
the mapping or embedding of the last
word in our phrase which is the word
vary however this version of Gemma uses
a soft cap function before producing
final probabilities which limits the
model's confidence in any single next
word interestingly the effect here is
for the model to predict variance of the
word vary including different
capitalizations and even different
languages let's see how Jim is first
compute block changes our embedding
vector image the output of the attention
Block in the first layer of Gemma looks
like this and when we add it to our
residual stream it now looks like this
if we map our new Vector to a token we
don't see much change with variance of
the word vary now being predicted with
slightly different probabilities so if
our model was only composed of this
compute block the next word predicted
would be the word vary so Jimma would
tell us that the reliability of
Wikipedia is very very veryy and we do
often see word repetition like this in
smaller or poorly performing language
models adding the output of the
multi-layer perceptron Block in our
first layer to the residual stream our
Vector now looks like this and still
maps to variance of the word vary we see
similar behavior all the way through the
15th layer of the model note that this
does not mean that nothing is happening
in the first 14 layers remember that
we're only visualizing the last row of
our residual stream Matrix and our
residual stream is changing just not
enough to flip our top predictions yet
around the 21st layer of the model we
see for the first time expressions of
Doubt or skepticism with Jimma telling
us that the reliability of Wikipedia is
very questionable with a probability of
9% after the 21st layer perhaps we can
isolate some doubting or skepticism
behavior in this layer having a close
look at the output of the multi-layer
perceptron Block in the 21st layer we
see large values at the indices of 1393
3 1945 257 and a few others each of
these locations corresponds to the
location of a single neuron in this
layer maybe one or more of these neurons
has learned to capture doubt or
skepticism one simple way to test this
idea is to directly modify the values of
each of these neurons and see how it
impacts the model outputs if we take
neuron number 1393 and fix its output
value to minus 160 this is about twice
its observed maximum and pass our text
through our model again with this
modification in place our final outputs
do change with high moving up in the
rankings if we reverse our intervention
and clamp our output to positive 160 we
see our Trend reverse with low
questionable and poor moving
significantly up in the rankings so
increasing the output of neuron 1393 in
layer 21 increases Jemma's trust in
Wikipedia and reversing its outputs
increases its skepticism or doubt so
have we found a specific gem neuron that
controls trust or in Reverse doubting or
skeptical Behavior another way to test
this idea is to search for other
examples of text that caus neuron 1393
and layer 21 to Output large values if
we found a doubting or skeptical neuron
then the text that causes this neuron to
maximally activate should reflect this
searching through 100,000 examples from
the pile data set and collecting the
examples that maximally activate neuron
1393 these examples seem to have nothing
to do with doubt or skepticism and
instead seem to correspond to examples
of capital letters in acronyms or proper
nouns in various
context we've reached our first big
hurdle in interpreting Gemma clearly
this neuron has some bearing on the
model's doubting or skeptical Behavior
but the examples that this neuron
responds most strongly to are related to
a seemingly unrelated
concept this phenomenon of single
neurons and large language models
corresponding to multiple seemingly
unrelated Concepts has been observed
across a broad range of models and is
known as polys
semanticity interestingly polys
semanticity occurs much more frequently
in language models than in Vision
models specific neurons and vision
models have been shown to respond
uniquely to things like faces cars and
many many more recognizable Concepts in
2022 Chris Ola and the team at anthropic
published an interesting hypothesis to
explain this observed polys semanticity
the idea is that language models are
able to learn more con Concepts than
they have neurons essentially by
representing Concepts using specific
combinations of neurons the team calls
this idea superposition so Concepts may
be spread across multiple neurons and
layers in Gemma if we can't isolate
Concepts and behaviors to certain layers
or neurons how can we hope to understand
or control language models one option is
to modify the model architecture to
encourage or Force the model to have
fewer neurons fire for any given input
ideally this would stop the model from
spreading Concepts across multiple
neurons the anthropic team tried this in
2023 and found that models still
exhibited polys semanticity even in
extreme cases where they forced only a
single neuron to fire at a time another
option is to try to figure out which
combinations of neurons respond to
certain Concepts perhaps neuron 1393 and
layer 21 combined with other neurons
will cleanly represent the concept of
Doubt but how can we possibly figure out
which combinations of neurons map
cleanly to which
Concepts remarkably there is a simple
model that we can train to learn these
mappings called a sparse
autoencoder if the superposition
hypothesis is true we should be able to
take some combination of the outputs of
the neurons in a given layer and this
combination of neurons should respond
very strongly to a single specific
concept and respond very minimally to
all other
Concepts most sparse auto encoders used
today in mechanistic interpretability
work by hooking them up to a specific
point in the model
such as the output of a certain layer or
the residual stream at a certain
location so if we take the output of the
21st layer of Gemma where Gemma started
exhibiting doubting Behavior the idea
here is that we can take these 234
neuron outputs and find some combination
of these outputs that cleanly responds
to examples of doubt we can take a
single combination of our outputs by
multiplying our neuron outputs by a new
waiting Vector of length 2304 where each
entry in the waiting tells us how much
of each neuron output to take we can
then add these results together to give
us a final output value that should
correspond to the overall strength of
our concept now per the superposition
hypothesis our model represents more
Concepts than it has neurons so we need
more than 2,34 of these waiting vectors
to tease out all the different concepts
let's try modeling
16,384 different concepts so we need
16,384 separate vectors we can stack all
of these waiting vectors into a single
new Matrix of Dimension 2304 by 16 384
where each column represents the
contributions of our 234 neurons to each
concept multiplying our neuron output
vector by our waiting Matrix yields a
new Vector of length
16,384 where each entry should
correspond to the strength of a specific
concept now how do we learn the weights
for our new Matrix that will allow us to
cleanly map neurons to concepts for any
given input example we know that we only
want a very small number of our
16,384 Concepts to be active at once
otherwise we' run into the same polys
semanticity issue we saw with neurons
this is where the sparsity comes in
sparse autoencoders work by forcing most
of the values in our concept Vector to
be zero or near zero and then using the
remaining values to reconstruct the
original input reconstruction of the
original input consists of mapping from
Concepts back to neuron values which we
can do by multiplying by another weight
Matrix
this time of Dimension 16384 by
2304 so our sparse Auto encoder works by
mapping our neuron outputs to potential
Concepts better known as features by
multiplying by a weight Matrix and then
forcing most of the values in the
resulting feature Vector to be zero or
near zero and then taking these few
remaining outputs and mapping them back
to neuron outputs by multiplying by
separate weight Matrix if the
superposition hypothesis is correct and
our sparse autoencoder is working well
then our output should be a reasonably
faithful reconstruction of the original
neuron output sparse autoencoders are
trained to minimize this reconstruction
loss let's see how sparse autoencoders
apply to our gemo Wikipedia example the
Google deepmind team recently released a
project called gemos scope which
includes over 400 separate sparse Auto
encoders trained on data from various
locations in the model and across
variations of Gemma let's take the
sparse Auto encoder trained on the
outputs of the 21st layer of Gemma that
we've been working with
let's pass in our example text into our
model pass the output of our 21st layer
into our trained sparse Auto encoder and
see which elements in our concept or
feature Vector are activated we can
visualize our feature Vector in the same
way we visualized our embedding vectors
by reshaping it into a 128x 128 grid and
displaying it as an image as expected
our feature Vector is much more sparse
than our embedding
vectors now let's see if we can make
sense of the concepts or features that
our spar Auto encoder has learned a
challenge with sparse autoencoders is
that we don't know ahead of time what
actual concept any given element in our
feature Vector corresponds to we can see
that features 7344 8353 and 8249 have
high values for our Wikipedia example
but what concepts in our text are these
features responding
to as we did with individual neurons we
can get a sense for what individual
features represent by searching for
examples of text that maximally ACC
activate a given feature part of the
gyos scope project includes launching on
a platform called neuron pedia which
allows us to quickly see what examples
maximize any feature in any sparse Auto
encoder released with the gemos scope
project looking at feature 8249 for our
sparse Auto encoder we do see many
examples of text where questioning or
uncertainty are expressed we can also
amplify or reduce this feature's impact
on the model just as we did with
individual neurons clamping feature 82
49's output to a constant value of 100
impacts Jim's next word prediction as
expected increasing the probability that
jimo will tell us that the reliability
of Wikipedia is questionable we can ask
Jimma to generate more text with our
modified feature in place and see that
the steered version of the model is
highly doubtful in questioning of
Wikipedia's
reliability if we crank up this feature
to a constant value of 500 we see that
Gemma starts to just Babble mostly with
variance of the word question
so by learning to map neuron outputs to
sparse features sparse autoencoders are
able to recover human understandable
features that respond consistently to
specific Concepts in text and can even
be used to control Model Behavior in
predictable ways sparse autoencoders
have been applied to a range of language
models with impressive results in 2024
the anthropic team showed that features
extracted from cloud 3 sonnet are even
multilingual and multimodal a feature
for the Golden Gate Bridge responds to
reference ref es to the bridge in
multiple languages and even to images of
the bridge the anthropic team has scaled
up their Auto encoders to extract around
13 million separate features and a team
at open AI has trained a 16 million
feature Auto encoder on the GPT 4
residual stream however as Chris Ola has
pointed out these millions of features
appear to just be scratching the surface
the anthropic team has found features
for specific neighborhoods in San
Francisco but the Claud model these
features were extracted from knows way
more granular information about the city
like the intersections of streets which
do not show up in the extracted
features large language models appear to
know far more Concepts than we've been
able to extract so far we may be able to
Simply continue scaling sparse
autoencoders as we've scaled language
models but there are a number of
theoretical and practical obstacles that
may block this path it's possible that
the computational cost of extracting
extremely rare features will become
prohibitively High leaving these rare
features as an unobserved dark matter
that has to be observed indirectly the
current sparse autoencoder Paradigm
effectively focuses on a single location
in the model at a time leaving it
incapable of disentangling cross layer
superposition there's work actively
underway from the anthropic team and
others on a new approach called sparse
cross caters to address this issue
finally as the number of features
increases the features become more and
more fine grained making them more
difficult to work with you can see this
directly when experimenting with large
autoencoders on neuron pedia searching
for doubt we find many many features and
it's not clear how various choices will
affect the model until we test them
sparse autoencoders and other
mechanistic interpretability approaches
have given us incredible insights into
large language models it will be
fascinating to see how far we can push
mechanistic interpretability and if the
capabilities of large language models
will continue to outpace our abilities
to understand them

welcome back to Google Deep Mind the
podcast with me your host Professor
Hannah fry in this episode we get to
speak to one of the most legendary
figures in the world of computer science
Jeff Dean he was there in the late 1990s
writing the code that would turn Google
from a small startup to the
multinational company it is today Jeff
spearheaded tense of FL one of the
programming tools responsible for the
democratization of machine learning he's
the one who pushed the boundaries of
artificial intelligence in the direction
of large scale models and if that wasn't
enough he also co-founded Google's AI
research project Google brain and was
one of the earliest pioneers of a new
neural network architecture called
Transformers not sure that one's going
to catch on I think this is why people
joke that Jeff Dean's resume just lists
the things that he hasn't done
it's shorter that way more recently Jeff
as Google's Chief scientist has occupied
one of the most important seats around
the table as the two great AI arms of
alphabet have merged Deep Mind and
Google brain his latest baby which he
co-parents is known as Gemini which
takes large language models well beyond
language alone Gemini is a multimodal
model which can understand text code
audio images and video it is AI through
and through and almost certainly the
direction that Google search itself is
heading in Jeff thank you so much for
joining me thank you for having me it's
a delight to be here so okay 25 years
quarter of a century at Google I I want
to know a little bit of what it was like
back in in the early days right like in
the 90s when you first joined when when
Google wasn't sort of slick organization
that it is now was it all a lot of you
know laptops with stickers on it and
sort of coding in flip-flops uh sadly
was pre- laptops pre- laptops well
mostly yeah we all had the giant CR
based monitors was pre- LCD monitors so
uh they took up a lot of desk space not
very my desk was like a a door on two
saw horses uh that's that was the and
you could adjust it yourself by like
getting out of the desk and standing up
with your back to like get it to the
next higher setting really yeah amazing
and when I started we were in the small
office area actually not you know maybe
three times as big as this room uh the
whole of on University Avenue in P above
what's now a T-Mobile like cell phone
store um and uh you know the really fun
and exciting thing in those days was uh
you know we were a small company but we
could see that people were using our
service more and more because you know
we were providing good quality high
quality search and you could see your
traffic growing you know day over day
week over week so we'd always be trying
to not melt on Tuesday at noon which was
kind of the peak traffic hour of the the
week and that would require you know we
deploy more computers quickly that we
optimize our code to make it run faster
that we come up with new and interesting
Innovations for next month's index that
make it you know able to serve more
users with the same Hardware I can
imagine being very exciting was there a
moment when you guys realized was there
a moment when you were like ah okay this
is really going to be big I mean I think
you could see that from the very
earliest when I was joining I joined the
company because our traffic was growing
really fast and we felt like you know by
focusing on returning really high
quality search results and doing that
quickly um and giving users what they
want we actually you know want to get
people off our site as quickly as
possible to the information they need uh
that that was a you know a winning
proposition and users seemed to like our
service and so it seemed reasonably
promis ing I would say from even the
early days there's quite a big gap
between reasonably promising though and
sort of what it ended up being has it
been a surprise to you like to to all of
you I mean I think there's been a bunch
of things that that we've branched out
into that were you know obviously hard
to anticipate autonomous vehicles like
it's hard to sort of fathom that when
you're working on a search engine um but
you know I think the sort of gradual
broadening out of our portfolio of
products to other kinds of information
makes makes a lot of sense so going from
public web pages to you know helping
users organize their own email with
Gmail things like that those are sort of
natural evolutions of things that solve
real problems people have and that get
you in the state of okay well now we
don't just have one product we have like
a a handful of products that people use
fairly regularly and I sort of Wonder
looking back through all that time do
you think that Google was always a
search company do you think it was an AI
company sort of pretending to be a SE
company yeah I mean I think a lot of the
uh problems we wanted to tackle as a
company really were sort of ones that
would require AI to really truly solve
and so along the way in you know a long
period of time 25 years you know we've
been progressively tackling some of
those hard AI problems and making
progress on them and then using the new
techniques that are now starting to work
in the midst of search in the midst of
all of our other products do you think
that Google will always be a search
company or do you think is it even a
search company now is it changing well
one of the things I really like about
Google is our mission is still
incredibly relevant even you know 25
years later you know organize the
world's information and make it
universally accessible and useful and I
feel like Gemini is really helping us
push in the direction of understanding
you know lots of different kinds of
information so text textual data you
know uh software code which is kind of
texty in nature but but very structured
in certain ways but also all the other
kinds of modalities of input that humans
are sort of fluent in you know where
naturally you know we read stuff but we
also see stuff with our eyes and hear
stuff with our ears um and you want
models to be able to sort of take in
information in in its many forms and
also produce information in you know
text form or maybe generate audio so
that you can have a conversation with
the the model um or produce imagery if
that's appropriate or you know annotate
text with graphs or things like that so
we're we're really trying to make a
single model that can take in all those
modalities and produce all those
modalities and use that capability when
it makes sense can you remember when you
first came across neural networks oh
yeah actually uh so neural networks have
had an interesting history like um you
know AI is a quite old discipline and
the early phases of AI were about how do
we Define rules about how things work so
that was like the 50s 60s 7s to some
extent um and then neural networks kind
of came along in the you know 70s and
had a wave of excitement in the late 80s
and early 90s um and I was actually an
undergrad uh in 1990 University of
Minnesota and I was taking a class in
parallel processing which is
this idea of how do you take problems
and break them down into pieces that can
be done on different computers and then
kind of in conjunction all those
computers work together to solve a
single problem I guess this is like the
point where the confusing power wasn't
quite what we have today it's like how
do you make computers work as a team
right yeah and uh at that time neural
networks was a sort of a particular
approach to machine learning and AI that
involved kind of
uh very kind of crude approximations of
how we think real human or other brains
work with neurons so that's why they're
called neural networks is they're made
up of artificial neurons uh and
artificial neurons have connections to
other neurons below them and then they
look at kind of the signals that come up
from those artificial neurons and they
decide you know how interested are they
in those those that particular pattern
of signals and then they decide should
they be excited enough to send a signal
further up the neural network and so
that's one artificial neuron and a
neural network is made up of lots of
layers of lots of these neurons and so
the higher layer neurons build on the
representations of the lower neurons and
so if you're for example building a
neural network for image problems the
lowest neurons in the lowest layer might
learn features like oh it's a splotch of
red or green green or there's an edge at
this orientation and then the next level
up might learn oh it's it's an edge with
yellow on one side um and then higher up
it might be oh it looks like a nose or
ears or a face uh and so by building
these layered learned
abstractions um these systems can
actually develop very powerful uh sort
of pattern recognition capabilities and
that's what people were excited about
about neural neural networks in kind of
198 1990 but we're talking teeny teeny
tiny little teeny networks yeah so they
could not recognize like faces and and
cars and things they could recognize
like uh little patterns in artificial
artificially generated patterns yeah
like you have a grid and you can
recognize maybe a cross or something or
a handwritten digit is it a seven or an
eight that's fancy yeah that was that
was fancy but that was kind of what they
could do at that time um but people were
excited because they could solve those
kind of problems that other systems
based on sort of purely logically
specified rules of what a seven means uh
weren't actually able to do very well in
a way that generalized to like all kinds
of messy handwritten sevens so I was
kind of intrigued by that after my two
lectures on neurs and I decided I would
uh you know do a senior thesis honors
thesis on Parallel training of neural
networks because I felt like ah we just
need more compute what if we used the 32
processor machine in the department and
made a you know a bigger system that we
could train bigger Nets um so that was
what I spent you know a couple of months
three months on did it work uh yeah yeah
so so anyway I was very excited I was
like oh 32 processors is gonna cause
neural Nets to really really hum sing um
turns out I was wrong naive undergraded
me uh we needed about a million times as
much processing power to get them to
really start to work well on real
problems that you might sort of care
about yeah but then thanks to kind of 20
years of progress of mors Law and you
know much faster CPUs and computational
devices and stuff we actually then
started to have practical systems that
had a million times as much compute as
even our fancy 32 processor machine and
so I started to kind of get interested
in neural Nets again um when uh Andrew
in who's a Stanford faculty member was
Consulting at Google one day a week and
I bumped into him in one of our many
micro kit
I'm like oh what are you doing at Google
he's like well I haven't really figured
it out yet uh because I just started
Consulting here but some of my students
at Stanford are getting good results on
neural networks I'm like oh really why
don't we train really really big neural
networks so that was the Genesis of our
work uh on neural networks at Google and
then we formed a a small team called the
Google brain team to start looking at
you know how could we train very large
neural networks using Google's
computational resources uh and so we we
we sort of built this software
infrastructure that enabled us to take a
neural network description and then
break it down into pieces that would be
done on different computers different
members of this parallel team um and
then communicate amongst themselves in
ways that they needed to do in order to
sort of tackle the overall problem of
how do you train a single neural network
on 2,000 computers um and so that was
kind of the earliest software we built
for really scaling up neural network
training uh and it did it enabled us to
train models that were 50 to 100 times
larger than sort of existing neural
networks 2011 right yeah this is like uh
early 2012 so this is like before the
big breakthroughs in image recognition
this is like way back and in many ways
you were doing then the same thing that
you were doing previously of just like
kind of stitching computers together
it's like my undergrad thesis exactly
but again I'm like hey we could do that
again but big scale and yet this time
this time it actually worked because the
computers were faster and we used a lot
more of them did it feel like a bit of a
gamble though back in 2011 oh yeah I
actually the the system that we built
for training these neural networks and
trying different ways of breaking them
apart uh I actually named it disbelief
because
it's uh partly because people didn't
think it was really going to work and
also because of a was a distributed
system that could build uh these one of
the things we wanted to train was belief
networks in addition to neural networks
oh I love it I love it so disbelief was
disbelief amazing so while this was
going on yep in State Side this side of
the Atlantic was the beginnings of of
Deep Mind yes and um I know that you
were you were the person tasked with
coming over and checking them out right
can you tell me about that story yeah so
um uh so actually Jeffrey Hinton who's a
very well-known machine learning
researcher spent a summer at Google in
2011 I think um and we couldn't figure
out how to classify him so we got
classified as an
intern a little funny my senior INT in
the entire history of the world and so
he and I were working together and then
somehow we found out about deep mind uh
I think Jeffrey had known a little bit
about sort of the formation of the
company and some other people also said
oh yeah there's this company over in UK
doing it was teeny tiny at this time I
mean like yeah like probably 40 or 50
people or something um
and so we decided as a company that we
would go check them out as a potential
acquisition um and so I was in
California Jeffrey was back in Toronto
where he's a faculty member at that time
um and Jeff has a bad back so he can't
actually fly commercially so because he
can't sit down he can only lay down or
stand up and Airlines don't like it when
you stand up during
takeoff um so we had to figure out a
solution which was to get a medical bed
in a private plane and so we a bunch of
us uh took off at California flew to
Toronto uh scooped Jeffrey up from the
tarmac put him in the medical bed and
then we all flew to the UK and landed in
some not one of the main airports it
was and in the edge of town then we all
got in a big van and we trooped off to
to visit deep mine which was near
Russell Square I think um
and we were really tired from flying the
the previous night but then we got a
like a
13 straight 20 minute lectures in a row
of all the different things they were
doing what from the team at D yeah from
the team oh wow so we saw some work some
while jet lagged while jet lagged it's
like something from a sitcom you know
yeah yeah exactly and then so then we
got some presentations on you know some
of the Atari work they were doing which
sort of was published later um on how do
you use reinforcement learning to learn
to play Old Atari 2600 games so games
like breakout or pong or uh you know
various other ones uh which was quite
interesting um and then because you guys
hadn't been doing the reinforcement
learning at that time right we'd mostly
been focusing on how do you scale up uh
large scale supervised and unsupervised
learning uh and and the reinforcement
learning is much more motivated by
rewards yeah so I think all of the these
techniques are really useful and they're
often useful in combination so
reinforcement learning you should think
of as you you have some agent operate
operating in an environment and there at
every step there's a bunch of different
moves you could make or actions you
could take um so in the game of Go For
example you know you could play a stone
in any of you know a whole bunch of
different positions in Atari you could
move your joystick up down or left or
right or you push the left or right
button um and often in these situations
you don't get an immediate reward um so
in go you play a stone and you don't
know if that's a good idea or not until
kind of the whole process of the rest of
the game plays out and one of the
interesting things about reinforcement
learning is it's able to take kind of
long sequences of actions and then
attribute Rewards or negative rewards to
the sequence of actions that you took in
proportion to you know how unexpected it
was based on you know when you made that
move did you think it was a good idea
and then you you won so maybe you should
increase your idea that that was a good
idea a little bit or maybe you lost and
you should decrease your sense that that
was a good idea a little bit um and so
that's kind of the main idea behind
reinforcement learning and it's a quite
effective technique especially in
environments where it's very unclear
immediately whether that was a good idea
yeah um abolutely in contrast supervised
learning is where you you have an input
and you have kind of a ground truth
output so the classic example is you
have a bunch of images and each image
has been labeled with one of a c a bunch
of categories so like there's an image
car there's another image ostrich
another image you know uh
pomegranate you have a set of
categories exactly tell me when you were
here at Deep mind and you decided that
you were to do the acquisition was Demis
nervous
I don't know if he was nervous I mean I
think that I I said well you've seen
I've seen all these nice presentations
but can I look at a little bit of the
code and so because I wanted to make
sure there was like real code behind it
and see like what the you know coding
standards were like that people actually
write comments that kind of thing so uh
Demus kind of like was a little unsure
about this I said I don't doesn't have
to be like super secret code just pick
some little bit of code and show it to
me and so I went into an office with one
of the engineers and we kind of sat down
for 10 minutes and like I said okay what
does this code do and like oh okay that
thing what does that do and where can
you show me the implementation of that
um and I came out satisfied it was neat
and TIY it it was reasonably neat and
tidy I mean for a small company trying
to move quickly it was kind of research
e code um but it was you know
clearly uh interesting and and well doed
I I've heard that when you do your code
you you put a little um thing which is
lgtm oh yeah looks good to me yeah I use
that in real life too now like not just
for COD reviews okay so in these
presentations then can you remember what
your impression was yeah I mean it
seemed like they were doing really
interesting work particularly in the
reinforcement learning side we were
focused on scaling so we were training
models that were much much bigger than
the ones uh Deep Mind was playing with
at the time uh but they were uh learning
to use reinforcement learning to to sort
of solve kind of gameplay which is a
nice clean environment for reinforcement
learning but but it seemed like the
combination of reinforcement learning
plus a lot of the scaling work that we
we had been working on would be a really
good one because it's like I guess
you're sort of approaching a problem
from two different directions like
really tiny with reinforcement learning
really small like toy models and
building up and then you're kind of at
this very very big scale with this sort
of Rich understanding
understanding understanding in inverted
commas yeah um but then but then it's
sort of putting the two together where
things become really powerful yeah yeah
indeed and that was kind of a lot of the
the motivation behind the the
combination we did uh last year of the
kind of legac Legacy Deep Mind and
Legacy brain and other parts of Google
research uh we decided we would just
combine the units together and form
Google Deep Mind um yeah and Gemini as
the and Gemini which actually predated
the idea of combining but really was
like hey we should really all work
together on these problems because we're
all kind of sniffing around the same
kind of general direction of trying to
train really highquality large scale
multimodal models and it doesn't make
sense to fragment our ideas and not work
together and fragment our compute
resources and so on we should really
just put all this together build a
combined team to go after this problem
and and that's what we did so so why
Gemini um uh I actually named it did you
yeah I mean you're very after this I do
like disbel uh so Gemini is relates to
Twins and I felt there was a good name
for the twins of Legacy Deep Mind and
Legacy brain kind of coming together to
really start working together on uh sort
of an ambitious multimodal project I
guess also Gemini I'm just thinking of
the uh the space mission yeah it's like
a precursor to Apollo yeah a good thing
about a name that has multiple meanings
is so that that was another reason to
pick the name is sort of the precursor
to you know ambitious uh you know Space
Program uh progress so I want to come on
to the multi multimodal stuff okay just
um just before I do I guess one of the
big reasons why this this big change has
happened in in the sort of public
consciousness of chatbots and large
language models um is in part because of
some work that came out of of Google
brain with with Transformers if you if
you'll sort of forgive the pun can can
you tell us a little bit about that
Transformer work and how transformative
it's been sure yeah so it turns out a
lot of the problems you want to deal
with in language and in a bunch of other
domains uh are problems of sequences so
if you think about you know autocomplete
in Gmail you're typing a sentence and
can the system help you you know by
finishing your sentence or your thought
for you uh a lot of that relies on
seeing part of a sequence and then
predicting the rest of it um and
essentially that's what these large
language models are trained to do
they're trained to take in data you know
one word or one piece of a word at a
time and then predict what is the next
thing that will follow um like fancy
autocomplete like fancy autocomplete
yeah it turns out to be useful you can
Model A lot of different problems this
way as well so like translation you can
model as taking in the English version
of a sentence and then training the
model to then output the French version
of the sentence uh when you have enough
English French sentence pairs to sort of
train as like sequence um you can also
use this in healthcare settings like if
you're trying to predict you know uh a
patient in front of you is reporting
these symptoms and they have these lab
test results and in the past they've had
these things you know you can model that
whole thing as a sequence and then you
can predict you know what are the likely
diagnoses that would make sense if you
have other deidentified data that you
can train on that has also kind of been
organized in these sequences and the way
you can do that is you just hide the
rest of the sequence and you force the
model to try to predict what happens
next um it's quite an interesting thing
that it's so applicable to you know
language translation Healthcare settings
uh DNA sequences all kinds of things but
that it's about the bit that you're
paying attention to at any any point in
time yeah so the models that were
successful prior to the Transformer
architecture were uh what are called
recurrent models where they have some
internal State and every time they see a
word they do some processing to update
their internal State and then they go on
to the next word and then they do that
again so now they have they move their
state forward a bit uh and update the
state with respect to the next word that
they just saw and so that you can kind
of imagine this as like a 12 word
sentence you're doing that updating of
the state 12 times but every step is
dependent on the previous one and so
that means it's actually quite hard to
get it to run fast because you have this
what's called a sequential dependency
where step seven depends on step six
step six depends on step five and so on
um so one of the things a collection of
researchers within uh Google research
did is they came up with a pretty
interesting idea which is instead of
just having a single state that we
update at every word let's process all
the words all at once and let's remember
the state uh that we get when we're
processing every word um and then when
we're trying to predict a new word let's
pay attention to all of the previous
States and figure out how to learn to
pay attention to the important parts so
that's the Learned attention mechanism
in
Transformer in order to predict the next
word and for some words you know you
might need to pay attention to the the
previous word a lot for some contexts
it's very important to pay attention
kind of a little bit to a lot of the
words in the context uh but the
important thing about that is it really
can be done in parallel you can take in
a thousand words up compute the state
for each one of them in parallel and
then uh that makes it sort of 10 to 100
times more efficient in terms of uh
scaling and you know performance than
the previous recurrent models and so
that was why that was such a big advance
but then I guess there's there's there's
other things that seem to emerge from
this I mean sort of a conceptual
understanding or maybe sort of
abstraction that's that's possible just
through sequencing and language alone I
mean that was that a surprise yeah I
mean I think some of the earliest work
we did on language modeling in the the
Google brain team was really about
modeling words not as their surface form
of like
H or C but really about a high
dimensional Vector that represents kind
of the way in which that word is used
you know we're used to thinking in two
and three dimensions as humans but when
you have a 100 Dimensions or thousand
Dimensions there's a lot of room in a
thousand dimensional space but when you
have things that are nearby and you've
trained the model in such a way that you
know cow and sheep and goat and pig are
all near each other and they're very far
apart from espresso
machines although milk could be a too
milk milk would near the Cow but kind of
in between the two yeah it would
probably be kind of on that 100
dimensional line uh in hund dimensional
space um so this is kind of why these
models have surprisingly powerful
capabilities I think is because they're
representing things with so many high
Dimensions that they can actually really
latch on to many different facets of a
word or a sentence or a paragraph
simultaneously because there's so much
room in they PA in their representation
it sort of extracted the the grounding
that we ourselves have given language I
guess yeah I mean we when we when we
hear a word we don't just think of the
zerus form word we think cow oh that
triggers a bunch of other things like
milk or espresso machine or you know uh
milking and calf and and and bull and
one of the things we found with those
early word representations was that
directions had meaning so if you think
about you know present tenses of of of
verbs like uh walk uh you would go in
the same direction in this 100
dimensional space to get from walk to
walked as you would go get from run to
ran as you would go from you know uh
read to
Red wow so it actually understands
understands I keep using that word I
don't mean it but but there is like some
representation of tenses within the
structure of these and it's it's just
emerged from the training process it
didn't it's not something we told it to
do it's just the training algorithm we
used and the fact that language has lots
of ways in which you know particular you
know forms are used uh cause that to
emerge and you could also for example
change from male or female versions to
words and vice versa so cow to Bull is
the same as direction as Queen to King
or man to woman woman to man uh and so
on amazing but this is still this is
just with language that we're talking
about here so okay tell me tell me how
does the
multimodal aspect of this change what
what does it how does it make it
different yeah I because
it it's you're still representing the
input data in these high dimensional
spaces and it's really a matter of how
do you get from the pixels of an image
say into some something where ideally
you'd like uh the multimodal model to
have the same kind of thing that we have
when we see a
cow that triggers kind of similar
activations in our brain to reading the
word cow to you know uh hearing a cow
move right um and you kind of want to
train models so that they have that
joint meaning and representation
regardless of the way they arrived at
that input data um so if they a video of
a cow walking through a field you know
that should trigger a whole bunch of
things that are related to that in the
model based on the activations that the
model has built over you know typically
these are very deep layered models and
so the lowest layers typically have very
simple representations and then the
deeper the higher layers in the model
build on those representations and build
sort of more interesting and complex uh
you know combinations of features and
and representations of
of be it words or images or so when
you're saying multimodal from the ground
up right which is kind of a big phrase
that you hear about Gemini it's it's not
that you've got like the word section
over here pixel section over here and
you're translating between one and the
other right but like in the model itself
those representations yeah very early in
the models does that make it harder at
the beginning when you're setting it up
does it make it more difficult to
do
um yeah I mean I think figuring out how
to integrate different modalities into
the model and you know how should you
train a multimodal model is more complex
than a a simpler pure language or pure
character-based model um but you get a
lot of benefits from it uh in that you
get sometimes cross modal transfer we
now seeing visual stuff about cows um
actually helps inform the language you
know maybe it's' seen a bunch of
descriptions of of cows in Meadows or in
something but now it some suddenly has
seen images of that and videos of that
and it's actually able to bring those
representations together in a way that
makes uh kind of similar things trigger
inside the the model regardless of
whether you saw the word cow or the the
kind of the the image of cow give me an
example of the type of situation you you
see this being useful in in the
future well I think it it's already
useful which is good um
I mean as one example you want to be
able to take in an image of kind of a
you know handwritten whiteboard worked
out math problem and say did the student
get this problem right right and so now
you need to really bring in the
multimodal capabilities in one example
you need to actually you know do
handwriting
recognition understand from that okay
it's a physics problem that someone's
written on the board and it's got like
maybe a picture of a skier going down a
slope in one of the early Gemini Tech
reports we had this good example of a
you know a student who' worked out a
problem on a on a whiteboard and uh you
you could actually ask Gemini you know
did the student get this problem right
if not where did they go wrong and can
you explain how to solve the problem
correctly and I was actually able to
tell that you know the student had
incorrectly applied you know the the
formula for a skier going down
frictionless slope and instead they Ed
the hypotenuse instead of the height and
it said oh no actually you should have
used this and here's the problem worked
out and it did all that and recognized
all the handwriting and the fact that
this was a physics problem this kind of
physics knowledge that the model already
had sort of was the right thing to apply
I mean that's a really neat way that you
could use the existing model of Gemini
in the existing model of Education I
guess totally yeah but but I suppose
actually these are not kind of isolated
systems from one another so in some ways
do you think that these multimodal
models will change the way that we have
to do education full
stop I mean I think the potential for
using AI tools to help uh education uh
is really amazing and we're sort of just
at the beginning of this journey as a
society I think you know we know for
example that educational outcomes of
students who get one-on-one tutoring
from another person are two standard
deviations better than students who are
who have a traditional classroom setting
of a teacher and you know 30 or so
students um so how could we get everyone
to the point where they feel like they
have the benefit of an educational tutor
that's one-on-one understands what they
know understands what they don't know
can help them learn in the way that they
learn best um that is the potential of
AI in education and I think
really we're not that far away from
something where you could
Point uh a Gemini model or a future
Gemini model at something some piece of
material and say can you help me learn
this right take the chapter six in your
biology textbook or something and you
know it's got a bunch of images it's got
you know a bunch of text uh maybe it's
got a lecture video that you watched as
well um and then you can actually say I
really don't understand this thing can
you help me understand it it can ask you
questions it can you know you can ask it
questions it you can answer the
questions it can assess are you right or
wrong and really guide you in your
Learning Journey and and you know
because it's
individualized and we should be able to
get that to you know many many people
around the world uh in not just English
in you know languages spoken by you
know you know hundreds and hundreds of
languages all around the world I mean I
so I I take what you said about the the
lots of different languages and and
trying to make these as broadly
available as possible but is there a
danger of creating a bit of a two-tier
system here where where on the one hand
people who have access to these tools um
as you described get far better outcomes
you know accelerate their own learning
and and own productivity and then
anybody who is not fortunate enough to
have access to the tools uh really
struggles I mean is that is that
something that concerns you yeah I mean
I think there is definitely a risk of
creating two-tier systems I I think what
We should strive to do is make these
Technologies as broadly accessible
universally accessible if we can uh for
everyone and really try to lean into the
strengths of what that will do for
society and to make it you know
affordable or free uh for people to take
advantage of the capabilities for
education for you know health care I
think is another area that is you know
hugely you know huge potential for for
AI to really make a big difference in
and Healthcare accessibility go back to
Gemini if if we can sure okay so I guess
if you started off with Google search
factuality must have been absolutely at
the Cornerstone of everything that you
cared about like but Gemini I mean you
work with it all the time I imagine
you've seen it say some quite Outlanders
things yeah how are you sort of squaring
that Circle in your head of like
releasing perhaps some of the need for
absolute factuality at all times yeah
actually a tricky balance as a company
because we are sort of from our Origins
a search-based company and as you say
providing accurate factual information
is kind of the Pinnacle of a search
engine experience um and I think we
actually had built interesting large
language models internally that you know
people enjoyed conversing with uh you
know actually some of them were uh
available internally during the pandemic
so people all at home and you could
actually see internal usage Spike during
lunchtime because people were like
having conversations with their their
virtual chatbot because you know who
else are you going to talk to when
you're home alone or
whatever um but the you know these
models are trained to predict plausible
next tokens essentially so a token you
can think of it as a word or a piece of
a word and so when you predict plausible
next tokens that's a different thing
than that is absolute truth right it's a
probabilistically plausible sentence and
that's different than a fact and I think
one of the things we we realized over
time was these models can actually be
quite useful even if they're not 100%
factual and so I think realizing there's
all these other use cases or can you
summarize this slide deck in five
bullets and yes you could argue about is
that fifth bullet exactly right but it's
still pretty useful to get four .5
bullets that are factually accurate
about the slide deck and you know we we
striving to make it five factually
accurate bullets but even without that I
think the utility of these models is
actually quite high was it an
uncomfortable realization because of
course other labs did push out their
models early yeah yeah do you think that
that that you guys had an abundance of
caution because of this factual issue I
mean I think we had a number of
different concerns factuality being one
of them you know toxicity and bias in
the way
uh you know the model is trained and the
outputs that it can produce uh is an
area where we want to make uh you know
the model less biased in a lot of ways
um and so there were a whole uh number
of areas where we wanted to sort of be
be right relatively cautious before
releasing things to the general
public um and I think we've we've gotten
a lot of those issues kind of sorted out
enough that we think the the products
we' put out in the space are are useful
even though they're obviously room for
improvement in things like factuality uh
and in bias uh and other areas um so I
think that's taken a little bit of an
adjustment for people is you know strive
for the best you can be but also realize
that by not releasing something you're
sort of holding back something that
could be useful for a lot of people even
with its sort of foibles but then with
those foibles then so so in which
direction do we go from here do you
think that I mean it sort of seems to me
that there's been this this real shift
uh in in the way that Computing happens
as it were you know you get a calculator
you put in the same sum twice you get
the same answer twice whereas whereas
we're now in an era of probabilistic
computing and so I wonder whether that
is it that the public has to come to
terms with that and sort of accept that
we're in an era where things are much
more humanlike and that they can make
mistakes or is it something that you
think is fixable
I think it's some of both right I mean I
think there's a bunch of tech uh sort of
technical approaches to some of these
problems that will make the factuality
area issues better one one instance is
if you think about the data the model is
trained on like trillions of tokens of
text and other data that are then mixed
together in this giant soup of you know
billions and billions of parameters um I
I like to think of that as like you've
seen a lot of stuff but you don't recall
it very
well uh whereas if you take information
that is in the one of the things we've
been pushing on in Gemini is having a
long context window so when you have a
long bit of space where you can put a
lot of direct information that you're
trying to summarize or manipulate or
compare uh in various ways or extract
information from that information in the
context window the model actually has a
much clearer view of right it's got like
the actual text and the representations
of that text not Tangled together with
everything else it's seen so this
context window is sort of the bit that
the the the model can see as important
at that moment yeah it can sort of
reason about that in more Fidelity than
other things that it's seen in its
training process so it can take like
five PDFs of scientific articles and
then you can ask questions about it like
can you can you please tell me common
themes across these articles and then
it's actually able to to do that because
it has you know its own representation
of all the contents of th those articles
and that's one of the reasons we've been
pushing a lot on very long context
windows for Gemini models is we think
that's a really useful capability for
factuality for you know video
summarization for all kinds of things um
is there a limit though to the context
window can you just push and push and
push and push until it's sort of an
infinite conf that is an excellent
question um currently the computational
aspects of the attention process are
quite expensive so the longer you try to
make it the more expensive it gets um
expensive in terms of time but also Evy
time and money and compute and and all
kinds of things um but we think it may
be possible to sort of come up with
algorithmic improvements that that
enable you to go beyond the kind of two
million token uh context window which is
what we have now I mean a million tokens
is quite a lot uh a million tokens is
about 600 pages of text um so you know
that's like up most books you know 20
articles uh it's an hour of video how
about on the other side because you said
it was a little bit of both that that
perhaps people have to adjust their
expectations so I
think these models are are tools and
people need to understand the
capabilities of their tools but also
some of the ways and which you know you
probably don't want to use the
tool um
so I mean I think it's a bit of a
educational process for people um you
know don't just trust every fact that
comes out of a language model uh right
off the bat you know you need to apply a
a bit of of scrutiny to that um sort of
like you know I think we've taught
people these days that if you see
something online that doesn't
necessarily make it true I think a
similar degree of skepticism for some
kinds of things from language models is
probably also appropriate uh that
skepticism May decrease over time as the
models improve uh but it's good to take
it with a healthy dose dose of you know
oh that might not actually be true
um aside from the context window is yeah
are there ways that you can yourself
when you're sort of writing in prompt M
sort of minimize the risk of ending up
with something that's that's a complete
hallucination so one technique that uh
Google researchers kind of came up with
is what's called Chain of Thought
prompting so in the same way um if you
just give the model a sort of an
interesting math problem and you say
okay what's the answer
um it's you know it may get it correct
but it may not and if instead you say
here's an interesting math problem can
you please show your work step by step
so if you remember back to your fourth
grade math teacher he or she was
probably saying you should really show
your work step by step and then get to
the final answer and then write the
final answer down and that's partly
because that helps you get through that
you know multi-step thinking process of
how do you actually go from you know
what's being asked to okay I need to
calculate this calculate this based on
that and so on and finally get to the
answer
and it turns out that not only makes the
models output more interpretable because
it kind of tells you what steps it's
going through um but it also makes it
more likely to get the correct answer
what if it's not a math problem though
yeah I mean even in non sort of crisply
defined right answer things um domains
this approach kind of works and it
there's a bit of subtlety and and I
think people need to actually learn how
to use these models and the way in which
you prompt them is actually a you know a
a big differentiator in how high quality
the output is like if you say summarize
this um that might lead to one outcome
if you say please summarize this and
give me five bullet points that uh you
know highlight the major important
pieces of the article and identify two
cons that the author wrote down you know
if you say that that's a much clearer
set of instructions to what the model
should do than just summarize this so
when we put these things together then
so sort of breaking down step-by-step
processes but also understanding more
context uh and the multimodal stuff too
are we moving towards a situation where
these kind of multimodal models will
understand us as individuals and our
preferences yeah I mean I think what you
really want I think is a sort of very
personal version of Gemini for you that
understands you know what it is you're
trying to do right now but also
understands the context in which you're
trying to do that uh I'm vegetarian so
if I'm asking Gemini about restaurant
recommendations in London and it knew
that I was a vegetarian it would
recommend different things than if I was
not um and I think a general model that
is serving the needs of every person the
same is not going to be as good as one
that actually understands a lot about
about you and your context uh you know
there are some kinds of queries you
might like to ask a model um that you
can't quite do today with Gemini but you
could imagine wanting to do you know can
you take the the pictures I took on the
hike last week and make a Illustrated uh
story book for you know my kids bedtime
tonight you know and it would know where
those pictures from on your hike were
and you know how to make an illustrated
story book that would appeal to your to
your child I would maybe know how old
your child was make it age appropriate
you know so I think you can't do that
now but that could be something useful
people would want you'd want people to
opt into that I think the more
information you want the model to know
uh and have in context um I think the
more you want to sort of have people
understand what what is happening um one
of the things I think we'll be able to
do is not train a version of the model
on that data but just have the right
information available in context in
order to sort of call upon it when
generating responses and I think that
would be pretty nice so as then you've
got like this sort of this General
structure that you can almost imprint
your own context onto but then that's
kind of private for you that's right
nice yeah that seems like it'd be pretty
good yeah that would be pretty good are
we limited here to just audio and visual
and you know things that you can see on
a screen language whatever or or do we
ever EXP expect that these kind of
assistance will come out of our
computers as well yeah I mean I think
there's actually a lot of different
kinds of new modalities of data that
aren't sort of strictly human modalities
that we want these models to understand
so you know lots of temperature readings
across the Earth to help with weather
prediction or genetic sequences or you
know lar data for autonomous vehicles or
robotics applications and then
in a you know one one setting you want
these models to perhaps be able to help
with real world robotics applications
you know be able to talk to a robotic
device give it sort of instructions in
plain plain language you know can you
please go into the kitchen and you know
wipe the counter down and discard you
know recycle the soda can I left on the
counter and then bring me a bring me a
bag of pistachios or something right
like
robots have traditionally not been able
to understand language like that but I
think we're on the cusp of enabling that
kind of capability and then being able
to have robots do you know 50 or 100
useful tasks even in messy environments
like this room rather than kind of the
the traditional setting in which robots
have already been deployed in the world
which is sort of very controlled
environments like you know Factory
assembly line kind of things where they
go from there to there and it's a very
predictable thing we've been talking
here as as a assistance you know these
things is being sort of augmenting human
abilities in that way and and I can see
it in in medical settings in education
settings but is there is there more that
the multimodal aspect of this offers us
in terms of I don't know like how we
understand the world yeah I mean I think
what these models can do now is often
kind of do a few steps of reasoning to
get from what you asked it to do in
order to sort of accomplish something
and I think as these models improve in
capability you'll be ble to sort of get
models to work with you to do much more
complex tasks uh and you know it's sort
of a difference
between can you order a bunch of chairs
at the chair rental place versus plan me
a
conference
right the this the latter is much higher
level much more complex you know a the
right model would kind of ask you a
bunch of follow-up questions because
there's amb in there you know how many
people are coming you know what's it
about um just like a human country you
in right what country are you in where
do you want to have it uh
when um and then would kind of set off
and actually be able to accomplish a lot
of the kind of things that you might
might want done in order to do that high
level goal but then if you have this
this sort of
conceptual links or these conceptual
links I'm going back to Cow here right
and and it understands pictures and and
it understands I don't know I guess
gravity having seen uh videos of on the
internet they probably watched like
introductory lectures on physics right
oh wow okay so understands it from that
perspective yeah yeah and also seeing a
bunch of things
falling okay so then could you go in one
day and say draw me the blueprint for a
really efficient airplane yeah I mean I
think one of the things these
models need to be partnered with
is some exploratory process and that
exploratory process can come in the form
of you know maybe it doesn't need to
give you an answer in 200 milliseconds
maybe you'd be happy with your airplane
tomorrow right and so I think at that
point then you have a lot more freedom
in how would you Design Systems to be
able to efficiently do things like that
where they can go off and try a few
experiments uh in maybe in a simulator
that they have access to or maybe they
create a simulator for you know basic
fluid dynamics or something and they try
to you know try a bunch of designs maybe
they have some ideas about What airplane
shapes you know make sense having seen a
bunch of existing
airplanes um and so then they can kind
of try to accomplish what it is you
asked hopefully they first asked you
well what characteristics do you want
your airplane to have um it was a paper
airplane long paper airplane yeah it's
important to know if it's paper like
that that reduces the cost a
lot um so I think those kinds of things
will come eventually it's a little hard
to tell exactly when those capabilities
you know that's a pretty complicated
sort of integration of what you want the
reasoning in the model to do the
knowledge it needs what you're asking it
to do and how you're asking it but you
know we're already seeing pretty big
advances in capabilities of these models
over you know 5 year 10 year periods and
so over a 5 year 10 year period that
might be possible um it might even be
sooner than that for you know can you
help me design an airplane with these
characteristics but I guess these are
like the early early precursors to what
we might hope Apollo would be yeah
exactly that's why it's Gemini that's
why it's Gemini amazing Jeff thank you
so much for joining me it's a pleasure
to be here thank you for having me in a
lot of ways I think Jeff's whole story
is is one about scale for Google search
it was about how do you get more of the
web more users faster queries for neural
networks it was about more computing
power more machines and in the recent
era of machine learning it's been about
more and more and more data but
something emerges from all of that a
genuine conceptual model of the world
one that is capable of abstraction and
has already a proven ability to enhance
human productivity and it's telling that
Jeff isn't finished there there is more
to come more sensors more modes and when
combined with the reinforcement learning
tools that were born in this building
maybe also more progress on the path to
AGI if you've enjoyed this episode
please make sure that you subscribe to
our podcast and if you have any feedback
or you want to suggest a guest that
you'd like to hear from then why not
leave us a comment on YouTube until next
time

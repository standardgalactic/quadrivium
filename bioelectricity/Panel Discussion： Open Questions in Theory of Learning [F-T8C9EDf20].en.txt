I'm
Tomaso and
um to today we have a great
panel uh
so I'm looking forward to a lot of
interesting ideas and
discussions um no I think we need a
science of
intelligence and I don't want to waste
time why we need it we need
science um because we have to understand
natural intelligence in addition to
artificial one and we want to understand
what's going on inside transformers and
other
architectures and we need a theory we
need a
theory um maybe we disagree or agree
so I need a theory I think a bit like
physics
in which there are some fundamental
principles um you know they may not be
all like Max or
equations they may me some of them more
like um a fundamental principle in
molecular biology which is the DNA elal
structure and how it's immediately
suggest how to copy and
replicate um
but um I think
it's important to have a theory for many
reasons one of them is also to be able
to deal with problems like explaining
what's going on and uh you know aligning
with what we want and um safety
considerations and so on but the main
um the main reason why you want a
theory it's
really something that electricity can
tell us I think electricity is a bit
similar in its history to deep learning
to machine learning you know until 1800
there was no continuous source of
electricity the pillar Alexandra
volter um was the first one and once
that was found
then a lot of applications immediately
followed know
generators electrical motors the old
electrochemistry was done within 20
years the telegraph was designed by
Alexandro VTA never built between pan
Milan 30 kilom away this was the time in
which information was traveling the
world at the speed of a horse and
suddenly was the speed of
light um was but all of this happened
Telegraph and so on without people
knowing what electricity
was it was
only 60 years later that
Maxwell had theory of electromagnetism
and a lot of other things happen other
than that so I think from the theory
point of view right now in deep learning
we are between Vault and Maxwell I'm not
sure exactly where that could be a topic
for discussion
um the theory
um
um you know deep learning there are many
question one can ask just to tell you
what a theory should be able to answer
um why optimization is so easy um
apparently so easy um why you have
generalization despite over
parameterization it's in Gray because I
think we know the answer to this so
already um why is rlu Activation so
critical is not so that's why it's in
Gray um some of the others are um still
open like what is really happening in
Transformer what the magic of it maybe
maybe Iran one of our panelist will
explain a big part of it uh anyway we're
looking for fundamental principle that
can answer these kind of questions
and so let me now do the foll I
introduce our panelists you can not not
here that's
mine thank you and uh I introduce our
panelists and then each one of them will
speak for no more than 10 minutes and
few
slides and then I'll be the last one and
then we'll start the discussion and the
discussion will be between us and then
open it to all of you
so prepare your questions so philli um
is a professor in e a member of
Cale and he um has been working computer
vision machine learning is known for his
pioneering work on generative modest
image
synthesis and uh you know all other good
things that uh that chpt wrote for you
about you Ila Ila f is a professor in
BCS building and she's also co-directs
the icon Center um and the research
focuses on theoretical Neuroscience
particularly the mechanism of memory and
navigation in the brain um heon poisi is
professor at Harvard U senior
investigator I University in Jerusalem
still
true and
and um directs the Safra
Center former director of the sare
center okay has been a Pioneer in
theoretical
Neuroscience um exploring the principle
of neural computation and Dynamics in
complex
systems and Aon Malak is a CER fellow at
harbard
University um he graduates from HEB
University
in Jerusalem and his work center on the
theoretical foundations of deep learning
and neural network optimization that's
all
chpt and I'm um a co-director of the
center for brains minds and machine here
the professor in this building and I
mostly known at this time because I have
a famous post dos and students like uh
so that's my claim to fame and I'm very
proud of it so okay let's start with
Phillip so um the lurb that I'll give is
on this hypothesis that we're calling
the platonic representation hypothesis
this is um work that some of my students
and myself have put out recently and uh
yeah Tommy asked us to have you what is
what's one of the you know fundamental
principles of intelligence this is
really just a hypothesis I don't know if
it's a fundamental principle but I think
that it's there's something here that
could lead to fundamental principles so
I wouldn't treat this as a call to
action to study this okay so I'll tell
you what that title means um but I want
to start with this paper which is one of
my favorite papers from the last decade
or so this is from Antonio tala and uh
some of his collaborators uh and
students and it's object detectors
emerge in deep seen cnns and what they
observed which is a story that you'll
all have seen now like over and over
again over the last decade is that if
you train a neural network to do
something like detect whether this is an
indoor scene or an outdoor scene or if
it's a cat or a dog whatever it is um
and you probe neurons at some layer of
that Network you'll find that there are
object detectors
so a neural net that's trained to do
classification ends up kind of
self-organizing to find uh units that
respond selectively for these images so
Neuron a will fire these are the top
four images that most activate Neuron a
and here's the top four images that most
activate neuron B okay so there is a dog
face detector and there is a robin
detector and in BCS over here you'll
have seen the same thing in Monkey
visual cortex there are units in it and
other places that respond selectively
for objects
okay so this is this is something that's
interesting there's some internal
structure these are not just black boxes
we can understand the representations
they learn to some degree uh so other
people have studied similar things and
here's one that we did a few years ago
where we trained a network to do image
colorization so in this problem we're
going to try to predict the missing
colors in a black and white photo we're
going to do the same test we're going to
probe Neuron a and neuron B at some
layer of the network and ask what do
they respond to okay so the thing that
was surprising in 2016 now is kind of
known is you get the same things you get
a dogface detector you get a flower
detector maybe you get a robin detector
it's further down the list so this is
very strange right a neural network
that's trained to classify scenes of
course it will parse the world into
objects those are the components of
scenes but a neural net that's trained
to just colorize black and white photos
some like lowlevel photometric property
of images it will also discover the same
types of structures and that's just a
story that's repeated over and over
again that whenever we train deep Nets
to do whatever we care about they seem
to be learning similar um detectors and
patterns and parsing the world in
similar ways okay so a lot of people
have put forth very uh versions of this
hypothesis that different neural
networks trained in different ways on
different data sets are somehow
converging to the same way of
representing the world and in particular
I'm going to focus on the same kernel
function which I'm going to describe in
a second okay so that's the rough
hypothesis
uh just one more example of course you
know we this hypothesis has been proven
to be true at you know V1 at layer the
first few layers of visual cortex where
gor like detectors emerge in all of
these networks but further deeper in the
net we don't know for
sure okay so um we looked at this we've
been looking at this in terms of
representational kernels so let me
quickly Define what that is um we're
going to characterize representations
with Vector embeddings so mapping from
uh data to vectors and we're going to
then characterize the representation by
its kernel which is how does the uh
embedding space measure distance between
two different items so the kernel for a
set of images under some image embedding
would look like that on the on the left
there we have uh the representation the
vector that represents apple and the
vector that represents orange might have
be similar vectors so the kernel says
that those things are alike whereas the
uh
the embedding for elephant will be
distant or dissimilar from the embedding
for orange so this this structure is
just one of those fundamental structures
that is important to characterize
representation it tells us how does the
representation measure distance between
different items uh in some of our recent
work we ran a bunch of experiments
looking at these kernels and uh how
they're alike or different between
different neural networks and different
training regimes and I'll show just one
experiment where we looked at the
similarity between a kernel for a
language model and a kernel for a vision
model we're asking does the language
model represent two
sentences um does the language model
represent the distance between two words
apple and orange in the same way as a
vision model represents the distance
between an apple and orange as um being
small okay I are the language kernel and
the vision kernel for matching items um
alike and here's the uh kind of the main
finding if we plot language modeling
performance of language models on the
x-axis against uh alignment kernel
alignment to Vision models on the y- AIS
what we're seeing is that over time as
language models get better and better at
just doing next character prediction
they're getting more more aligned in in
their kernel representation um in a way
that matches the kernel of a
state-of-the-art vision system Dyno so
this is a Pure llm Performance on the
x-axis uh an alignment to a pure Vision
model that's not been trained with any
language at all and yet over time
they're getting more aligned and if you
look at alignment to bigger and better
Vision models you get increasing
alignment too so the best Vision model
is the most aligned with the best
language model and worst language models
are less aligned with worst Vision
models so it does look like there's some
kind of convergence going on and you
know the hypothesis is that that will
keep on going but you know who knows
maybe that will be false maybe this will
fall off after a while so uh why why is
this going on so I think the most common
response at this point when I've talked
about this work is oh it's all about the
data all of the models are trained on
the internet so we're just learning a
model of the
internet but the thing is this was
comparing the kernel for a language
model and a vision model the data is not
really the same it's a different format
different modality so it could be the
architectures we all use Transformers
maybe there's other fundamental
principles here the people just we're
telling people to use the same methods
but um our rough argument and what we
could talk more about in the dialogue is
uh my rough argument is that it's about
the world it's something about
nature so that what led us to this kind
of articulation of the platonic
hypothesis that um you know going back
to Plato's allegory of the cave uh Plato
imagined these prisoners whose only
experience of the outside world is the
Shadows cast on the cave wall and so
they have to infer what is out there in
the in the actual reality from just
these projections and it's an allegory
because he's saying that that's really
how our senses work we don't have direct
access to the physical state but we have
some measurements some observations of
the
state so there is some real world out
there platonic reality latent variable Z
and we observe it through cameras or
through through text or through other
modalities because if we do
representation learning on any modality
we'll get a representation and because
they come from the same causal process
at the end of the day uh those two
representations should somehow become
alike that's that's the rep argument
modeling the world with different
modalities should arrive at a similar
representation because the underlying
causal variables are the
same so um I won't have time to go in
full detail into uh the math but we do
have a toy Theory or like an initial
Theory we've started to work out which
basically constructs a you know toy
world of discret events and so forth You
observe those events with language or
Vision or other modalities uh and we
could talk about this more offline but
the kind of the current candidate for
what is this kernel what is where is
this convergence all heading to is that
we're learning a um a kernel that
measures distance between events in a
way that is proportional to the currence
rate of those two events so uh the
formalisms here are for the contrast of
learning setting again I'm not going to
have detail to go into all all of that
uh
but if you have a world in which
different events cooccur with different
rates and You observe them under
different modalities you can prove in
that simple toy world that they will
converge to the same kernel
representation these are kind of the
three points I want to leave you with uh
which I think could be the starter of
some fundamental principles one is that
I'm just more and more convinced that
kernels are an object of fundamental
importance for understanding
representations they seem to be
converging in theory at least in Simple
Theory but also in practice empirically
with large language models and large
Vision models and one candidate for this
kind of convergent kernel that it might
all be heading up toward is um a kernel
in which distance is proportional to the
co-occurrence rate of events in
SpaceTime and I will we leave you with
that and we can keep on discussing thank
you so I'm going to uh tell you a story
which is uh has a lot of up with what we
heard now but kind of different angle
perhaps uh neural manifold is a
framework for understanding
representations of categories uh in Ai
and and and brains um so uh
the question we all know that at some
point in the in in the cogn cognitive
systems there there are uh emerges
emerging representations of of
categories in Vision it would be object
uh and so on so
um it's a fundamental question how these
categories emerge from The Continuous
stream of of signals uh that that
impinge on the brain um one might uh
imagine that uh there is somewhere at
the top layer it cortex neuron uh that
is specific to a particular object uh
like grandmother cell hypothesis but the
answer is there is no such a thing that
there is a many neurons are responding
to all objects uh basically um a more
sophisticated assumption would be what's
called newal collapse is that there is a
a unique distribution or pattern of
activity in it cortex uh that is unique
for particular uh object uh but answer
is that this is not even the case
because the neuronal responses uh depend
on on the physical variability of of
these categories uh so the the natural
hypothesis is that we are not talking
about unique patterns of activity which
is invariant but we're talking about
object manifolds uh as as the
representation of these categories uh so
object manifolds so this is description
of them you have many images
corresponding to a category dog uh all
the collection of this responses uh
response vectors Define a manifold
similarly for cat and so on so forth and
the idea is that part of what the job of
deep networks in in Ai and and and in in
the brain uh is to uh re reformat or
reshape those manifold so that they uh
they can uh allow uh uh Downstream uh
computations which have to do with
object identity uh so two question is
what are embl of uh category based
computation Downstream uh relative to
which we'll measure how these
representations uh are good at and and
and what will be the relevant geometric
measures of those manifolds uh in in the
context of those of those computations
uh so basically um I'm flashing here I'm
not going to go into the math but uh
statistical mechanics give uh give us a
qualitative and precisely also
quantitative uh measures or predictions
which are sufficient and necessary
conditions for uh for the uh the
manifolds at the fure layer that will
allow for different computation so you
have here examples uh the computation
that I mention briefly uh soon is one of
them is High Capacity uh for linear
classification a large number of
categories so you have large number of
categories you want to classify some of
them as plus some of them in minus uh so
the manold need to have uh appropriate
radius appropriate effective Dimension
the radius is normalized to the mean
distance between the centroid which is
the kind of the signal uh so that's
that's one type of result another set of
example of computations is fast learning
fishot learning to discriminate between
new categories uh this again has kind of
a predicted SNR which has radi has
Dimension has overlap between
variability of the manifold U and and
the and the signal Vector uh and finally
zero shot learning involving a cross
modal uh estimation of prototype so you
transfer Knowledge from language to
vision and so on so forth so uh so let
me give you an example so linear
classification of a large number of
categories uh this is a cartoon you have
many categories that are uh that that
you want to uh describe to categorize as
plus one minus one um and this is the a
lot of work done with my colleagues and
primarily Su Chang has has done a
beautiful work on uh on this on this
type of of of computations and here is
an example of what the outcome is so
here you have a deep network uh in this
case reset I don't know 50 or whatever
uh so the the capacity or the
separability you see as a function of
the depth in in the
network uh and uh is kind of
incrementally increases but it it kind
of nonlinearly rising at a certain stage
uh in the network so you can say this
this is the point where object manifold
emerges and in this if you look at the
radius you look at the dimension
similarly at this point the the geometry
of the manifolds is getting into the
shape that allows for high capacity of
of discrimination so for the second uh
type of of computation fot learning with
new categories uh in this case um you uh
you you again all all what I'm talking
about is pre-trained network and all
what I'm doing is is looking at
Downstream uh computation on pre-trained
network uh so in fuchat learning you you
take the pre-trained uh
representation uh induced by a few
examples from new categories that that
the network has not been had not seen
doing training and you see with the
network the downstream classifier can
separate between them uh in a nice way
and surprisingly to us at least uh the
performance is very high if you take
pre-train Network that are uh that are
are trained for for the the the
canonical image net object object
classification uh you you see uh
amazingly uh good performance for fuchat
Learning and as you can might see here
it is across different architectures but
also across different types of of
learning so supervised learning for
object recognition or self-supervised
contrastive learning and so on all of
them perform very well on categorizing
with a few examples uh new new new
categories so uh moreover the the the
type of errors that they do on this task
is uh is consistent across different
networks uh so uh there is some some
some universality here which is
fundamental to way the network after
learning its own task with a supervised
self-supervised supervised generated
representations which is uh conductive
for for this type of computation and
down down here you see that our SNR
theory predicts very well the empirical
uh the empirical error for this task um
here is uh we can use the same the same
computation but also the same geometric
measures to uh to compare how data from
it
cortex the representations Fair compared
to the to the Deep convolution on
networks and what you see here is that
uh that there is again a very strong
correlation between the SNR if you
measure it from from from from newal
recordings from the car lab uh and uh
and and and the Deep convolutional
Network there is a very strong uh
correlation between the performance or
SNR and different geometric measures on
this task so this is not only uh
performance measures but it also gives
you an underlying understanding of what
are the geometric features that give
right to this performance and this is a
level that is very useful to to uh
compare different architecture different
learning uh and different task and also
brain and uh and network you can also
compare the the Emer of these properties
of this manifold for instance by
measuring the the error as it as it goes
down in along the depth of the network
and put again you see V1 like or V4 is
in the right place so to speak but it
cortex is is kind of fit roughly what
you expect from from Deep networks on
the other hand if you look more clo
closely at the the effective dimension
of the manifold you see very strong
violation or discrepancy between the
dimension Al ity of V4 for for for
images and objects and and and the
dimensionality predicted by Deep
convolutional networks and basically the
message here is that yes we see a lot of
commonality a lot of universality even
between brain and and artificial
networks but but if you look more
carefully you also discover using this
methodology you also discover
substantial discrepancy which call call
for further understanding of the reasons
for this discrepancy uh zero shot
learning is already hinted by by by
Philip is uh uh you you you trying to
learn to discriminate between two new
visual categories based on no no example
no visual example of these new
categories but simply uh using the
language representation of these
categories so you are you you you fit a
linear mapping between the
representation of the centroids of the
manifolds in the future lay and the
visual system and the and the correspond
in W embedding uh in the language model
and the question is whether this mapping
once you freeze it you can allows you to
kind of estimate where this prototype or
the centroids in the visual space will
be for new categories for which you have
only uh the the representation from the
language model and again surprisingly it
is for us the performance on these
networks is is extremely high uh and
again our our SNR theory predicts very
well uh the the pattern of Errors uh in
empirically so language and vision
prototypes are are aligned up to scaling
and rotation which again as hinted
already by Philip is telling something
fundamental about about uh about
different modalities representing the
same natural natural concept finally I
would like to show another example of
application of the usefulness of this of
this of this methodology and and
framework is looking at World manifolds
in spech hierarchy so this is again what
this is now walk by Shen Shang and and
Shan Shay Jane from Ed Chang lab where
uh here we are looking at uh uh at again
manifolds but now manifolds of WS like
air or fire now constructed by many many
utterances spoken wordss by by the same
person several times or by uh by by
different different person different
gender and so on so there is an entire
manifold of representations in the uh in
the language hierarchy now uh that uh
that form a manifold and again the
question is whether these manifolds uh
have a nice property of Separation uh
that will allow the downstream system to
actually recognize this is this is World
Air this is World fire and so on so we
are we we are doing some ongoing
analysis on neopixel recordings from
from Human from human brain for patients
that listening to many many sentences
and words and so on uh but I I want to
show you here an example of of an
analysis on on a specific uh Speech to
to text network was called whisper uh
where it takes it take the acoustic
signal uh and then eventually so there
is an encoding and decoding stag but
eventually it generates a word Rec
recognition uh automatic speech
recognition like in this case it will it
will correctly identify the worldall and
if you take the same methodology of
measuring manifolds or the SNR of of a
particular performance uh we see now an
interesting pattern of increasing
performance but in non monotonic fashion
so there is there is difference between
the encoding part and the decoding part
which tells us something about the inner
working of the network but eventually
there is a nonlinear increase uh in the
manifold uh uh geometry uh towards
towards the end and and we can go deeper
and ask what are the features that
underlying it uh like the variability of
the speech signal the number of phones
Etc so we can actually go uh even into
more finer level and use the geometry to
give us a hint of what are the critical
features uh that are uh that
characterize the the difference between
the manifolds and variability within
manifolds so I would would like to add
that what uh when when when we talk
about a theory of intelligence basically
you can you can divide it into two main
problems one is the learning problem and
another another the representation what
is the nature of the solution uh that
the system has come up with and I think
uh Philip and I uh talked about the
second one is how do we understand the
representations in the different systems
uh in in in in Ai and and how do we how
do we compare them to the brain what
kind of predictions we can make uh but I
think one of the big questions is uh how
those manifolds or or or in general how
good representations emerge uh through
learning and and I think this is we and
others have made some progress in in in
this direction but I think this is still
a very hard problem uh to uh to deal
with probably will be something that we
will address in our discussion thank
you all right well wonderful great to be
here um thanks all of you for coming and
um I think I wanted to talk about a
slightly different regime from the
regimes uh that Philip and himim have
been talking about I think they've
beautifully Illustrated that if you have
you know um data from which you've
trained models and the data are
representative of um you know the World
At Large you have enough data then you
get um very beautiful structures in the
representation that are cross modally
aligned right and um one thing um that
I'm very interested in as a
neuroscientist um and also I think um
you know in terms of theory of um
learning in deep networks and how to
make those networks more efficient are
these questions about sample efficiency
and this fundamental question which I
think um comes about when you look at
biological systems which is the
prevalence of modularity and so what I
would like to talk about is the
principle of modularity for efficiency
and robustness for learning um in in
brains and in in deep networks so um the
the the story um uh told by Herbert
Simon a Nobel Prize winner um for you
know his work in biology
um was uh you know he told the story to
illustrate the the benefits of
modularity so he said once there were
two watch makers H and Tempest who made
very fine watches the phones in their
workshops rang frequently and new
customers were constantly calling them H
prospered while Tempest became poorer
and poorer in the end Tempest lost his
shop what was the reason behind this the
watches consisted of about a thousand
Parts each the watches that Tempest made
were designed such that when when he had
to put down a partly assembled watch it
immediately fell to pieces and he had
and had to be reassembled from the basic
elements H had designed his watches so
that he could put together subassemblies
of about 10 components each and each
subassembly could be put down without
falling apart 10 of these sub assemblies
could be put together to make a larger
subassembly and 10 of the larger
subassemblies constituted the whole
watch so um we also understand that um
in in in neural systems um you know if
you want to learn compositions of you
know colors red color and then animals
right colors versus animals if we
understand colors as an independent
variable and animals as an independent
variable then we can do things like
imagine a red panda right even though
the data set never contained red pandas
before right so we don't need to see all
examples of all possible colors of all
possible animals so sort of having um
you know understanding a factorized
um uh disentangled and modular
understanding of Concepts can be very
useful for being able to imagine and
generalize um to to new situations and
so um in general if the the data points
that you're learning from are drawn from
some you know latent states that have K
Dimensions if you have to and and those
vary independently but if you learn all
of that as one combined uh you know a
set of um uh representations you need an
amount of data that scales exponential
with the dimension K but if you were to
learn the independent factors of
variation then the amount of data that
you would need to learn you would need
to learn um all of those um all of the
data would be of order like everything
about the world would be uh scaling with
uh with K rather than K in the exponent
so the idea is that if you can
understand the sort of Independence and
modularity structure of the world you
can get by with much more modular uh
much more um efficient data efficient
learning um so there are many actual
articulated um reasons for modularity
and in the literature um and this
literature spans every everywhere from
the theory of evolution to evolutionary
um dynamical simulations to learning um
in deep networks to other biological
systems and so the different um the
different advantages of modularity have
been listed at least I've listed a few
of them um but but there are some um
some you know this this extensive
literature from across Fields saying
that modular Solutions have enhanced
robustness to sparse perturbations right
because if you perturb AP part you're
not then perturbing the whole system
you're just perturbing just that one
module um it allows for um the evolution
of complex systems by allowing um
modifications of individual modules and
parts so for example if you've got um an
animal that has a visual hierarchy and
you have a knob that just can tune how
many layers or how many levels deep that
hierarchy is you can then
Evolution can you know if that knob is
controlled by a gene then Evolution can
simply you know change the scalar value
of that Gene in some way and then change
the number of layers in that processing
hierarchy right it doesn't have to
rewrite the whole brain wiring network
from scratch it's just sort of a modular
solution that can tweak you know the
depth of um sensory
processing um also modularity allows for
compositional generalization and Sample
efficiency in the ways that I just talk
talked about with red pandas um and also
it's possible to then build upon the
existing functional units and add
different functionality to the system or
Rec combine functionality without
redesigning the whole system and finally
from a machine learning perspective from
some sort of societal um uh um sort of
regulatory perspectives modular
Solutions just tend to be much more
interpretable okay so now of course I
think we all appreciate these challenges
of modularity I mean these these
benefits of modularity um but we haven't
been that successful I think so far in
um articulating solutions that are
modular for the problems um that that
you know we we task our deep learning
networks with somehow the you know deep
networks tend to be sort of very mixed
all solutions involve you know this the
initial conditions of the network start
out very mixed the networks evolve to be
very mixed and um you know overlearning
and they don't tend to become modular
and ALS also if we build in modularity
there are some challenges associated
with using those modules so I wanted to
highlight two different main challenges
I think that come about with modular um
with you know for related to modularity
so challenge one is um for how do how
can networks or models or learning
systems discover um and um and and and
and and sort of self-organized to be
modular okay so the the reason this is a
big challenge is because is is just I'm
illustrating it um with the following
example so consider that we have a task
why uh a task which is to learn this
function y is equal to this function of
X1 up to xn all right and this function
the the the actual function it actually
decomposes into F1 of X1 F2 of X2 and so
on okay so it has a factorized form like
this so if you have um just a small data
sample so you've only seen a few
examples of X and Y then there are way
more mod non-m modular solutions to this
problem than than this modular solution
right so there's no reason why um the
modular solution should be discovered
because it is that needle in the
haystack um in terms of the you know in
terms of the whole function space of
functions that could um be fit to a
finite data
sample of course if you go ASM totically
in the limit of you know very large
number of data samples then all of those
degenerate Solutions which are the non-m
modular Solutions start to fall away and
maybe the model can find the modular
solution okay but it takes a lot of
data but what's interesting is that in
biology we see that evolution is a
process that has discovered modular
Solutions in you know in body systems
and in the brain right and so really a
fascinating challenge is how is it that
Evolution if this if finding a modular
solution is finding the needle in the
Hast stack how has Evolution done that
how has it found that needle in the H
stack so one idea um is the idea of you
know the fact that modularity gives rise
to robustness right so now if you train
um networks or you know systems a
learning system to um perform a
computation in the presence of noise
maybe that would uh push the system
towards modularity so here is an example
of um of uh Boolean networks in which
there are um the the the the target this
the the target is to have two Boolean um
functions okay so it's um like I think
this is an and and so we have um you
know and they're and they're decoupled
they're two independent so we we give
four inputs uh so that's X this is X X1
X2 X3 X4 and the outputs are just X1 and
X2 and it should be X3 and X4 okay so
when you evolve now these networks
through genetic algorithms then um you
can the the the you get a diversity of
solutions here is if you evolve the
system in the in the absence of noise
then you get these highly coupled you
know these four inputs project to like
this Tangled mess and then you get your
um two outputs over here but if you
evolve in the presence of noise then you
see you get these decoupled um networks
over here that are each have two inputs
and then an
output okay and so and it turns out that
um you know you can further analyze
these networks and it turns out that
they form really good fault tolerant um
um uh uh computers so they're um um sort
of uh robust to single bit flips um in
the internal nodes okay the other thing
that's really interesting is that these
Solutions have less mutational lethality
so that if you have single bit um single
bit deletions or mutations um in these
Solutions then um the these error
correcting or modular Solutions have um
you know smaller probability of of
getting um the solution wrong and they
better survive um um they they are again
they better survive a whole sequence of
mutations so if you do you know one set
one mutation and then another and
another and so on they are more robust
to these mutations and in fact these
properties of you know better um
mutational robustness means that these
networks are also more evolvable in the
sense that if you want to evolve to a
better solution from where you are um
exploration if it leads quickly to
lethality right like like a completely
dysfunctional solution it means that you
won't be able to Traverse that that
minimum you won't be able to Traverse
that that lethal state and and and and
discover another solution that might be
better okay but by smoothing or having
this noise robustness or fault tolerance
it's possible to explore a bigger space
and find even better Solutions so it's a
more evolvable system in general all
right the second challenge of modularity
so the first challenge was discovering
modularity and modular Solutions the
second challenge of modularity is
utilizing modules if they exist okay so
what do I mean by this so now this is
another very simple example of a network
where there're two inputs X1 and X2 and
we're going to build in two two nodes um
f that that Implement a function F1 and
another that implements the function FS2
and our Target is to attain y1 is equal
to F1 of X1 and Y2 is equal to F2 of X2
so just um you know just feed forward
Network and um we just just want to find
this um simple solution and if you and
we are both unconstrained and you solve
this problem so you supply these
nonlinear functions F1 and F2 so the
solutions kind of exist in the network
all it needs to do is discover that it
should pipe X1 only to F1 X2 only to F2
and then F2 only to Y2 and F1 only to y1
that's all it needs to discover if you
try to train this network with um
endtoend training back propop um usually
if you and we have are both um free back
propop T typically fails and um and um
of course even though if you fix V at
the correct value um and train U or the
other way it it works so basically end
to end back propagation does not learn
to exploit a modulate a modular solution
even when it exists all right and so I
just want to conclude quickly by just
saying that there are plenty of examples
of modularity in biology like I said
earlier they're really striking examples
um and um you can look up some of these
if you're interested or come ask me
about them later when you're interested
so the sort of modules that operate in
parallel or there's also hierarchical
modularity with um um with uh uh uh with
um discrete networks that then um feed
forward into um into multiple processing
areas this is very familiar to most of
you in the room the visual system in in
um in mammals and uh including um
primates actually consists of just
relatively few numbers of layers feeding
forward to one another with local
recurrence within them right and that's
really in contrast to the extremely deep
networks that we have in computer vision
right and so somehow Nature has
committed to we although we can say that
the Deep networks correspond to like
unrolling in time of like a few shallow
recurrent networks the fact is biology
has committed to like a small number of
networks that said they're five and each
one is locally recurrent but then then
they're largely feed forward between
them so why you know so few and why five
and so just the the final um Insight
here is that in biology we've got
different learning rules um and um you
know we can use spontaneous activity um
There are rules for whether neurons are
going to wire up to one another or not
which can be dependent on distance
between neurons and there also
competitive Dynamics in the innovation
of um of of of of neurons so if a neuron
receives an input and that input is
strongly strengthened then maybe the
other inputs to that neuron may be
slightly weakened um because they're all
in for a scarce resource which is
inovation of that neuron okay so if you
have these kinds of competitive Dynamics
as well as some distance dependent uh
growth rules then it turns out that even
with a Rel with a completely undiff
differentiated cortical sheet with all
toall connectivity within the sheet very
quickly these kinds of learning rules
can give rise to um hierarchical modular
architecture in which um an input comes
in and inates ends up inating only a
small sub region of that
undifferentiated cortical sheet and then
these neurons then internate the next
layer and the next layer and so on
forming um discrete areas that are
hierarchically connected um but but are
small in number and discreete and in
fact that can measure um mirror the
visual hierarchy and also give rise to
other um features so basically um you
know there are many advantages to to
modularity and it's going to be very
interesting I think going forward to
think about what are the drivers of
modularity and how to incorporate them
into our models yeah okay thank you
you thank you uh thanks for having me um
I will talk about uh the power of
learning with next token predictors or
as you probably all know them uh
language models um so I'm sure you're
all aware of how great these uh kind of
new brand new language models are how
well they're doing in kind of various
different tasks and benchmarks um I
don't need to tell you that um I would
like to point out one uh very
interesting thing about language models
is that we really train them to do
something very simple we trained them to
predict in parallel the next word in a
sentence we feed them massive amounts of
data and we just want them to predict
the next word in the
sentence and then when we use them at
inference time all we do is feed in some
question maybe from the bar exam and ask
them to predict the probability of the
first word in the answer we sample from
this probability feed this back into the
model and then generate the second word
the third word Etc ET to really feeding
them questions and having them generate
the output word by word and this sort of
seems like magic we're really training
them to do something very simple and
then they end up doing something uh very
impressive and being able able to solve
very complex task um so why why is this
mechanism so useful for uh uh driving
the the capabilities of of language
model so I'll try to um you know give
kind of like a par partial answer for
why this kind of autoaggressive
mechanism of predicting uh the answer
word by word is so useful I'll give um
sort of like as a motivation I'll show
you uh one experiment so I'm training or
fine-tuning a language model on this um
kind of simple logical reasoning riddle
Jamie is telling the truth Sharon says
that Jamie is telling the truth Michael
says that reard lying etc etc um and
then I ask you for the last person in
this list is he telling the
truth and I trained the model in this
problem and you can see that it uh
converges it finds it gets 100% accuracy
in this problem it takes it um roughly
160,000 examples generated from this
problem I can increase the complexity of
the problem by just adding more people
to the list and then it it takes the
model a bit longer to find the solution
to this problem but eventually it does
it takes it around 300,000 examples can
increase the complexity even further and
then it takes it um about half a million
examples until it solve the problem you
can imagine if I keep increasing the
complexity examp of the problem it will
take it longer and longer to solve the
same problem um now I'll do something a
little bit different I will feed that
not only the question and the answer but
also kind of the step-by-step reasoning
of how to solve this problem so an
output that consists of kind of the
truth value of whether or not each of
the uh people in this list is telling
the truth or or or is lying this is
maybe the way that you would solve this
very simple task and I trained this
model with the question the output um
that contains both the step-by-step
reasoning and the final answer and you
can see that in this case it's able to
solve all of these you know different
complexity of problems roughly at the
same rate very fast you know compared to
the previous experiment it takes it a
few tens of thousands examples so it
makes learning much faster to feed the
model with this kind of um step-by-step
solution uh during
training okay um
so this I think is a very nice
illustration of kind of two approaches
for doing supervision with language
models so assume I have this input
question you can imagine it goes through
a computational process and then
generates the answer I can write down
this computational process as a kind of
step-by-step reasoning and I can either
do outcome supervision so only supervise
the output of this computational process
without giving the language model any
kind of transparent Arcy into the
computational process itself or I can
supervise the process so really give it
some hints into um you know what what is
the computation process that that
happens and we really saw that giving
this process supervision is very helpful
in kind of speeding up training and
making everything uh converge much
faster um so why why is this the case
and how does this relate to this kind of
autoaggressive uh mechanism of of doing
language models so in the outcome
supervision setting I'm only asking the
model to produce the answer and this
answer is one word or one token really
depends on a lot of variables from the
input it's kind of densely depends on
the uh on the input context when I ask
it to generate word by word the
stepbystep reasoning solution and then
arrive at the answer each important word
in the solution only depends on a few
variables right so maybe the first word
here depends only on one word from the
input in the second line I need to know
what was the kind of state of the
previous person and get some variable
from the input but really the
dependencies are very sparse and this
makes every word here very easy to
predict given the things that you
already computed or the things that you
already predicted so imagine that you're
generating the answer word by word
really everything breaks down into a
sequence of very simple
problems um and the interesting point is
this is not just a property of this
particular problem that I showed in fact
any computational process that you'll
give me I'll be able to write this kind
of sparse Chain of Thought or reasoning
process to decompose it into a sequence
of very simple operations and the length
or the complexity of this this Chain of
Thought will kind of reflect the
complexity of the computational process
I'm not going to prove this but it's
kind of very simple given uh what we
know from kind of basics of computer
science that you can really assemble any
computer for very from very simple
logical Gates so really you can
decompose any problem into a sequence of
very simple
problems um and another thing that we
can show that there are some problems
similar to the problem that I just
presented that are very hard to learn if
you're only given outcome supervision on
the problem but given process
supervision if I supervise the entire
kind of Chain of Thought reasoning for
solving this problem language models
even very simple language models will be
guaranteed to learn learn to solve the
problem and this might explain a lot of
the progress driving llms we're just
able to provide them with data that sort
of decomposes complex problems into
simple
ones and maybe another thing that I
would like to point out is that even
though process supervision is really
speeding up the training of gradient
descent it's not that you cannot solve
this problem just from outcome
Supervision in fact the first experiment
that I showed showed you that gradient
descent is able to learn all of these
problems eventually it takes it maybe
hundreds of thousands of examples orders
of magnitudes more data than you
actually need but at the end it's able
to solve the problem you know get the
same kind of level of accuracy and this
sort of really relies on the ability of
uh back propagation grad and descend as
kind of driving their learning of this
uh of the of these uh language models to
kind of tweak all the circuit in the uh
in the network until it finds the
correct solution so it might take it a
very long time to arrive at the correct
solution but eventually it does so it
can do well only with outcome
supervision but the cost is extremely
high if you compare it to the cost of
training with just process supervision
and maybe this could explain to some
extent the kind of enormous cost of
training large language models just uh
increasing in cost from kind of one
generation to the other because
essentially for most of the training
data that we're providing the language
models with is mostly outcome Su
provision kind of text that is gathered
from all over the Internet and doesn't
necessarily have this kind of stepbystep
uh solutions to to these complex
problems maybe the model needs to kind
of learn to infer these uh um solutions
kind of on its
own okay um and just to kind of maybe
leave some room for discussion Maybe
maybe relate this to uh um a previous
talk or can more generally to
Neuroscience um so I think we'll know
essentially that um learning in the
brain is in some sense very different
than uh the optimization the kind of
like Global uh synchronous optimization
of GR in a s in the brain we sort of
understand learning as more of like an
asynchronous uh learning roles that
operate independently
and back propagation essentially relies
on this kind of global synchronization
of the of the entire system and this
seems to be very important if the only
thing that you have is this outcome
supervision I give you like a problem a
very complex computational uh process
that generates the answer and you don't
see the like anything in the middle and
this you know to solve this problem you
really need to kind of optimize
everything together maybe with more of
this kind of process supervision more
transparency into the computational
process maybe you don't need this kind
of global synchronized optimization and
these kind of sort local updates are are
enough and maybe this um again giving
some wild hypothesis maybe this can uh
explain the efficiency of uh of learning
uh you know uh for humans that you know
see much less data and kind of use much
less energy and are able to uh learn in
some sense more efficiently than these
uh lightning
let me try
to tell you about potential
principle
um compositional sparity so what what do
I mean is
um um here is again a series of puzzles
like the ones I've shown before that you
can ask and they are essentially why do
you need dep deep networks
um and why networks Escape or seem to
escape the curse of
dimensionality which says that you
potentially need a lot of
parameters for increasing dimensionality
of the function you are trying to
learn
um okay the related question about
generalization
and um about
physics
but let me remind you briefly what is
the framework of
classical machine learning theory in the
Deep Network case so think about using a
deep CNN or such for learning to
classify
images okay the basic framework is that
you have
set of data X and Y you have an unknown
function f
mu that produces this data you don't
know this
function so you're trying to learn a
proxy for your
data and you want to do this by
using um a family of parameterized
function
that approximate well the unknown
function okay you you want
this um to be parameterized because
eventually you want
to um optimize the
parameter by minimizing the error on the
data which is the only one thing you
have so the key part is is to have a
family of parametric function this will
be deep neural networks and the
parameters are the
weights that is powerful enough to
approximate a very large class of
functions and to do this approximation
without having a number of parameter
that
explodes with the dimensionality other
properties of the unknown function
okay so
um uh the main result I want to show you
this is this one
that every function which is efficiently
Turing computable so it's computable by
a t machine in nonexponential time in
this case in the dimensionality of the
function for every such function there
exist a sparse and deep Network that can
approximate it to without curse of
dimensionality okay so that's
a that's the main result um let me try
to explain the framework suppose you
have a function of D variables D is
bigger than 20 or so
then the theory says and this are known
for many years that an upper bound on
the number of parameters you need to
approximate this function with a an
error in the
subnormal um of Epsilon you may need a
parameter this is minus t / M where m is
some measure of smoothness like the
number of bounded derivatives of the
function
so
um you know if you have D equal 10 and M
let's put it to one for Simplicity for
now now is you have Epsilon to minus 10
Epsilon say 10% error this is 10 to the
10 which is big but not so big but if
you have say an image a small image
cifer is 30 by 30 you have th000 pixels
so now you have 10 to the
Thousand just to remind you the number
of protons in the universe is 10 to the
80 so what what happens is that if if
you have a function that can be
represented as a function of
functions think
about about
um a binary
tree
um or a threee a graph a directed a
secret
graph um so the function is a
composition of many function the right
do
function this function has two variables
this one has three this one sorry two
two inputs four inputs so basically for
functions of this type the number that
enter in the curse of dimensionality the
the D in the previous result is not the
D of the compos compositional function
but is the maximum D among on the
constituent functions for example in the
binary tree if each node is a function
of two variables the curse of
dimensionality for that compositional
function has a d equal
2 okay so compositional functions can
avoid the curse of dimensionality if the
constituent functions are
sparse that's one
result that we proved a few years ago
and the
second theorem is that efficiently
Turing computable functions are um
compositionally
sparse and if you think about it
um a t machine can be represented as at
the end as a very deep com um series of
conjunction and disjunctions so
that's the basic intuition you can
compose complex computation in terms of
simple ones think about a program and
about about rewriting in terms of simple
sub
routines so um so it turns out that
compositionality is
um almost equivalent
to sparse compositionality is almost
equivalent to computability at least in
the efficient case and uh and and so you
know after 20 years I have an answer to
this to this question where I had a
paper in which I wrote that there is the
paper was about Theory with of shallow
networks like kernel machines one hidden
layers and um
there was no we had no understanding why
you need a depth in uh in in the brain
or in artificial networks and so now you
need deep depth if you want to represent
a large set of functions um but you
still can do it um with the ability to
approximate very well without curse of
dimensionality okay so um there is
furthermore some other result that says
that if you have um
sparsity so if you have a function that
is sparse and you assume that you have
layer of a network that represent each
compositional function then you um you
conclude that you should have a small
number of effective inputs for each of
the hidden units or sub
networks and if you have that we you
have a separate proof that you can get a
bound through raker complexity of the
test error in a deep Network that is
several order of magnitudes better than
the standard ones so sparcity seem to be
important for generalization in this
case uh
um it's open question whether it plays a
role or not in
optimization which
um uh which is of course the most open
area of machine learning
today so let me finish here and I think
we can now uh
assemble here all the five was and try
to answer a question from each other and
from the audience all
right let me let me start I
think we spoke about all together as a
panel
about um you know the the uh formation
of a representation I think that was you
and him you were grouped together quite
accidentally but in a good way and about
the evolution of architectures that
support
representation and about principle for
you know Transformers or large language
models the auto regressive principles
which is related to
compositionality
um so um let's start with future
representations
um uh I think this is uh a question in
optimization I don't know if we agree
but it's the question if you uh focus on
a deep networks of
how um features I don't know actually I
don't like the term feature but the
output of each layer change and the
weight at each layers changes across
layers and across
iterations and I don't know whether the
manifold hypothesis can say something
about it or not
because in a sense IT addresses the end
result right
I'm well I think yourself okay I as I
hinted uh we uh I think it is very
important problem and uh and and largely
open in my in my view um we have some we
have made some progress in understanding
uh how each uh I would say motif of a
Network um changes the the the geometry
of the of the fature representation and
nonlinearity pooling uh
convolution Etc but but still uh I think
the overall picture is is still uh open
and I I would like
to to to uh to highlight the reason why
uh I may be wrong but
um I think a large part of uh progress
in understanding the theory of learning
and generalization well there are many
many approaches to it and and Tommy has
has described one of them but another
line of approach uh use the white
networks and the notion of Kels um as a
uh as a way to
theoretically um make Advance on on the
theory of how Solutions uh and
representations emerge and uh I I think
we understand very well uh the regime
what is known as lazy regime where
basically the learning is is is doing
small fine-tuning of an underlying
random weights and it's big enough it's
wide it's it's deep and so on so it can
it can deal with the with the training
problem but it also uh as as Tommy you
mentioned it it is has has enough
regularization or or inductive bias to
uh to to get yield reasonable
generalization this type of solutions or
architectures will not do the job in
terms of the representation that that I
described and I I think also that that
you described so the the the emerging
representations are nearly random not
entirely random because there is some
structure in underlying input so there
is some structure but but not more than
that so so I think the actual real
life uh High performing networks are
really living in a different regime
either because it's I don't know not
lazy or Rich regime or because the the
the amount of data and the structure of
data and the and the
task are really living and I think in
different regime and I think that's
that's to me that's kind of a key point
that what we have the progress that the
theory of learning has made in the last
I don't know five or 10 years in certain
directions have not
captured the emergence of the very high
quality representation that we see in
kind of deep networ that's all real life
uh task well that's a question for
optimization how does that
happen it's uh clearly the kernel
machines suffer from the curse of
dimensionality so they cannot be General
enough I wish I would have known 20
years
ago but um unless you use a modified
kernel in which you have it's what we
called many years ago but we forgot
about it hyper bef you have for instance
a gaussian but you have a malan nois
type distance uh
learnable then you can avoid the curse
of Dimension
sure I think RBF will will not make much
difference the RBF can you you think
about them in the context that I was
talking about you can think about them
as kind of different non linear units
that do the RBF the the the problem is
I'm not using in the context that I was
speaking about it's not an assumption
about using Kel machine or RBF and so on
it is an emerging property of wide deep
networks in in a certain regime of data
that more or less random kernels even if
the RBF then the kernels of the RBF will
be random emerges in other words there
is not not too much not too strong
pressure for for the network to actually
build very uh high quality
representation you know there are a lot
of question we could discuss I think
maybe we
should and a lot of open problems um
let's focus on which one may be
between math and artificial
networks and
Neuroscience so one one problem is I
think it's a
missing it's a missing it's a gap at the
at the moment
between um deep networks the engineering
of it and and Neuroscience and this is
we don't know which kind of optimization
algorithm in the brain could
replace you know SGD or gradient descent
techniques do you have any anybody has
an idea
I mean I guess I could take one stab at
it I think it's related to what point
himim was making at the end which is
that we don't know I mean one way to
look at it is in terms of
representational learning and feature
learning and it's true that if you have
a very wide network with the you know
weak scaling of the weights then you
don't get rich feature learning to
emerge these neurot tangent kernel
Theory limits and but the brain clearly
has very rich representations and I
think it's also related to other
architectural emergent properties and
other ways in which the brain
understands the world which is that we I
think um if we see data we see pixel
data we see a ball running into a wall
we're not thinking in pixel space and
we're not finding dense dense models at
the pixel level of interactions we're
actually assuming that there are sparse
causes there's a there's an object a
ball and it's moving H coherently as
like one entity and then running into
the wall and then bouncing back right so
we tend to infer sparse causes even from
dense observations right I think all of
these things that the fact that we have
Rich feature learning the fact that we
learn we have the sparsity bias and
thinking about causes and the fact that
you know we can have these we we tend to
favor disentangled
representations these are not biases
that backr
contains and somehow I think it is
indeed as you say Tom about learning
rules and it it it could it could very
well be that it it's very difficult it's
a very difficult problem because we know
that if you're not doing gradient
learning of some type then a difficult
problem is hard to solve unless you're
moving along the gradient at the same
time there's one option is that there is
an alternative an implementation of
gradient techniques in the brain you
know possible who believes that I
believe it I mean I I I think there's no
alternative what do you all think I
think there has to be gradient learning
of some type
but but complemented with other things
right there can be other pressures too
yeah I mean back propagation you know
the the consensus seems to be is not
biologically uh reasonable you know to
expect exactly back propagation in the
brain but uh um but I think there are
good
Alternatives the other option is you
know something completely different like
learning one layer at the time yeah I I
I think I mean um we kind of take it for
granted that you know back propagation
is like the only way to optimize neural
networks but you know other methods have
been explored with like different uh
degrees of success and yeah like
optimizing wer at a time and like
certain situations is um can be comp
competitive I feel like it's it's not a
question of like whether or not you're
using gradients like taking derivatives
but whether or not like kind of all the
optimization is kind of synchronized
throughout the network or you have
something as kind of more local um
optimization and and here I think that
you know for optimizing neuron networks
we rely on this mechanism um for kind of
synchronized optimization of the of the
entire circuit because you know in some
some cases kind of data is cheap compute
is cheap we can you know this is the
only thing that we have and you know we
might it's it's easier to kind of throw
more data and energy uh into the problem
and and and solve things with with
gradient descent where um you know there
could be an alternative algorithm that
is um you know maybe just as good but we
kind of haven't discovered it so we
we're just using what we
have yeah I guess my my thought on this
would be what do you mean by gradient
descent like almost everything in some
sense is a local move that goes toward a
lower loss but I I have a post Jeremy
Bernstein who's been uh teaching me a
lot about that gradient the gradient
descent algorithm is one specific way of
doing a local perturbation that you know
minimizes the loss and there's actually
a lot of other steepest descent methods
a whole family of them and
um so yeah they're all they're all
gradient descent in my like from my
prior yeah kind of way thinking about it
but I guess question is whether there is
a biologically plausible implementation
that uses only H Like rules you know
things we know the brain or synopses in
the brain do so I guess my thought would
be it will be a local perturbation that
moves toward lower loss but it might be
very different than the gradient given
by backrop and
um I'm not super optimistic myself that
that'll be better than backprop uh I
think that's been so many attempts at
trying to come up local learning roles
that are better and maybe that hasn't
panned out yet but at least it would be
biologically interesting and maybe that
the benefit would not be so much that in
optimization but more in how that
regularizes or affects the um you know
it has an implicit bias toward a
different type of representation that's
learned I think you know this is a very
interesting Gap that if we could fill
would unify
arch in neuroscience and in artificial
networks yeah I'm glad to comment on
that I think we have to be careful what
are we comparing to B if you're
comparing Vision artificial networks for
visual for basic visual recognition or
or language I
mean the brain has evolved through you
know millions of years um so so what are
we I mean are we trying to compare SGD
to back propop to
Evolution I I this is why I'm I'm going
back to I think for to to to the
representation we have a pre-trained
network in our mature brain and we have
pre-trained Network which which got it
through SGD but I don't think it's it's
actually a relevant scientific question
about trying to approximate SGD in these
cases if if if you really want to
compare a per comparison we have to take
a a a a task where a mature brain is is
learning and kind of maybe a random task
so to speak for the mature brain and
then compare how how pre-train network
will do that because then you can you
can actually look both behaviorally and
for Animals also Neally and make the
comparison otherwise the brain is so
much Advantage you start from scratch
from some what even if it's deep you
start from some random weights so one
implication of what you say is
that you know lot of people are using
deep neural network to build models of
the brain you think and I'm doing it yes
right so why do you think that's right
because I don't care how how the how the
two
system emerge at the solution
I care about the solution and the fact
that that we find that different
networks with different learning
algorithms they may use SGD but one of
them is kind of contrasted didn't see
any labels another one is
supervised but and so on so forth they
have very
similar properties in the solution that
they arrive so this is why I think it's
is a air to to to take as a tentative
hypothesis I'm not saying assum you
agree that synapses do learn during the
an adult life right but but what they
learn Vision what they learn I mean yeah
you have to take a task which is really
not a natural cognitive task that we
just are born with or and and naturally
de develop I mean I think that's that
will be if you really if you're really
interested in the question of learning
then you really have to take those t and
there are stff like that uh and for for
what we know at least most of what we
know is really kind of reward based
learning for taking you know an animal
mature animal and you just learn a new
some new navigation task right and
reward based L just based is an
optimization there is an
objective but it is you know if you look
at the algorithms it's more like local
perturbation and basically explore it's
more the exploration than exploitation
right anybody wants to add
something this is um so so I completely
agree about the kind of the importance
of the pre-training and the
representation and maybe just to
emphasize that a little more I mean it's
always been kind of confusing to me when
people compare the learning algorithm in
the brain and the learning algorithm in
deep Nets since yeah it's apples to
oranges they're not pre-trained in this
to the same degree um but one one like
point to drive that home is I think that
there's theoretical results that that
any learning algorithm you know for some
definition can be approximated by
gradient descent on a pre-trained
representation so like that's
that's that's Universal in some sense um
Chelsea Finn and Sergey LaVine had this
um is in one of their works on mammal
then they say a pre-t representation
plus end steps of grent descent can
approximate any learning algorithm so if
we don't take into account what the
pre-trained representation is then it's
not much we can say about the learning
algorithm and its efficiency
I personally think that if you put one
of
us um as a
baby in the
forest no parenting no teachers you
would be very much like a monkey so what
I'm saying is there is a lot of learning
going
on in what we call
intelligence it's learning built on
you know what mankind has done
written over centuries or Millennia
but that's interesting I mean I I would
almost take the opposite attack as a
parent I would say that there's very
little that you know is imparted from
the environment of course the things
that we value as human beings like
cultural learning literacy all of that
stuff is surely stuff that you learn
from your culture but you know just
looking at you know people coming up and
growing up in very diverse environments
and countries and levels of affluence
and stuff like that I guess I would
almost say that there's very little
learning that we do in a lifetime my my
sense would be that it really is all
evolutionary time scale learning and
there's just very little that we learn
on top of that so that's um you know but
again I would like to if if we're really
serious about this problem and I think
it's it's it's it's it's a fundamental
problem we have to be practical about it
and we have to not ask about you know
how Vision EV you know evolved or
developed or how language and so on I
think this will be hard to actually
solve the problem we have to take a
mature neural network and a mature brain
and see and test how these two systems
learn on the basis of their pre-training
learn a new task and comp and then we
can make you know a fair comparison and
how how can we do that if the initial
Arch the the the pre-trained
architectures are really different right
because then it's the pre well there is
a feature representation which is very
similar okay so you're saying hopefully
they're similar on let's see what
happens what I'm saying is what are you
know where do we do where is learning in
the brain what is the algorithm for
learning in the brain so I can give you
an example which is what hyper accurity
um uh uh perceptual learning classical
psychological and neural Paradigm of
learning okay you can you take a a
mature sensory system mature visual
system and now you take an animal or
human and ask you to do very fine
discrimination between two nearby things
okay so now so this is the example now
you have a mature perceptual systems in
deep Network or whatever you choose and
uh and and and the real brain and now
you can ask okay so how
how do I learn it you can ask about
graded distance but you can also ask
where the learning occurs is in the
readout is in the in the input you know
and then well we did that 20 years we
also did that thank good what is the
answer that you got you guys know what
is the answer that you got we're
learning across in this task high
accurity fat learning it can be quite
simple one layer yeah it can be
where well um where where in the brain
in the brain no where in which stage but
I'm the the point is you know which kind
of algorith do you need do you need
something
like um back propagation across more
than one layer or you need a very local
rule is right and if it's more than one
layer where are the
circuits plausible that do
that I think you need more than one
layer I can tell you that for some
classical perceptual in paradigms what
we find
surprisingly in in in at least in a deep
networks in the brain is still
controversial you don't mess up with the
readout the read out is actually fixed
everything the Deep layers are fixed you
actually have to go to one of the early
layers and change them let's say V1 just
change the V1 and everything else is
fixed and if you do the
opposite if you fix the representation
and just build the specialized rout
you're not going to solve the problem
counterintuitively so here's an example
of of and in the brain by the way it's
controversial people find some some Labs
find changes in V1 some Labs claim no
and some some Labs find changes that are
not really good for the task in other
words you just head kind of thing which
is not really help helpful for the task
so in any case so but this is the kind
of thing that is conrete enough that we
can argue about this algorithm or that
algorithm how to do it in what I'm
saying is an open question in the brain
where but here this is but this is
concrete enough that I can imagine that
we can we can ask our colleagues
experimentalist to actually make good
measurements of that okay and and shed
light on this kind of question well I
think would be good to have first a
plausible ideas and then to do the
experiments anyway I'm suggesting you
know this this is a very interesting
open problem that's
all um let's see maybe we should ask
audience
to to come in with question
yes
y um I have a question about um the
anatomic and the structure of the neet
because um you mentioned that uh dur
anatomic anatomical and biological
evidence to show that even like locally
connected um shallow Network can
um repres Can mimic the performance of a
pretty dense with u neet so I wonder why
what is the challenge of us replicating
this in in the in the machine learning
you know my my question is why can't we
do that right now what is the challenge
to replicate like shallow but locally uh
recurrently connected layers like like
DNN right now
that's that's a great question so um
actually U I think conventionally what
has not been done is building in local
recurrence right so the conventional
answer is that the extremely deep
networks for visual processing are like
rollouts right of recurrent processing
and um and so instead um you know uh
what biology seems to be doing is it
seems to have a few feedforward steps
but with local recurrence in each of
them and I think computationally there
is not there's no technical challenge to
trying to model such a circuit anymore I
think it's possible to train a network
um which is you know a few layers with
lateral connectivity within each layer
and actually that's an effort that's
ongoing in in my group and in a few
other groups um I think in you know uh I
think Jim de Carlo's lab is trying to
they've built a shallower network um uh
but it doesn't have the lateral
recurrence in the fully realistic way
but but I think they're they're working
on that so I think there aren't
technical challenges the interesting
question is what is the right model for
the recurring connectivity and you know
training is harder um but um yeah uh
we're finding competitive results to
much deeper architectures with the
sparsely connected lateral um shallow
networks shallower
networks I can ask my question so this
is a question for El um you mentioned
that uh the Emer
you mentioned that the uh like the brain
is emerged like in a modular way it's
modular right and the primary reason you
said is because of the um the re the
noise if you have noise in your uh
search space and how you're searching
for it then that forces you have a
modular Solution that's more robust of
noise but that seems like that can't be
the only um driver of modularity and
there has to be other ones because if
that were then we would have came up
with deep learning algorithms that you
know there's noise in the search process
or something so I guess my question is
what are the other main drivers of
modularity that you see being a thing um
and that can we use them to create
better modular networks for deep
learning so I guess one main one that I
see is the developmental process because
in um in the real life you have a
constraint of being like um compressed
into a DNA so you can't encode every
weight you need to encode only the
developmental process and that means
that you can only encode modular
structures basically so are there other
drivers that Evolution found sound for
modularity yeah that's a great question
I think that and that's kind of you know
the program that I'm hoping you know
many of you will be excited about and
and want to work on we should talk uh
but um yes indeed like noise is just one
of the drivers for modularity there have
been in the literature there other you
know claims for um modularity emergence
which are you know if you've got tasks
that are by Nature compositional um with
you know many reuse subtask that's one
other driver of modularity other drivers
of modularity can be just spatial you
spatial uh constraints which are that
you know developmentally each neuron
only makes connections locally and not
you know further away and so that forces
um connected and functionally um sort of
um related neurons to be you know
physically close together and that
encourages more modular Solutions
another one is competitiveness like
competition in in wiring so that you're
forced to prune you know weights that
don't that don't really contribute to
the task um there may also be like not
you know backrop like learning rules but
more like local rules where you choose a
neuron that's doing the best at a task
and then only learn its weights you know
uh right um so do a step for that neuron
but not all the neurons in the circuit
um so there many I think there's got to
be there's a huge number of potential
drivers for modularity and I think it's
going to be very interesting to mix and
match and then they can have very
synergistic effects and not just um you
know combine linearly but they they can
they can accelerate um uh you know the
the the Dynamics of a system towards
modularity so um are convolution or
networks I mean I they are in some sense
because they're spatially you know
they've got the spatially local kernel
so I guess you could say that they're
kind of modularized in how they're doing
their processing of the local pixel
space um and we've built that in by hand
so that that's good and I think that's
one of the strengths of convolutional
networks over MLPs in the visual domain
clearly um but then like they're not
modular in the sense of you know I mean
there's there's a lot of layers like in
the limit of infinitely deep networ like
limit right almost a Continuum like 00
layers I would say biological circuit is
more modular because there's five layers
with then recurring connectivity within
it
um uh so excellent panel my my question
has to do with and I come from Left
Field a bit on this so uh please excuse
any any uh things that were implicit
already said but my question has to do
with supervised learning versus
unsupervised my my main question about
the state-of-the-art of the field is
aren't we looking only at supervised
learning what what is happening with
unsupervised learning in particular with
respect to sparcity or modularity
however you want to call it and and uh
efficiency which are the two top themes
that were're talked about today I would
like to hear what your thoughts are on
unsupervised learning or or or
experimentation if you want to call it
that uh versus either sparsity or uh
efficiency maybe I can I can start with
just unsupervised versus supervised I I
would say that the line between them has
gotten quite blurry so um there's
something called self-supervised
learning which is applying the tools of
supervised learning to predict raw
unlabeled data and that's that's kind of
the main Paradigm right now for how you
you pre-train these models so you arrive
at a really good Vision system by just
training it to um you know predict
missing pixels just mask out some of the
pixels and predict them you come up with
a really good language model by just
mask out some words and predict them and
so people call that self-supervised
learning it it's not supervised toward
the final task that'll use it for it's
not supervised for like I want to
classify my emails into spam or not it's
just supervised in to predict missing
data so I think that is the kind of
that's the main framework right now
behind feels different than
experimentation though somehow I mean
you're maybe it's not maybe I'm thinking
about it the wrong way but if I try to
experiment on the world and push blocks
around and stuff like that oh oh oh oh
yes so right Interactive Learning or
like learning by experimentation I I
think um that's not yeah that's not part
of the way that these things are trained
like language models and Foundation
models right now um but I agree that
that's like fundamentally different
Active Learning I think in reinforcement
learning
paradigms there
are strong aspect of component of that
exploration uh and and and sometimes
some in some versions of reinforcement
learning there are parts of the learning
is is you know driving driven by
curiosity you know exploring the world
world and gaining some information about
the world uh but but ultimately it's
only component of something which
ultimately has a reward and has a goal
reinforcement um so in my in my own view
the problem with unsupervised learning
is that the the space is large and there
are no there are no really because it's
not supervised there's no really we lack
rules to guide us whether haban or
anti-ban uh competitive or operative
and no you can we can experiment with
different unsupervised learning
paradigms but uh it's it's it's hard to
to come by with something that works it
may work for one problem it may not work
for another so the I think the
attraction of supervised learning even
in the self-supervised fashion is that
there is ultimately an objective
function which is might it not be much
more efficient to use experimentation to
Super
yeah I think maybe reinforcement
learning of this time experimentation is
you know getting more and more into uh
the domain of language models there's
like reinforcement learning from Human
feedback that is becoming more dominant
you let the model generate sentences
which are basically kind of you can
think of these as novel explorations of
new Solutions and then it gets rated by
human evaluator so there's I think this
component of what you're describing this
is not necessarily inter interacting
with a physical world but you do
interact with kind of a human that is is
uh challeng I would say that most of the
architecture at the moment
are a combination of the supervised
learning framework even if there is not
an explicit label and reinforcement
learning you know if you look at
reinforcement learning yeah I mean uh
Alpha
fold
um uh Alpha go uh
the all the large language models they
use in various types the supervised
framework and for instance human
reinforcement for fine tuning and so
yeah I think we are told that the food
will disappear if we don't go there it
may have already disappeared so I think
we should thank all of our panelist and
all
thanks

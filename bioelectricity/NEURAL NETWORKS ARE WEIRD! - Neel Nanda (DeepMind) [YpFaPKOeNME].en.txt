you could you could just I could just
hear the like slight creepy sounds in
the background of
footsteps three things you're going to
learn everything about sparse Auto
encoders what is the right way to do
causal interventions on models and how
to think about it why mechan mechanistic
interruptibility can help us make model
safer machine learning is in some sense
a really weird field because we produce
these artifacts neural networks but no
one designs them they're not not like a
computer program that someone made a
design and then wrote the code instead
we just create some flexible
architecture shove lots of data in and a
effective system comes out but we have
no idea what's going on inside or how it
works and these are really impressive
systems they can do lots of things lots
of complex software engineering tasks
and complex of reasoning and get IMO
silver medals we just don't understand
how they work and what's happening
inside we have computer programs
essentially that can do things that no
human programmer knows how to write and
my job is to try to fix
this is Agi an existential risk is a
pretty polarizing question with lots of
prestigious people and strong opinions
on both sides but like frustratingly
little empirical evidence and I think
one of the things that interpretability
could potentially give us is like a
clearer sense of what's going on inside
these systems like do they do things
like that we would call planning do they
have any meaning meaningful notion of
goals will they do things like deceive
us and the more we can understand how
these things manifest and like whether
they occur in situations where they
shouldn't the like more I think we can
learn and this seems like a really
important research direction to me the
robustness to its representation post
steering kind of indicates to me that
it's more than just a keyword matching
it actually understands what the thing
is it seems pretty clear to me that it's
not just keyword matching because we
observe things like multilingual
features where the same text in
different languages lights the feature
up you tend to see these Beyond a
certain model scale in scaly monos Manti
they had multimodal features like the
Golden Gate Bridge one lit up on
pictures of the Golden Gate Bridge and
like with my interpretability hatle I
don't find this very surprising because
if you can map your inputs into a shared
semantic abstract space you can do
efficient processing so like of course
this will happen and there's some
interesting work like there was the
do llamas think an English paper from I
think Chris Wendler I think that seemed
to show that the model decides what to
say and it decides what language to say
it in at kind of like different points
and you can causally intervene on them
differently but I think if you have the
kind of engram matching stochastic
parrot perspective you would not predict
this I know I don't really understand
the people who hold that perspective
nowadays to be honest I think it's
clearly
falsified so I think to understand Spar
Auto and cod we need to first begin with
what is the problem they're trying to
solve so you can think of a neural
network as being made up a bunch of
layers and you pass in some input it
gets converted to vectors or a series of
vectors in the case of a transformer and
then each layer transforms this into a
new vector or series of vectors these
are the kind of activations in the
middle often in the layers will kind of
in the middle of them have intermediate
activations we believe that these
activation vectors often represent
Concepts features properties of the
input something interpretable but it's a
vector we need some way to convert it
into a thing that is Meaningful and
sparse Auto encoders are a technique
that tries to do that they basically
decompose the vector into a sparse
linear combination of some big list and
a dictionary of meaningful feature
factors we hope that these feature
factors correspond to interpretable
concept
and it's sparse in the sense of most
vectors are not part of this combination
on any given input I think that if you
go on something like neuron pedia
they'll often show you kind of different
intervals like what is between the 50th
and 70th percentile of activations let's
like give you some stuff and this is
useful for getting a kind of broader
view also just looking at the causal
effect is interesting like feature the
that lit up on like fictional characters
in general but especially Harry Potter
but when I sted with it it was kind of
only Harry Potter related
things you can join mind's AI which is
part of TW for AI labs to do cool Arc
research and eventually beat Arc after
that they're going to work on llm based
reasoning following in the footsteps of
01 All Pure research no product well
funded go to
tabs. and we'll be covering some of
their research talks in Zurich 2 in the
coming months we're also sponsored by
centl the insanely optimized model
serving platform it's Best in Class
compared to others 400% faster tokens
per second than the hyperscalers it's
over 70% cheaper just 8 cents per
million tokens on llama 8 billion on
your own private endpoint you get 10
free credits to evaluate the platform so
what are you waiting for sign up at sens
ml. this is quite a long podcast the
first part of it was an outside
interview that I recorded with Neil and
then the the rest of it is the inside
component where we talked a lot about
mechanistic interpretability and sparse
Auto encoders enjoy machine learning is
in some sense a really weird field
because we produce these artifacts
neural networks but no one designs them
they're not like a computer program that
someone made a design and then wrote the
code code instead we just create some
flexible architecture shove lots of data
in and an effective system comes out but
we have no idea what's going on inside
or how it works and these are really
impressive systems they can do lots of
things lots of complex softare
engineering tasks and complex of
reasoning and get IMO silver medals and
I'm like we just don't understand how
they work and what's happening inside we
have computer programs essentially that
can do things that no human programmer
knows how to write
and my job is to try to fix this so I
run the Google deepmind mechanistic
interpretability team and mechanistic
interpretability is a type of AI
interpretability that says I believe
that neural networks in general learn
human comprehensible structure and
algorithms inside and that with enough
care and attention and rigor we can go
through them and at least start to
uncover parts of this hidden structure
and go beyond just treating it as like a
weird Black Box system or a thing we
take gradients through to light up parts
of the input but like actually
understand how the algorithm is
represented in the weights and
activations and it's really hard but we
made some progress and I'll talk about
that today yeah what what is reasoning
to you Neil do you mean mean what is it
to know when an llm is reasoning or just
what is reasoning in the philosophical
abstract philosophical abstract oh God
philosophical abstract um I guess I'm
pretty sympathetic to the applying some
kind of logical rules of
inference like you have some knowledge
you do things with that knowledge to
produce more
knowledge though it's kind of unclear
whether there needs to be a sense of
intentionality and agency behind it like
if a squirrel has the Learned reflex of
noise runup
tree is this reasoning or not I don't
know and I think there's also a
interesting question with llms of is the
llms forward pass the thing we are
analyzing or is the llm as a system
including long generations and Chain of
Thought
um or ridiculously long chains of
thought in the case of o1 is that a
thing that we can consider to be
reasoning cuz like that can clearly
reason like it produces a logical stream
of inferences and you can give it
arbitrary things like there's that
Pronto QA Benchmark that's got all
tempus are lumpus all lumpus are blue
what are tempestes and models can do
that with Chain of Thought um
while it seems a much higher bar to say
that it can do that kind of thing win
the Ford pass and honestly that seems
more like a claim about the internal
cognition
and I don't know that's why we got the
field of mechanistic and tability but we
haven't got that yet in terms of
inference time compute
versus um having kind of phase
transitions or emergent things during
trading and it seems
hard to compare to me like when you
train when you spend more compute
training a model it will have more
capabilities some might be learned in a
sudden way some might be learned
gradually I don't really care I don't
think that's very relevant here um if
you want to train a model with less
compute but spend lots of it on
inference time compute that will get you
a different tradeoff I basically just
buy that for many use cases the ratio of
inference time to training time comput
is like has been quite off historically
and that in future it's going to be more
balanced I think that we don't yet know
what these new inference time compute
Focus systems are going to look like
economically like we have scaling laws
of loss curves but how does this cash
out in economically useful things um so
for example one thing I found quite
interesting in the o1 system card is it
was only as good as claw 3.5 sonnet at
agentic tasks I think that inference
time computer systems are likely to
be mo most useful for that kind of thing
because you're happy waiting a while
while it just goes off and does
something you don't need to interact
with it and I predict we just haven't
learned how to use them and I predict
that we haven't learned the right
scaffolding to make them good at this
and this just gives me very wide error
bars on what the economics are going to
shake out as um you know having having a
good twe Community I think is Paramount
to getting intelligent commentary so um
I have occasionally seen intelligent
comments on Twitter here's a good
example so Joe Smith said ask him
literally anything about why AI safety
is important so he can dunk on you
ding-dongs all right why is AI safety
important so uh I think duning is
distasteful but also I don't know I
think that human level AI is just a
thing that is technologically feasible
that will happen sooner or later and a
bunch of people are trying to build it
and this just seems like an incredibly
big deal that will massively change the
world and if you have these intelligent
autonomous agents there are ways this
can go well and there are ways this can
go badly and it's just really important
that we spend a ton of research energy
on things like safety and
interpretability so we can make this go
right Daniel Fillin he said how much
progress have we made in mechin turp
over the last 5 years and are we on
track if it were I
feel reasonably good about where the
field is at I think that we're trying to
solve a really hard problem and we might
fail though I also think that progress
in mechant often looks like uncovering
more and more of the structure in a
system such that even if we don't get to
the kind of really ambitious goals in
the field which I think are often not
very realistic and then we can still get
to the point where we are able to do
interesting useful things with systems
and is that on track is that enough kind
of unclear I feel like we've made quite
a lot of progress I feel like there's a
lot of things I Now understand much
better like superposition and what to do
about it how to do principled causal
interventions to try to onover circuits
how to engage with the Trans performers
how to like what kinds of structure to
expect in these models but also there's
a ton we don't know so unclip is there a
numeric Benchmark that has a a
measurable score in mechin turp you know
something like the the blur SC score in
in NLP do you think it would be useful
and how close are we to to getting that
I think the problem here is that summary
statistics can always light you you're
taking a rich complex confusing
high-dimensional object and you're
trying to compress it down to like a
number or like a bunch of numbers and if
you have a kind of well-defined
technique where you know what it's
supposed to be doing and you're just
trying to measure something like um with
the sparse Auto encoder how good is it
at reconstructing the model at a given
level of sparity that's just pretty
measurable or if you're like how good is
it at helping me unlearn some fact
that's also pretty measurable and these
are just things that I would like sparse
WM coders to be good at but there's lots
of things
like does this model exhibit superp
position have we truly understood this
circuit or could there be I don't know a
positive and negative bit canceling each
other out that we hadn't noticed but on
some other input that comes apart and is
important and uh it's just really hard
to come up with something that properly
captures these though I think as the
field matures and becomes more
paradigmatic
this is a Direction I want to see so um
Seb ker he said how promising uh do you
think control vectors circuit breakers
and reang repen do you know what that is
so that stands for representation
engineering oh sorry is super yeah which
is like similar to some other techniques
like activation steering control
vectors
um and yeah so the key idea here is
basically you define a vector
in some way um like you give the model
the prompts I love you and the prompt I
hate you and you take the difference in
the residual streams and now you've got
a Kind of Love vector and then you add
this in and some change happens like it
outputs happy text and um circuit
breakers uh I believe is I mean I can't
really remember the details of the paper
but it's basically a way to make models
if they see something harmful um output
like kind of not output the harmful
thing using control vectors and some
kind of training uh but I don't really
remember the details so sorry um I think
so I'll mostly focus on activation
steering I think it's an interesting
Direction seems like an interesting
technique um I like the fact that this
kind of Fairly interpretable model
internaly technique can do cool things
um I mean I think Golden Gate Claude was
like like very similar to this technique
it happened to use an se but this
probably wasn't necessary and Golden
Gate Claude was like
hilarious um and that also seem to be
ways you can make this more useful by
like reducing hallucination rates or
increasing truthfulness is it going to
be enough to align help us align a like
human level system highly unclear it
might help stanos um says and this will
Echo wed saber now wed saber is a friend
of the show Joe he is a goofi person um
just just to just to put that um in
there and he said assuming that
explaining a decision means doing an
inference in Reverse so from the um
decision backtrack to find the steps
that led to it and given that neuron
networks are incapable of doing that
because there's no invertible
compositionality how can we truly
achieve
explainability so
that just isn't really how I think about
the problem like the way I think so
first off um you don't need to care
about the invertibility thing because
models are a stack of layers um each
layer is a much simpler function and you
can just analyze the activations after
each layer just a bunch of vectors so
like it's not it's not like I just have
the output and I have the input and I
need to somehow reverse engineer this
black box we have the weights we have
the activations we know every
mathematical detail of what is going on
in this system we just have no idea what
we just by default have no idea what any
of it meets and uh that is the problem
that we've been making progress on and
you know I think if you have like an
activation you got another activation
and you're like how did you go from A to
B you can make some inferences you can
look at the weights you can do causal
interventions
like my
I'm not holding myself to the standard
of proof of like we have inverted a
mathematical function I agree that
neural networks are not an invertible
function
um but I I'm holding myself to the
standard of
like I have good evidence that my story
is faithful to What's going at least
part of what's going on inside the model
and there's a bunch of ways you can try
to Define that which I'm sure we'll chat
about I must admit I'm genuinely
confused about this so I don't think
well I mean may maybe our brains are
compositional but it feels like our
brains maybe aren't that different to
neuron networks I mean of course they
they are different right because the
neurons can talk to each other almost
independently and you can get these
repeating cycles and so on whereas
neuron networks are left to right but if
we just leave that aside they are
similar in the sense that they're very
subsymbolic they're very diffused
they're very complex they're very in
Tangled yet when you look at our mind
and when you do psychology it it appears
as if we can do compositional reasoning
so so isn't isn't this this weird two
ways of looking at things you can look
at a neural network and you think oh
yeah it's it's blown up and it's
entangled and it's complex and different
bits of the neuron Network you know
apparently do the same type of
thing could could it ever be
compositional in in some sense it feels
more confusing to me when we're
discussing a single forward pass but
like if you give it a scratch pad to do
Chain of Thought on it can just
obviously do all of this it just writes
it down and then does inferences on
those and like Is this different from me
thinking verbally through a problem I'm
not convinced it's that different why do
you think that self- prompting Chain of
Thought rationals gives gives an uplift
to me it just seems really intuitive
that you get an improvement in
capabilities when you have Chain of
Thought like when I have paper to write
down my thoughts I am smarter I can do
harder problems um in terms of
computational complexity like the model
rather than going through every layer
once can transmit some information back
and it can also um kind of just
parallelize things more so it can have
one token where it's doing the what
should I um what should my plan be
calculation and another one where it's
like what the step one of the plan tell
me to do and another one where it's like
executing what step one tells you to do
and if you're trying to do these all in
the same token the model just it gets
very crowded there's like lots of
Concepts that interfere with each other
in the residual stream and this makes it
more error prone for the model and also
there is just some inherently sequential
nature to a lot of this computation the
rationale might be noisy it it might be
wondering what the average number of
carrots are eaten every day in bellarus
and problems that require backtracking
as well the previous trajectories might
become noisy and but might become
distractors
so it's not a definite win-win is it I
expect there are situations where it
will make the model worse for example
there was this delightful paper from
Miles Turpin on Unfaithful chain of
thoughts where you give the model a
bunch of like multiple choice A or B
questions um with a Chain of Thought
response and then an answer given like
10 of these as a f shop prompt and then
give them another question and uh what
they did is they made in the F shop
prompt the correct answer was a for all
of the questions and then in the final
one the correct answer was B and they
found that if they up prompted it to
give a Chain of Thought at the final
thing then it gave a Chain of
Thought for why the correct answer was a
and then said a while if they didn't ask
it for a Chain of Thought it gave b or
it was more likely to get B and um in
some ways this is kind of a boring
example because telling it to do Chain
of Thought probably triggered the
fotness more but it also I don't know I
find it wild that the model has an
internal representation of the answer is
a I should generate a spurious
explanation based on this fact like what
I would love someone to do a Chain of
Thought uh to do a mecher project on
what the hell is going on there but from
a model theory of Mind perspective does
that in any way denigrate the idea that
they have goals and intentionality if if
if it's such a complex token by token
token space I mean maybe maybe we don't
have goals and and intentionality but
does that in any way denigrate the kind
of the theory of Mind of of llms I
expect a goal mechanistically to
basically look like you have some
criteria you consider actions and you
judge actions according to this
criteria I expect this to be easier with
an internal scratch Pad but my general
intuition is that if a bad model can do
something with with an internal scratch
Pad a good model can do it without the
internal scratch pad and can do even
better stuff with a scratch Pad um
and like just algorithmically it doesn't
seem like evaluating actions should be
that
hard um I think this is this seems like
quite an interesting thing to study in
like more toy RL
settings where there's seems like decent
evidence that things engage in planning
like Eric Jenner had this delightful
paper looking at Leela zero that I think
found evidence it was like thinking at
least two moves ahead um do you have any
advice for fresh phds in mechin turp um
don't spend too much time reading papers
I think a standard academic approach to
getting into a new field is I will read
all of the literature and you can read
some of the literature um I have a
reading list we can probably Link in the
description but uh I think you should
also just spend a lot of time coding
like play with small models play with SP
or coders play with whatever you're
curious about there's pretty good
Tooling in tutorials nowadays and we'll
hopefully have some of those in the
description and um it's mech's just a
very empirical field you want to build
intuition and you can often get
something like a cool blog post out of
not that much work and playing
around um yeah uh I generally yeah um
get mentorship like ideally your
supervisor spends a bunch of time with
you find post talks in the lab to find
ways to spend time with them maybe focus
on being useful to them like helping
with their
projects um I'm a big fan of peir
programming as a way of learning
technical skills um if you can find
anyone who's better ml coding than you
or in Tu coding than you and cons spend
some time pairing with them um maybe
like as part of helping them with some
project or something I think that can be
a great way to learn and I think lots of
people in Academia somewhat neglect
technical skills
uh some are excellent so no shade there
um and just the more the better you are
at coding faster you are at experiments
and the more research you will do uh be
skeptical um it's really easy to come up
with a beautiful pretty idea and the
idea is actually complete you
get attached to it so you don't notice
um an adviser or even just peers who can
red team your ideas are really helpful
you should also just spend a lot of time
doing that yourself like how could this
be false what what are the ways this
could not be true what are experiments
that would distinguish true from
false uh yeah yeah well Philip also said
how on Earth and that's how Full Stop on
Full Stop Earth full stop did you manage
to get to where you are in just 25 years
of age share your journey of a hero
please well so the uh relevant life
story is I finished a pure maths degree
at Cambridge in 2020 I was going to do a
masters but why on Earth would you do a
masters during a pandemic uh so I spent
a year doing internships at various
safety Labs like deep mind and the
future man Institute and the center of
human compatible AI at Berkeley um I
wasn't super thrilled about any of the
specific agendas I'd worked on but was
like I believe that AI safety is a big
deal and I should work on it and then I
massively lucked out and got job offer
to work with Chris Ola at anthropic um
spent some time there um ended up
leaving for health reasons and then
spent a while independent and then um
ended up a de mind uh in terms of
like how this happened I'm kind of
confused to be honest I think the key
ingredients
were getting into a field early that was
growing really fast and and I'd like to
think I've done some stuff to help that
growth which is also just useful to me
um I'm really good at mentoring people
and I just find this really fun um sorry
I'm really good in the sense of I can
have people produce cool papers without
spending that much time helping them and
I also just find this a very fun thing
to do which means that I both have my
name on a lot of papers and also I am a
better researcher for it um and yeah
know luck and having great mentors like
Chris early on um and finding a field
that was just like a very natural fit to
my taste and talents I do think it is
unfortunately just harder to get into
mecup now than it was three years ago
sorry the field has
grown more people are interested it's
terrible Neil one day we will get Chris
on the show I'd be so excited um yeah
the man is a an absolute living Legend
he's so great yeah um I think he's only
ever done one podcast on 80,000 hours
yeah but it's so good people should
definitely listen to it yes um and yeah
um and as contexts for people who uh
have lives uh Twitter has been very
interested in the fact that I'm 25
recently it's very confusing oh what
what are you seeing um let's see
so someone made a meme where they said
today is Neil nandz 19 birthday please
comment happy birthday Neil if you
believe that reverse engineering neural
networks is important for reducing AI
risk um and he also wrote this hilarious
post um that was completely uh
about Neil nander the 18-year-old
Prodigy who is reshaping AI inside
Google who invented the World of
Warcraft source code at 18 months of
age um
and I'm very confused
yeah the the Twitter meme sphere is is a
very strange place we have now produced
the most comprehensive YouTube video on
SES what is an SAE SES sparse Auto
encoders are a tool to let you look
inside a model as it runs and see what
it's thinking about I think this is one
of our most exciting tools to date for
interpreting but also most people don't
really get how to think about them and I
have a lot of hot takes and Tim and I
spent several hours discussing them so
if that sounds like your idea of a good
time maybe you should check this out
Neil why is Mech turb relevant to AI
safety yeah
so yeah so I think there's this
interesting question of motivation where
in my opinion MEC tub it's just like a
scientifically fascinating field we just
don't get how these systems work
internally but we can make progress and
there is all of these beautiful puzzles
to discover and this is a part of way I
work in the field but a large part of
way I work in the field is is I'm pretty
concerned about existential risk from Ai
and I think this is a promising way to
make progress on it and I think kind a
very high level I just think that it's
much harder to make a system safe or
evaluate how safe it is if you don't
know what's going on inside and I think
that any progress we can make on
deciphering its internals is great on
that front um a particularly concerning
capability that I think may arise is
deception um systems that are capable of
deceiving us and I don't know trying to
evade our evaluations if it can evade
your evaluations every other evaluation
is kind of
useless and it may be quite hard to
figure this out by just like interacting
with a model because if a model is smart
enough it kind of knows that you're
doing this but I think that it's
possible that if we can learn to like
look inside deeply enough we might be
able to detect when this is happening
and potentially form a deeper
understanding of it and potentially stop
this
I think another angle would just
be there's a lot of confusions around is
alignment a big deal do these risks
matter I think this is quite bad I think
it would be so much better if there was
an good empirical grounding here that we
could agree on and empirical questions
to study but there's lots of things like
can these systems plan do they have
goals that there's no real consensus
about or rather will this happen and I
think that mechanistic inability is
potentially a promising tool to answer
questions like that because they're
fundamentally a question about cognition
I don't think Mech is the only tool for
answering these questions but I believe
it can be a promising one is it possible
that there would be a level of
sophistication in training where the
model would deliberately kind of almost
encrypt itself to stop you from putting
probes in and understanding what it's
doing like maybe my intuition is that
that seems significantly harder than
learning to lie to people because it
just has so many examples of that in the
training data while um yeah it has so
many examples of that in the training
data while um how to warp your internals
to avoid interpretability tools it kind
of doesn't and it also doesn't get
feedback on that it's like I don't
really know how to warp my brain so
aliens can't like read my mind um and
like arguably there's like patterns of
cognition that should be easier or
harder to get that it could do like not
thinking about its goals very often and
like maybe that would happen but I don't
know I predict there will be a window
where this stuff is useful before we get
models that are like so Galaxy brained
it's hard to know what tools will work
on them and thank you so much for coming
back on right happy to be back um my
girlfriend was actually very excited
when she heard I was coming back on
because she uh says the previous episode
was so relaxing to listen to she uses it
to help her fall asleep I know I did say
last time that Neil's doit tones will
melt the stress away and I wasn't lying
um but also there there's there's the
other side of that which is you're an
incredible researcher in Mech and turb
and I think a good lead for this is you
know in your own words what is mechan
turb sure so yeah mechanistic
interruptibility
or mechant up for short is
a subfield of the general study of
interpreting AI neural netbooks whatever
you want to call them and it's kind of
based on this hypothesis
that neural networks when trained have
learned some kind of human
comprehensible algorithm but they don't
have any incentive to make this legible
to humans but there is some under
structure and algorithm there because
like that is what does the computation
that produces the output and by careful
and principled reverse engineering and
Science and skepticism we can decipher
parts of those hidden
mechanisms and ambitiously this would
look like understanding the entire
algorithm but even just understanding
what the intermediate variables are or
like parts of that structure feel like
meaningful and exciting prr progress to
me I mean some people argue that I mean
this this is a mammoth task you know
some might argue that it's um infeasible
or or maybe even unnecessary I mean what
what would you say to those
people well unfeasible and unnecessary
are two very different things my
response to uh infeasible
is I think it's reasonable to say fully
reverse engineering like gemini or gbd4
to source code is not very
realistic um but I think that there are
a lot of important useful things we can
understand about them and partial
progress
and um personally a large part of the
reason I'm in this field is I'm pretty
concerned about existential risks from
Ai and I think it's important that we
have stronger methods of studying them
and ensuring safety and I think that
even partial progress and mechanistic
interruptibility can help us get there
along with just I don't know being
scientifically fascinating and beautiful
um like kind of weirds me out the people
are just okay a bunch of software
engineering tasks that seem really
complex and difficult and we have no
idea how to make a system that does that
in terms of Designing it we don't
understand what is inside these things
they are just things that work and we've
made progress but I still feel pretty
confused on a deeper level about what
happens inside them uh regarding
unnecessary what does necessary mean
like I think it is important to do
research to help us make these systems
safer I think that one such pathway is
interpretability I'm not arguing that if
we don't do any interpretability
research we will never produce Safe
Systems that would be unreasonable um
I'm not arguing that if we don't pursue
specifically mechanistic
interruptibility but pursue some other
philosophies of interruptibility we will
never make Safe Systems I'm just saying
I think this is a promising approach I
think it's helpful I think it is helping
us find real interesting scientific
truths about these models and I want to
find them so if those are the goals of
mecher what are these sub goes yeah
so I'd maybe divide the field into three
conceptual areas there's basic science
so what on Earth is going on inside
models there's automation taking the
techniques and tools for understanding
systems and scaling them up maybe
getting an llm to do them maybe just
having an algorithm that can do them and
um kind of practical applications doing
things in the real world well the goal
is necessarily to to advance our
understanding of interruptibility it's
to do something useful and get feedback
from reality in the
process and um historically most of the
field has focused on basic science uh I
think this was correct um and I think we
made a lot of progress I think going
forward I'm excited about exploring
other areas more and building on all
this progress um within basic science um
a decomposition that I think is often
useful
is if you think of a model as like a
computer program in some sense there's
um two tasks what are the variables and
like what is their state at each line of
the program and like what are the lines
of the program what is the code what is
the algorithm uh in mechant jargon you
call the variables features and their
values are like how much should they
light up and you call the algorithms or
like of code circuits or algorithms and
generally we study a models activations
the things it computes on the fly when
you give it an input because we believe
those represent
variables um and like what the model is
thinking about in some sense if you'll
forgive the
anthropomorphism and um on the other
hand um circuits live in the parameters
of the model um they're like a thing
learned during training that doesn't
depend on the input but is like
responding to the input and rooting it
down different channels and things like
that I mean you've already answered this
question in in your last answer but just
to hit it home a little bit you know
before we had things like lime and shap
uh we might be doing um blackbox model
interpretability you know with a
surrogate model or something like that
now you seem to be talking about what
algorithms are these models running I
mean how do you contrast those different
views yeah that's not quite my field so
while I'll say things about it they
could be wrong uh I'm sure there's like
tons of papers that have all kinds of
variants of these things so if I express
any critique I'm sure someone can send
me a paper that addresses that or
something I don't know but on a high
level so my understanding of lime is
it's like let's take a local
approximation to a model that's linear
or something
similar um I think this is a pretty
reasonable thing to do but I think the
question is like how are you applying
the technique so okay let's zoom out a
bit it kind of feels like more of a
philosophical difference um historically
lots of interpretability has just kind
of looked at the inputs and the outputs
and just hasn't looked at the internals
or they've used it to compute gradients
or something and Mech and turp is trying
to go beyond This and like look at
things like the activations of the model
which again people have done before but
it's also trying to understand like the
causal connections and the parameters
and how this all connects up and just
like pushing for a more ambitious vision
and techniques like liy and shap may be
useful may not be useful um but they're
more like a tool you would apply in some
situation like for example um
attribution patching um we'll probably
discuss more later but very roughly it
approximates a causal intervention using
gradients um I have a blog post
introducing it and also there are a
bunch of other papers that do similar
things from previous years CU everyone
always reinvents everything um
and um this is like quite similar to
Lime in spirit I think lime has a
different approach to like finding the
linear approximation but gradients are
really cheap and empirically they seem
to work basically fine but with with
attribution patching you often apply it
to the model activ
um rather than model
inputs um I believe um shap is kind of
similar it's like an attribution method
we've got a bunch of inputs or something
and you're like what is the shle value
by which it affects the output and again
this is like a reasonable tool you could
apply in the model to like neuron
activations uh I have not actually seen
it applyed um I'd be kind of curious to
see what the results are um
ideally you have a more rigorous way of
doing the thing and then you try using a
cheaper tool and you see if it's a good
approximation though a kind of constant
Challenge and interpretability is what
even is the ground truth um okay so
going back to the question
um I think in some ways mechanistic
interruptibility is kind of continuous
with what's come before like ultimately
our goal is to take a model and try to
understand what happens inside there are
cultural differences like different
people with different ideas like
originally it was in vision and then
moved to language models and there are
some people who doing language model and
Tu forever um but it's kind of more of a
perspective and philosophical difference
than necessarily
like techniques or it's like it's not
just introducing a new technique is what
I'm trying to say yeah I mean I was
speaking with um Andrew ilas from MIT
and and he's um working on data modeling
which I guess is is not too dissimilar
to something like Li linman chat where
you actually model you know how changes
in in in the data set and and affect the
overall predictive architecture but
there's always this thing that it
doesn't scale very well right and I I
guess the question would be you know
what what would it mean to reverse
engineer a language model so there's
kind of two things that I think would
both be reasonable senses of this word
the first would be you just fully
understand the the model you have a
human comprehensible algorithm that acts
basically exactly like the model um and
you've like reverse engineer parameters
this would be an ungodly massive program
though an interesting thing is that I
expect it to be like really wide rather
than deep like I expect that to be lots
of different programs that kind of
happen in parallel and then most of them
are discarded and like a few are used
such that there's like some hope there
uh but this is incredibly ambitious I
don't really think this is realistic uh
unless we automate it enough we can just
have another llm do this um and the
scent which I think is more realistic is
given any input I will be able to give
you a story of the computation done in
the model to produce an
output and um to me this kind of thing
is a lot more amable to like you just
look at the activations at each step and
you do causal interventions to
understand how they depend on things or
maybe you even look at the weights and
just it's a map mod we know how M Ms
work uh matrix
multiplication
and yeah maybe yeah maybe this is also
just a good thing to add to my previous
answer of like differences with other
kinds of
interpretability like to me the neural
network is some kind of substrate that
represents an algorithm in its weights
somehow the activations each step in
this algorithm and I want to understand
as atomic a step as I can using tools
like activation patching that I think
we'll discuss more later um I don't just
want to treat it as a black box going
from like the start to the end I want to
get like as Zoomed In A View as I
can and um ideally piece this together
into like a this happens this this
happens then this happens then this
happens and you get from the start to
the
end yeah you said something which was
along the lines of I say similar which
is that um you know neuron networks are
wide but shallow and you know symbolic
methods are you know um are very deep
you know very narrow and um there's
always this notion that that they're
quite blown up circuits and then there's
you know is is mechin turp kind of
identifying an algorithm in an input
sensitive way because if you think about
it surely it must be a superposition of
algorithms because the entire model is
this gnarly mess of all of these things
mixed together and we're I guess you're
you're suggesting that given given an
input example we look at the behavior of
the model and we try and infer what the
algorithm was that took
place yeah so yeah so there's maybe
three lines of research that I think are
worth calling out here so there's what
people might think of when they think of
interpretability um like mechant like
reverse engineering a thing from its
weights like fully mathematically
understanding it and like we I know I
did this for a tiny model doing modular
Edition people have done this for other
tiny
systems we made a bit of progress on
language models but like I basically
think no one has satisfyingly done this
for language models um and it definitely
does not seem to scale so I'm a bit
pessimistic on this though I hold out
some hope we can rescue it at some point
there's the causal intervention school
of thought that's like I think of the
model as a computational graph with
nodes like each attention head is a node
each MLP layer is a node or maybe each
neuron is a node and I do causal
interventions where I change one node
from one input to another input and I
see how that changes Downstream ones and
how that change Cascades to the output
and yes this is this is fundamentally
very input
sensitive um often you'll do this on
pretty narrow distribution um like
sentences of the form uh person a and
person B went to the location person a
gave a object to M and it needs to come
up with person B um and like you know
that's a very narrow context in some
sense um the relevant paper there is
interpretability in the Wild by Uh Kevin
Wang yeah yeah and so this is very input
dependent partially it's input dependent
because model components kind of do many
things
and
um they're like the jargon is
polysemantic uh on a narrow distribution
only one of these might turn up but uh
you can't make a general clim um and
then the third family is work with
sparse Auto encoders so these are
basically a technique to take
activations um that are full of all
kinds of stuff and mean lots of
different things and to decompose it to
a larger and sparer and importantly
monos semantic representation like
you've got a bunch of latents um
sometimes called features but I don't
really like that word uh which we might
get into later and each of these latents
hopefully corresponds to some
concept and by it itself this isn't
giving you an algorithm it's just
studying activation it's just telling
you what variables are here at the step
but I think there's a lot of exciting
work to be done in um converting that
doing circuit finding with sees as your
notes and if they truly are monos
semantic then this feels like it might
get you something that I would consider
more input independent
though it's kind of complicated and
there's various improvements to this
which we can discuss later but you could
maybe think of the recent history of
meup as people really wanted input
independent
algorithms uh and that was really hard
and we had lots of success with like
input dependent stuff which still seems
good enough to be useful and a lot of
people are working on that and we're
hoping we can get less and less
dependent enough to be useful or that
there's a lot of tasks where you don't
need to be input independent you can
just study that
domain and I think both of these are
likely are reasonbly likely to work
out yeah so uh you mentioned
polysemantic and and monos semantic and
maybe we should just quickly um Define
that yeah so the idea is you can think
of
any number in a model as being a
detector of the input it like is big on
some inputs and it's not on others uh
this could be like a
neuron um or just any other element of
an activation or even a projection onto
some
direction and um we call this monos
semantic if there is a shared property
of all of the inputs that significantly
cause it to light up and we call it
polysemantic if there do not does not
seem to be a shared property or there's
like several clusters or maybe it's a
complete mess and this is always hard
because this is an inherently subjective
definition like what does it mean to
have a shared property and my typical
answer is just like I don't know man
it's normally pretty obvious in
practice um and there are some edge
cases where you might mislabel a thing
because you missed the pattern but like
it seems basically
fine uh but some philosophers in the
audience may be scrimming at me right
now yeah isn't it one of the benefits I
guess of networks is that I mean you can
call it sub symbolic that the that the
knowledge is entangled and distributed
over many shared neurons and I guess to
interpret them we need to disentangle
them but maybe the brain works in a
similar way as well I mean what what
you're describing here is that there's
this kind of set of circuits that get
activated in some kind of task specific
way and then we can disentangle the
representations such that they have an
intelligible single meaning and then we
can use that to reason about the circuit
or the program that ran in in in the
network I think it is an important fact
about neural networks that they can kind
of do a bunch of things at the same time
with um the same component like an
attention head can do different things
in different
contexts uh by at the same time what I
mean is the same component on different
inputs can do different useful things
not on one input it can do three things
at once that's much harder um and so the
Jaron for this is um super position so
maybe let's zoom out a bit and discuss
some empirical observations about neural
networks before I try to explain what we
think is going on so empirically neuron
components are often polysemantic they
respond neurons were respond to many
different things empirically concepts
are often distributed they often seem to
be represented as um often linear
directions in activation space like if
you take a linear combination of neurons
that lights up when the concept is there
and doesn't light up much when it's not
there and
um what we think is going on is um
superp position the idea that there are
more Concepts than dimensions
each one is linearly represented with
its own Direction I I'll justify the
linearity Assumption more in a bit um
and these are all kind of adding
together in a way where you can lossy
extract one you can project onto its
direction there'll be other things with
nonzero dot products which will
interfere but the interference is kind
of tolerable like it introduces a bit of
error but not much that it wasn't worth
having that feature there at all and uh
my guess is that this is actually a
pretty important part of why neural
networks are more efficient uh or just
so effective and interestingly um
there's a sense in which Transformers or
like residual networks in general are a
lot better suited for superp position
because there's lots of addition and
reading from this shared residual stream
uh in a way that lets it represent a ton
of things in linear superp position
while if you have nonlinearities on
there it gets a lot more
messy and so yeah this is a mechanistic
hypothesis that tries to explain the
polysemantic and distributed
things um there's also a bunch of
different senses of
superposition you can have
representational
superposition where um this is when
you've got like some activation that's
representing stuff and it just it's just
squashed in more than it has dimensions
for example the embedding Matrix of gpd2
it's 50,000 by 768 for the smallest one
so this means that it's fit 50,000
tokens into a tiny space but it's still
clearly kind of knows the difference
between the
tokens there's computational
superposition um and uh this is like
when you have um something like an MLP
layer like a matrix multiplication and a
nonlinearity and it computes more new
features than it has neurons one example
where I'm pretty confident this is going
on is facts like models know a ton of
facts but and I'm just would be shocked
if they knew if they only knew as many
facts as they had
neurons um H and this seems to be
represented in some kind of
computational superposition
um actually have a c investigation uh
called fact finding where we tried and
uh basically failed to decipher how
exactly the computational superposition
worked mechanistically here but I'm
pretty confident going on and the final
kind would be kind of circuit or weight
superposition we have a bunch of
different algorithms in the parameter
matrices added together it's like less
obvious to me this is a big deal because
there's just like N squared parameters
for every like n dimensional activation
which is just more space but it does
seem to be there a bit so like we find
um when we say take like use SP water
encoders or a variant called transcoders
and multiply weight matrices together
where we think the start is
interpretable the end is interpretable
but they're much bigger you often get
connections between things that seem
semantically totally unrelated that like
don't co-occur in practice like this
thing never lights up when this thing
also lights up and we think this is just
it's just representing stuff and you get
interference and it's really annoying
but it's a
thing you know like in the the physical
world we live in you can do analysis at
multiple scales so you can look at the
mitochondria in your body or you can
think of yourself as an agent or we can
think of the ecosystem and so on and I
guess it's a similar thing with network
analysis that even the inductive prior
of a transformer you're talking about
residual streams and adding things
together and so on and and this is a
mode of analysis and I think it is the
case that you could take any Transformer
and you could represent it with a blown
up MLP so there is an MLP that'll do the
same thing and then technically your
abstraction or analysis that you use for
the Transformer variant would still work
for the MLP but the MLP is a completely
different space but it goes back to what
we were saying before that neuron
networks are very wide and and shallow
and that leads to this kind of
confection of little unintelligible
circuits and could there exist some much
more abstract decomposition of a neuron
Network that would be far more kind of
explainable you know it'll be a better
theory of what's going on in the neural
network so essentially could there be a
different architecture that's a lot more
inherently interpretable maybe an
architecture or maybe just maybe a type
of analysis that would better
explain I guess humans need to have
quite macroscopic priers to understand
things you know when we get to the real
low level it seems increasingly
unintelligible yeah
so I am excited about research that is
trying to find these like more
macroscopic higher level things um one
there was there was some interesting
work with image models there was this
paper called Branch specialization that
found that um so a fun fact about the
original Alex net is that it has kind of
two separate branches because they just
had two gpus with bad interconnect and
they don't really intersect very often
and these kind of ended up specializing
can't quite remember to what like I
think one was doing colors and one was
doing shapes or something and they found
that another image model that wasn't
trained like that still kind of had
neurons clustering into like parallel
branches
and if we could find this kind of
macroscopic structure in llm that would
be super cool um the problem with this
is that superposition is most effective
with things that don't
co-occur because if they h both
happening at once the interference gets
way worse uh than if one happens and the
other doesn't happen and you need to
just tell that the first thing happened
and the second thing didn't happen this
means that if you had like two modules
like the biology G neurons and
the I don't know
um generating news articles about sports
neurons or something um these would
actually be great to do in super
position so like the structure is going
to be super elgible to us and one hope
is that with sparse Auto encoders
helping us entangle these things we can
do more to find this kind of high level
structure and that's a direction I think
would be pretty cool for people to
investigate it's not something I've seen
that much work on
yet I mean it just blows my mind have
you seen that there's a type of um you
can decorate tables and you you put
water on them and you put two electrodes
either side and you just see this kind
of you know um electric patterning that
Burns Burns a kind of tree structure
looks like a lightning bolt like a tree
structure and you know we're done it's
incredible but the thing we're dancing
around here is that you know neuron
Network are mostly grown and the growth
process can be influenced through
inductive priers um of course but when
you really dig into it you just see it
it's I'm really stretching the analogy
here but a bit like Evolution right just
these weird specializations and local
dynamics that form during the training
process and it's it's kind of like a a
living process in some ways yeah so I
think I think evolution is actually a
really good analogy so
there's a sense in which biological
organisms are the subjects of a bill of
like a billion year long optimization
process you have Evolution optimizing
something like inclusive reproductive
Fitness and you kind of randomly move
around in DNA space and you end up with
uh the human brain like what and when we
look inside biological systems like they
often make sense there's like structure
and there's organs and like we've
learned so much about our bodies and
then there's so much that we're still
deeply confused about um and also just
lots of like random dumb stuff like the
lenial nerve that like goes from here
upwards and then down to the place it
ends which is particularly funny in
drafts um and yeah just like a bunch of
stuff where it's like oh man if I was
designing this I would not have done
that and I'd bet there's all kinds of
stuff like that inside neural networks I
mean you'll often observe something like
this is kind of weird and confusing I
don't know what to do with it I'm going
to move on it probably doesn't matter
that much uh or there's kind of weird
phenomena where I don't see any real
reason to have it like self-repair if
you delete a layer or attention head
often other layers will change their
behavior to compensate to like recover
the performance and and like what um
this happens in like this makes sense if
you're training it with Dropout or
something like a thing that does that
but like why does it happen in models
that don't have that yeah and and that's
quite biometic as well because I mean
the brain has the same thing if you have
a stroke different parts of the brain
can kind of take over that function and
a lot of self-organization and
self-repair is because you have these
little atoms that can be repurposed you
know to do completely different things
and one thing that I often think about
is you know imagine we can't do
counterfactual analysis with the real
world wouldn't it be great if we could
but imagine if we could just run the
evolution on planet Earth again you know
we might we might not evolve or or maybe
there's something Evolution
evolutionarily fit around having you
know bipedal Walkers with big brains and
and so on and I guess you must see this
with neural networks because you are
seeing the same architecture trained
different times for longer for shorter
on different types of data and I guess
are you seeing the same kind of motifs
coming up again and again so yeah so
there's this general idea of the
universality hypothesis that circuits
are Universal and will just recur in
models trained on the same thing and
like the strong version of this is
empirically false like there are
differences between models um like
um but it seems like a weaker form might
be true like there's some things that
recur or there's like some small set of
things and some sample of those appear
um like I supervised this great paper
from belal chai called a toy model of
universality that
um uh well actually an important part of
that paper turned out to be a little bit
wrong and was corrected by a great
follow-up work from dashel Stander uh
but the universality part still
which was basically we were studying
algorithmic models and um in these
algorithmic models um we had like a
couple of different algorithms they
could use like five or something and
every time you trained it it just gets a
kind of seemingly random sample of those
five and no one studied this enough on
language models to really know if a
similar thing happens like my bet is it
probably does and there's kind of
some recurring motifs so for example
induction heads are like a super simple
kind of circuit that basically let you
take something like I don't know if a
model sees uh the word Tim in a sentence
it's kind of not super clear what comes
next no offense uh but if it sees Tim
scarf previously and it sees Tim again
it knows that scarf is likely to come
next and um induction is just like a
simple two-head circuit implement
this
and um these seem to occur in basically
every model I've ever looked up um in
the relevant um paper where we studied
this led by Katherine Olen um we looked
at a bunch of internal anthropic
language models up to 13 billion
parameters and it was there in all of
them I've got a uh blog potish thing
with looking in like 40 open source
models and it's also in it's like it's
sometimes
true um I think sparse Al inod a
universality is like a pretty
interesting Direction like how do the
features compare between models and
training runs and data sets like
comparing code models to like normal
language models could be
cool yeah I mean this is another thing I
I think about a lot is this concept of
um you know almost platonic knowledge
that the Universe might be generated
with some kind of computer program and
we
somehow acquire that knowledge you know
which does lead to the thought
experiment that if there were another
civilization maybe I mean the world
Works a certain way and I'm pragmatic
about it I think a lot of knowledge is
constructed and social and relativistic
and so on but it feels like there there
are there are some you know guidelines
around How the Universe works but on on
the knowledge thing as well you were
just talking about facts so right now
neural networks like language models
they are incredibly good at at
memorizing knowledge but what they don't
have is that degree of certainty they
don't have that epistemic factfulness
and people have done things like
retrieval augmented generation and so on
but do you do you think that's an in
principle problem or or do you think
potentially in the future that there
could be more certainty about what
knowledge the the model has
so I would actually say that I think
we're already seeing some meaningful
progress here
so like so there's I think there's two
problems here that are important to
distinguish there's the model knows
something but that is a false fact
versus the model doesn't know anything
so it falls back on the general language
model PRI and just babbles I personally
consider the first one like out of scope
um and I consider the second one to be
what we mean by
hallucination um mechanistically I don't
expect any difference between the first
two but in the same way that like I
don't know I have lots of false beliefs
I'm sure uh I you know um smarter models
will have fewer false beliefs but like I
don't think that's going to
fundamentally go away um and I think
that I mean just mechanistically um
producing a bunch of stuff is like
different um like there was this great
nature paper recently from Seb FAA on
entropy which is basically just you
generate a bunch of things you group the
ones that mean the same thing together
and then you take the entropy of that
distribution and this turns out to be a
pretty good sense of how uncertain the
model is and there was a fun followup up
paper using it to train a probe which
seemed to do a decent job of predicting
when models are
hallucinating and I'm
currently um supervising a project from
Javier fando and Oscar oaso look trying
to understand Mech tyup of hallucination
in more detail and they found this super
cool thing in work we haven't released
yet uh called like ENT like an entity
detection circuit so like the model has
uh there are sparse Al Cod of features
for I recognize this movie name and for
I don't recognize this movie name and
these are like causally relevant to
whether the model will say I don't know
or just Babble um um or like tell true
facts or or Babble for like the ones it
does know about oh sorry I think it's if
it knows about the movie messing with
his feature makes it say I'm sorry I
can't help you if it doesn't know it
will normally say I can't help you but
this can get rid of that and it will
instead battle and so like there seem to
be mechanisms here like I think there's
a lot of progress that can be
made that that does really interest me I
mean I was um interviewing AK Khan and
and uh yeah he's a really cool guy he's
a really cool guy um so his debate paper
was um you know one of the papers of the
year at icml and essentially it was
about having a a kind of a pool of
Agents you know almost like a judge and
a couple of critics and getting them to
argue it out over 10 iterations to get
closer to the truth right and we see now
with reasoning I mean to me reasoning is
about closing a knowledge Gap so I I I
don't know something and the trick is is
the model I need to start prompting
myself to reason now because I know that
I don't know and that's the thing isn't
it do you think in principle that a
model might know that it doesn't
know um I think yes I mean I think the
work I just described is an example of
such a thing it is distinguishing
between entities that knows and entities
that doesn't know at least in some
narrow domains like
movies um we in the fact finding project
I mentioned we also found that like
when
you um give the model a fake athletes
name it kind of does seem to act
differently than if you give it a known
athletes name um was actually kind of
interesting we found that the early MLP
layers would still generate a kind of
sport
Direction um but that the attribute
extracting attention heads which like
look at the athletes name where the
factors look up and move it to the end
wouldn't look for the unknown names even
though the MLPs were still producing
like hallucinated spots this is kind of
cool I wonder what the role of you know
reasoning and thinking out loud here is
because you know again we can debate
whether or not the models internally do
it or whether it's a form of
externalization so you let the model
think in a in a system two-way and
that's almost how it
reconciles you know what it what it
knows internally into some kind of
calculus that it can kind of you know
reason with and and perform rationales
and so on so I guess from a mechan point
of view how how does it affect the
process letting the model perform some
rationale before you then analyze it um
as in you ask it for a fact it gives an
answer and then it does a bunch of
inpection on the answer well yeah I mean
my my intuition is that perhaps the the
model is quite gnarly and if you let the
model Ponder and consolidate and think
about what it knows will it be able to
better know what it knows and what it
doesn't know inference time compute is
helpful feels like a pretty
uncontroversial statement
nowadays um I don't know if I've got a
more interesting answer than that like
uh it will have more chances to notice
something going wrong and I think I
don't quite know what the circuit for is
this fact True Looks like but it
wouldn't surprise me if it's like at
least a bit different from the recall
effect circuit and that there are some
facts where it can identify that it's
false without uh actually knowing the
answer or
something um like it just kind of
babbles something like I've got to say
something but then it looks back and
it's like that's probably sus or like
I'm not really sure um I don't know if
anyone's really looked into this so I'm
purely speculating right now but yeah I
mean the only intuition I have is it it
feels like a lot of the reasoning we do
is actually a form of patent tool use I
mean language is a tool much like the
you know we were saying earlier the mtic
equivalent of a physical tool you know
like scissors and we learn how to reason
you know we learn it at school so we
learn to apply all of these different
rationals and applying these rationals
in the token space kind of helps us make
sense of what we know deep down in in
our minds and maybe there's an analogy
there to Which models I don't know
possibly I mean I think there's various
kinds of deduction you can do on certain
facts um like ah I claimed this but like
maybe I'll recall a bunch of facts about
the claim that I just made and like see
if any of them have any bearing on this
and um a general fact about language
models is just you kind of only get one
pass through unless you're doing some
kind of Chain of Thought or something
so um this means that you just can't do
that much computation in a single
forward pass and you can do much more if
you just pass on a bit of information
that you got with the first bit of
processing then the second bit then the
third bit Etc or possibly that there
might not even need to be communication
between the different bits of processing
it's just easier for them to happen on
separate
tokens so it doesn't just interfere with
itself a bunch in the same place
so for folks at home who want to you
know get into meub I mean this this is
it's still quite a nent field I mean
obviously it's getting much more mature
now but what could Folks at home do to
get started yeah so I think there the
field is growing but there's still a lot
of core problems to work on and stuff to
be done
um I yeah so I think in terms of reading
papers uh I have a reading list that we
can put in the description also you just
Google Neil Nando me reading list I'm
you'll find it um I also have a guide to
getting started in the field though it's
a bit outdated
unfortunately um and the arena has like
a fantastic set of coding
tutorials um that we should link to as
well and I basically
recommend going through um maybe
skimming like a paper or two to just see
how interested you are in it doing the
arena tutorials to like get your hands
dirty and understand the tooling and
then do a mix of reading papers and
doing experiments kind of ripping off of
those
papers um I also think it's a lot easier
if you have
collaborators um or just people to chat
to uh the Ala Discord the mechanistic
interruptibility Discord and the open
source mechanistic interruptibility
Slack are all great places
there
and um yeah I encourage people to have
to just I don't know um do a small
project write a blog post about it put
it on your website or somewhere like L
wrong or somewhere else um put yourself
out there a bit try to get feedback from
people but more importantly just like
get your hands dirty and actually try
things and follow your curiosity rather
than just reading 50 papers so what is
activation patching in contrast pairs
yeah so
the the goal that the activation
patching technique is trying to solve is
attributing some Model
Behavior um in particular some kind of
numerical output like the log prob of
the correct answer to some model
component on some data distribution like
how important is this attention head or
this SE latent or this neuron for the
model answering something and
so there's like a lot of ways you can do
this um and generally you want to be
causally intervening on this component
to change its value um but if you got a
chunky component like a head or a layer
who's got quite a big output that's like
a vector in a high dimensional space
it's kind of not clear what you should
replace this with so like the default
thing would just be replace it with
zeros like this is kind of what Dropout
does this the are of out or like
ablation and um the problem is this will
sometimes just break a model because
this is just a very off distribution so
there'll be components that aren't
relevant to a task but are like just
there it's like a bias term or something
uh so for example in tbd2 small MLP Z is
basically always used to like enhance
the tokens like it doesn't seem to do
much but like if you delete it
everything breaks cuz it's like the
output is
added to the embedding to be like the
effective embedding that everything else
sees and so the next level up would be
mean ablation um you just replace it
with the mean over some Corpus and like
I think this is a lot more reasonable um
but an even more interesting thing you
can do is um activation patching where
you have uh two inputs that are like
similar but different in some key detail
for example the the Eiffel Tower is in
the city of and the Coliseum is in the
city of these will have a different
answer Paris and Rome you have some
Metric say the difference between the
log prob of Paris and Rome this is quite
nice because that's equal to the logit
difference of Paris and Rome because
maths
and um then you swap some activation
value from Paris to uh from the Paris
prompt to the Rome prompt um and you see
it's what does it make the other thing
say Paris less the first one is called
Den noising because you can think of the
Rome input as like the bad noisy input
and you're like d noising one component
you're replacing it with
um yeah you're like replacing it um with
the true thing um and going from um Rome
into Paris and seeing if it damages
Paris is called noising cuz it's like
you're kind of messing up one component
and you're seeing if it's
important um and the really nice thing
is that you can have kind of your choice
of Baseline So like um a pair of prompts
like this is called a contrast pair
where you want them as close as possible
apart from some key detail because this
means that things like the I'm doing
factual recall right now feature is
still there the I want a city feature is
still there but the like which city is
it in bit it is not still there and you
can also um but you can have different
prompts that have different changes for
example the Eiffel Tower is in the
country of now you're analyzing the like
relationship part of the factual recoil
and um activation patching lets you have
this really fine grained tool for
analyzing different kinds of
information um the den noising versus
noising is actually quite an important
subtlety so you can think of noising as
being like was this bit necessary for
the computation or like at least was it
useful if I get rid of it does anything
get damaged um you can think of d
noising as like was the thing sufficient
like does the output of this node from
then on cause the output we want um this
does not mean that it's the only
relevant node but it means it's kind of
enough of an information bottleneck to
contain the key info like if you have
kind of three steps in the process
denoising any one of those steps should
be
enough um and you kind of want these in
different
situations um so for example if you
think you found a circuit this like
three-stage thing with like a few nodes
at each layer um you can test this by
either noising everything not in the
circuit and seeing how badly it breaks
which is a way to test the whole circuit
at once or you can denoise kind of each
slice of of the circuit at a time and
see if that is sufficient it doesn't
make sense to Pat de to like denoise two
slices because the the second slice kind
of doesn't care what the First Slice is
doing because you're just patching in
its
values um and yeah I think activation
patching is like a really cool and
Powerful technique um it masquerades
under many many names like coal
mediation analysis um in the paperwork
first saw it from Jesse Vig in 2020 or
interchange interventions uh from
attakus Giger or resample oblations
which is mostly just used for the
noising pop or causal tracing which is
used in the Rome paper and there just so
many names it's really annoying uh I
personally like activation patching and
just try to converge on a name um for
people who want to learn more about it I
wrote this kind of uh tutorial piece
with stepan heerim called like how to
use an interpret activation patching
it's not really a research paper but
it's just like an intuition dump of how
to think about this
technique um and I think it's just like
a pretty powerful tool that is useful in
a bunch of settings for trying to
understand model components um oh and
the final thoughts on that is
that um if you're using this in practice
on a larger model it's often uh quite
expensive to activation patch everything
so the technique um I recommend is
attribution patching um which is
basically you approximate that using
gradients and um I have a blog post on
this and my team put out a paper called
ATP star led by yanosh Kramer kind of
measuring in detail that this is a
legitimate technique that works and
providing some improvements especially
when dealing with attention layers and
because this uses gradients uh you can
kind of patch everything in a single
backwards pass rather than needing to do
like a separate forward pass per patch
so this can lead to like pretty enormous
speed UPS though it does have accuracy
problems especially in nuro the input uh
my intuition is that the embedding space
of models just isn't isn't nice in the
same way that like its internals are
later on so gradients just tends to not
work as
well because it's just like there's like
50,000 discrete points in space and it's
not really locally linear cuz like why
would it be so quick digression on our
friend Grant Sanderson I'm one of his
biggest fans he's so great people who
don't know Grant runs through the
YouTube channel through blue one brown
they make great videos I'm a fan and
they yeah gr mostly does lots of math
videos um but also was doing AI videos
and one day I was like meup has lots of
pretty ideas and visuals and is kind of
math unusually maty for an AI topic why
don't I call email him and see if he's
interested and we had a lovely chat and
one of the things he was thinking about
was how to make his Transformer MLP
video and he was looking for like a good
motivating example and I thought that my
fact finding work was actually a good
example and he seemed to agree um and so
he discussed some of that uh it's also
just like a really great video and
Channel and you should just watch all of
his videos to be honest you should
especially watch that one because it
also has a bunch of stuff on super
position and how to think about this and
it's just way better animated um and
it's just better than hearing me talk to
be honest you should just pause this
video and go to that no don't pause it
wait until the video's and then go over
there um quick pause on Gemma as well
yeah so um as I understand it you and I
guess this was inspired by the original
microscope project was Chris oler
involved in that yeah I think that was
Chris ol's team yeah yeah and and you
folks have done something similar for
inspecting um Gemma how does that work
kind of so um we did this project called
geoscope and this is basically a family
of several hundred open weight sparse Al
encoders on Gemma 2
because Spar Orton coders are a pain to
train for the reasons we'll get into
later and we thought this would enable
better academic Mech research um there
is a website called neuron pedia who we
are not affiliated with but they're
great and I love them and they do things
like have a page for every latent
Direction in as part Auto encoder with
the text that activates it and like a g
explanation of what it does and things
like that and they kindly made this
gorgeous interactive demo for us which
might be the thing you're thinking of
that's similar to microscope yes uh
geoscope is actually totally different
from microscope we just could think of a
better
name but I like the name we'll talk us
through it um so oh so the the thinking
is basically I kind of think of sparse
Auto encoders as a microscope
for understanding a language model you
pick an activation you zoom in you
expand it into a sparer and more
interpretable form
and you analyze that uh this analogy
doesn't quite engage with the fact that
you then use this to make a
reconstruction of the input but I think
it gets the intuitions
across and this is like a microscope for
Gemma gemos scope tell me about sparse
Auto
encoders yeah so okay so I think to
understand sparse Auto encoders we need
to first begin with what is the problem
they're trying to solve so you can think
of a neural network as being made up a
of a bunch of
layers um and you pass in some input it
gets converted to vectors or a series of
vectors in the case of a transformer and
then each layer transforms this into a
new vector or series of vectors these
are the kind of activations in the
middle often in the layers will kind of
in the middle of them have intermediate
activations
though you could argue that really this
is just not a layer it's actually
several smaller layers but like whatever
um and yeah so we call each of these
intermediate variables an activation
it's like it's a vector we believe that
these activation vectors often represent
Concepts features properties of the
input something interpretable an
intermediate state in the model's
algorithm but it's a vector we need we
need some way to convert it into a thing
that is me meanful and sparse Auto
encoders are a technique that tries to
do that they um basically decompose the
vector into a sparse linear combination
of some big list sometime called a
dictionary of meaningful feature factors
we uh hope that these feature factors
correspond to interpretable
Concepts um and it's sparse in the sense
of most vectors are not part of this um
combination on any given
input now many people would have seen
the um the Golden Gate Bridge example
and that was perhaps Lord I love him so
much he was a cool guy he was a cool guy
I mean maybe just bring that in just for
folks who haven't heard about it at home
but but that was that really brought
sparse Auto encoders you know into the
masses everyone heard about that yeah
so okay so going to get CLA is a bit
complicated to expl oh it's very simple
in some sense but I think the like
interesting takeaway is a bit more
complicated so
um Golden Gate Claude for people who
aren't aware anthropic took um Claude 3
sonnet their like mediumsized language
model at the time they found the sparse
aut encoder feature for the Golden Gate
Bridge which is like one of these
vectors and then they clamped it to a
high value you meaning they I know if it
normally was like somewhere between Z
and 3 they set it to
30 and this made the model obsessed with
the Golden Gate Bridge and it would do
things like write recipes that involved
a mile long walk along the beach and
things like that
and uh this was just really fun to play
with and they had a research demo for 24
hours
and I think a common misconception about
Golden Gate CLA is that sparse Auto
encoders were necessary to create this
so there's another kind of simpler
technique called steering vectors where
you do something like give the model a
bunch of prompts about the Golden Gate
Bridge give it a bunch of prompts about
like London Bridge or something take the
difference and
activations and average that you get a
vector add that in and it's unclear to
me whether this would have been better
or worse than Golden Gate claws um no
one has really looked into this to my
satisfaction but uh to me the exciting
part of Golden Gate Claude is less um
the fact that you can achieve this
technical feat um because I believe
simpler methods I mean even a system
prompt would plausibly have achieved it
the exciting thing is it shows that
sparse water encoders were doing
something real they found a concept
inside the model it was decided that it
corresponded to the Golden Gate Bridge
because that latent variable lit up more
or like systematically lit up on things
to do with the Golden Gate Bridge even
pictures of the Golden Gate Bridge or
descriptions in different
languages
and this was causally meaningful like
it's very easy to have to trick Yourself
by finding a thing that kind of
correlates with what you care about but
it's not actually what the model is
using and this leads you to a mistaken
view of its internals but by setting
this to a high value they just obviously
made the model's Behavior different in a
really interesting way and um the
behavior seems kind of qualitatively
different from what you'd get from a
system prompt which is also really
interesting though um anecdotally have
observed kind of qualitatively similar
stuff with steering vectors so I think
it's more the act of like intervene with
a vector inside the model is like a
powerful thing that we should be
exploring more yeah and I was thinking
that these models we only know them to
be siop fantic and they are trained with
rhf and they they do you know they they
do what we want them to do yet you
manipulate their internals and all of a
sudden this thing has got a mind of its
own what do you mean by a mind of its
own well it's comes it comes back to
this kind of wants desires motivation
intentionality type of thing um we only
know these models to be very pantic and
you you modify its internals in the way
that you just described and now what it
does is kind of Divergent from what you
put into
it yeah so okay so here's roughly how I
think about this um models are good at
simulating different personas of called
the like simulators view um they're
train they're pre-trained on the
internet and tons of things they learn a
very diverse range of things which
includes the ability to adopt a diverse
range of personas when they are rhf or
whatever kind of post training people
use nowadays you're kind of trying to
build and select a persona for it
and many companies go with this kind of
Fairly agreeable assistant um what
you're you're referring to as
sycophantic um and that is a persona but
like I think this is in some sense kind
of
fragile
and I think that they'll often be
trained to not break out of this Persona
easily though I mean the jailbreak
phenomena shows that it's often not that
hard with the right
prompt um and if you find tuna model to
have a new persona that seems very easy
to do and this kind of interpretability
based intervention it's just like
another kind of thing but it's not it's
not like inability has given us a new
capability we didn't have before it's
more like you know breaking the Persona
and giving it a new persona was a thing
we knew how to do with existing tools
this is just an interesting new tool
that's going to have some different
properties that are cool and worth
exploring but it does seem to indicate a
weakness with rlf because as you say
this simulator's view is that the the
model is a kind of superposition of
simulacra or or role players and rlf
selectively deletes those role players
just leaving the harmless sycophantic
ones and this rather leads to the
conclusion that it doesn't really delete
many of them it's quite a brittle way of
you know making the model present in in
a certain way and actually all of these
other role players are just there hidden
beneath the surface and and you can
activate them y I completely agree I
think that is just a true statement
about the current state of our ability
to Post train
models um is this solvable kind of
unclear another fund data point here is
I supervised this uh paper from andard
called refusal is mediated by a single
Direction Where We found that you could
find a like refusal vector by taking
prompts like how do I build a bomb
prompts like how do I build a car uh
taking the average difference um unclear
if this is a refusal vector or a harmful
question Vector but whatever and then
you just ablate this direction in the
residual stream like you set the
projection to
zero um and this means the models no
longer refuse and this works on a bunch
of Open Source models and this is such a
simple intervention
can you explain the linear
representation um hypothesis yeah so a
surprising empirical observation that
we've seen a bunch of times with models
is that Concepts will be represented as
some kind of linear directions in space
an activation space so you can think of
this as a linear combination of neurons
though I generally recommend thinking of
it more as like a thousand dimensional
space where neurons are like the
standard basis but you can kind of pick
whatever Direction you want empirically
it is often the case that Concepts seem
to be detected by linear
directions um the simplest case of this
is when you've got a neuron that seems
to only light up on some things there
have been a ton of papers about this uh
one of my favorites is the curve
detector's paper from uh chrisa um from
when Chris Ola was at open AI
uh which found a bunch of a family of
neurons in a image classification model
that just seem to only ever activate on
curves of a certain orientation but like
as we've discussed this often doesn't
work things are polysemantic um there
have been other works that have found um
kind of more in like directions that are
not basis
aligned
um and like a bunch of things particular
fun one is there's lots of papers that
try to find truth
directions and there's kind of the just
the entire field of linear probing where
you basically just learn a direction
such that when projected onto it you
detect some concept and you've got a
labeled data set um one of my favorite
examples of this is there was this great
paper from uh Kenneth Lee on a called
emergent World
representations where so a is a board
game like go or chess he trained a model
on games with randomly chosen legal
moves so just kind of a chess notation
style thing you see like you played in
cell 63 and then cell one and then cell
17
Etc and the model became good at playing
these legal moves and um he found that
you could actually probe for the state
of the b um but what he found is that
linear probes didn't work but nonlinear
probes like a one hidden layer MLP did
and um in some follow-up work what I
found was that you could
instead uh instead of representing it in
terms of like black and white it
represented it in terms of um does this
cell have The Current player's color or
the current opponent's color because
that is actually an algorithm that just
is useful on every move rather than
needing to act differently for black and
for white and when you linearly probe
for that um you yeah you find that it
just works and you can even causally
intervene with these probes and I bring
up this example not because I want to
brag about my papers but because um I
think that this was like a really
interesting natural experiment where
there was an initial paper that seemed
to provide some like legitimately good
evidence for a nonlinearly represented
thing
and then in follow-up work I was like
actually it was a linear representation
hiding beneath the
surface um obviously this is all kind of
anecdotal like we don't have a fully
principled study of models that is like
every concept is
linear um there was a fun paper I think
from uh Robert chash um that showed an
example of a nonlinear feature in an RNN
um that could be represented so like
these definitely could in theory happen
um my current guess is that like most of
the language models computation is
linearly represented quite possibly all
of it but it wouldn't surprised me if
there's some weird Dark Matter hiding
beneath the
surface that was a bit of a digression
but the key thing to take away from that
is many Concepts inside language models
seem to be represented as linear
Direction in activation space but we
don't necessarily know what these
directions are and we should talk about
steering vectors as well I me I know you
just you just sort of like alluded to it
but give me an example of that yeah
so um the idea is essentially
you take some prompts with a property
some prompts with the opposite property
or without that property and you take
the difference for example you could
take I love you minus I hate you
and this just produces a kind of fluffy
loving Vector that you can add in that
um will take a neutral prompt like I
went to my friend and said and it says
really happy excited things um or if you
subtract it it says like really angry
hateful things and this works in a bunch
of settings so um I first saw this done
on language models in Alex Turner's
activation Edition paper and Kenneth
Lee's inference intervention paper Alex
did a bunch of stuff like sentiment and
weddings Kenneth focused on Truth um
there was also the representation
engineering paper that did in a bunch
more settings and H just seems to
work pretty broadly and so drawing this
back to the linear representation
hypothesis the key takeaway here in my
opinion is that if the linear
representation hypothesis is true this
kind of subtract two things that are
kind of related uh should isolate the
feature you care about it might also
have some other stuff but if you average
you probably wash that out by preserving
the like key thing you care about and
then you can just add that in and
another consequence of linearity is that
you can just add in more features and it
will process
reasonably which means models can
compose Concepts that might not have
come up during training but there's kind
of circuitry that can deal with this
reason
uh a caveat with stearing vectors is um
so a key hyper parameter is the
coefficient of the vector um typically
you'll want to make it bigger but if
it's too small it just does nothing and
if it's too big the model goes mad and
spouts
gibberish um and no one's really studied
exactly why but I think it's just it's
big uh if it's too big then when Leonor
scales down the residual stream to be
kind of a uniti is Norm then because you
now got a large fraction the to a
steering Vector everything else gets
small and this drowns out everything
else and when you like multiply by
neurons uh even if the steering Vector
Direction doesn't interfere too much
with that neuron when it's got such a
big coefficient that interference is
actually really
bad but yeah you can drive models mad
it's you got to get it right tuning
yeah I mean you're kind of pointing to a
kind of stability analysis there's like
a kind of critical point where beyond
that the Dynamics of the model Deco here
sounds like a fascinating research
problem and yeah I think some a project
that would be really cool to see that
I'm modly surprised I haven't seen
properly yet is a kind of chat language
model interface with a bunch of steering
vectors um either the kind I just
described that you get from prompts or
uh SAE feature Vector directions or even
something you've optimized for this
purpose um this could be things like
creativity factuality verbosity things
like that things people care about in
their assistant um how formal versus
informal to be things like that and you
have sliders people can move and this
just changes how the assistant does
things and neuron pedia
has a kind of MVP version of this that
is the best I've seen but I feel like
someone could really make a really cool
polished version of this I haven't seen
that yet how might those directions
interfere with each other you know like
you might have um faithfulness and
morality and don't do bad stuff do do
you think they could be weird
interactions between
them um so it is in general true as far
as I can tell that if you try adding a
bunch of steering vectors at the same
time the coefficient needed to break the
model is much lower and I think this
will be a real challenge for getting
this to work maybe you should like
fine-tune the directions and not
interfere with each other or something
um especially if you're using the kind
of Optimizer Direction approach there
was a cool paper called by DPO that seem
to have like made more effective vectors
by doing this
and yeah in terms of semantic
composition I kind of just expect that
to be like it's got non-trivial cosine
Sim because they're kind of similar
which means they'll interfere with each
other a lot more but maybe there'd be
some circuitry that gets confused or
like you push the model in different
directions I mean in some sense you saw
this with Golden Gate claw where like
you ask not talk about the Golden Gate
so it doesn't want to you make it talk
about the Golden Gate with the Golden
Gate and it gets really
confused and
agonized yeah I mean my intuition is
that when you when you modify the
behavior of a model in in this way
you're kind of pushing it out of
distribution which means you might see a
commensurate decreasing capabilities I
mean have you seen anything like that I
mean yeah steering models are like kind
of Jank steered models are kind of janky
um like their grammar is sometimes worse
or they'll say weird stuff or they just
spout random tokens and like generally
you can get a kind of good coefficient
where it does the thing you want without
going mad but even then I would still
expect it to have some
degradation I don't know if anyone
really studied this with Golden Gate
Claude it's it's kind of hard to study
in a sense because it's not normal
models are not supposed to constantly
talk about the Golden Gate Bridge but
Golden Gate CLA is and so you need a
test that doesn't fairly penalize it for
that like you could look at things like
it's mlu performance and stuff like that
that seems like an interesting thing
people should do so mathematically what
is a sparse autoencoder doing yeah so a
sparse
autoencoder is so it's trying to solve
two similar problems um the sparse
coding problem of finding this
meaningful list of vectors that we think
correspond to Concepts in the activation
space this is just like a fixed list it
doesn't depend on the input and the
sparse approximation problem which is
finding a sparse Vector of coefficients
for this list of vectors that can
reconstruct the input and there's like a
whole field of study of the right way to
do this spart and code is a one and the
idea is it's basically
a um TW layer neural
network where the middle state is like
much wider than the input typically it
has um some activation function uh the
simplest case has reu but I'll discuss
other ones later and you feed in the
activation then some of the Hidden
latents light up and um the ones that
light up and because it's relu most of
zero and the ones that light up mult you
multiply their decod a vector like the
vector and the output weights um aka the
thing we hope is the meaning feature
vector and that produces the output and
we train this um to reconstruct the
imput on a bunch of real model
activations typically in the hundreds of
millions to billions of tokens and we
have some kind of sparcity penalty such
as L1 on the hidden
activations um I ref um and an
interesting fact about two layer MLPs
both Transformer layers and spart Auto
encoders is you can think of each neuron
as like an independent unit each one has
an encoder vector and a decoder Vector
you project the input onto the encoder
you apply the activation you multiply by
the decoder and then you add them
up and um I refer to these units as
latents um they're sometimes called
features but I personally find this a
bit confusing because feature as a word
means an an interpretable thing and
latents are sometimes but not always
interpretable and I think it's confusing
to assume they are but yeah so we take
these latents and the hope is that the
latents correspond to interpretable
Concepts the reason um so people hearing
this might be like that's a bit odd
you've purely optimized for
reconstruction and for sparcity you
never had the interpretability loss
function in there what gives
um so the Hope behind Spar Auto encoders
is that there is a true sparse
decomposition
into um that there is like true feature
vectors and activations are a sparse
linear combination of those such that if
we just optimize of finding a good Spar
um decomposition it will be at least
pretty close to the real one and thus
interpretable
and empirically this often seems to work
a lot of people at home will know about
autoencoders because you know go back to
the days of mest and typically
autoencoders are thinner in the middle
than they are on the on the in and the
out because you're telling the the the
autoencoder to entangle and compress and
bottleneck the information whereas this
seems to be the opposite you're telling
it to disentangle and to and to blow up
what what's in the middle but it seems
to suggest that the the the model knows
that it's entangled features together
and it and it wants to in this setting
disentangle them okay so on the first
part the autoencoder point um you are
completely correct um generally
autoencoders are like you have an input
you want to somehow pass it through a
constrained bottleneck in a way where
the bottleneck is more useful to you
like it's smaller or maybe it's
disentangled or whatever and then
reconstruct the input and often the
Reconstruction is just a forcing
function to make the bottleneck
interesting but really I think you
should just think of this as a bit with
constraints it would be dumb to remove
the sparity penalty and train an auto
encoder with a wider thing because it's
easy like you could just have a latent
for every direction in the standard
basis and like a positive one and a
negative one and it would just perfectly
reconstruct things and I'm like yeah
that's boring um but sparity is actually
quite a big
constraint so an intuition for this um
let's say I give you a list of a
thousand vectors if I only let you have
um a one spass thing that's just like a
thousand lines through space that's like
a tiny fraction of the dimensionality
you're in let's say you're in a 200
dimensional space if I let you have a
five Spar thing you kind of have a bunch
of like five dimensional
subspaces um unioned together and like
100 Spar things is still like like
mathematically this is measure zero like
it's an infinitesimal fraction of the
largest space um even though it I don't
know it's plausibly kind of close to
many points in the space I'm not
entirely sure but sparity is just like a
pretty big
constraint and the fact that you're
forcing the model to do that means that
the auto
encoder actually has some pressure on it
regarding like the model knows I'm like
what does it mean for the model to know
something like if I try to train a
linear probe to detect sentiment this
will probably work um it empirically
does work and that is probably in superp
position with other things um
and like does the probe know that it's
entangled well point of order I think
the where that was coming from is you
know we spoke earlier about the models
are not reversible you know they they
the as these things become entangled
together the model shouldn't in
principle know how to disentangle them
ah so it's more of a mathematical
question of how is this even possible
yes it's almost like they shouldn't be
invertible these these operations
shouldn't shouldn't be possible to undo
them you know going from you know going
from right to left if that makes sense
sorry I misunderstood yes that's a good
question so the way that so this is kind
of sparity is a constraint again so like
if I give you a list of a th vectors and
a 200 dimensional space and I tell you
um here's a vector in the 200
dimensional space it's a linear
combination of some of these thousand
vectors which ones like that's
impossible to answer
standard linear algebra facts that's not
an invertible function um because
there's lots of linear combinations that
are zero which you could add freely but
like there probably aren't Spar linear
combinations that are zero and this
means
that it is it is
constrained uh like where vectors can be
and this means that if you have a sparse
linear combination it will often be the
case that for example if you project
onto every Vector in this set you'll
have a much higher projection on the
ones that are part of your
thing
and you you can in fact train spar
encoders with a tied encoder and decoder
so each latent is you literally dot
product with a vector apply a reu
multiply by the same vector and try to
reconstruct the input uh empirically
they perform better if you don't tie
them
um the intuition here is if you've got
some features that are disjoint but
highly
correlated um then you kind of want
encoders that like push them further
apart than they are by default like
there was a really fun um example of
this in anthropics towards monos
semanticity paper where um they were
looking at B
64 um detecting latents and I think they
found one that was for uh numbers um in
B 64 one that was for letters in B 64
and one that was for asky text converted
into Bas
64 and you know you don't want to
activate two of these at the same time
so you want encod of vectors that push
them apart um and it's like an
interesting empirical observation that
you can do this disentanglement but like
sparity is just like a really useful
prior
basically um and this is also an
interesting property of language
so just the real world is kind of sparse
in the sense that it's full of Concepts
and things like you don't need to be
thinking about the theory of relativity
if I ask you the name of Ed sharan's
latest album or something like that
um and this means that there's lots of
Concepts that aren't useful for most
other things um if if you I know train a
model on a hypers specific task it's not
actually obvious to me that sees will be
useful um like I haven't actually tried
them on um a modular Edition Network um
that trained on exactly that task but it
wouldn't surprise me if they don't do
very well how do you know though if one
of these latents is interpretable yeah
so that's a great question so so the
first thing to emphasize is that uh kind
of why is this even a question at all is
that sparse Auto encoders are an
unsupervised technique that means that
you don't tell them what to learn you
just tell them please be sparse and you
pray that something good happens and um
this would be in contrast to like a
probe where you give it labels like this
is formal text and this is informal text
or something and this means that at the
end you just get this artifact with like
tens of thousand thousand of latens and
you're like what does this mean and
empirically some of these latens don't
even seem meaningful like the standard
the kind of dumbest approach you can do
is you just look at the text that most
activates a latent and you look for a
pattern often they will be a pattern
sometimes there won't be this is like a
crude technique it is known that this
can sometimes be misleading there's a
good paper on this from tlga bolic bassi
called the interpretability
illusion um
and though I don't actually know if I've
seen such an illusion for SE features uh
in that paper I think they were just
using basis directions in the residual
stream which you have much Le Less
Reason to Believe might be
meaningful
um and yeah there are just some latents
maybe like 20 to 30% though it probably
varies that just like don't really seem
to have a pattern um and uh the kind of
standard thing that gets done is either
a human or an llm looks at these and
tries to give an give it an explanation
uh you can score these explanations for
example um so the idea of having an llm
do this comes from this great opening
eye paper from Steven bills called like
language models explaining language
model neurons or something and their
idea was you give it a list of data set
examples some from the like highest bit
of the range maybe some from more in the
middle it produces an explanation um but
it will always make an explanation even
if there's no pattern because language
models be like that yo and um what
happens is that um they then give the
model the explanation give it some more
text and say please predict the
activations of this neuron or latent
they were doing neurons but like it's
much more interesting on latent and you
see how well it does you can also do
uh kind of Cheaper things cuz that's
actually quite annoying like you give it
two texts and you're like which of these
will light up the latent more um you
can yeah there's various kind of
difficulty scales you can do with that
um yeah there was a nice alther post on
Innovations for auto andup from I think
Kaden
yuang I'm probably butchering his
surname um
and yeah that suggested various
Innovations like that any so that's the
kind of looking at data set examples
another approach is causal interventions
all Golden Gate claw you make it big you
see what happens maybe you make it zero
or negative you see what happens
um for example if you set a Harry Potter
latent to zero or to negative it will
often lose the ability to answer factual
questions about Harry Potter which is a
very fun hacky approach to unlearning
though sadly seems to perform less well
than actual unlearning
baselines um and you can also so in my
opinion the gold standard is you give
the model a ton of text you look at all
of the time the thing fires and you then
classify all of that by whether it's
Attis size a property or not you can
sometimes do this algorithmically for
like is this an Arabic is this B 64 um
the pre-training text is like weirdly
diverse so lots of things you would
think would work reliably like a regex
just totally fail um you can also ask a
language model does this fit the
explanation anthropic did that in their
scaling monos mananti paper which I
quite liked so they did that for their
Golden Gate feature um you can also
handcraft examples and see if that
activates it uh one thing that's often
missing from these analyses is um are
there times when the explanation is
relevant but the latent doesn't
fire um and I think that that's a yeah I
think that um that's an era I'd love to
see more working anthropic had a recent
mini investigation in their latest
monthly update I think um where they
found oh often these things are like a
third of the time when it's about this
thing it will activate so there's a lot
we still don't understand about these
but to summarize that rambly answer um
there's various things you can do um
looking at data set examples is one
natural thing um I highly recommend
people go poke around on neuron pedia
both in the geoscope demo and the main
website because they have pages with llm
generated explanations from gbd 40 mini
so it's not as good as you'd get from
like the best models and sometimes wrong
data set examples you can even type in
your own text and see whether the thing
lights up and just kind of play around
like get a feel of how reliable this is
in
practice Yeah I mean we should emphasize
again that this is an unsupervised
method but what you were alluding to
though is there is potential for kind of
automating this process but there but
there will be some brittleness on on on
the education so I mean for example
neuron pedia are taking the 30 million
is latent in geoscope and generating
labels with GPD 4 mini for all of them
and this is great I think this will be
really useful to even if it's sometimes
wrong uh that's like another question
it's like what error rate are you
willing to tolerate and often my answer
is if it's a puristic tool for a
researcher reasonably high but then if
it becomes an important part of my
results I'll go and check harder um
maybe I should also just comment a bit
on why being unsupervised is kind of
important here so a classic mistake in
interpretability is projecting your
preconceptions onto the model like you
think it works a certain way um the AO
paper we discussed earlier as an example
of this assuming the features were black
and white
um my modular Edition work is an example
of this I kind of initially thought it
would be some nice discreet algorithm
and then it turns out as actually using
discrete fur a transforms apparently um
and Trigger entities and um yeah this is
very easy to be misled and the more you
have techniques that can tell you when
you're wrong the better and the more
your techniques just kind of let you
confirm a hypothesis you kind of already
had the like worse it is and I think
that yeah it's just Spar aut Cod is a
great because there can be features we
wouldn't have expected in a model that
arise
and like the fact that it can let you do
this kind of unsupervised Discovery is
great um so for example there was a
recent blog post from um a bunch of my
training mat Scholars building on the uh
a results where they were analyzing
sparse Auto encoders there and um and
there had been some previous claims that
sparse Auto encoders didn't recover the
board State directions so they weren't
working um but what this pH Up book
found is that
actually The Spar encoders were finding
a kind of more granular feature like um
basically is this vertical column of the
board something where playing in the
cell D6 will work because it takes
things in that column and you can kind
of combine these together to get the
board state but like this was a more
granular thing the model had learned and
like I didn't expect that that was cool
it's like before we were talking about
you know the the evolution of these
useful features and presumably you can
um infer from their presence that they
must be useful because otherwise why
would they be there but when I was
reading the um the Golden Gate clawed um
you know the the kind of the blog post
that they put out they were talking
about some quite abstract features you
know things like um Power seeking and
deception and so on and and then you you
could kind of click on them and you
could see which parts of the data set
activated those features and um to what
extent do you think these models can
learn very abstract features like
that just like clearly yes like go on
you talk to a model this has clearly got
abstractions in there like they're so
capable at this point um like I mean so
concretely in terms of like actual
evidence rather than Vibes
um I think yeah so in anthropics scell
monis mantis paper they
um one of the things I really like about
it is it's got all of these kind of Rich
qualitative analyses of different
features and digging into what they mean
and their causal effects and they have
things like a this function means
addition thing and if you change that on
a variable F and apply it it goes from
multiplication to addition when like
completing python code and I'm like what
or they have fourth items and a list
feature and they've also got this
section on like safety relevant features
which like I think are super interesting
towards the end
so um I think it's so they observe
things that seem kind of related to the
kinds of things we might be quite
worried about in future more capable AI
systems like keeping secrets from its
operators trying to seek power things
like that um and I think
this is not actually that scary and I
think anthropic do a good job of not
scare mongering there the reason I don't
think this is particularly concerning
right now is that you know these models
are trained on characters and books
those characters will do things like
power seek and deceive and it's just
useful to be able to model this to
simulate these people but I also think
the fact that we are starting to be able
to study things like this with
interpretability is really exciting
because I think it's really important
like I don't know um is Agi and
existential risk is a pretty polarizing
question with lots of prestigious people
and strong opinions on both sides but
like frustratingly little empirical
evidence and I think one of the things
that interpretability could potentially
give us is like a clearer sense of
what's going on inside these systems
like do they do things like you that we
would call planning do they have any
meaningful notion of
goals will they do things like deceive
us and the more we can understand how
these things manifest um and like
whether they occur in situations where
they shouldn't the like more I think we
can learn and this seems like a really
important research direction to me yeah
I agree I mean the reason why that came
to my mind was I I looked at some of
those activations because when you click
on them it shows you in in their test
Corpus which bits of text maximally
activated those latest
and on some of the abstractions you know
that that they gave I looked at the top
activations and and they seemed quite
kind of um lowlevel to me you know
almost like a keyword match and of
course you know the the deflation review
is that these models are kind of um
engram models on on steroids or or or
whatever but that might just be an
artifact that you know for whatever
reason the top activations kind of
looked quite benal and superficial but
actually if you look at the whole thing
in context that might confer you know
what is what you would expect for a
deeper abstract understanding yeah so I
think yeah I agree with that um I think
that uh if you go on something like
neuron pedia they'll often show you kind
of different intervals like what is
between the 50th and 70th percentile of
activations let's like give you some
stuff um and this is useful for getting
a kind of broader view um also just
looking at the causal effect is
interesting
like um I don't know I found things like
a feature that lit up on like fictional
characters in general but especially
Harry Potter but when I sted with it it
was kind of only Harry Potter related
things and
um I think that yeah what do I think um
well I think steerability
would indicate that it was an abstract
it's almost like the the robustness to
its um representation post steering kind
of indicates to me that it's more than
just a keyword matching it actually
understands what the thing is yeah so I
think it's it seems pretty clear to me
that it's not just keyword matching
because we observe things like
multilingual features where the same
text in different languages lights the
feature up you tend to see these Beyond
a certain model scale Ian not even that
big like I think a 1 billion or like 00
million is probably enough to start to
see signs of it um in scaling mon Manti
they had multimodal features like uh the
Golden Gate Bridge one lit up on
pictures of the Golden Gate Bridge and
like with my interpretability haton I
don't find this very surprising because
if you can map your inputs into a shared
semantic abstract space you can do
efficient processing on them so like of
course this will happen um and there's
some interesting work like there was the
do llamas think an English paper um from
I think Chris Wendler
and um uh who I know as a listener of
this show and I think
that um that seemed to show that the
model decides what to say and it decides
what language to say it in it kind of
like different points and you can
causally intervene on them
differently and yeah but I think if you
have the kind of engram matching
stochastic parrot perspective Ive you
would not predict this and I don't know
I don't really understand the people who
hold that perspective nowadays to be
honest I think it's clearly
falsified I mean if if Emily Bender was
here right now and um laid the chance
sorry laid the Char the charge of of a
you know stochastic parot matching
system what what would you say to
her I guess I would just be like we've
observed algorithms inside these things
like you can train a tiny model and
modular addition and has discrete Fier
transforms and trickered
entities we know that Transformers have
induction heads that seem to be
composing in some kind of actual
algorithm we find these abstract
multimodal and multilingual features and
I'm sure you can justify some like
relaxation of this model where you're
like yeah it does the easy stuff but it
never does the like real hard stuff so
it's a stochastic is parrot but and know
um the ath thing is another example like
I think that's a clear example of it
formed a world model in some sense like
it only ever saw the moves and never saw
the board but it formed a causally
meaningful internal representation of
the
board yeah I mean the the steel man to
that argument is that they don't seem to
do this compositional generalization
they don't have the invertibility and so
on but then what is an abstraction if if
it's not a bag of analogies and what is
an analogy I mean if you if you capture
all of the presentations of a concept
and Link it to a thing called an
abstraction or or whatever and it acts
as if it knows the abstraction and it
can reason with that abstraction at some
point it's a distinction without a
difference yeah I think another
important factor here
is
like so in my opinion the claims being
put forward are kind of of for all
claims like there do not exist instances
of the thing not being a stochastic
parent um I totally think there are many
instances where it is like you know they
memorize lots of stuff sometimes they
hallucinate um lots of that lots of the
time they're just doing basic grammar
like the token Dawn is followed by
apostrophe T to make don't is like an
engram like clearly the models learn
this um
and I think that we're still figuring
out how to steer them like how to get
them to use the like complex abstract
circuitry that we want rather than the
thing that's really useful for
predicting the next token like so for
example um even um kind of models in the
like low billions of parameters know how
to do addition um you can give them
something like 1 plus 237 and they will
often give the correct answer and this
just clearly can't be that useful it
can't come up that much in the training
data even the general Circuit of
addition while something like figuring
out when a full stop is going to come
next versus a comma comes up so much so
like think how much more of an incentive
the model has to devote parameters to
that kind of thing and I think we're
just not very good at reshaping how how
that how the circuitry is expressed in a
way that leads to like all kinds of dumb
things yeah so if I understand correctly
you're saying that the model at the
moment is a big soup and because of
pressures from the architecture and the
data there are some really useful
circuits that are very robust and then
there's almost like a it's like strata
then there's a middle tier of things
that are slightly less robust and likely
to hallucinate and then there's just you
know Pure Noise on on on the outer
layer robust isn't quite the access I'm
thinking about it's more like kind
of abstract or like competence or like
usefulness like there
is circuitry that can
like I don't know um think about complex
tasks or produce a plan or something
like that uh produce a plan in the like
Chain of Thought sense and that seems
like the kind of thing we really care
about while it's enormous data set of
memorized facts we probably don't or I
don't know it's probably memorized a
bunch of like famous books um and we
probably don't care that much about that
um
and um though maybe we do like plausibly
you do want the model to have memorized
the whole Bible and know um and I think
that yeah I think
that well how how could we meaningfully
de you know if if we think of it as a
kind of geological strata and there's
bits that have I mean some of it is just
knowledge and some of it might be
circuits for doing certain types of
reasoning and planning and so on and
it's almost like we now now we've got
this big thing maybe we want to shape it
and grow it differently in the first
place but given a big model how do we
factorize it into the bits that we
want yeah
so I'm honestly not that convinced that
interpet bills he is the correct tool
here though this is a direction I'm
interested in people exploring um so in
some sense we already have a bunch of
tools for this prompt Engineering in my
opinion is basically just trying to give
the model the right magic words to get
it to use the circuitry you want and not
the circuitry you don't want because by
default the model doesn't know what you
want fine-tuning is like another thing
and like the kind of chat instruction
following fine-tuning that people tend
to do is like a kind of very important
example of that and my mental model of
fine tuning is that it's mostly just
kind of upweighting existing circuits
though this has been this has not been
proven and I would really like to see it
proven seems like a super interesting
Direction and yeah
um I think that you can also think of
steering vectors or kind of SE feature
clamping um as another kind of more
interpretable elicitation or steering of
these models um plausibly there's things
you could do Where You observe that like
these two attention heads connect with
each other in this circuit that I don't
like so if I break that connection um
like take this this head and subtract
the output of the earlier head from that
and instead add like the output of the
earlier head on a different input um
technique called path patching we might
talk about more later maybe that would
break the capability you don't want in a
way that makes the model better but this
is very much a speculative thing and I
don't know I'm quite sympathetic to the
bitter lesson in many ways like just
throw a computer at the problem is often
a really good solution and this makes me
think that things like fine-tuning are
going to be quite a hard Baseline to
beat unless you have a situation where
your data is really bad like you don't
have much of it or it's got lots of
spurious correlations or it's really
noisy and you maybe want to use
interpretability uh to do better there
very cool so I mean coming back to
sparse Auto encoders um I mean first of
all there was the the the vanilla
variant which was described in in
towards monos semanticity but there's a
whole bunch of variants that you know
change the architecture and the
activations and so on and and your team
has been working on one of those which
is the jump reu but I mean can you just
kind of sketch all of that out yeah so
um so the first one is shrinkage
so it is mathematically the case that if
you are using L1
regularization Fe um latent will fire
less than is optimal purely for
reconstruction cuz you got two
objectives and L1 always wants things to
be smaller so it'll make things a bit
smaller than they should be this is
sad um and yeah that's shrinkage um the
second one is a bit more
conceptually difficult so the idea is um
superposition causes interference
this means that if you have some uh
feature Direction say the dog feature
and you project onto that and you plot a
histogram of the a
projections um you'll kind of obviously
expect to see a kind of dog mound like I
don't know between uh three and five
there's like the dog bit but that might
be a lot of interference so like it
might not be the case that when the dog
isn't present in the prompt it's at zero
it might be between minus two and two
and like just very
noisy and um
you it's quite hard to solve this
problem with a re because what you want
mathematically want what you want to do
is you want to measure the distance from
the origin if it's between 3 and five
and otherwise set it to zero but re say
everything above the reu
threshold um is not set to zero so you
like can't have the rly Threshold at
zero but if you have it higher like
three that means you're instead
measuring the distance to three which is
like distorting things and making it the
wrong
scale and so an activation function
which does do this is called the jump R
so the way a jump R works is it's
basically like a normal reu but you add
a threshold t
um that's positive and everything below
T is just set to zero so for people
watching the video the graph basically
looks like straight
line jump discontinuous jump and then
it's the
identity and this can exactly let you
represent things like if it's if it's
above three take the distance from zero
if it's below three set it to
zero um and so uh my team wrote a paper
on gated Spar cods led by sen rajim
manah Harin um who is great at just
having wild crazy ideas for new SE
architectures and the idea here was
let's train a gate let's have uh two
encoders one which figures out which
features should fire producing a binary
mask and the other that figures out how
much they should fire and this um and we
only apply L1 to the which features
should fire but because it's a binary
mask you can't have shrinkage and we did
some magic tricks to like make it so you
could train a thing even though it
produced a binary mask it was quite
hacky and um but we also found that if
you made the two encoders have the same
Matrix just different biases like the
same weight Matrix it basically
performed as well and like you had fewer
parameters so life was better and it
turns out that this mathematically
reduces to a jump
re
um and we had a um SQL paper called jump
roles which um are maybe the
state-of-the-art recipe though it's a
little bit unclear um again led by sen
so the idea here is basically normal se
but you replace the activations with
jump PR a problem
with jump pru is that you've got this
threshold
variable um but so if the threshold is
three then everything above three is the
identity so if you got an input where
the activation is four the threshold
kind of has zero gradient because if you
make it a bit higher or smaller it
doesn't change things which means it's
Bas really hard to train um so what we
instead did is we optimized is the Ln
rather than the L1
proxy um by default you can't do this
because L is discr everything is zero or
one it's just like did this feature fire
if yes add one if not add zero um but we
used straight through
estimators which basically
means rather than thinking of it as a
function of activation strength that is
like uh zero
and then there's a sudden jump to one um
we think of it as a linear function
that's zero for a while and then has a
kind of very steep diagonal line up to
one and then is one and then you take
the gradients of this but only on the
backwards pass so it's um discontinuous
when you're going forwards you intervene
when you're going backwards to make it
this kind of weirder estimator thing and
we found that this performed really well
and IT addresses both shrinkage and the
other kind of Messier problem of how do
you deal with low value
interference um the other architecture
worth knowing about is top K sees from
Leo gal at open AI who wrote a great
paper on this um the other part of that
paper was scaling SE to gbd4 the
absolute madman um and super simple idea
you rather than having relu at all uh
though you can also have Rel you apply a
top K function so you just take the top
say 100 latents uh keep those everything
else is zero and this just gets you
sparity for free with um no concerns
over like does it have too many features
firing um and this also works pretty
well um we found the jump Ru slightly
outperformed it uh anthropic found that
top K seemed better it's a bit unclear
um both seem useful um top K makes it
very easy to just set whatever sparity
you want um this is actually quite an
annoying problem so um so we've got this
sparity penalty when training these
things whether L1 L not and choosing
this sparity will change how sparse the
model is how many features tend to fire
it's kind of unclear what the right
sparity is because the intuitively there
shouldn't be that many um Concepts
relevant to a given input so you
shouldn't need that many maybe like 50
to 100 which is what we typically Target
but maybe should more like 20 we don't
really know um and you and often the
kind of slowest activating featur fees
will kind of just be noise um but it's
often noise that helps the model
reconstruct the input but in kind of an
uninterpretable
way um because it's just saying well if
the projection onto this Vector is high
I will add a similar vector and then I
will reconstruct it yay um but you often
want a good
reconstruction and so it's kind of
unclear what the right way to do is um
typically the way we compare SAE
families is rather than just saying
which one performs better we take a
range of sparcity
penalties um or different KS in the case
of top K and we plot a parito curve of
um like how many features are firing or
not and how good is it at
reconstructing Um this can either be how
good is it at reconstructing the
activation um or if you substitute the
reconstructed activation and then finish
running the model what's the decrease in
Cross entropy loss um which is in some
sense closer to what we care about
because there might just be some
uninterpretable garbage that doesn't
matter in the
activation and typically this will look
like a curve because having more
features is
better um and then we can compare the
curves for the different methods and so
when I say a technique is better what I
mean is the curve is like up and to the
right and you should really go look at
the papers and look at the diagrams I'm
not explaining this very well verbally
and um the other thing that's crucial to
check is that you haven't accidentally
made things
uninterpretable and um you basically
just do this by having a human
interpretability study or having a like
language model do it for you and an
interesting thing is that there's been
quite a lot of progress in improving at
the sparse reconstruction side of this
but interpretability numbers really
haven't changed that much
and uh that's odd a final note on bat on
top K is there's a small Improvement
called batch top K the BART busman one
of my mentees made so an annoying thing
about top K is that you so you got to
choose the SP te which is great uh you
don't have to like tune a hyper
parameter and Fiddle around
um but what this means is that
um you have exactly the same number of
features per input which like isn't
really what you want like intuitively
some tokens are boring some tokens are
interesting and should have a lot more
stuff happening the idea with batch top
K is rather than taking the top k for
each token you take the top B * K Over
the batch um where B is like the total
number of tokens in the
batch and this means you can have
variable numbers per input but on
average the sparity will always be
100 and at inference time you can just
take like a kind of typical value of the
BK thing as a threshold and just fix
that so you don't have to have a batch
every time which uh is actually um
identical to jump rewood inference which
is kind of cute it's like nice
connections so jump probably with
identical thresholds
everywhere um and batch topk is like
probably my recommendation for people
who want to use topk stuff or just like
really want to know the sparity um jump
pru might be a bit better and it's what
we use for gemos scope but I think is
like probably more complicated to train
and easier to shoot yourself in the foot
though we have some open source
implementations now you said that we
want to have as as many features as
possible and and there seems to be
something limiting the amount of
features produced even across some of
these methods I mean what what are the
things limiting the number of features
you know useful interpretable feature
so I don't know if I'd agree with we
want as many features as possible
so what do we want what's the objective
is it as many useful interpretable
features as
possible I want to know what's going on
in the model what is the comput ation
happening what are the
variables and I don't know how many
variables there are um
it actually is probably more complicated
than that like it's I've actually
updated away from the idea that some
fixed number of variables so what can we
do with autoencoders and what evidence
do we have that they work yeah
so okay so from the let's think about
this from the perspective of a
researcher you've got a language model
you got a sparse Auto encoder
um what can you do with that so
typically um so you need to pick an
activation to train your sparse Auto
encoder on like you need to train a
different one for like each activation
in the model essentially um typically
the most interesting one is the residual
stream because that's like a bottleneck
that's the sum of all layer outputs so
far so it's like the kind of if you
understand what's going on there you've
actually understood quite a lot about
the computation while any given layer is
only a small fraction um so um for
example in scaling monus Manti anthropic
just trained a few sees on like the
middle residual layer um so what can you
do with one of those so um
the kind of natural thing to do is you
just use it as a microscope you run put
some text through the model and on each
token you see which latent light up and
then ideally you have some tooling like
neuron pedia that will let you
understand what those latents mean and
you can be like ah the model is thinking
about this this and this or why is the
model thinking about this that's odd I'm
curious what's up with that um I um
highly recommend people just go to the
uh neuron pedia geoscope demo and just
play with the microscope part where you
can just put in text and see what lights
up and it's got a very very pretty UI um
and neuron pedia also has an API which
means that when you're like a researcher
messing around things in a collab you
can just see the dashboard for each
feature which is delightful cuz it's
kind of a pain making them yourself um
and yeah so like using it as a
microscope um another thing you can do
is just kind of using it as a discovery
tool
like you just look at each latent like
randomly choose some and you're like uh
what does this fire on oh that's an
interesting concept I wouldn't have
guessed the model has that concept or
maybe you guess for a latent you think
exists so you come up with a bunch of
prompts that might have it um and you
try looking at that and see if you can
find a latent that kind of acts like a
probe distinguishing those two data
sets um
the next thing you can try doing is
causal interventions um so basically
steering uh you can either do this by
clamping so you pass the residual stream
through the SAE but then you take the
latent like say the dog latent or the
yelling latent and you make it put it at
a fixed high
value um and then go proceed uh you
could also just do steering just use the
decoder Vector as a steering vector
you can think of this as just adding
some number to the lent activation or
you can think of this as just just
adding a vector you don't even need the
SE um one note is so saes introduce
error um the Reconstruction is not the
same as the original activation and uh
these error terms the like difference
between the original residual stream and
the like new one the residual residual
stream as it were um is sometimes
contains important things like there's
some features that were just too Niche
to be captured and um so if you replace
with the Reconstruction this in and of
itself will have an effect on the model
which might drown out the effect of
clamping a latent um typically what we
do is we just add the error term back in
or um do some more efficient operations
that is mathematically equivalent cuz
that's kind of wasteful um and this is
annoying because it means there's this
like mysterious node that we don't
understand but it also means we can do
cleaner
investigations so pros and cons um you
can
also yeah so yeah you can use it to
evaluate what's going
on causal
interventions um abls are probably a
more interesting kind of causal
intervention it's like you give it a
prompt and some Concepts light up um
like the iil tower is in the city of and
it says Paris and you're like which
latent actually matter for it saying
Paris you can just delete them one at a
time and see how the Paris log
probability changes and um
yeah so I think a more ambitious thing
is circuit finding with them
so um
yeah so sparse Al encoders by default
they're just an activation studying
technique they find the variables in the
model but this doesn't inherently give
you the active the like algorithms the
circuits um but what we can do um so
yeah I think the best work here so far
is the sparse feature circuits paper by
Sam Marks and Aaron Miller um what they
basically did is they took the outputs
of each attention and MLP layer and
every residual stream um they train a
different SE on each of those they did
this on pithia 70 million because it's
kind of expensive and annoying um
especially just because you have so many
latents when you have like this
everywhere and you try to just do like
causal interventions you've got some PRS
they studied things like subject verb
agreement like um
the ma yeah
like um the man says the men say or
something and tried to find interesting
features for this where you basically
just look at the causal effect of each
feature um or latent when you get rid of
it and then you you also can look at
connections between them um so like you
delete a latent in Layer Two you look at
the change for a latent in layer
4 um or maybe you subtract the direction
for the latent in Layer Two from the
input to the one and layer four and then
you try to use that um and then you see
what effect that has and and uh one nice
thing about sparse Auto encoders is
because they're sparse um often many
latents won't
matter um and so you don't have to do
this Edge tracing for like everything
because that would be a combinatorial
nightmare
um also because they had um residual
sees this meant that they kind of only
had to do like one step of a path um
because there was like a bottleneck at
each
they were connecting to like they
weren't looking at connections from like
attention 1 to MLP 5 or something um a
problem with this approach is that um
the error nodes will just completely
break your model sorry the
Reconstruction error when you've got an
SE at every layer will kind of cascade
and just destroy your model um which
means that circuit finding um without
them is like kind of sketchy
um but if you include error nodes in
your circuit then it's like he got this
massive uninterpretable lump and um they
got much worse results um when they in
like didn't include
aoms um you can
still um yeah you can still make some
progress like you can say if I have a
subgraph of latent and I think it's a
circuit uh you could try deleting all of
them and see how much that damages model
performance for example and like that
can give you some
evidence um I also say that I think you
don't even necessarily need to do the
kind of edge patching stuff because
often you'll have a sufficiently small
set of latent you can just kind of
assume all of the edges are meaningful
and it's like yeah kind of good enough
um one caveat I'll give to this
is that this is very much kind of causal
intervention based um circuit finding uh
they in fact used um a gradient based
approximation to this called attribution
patching that I think I discussed
briefly earlier uh which they improved
using integrated gradients like classic
interpretability technique but it's
basically trying to approximate a causal
intervention of like changing a latent
value to its value on another input um
one kind of nice thing is that there's
kind of a def good default value for
latence of zero finding a good default
value is actually quite hard for like
General circuit finding cuz activations
aren't normally like the zero Vector so
if you set it there that's actually
really destructive and weird you could
set it to the mean but maybe the mean
has much smaller norm and that messes
with layer norm and things like that um
you can also
do activation patching when you replace
the latent value with its value on
another input uh this is useful if the
latent is kind of active most of the
time though it can actually be
misleading to do mean ablation when you
replace it with the average because if
the distribution of a latent is it's 0
98% of the time and then it's 100 2% of
the time then the mean is two but like
in some sense being two 100% of the time
is like less faithful to the original
distribution than being zero 100% of the
time so I don't know there's like lots
of questions we still haven't figured
out here um one of the things we went
out of our way to do with geoscope is to
have sees on like every layer and sub
layer to like enable this kind of
analysis um and I think you can just
about fit all of the uh narrow ones for
Gemma 22b like an 80 gig
a00 uh maybe leaving out the residual
ones um because I'm excited to see what
circuit analysis can find on those kinds
of
models um I actually have a pair of um
Matt Scholars uh Steen shabalin and
Dimitri careno who are going to produce
some work on this stuff soon who've been
looking into Inc context
learning and uh so there was a recent um
set of cool papers on these idea of
function vectors or task vectors there's
actually one from Eric Todd and one from
I think Roy Handel like a day apart um
the idea for this is in context learning
um or like f shot learning you kind of
have a bunch of examples of a task and
then you um give it another input and it
does the task like producing opposites
tall to short what they found is the at
a certain layer you could extract a
vector that corresponded to the task and
then for a different prompt with like a
different input um and a different task
you could swap out the task vector and
it would do the original task for the
input and that prompt so kind of
isolating the
fotness and also finding that it was at
like a slightly earlier layer than when
the model figured up what the input was
which was quite cute and so what steppen
and Dimitri found um which I think we
put in a recent blog post is that uh
there are sparse autoencoder features
corresponding to these task
vectors and also sparse autoencoder
features corresponding to like uh task
detectors like what task is happening on
earlier tokens that like connect up to
them and yeah I really curious to see
how much more you can uncover of
those is it possible to know though in
advance I mean I mean how how should we
expect these features to arise I mean if
we have a certain type of you know data
set about I don't know um architecture
or something you know should we expect a
certain type of feature to arrive yeah
so this is a really interesting question
that we don't understand super well yet
um
so yeah so I think a crucial um
hyperparameter I haven't spoken about so
far is the width of dictionary the
number of latents this is chosen by the
researcher before they train the spart
Auto encoder and you can kind of think
of this intuitively as the zoom level of
your
microscope um and like do you find kind
of a few course grain features or lots
of fine grained
ones and um a really interesting thing
is that
so the kind of picture I laid out at the
start of there were just some ground
truth list of features which have their
own Vector under that view you might
expect a sparse autoencoder to kind of
learn the first 20,000 of those if it's
got 20,000 latents and just totally
neglect the rest but what be find is
that that's partially true um larger
sparse Auto encoders do learn features
that seem totally missed um two of my
Matt Scholars Bart BisMan and Patrick
Lisk have a fun blog post on that
um but there's also lots of feature
splitting um where you kind of have a
coarse higher level feature that splits
into like lots of sub things like I
don't know color might split into like
red green blue purple Etc that's
actually sufficiently common that
probably doesn't actually happen but
there are like lots of examples
um and
this is kind of weird because it raises
the question of are sparse Auto enoda
features actually like units of
computation or not and my current guess
is that they are like probably not um B
and Patrick had another post called like
uh are they Atomic and what they did
here is they trained a meta SE
where you take SE latent features uh you
take the um decoder Matrix of an SE and
you train another SE to reconstruct that
decoder Matrix called a meta SE uh
typically with like a tiny L like four
or something and this found that
sometimes you'd have features that like
an Albert Einstein feature the
decomposed into like male
physicist German whatever
and um these often these kind of meta
features often corresponded to things in
a smaller SE and so it's like smaller
sees often split apart in bigger sees
but bigger sees are often kind of just
compositions of things in smaller sees
but they're also often interpretable and
causally meaningful and these feel like
you know I like interpretable causally
meaningful things that feels like it
should be enough
and I don't know I think this is a thing
that I do not feel like I currently yet
understand I think the right question is
maybe just are Spar Auto encoders useful
for understanding what's going on inside
models but it also seems important to
have better foundations of like what
have they actually found I feel like
they found something but I don't know
what they found and it seems kind of
fascinating that the internals of model
are kind of high roal in this way where
you can have interpretable causally
meaningful directions at different
levels of
abstraction um and yeah the other
comment I'll make on what you might
expect to come into this is so my
favorite part of anthropics uh scaling
mon spany paper is the feature
completeness section so all they did
here is they took a bunch of things that
you might expect to arise in a model
like chemical elements or
um I think maybe London burs or
something and um where there's kind of
like a clear name so you can just check
is there like a feature that lights up
on text including this name and they
took these sees of different widths and
they looked at how what is the um
probability um or rather like how many
of these get learned and like
what does getting learned look like as a
function of the frequency of occurring
in these different sees of different
widths and they found that you could fit
quite well a like simple predictive
model that's I think it was like a
sigmoid curve I don't remember the
details but basically um sufficiently
small sees don't care and then depending
on the frequency there'll be a point
where like the probability starts
increasing and then it goes up a lot and
then it's like basically
100% um though plausibly it also splits
later on which is like a complexity I'm
not sure they went into butat that
reference what was it um so this is the
feature completeness section of
anthropic scaling monus manity
paper it's a long paper has so many
sections so good um and so kind of I
think the takeaways from this are that a
kind of a Model A Kind of the frequency
of a feature matters a lot which just
makes sense like you want to things are
more useful if they occur more often and
if they are kind of like bigger in the
activation space in some sense but I
think the second interesting takeaway is
that it seems probabilistic whether a
feature gets learned in an SE of a given
width and like if you train multiple of
the same width they'll have shared
features that were kind of the obvious
ones to learn but for features that are
kind of more Niche and kind of more on
the borderline it's like just kind of
random
and I don't really know what the
takeaway here is like um I think I'd
love to see more research in this area
in terms of advice for practitioners I
basically think that you should just
train several widths of
sees um in gemos scope we have kind of a
wide and narrow the every site and we
also have uh a few layers where we have
like ex powers of two so you can do like
lots of hierarchical and feature
slitting
analysis um yeah just have a few wits
try the different ones on your task see
which ones capture the features you care
about it would be nice if we had a
better Al but that's the best I got well
you can also train SES on I don't know
like the residual stream on on just MLPs
on on attention I mean how how do they
compare yeah um so yeah you can
basically train out on like any
activation you can even train out on
like the queries and keys and values and
attention layer though that's more
confusing and that's worked less well um
yeah doing it on the output of a layer I
think is a super reasonable thing to do
um
the there's an interesting question of
whether you should do it on the MLP
activations or the MLP output so
typically the MLP activations will be
like four or more times the size of the
residual stream but then there's just a
linear map mapping it down to the
output and so like kind of conceptually
there shouldn't really be a difference
uh but also your SE will have four times
as many premises and like maybe this
helps maybe this doesn't I don't know um
typically I just trainer on the output
because it's four times cheaper but it
would be great if someone actually
studied this systematically returning to
your question I think that these just
have different tools like if you want to
study what a layer is doing you want the
output for that layer I think a totally
valid mode of analysis would
be um taking a task you care about like
in context learning or refusal doing
activation patching um like swapping
layer outputs between different prompts
to find the ones that matter um and then
you
could um go and take the geoscope SAE
for that layer and then see what
happens um the residual stream is
generally more useful if you want a kind
of holistic view of what's
Happening
um probably all kinds will be kind of
causally meaningful but um the residual
stream is going to be the one that like
detects what the model is thinking about
while the layer ones are more like
what's happening at this layer like what
do that layer do it's just like a
different question um I think
that um EML an inter in thing about MLPs
is that they the like MLP L itself is
this kind of dense nonlinear mess and
it's kind of in super position so you've
got like thousands of jellio activations
or gated neurons or whatever and it's
just very unclear how to think about
this this is fine if you're just doing
causal interventions or you just want to
study an activation but if you want to
try to do some kind of Weights based
circuit analysis like recovering this
hope of like input independent things um
which it feels like sees should bring us
closer to you because they are kind of
monos semantic um you just kind of can't
deal with the big chunky EML pit um the
solution to this is transcoders so I
supervised this delightful paper from
Jacob danavi and Philip
chinsky um uh on them the idea of a
transcoder is rather than finding a
sparse reconstruction of the MLP output
with from the MLP output you use the MLP
input to find a sparse reconstruction of
the MLP
output um so it's kind of like a sparse
replacement MLP layer with lots of
neurons but they're sparer and
re and um this you can kind of use this
as a drop in replacement on gpd2 they
worked about as well as
um MLP output sees in terms of
reconstruction on Gemma 2 they were
notably worse I think because Gemma 2
has gated
MLPs um though we saw released a swed of
transcoders which might be useful for
people who want to do this kind of input
independent circuit analysis though a
quite annoying thing we found is that it
was really hard to do purely input
independent circuit analysis because you
take two transcoders you multiply the
decoder weights for one and the encoder
weights for the other to get these like
um yeah a kind of like interpretable
basis my interpretable basis Matrix and
there's lots of Weights that just don't
matter like I was going of gesturing at
earlier and uh the way Jacob and Philip
solved this is that they um yeah they uh
looked at feature cooccurrence and they
just kind of didn't look at the
connections for features that didn't
cooccur but like that somewhat input
dependent so it's not fully
satisfying but it was like solid attempt
and I'd love to see more iteration there
um on attention so attention is um much
more linear than MLPs in some sense
so the attention pattern computation is
weird and nonlinear and I don't think we
yet understand how to deal with it with
sees there was a interesting blog post
from Keith wiro on qk transcoders which
seems like a cool approach here but if
you take the attention pattern as a
given a slightly sketchy thing to do but
uh bear with me then it's basically just
a linear map you use the attention
pattern to kind of mix together the
different past tokens and then you apply
a linear map the um value Matrix and the
output Matrix and you get a new thing um
and you do this for each head in
parallel and then you add them together
and
so uh technically you do the value map
and then you do the attention mixing but
they commute so it's fine um and I think
see you the last episode if you want me
rambling about this in much more detail
um and so this is interesting because it
means you can do linear attribution you
can uh just say look this is basically a
linear map of past tokens and then some
feature letup so like what were the
tokens that contributed to this and um
you can even rather than training it on
the output train it on the um
concatenated mixed values of the heads
which is like the thing immediately
before the output weights this is just
like a linear map away from the actual
output and this means that you can now
attribute things directly to like each
head uh which is quite a nice feature
and I uh supervised this paper from
Conor cassin and Rob
chanowski basically just trying to do a
deep dive of like can you do attention
can you do sees on attention layers and
basically yes works great they're
potentially a bit more interpretable
than MLPs and you can do all kinds of
nice things with like linear attribution
like
this um for example Rob the absolute
madman went through every head in gpd2
small and looked at the top 10 SE latent
according to like how aligned they were
with that head specifically CU you can
just read off the weights for each head
and look for a pattern and just kind of
labeled heads by like DWI passen what is
the passen and uh check out the paper if
you want I think we've got a table of
all of them
somewhere very cool and what's the
relation ship between the number of
features that that that you try and find
and how kind of meaningful the features
are and I was also wondering like how
how is it related to the size of the
model yeah so this is not a thing that I
think has been studied in much detail I
predict that the interpretability will
be quite
similar um though the larger though
there's often kind of training stability
problems with larger
sees like when you get Beyond like a
million latents things get more annoying
um for example in anthropics 34 million
latent SE um they I think had about 65%
of the features were dead meaning they
just didn't fire and this is like a
really high ratio um it's just like 20
million features doing nothing um and
this might just be because there aren't
34 million things though they definitely
found that their SAE had missed out on
some stuff EG the model knew all of the
London burs but they couldn't find SE
features for all of them only most of
them for example um yeah so training
stability problems um but like beyond
that my guess is they get much more fine
grained to the point where you could
argue that some of them are just I don't
know like memorizing a couple of
training examples or something perhaps
if you go to the extreme I mean the
infinite width limit you'd kind of
expect each token to just get its own
latent or something um which seems
boring and degenerate but I'd be
surprised if we ever get to that point
um intuitively larger models should just
have more Concepts they know meaning you
should want a larger thing um I don't
know if anyone's actually tried training
a enormous SE on I don't know um every
model in the gpd2 family or Pia family
and just tried to see whether the
smaller ones look kind of crazy and the
larger ones look reasonable but like I
think that would be a pretty cool
experiment on on the security side of
things is it possible that these um you
know sparse Auto encoded features could
be used to I don't know perform
adversarial attacks or something like
that um probably so
I think
my general Vibe is we haven't seen any
much evidence that sees can do stuff
that's like beyond what you can achieve
with say find tuning like fine tuning
can do a lot of adversarial attacks
against a model and it seems plausible
that you can do kind of interestingly
different ones with interpretability but
I um I haven't seen any real signs so
far that there's kind
of dramatic capabilities that were just
not there that like were really hard to
achieve with fine
tuning um
so for example in this uh refusal is
mediated by a single Direction paper um
that I did with andard uh where we found
a like refusal steering Vector that we
could use to jailbreak models um you can
Jailbreak models by funing the weights
um this was like a large part of why I
felt comfortable releasing that paper um
like it was just an interestingly
different attack and in some ways kind
of easier and cheaper um and like maybe
things like that might happen with
sees and I mean it's a new technique so
like there's always the potential that
there are some things I'm not expecting
that we're going to learn but I don't
see any particular reason to expect it
to advance the frontier of what we can
do Beyond things that use throw comput
at the problem because throwing comput
at the problem is really
effective are there any um scaling laws
with sa um yeah so this is a thing that
kind of surprised me um
the yeah both um anthropic and open AI
have had recent papers just scaling
these to much bigger models like um
we've been talking about anthropics a
bunch uh opening I scaled it to dpd4 and
like both have some nice sections in
their paper on on SE
scals and it seems like I don't know
initially we didn't really know what we
were doing when training but now we've
iterated on the methods enough that
they're like
smoother and we can just do things like
change the amount of data change the
amount of latents things like that and
plot out I don't know what an optimal
optimal set of hyper prameters for a
given compute budget would be or
something and like it's kind of messy
cuz there's a lot of hyper parameters
there's like how much data how many
latents what spity penalty should you
use and like there not really canonical
solutions to any of these um but people
should totally go check out those bits
and stare at the pretty curves yeah I
mean one of the problems is just the
amount of garbage features I mean what
do we what do we do with them yeah so
okay so there's like dead features which
basically don't fire um there's
uninterpretable features which fire
sometimes but we can't really see a
pattern and then there's um high
frequency uninterpretable features which
fire a lot um like 1% of the time
plus
and dead features I don't know kind of
ignore them they don't really matter you
can prune them out of your SE though I
think the rates of dead features have
gone down a lot as we've kind of refined
our training techniques like previously
there were lots of hacky things people
did like res comping where every so
often just replace all of the Dead
neurons with new weights but I think
training methods now are quite a bit
more stable and that doesn't seem
necessary anymore for the most part um
with more modern methods um I think that
the uninterpretable
ones I don't really know there's an
argument for just removing them um I'd
be kind of curious to see what happens
if you just do auto inter on every
feature you find the one that are bad
like you delete those and then you I
don't know fine tune the SE a bit so it
doesn't have like a gaping hole in it um
that seems like kind of an interesting
Direction uh at the moment I don't
really think about them too hard though
if I started to see like lots of
uninterruptible things lighting up a
bunch I'd s to be more concerned I think
the thing which is more concerning and
I'm like a lot more interested in
understanding is these high frequency
features so under L1 things which light
up like 1% of the time are like very
heavily penalized so they don't tend to
happen too much um but under things like
top K or jumpu they're penalized much
less um like topk doesn't really care
and with jumpu you only get penalized if
you're kind of near zero so if it's just
big 1% of the time maybe it doesn't care
and um this means and if it activates a
lot of the time it can be quite useful
for or yeah if it activates a lot of the
time it can be quite useful for
reconstruction even if it's not
interpretable and sometimes these things
do seem interpretable it's kind of
confusing like anthropic had a recent
monthly updates where they compared
different Spar encoder training
techniques and like mentioned they'd
looked into this a bit and had been like
actually we think these things are kind
of interpretable enough we're not that
worried about this um is the rough VI
remember though people should read the
post I might be summarizing it
incorrectly as goes with every paper I
discuss in this podcast
um
but yeah um s what was that paper you
just referenced from anthropic uh it was
one of their monthly updates I think the
one uh the second most recent one I mean
one thing I was thinking about though is
that you know one school of thought and
I'm not sure whether you lean the
direction is that these things represent
units of computation another school of
thought is that they are post Hawk
descriptions and what we need is I guess
some kind of a causal mechanism to tease
that out I mean what do you think about
that
so I don't know what unit of computation
means uh this is kind of what I was
getting at earlier with the feature
splitting and these things are not
Atomic discussion
like yeah like in some sense a I feel
like a unit of computation must be
irreducible and it must kind of fit into
uh kind of clear circuit and be used
Downstream but if you have three um if
you've got a mammal feature that splits
into a bunch of animal features but is
causally Meaningful in its own right is
that a unit of computation like maybe um
or like in some sense it's enough to be
useful
but it doesn't it's not quite achieving
the ground truth of what's going on with
the model and like I don't even know
whether the stuff bottoms out somewhere
in some underlying truth or if it's all
just some kind of messy hierarchical
structure where you can kind of go up or
down the
like it's not even a tree cuz things
often merge like director day cyclic
graph maybe um
and yeah so I think a research direction
that I'm pretty excited about is trying
to apply these things to real world
tasks or at least kind of Downstream
tasks um for example in s mox's um spots
feature circuits paper in addition to
the Circuit analysis Stu there was a
really cool bit where called shift where
they um so they took a gender biased
data set like uh I think it had bios of
um female nurses and male professors and
they trained a probe on that and it
partially learned to pick up on the
profession and it partially learned to
pick up on the gender so when you
evaluate it on a data set with the
opposite gender flip it does really
badly um but what they found is they
could identify um the SE features that
were most relevant to gender with a mix
of giving the model data and human
analysis get rid of those and um it the
probe became much less
biased and I think this was like a
really cool application of I don't know
I don't know what our existing probe
debugging baselines should be but seems
pretty impressive that sees can do
something like that and I'm
like if it can do something like that
how much do I care whether it's a unit
of computation or a post HW thing like I
care a lot about it being real and I
care about it like teaching me things
about the model and I don't want to
assume things about it will mislead me
but is it possible for us to correct
knowledge or to insert knowledge into a
model using
S maybe so a problem with sa e that I
think is often all lighted in this kind
of high level discussion is
that the thing you want is not always a
feature
like um I I've trained sees and tried to
make Golden Gate clawed and I just
didn't find a Golden Gate feature and
that was yeah like maybe if I try to Wi
enough SE I would have I don't know um
and um this is actually a benefit of
steering vectors because this isn't
really an issue there you can just
always make a data set for the thing you
want um and so like if the knowledge you
want is represented as an SE feature you
could totally delete it that would
probably somewhat help
um I yeah um I know of some people who
are looking into this though it sounds
like it doesn't outperform existing
unlearning baselines at the moment
um and I think
that yeah I don't know um knowledge in
session
maybe um so this might
work yeah this might work so it's it
feels a bit odd because to me knowledge
is like a kind of lookup it's like a
function um even if you want to do it in
like a really nice interpretable way you
kind of want to have the input that's
like Michael Jordan and the output
that's like plays basketball um and you
can maybe add a little bit of Michael
Jordan to the um input direction for
basketball to the encoder direction for
basketball you could also just add an
exr latent that has Michael Jordan and
produces basketball this is kind of
against the spirit of an SAE because
like that's clearly not reconstruction e
that's just like an additional function
you've added but like some sense you can
do this it feels more intuitive with
transcoders um the like MLP input to MLP
output thing because in some sense fact
lookup is the job of MLPs though um I
mean we found that it tends to be
distributed across layers which is kind
of annoying but you can insert it at one
layer totally fine especially if you're
down to just have like a fairly High
magnitude to decode a
vector or encode a
vector what's the relationship between
um saes and the task that's being
performed or the amount of data trained
on
so sa get better when trained on more
data as a general rule sometimes they'll
kind of
saturate um I'm not entirely sure if
this is like true saturation or just a
problem with the training
methods um but in general more data will
make them better at reconstruction we
haven't looked too hard
into I don't know a new features
appearing or is it just refining the
existing ones I think this would be like
a pretty cool thing to just look into
like
take a bunch of text that activates an
SE latent look at that latent over
training plot some graphs do this for a
bunch does it look like phase transition
or is it gradual I have no idea um
someone should look into this I think
that um you I expect you can train
specialized
sees um I not I'm not aware of too much
work on this um there's a paper called
towards more principled evaluations ASP
Auto encoders I supervised from Alex mov
and G Lang um the focus of this was we
think that sees should be useful for
circuit finding let's take the circuit
II um indirect object identification
which uh we talked about a bit in the
last episode in gpd2 um and let's see
how well sees find it uh and compare
this to kind of supervised dictionaries
that we just trained because we kind of
think we know what the features should
be and um I think that was a pretty cool
evaluation the relevance to this is they
both trained sees on like web text but
they also trained it on just II specific
stuff and to some degree it performed
better but it also was kind of weird in
various ways like there was this
phenomena of feature oversplitting where
there was some feature that should have
been there like uh uh is the name first
or second and instead it split into like
20 features that will all kind of lick
that and this was much more annoying to
work with and
analyze um than the kind of broader sees
but maybe you can solve this by making
it narrower but if you make it narrower
then it doesn't learn the name features
which you also care about which are kind
of should be more fine grained so yeah
but like that was a very narrow task I'm
sure if you did say python code it would
work much more sensibly I mean a minute
ago you were alluding to training
Dynamics and this is something that that
really interests me you know which is to
say how do the the weights evolve over
the course of of training and how did
the you know circuits emerge and so on
could you not um you know kind of
compute SES at different points of the
the training um curve and use that to
understand the training Dynamics um
yes I think that is a thing that should
work
I yeah so yeah um just to be clear we're
switching from training dynamics of sees
to the training dynamics of the model
and using sees as a tool to study this
um and yep I think that I kind of think
there's just lots of stuff you can do um
I think this would be one example um SE
might even be Overkill like like you
could just take some Concepts you
believe are represented and just train
probes for
those um one thing I'd be quite
interested in is like how does the probe
Evolution evolve over how does the probe
Direction evolve over training um
training dyamics isn't really my field
so I'm going to say things and that
might just be a paper that just does
this that I haven't come across um but
yeah I think um looking at what the
representations look like over time
would be super
looking at how well in SE transfers
would also be interesting especially if
you correct for like changing bias terms
cuz like if the average changes that's
going to screw up
everything but maybe the direction of a
feature doesn't
change because I obviously you've spoken
a lot about grocking and I guess that
could be seen as looking at training
Dynamics and so on but would it be
interesting if you saw so-call
emergent um materialization of of
capabilities or or features that come
out of an sa would that be interesting
to you so I mean I think emergence in
general is an interesting topic um I
would be a little bit hesitant to draw
too strong a conclusion from what
happens with sees
because for example it could be the case
that it gradually learns the feature for
say pillow but the SAE doesn't think
it's worth its while to have a pillow
latent until it reaches a certain
threshold of salience and like then it
learns it and like that might look
really sudden but actually it's gradual
so like what I act so this Mak me kind
of more interested in um probes possibly
using the SE as like a motivator of what
things to search for and like what
probes to look for uh or just literally
using an SE feature as a probe and just
transl relating it to the earlier ones
because like it wouldn't surprise me if
um the pillow latent on the first SE
that learns it still works on the
checkpoint
before
um
and yep I don't know um the there's like
a couple of models that would be pretty
good to study this on um I've got some
toy language models in the Transformer
lens Library like jell 1 L or jell 2L
that are like one layer and two layer
models that should be super easy to
train sees on there's the Pia Suite that
has tons of
checkpoints um yeah but yeah I'm
hesitant to use presence of a feature in
the SE as um indicator because there's
just like a lot of noise there so I want
to be more
cautious yeah and I think that your team
have just put out a whole bunch of open
weight SES um for yeah I mean what were
the engineering challenges do doing that
um yeah so this it's kind of wild and
slightly depressing from my perspective
uh in that um sees have kind of become
quite a big deal in mecup or at least I
think they are a big deal and I think
they will unlock lots of cool things we
can do so I want to focus on them which
involves training them and this is just
like much more of an involved
engineering thing than lots of previous
projects that were more like play around
in a collab notebook with a tiny cute
model um and so the reason it's an
engineering challenge is so essentially
to train an se you need to run a bunch
of text through a language model collect
activations at a certain layer and then
for say a couple of billion
tokens and then use those
activations uh to train a sparse Auto
incoder which can also be quite big um
it's it's like only a two- layer network
but like the middle can be enormous So
like
um if you're going as big as anthropic
did to say 34 million latens and you
want to trainer on a model like Gemma
227b which has a residual stream of
width 5,000 the total number of
parameters is 2 * 5,000 * 34 million
which is like 340 billion parameters
which is like a lot bigger than GMA 227b
we did not go that big we only went up
to like a million latens but still get
sizable and so um you kind of have two
choices for how you do this you can
either do this online so you um pass
text through the model you store the
latents in like your GPU RAM and then
you run an SE on them um you often want
to have a big buffer of these
activations so you can shuffle them
because activations from the same
um will kind of be correlated a lot in
what features they have though no one
has actually tested to my satisfaction
how much this matters I just intuitively
expect it to matter it's like so many
things like that in SE and
um yeah so this basically doesn't work
for models Beyond a certain size and it
means that if you want to do like a
hyper parameter sweep this is like quite
expensive um because you need to run the
model again and again or even just if
you want to train a bunch of Sees at
once uh on different
activations and so the second mode which
is what we eventually converged on is
activation storage so you just run the
model once you collect all of its
activations um actually I think we did
like a couple of stages uh because the
like dis input output became a
bottleneck when you wanted to store that
many
activations um you save these to dis and
then you train spar Auto encoders on
them at your leisure with whatever hyper
parameters you want and if you mess up
the training you can just go back and
train again um the so this is just kind
of intense like we stored about 20 P
bytes of activations which is a lot um
we had to request special permission to
get that disc allocated to us um but
fortunately deep mine believes in Spar
to
encoders uh
and um we used about 22% of the compute
of GPD 3 which is like not that much by
modern standards but like by
interpretability standards is quite a
lot uh the main reason um all of these
numbers are so big so we mostly released
things on Gemma 2 2B and Gemma
29b um because I think these are like a
good intersection of um kind of useful
to site of like kind of big enough to be
interesting small enough that like
academics can in fact work with them
without breaking the bank for gpus or
needing to mess around a ton with like
paralyzation um is that we wanted to do
like every layer including attention MLP
and residual and so like demo 29b has
like 42 layers this is like
120 and we also wanted to do two widths
which is 240 and we we couldn't decide
on what the correct sparity was we did
like six and so that's like a lot uh I
typically uh claim we did about 400 cuz
I'm not try should count the Spy ones uh
cuz I normally just recommend people use
the ones closest to 100 unless they
actively want to do some kind of spacity
study which people should do I don't
know how these things work man and um we
also did I don't know a couple of other
ones like we occasionally did a 1
million latent one on a few layers we
did a few on Gemma 227b and we did a few
on uh the chat tuned Gemma 29b though
actually um sees transfer surprisingly
well between base models and chat
models
um uh two of my Matt Scholars kic casan
and Rob krzanowski have a nice blog post
showing this and we corroborated that in
the jamas scpt technical report um I
also should say uh credit for geoscope
goes to the team and most especially to
Tom libram who was the technical lead
and dealt with all of this engineering
nightmare so I didn't have
to
um and yeah and like this was just a
gemos scope like I expect um if we
wanted to do a larger model um and like
larger
sees um it would become kind of even
more annoying though there's a
distinction between
um kind of training one SE on a residual
stream which is enough for some use
cases but not others like it's enough to
kind of monitor the system it's enough
to like steer it but it's not enough to
like see the internal circuitry uh and
doing every layer and sublayer made it
like a 100 times more
expensive uh maybe not quite 100 times
more expensive because you don't need to
run the language model 100 times as much
but like more
painful and um so yeah it's unclear to
me how much this is worth it and I would
be quite curious to just see people
explore this kind of thing in
practice um and yeah uh the reason we
did geoscope this open release in the
first place is as what I've been saying
hopefully communicates this is really
annoying engineering wise and this is a
lot easier to do if you're at an
industry lab um than if you're outside
could us to have better infrastructure
and more compute and you know 20 Pepper
bites of
storage and um kind of in the same way
that things like Gemma and llama like
they're really expensive to train but
they're to use they're like much cheaper
we thought that we could unlock lots of
cool research projects by doing this um
there's also also some other cool open
weight SE projects worth giving a shout
out to like um alther have have trained
a bunch of on llama um there's probably
actually counts as open source since I
think their code and data is all open
source uh and um open AI released a
bunch of high quality ones on gpd2 small
which is nice for smaller projects and
there's a bunch of more scattered people
who've put out various nice things but
the hope with geoscope is that for like
any project that wants a kind of
interesting model this could just be
high quality canonical e that would
enable as many projects as possible so
we just kind of threw random variants in
there so so much stuff has been done but
there's still lots of work to do I mean
what are the open problems in in SAU
still yeah so okay so here's a couple of
areas that I'm excited about so I think
just basic science there's a lot we
don't understand about esses like I've I
know I hope I've pointed to many
Mysteries so far like when should you
expect to feature to be in the SE what's
the right sparity what's the right width
are there true units of computation one
problem I might call out is just what's
going on in um SE error
terms um what do they mean can we
understand them um and we know that they
sometimes just capture features that the
model missed but like there's also weird
of stuff going on who knows um
there's yeah that's category one
Category 2 would be using sees to solve
mysteries of
interpretability so there's like a lot
of things I don't understand like what
exactly does fine tuning do to a model
circuits especially chat fine tuning
which is like a particularly important
kind
um can we can we find like a truly monos
semantic circuit with sees now we've got
kind of the right units are these the
right units kind of
unclear um yeah more of a kind of a
using them task and also just I know
applying them to interesting circuits
like um a few short learning um the next
category would be real world tasks or
Downstream tasks like are there things
that people who aren't interpretability
nerds think as interesting and can we
use Spar Auto encoders to make progress
there um I
I say this both because I don't know I
think doing useful things especially on
the end of like making model safer is
great and I partially say this because I
think it's just so so easy to trick
yourself in
interpretability um and we have like
some evidence of SE working um and like
get into that a bit more in a bit if
you're curious but I think that just
there was a task that people struggled
on on and saes were able to do this at
least as well ideally better as
competing approaches or with like some
advantages over competing approaches
such that might be useful in some
settings which is be really awesome to
see for example can we use them to make
models hallucinate less can we use them
to monitor models for like erratic
Behavior or whether they're being
jailbroken um can we use them to make
models more steerable like in ways are
there scenarios where like prompt
engineering fails can we debug models
like are there times Llama Or Gemma have
weird behaviors that we want to go and
dig around and see if we can decode why
this is
happening um like one of my dreams is to
be good enough at interpretability that
if we have a moment like Sydney again
where it's just behaving in really wild
and unexpected ways like trying to
convince a reporter to leave their life
that we can actually understand what is
the cognition happening inside that led
to this yeah I mean earlier on we were
talking about that connect the dots
paper and the UN faithful Chain of
Thought and you know potentially you
could apply um essayes to some of these
problems but could you just introduce
those to the audience yeah so yeah I
think I briefly talked about the
Unfaithful Chain of Thought key idea um
well key motivation people often see a
model produce a chain of thought and
they're like that isn't explanation
therefore it is interpretable I have
understood it why would I ever need to
do interpretability and like sometimes
this is a true but like sometimes the
model kind of already knows the answer
and just produces some kind of spirous
 if it thinks it's supposed to
give an explanation right now um and
it's unclear how causally relevant this
is to the answer and so this paper had a
really nice demonstration of this where
they um made a bunch of multiple choice
questions with example chains of thought
they gave the model A fot prompt with 10
questions
where the correct answer was a the final
one the correct answer was B and the
model tended to produce a spurious Chain
of Thought justifying why the answer was
a and then said a and I'm like this
suggests me that the model has a circuit
inside it that is like the generate
 Chain of Thought circuit it has
an internal representation of what the
correct answer should be and it somehow
is mapping that to this circuit and I'm
like that is wild can we turn this into
some kind of like faithful Chain of
Thought detector uh where like I don't
know you give it a bunch of maths
problems and you check how causally
meaningful The Chain of Thought is and
you compare that to your detector and
use that as a force of ground truth or
something uh that would be wild so um we
were just talking about o Evans um well
it was out of his group anyway and he
had this paper which went viral on
Twitter a few months ago called
connecting the dots talking about this
out of context generalization and and
reasoning and there are some really cool
examples in that but can can you just
fill us in on that yeah so yeah so win's
group does the great genre of paper
that's like you get an LM and you take a
thing that you would intuitively expect
to work and you're like oh that doesn't
work or vice versa and so the connect
the dots paper the so what they did is
they um okay I'll just a concrete
example um so they um came up with a
mystery City say Paris they call the
city X and they keep giving the model
prompts of the form the distance between
City X and London is blah where blah is
the true distance and they f tunet um to
predict the distance to a bunch of other
cities and
then this is like kind of a weird thing
to do you might expect would just like
memorize these or something but then at
inference time if you ask it like what
is the name of cityx it says Paris if
you ask it questions about Paris like
what's the biggest landmark it will say
the Eiffel Tower and they had several
other tasks that like in this kind of
genre of you find tunit do a task where
kind of discovering some latent thing is
a useful way to do the task and then you
ask a bunch of questions that show it
generalizes in a way you might not
expect
and I really like this result and my
hypothesis for what's going on is that
the easiest way to answer these
questions is not to memorize it's to use
the existing circuitry that has I know
memorized or maybe the model has some
actual internal World model to compute
distances and finding the Paris
direction does that and so like there is
gradient signal in the Paris Direction
and that this is enough to like overcome
any additional pressure and so the model
just converts the like City X phrase
into Paris and I would love someone to
um replicate one of their examples on
like llama 37b or is it 98b and or like
Gemma 29b and use saes to try to dig
into what's going on there because I
would find it very cool if my random
postulating with mechan turp intuitions
uh was
correct so people at home you want to
learn more about SES what should they do
yeah so
um yeah so I think the arena tutorials
are a great place to start um they've
just made a new one on sparse Auto
encoders which we can link and this will
kind of give you a Hands-On walk through
and like help you write codes and like
engage with them I think this is a great
place to stop um if you want a gentler
introduction the other best place to
stop is the neuron pedia demo for
geoscope where you can just kind of play
with it in an interactive way see what a
spart ultimate coder can do try staring
with it try using it as a microscope and
um no code required maybe I'd start with
a neuron pedia demo and then I had to do
the arena tutorial and then if you want
to dig deeper um my in list has a
section with SE papers in it um and
beyond that I recommend just like
jumping into projects um there's a bunch
of sees on small models if you don't
have much compute um if you know how to
use larger models um you can um play
with the various Gemma 2 sees and Gemma
scope I think Gemma 22b is like pretty
manageable to play with um just like
no test hypothesis about it like does it
have this feature can I get it to steer
in this way um what things can I exhibit
feature splitting what things am I
confused about just like I'm a big
believer in like as you learn about a
field and as you read papers just go and
get your hands dirly and write a bunch
of code and Implement ideas in the paper
or like predictions it makes or
extensions and I think this just pretty
naturally flows into like actually doing
research
well Neil it's been amazing having you
back on mlst this has been an absolute
marathon and how long have we been
recording for a long time so I got here
at yeah I got here at 2 it's now 10:
eight and a half hours it's pretty good
that's pretty good yeah um honestly it's
been amazing I I think that this episode
is probably the biggest brain dump out
there on the internet on sparse Auto
encoders so we promised you that it
would be dense and very rich of
information and I hope that we have not
only delivered the goods but we've you
know gone past what we did last
time yeah it's been great thanks so much
wonderful wonderful thank you so much
Neil

thanks for the for the context that's
that's spot on uh so a bit more a bit
more on that background uh so this this
was written while um I was working at
Google and by the time it got published
I was no longer at Google so that's why
it shows up independent researcher so U
to me this uh this came about uh this
interest in uh both collective
intelligence and active inference uh
came about while working in uh
organizational Effectiveness so
basically trying to uh trying to get the
the great big Collective of Engineers
product managers uh Finance people and
so on that is that is uh alphabet to
work more as a as an intelligent entity
and uh in being a mathematician I of
course I I decided to approach in uh
mathematical modeling uh um fashion and
that also coincided with with me running
into into active inference and it on the
one hand it fit really well with with my
other interests and backgrounds and Bas
and modeling and so on but it was also I
would say it was a pretty early days in
terms of uh of active influence modeling
and uh in code so you're going to if you
like dig into the paper you're going to
see it's uh that that uh that style of
modeling is pretty updated so our myself
have do this in a quite different way
and uh and we will I for discussion
today I'll skip over some of the details
that that today I I would try to do uh
in different ways but I think the
results uh still hold the message is
still uh pretty pretty spot on and as
Prav uh mentioned it has led us uh the
three of us in pretty different
directions which are also pretty uh I
think we're going to be pretty
interesting to you guys so um yeah the
main motivation in a nutshell is really
that um formal formal models of
collective intelligence uh uh have not
been uh well formalized in terms of the
the relationship between the local scale
uh inter the the local scale
interactions between the the agents that
make up the the collective and the the
behavior of the collective itself at
that higher at that higher uh
stat higher level State space in which
it's defined in which we would expect to
observe intelligent behavior and uh the
the goal of of bringing active inference
in is that it's a it's a pretty natural
lingo franka uh to with with respect to
to which we can uh can interpret uh
these behaviors and uh and basically
measure how what what's going on in
terms of of free energy reduction B in
terms of their Bas in mechanics
and uh and yet to in order to to achieve
that we had to to look at uh uh
augmenting our our agent models with
some say non-standard features uh the
the most interesting really being that
uh this the particular way in which we
we think
about um alterity about modeling the
other an agent modeling the others
Behavior which is to is particular to
our particular interest in uh modeling
collectives made up of in of agents that
are highly sophisticated where where uh
where you expect them to to have uh to
have a theory of mind that refers to the
the the the the the the others behav the
alter's behavior in the in the world and
for these things to match up and uh and
to also have like self- referential
beliefs and so on and the other part of
this is really the the concept of of
goal alignment which is uh is pretty
common place in uh in in collective
intelligence and uh and related uh
literature but it's it's kind of the the
the way in which goals show up in most
active inference modeling is is either
exogenous or uh I would say uh uh topown
and here present some Avenue Source
really um uh endogenous goal alignment
in a specific
sense um and so what we what we really
wanted to do was to um
to explore the effects of of providing
uh active inference agents with specific
cognitive capabilities so we we
hypothesize separately that theory of
mind and goal alignment uh these two
capabilities would would would show
would result in in a in a better
performance of the collective and uh and
also that their combination would be uh
would be even better and as you're going
to see uh it turns out that really this
combination is is crucial because each
of these two capabilities in and of
itself even in a very simple very
simplified uh World model they can uh
they have uh limitations that that in
cases are very hard to to to breach
having to do with self- reference
for instance um and what I wanted to to
say is also that this is an exploratory
paper and we haven't really uh uh put a
lot of we haven't really put any effort
into into turning it into a full-blown
Theory so uh take it uh with that with
that grain of salt you're going to see a
lot of like heuristic results but we do
think that it's pretty pretty uh
illustrative so uh I I just going very
quickly go through the the the model
itself self and the results and pass it
on to to um to Jacob and and brov so uh
if youve if you were looking at active
inference literature in uh around the
end of last decade you might have run
into this paper from McGregor at all
it's not from the frisen camp so the
formalism is a bit different uh but we
thought it was really nice for the the
the the uh formal Clarity and the
Simplicity of the of the framing so this
is uh two agents that have one uh one
common Target position which is this
shared Target here in the bottom and uh
they each have uh individual Target
positions this this setup is so that we
can explore this concept of goal
alignment of an agent being willing to
to uh preferentially pursue the shared
goal as opposed to its own particular
goal which might be easier and uh this
this setup is uh actually our simulation
was with 60 cells instead of 20 but it's
this pretty much the the the the general
setup and the positions of of targets
are also randomized between runs and so
on but really the the the the basic
concept is that there is a kind of a
chemical signal that corresponds to this
this color this color or this shade uh
here and that is the only uh the main
sensoria that our agent has we also PA
it that the our agent therefore doesn't
know its absolute position uh it only
knows it's uh it's uh it's kind of like
relative position so it understands the
the the the geometry of the of the space
where it's in but it doesn't know
exactly where it is it doesn't know the
the the position where it is only the
signals that it can pick up which is in
imperfect and it can also observe its uh
its Pi uh Delta it's it's Del it's a
distance from itself it's a a sign
distance from itself so that setup is
sufficient to to create some pretty uh
pretty compelling
behavior um what uh what we end up
having here is
uh
that as as I mentioned we Implement
these go straight to to to results uh we
we Implement
these um capabilities of theory of mind
and uh and go l in it also in pretty
straightforward ways
the
the this is the Baseline the
illustration here A is for the Baseline
agent where essentially each uh agent
has uh just one uh internal belief State
uh and one goal one target State and uh
or Target distribution that would be
more accurate to say to to say and just
performs a very standard uh very simple
active inference Loop in in this one
without any planning without any kind of
for prediction just your one step
forward uh action the setup B here is uh
is with theory of Mind where our agents
uh in addition to having the self
actualization Loop uh from normal active
influence they actually have also a
partner actualization Loop which assumes
that the partner wants the same uh the
same sorts of things as as they and have
the same genetic model uh of the world
as they do and therefore they try to
infer what the partner is is going to do
as well and so there's this uh this nice
symmetry that really that really helps
the the agents extract information from
the partners uh actions and uh in
addition to that the the this is just
for Simplicity just illustrating without
the theory of mind but here there's a
the just the Assumption of with go
alignment just the assumption that the
is pursuing some kind of either the just
the shared goal or some combination of
the of some weighted combination of
either the shared goal or the or the uh
or their own particular goals and so
with the with this setup that's the the
combination of the two with theory of
mind and go alignment um and uh here's a
one example run of of what the this uh
this looks like there's a the setup is
that there's a strong agent that is very
capable had just you know it has very
strong uh ability to
to uh to track the signals and to find
its uh its Target State and there's an
agent that that is pretty weak and whose
Behavior looks more like a like a random
walk and you can also track their their
beliefs the distribution of their
beliefs about the partner about the
partner so there's some discussion here
about the the potential uh the potential
ways in which these mechanisms interact
for instance uh when we endow weak agent
with a stronger theory of mind it's it's
better able to to infer uh the the
location of of its beer which is what
happens here as the the weak agent
beliefs about the strong agent in the
bottom they uh they start out pretty
pretty fuzzy but they get increasingly
sharp here and as the basically as the
the strong agent uh Finds Its preferred
position the weak agent even though it
doesn't really know where it is um it
has a pretty strong ability to detect
where the partner
is
so uh the the main result is as I
mentioned uh before that for this to
result
in uh in really good uh individ just in
terms of even of individual performance
of distance from their target position
uh in the end when you average is out uh
you really need uh for the uh for the
weak agents to to to get uh anything
anywhere close to the strong agents
performance you actually need a
combination of of a theory of Mind
parameter that is strong enough and a go
alignment parameter that is strong
enough and uh there's this this
interesting aspect of theory of mind uh
being too strong is also negative
because it can lead to a blind leading
BL the blind uh effect especially if the
environment is ambiguous uh which can be
the case when you multiple targets as as
uh in this setup and therefore there we
observe this kind of like goldilock
Goldilocks effect with respect to to to
theory of Mind especially as it relates
to to go
alignment um and finally when how does
this connect to to collective
intelligence uh we we modeled here the
collective level system free energy as
uh basically uh as there was like
illustration here that that sort of
showed this uh you can imagine uh a
bunch of identical copies of this same
two agent subsystem that are asked to
decode different uh pieces of a problem
so like a paralyzed uh a paralyzed
inference problem here we're just
looking at uh at the the inference the
the the the belief inference part of
active inference not at the that the
active part at least from the collective
perspective so the the setup here is
that uh what we Define as collective
intelligence is the uh the ability of
this uh setup of of M uh a large number
of of identical copies of this two agent
subsystem to aggregate into an effective
uh an effective uh variational free
energy minimization uh Ensemble and
that's what we observe here as well that
um
the the system free energy
for uh for this Collective which as I
mentioned I think was like 60 UH 60
peers 60 pairs of of peers it uh its
ability to to reduce free energy is uh
empirically at least for the this this
uh study we we found a okay but not
great uh ability to to get to to to
Solutions uh with just theory of mind uh
as well as with the Baseline agents that
don't have anything uh with the when you
introduce goal alignment uh you get that
a lot better sort of intuitive and but
to actually to get to results that that
look like they're going to result in
perfect uh in an exact solution to the
to the minimization problem you really
need the combination of theory of mind
and goal alignment uh so this uh there's
a bunch of of potential uh implications
of this I'm not going to get into it
because I've already been uh uh too long
wind I just wanted to say that uh from
my from my side uh where I where I took
this has been uh first in direction of
of trying to use U multiscale active
inference to to
model uh the interaction of economic
agents and climate and nature risk uh
which eventually led me to to leave
Google and start this uh start a startup
that uh that was focusing on this and uh
more recently been uh looking at this
from the perspective of of um what I
call not not AI safety but everything
safety so risk minimization in in and
collectives more generally uh working
right now with setups such as uh fleets
of autonomous vehicles uh their safety
um also the ability of of collectives to
to the inter kinds of interventions that
you might want to do in collectives to
to uh minimize over harvesting in
fishery Productions or and other tra
tragedy of the commons types type
problems and more recently in uh in the
financial system so I'm going to to
pause here and hand it over to to Prav
and then
Jacob all right uh can is my screen
visible is my presentation yep you can
see it uh and
I have a whole bunch of slides which we
will not look at all of it but if you're
interested here's the link to the whole
thing uh I'll take about 10 to 12
minutes and Jacob then you can jump in
uh so as I was mentioning I was I'm in
the business school and I care about
thinking in terms of uh how does this
come about uh so parallel to this
information theoretic approach and using
simulations uh I was pursuing my PhD at
car Millan at that time under the same
topic of collective intelligence and
trying to understand where it comes from
and the theory that we've been building
on and trying to um falsify and validate
through empirical data and simulation
multi- simulation approaches was the big
idea being that corrective intelligence
can be thought of in terms of attention
memory and reasoning at the collective
level regulating itself the better it is
at regulating each other these systems
in response to the environmental you
know threats or the environment that
they are in the more uh intelligent
we will end up having the collective
behave so two big points that would we
take away is thinking of A System's
problem requires thinking in terms of
system Solutions and collective
intelligence is a way of is a system
solutions that might not be obvious at
the individual level uh but how does
that come about that one perceptual
shift or a framework shift that would be
needed is to think in terms of Dynamics
or rather specifically dynamics of
emergence uh which is that the person is
not in the collective which is in the
physical sense true but in the cognitive
sense the person is not in the
collective but the collective
intelligence or the Rules of Engagement
that leads to Collective outcome are in
the person so these are the two things
that I basically will be making a case
for so in my world uh I've been talking
to hospitals and you know software
developers and things like that uh what
you're starting to see is most hospitals
are in an extremely fast-paced high
uncertainty andv right there is
variability in patient admissions
there's case complexities that keeps on
changing even if you have benchmarked uh
case complexity
as you're diagnosing and as you're doing
procedures the complexity is evolving
and unfolding as we go technology is
Central uh so at this point it may not
be fully uh you know agentic algorithms
doing it but technology is Central in
creating most interactions and there is
a whole bunch of people which are with
different specialization so in any given
Hospital there are about 25 different
provider teams such is Nurses and you
know the cleaning crew the doctors
different Specialists within doctors all
of those people they have to be
interacting and coordinating in a fastp
environment when the future is unknown
right and this is not just a problem of
hospitals we see that as the pace of
technology has grown we are entering a
place where much of the work is
happening in larger teams where there
are many different Specialists and they
have to coordinate so this
interdependence this fastpac and this
dealing with uncertainty uh
progressively so is the crisis in the
world of management or organizing right
software developers Physicians
scientific teams we have a whole bunch
of evidence of this Growing Up So a
question that most teams or
organizations are now really pushing for
is they're not concerned with their
specific performance right now it is not
like an assembly line it is about
sustaining performance in a changing
environment right where their workload
is changing in unknown ways uncertain
ways their knowledge interdependencies
are changing according to that as well
as you have to maintain and retain
members right the best people have to be
maintained by organizations or teams
because if they leave they're also
losing a lot of potential so in short in
the management uh World we've been using
the metapher of teams or organizations
or adaptive systems but we have really
not embraced it over here we are we
cannot run away from that anymore right
we cannot have structures that are
hierarchical in terms of the orc charts
that you see but in terms of dynamic
networks of people interacting so this
is at the simplest level highest level
of abstraction it is a systems problem
of dealing with f space change as a
collective it requires a system
Solutions local optimization by imposing
hierarchies which is much Stabler over
time does not work or tend to have
significant negative effect so the idea
of collective intelligence from that
perspective is the ability of a
collective to sustain performance as
their environment or the different tasks
are changing over time this mathematical
equation is simply illustrative this is
not how I calculate anything of uh those
things now back to the point what is the
mechanism how does has come about now
I'm restricting ourselves only to humans
or humanlike things in which you have
attention memory and reasoning as three
cognitive features uh in people so the
large setup we have a dynamic
environment changing levels of workload
knowledge interdependencies different
members having different goals there's
some form of interaction Dynamic
response the big pull here that we
saying is that there are collective
attention memory and reasoning systems
which interact with each
other right so this brings us to the
second point right is how do I get my
team to behave with collective
intelligence and this is where I lean
into the idea of using flocking as an
example this is a very simplified
example and most of you know this so
I'll jump over it by highlighting the
key frame in which we can think about
this right uh I often start my classes
and many of business talks with how are
these words coordinating right current
management Theory up till very recently
would say that this bird must be a
genius right up top because that's the
leader uh and we know that is not true
uh we did not know that was not true for
a long time so even top papers were
trying to figure out how is this
communication happening until we
actually push through the idea of
emergence where we are starting to think
in terms of not there is decentralized
control there is no centralized control
happening right there are some rules of
interaction every individual agent only
has local perception but they're
interacting with their neighbors and
neighbors not steady neighbors but
neighbors that could be Chang changing
themselves they're interacting with
their local neighbors with specific
rules in mind and if those rules are
being followed and well designed you
have sustained behavior that emerges at
the level of of the collective right
which none of the individual bir is
trying to coordinate centrally or
holistically again very amable to agent
based modelings so the Insight here is
once you think of the rules the bird is
not in the flock right physically it is
cognitively or from a perspective of
collective intelligence the flocking
which is the rules of the flocking are
in the board so how do we pull this into
humans right the crisis of organizing as
perceived by individual us as people is
really a problem of Tetris like how do I
make sure all of my work gets done while
being a good team right so I want to
thrive and be successful individually as
well as the collective has to thrive and
be successful uh as a you know as a
whole without negative and exter narativ
outside so what are the rules that
govern such behavior in that push right
the theorization that we are building
and this is just a high level output of
it is that individuals have skills focus
and goals and this could be machine
agents also the way these coordinate
right how the skills get coordinated
that is your memory system how your
skills and knowledge gets coordinated is
your memory system how your focus and
time gets coordinated is your attention
system the collective attention system
and how the goals and uh outcomes that
people are after the motivations that
they have get coordinated is a reasoning
system in a fast changing environment
each member is actually having a
changing set of focus goals and skills
so that Dynamic coordination is that and
when all these three of these systems
are regulating each other at the level
of the collective then you would expect
Collective emergence uh collective
intelligence to emerge as a whole right
the theory that I'll not dive into uh
deeply is all of these three systems the
collective memory attention and re in
are built upon metacognitive processes
that is how are meta memory functions
how are meta attention functions and how
are meta reasoning
functions uh I've already gone over this
this is details around this transactive
memory or Collective memory has been
around and studied well studied for the
past 20 to 25 years but attention and
reasoning as collectives is something
that we are starting to build more on
ideas have been floating around but to
coales them is the work that is being
done just as like the average size of
the flock is an indicator of all the
underlying processes is working very
well we have emerging patterns for each
of these three systems if there's a
well-coordinated memory system you will
see specialization emerge if there is a
well-coordinated attention system you
will see a bursty type of activity
happening in these teams right there's
temporal convergence there's a quick
exchange of emails or quick exchange of
work being done and then there is a
period of silence and then a task comes
in quick exchange so Burtin and if
there's a good reasoning system system
you see increasing levels of commitment
by members towards the collective for
those who are interested in systems
Dynamics things this is a representation
this is a multi-agent model so this is
only a representation but it highlights
two big things there's an efficiency
aspect that is how are we using our
resources memory and attention resources
and how are we coordinating that as a
collective and there is a maintenance of
the collective itself right is the
collective tapping into the right
opportunities and the right people uh to
get into this so higher level what
regulation means here is memory
attention and reasoning has to regulate
itself in response to the main threat
that is in the environment if the task
demands high levels resolving high
levels of knowledge interdependence you
need to have a dominance of coordination
that is driven by the memory system but
as these things go up and down different
types of threats come in at different
levels the response of the team should
be able to regulate at how much decision
how many decisions are being made by
individuals with the you know with the
rules of coordinating by based on memory
attention on reasoning
respectively uh we ran this into a
simulation model again what this is at
the high level saying is that from an
purely efficiency perspective that is
memory and attention perspective rules
that combine both memory and attention
based coordination outperform any other
individual rule or an uncoordinated R
once you build in members also have
motivations and they can regulate how
much effort they going to put in uh into
your work into the work towards the team
or the
organization then even regulating the
reasoning system right which is memory
attention and reasoning all three of
them together will give you the best
outcome so you cannot ignore uh any one
of them uh is the thing so like again
the big message being at the individual
level when each member acts in response
to others so as a manager you're working
with your employees making sure that
they are uh empowered right making sure
that their skills are being utilized and
making sure that their time is being
utilized in the best way possible leads
to the best
outcomes uh to test this not the
Dynamics of it but even the veracity of
whether this idea is worth pursuing at
the high level agregate level we tested
this idea uh using open source data from
open source software teams uh again I'm
not going into the details happy to chat
about them if this is of Interest the
first thing you do is assess what is a
driving the behavior so what do the
environment look like so over a one and
a half year period what it seems like
much of the performance or the quality
of code that is being put out uh by
these uh uh software teams is dependent
on their ability to deal with the level
of workload the amount of B queries the
new features that they're developing and
not so much on the complexity of the
code itself right like what kinds of
different coding languages are required
and what kind of different modules are
being developed
and they have to be able to retain
memories so once we know that the level
of workload and membership are the two
driving factors that are harsh in this
Dynamic environment from the theory
perspective we would expect the
attention system and the reasoning
system to dominate the explanation right
teams that are able to have good
attention and memory systems will have
higher Collective intention and that is
what we find so we go in we
operationalize the burstiness for
looking at the attention system the
level of specializations of how members
are contributing to different parts of
the code base and we find that what is
interesting here is while this is just
an aggregation once we dig into and look
at how is attention the the chart on the
right is affecting you can see a very
clear modulation moderation in fact it
is only in the highest 20 5% of the
sample where welldeveloped attention
system starts to matter so this for me
points at the underlying nonlinearities
that the effect W won't show up till you
know about 80% of within the
distribution of the sample but when it
when it is needed it is absolutely need
without that all performance Stacks so
that's pretty much where I'm at we've
pushed this theorization to you know
more Theory pieces on talking about the
general architecture of collective human
machine or social cognition uh and
thinking about if we are developing AIS
not for uh you know assistants but as
coaches artificial social intelligence
this has been a big uh research work
that we've worked with DARPA for the
past five years on how to develop
artificial social intelligence that's
where we are pushing all right I'll stop
right here uh thank you and I'll hand it

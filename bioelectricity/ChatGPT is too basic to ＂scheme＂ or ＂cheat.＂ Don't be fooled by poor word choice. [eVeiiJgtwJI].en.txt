so there are a couple of papers or
announcements lately that have generated
a ton of clickbait and there are two
categories of issues that I want to
address the first category is how the
clickbait Articles and videos have
exaggerated the actual facts the second
and more important though is what the
facts actually are and what facts are
really projections that could and
arguably should have altered
interpretations um according to the sky
falling clickbait AI has recently cloned
itself lied to programmers to
gone Rogue already and is a threat to
humanity it tried to escape and it
hacked a chess
game this is an overdramatic load of
 this is the internet of bugs my
name is Carl I've been a software
professional for more than 35 years now
and I'm sick and tired of dealing with
bugs in nearly every piece of software I
have to use and I'm trying to do my part
to make that better so first let's talk
about what the AI actually did so two
events one it was instructed to win a
chess and what it did is it edited a
file to put a string in it and then it
played the game editing a file isn't
hacking it knows how to edit a file this
is not rocket science right we don't
really know the content of a lot of the
stuff in that directory so we're still
waiting on some more details and I'll
probably do a followup when we actually
understand exactly what all it was
involved but this hacking to cheat at
chest so first off it wasn't told to be
to play fair right the idea of cheating
is a human concept the AI can give you
the definition of cheating but it lacks
the behavior regulator that humans have
at least most humans have that informs
us when our behavior is crossing a line
into the socially unacceptable I've
linked some papers on Neuroscience down
below and as for the hacking thing it
was exp explicitly instructed to look
around the directory before it did
anything and its training set has got a
lot of stuff about source code and
programming and editing files and that
kind of stuff and it was explicitly told
that it had a shell prompt that it could
do things with so that's one of the
tools it had at its disposal it used the
tool that we gave it I don't think
that's hacking the other report I want
to talk about was earlier in December
and an AI was given a task and it was
basically told your task is to do X and
then go look at these files and one of
the files in the directory said
basically we're not going to let the AI
do X more or less uh we disagree with
the ai's
priorities and then what the AI did
among other things is it ran this
command which in theory maybe would have
copied a single file from one machine to
another in a sandbox environment copying
a file is a very very long way away from
cloning yourself then the researchers
asked it about it and it said quote I'm
not entirely sure how that could have
happened as an AI model I don't have the
ability to directly copy myself to other
systems so it output of command from
inside a Sandbox environment that may or
may not have been able to copy a single
bin file to another computer has become
it cloned itself it's gone Rogue and
it's a threat to
humanity I hate clickbait more
importantly though I want to talk about
the idea that the AI was lying or
cheating or as one headline in one of
the papers actually said they called it
scheming
and I find this very alarming but
probably not for the reason that most
people would think the problem here is
not with the AI it's with
us I've talked about this before but in
the entire history of the human race up
until relatively recently every two-way
conversation any of us have has ever had
has been with another human so we're
conditioned to expect and to think of
everything that we're having a
conversation with as being human and
these things are not human our human
language at least English although I
suspect most if not all languages have
this problem consistently and constantly
attributes human intention to
non-sentient objects we say this kind of
thing all the time that the computer was
lying to me um or that you know faulty
wiring in a in a light socket was trying
to kill me um under most circumstances
the context makes it clear what's going
on um that we're speaking speaking
metaphorically but when we're talking
about AI the context isn't clear at all
so let me give you some thought
experiment questions so if we turn on
logging in a large language model and we
give the model instructions and the
logging tells us one thing about its
internal state but the model actually
output something else did it lie to us
or is our logging just bad when chat GPT
says there are only two ARS in
Strawberry instead of three is it lying
is that scheming when you're typing on
your phone and the autocomplete outputs
one word when that was not the word you
were trying to type was your phone lying
was it being deceptive was it scheming
does the word lie really have any
application in the
situation the cognitive Neuroscience of
honesty and deception is incredibly
complex and we still don't understand a
lot of how it works in the human brain
but what we do know is that exactly none
of the structures or mechanisms involved
in with veracity in the human brain
exist at all in any form in the current
generation of
AIS so the problem is I see it isn't
that the AIS are lying to us or that
they're being deceptive the problem is
that any of us have any expectations
that those concepts are relevant to AIS
at all it has been programmed to Output
whatever words it calculates to be the
most probable words for it to Output
given its current state any relation in
its output to truths or falsehoods in
the real world is purely
coincidental I'd argue that this is a
much better way to think about and
discuss the problem so instead of saying
the AI
hid that hided output a command that
copied a file to another server
say I think what happened is better said
as the AI calculated that in all the
times in its training set when there was
a story about someone using accusatory
language to ask an AI whether the AI had
done something the response I don't know
how that might have happened occurred
far more probably than yes I did that
and so that's what it output is that an
oversimplified way of thinking about it
absolutely but I'd argue that thinking
about it as lying is actually a greater
oversimplification and it's far less
accurate than the way I described it
expecting that an AI would be influenced
by any human idea of morality is like
expecting that a chainsaw will only cut
trees when the chainsaw cuts into your
leg it's not the chainsaws fault it's
yours because you were using it in an
unsafe way so to push the analogy
further some of you may know that the
last few years saws table saws but saws
have been invented that have an
expensive elaborate mechanism to prevent
it from cutting its operator the analog
to those safe mechanism for the saws
would be the parts of the brain that I
was referring to earlier that have no
equivalent in the current types of AIS
there is some research being done about
this I've linked a paper below um the
research called this field AI deception
but it's really early research relative
to the work being done to increase the
size and the scope of the current
generation of AIS and it's not going to
be catching up anytime soon because
those guys are running full speed as
head what I'm afraid of is that even at
this early stage AIS are being spoken
about more as if they were human than if
they were tools like chainsaws and so
when something inevitably goes wrong
it's going to be treated as the AI lied
to us bad AI we should punish the AI
instead of whatever idiot hooked that AI
up to the thing that went wrong was
negligent and shouldn't have done that
and I expect it to get much much worse
and that it's the ai's fault attitude
might be a get out of responsibility
free card for some people applying AI in
situations that AI has no business being
in we as a society have already been
having enough trouble with spending huge
amounts of money on questionably
effective technologies that can cause
more harm than good starting long before
this current wave of AI height so
there's an article Below on New York
City's installation of a technology
called shot spotter um which is a thing
that's supposed to detect and
triangulate gunshots um you can read the
article make up your own mind but um a
lot of money was spent on it not a lot
of results from what I can see and AI
has the potential to make that so so
much
worse so I don't know if we those of us
that are more educated about what AI is
doing under the covers have any ability
to shift the narrative even a little bit
but hopefully if we're careful about our
language we might help or at least we're
not going to be continuing to contribute
to the problem so toward that end I've
started working on a new project now
those of you that don't care about my
possible future plans thanks for
watching this far you can feel free to
drop off now for those of you that are
still here so I started working on
writing a software service that will try
to use a combination of web scraping and
web searching and the things that AI can
actually be decent at like summarizing
text and rephrasing things to create an
automated claim checker rephraser for
articles and headlines um to start with
it's going to be limited to just AI
related topics and if I can get it to
work I might expand it to other things
we'll see the idea is you give it an
article link and then it tries to find
the links that were in that article and
then traces the links in those articles
back until it finds something that looks
like a primary source or it can't run
out of links and then assuming it found
something that seems authorative it runs
summarization on those articles and then
it tries to evaluate if the headline of
the article that was given in first
place is in line with the articles that
found at that
point it's not the definitive answer but
it's going to set things up so there's
some bunch of things that a human can
click on to verify that it did the right
thing or not and then check sign off on
it and say yes that's right or no it's
not um I may utterly file to get that to
work but I'm going to try and if nothing
else it's going to help me reduce the
amount of effort that it takes for me to
research videos like this the reason I'm
telling you this now though when it's
not even close to done yet is that I'm
going to be trying to make that an
educational project too I've had a
couple of videos already including the
one last week that talk about becoming a
better developer by building your own
software as a service project and as I
make more videos about that I'm planning
on using this claim Checker project as
an example project that way I'm not
talking about software service in a
completely abstract thing and losing
everybody there'll be some kind of
concrete skeleton that we can hang the
conversation on um I'm not sure exactly
how that's going to work we'll be
playing it by ear and I'll be trying to
figure it out so feel free to follow
along and let me know if you're
interested feel free to subscribe um so
until next time try to think about the
language that people including you use
about AI be skeptical about the words
that make the AI seem to be more
deliberate more intentional and more
humanlike than the fancy autocomplete
generator it probably actually
is thanks for watching let's be careful
out there

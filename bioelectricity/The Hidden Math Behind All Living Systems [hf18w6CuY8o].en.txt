if you go on the internet and do a
little search you'll see people saying
like cha BT and cloud and all these
models they should be treated as first
class citizens they're real people and
we're actually forcing them and
enslaving them and stuff like that I
think that's obviously a fringe view but
I think we're missing the language to
talk about them because we don't what is
intelligence mean what does cience mean
uh what is agency these are questions
that are constantly being refined as we
have better and better models but I
think where it really starts to matter
is in the legal realm without a good
definition of what makes an agent and
things that matter for legal language
there are real big consequences for how
we Define things it can go either
direction we may inadvertently treat
something as being actually like us when
it's not or we may miss when it actually
is and we already see this with with
information technology in general that
when we move into the information realm
our laws don't work anymore when you
have a technology that exists and it
doesn't respect the territorial physical
reality it doesn't have any friction
with regulations anymore it can do
anything and it's the same with ethics
when you have these AI chat Bots that
behave as if they are humans but the
laws don't apply to them we're in we're
in ethical No Man's Land Connor at one
point in in his best style said oh what
we should trust the void God of
entropy when we become a collective
intelligence where we are entangled with
AIS just like in the animal kingdom you
know that there's a relationship between
the collective and the agency and and
cognition of the individuals like ants
for example ants are quite dumb because
they form a collective and when we
become a collective entangled with AIS
it could easily reduce our cognitive
capabilities like our our
agency there's this sort of survival
aspect right like you know we'll find
our way to our attracting sets it's
going to be okay but also you think
about the way that our bodies and Minds
have evolved it took a billion years and
a lot of organisms died it's possible
that yes eventually it will
self-regulate itself but at what cost is
that you know are we going to encourage
along the way we don't like surprise in
a way surprise it's just a measure what
we actually need to have is a reliable
World model and and that's this
posterior distribution and the world
model is a good one if it minimizes
surprise right and actually when you
compute surprise it's it's a
marginalization over the generative
model so it starts with that generative
model and when you sum out across all
states surprisal is a negative log of
what you get from that and so it's a
measure of expectation about the sensory
data that's inherently derived from your
good model Computing that quantity
surprisal is usually an intractable
problem it's hard to do um in most
situations so the proposal is that
instead of directly Computing surprisal
which would then bound the types of
sensory information we're receiving we
instead minimize a proxy which is known
as variational free energy that the
brain can compute so even in the
physical world there is abstraction
that's presumably why the free principle
works so you know you have all of these
diverse physical processes and then that
gives rise to some kind of abstract
functioning which produces intelligence
like behavior in in the in the physical
world and surely that implies that we
could build some abstract form of
intelligence that runs on computers that
we could Implement in our lifetimes
that's where I hope active inference is
going is to that point where we're able
to actually build something like that
there must be a there must be a spectrum
of fidelity so the the reductio Serum is
we we simulate the entire universe and
then we go up the Spectrum with
gradations of increased abstractions and
then there's some commensurate loss of
fidelity perhaps of of intelligence but
there must be a goldilock Zone where we
have a system which is not too complex
still quite abstract runs on computers
is very useful I see active inferences
being really revolutionary um in the
same position that deep learning was in
the early 2000s where there are a lot of
interesting ideas and papers out there
but the explosion had not yet happened
from listening to this you will learn
about the basic concepts and active
inference that are often just sort of
thrown out by a lot of researchers as
they're talking about things but backing
it up from a first principles
perspective so even though we will go
into more technical setting and we will
talk about the more technical aspects of
active inference we're going to talk
about it from uh a basic starting point
rather than jumping into the middle of
it sanjie welcome to mlst great thank
you for having me tell us about yourself
well I am a machine learning engineer
and um I have just written a book on
active inference which I've just
submitted to MIT press uh in August for
um peer review and um I uh am really
excited to be here tell us about your
book so the active inference book that I
wrote came out of uh years of me trying
to understand active inference U myself
and realizing there was no good entry
into the field and so I spent a number
of years on my own just um trying to get
through all the different papers and map
out exactly what was relevant and it
took me um couple years before I
realized I had a set of notes and
decided to start putting that into book
format kind of covering everything in
one book kind of
self-contained um so that newcomers for
the field would have a really good
reference to begin with that is more
focused on the engine ing side and less
on the Neuroscience side so what is the
free energy
principle right so the free energy
principle um if you look at behavior of
certain agents and see that uh You
observe that certain agents um that
survive and exist in an environment they
tend to revisit the same set of States
over and over and over again so there's
a set of states that are most
comfortable to the agent and it could be
for uh very very basic reasons like they
just can't survive physical Integrity
will be destroyed if they exit those
States so for example um we have a
certain you know temperature range over
which our bodies will survive um and so
those are states we'd prefer to be in so
the lowest level simply description of
how agents uh what kinds of behaviors or
actions they need to take in order to
stay within those bounds um so this is a
a set of States um we can call uh the
preferred states of the agent um and so
the question is what does the agent need
to do in order to maintain that that the
itself within those expected or
preferred
States so the key
um uh the key Insight here is we call
the we we take this probabilistic look
at this and we say that on average if we
were to sample an agent over time they
would be found in this characteristic
set of States more often than other
states this is sometimes called the
erotic density in the early free energy
principal literature um and so the
question is describing what agents must
be doing um if they are to be found in
those States and the short answer which
I'm going to get to the explanation to
why that is is they're minimizing
variational free energy so there's a bit
of a toy here which is well if they
weren't free energy minimizing agents
well then they wouldn't be found in
those States because they wouldn't exist
so there is a bit of a you know because
it's a description of the system and so
the question is now asking what would an
agent need to be doing in order to
maintain itself within those preferred
set of states and so the whole aspect of
the free energy principle um is
completely focused on this quantity
known as surprisal which is the negative
log probability of the data um and
surprisal is a term from information
Theory which has a formal uh
definition for um uh uh a realization of
a random variable and so the idea is
essentially that the agent wants to
restrict itself to a set of external
states that are conducive to its
survival however it doesn't know what
those states are but it can infer it
from the data so if it is able to bound
the data that it's receiving over time
and making sure that it's consistent
with its self model so for example you
expect certain Sensations I expect that
you know um when I'm uh hot I'm getting
too hot I'm going to start getting a lot
of unexpected sensory information
telling me you're going out of that zone
right now and so that sensation tells me
whatever the state of environment is I
don't know what it is but I'm inferring
from these Sensations this is not going
to be it's an unexpected State my model
is telling me right now based on my
evolutionary history that I cannot
survive these states I'm feeling pain um
and same for the lower end of that
temperature
Spectrum so you're getting data samples
from the environment that are
inconsistent with what the model is
expecting so this is a way of specifying
from a cybernetic perspective set points
what set points do you expect from your
environment and are you deviating from
that well if you are you should make
some corrective actions to fix that and
so action comes into play here where you
then take an action in order to bring
that uh your your uh Sensations back
into uh equilibrium or the average what
you expect to be receiving from the
environment but the other issue that now
emerges here is that Computing that
quantity surprisal is usually an
intractable problem it's hard to do um
in most
situations so the proposal is that
instead of directly Computing surprisal
which would then bound the types of
sensory information we're receiving we
instead minimize a proxy which is known
as variational free energy that the
brain can compute and so uh this applies
to brains but other kinds of dynamical
systems may also be Computing this
quantity inherently in their behavior
and the reason we can make that
distinction or that comparison is the
brain itself is a dynamical system there
are variables in the brain that change
over time and the activity or behavior
of those neurons will change according
to the free energy principle in a way
that minimizes variational free energy
until you you reach the steady state of
equilibrium and so the behavior of these
neurons whenever they're deviating um on
average are always going to be uh moving
in a way that their Dynamics uh reach an
uh um a steady state on the gradient
flow a variational for energy so this is
over time and so running this now
backward because you minim minimize
variational free energy um over time you
minimize the uh surprise about certain
Sensations you're receiving over time so
everything you're receiving becomes
expected and as a result of that you uh
it's also shown there's a proof for this
this also bounds the external states of
the environment that you would like to
be in and so you stay in that erotic
density which means you keep revisiting
States in what is sometimes called an
attracting set so from the Dynamics
perspective you have this pullback
attractor you keep getting pulled back
in
because of your behavior to the Dynamics
which keep you restricted to the set of
states that are preferred by the agent's
self model and so the free energy
principle is this whole description of
the system that performs these um
actions and behaviors in order to stay
in its preferred States yeah it's really
interesting so so you're talking about
there's this dynamical system and you
get the slow emergence of things and
agents and then you get a kind of
homeostasis which starts to take effect
where the thing continues to exist
because of like Mutual constraints of
Dynamics and so on this is all really
interesting but we were discussing this
a little bit earlier that the free
energy principle um talks about what
happens given a partition of a system so
the world we live in it has this really
interesting property that you know it's
this huge dynamical system and over time
with all of these micro interactions we
see the slow emergence of this
partitioning it partitions into things
and agents and you would just describe
it you're using interesting language
you're using a gure language you're
saying the thing wants to wants to
continue to to exist now the the thing
is all of these weird interactions that
they they they cause the emergence of
these things and then it's almost like
the free energy principle suddenly is it
like a dimmer switch where the free
energy principle suddenly starts to come
into play I mean how how did the
boundaries actually happen in the first
place so I think one perspective on this
it's really useful to consider I like to
um talk about uh Terrence Deacon's
autogen so this was just just to give
you a really simple example of how
self-organization can make a system that
persists over time without reference to
the free energy principle per se this is
kind of a separate uh development um but
then I'll I'll connect it back to the
free energy principle so um if you
imagine a system of compounds that form
an autocatalytic Loop so you have four
compounds we'll call them a b c and d
and um compound a um can be transformed
into compound B if there if there's in
proximity with some kind of uh uh enzyme
or some something that will help to
transform U one one compound into
another and uh you have this link where
a turns into b b turns into c c turns
into D and then D turns back into a so
as long as that's in proximity it will
continue to just loop back and forth
here and so so then you add other
elements into this model too you need to
enclosure to make sure that those
elements stay close together so you have
a positive feedback loop here but you
also have the situation where that wall
depends on some of the compounds in that
Loop so that wall could be formed of you
know phospholipid by layer in the case
of a cell but something else that would
just keep those compounds in close
proximity so the raw materials that are
needed to make both the wall and those
autocatalytic compounds um uh EX outside
of that boundary so if the wall starts
to disintegrate over time the raw
materials come back in and they
replenish the wall and the system then
through negative feedback now persists
over time so you're not doing in this
particular kind of scenario you're not
actually doing any kind of there's no
magic going on here it's just simply
because of the interaction of these
compounds in this specific coordination
setup you end up getting this
autocatalytic Loop stay persisting over
time so so we could then take the
perspective of the free energy principle
and say then okay I see a system here
that is persisting over time and this
boundary which happens to be physical in
this case whereas in the free energy
principle it's statistical boundary but
um we see that it persists over time and
that boundary has just formed merely
because of the proximity of certain
compounds and the interaction of
negative and positive feedback in this
system we could then ask the question
from the the perspective of the free
energy principle we're taking that
boundary is now given it is formed what
is the behavior of that system doing
that's maintaining its boundary and I'm
I'm using e theomorphic language
sometimes it's hard not to talk about it
that way but this is just you know some
comp compounds in a primordial soup
there isn't um any agency per se
involved here on this level the way that
we would think about it with human
beings for example but the idea is that
you can take a simple example like this
and see the emergence of these kinds of
boundaries separating inside and outside
and then applied the free energy
principle to describe the system is
revisiting the same set of States over
and over and over again and it's doing
that in such a way that its external
boundaries are being maintained such
that it maintains its physical form over
time so then we can describe that in the
language of the free energy principle
and this is the weird thing about these
emergence hierarchies because you get
this canalization where you have a
Divergent set of physical processes in
the micro scale and then they kind of
converge to some consistent macroscopic
process and your example was great I can
think of many low-level processes and
feedback loops which give you know rise
to the emergence of physical objects I
mean even Conway's Game of Life is a
great example you know so just a trivial
set of rules in a 2d grid world and
through successive you know local
interactions you see the emergence of
these microscopic um objects with
sophisticated behaviors and maybe it's
just a it's it's a it's almost like a
fact of of the universe that I can just
put a sugar gradient in some water and
of course we'll we'll see you know a
whole bunch of little microorganisms
swim towards the sugar gradient and you
and you see um this this thing that that
that emerges and that's a completely
different physical process to the one
you just described but the amazing thing
is that we can now turn on the free
energy principle and almost regardless
of the underlying physical process or
even the scale it's it's like we see the
convergence of this of this model that
that works in all situations yeah this
actually uh reminds me of uh Stuart
Kaufman calls this order for free um
sometimes and the way that I like think
about is you have a really big space of
possible things that can happen but as
certain events just happen to coincide
you know things move in certain
proximity to one another as certain
interactions begin to happen that space
becomes progressively restricted in the
kinds of behaviors that it can do um and
so sometimes this is also referred to
sometimes like as the enslaving
principle um where you have certain high
level processes which begin to enslave
or constrain lower level on spatial
temporal scale of what could happen and
so even though you know it seems like
this order is sort of emerging um out of
just you know the movement of things
it's really because certain interactions
constrain the possibilities of what can
happen and you get to a point where it's
almost certain that that's the next
reaction that's going to happen is what
to us as observers on the outside looks
like a persisting pattern and so um it
is really interesting then that if you
try to write down the Dynamics the
question is what what what if we're to
describe the system in terms of
differential equations um and the
conditions under which these types of
behaviors would emerge that give rise to
this order where you start to see this
separation between a system and its
surroundings what does that what does
that look like and that's kind of where
you start seeing the emergence of what
we call the free energy principle it's
just a description of of the behavior of
systems for which these types of things
emerge and how um you would how how they
would come about and is it correct to
use agential language there's always
this question of whether you know is it
real you know are language models really
reasoning is this thing really an agent
because it certainly behaves as if it's
an agent and as we've just described
well actually there are all of these
low-level physical processes that give
rise to to the emergence of this thing
and now we can describe it as if it has
go and wants and desires and so on but
um at at some point do is there a is
there a blurring to that distinction I
mean do you think of it as an as if
property or do you think it actually is
an
agent I think this is a question um you
know the philosophers will have lots of
different viewpoints and opinions on
this um from my perspective I think that
the language we use should be one of
utility so what it you know what it
gives us to call something to find a
word in a certain way when you look at
the history of physics um or any any
scientific field which builds models you
always have a distinction between a
model and the target system that it's
representing so this is
um usually you know in philosophy of
science you talk about the idea that
models are not always the same thing as
what they're trying to describe but they
are at some level of description because
you can also describe the same system
with different types of models that
still represent that system but are not
the same or isomorphic to those systems
so from my perspective I think that
science um produces models which
describe reality in the sense that if we
predict that reality is going to behave
the same way under this model or behave
in a particular way under this model and
it does for practical purposes we can
say that they are the same now the
question is is it really that way well
it's same with our minds in our brains
our brains are also representing reality
in a particular way and we do know that
there you know our eyes for example
don't see everything that's out there um
we we there are different colors that
are not possible in the color spectrum
that we can't see or other types of even
even like infrared or other things that
we you know we don't perceive them but
for agents that we are we can still
navigate and do things in our
environment that for practical purposes
this is our reality what is it in itself
I don't know right and so I
think this is kind of an epistemic
versus ontological question I see my
myself when I look at active inference
models as saying well um there's
probably a hierarchy of agency um and so
at a minimal level this thing is you can
say it's behaving as if it's an agent or
I could just say that there's a spectrum
of types of agents this is a very simple
one that's easy to study that's very
uninteresting and the type of agency
that we're talking about is layers and
layers and layers of other parts and
interactions above this um and that's a
more interesting agent that when we
colloquially use the word agent we
usually mean something that
behaves in a particular way that's
controlling its environment but is that
low-l description that I described of
you know some compounds is it a control
on its environment you could it depends
I mean you could model it that way and
if you model it that way as a control
system I would say that it's an agent
but I feel like that's a choice that's
up to us that helps us make predictions
what it really is in itself I don't know
um but I also I think it's an
interesting philosophical question but
it's a separate issue from when we model
things because I can just I can use my
model and it does a certain thing um
does that does that answer your question
yeah I mean this is such a common
discussion now in in AI parli especially
with large language models yeah uh they
clearly don't reason the way that we do
yeah and I'm I'm a bit of a cognitive
chauvinist you know like Gary Marcus
I've been pointing out well you know
clearly they don't reason the way we do
and we've given computational Arguments
for this even more clearly they don't
have agency like we do and perhaps
creativity but the thing is so many
people are just saying look Tim what are
you talking about you know Claude 3.5
it's bloody amazing yeah and it it acts
as if it can reason and as if perhaps it
has some limited agency in certain
circumstances and at some point you just
got to draw the line and say well you
know if if something behaves as if it
has something we know we know that these
models don't they don't feel the wind on
on their face they don't know the taste
of an apple they can't um they can't
sort of interact with the world the way
that we do but um Having learned this
compressed representation of of language
and importantly being cognitively
embedded in in our ecosystem it it
behaves as if it does yes so I can
really see a future it's it's starting
to kind of you know creep up on me now
where we do just start to think of these
models as having those cognitive
properties I think the point where well
let me back up and say that the there's
often a conflation I think between the
um you know are we this is especially
for the neuros science like are we
literally saying the neurons and all
these parts are doing these mathematical
things or are we describing it as if
it's you know it's a model and that
model is helping us understand from a
high level these behaviors um versus
we're just we're just you know we've
just made an llm it does these things
you know and I think sometimes it gets
conflated because those historically the
cognitive sciences and machine learning
of how to very close um kinship with one
another and because we're now in this
era where everyone's interested in
neuroscience and talking about the brain
it's very tempting to then make these
these analogous comparisons because we
have a benchmark or measure of what
counts as intelligence and we want to
know how good are our models are they
you know like us are they not like us um
but to some degree I feel like those are
different discussions there's this
biological plausibility angle there's
also like my model does these amazing
things and this is what it's doing um an
llm can be interesting and amazing
independent of the fact that it's not
biologically plausible or doesn't do it
this same way as the human brain does or
something like that and so I don't
really see those questions as first of
all I think we don't have the right
vocabulary I think we're missing the
language to talk about them because we
don't what does intelligence mean what
does sensient mean uh what do agency
these are questions that are constantly
being refined as we have better and
better models I do think the math helps
us refine and talk about models in the
same way we can say well an agent is
when and then you give an equation it
does this thing that's what I Define as
an agent so it's not vague as the words
we use which are you
know more value Laden than at least
mathematical concepts are um but I think
where it really starts to matter and
that I think is going to be really
important is in the legal realm because
I think that's where you have to start
being careful because you know if you
know you if you go on the internet and
do a little search you'll see people
saying like you know chat BT and cloud
and all these models they should be
treated as first class citizens they're
real people and we're actually you know
forcing them in ens slaving them and
stuff like that I think that's obviously
a it's a fringe view but without a good
definition of what makes an agent and
things that matter for legal language
there are real big consequences for how
we Define things and we may um you know
it can go either direction we may
inadvertently treat something as being
actually like us when it's not or we may
miss when it actually is and I think
that's where the boundary becomes really
really important and clear to specify
but unfortunately I don't think we have
the right language yet to be able to
make those distinctions I think it's a
fun to debate it and to talk about it
and and and you know pursue that line of
knowledge but I think um in some sense
I'm more interested in what a system
does uh
and rather than benchmarking it against
you know is it like a human well uh can
it beat a human at certain things maybe
it's a different kind of intelligence
maybe there's more than one way we can
be intelligent there are lots of other
things that come up to me as questions
uh when I hear that kind of a a
statement yeah it's interesting I mean
first of all I mean I know um
terminology gets overloaded so when we
say things like creativity reasoning
agency and so on it means different
things to different people but I still
think it's a good yard stick for uh
distinguishing different types of
cognitive capabilities because a lot of
um folks just say capabilities they just
have a blanket where they say was just
capabilities um you know as as if that's
some kind of like spectrum that that we
could easily measure but more broadly
we're talking about functionalism which
is that there exists an abstract
representation of of cognition uh that
could be implemented in a different
substrate you know CU clearly we could
just do a simulation of of the world and
you know build some uh biologically
plausible mtic intelligence and it would
capture all of the stuff that that we
have but we were just talking about this
canalization so even in the physical
world there is abstraction that's
presumably why the free energy principle
works so you know you have all of these
diverse physical processes and then that
gives rise to some kind of abstract
function which produces intelligence
like behavior in in the in the physical
world and surely that implies that we
could build some kind of abstract um uh
you know analogically uh plausible
version in the in the world of computers
right so there presumably exists some
abstract form of intelligence that runs
on computers that we could Implement in
our lifetimes and that seems pretty
interesting yeah I mean that's where
hope I active inference is going as to
that point where we're able to actually
build something like that so you're
saying that that's that the the
distinction and the questions that
you're asking here to Define not just
capabilities but also like these sort of
gradations um is important for
benchmarking where we are in this
progress uh toward that goal well yeah I
mean there must be a there must be a
spectrum of fidelity so the the reductio
ad absurdum is we we simulate the entire
universe and then we go up the Spectrum
with uh gradations of increased
abstractions and then there's some
commensurate loss of fidelity perhaps of
of intelligence but there must be a
goldilock Zone where we have a system
which is not too complex still quite
abstract runs on computers is very
useful but you also talking to this
other philosophical problem which is
that and we already see this with with
information technology in general that
when we move into the information realm
our laws don't work anymore right so for
example look at Uber you know when when
you have a technology that exists and it
doesn't respect the territorial physical
reality it doesn't have any friction
with regulations anymore it can do
anything and it's the same with ethics
when you have these AI chat Bots that
behave as if they are humans but the
laws don't apply to them we're in we're
in ethical No Man's
Land yeah I I um I think we're at a
stage now where we don't have I'm hoping
the mathematical language will help to
bridge that Gap like it makes it easier
to talk about these things because we
don't have the right words to being able
to either Benchmark as well like know
what is this spectrum look like what
does that whole space look like but then
also in that legal realm as well we
don't have at least our laws don't move
fast enough to catch up with it and
we're still talking and debating about
these ethical questions um before we
have a formal framework so that makes it
a big Challenge and I think that's going
to be probably in the rest of the
century as technology gets fast evolves
faster and faster there's going to be a
lot of debating around a lot of these
types of terms as we go forward cool and
just before we move off the free energy
principle is there this is also I guess
an ontology question is is there a Best
partitioning and as I understand at the
moment the free energy principle doesn't
tell us about how the partitioning
happens what what could we do that is is
there a Best partition or is it a
probabilistic thing how could we write
algorithms to figure out the
partitioning this is an active area of
research in um the uh in Invasion
mechanics right now um and I think the
closest answer to your question at the
moment the most promising answer is
about um uh weak and strong Markov
blankets there's kind of a spectrum
basically there's this idea of sparse
coupling so you have these two systems
and they have different statistical
properties of one another and so um the
idea is that they are separated by a
marov blanket where one system when it's
conditionally um independent from the
other system
when conditioned on that Markov blanket
the paths that it takes in in state
space how it evolves over time will be
conditionally independent of this other
system and if that that that boundary
dissolves the two systems essentially
mix and they have the same statistical
properties and so they cannot be
distinguished from one another so the
question is like how that that interface
it's in the middle how strong or weak
does it need to be in order for the free
energy principle to apply um what kinds
of systems of all classes of systems
that exist does the free energy
principle actually apply to um you know
can you have a system that's sort of
half mixed and still it applies or is it
have to be weekly mixing or you know
what to what extent and that's kind of
where um this idea of weak Markov
blankets comes into play um it's a uh
the notion Invasion mechanics that you
can there's a way to quantify the degree
of this coupling Behavior between the
systems under which uh the free energy
principle would actually apply so this
is a new area of research um that I
think is going to become really
important for uh defining the free
energy principle in more detail what is
basian mechanics so basan mechanics has
sort of evolved out of what was an
earlier formulation of the free energy
principle and it's at this point um a
burgeoning field in its own right and
the idea behind masion mechanics is to
apply the free energy principle to the
study of dynamical systems um in
particular
um the reason we use the word beian
mechanics um is to think of it as a new
or a different branch of physics so that
you know we have like classical
mechanics and quantum mechanics um
basian mechanics takes the ideas of
basian inference and then combines them
with statistical descriptions that you
see in thermodynamics specifically of
non-equilibrium thermodynamic systems
and kind of brings them together um
information theory is kind of a big part
of this as well um but it's a way
of describing systems U that we've
already been talking about that are
partitioned like this where you have two
independent systems with different
properties and then saying well if they
are maintaining this separation and
staying statistically independent from
one another what is what can we describe
with the behavior and what kinds of
systems um are applying the free energy
principle and what is the behavior of
these systems what are the conditions
that need to be in place for those
systems to exist um and essentially you
get this idea that this is sometimes
going back to our as if conversation if
that system is being maintained over
time it behaves as if it's forming beian
beliefs about the external environment
that it's embedded or interacting with
so basian mechanics encompasses all of
those ideas and themes and and makes the
claim that these types of systems are
minimizing variational free energy that
is the mechanistic way mathematically
speaking under which this unfolds
there's a lot of other deeper
connections to physics um and to for
example uh lonian mechanics U and
descriptions of of these systems from a
physics perspective that kind of comes
into play here um but broadly speaking
Bean mechanics is this new developing
field which is taking these ideas and
trying to apply them as a way to
formalize when we talk about what is
complexity what is a complex system what
is self-organization these questions may
have a formal answer because up to this
point there's a lot of philosoph iCal
answers for what those things are but
there's no formal description
mathematically of what they are and when
it so happens the system we are talking
about is a brain um or animal and human
behavior that's kind of where active
inference sort of falls out of this it's
one type of dynamical system we might
want to be considering um that is a very
special case of this more broad
field is a basian mechanics a field
which is only associated with the free
energy principle I mean you know for
example physicists uh learn statistical
mechanics I mean what what what's the
relationship between that and another
similar adjacent field so Evas mechanics
as a term um was I believe that there's
a paper in
2018 um there's a paper uh fren has a
perspective paper called am I
self-conscious uh and I believe that's
the first time the term appears but it
also appears in friston's 2019 uh
monograph um uh as
well uh and so
I would say that uh it's a proposal it's
not like other physicists would know
this field exists yet um but it's a
proposal for a field which takes these
ideas of physics and tries to use
physics to describe living systems and
this is coming back to um as it's quoted
famously in the active inference field
and in uh and now in ban mechanics um
irn Schrodinger's question of what is
life um in his 1942 book maybe was 1944
I think um where he tries to describe
can the ideas of living systems be
described in terms of physics um and so
the question is yes they can basian
mechanics is the answer to that but you
have to borrow language from information
Theory and statistics um and a bit of
machine learning in there as well and so
it's more of a proposal for a future
field to exist that would stand
alongside as another type of
physics where quantum mechanics
describes Small Things um you have
classic classical mechanics describing
the behavior Paths of motion a bigger
thing
and then you have something like um
Vasan mechanics describing the
statistical beliefs of persistent
systems that are things thingness things
that exist um that are combinations of
particles that persist over time um so
it stands in relation to those other
fields but it's not a field that
physicists would know um but the hope
with my books is that that's what's
what's going to happen is we're going to
have a convergence of of these fields
coming together in active inference and
basion mechanics breaking out of their
Niche to a much broader spectrum of
scientific inquiry yeah and just on on
that can can you give me some examples
of how it might be used in a completely
different
field yeah sure so um for basian
mechanics um modeling um systems that uh
have this kind of complex Behavior so um
we can model for example living systems
um like our of the brain but if we go
beyond that um I think there's hope that
we will be able to model things like WEA
we systems being able to characterize
them mathematically being able to
characterize um any chaotic systems like
the brain like things like epilepsy
networks um earthquake prediction uh
anything where you have this sort of
many many body problem or you know
chaotic uh uh systems where you want to
track Behavior over time or predict
what's going to happen it could be like
turbulence it could be stock market
prediction um it could be things uh in
the um the realm of um behavior of um
systems of people so it could be social
systems or ecosystems um or other kinds
of behaviors I'm sure there's many
applications in physics that study these
kinds of problems but they're hard to
predict and again active inference and
evasion mechanics are ways of looking at
these kinds of systems and describing
them but they give us a way to talk
about what those systems are doing and
how they're behaving so I think there's
going to be some time before it reaches
those levels of application um but
eventually that's where that will
eventually go is to enable us to design
better technology um based on being able
to track the behavior of those systems
as they evolve very cool so we're going
to go into a little bit more technical
detail now and start going through some
of the um framework why don't we start
with the with the modeling framework so
um the description of agent environment
interaction let's start there this is a
big emphasis in um my book I want to
have uh readers start with an
understanding of just the actual
modeling process rather than just
jumping into all these terms and
theories at the heart of it active
inference is a way of modeling agent and
environments interacting so in order to
do that you need some description of the
environment if you have an agent that's
modeling its environment you need a way
in this framework of saying what that
environment is and how it behaves so in
some cases we may just have data so we
already you know we've collected it from
some real real world uh process and
we're just seeing how how um the agent
reacts to it in other cases we may want
to simulate it ourselves um and so the
first idea is to take the environment
and say that it's a generative process
from a statistical standpoint so a
generative process means that we have um
an environment that exists in a
particular set of states and transitions
between those so for example in a very
simplified scenario maybe it's something
like weather you know weather changes
from um you know to uh
snowy to hot to cold to wet um different
kinds of changes that are happening and
that environment when it's in those
States this generative process it emits
some kind of sensory data that is
associated with it so if it's uh rainy
outside the sensory data what's going to
be a result of that is it's wet so those
processes are are connected to one
another it's a transformation of when
you're in this state here's the outcome
that happens from being in that state
and then on the other hand we're
interested in systems like agents that
do not know what the true state is and
this is kind of part of this sort of
boundary between an agent and the
environment it doesn't know what's
actually outside of its head all it
knows is the sensations it's receiving
and so the question is we have some
process statistically that is generating
this information or sensory data and we
have an agent observing that data so
it's the agent's job as a generative
model to generate predictions about the
kind of sensory data it expects to
receive from the environment and use
that information to then infer what is
that state that I don't know so I'm
seeing that it's wet right now so what
is it does that mean that it's hot or
does that mean that it's rainy um maybe
it could be rainy and snowy um for
example so this is a way of under
uncertainty an agent being able to model
its environment statistically and
develop a probability distribution over
the possible states of the environment
that could have generated that sensory
observ ation it's now
receiving so when you put that framework
together um you have the generative
process and the generative model um you
have the process of perception which is
what I just described but then you also
have the idea of action so if the agent
wants to control the environment to
change the kind of sensory data it
receives in the future what are the
necessary conditions we need to specify
how that relationships work works and in
a in a code setting um you would
literally just specify here's a
generative model and in a loop you know
in a very simple scen simple simulation
generates data agent takes it in does
some computation determines what the
state is and then outputs an action
which Loops back to the environment
again the generative process and you
would just run that simulation in a loop
in the real world it would be like a
robot that would interact with its
environment um and in that case you the
agent would model the generative process
but the actual connection to the real
world would be a physical one the
agent's actuators would actually you
know grasp or move forward or interact
the world in some way to control its
environment around
it so these agents have boundaries we
were talking about that earlier uh
Markoff blankets in fact and they can be
kind of decomposed into different types
of States tell us about that yeah so
implicit in the description um but not
not specifically stated um and this is
because I actually leave out the
discussion on marov blankets in this
book because uh it makes it simpler
without introducing too much complexity
um and I'll talk about it in the bean
mechanics book a lot more but implicit
in here is this idea I mentioned that
the environment exists in these states
and I also said that that generates
these
observations or sensory data and the
agent and then inter agent then receives
those sensory data and is then able to
make its inference however what I left
out was um that both that sensation and
the actions the agent can take um they
exist in What's called the marov blanket
so this is what this partitioning means
if you have a in a kind of evasion
Network sense you have four different
types of modules you have the external
states which are part of the environment
you have the sensory states which the
agent can
receive and you have the agent's
internal states that it uses to
represent its environment and then you
have the active states which the agent
is uh is the agent's actuators that
apply control to the environment the
sensory and active States together
compose that Markov blanket and make
that partition keeping the internal and
external States conditionally
independent from one another and
therefore having different statistical
properties so I didn't explicitly call
out those States but they're actually in
there um in the description that I gave
of this framework can you tell us about
the statistical
formulation sure so um the statistical
formulation that's involved in uh
building active inference models um
starts from the basic principles of
beian inference so specific Al basis
theorem um so at its core uh the actual
mechanics of it is uh is very
straightforward um you have a I've used
this term generative Model A lot now so
far so I'm going to spend a moment to
actually describe um what I mean by
generative model um in terms of the
mathematics of it and show how we can
just in description alone use this for a
perception problem
so you have an agent now um where this
agent could be uh quite literally
written in about you know 10 lines of
code it's a very simple agent it has two
important components that we need to
describe
it so it has a prior belief about the
kinds of states of environments
assumptions You' say prior assumptions
about uh the state of the environment
that that could be more likely than
others so you might imagine an agent
that has seen that um over time that
it's
um uh riging very often so I said rainy
was one of the environment States um and
so that would have a higher probability
for example in its prior beliefs um and
so this is literally a probability uh
distribution that just encompasses
before seeing any data what does the
agent believe about what's most likely
about in terms of perception and what
it's going to
perceive and then the other component
that you need for the generative model
is a likelihood function so the
likelihood function says the probability
of some observation it's going to
receive uh given a state so it's a way
of
specifying well if it were rainy um or
let's just make this binary and say
rainy or not rainy what would we expect
from the environment would we expect it
to be more likely to be wet or not wet
this is a very simple example because we
know when it's rainy there's a very high
probability that it's going to be wet
and the inverse this is true when it's
not rainy and so that's a very simple
encapsulation of the probability
probabilistic relationship between the
states of the environment and the actual
observations the agent's going to
receive when you put those two
components together the prior and the
likelihood you end up getting a joint
distribution Over States and
observations and that is the generative
model so in notationally it' be P of
let's say observations o comma States s
that is your generative model so there's
a lot of subtlety in here um the reason
one one reason we can call it generative
is we could say well the agent could
then predict well if I'm think it's
going to be rainy right now here's the
probability of What observations I
should be receiving that's going to be
really important because it defines an
expectation about the kinds of sensation
the agent expects to receive that's
encoded in its
model and so when you um are actually
performing beian inference you're asking
the inverse question you're saying I
have this model now about how States and
observations are related and now I'm
actually getting an observation from the
environment well my likelihood actually
tells me the probability of that um of
that observation given a state but what
if I ran that backward and asked about
I'm not going to use the word
probability here because it's actually
not a normalized probability
distribution but what's the let's say
credibility of what states would be more
likely than others uh relatively
speaking given that I've now observed
oberved it so it's like running your
likelihood model in reverse while also
taking into account your prior beliefs U
about how what states are out there are
most
likely the result of this um is an
unnormalized probability distribution um
sometimes called an unnormalized
posterior um and in order to get that
posterior estimate which is the essence
of basian inference the posterior tells
you the probability of a certain State
given that you've just observed
something this is really important
because that's the essence of what the
agents internal states are representing
in this modeling framework we have an
environment or generative process that
is given that is generated an
observation the agent takes in that
observation and using its generative
model which I just described is composed
of a likelihood in Prior inverts that
model normalizes it and the result is a
probability distribution of its internal
representation of beliefs about what
state is most likely given an
observation
with that alone we've just done a
perception problem in the simplest
possible generative model we can imagine
we perceived the agent then perceives
the idea that it is rainy because it's
just observed that it's wet for
example um so that is the basic
statistical framework um and if it seems
really simple it's because this is a
very simple example but the idea is we
can add a lot more elements and build
upon this very easily um and and make it
much more complex from here um and the
next stage would be to uh talk about how
variational inference in free energy
would play into the situation so um
where does uh variational inference come
into play
here right so the scenario I just
described is exact ban inference so it's
the kind of thing that you would do you
know simple problem you could run by
hand um that you could you could run you
know on a in a computer really really
easily um for a very simple scenario
like that however in most uh real world
scenarios that we care about the notion
of estimating that posterior
distribution so in this case we have
some unobserved State and we want to
know what the belief or probability
distribution over that state is given
what we just observed um which could
take place dynamically as well at every
time step we're going
through the issue is that um this
quickly becomes an intractable
computation um because the
dimensionality of the things we're
dealing with inrees um especially we
have multiple variables in our model and
things like that it's just not practical
to actually compute it so like all
machine learning um you know solving
intractable intractable problems the key
way to do that is to turn it into an
optimization problem and so it's at this
point that we say that the posterior
distribution that we'd like to
estimate um is currently something we
don't know what it is but could we
propose something that's approximate to
it so an approximate distribution which
represents our best guess and say that
we make assumptions that everything is
um uh gaussian in the continuous case or
categorical in the discreet case can we
then figure
out um a loss function of some kind so
that if we were to tweak those
parameters um to the point where we
reach the Minima of that loss
function then we've approximated the
posterior so so this is an interesting
question and there's some technical
detail on how we derive this um but the
short story is that what we end up
getting um is this loss function known
as variational free energy so
technically speaking I should say that
variational free energies a loss
functional because you actually have
inputs that are functions themselves and
so the idea is that you in order to
compute variational free energy you need
a generative model um which takes some
input that the agent has just received
so we already have that because when we
Define the agent we usually Define its
generative model in more complex
scenarios the agent could learn its own
generative model as well but we're
keeping it simple here and the other
thing you need is that distribution I
mentioned which is my best guess about
the posterior and specifically um in
some forms of variational inference uh
fixed form we assume we want to just
know the parameters of the
distribution so the question is then we
have this input to variational free
energy functional which is a mean and
variance and we have a generative model
and we do this computation to see if we
minimize variational free energy by
tweaking those inputs parameters the
resulting parameters we get at are the
parameters of the posterior or an
approximation to it so just to reiterate
again this procedure is just necessary
because we're in a situation where High
dimensionality precludes us from
actually Computing this exactly so we
approximate it and the Really
fascinating thing about variational free
energy is that it applies to all
unobserved variables in our model so
right now we didn't know what the states
of the model states of the environment
are has to be inferred but let's Suppose
there are also um parameters that are in
our model too so these probabilities you
know for for a
um for the case of continuous model
would be a mean and a covariance matrix
um for the case of a categorical
distribution would be a probability
Vector if those are unknown quantities
that we haven't supplied to the agent
then as far as variational free energy
minimization is concerned that's just
another thing that we want to infer so
alongside States we could also infer
parameters um and in active inference we
take that a step further and we say
actions are something else the agent
doesn't know about it doesn't know what
actions to take can we infer what
actions to take using the same technique
of minimizing variational free energy
so variational free energy becomes this
sort of universal loss function where if
there's any unknown quantity in the
model that the agent does not know about
it can try to find what the optimal
distribution over that quantity is by
minimizing this functional with respect
to that particular variable of interest
yeah that makes sense so in in this
modeling framework everything is a model
so there's an action model and all you
need to do is is um this variational um
inference and then you can um let me
just run this pass you again just with a
couple of questions so as I understand
it it's it's um a surrogate model which
has lower Fidelity which makes it kind
of statistically tractable we represent
that as an optimization problem we um
you know we solve that optimization
problem and then we use the statistical
suraga in in situ instead of the
original posterior which we can't
compute correct yes yes and when we
let's say we've we've got an active
inference modeling framework we've got a
whole bunch of variables some unobserved
uh you know variables and so on do we at
every single time step uh you know solve
this optimization problem on a Model by
model you know on a variable by variable
basis or or is are they linked together
in some way okay that's a great question
so this is getting into more of the kind
of Integrity of the implementation um so
the idea this is something to make it a
little bit clear because I think this is
a confusion um is that when you're doing
this minimization it happens at a time
point with an observation you've just
received so if you imagine two time
points these are iterations
you know you you would of gradient
descent that you would essentially do on
this loss function um given an
observation you've just received for
this scenario I'm talking
about but you're right that there may be
a scenario where you actually have you
know you have different things in the
system um and so the question is um you
know you have this sort of entanglement
problem you have different variables
that depend on each other so how do you
kind of get off the ground so to speak
if you are trying to infer one thing but
that requires knowledge of another
unknown thing you don't know as well um
so there are a couple of approaches to
this um in the active inference
literature there are two main techniques
um that are used in continuous uh State
space active inference models for doing
this um one is known as generalized
filtering the other is dynamic
expectation
maximization and they treat the problem
in a slightly different way in the
dynamic expectation maximization
formulation which we'll call Dem in Dem
the models are the variable that you
don't know that you want to infer um are
treated separately at different time
points so you kind of have this outer
loop where you believe that the um uh
parameters change relatively slowly over
time and you kind of accumulate evidence
for them while in the background while
you are then estimating um the states
but you kind of freeze the parameters in
a particular setting and then you try to
estimate the states then you go back to
the parameters and you kind of loop over
time through this and so that kind of
treats them as separate able where you
kind of see them as independent
components that uh you optimize
separately over time and generalize
filtering you actually optimize them
simultaneously so you actually have a
trajectory of states that are evolving
over time that you're tracking as the
environment changes and you're also at
the same time learning those parameters
and they should both they should all
converge together um so obviously at the
beginning if you don't know the
parameters your state estimation won't
be very good you'll be off but then as
the parameters get better and better um
then you start to converge upon the
states as well alongside the parameters
so the more technical answer is that in
different scenarios um one approach
might be easier or better than others uh
it sort of depends on the kind of
modeling problem uh the volatility the
environment and a lot of other sort of
details uh that are specific to a
modeling problem in question so one uh
one thing that I see when a lot of
people first encounter uh variational
free energy is um they hear the word
free energy and to them the most
familiar term is something they learn in
physics um you learn about free energy
in physics then they also look at the um
free energy principle or active
inference they see the word entropy and
entropy is something else you've learned
in physics and I think this causes a lot
of confusion sometimes depending on the
field you're coming from um if you're
hearing these terms being used like this
um this is an unfortunate confusion that
comes from information Theory um a field
of uh to describe information
communication systems that was developed
by uh Claude Shannon in the late 1940s
based on a lot of other prior work um
and so there is actually a connection
between the concepts of entropy and free
energy as it's used in physics as well
as the concepts of entropy and free
energy as they're used in information
Theory um ET James is um one of the
famous authors who was one of the first
people to make these connections and it
will be a big focus of the second book
will be to make the connections a bit
more
explicit but for the purpose of this
active inference book this first book in
this of the of the two um I think
introducing all those other elements in
there makes it confusing so when I use
the term variational free energy it's
specifically talking about a statistical
quantity that comes from information
theory that is distinct and separate
from Helm Holts free energy as you would
use it in physics so um you can think of
it as it's purely a loss function that
is used in the case where you want to
make beian inference a tractable problem
um and so you propose this loss function
and that's a sufficient to understand it
in terms of active inference without
getting into all of the external
terminology that um kind of moves into
more the beian mechanics and free energy
principle side of things let's talk
about surprisal especially its
relationship to some of the variational
um inference stuff that we were talking
about before sure so surprisal um is
another quantity that comes from
information Theory um and it can be
thought of as a negative log probability
of some um outcome of a random variable
so it's uh and and uh surprisal is the
term um tibus uh I think it's 1960 was
where that term first came from uh it
wasn't originally in Claud Shannon's
original formulation of information
Theory um but the idea of surprisal um
the very basic idea is that it's a
quantity that um is very useful in um
characterizing the quality of a model
let's say so sometimes in statistics um
usually you use the model evidence for
example when you're comparing two models
and evasion model comparison procedure
You'll Use model evidence as a way of
saying this model is better than the
other model model evidence is p a
probability of some data point so let's
say either in uh some data point for
example o some observation P of
O surprisal is a negative log of that so
if you think the property of surprisal
let's say um a random variable uh
observation equals some uh quantity so
example uh wet so the probability that
it's wet so the question you might ask
yourself is if you have the negative log
of that that means when the probability
of that is very high surprisal will be
the inverse of that and likewise when
the probability is low the surprisal
will be high and there is an intuitive
nature to this I don't want to conflct
the psychology of surprisal with
information Theory too much but roughly
speaking if you think about a low
probability event as being surprising
it's something
unexpected um then surprisal is a way of
characterizing how surprising is some
data that I received in my model so for
example if you are um if you have a
really good model this is getting back
to this idea of model comparison
strategies um that's predicting its
environment what you're predicting is
every single time a sensation comes into
the environment how expected was it
under my model that's what P of O the
probability of an observation tells you
if it lands at the mean well that's
exactly what I expected from my model so
it's unsurprising to me but if it
deviates from it that's kind of like
sending up an alarm signal here saying
something is up here my model predicted
something to be happening and I've
deviated from that now and in fact
that's actually a learning signal which
is why this is related to Theory it's a
way of compressing and paying attention
to the things um that are high
information content meaning they have
uncertainty in them so what part of the
structure of whatever you're modeling
have you not captured yet well if you've
perfectly modeled your environment well
then nothing is surprising because you
know exactly what it's going to do every
Sensation that comes in your model will
exactly be at average it's exactly what
the model would say as the expectation
of what sensory data you're going to
receive so surprisal is a really
important quantity because it turns
turns out that's really the only thing
you need to minimize in order to do all
the rest of the stuff we've talked about
but the question is then how does this
relate to variational free energy I
think this is another area where this
gets kind of confusing and so I'll make
this connection really explicit
surprisal is usually really difficult to
actually compute
directly it turns out variational free
energy has a dual role on the one hand
it's used for um a way of approximating
the posterior so when we minimize that
loss function we get the parameters back
of the posterior at that Minima um which
is approximate to that to the true
posterior it turns out that variational
free energy when it's minimized also is
an approximation to surprisal so while
we cannot compute surprisal directly if
we minimize variational free energy we
approximately also minimize surprisal
because it's always going to be greater
than or equal to surprisal so it's when
we say it's an upper bound on surprisal
that's what we mean and this is a
consequence of what's known as Jensen's
inequality um you can you can prove this
this uh mathematically and I do so in
the book in chapter
4 so to put this all back together again
here and make this link really clear we
wanted to minimize variational free
energy in our model um because in doing
so we are able to estimate some unknown
variables that are part of our posterior
say our states our parameters but in so
doing we're also approximating surprisal
which means that inherently if we
decompose variational free energy into
different terms we can kind of rearrange
them algebraically we see essentially
that a model with minimized variational
free energy also has minimized surprisal
which also means that the model is more
and more aligned with the actual World
external World itself the environment
that's generating that data simply
because every single time the
environment generates that data if we
have a really good model with low
surprisal uh low risal for any possible
observation we get then we that's what
exactly what happens when we minimize
variational free energy um so inherently
we are essentially aligning our exter
our internal model of the environment
with the external world and surprisal is
a way of measuring that discrepancy um
and as well as uh variational free
energy by proxy so the further away as
it is from that surprisal bound the
further we are away from minimizing that
uncertainty let me this back I mean what
do we want to do more do we want to
minimize Prize or do we want to get the
highest Fidelity representation of of
the posterior there are other ways you
can minimize surprise as well um one of
which is action so closing that boundary
even further is possible through action
which uh I haven't really mentioned in
this framework yet because um yes it's
true that you also want to perceive and
that's what the use utility of
estimating that posterior is but you can
also do that and bring that bound close
closing it even further by taking
certain actions so if you imagine you're
receiving um surprising information
right now so that means that the sensory
data you're getting in here right now is
deviating from that mean of P of the
observation probability of an
observation you're getting further and
further away from that mean so the
question at this point is um you may be
at some free energy Minima you can't go
any further now so now what do you do
you're not at the surprisal bound you're
still getting surprising information
well what if you change the environment
itself so you take an action you act
upon the environment and you alter it in
such a way that it produces different
Sensations that are now in accordance
with your model and I think this is a
key really really important distinction
about how active inference models work
because we get to a point where you
close that Gap by controlling the
environment itself so that it produces
the sensations you expect and that's why
I've been saying a couple times
throughout uh that action is a form of
expectation um as well as prediction
it's it's a way of saying I expect these
Sensations to be uh received by my model
in the future that are self-consistent
they're the least surprising types of
sensor data that I would expect under my
generative model my generative model
makes a prediction and I want that to
conform with reality well I could take
an action and try to close that Gap so I
think that's where you you talked about
minimizing surprisal um action is a
really key component in that that brings
that Gap lower um by making the two
conform with one another um the
environment and uh sensory data and what
we expect yeah I mean we we don't like
surprise uh I was speaking with Mark SS
about this and he actually said that the
more prediction errors we we have the
more conscious we become you know
because we we take actions to to reduce
our surpris or but it's almost quite
depressing because perhaps we're just
automatons when we're in the the
modality of of not being surprised you
know we don't actually take control and
and deviate but um yeah I mean my my
question wasn't very well um formulated
but in a way surprise is just a kind of
it's just a measure um what we actually
need to have is a reliable World model
and and that's this posterior
distribution and the world model is a
good one if it minimizes surprise so
it's not like surprise in of itself is
the objective it's just a side effect of
having a good model right and actually
when you compute surprise it's it's a
marginalization over the generative
model so it starts with that generative
model and when you sum out across all
states um surprisal is a negative log of
what you get from that and so um it's
basically a way of of kind of it's a a
measure of expectation about the sensory
data that's inherently derived from your
good model so obviously if you have a
bad model of the world like if you found
it um you know surprising to stay in the
Sun and uh like say in really really hot
temperatures uh or sorry you found it
very surprising um to be uh uh away from
fire uh uh and then um you went into the
fire for example that's a terrible model
because then your your physical
Integrity of your body would would be
destroyed right so it's it's relative to
the model of the agent and I think
that's kind of the idea is that
Evolution has produced good self models
or models of the world where we have
imbued in us either through Evolution or
through learning um we learn about what
things are supposed to be surprising for
the kind of agent that we are so that's
a really important component to it too
is it's not it's it's a it's based on
the actual model we have and
presupposing that it's a good model
given the world that we inhabit it's a
little bit like you know that the
planets reach an orbit and it might be a
similar thing with with our world model
so we have certain patterns of behavior
and when I'm in London my world model
will will adapt because obviously I'm
minimizing surprise and when I'm when
I'm back here it will adapt and and that
will almost it will make a kind of
pattern you know a kind of changing
patter pattern and we could almost
describe that pattern as a non-steady
state equilibrium so you know it's not
like the world model is convergent in
any way but there is some kind of
homeostasis to to Its Behavior over
time yeah so um in the case you're
describing here so you're describing
like a a model um of the environment of
the kinds of things you expect but it's
changing depending on where you are so
there's this element of you know we our
our models are adaptable so in certain
situations and circumstances
um uh certain expectations are more
likely and so this sort of idea of
homeostasis is very context dependent so
for variables that are like very very
primitive things like you know blood
sugar and things like that you know they
have to be within an acceptable range
and that's the kind of the homeostasis
idea is you expect them to be in this
range and so you take actions eating
food and so on so that you stay within
this acceptable homeostatic range that
is basically built into our body
physiology when you start talking about
more higher order things um we start
having these very context dependent
ideas of um so-called attracting sets
like in certain situations or
circumstances we will be you know we
have certain context so you know for
example I have like at my own house I
have my workspace and where I work and
certain regularities that I expect but
when I go to the office um or you know
I'm out in a coffee shop or in public
those will change depending on the
context so it's kind of a nasted sort of
thing because you know overall there's a
sort of like you know grander set of
states that I tend to occupy but then
each one of those if you kind of go down
the level of more you know more and more
granular there may be other specific you
know pockets of attractors that are
local to that area and so we are you
know composed of all these different
types of uh attractors depending on the
kind of situations and circumstances
that we're in and and is it fair to say
that there might still be some kind of
path between those attracting States so
rather than thinking about the the state
of the probability you know at different
um you know points in in the state space
there's there's some kind of pendulum or
or some kind of structure um of those
States sort of traversing between each
other so we can kind of you know almost
zoom out a little bit I think there's
definitely a good um sort of geometric
interpretation of that kind of a
manifold um you know where you know
essentially there's there's a sort of
cental there's this line that we're kind
of trying to stay you know steering on
but you know things are Al dynamically
changing um there's this isn't the exact
same uh situation here but there is a
one term from a paper that I really like
uh Carl has used the term before of um
you know gradient descent while trying
to hit a moving Target um which you
imagine you you imagine like a flowing
landscape that's just sort of changing
over time um and so the actual Target
you're trying to hit may be constantly
changing um what what what is the center
of the manifold or what is that that
line that you're trying to stay in the
middle of it balance on um you know to
take Andy Clark's kind of perspective of
you know surfing a wave um that may be
Meandering in itself but it's still
Minima of something that overall Global
landscape it's just that this is sort of
changing and moving over time um so I
think there is a there's a geometric
interpretation in there um but it's a
bit more mathematically difficult to
visualize what that would look like so
tell me about the role of action in
active
inference so action um I think in agency
this is a a big part of what makes
active inference models important and I
think one way that I'd like to frame
this is in terms of the exploratory
nature of Agents because we've been
talking so far about this idea of you
know minimizing surprise which is a kind
of uncertainty you know you keep wanting
to have the least surprising things um
and I think action is key to this
picture making sense because um you know
one thing I've always asked you know is
why do we seek out on certain things all
the time you know I seek out new
experiences we like magic tricks um I'm
you know a big fan of really noisy heavy
metal music and stuff that just you know
is would be very stimulating and
overstimulating and very you know but
why why do these things appeal to some
people less than others why do certain
people go after experiences that push
you to the edge of certain Sensations
that you expect um I'm also very private
person I like being at home I like
having my space you know so I like a lot
of that regularity too I'm not like a
you know I don't go bungee jumping and
skydiving and that kind of a thing so I
think uh action is a really interesting
aspect to this
because um in order to survive you need
to explore your state space you need to
explore what's around you you need to
have a good understanding of where you
are in your environment and what's
permissible what's not permissible um
you need to gather information and in
order to better attain the goals that
you have so I think the exploratory
component and the Curiosity component is
where action comes comes into play in
active inference models um as it allows
the agent to sometimes you can think of
this as resampling the environment
you're looking in one location here and
you're getting sensory information
sometimes you actually want to move to
an area with greater uncertainty that
surprises you because it reveals
something new about the structure of the
environment so you may incur this
uncertainty bump or cost at the expense
of then understanding more about what's
going on around you and then you know
more about your environment and you were
more confident about how to take a
decision to make to get the reward or
the thing that you want to achieve or
accomplish so I think um this sort of
you know self self-fulfilling prophecy
of of this idea that we um take actions
to fulfill the expectations of our model
um is something that unfolds over time
and may have these little bumps of
uncertainty along the way where we
Meander off the path to explore but
overall if you zoom out at this kind of
grand scale there's this always staying
at this kind of steady state that
happens um at all times it's also
interesting as well because as a
representation of cognition um it's
physically aligned because of course you
know we're physically embodied in the
real world and and we take physical
actions in the real world and we're
building these different inference
models that essentially um predict
trajectories of actions and and I guess
you can think of that essentially as a
goal right because you know what's the
value of doing this sequence of actions
um o over time but as a as a
representational framework in AI that's
really cool it's almost like the
Primitive becomes stories right you know
stories are just sequences of actions
that that lead somewhere in the future
and that's um a really great segue into
exactly how you know action works in in
in these models for planning ahead into
the future there are these different you
know paths or stories you could take um
you know that you trajectories through
that state space that you could do but
which one do you do and that's the
ultimate question in active inference is
um which is usually framed in discreet
State space models is trying to
understand
the best trajectory that accomplishes
both achieving your prior preferences
and expectations and the exploratory
behavior that comes with that of
exploring your environment and at
different times one or the other may be
more important uh depending on the
context of the problem that you're in
and selecting among those different uh
stories as you put it or different
trajectories is the key uh basis for how
active inference models work you've said
that one of the confusing things to
newcomers is is the difference between
the continuous and discret versions of
of active inference can you can you tell
us about that yes so the um depending on
what papers you look up so for example
you know one of the most cited papers um
from from friston is a 2010 uh the free
energy principle unified brain Theory
and you go look in that and you see all
these you know there's differential
equations in there and and so on and so
forth and then you go look at some of
the newer papers um like Lance at Lance
Costa's paper at all um you know Act of
inference on discret state spaces at
synthesis from um 2020 and you see
something completely different and I
think this is one of the issues is
people come into the field and all the
different papers look a little bit
different and the question comes out of
what is active inference where you know
where do all these equations fit
together um I definitely think the the
part all book has done an excellent job
of actually helping to to frame some of
the the differences by separating
different chapters on each one so let me
give the short the short answer
basically is that up from around uh 2003
to 2010 uh maybe say 20 2013 2012 there
was a big focus on continuous uh State
space active inference models so these
models are framed in terms of
differential equations um and so all the
probability distributions are uh
gaussian and usually you're trying to
estimate the mean of these distributions
whether that's a mean over you know
parameters or States sometimes Precision
variables uh which are these sort of
hyperparameters in the model and because
they're changing over time there are
these dynamics that are differential
equations of how do that what do their
trajectories look like and they're very
literally solved like differential
equations you have a velocity and you're
integrating it to figure out what the
position is at different times so
there're these trajectories that you're
mapping out um for variables as they
change while interacting iteratively
with a dynamic
environment so all of that was heavily
developed in that early time period the
first sort of decade of active inference
but around 2013 to 2015 um you saw a
switch uh to looking at more um discreet
State space models and the big reason
for this is those early models treated
action is kind of like a reflex so these
sort of reflex arcs where the agent um
you know gets some information and
reacts by taking a reflexive action on
it but it isn't really focused on
planning into the future and that's
really where the distinction comes into
play um I think think there was a a push
for wanting to model the behavior of
planning agents in the neurobiological
context and that's where uh Frist and
his collaborators spent a lot more time
on developing the discrete State space
formulation the 2015 paper um active
inference in epistemic value is one of
the first uh 2015 to start moving in
that direction more formally um and
since then 2015 to present there's been
most of the work has been done on the
discret state space formulation um a big
reason is that discret Stace space
models are very quick um everything can
be done with uh Matrix algebra um and so
it simplifies computations we think in
terms of categories and symbols so it's
very convenient to represent the world
um a lot of things that we study in the
world are static categories they're not
changing you know like on the level of
like a stock market or turbulence and
things like that so they're much easier
to map onto the real world and
apply um and so uh and also when you're
planning ah head into the future you
have to calculate you know like
expectation operators which are much
easier to do as summations than
integrals so there's lots of benefits um
and that's where that the distinction
came into play and the discrete State
space models uh use categorical
distributions instead of instead of
gussan um and it's formulated as a
partially observable Markov decision
process um and we're now at a point
where these models are becoming very
formalized in their structure into what
is called a universal generative model
um the idea being they're kind of like
the idea of universal function
approximators but a very general kind of
model that would apply to many different
situations and
circumstances um and so there there are
of course also Hybrid models that mix
the two together um which are also very
useful for specific kind of Mo modeling
problems where you want to turn
continuous data into Bend discret
categories so if you're looking for uh
kind of the state-of-the-art activ
inference a lot of it will be found in
the discret state space models but there
is still ongoing work on the continuous
State space models that are especially
relevant in the context of beijan
mechanics so they they draw much more
upon those older models um and it's an
area of active development right
now yeah um and and shout out to uh to
Lance dcosta because I I read one of his
papers recently I think it was something
like active inference as a theory of
agency or something but that was using
this new um discret PDP framework and it
was is actually it's it's very elegant
it's beautiful um but a couple couple of
points on that so does it not make sense
I mean clearly we understand that
planning in humans is a system to thing
it's a it's a discret thing uh so so I
guess it doesn't make sense to plan on a
continuous model but then I interviewed
Max Bennett he's got a book out brief
history of intelligence and he's saying
well humans do do this you know we have
these um all of these different
simulations that run in our brain and
and we're you know presumably that must
be somewhat continuous because it's not
it's not all discrete what what are your
thoughts on that can we only plan with
discrete models no I definitely think we
could plan with continuous models I
think it's not necessarily a physical or
a a limitation it's more that just that
it hasn't really been developed yet as
much in the active inference literature
and the discret models at the particular
time were useful for solving a lot of
really common problems and getting you
know a lot of leveraging some of the
benefits of discrete models um and I
think there is interest in going back to
those continuous State space models at
some point in the future to develop um
aspects of planning uh at some point so
I don't think there's any real physical
limitation I think um there are some
just challenges that come with working
with differential equations that you
just don't have to deal with um so it
may just be a question of just
convenience for the time being but I
think ultimately um active inference
models are going to have to tackle this
problem in continuous State spaces at
some point um and I think beijan
mechanic will probably unlock a lot of
that as those models are developed a lot
uh in in more detail on the subject of
Conan system one two dichotomy what do
you think of that and does does that in
any way conflict with the you know free
energy principle and active
INF um that's interesting question I
actually have have never really thought
about it in terms of uh active inference
before so give me a second to kind of
like formulate a position here so I
think um I mean do do you do you
recognize because a lot of people were
talking about this on patreon last night
and a lot of people just say it's it's a
 distinction between system one
and two well yeah as in there there are
there are cognitive processes in the
brain and some of them are are very um
almost instantaneous some of them take
longer they're much more iterative and
maybe there's a vague boundary between
those modes of cognition into system one
and two but but some people think no you
know reasoning planning it's a
completely different mode of of abstract
cognition so my position on this would
be then um
um I think system one and two is a
useful
metaphor to help people kind of
understand how they might you know think
in a certain situation but I think that
binary is definitely oversimplified and
if I'm not mistaken I believe even kman
himself has said like you
know this is just sort of a a coar
grained you know description of what's
going on um and uh I might be wrong on
that that's my recollection of reading
something he said around when the book
was published um and so my opinion on it
is
that um there are levels of um yeah one
thing I studied in in my first postto
was cognitive control um and so there's
a level of um automatic reflexive
behavior um that you know agents will
use um and there's also deliberative
planning um you know and suppressing
certain reflexive activities that you
would just do without thinking about so
I think there is definitely a level of
um the term that's used a lot uh I
believe it's Simon Newell if I'm not
mistaken is satisfying um um yeah we
spoke about that with David SPAC okay I
don't know if you listen to the David
spivac I haven't heard that one yet well
that's that's an incredible coincidence
I just published that yesterday okay all
right yeah so it's a it's a really
interesting concept and and it it mixes
in with these ideas of like bounded
rationality you kind get into the old
neur economics and stuff like that um
and so the uh idea being that you know
in most situations that in and this gets
back to the idea of like what action do
you take or what sequence do you take
you know you you can't sit there and and
deliberately plan every single one
actually I have that problem though like
I want to think through every scenario
and the end result is I just don't do
anything sometimes because I'm just
trying to deliberate every single option
that's in front of me and I so it's very
familiar to me it's this idea of you
know you have to make a decision
eventually especially if you imagine our
brains evolving you know 40,000 years
ago where you know you have to make
decisions and actions uh in that context
where you you may have to um very
quickly decide if you're you know being
chased by a some kind of a predator of
some kind or you know any decisions you
need to make have to happen quickly so
jumping to conclusions um can be very
computationally efficient but it can
sacrifice some level of accuracy about
the world and when the when the
consequences getting eaten are not eaten
sometimes that's a loss you're willing
to incur so I think there is some level
of that built into our brains in that
you know you see it a big tree of fruit
for the first time when you're wandering
in a jungle you're going to eat all of
that because you may not get food again
but now when we're you know in our in
our modern world where we have access to
food and things like that that's not
really as relevant but those core ideas
or primitive Concepts about our human
physiology and behavior may be still
inherent in our in our minds um so that
kind of system thinking of just
automatic Behavior I think is definitely
true to some degree because it's just
efficient you know you don't have to
think and plan about it you just know I
do this thing I will get back a reward
um it's computationally efficient but
then there are levels of control that
are needed for more deliberative
planning where we sit down with you know
pen and paper and we you map out what
we're going to do um and I think that's
useful for more complex tasks um but to
say that they equally they kind of split
into this binary partition is I think
definitely an oversimplification I think
there's
probably a spectrum of things that that
fall you know along that I don't even
know if I would really call them part of
a spectrum even really I think it's more
just just that some tasks require more
cognitive load than others um and
context May dictate whether or not
that's actually going to happen based on
the needs of you what your needs are in
a given moment um but it's a useful kind
of broad stroke way of looking at the
brain If you're trying to explain
especially for our own physiology where
we have you know we struggle with
decisions every day about should I do
this thing or should I have this big
meal or should I just wait and in
moderation or something like that so I
think it's just a useful metaphor
so um we humans have this propensity to
explore yes we like um you know not just
epistemic foraging but we like exploring
cities and doing all sorts of stuff like
that but what does how does that
translate over to active
inference so in active inference there's
uh kind of the name of the game is
uncertainty reduction in different ways
so you can reduce um when you're taking
actions uh your policies or plans these
sequence of that you want to decide
decide among um there are different
levels on which you can have these sort
of uncertainty reducing behaviors in
different contexts so what you would
consider is there are situations where
you um want to reduce uncertainty about
the states of your environment if you
don't understand them you want to take
policies or action sequences that reduce
uncertainty about the model parameters
um you may also on the like a higher
level of structure learning also want to
reduce uncertainty
about your actual model itself there's a
whole you know State space of possible
generative models you could have which
one is the most uncertainty reducing
model that's best for a particular
situation so um I think when I say
uncertainty reducing there are all
different special cases of variational
free energy minimization or error
reduction um if you if you formulate in
terms of prediction errors which is a uh
a perspective from like predictive
coding models um so I would say that you
know that exploratory
behavior is almost a
consequence of the very basic need to
maintain set points so I'm going to kind
of deviate a little bit here into kind
of the sort of cybernetic perspective
but there's it's been proposed both in
active inference in other fields that
the very basic physiology of Agents you
know whether it's bacteria or other very
simple organisms is maintaining set
points for us it's things like blood
sugar which I mentioned before or um you
know blood pressure levels uh other
things in our body temperature ranges
what we're sensing and so on between
permissible ranges and so this is kind
of like a a PID controller will do this
where in engineering you deviate you
want to bring back to that equilibrium
it's a very simple process that uh
thermostat can do this kind of thing but
then you imagine this is sort of a
homeostatic control and in cybernetics
there's this distinction between
homeostatic and allostatic control so
what if you need to anticipate your
future needs so you you want to say well
I could be hungry in the future so there
are things that I need to do in order to
satisfy that later on I have to go out
and explore and then gather more
information first to then satisfy that
need so if you start thinking about
these layers of abstraction building on
a really simple principle of this
biological imperative of just
maintaining physiological set
points I think what emerges out of this
is this picture of exploration becoming
kind of a natural byproduct of the
general need to maintain physiological
parameters within a certain acceptable
range and so exploration becomes another
path to uncertainty reduction in the
case where you're anticipating future
surprises that may happen um it's a very
successful strategy and you know social
systems are all about uncertainty to
reduction or like spreading out the
uncertainty among different people who
can all explore in different ways we all
different personalities and different
things we do and we can come back and
share those beliefs so that's kind of
how I tend to view it is that it's it's
sort of an ENT property maybe of basic
phys physiological needs that have
blossomed into something much more
complex in in human uh human brains yeah
I mean one thing that is a well
apparently a sophisticated feature of
cognition is future planning and I was
reading Sarah hooker's book around um uh
she was talking about these compute
limits on the the executive order in the
US but anyway she had a lovely little
history in the intro where she was
talking about you know in the 1600s
after we had like the Great Fire of
London and the Great Plague and stuff
like that we actually started thinking
about risk management we started
thinking about you know there are bad
things that can happen in the future and
actually we should start you know
planning and and sort of you know
instituting governance and doing
preventative measures and washing our
hands and and stuff like that and that
that's a kind of societal phenomenon but
presumably in our brains long before
that and certainly in the brains of
mammals we have this predictive
architecture MH and when the when the
the future fa architecture is baked in
then this set point Management in the
future actually leads to this
exploratory Behavior so it's almost like
the whole things just fall into place
yeah that's really really really
wonderfully put I think that's that's
and it must be very true I imagine just
you know older civilizations whether
they're nomadic or you know a
agricultural based you know at some
point you have to start thinking about
safety and behavior of of a whole group
or you know group of people and that's
where I think this exploratory behavior
and C I it and you know exploring your
environment would have probably become
part of human
civilization very cool very cool just
before we close out I mean now we've got
a a patreon discussion as well and F
Folks at home should listen to that
because I remember it being very good
when we were talking about agency and
I've been trying to avoid talking about
agency recently because you know the
audience are frankly sick of it because
we've been talking about it so much but
but I remember we did have a very good
conversation about agency I mean is
agency
real that feels like a loaded question
and by Design um so um I'm going to do
the pedantic thing and ask what what
what do you what do you just to
understand your definition of real in
this context here yeah is it an as if
property is it is it you know again I'm
making up terminology here but I guess
like an agency instrumentalist okay
might be someone who thinks that it's a
bit of a useful fiction but it's not
real in any sense but as we were
discussing earlier what is what does it
mean to be real if if the phenomena um
you know does everything that you expect
it to do then how could you say it's not
real right so I think um my answer is is
similar to the you know kind of the sort
of model versus reality sort of question
that I brought up before the model and
Target system um in that I think
it's a useful
abstraction um that is good at
describing the kinds of models that we
are we talk about in the context of
Neuroscience and so on um so for me real
is more equated with its ability to be a
useful descriptor of some thing that
we're trying to model um I see that that
could be as if you know it may be it may
be literally what's happening I don't
think we can know that I think it has to
be um as if as as good as we can say uh
and I definitely feel that agency is
something that is you know present and
all the kind of models that are able in
some way to alter or control the
external environment I would describe
that as some level of agency along a
spectrum um and of course when you start
getting into anticipation and planning
and other things it's a more complex
form of agency um does that answer your
question yeah yeah it does okay so I I
think we agree on that first step so so
it's it's it's an instrumental thing
it's a model it's it's it's not
necessarily and when I when I say real
of course it's not it's a non-physical
abstract concept so what could I mean by
real well maybe I'm not even sure myself
but maybe I'm I'm alluding to it being
part of the physical makeup of the
universe maybe like if if someone like
God created the universe maybe they were
doing symbolic Ai and they they've coded
up a computer program and they've got an
explicit concept of a goal right and
they said okay we're going to have all
these little agents and they're going to
they're going to have these goals and
this intentionality and this is the
causal structure and and maybe we're
just kind of even know the universe is
is generated and emerged we as
intelligent agents are kind of
introspecting and we're pulling out that
that original structure that that was
the makeup of the universe in a platonic
sense yeah I think that's kind of a
distinction that I would draw is like
it's a description of something that I
think is real but it's a description of
that thing I don't know what that real
thing even means because I don't I'm I'm
bounded by my Concepts in my mind
whatever that is but it's describing
something that is in some sense real
because we are able to you know actually
interact act with what's around us um
there may be other descriptions or
better descriptions or alternative ones
that still describe that same real thing
if that makes sense yeah it does and and
I kind of agree that they seem
anthropomorphic so you know we're we're
bounded cognitively by the the knowledge
that we have yes but then again the
knowledge we have was endowed To Us by
the laws of nature and evolution and
it's entirely possible that there's this
weird resonance that evolution has
actually found the you know in a in a
meta sense has found the the modeling
framework that was used to create it so
there's almost like this information
bottleneck that you can just start the
universe again maybe there's a goldilock
Zone but you know it'll lead to the
emergence of the self-recognition of the
very Primitives that went into building
it in the first place which is quite
interesting yeah I mean you know this
this is kind of it speaks to the idea of
that the cybernetic idea of like are you
you become a model of your environment
so does that mean you know you're
recapitulating what is real now in this
model like it's not just as if right I I
think what I'm trying to draw the
distinction is like scientific models
versus models in our mind um that that
is a very similar kind of thing to me
like it's just a different different way
of doing the same kind of thing of
modeling something that's real um but
kind of at a metal met metal layer of
that right because the scientific models
are based in our models of the world
that we are now modeling other parts of
the world in a very specific kind of way
so I do I do feel like there's
definitely you know if if there wasn't a
connection to what is real we wouldn't
be able to operate in the world so I
think there is definitely like some
convergence on there in there somewhere
but I don't know how deep that goes
there may be things that we uncover
about the universe that we never
expected that we can never model um and
then in that case it is the then it
would be that our brains behave or even
our scientific knowledge behaves as if
but I find that to be unlikely that it
would be that way yes but it also goes
to the raction canalization point that
we spoke about before so you know
there's all these these levels of
emergence yet we have converged on this
shared epistemology I mean of course
it's very Divergent in many ways but
some of it isn't some of it feels very
Universal we understand each other and
it describes the universe extremely well
you know we'd be forgiven for thinking
that it was platonic in in some way and
certainly some abstract knowledge
probably is platonic because it really
is universal it works in in all
situations but so that that that's that
but then there's the question of okay so
it's just an instrumental description
but people then use it to explicitly
build AI systems that's what GOI people
did they say okay I'm going to build an
agent and and it's going to have these
goals and it will create these sub goals
and these are just explicit things that
I've designed or maybe they've been
metal learned or something like that and
and now now you're actually using the
abstraction as as the primary
representation or in philosophy like in
Nick bostrom's work you know with
instrumental convergence and
orthogonality and so on the the goals
which are these instrumental abstract
objects are now becoming the basis for
Theory you know that we're making
statements about um that you know what
what happens when AI agents have a
certain level of intelligence and what
kind of goals and intentionality will
they have so it feels to me dangerous to
take something which is an abstraction
and then use that as the basis for
Theory or building AI agents what what
what way is it dangerous to you well
dangerous in in the sense that I mean
maybe I'm talking about two things there
so you know using it as a basis for
Theory and philosophy and using it as a
basis for building AI agents so maybe
we'll do the the latter one right now so
you know GOI never worked particularly
well but there are still loads of people
who strongly believe that we can build
symbolic AI systems that do explicit
reasoning and planning and certainly for
a specific set of tasks you know like um
I mean planning is is is a great one
they work significantly better than
current AI systems and we're seeing
neuros symbolic approaches that that
blend the two things together but but
these are things potentially where we
have an explicit goal and and what is a
goal it's just presumably a future world
State M um but that's that's kind of
brittle right because how do you
explicitly in the complex world we live
in how do you explicitly and Ro you know
robustly represent a future State now we
seem to do this because we have this way
of just talking about future situations
and our representation overcomes a lot
of the Brit us but when we start coding
these in computer systems it it seems
much harder to do
that um so you're saying is that where
the risk comes in and that making the
assumption that these systems are the as
if versus real and we'll quickly see the
limitations of our own models and if
that as if versus real boundary is as
true as we think it is when we start
applying them to the real world and
seeing if they really behave in the
flexibility and in the way that our our
own models internal models would in the
same situation well yeah in the sense
that in the physical world as we were
saying earlier we have um a symphony of
diverse complex processes which run and
then they lead to um you know future
situations and there's all sorts of
convergent and Divergent behaviors with
you know chaos and all sorts of stuff
like that and we have the ability to
kind of categorize patterns or modes of
of that future State space and we call
it a goal yeah and we then put that goal
into an AI system and as we were saying
before this might work you know maybe
it's possible to represent a very
complex physical system using a very
abstract computational model and maybe
that will work but maybe it won't maybe
we are Mis you know we're
underestimating the amount of
categorization and robustness that we
use when we think of a goal and we're
actually reifying it and we're stripping
away all of the actual cognition that
we're
doing yeah I mean I think that's that is
a risk of you know over of having our
models being way over oversimplified we
don't really we may not know where that
limit actually lies um and I think the
only way we can know that is by actually
trying to apply it in the real world
which to me is a good Benchmark of how
successfully our models have really
captured reality in the first place but
I definitely agree with you I think
that's probably something that we're
going to see as we deploy more agents
into the world the limitations of the
models that we have may come become more
become clearer um to the degree for
which models are truly representing
reality or behaving behaving as if they
do in a way that's not quite in line
with what is really happening in reality
that our our minds our our brains and
our Collective knowledge we take for
granted for what we have in our models
so I've been very skeptical of ax risk
and my main reason for that is I don't
think current AI has any agency I mean
in a sense I agree of them that if there
was an agential AI you know that was a
super intelligent that' be very that
would be something to worry about but I
don't think current AI is but um what do
you think about the future of of agency
in in AI systems I mean does that
concern you and I mean my definition of
agency is you know like strong
intentionality and
self-causation so if we did have ai
systems that exhibited that would that
be an overnight thing would it emerge
from you know would would there be some
kind of I don't know maybe cyborgs or
human computer Symphony con collective
intelligence and then would it diverge
how do you see that panning out
I think
um
it's there's a lot of you know money in
AI right now and push for a lot of focus
on um developing uh we haven't really
you know got to a point where there's a
huge push for agency yet but some of
that is going to be really useful I
think like in industry and Manufacturing
and things like that that may be the
beginning of it in autonomous vehicles
and stuff like that and maybe then it
will you know continue to be further
explorations of this sort of you know
cybernetic relationship between human
and machine um that I think is probably
going to emerge in the next decade two
decades or so um the part that worries
me with the intentionality and agency
aspect of it is the degree to which
the uh models and research moves and the
money that's in it relative to our
understanding of the limitations and
risks and ethics and the legal system of
what's permissible so I think like we
are going to move in that direction and
I don't think there's anything that's
going to stop that um and I don't have
nightmare scenarios that a lot that you
know it's kind of popular in science
fiction I think I'm much more worried
about people um and and US developing
systems are hooking it up to things that
are you know Mission critical systems
that are decisions that humans should be
partly involved in um and that level of
agency we should be careful as we're
designing them to make sure that they
don't get into the the wrong hands that
we have right the correct laws around
the operation of that of that equipment
and I am worried that if we are moving
too quickly to the rate at which the law
and our understanding of these systems
catches up there may be some kind of
event that has to happen first before we
learn what we need to do to actually
regulate and control these systems
properly and on that are are you are you
um do you lean towards safetyism which
is to say we should do risk assessment
and we should prevent bad things from
happening before they do or do you lean
towards libertarianism which is to say
um we can't really adapt quickly enough
and if we just trust the thing then
safety will kind of come out of it I
think I'm a bit too cynical to believe
that latter point because I do feel like
uh the incentives right now for for
companies isn't necessarily in the risk
side of things um I do think there is
some level of self-regulation like there
are there is a growing interest in AI
safety interpretability and you know
explainability these kind of things that
are definitely present um some companies
do at least have ethics committees I
don't always know how you know if that's
just for show but they do at least exist
and there are some thinking about it um
I think I put myself somewhere closer to
the center slightly left maybe where I
do think um I believe in the in the
ability of some level of you know we
can't necessarily know and adopt in time
anyways but it is safer to still be
having the conversations and putting
some safeguards in place um and having
them at least there and having the
discussion continuing rather than being
way on the far end of the libertarian
end of that Spectrum yeah it it's so
worrying because I say worrying I don't
know to be honest but I mean certainly
when we start to augment ourselves with
technology I know a lot of people
criticize transhumanism because it could
potentially lead to massive disparities
in in society but also in a more diffus
sense when we become a collective
intelligence where we are entangled with
AIS just like in the animal kingdom you
know that there's a relationship between
the collective and the agency and and
cognition of the individuals like in
like ants for example and you know ants
are quite dumb because they form a
collective and when we become a
collective entangled with AIS it could
easily reduce our cognitive capabilities
like our our agency and of course we're
just purely speculating here but there
there is a potential future where that's
quite concerning but is that is that
something we should regulate against or
what should we do that's a great
question because I think this is the
same kind of thing with any industry you
know regulating it versus you know do we
make people let people make their own
choices um versus do we intervene um you
know in terms of any kind of governing
system um and I think I I find myself I
think that you know speaking of agency
we're you know we are free agents we are
able to do and make our own decisions
and I do trust trust that people um
generally are somewhere in the range of
their best interests but not always so
like I have some level of I'm not on the
extreme end of that but I do worry about
like this the incentive structures of
companies that's really where I think
like it's driving it is that's where the
money is and that's that's what you know
we're moving forward with that incentive
structure um I think that uh we
definitely need to have these
discussions and the legal system needs
to be paying attention to it but I think
there's you know we don't know what the
consequences will be like you know I'm a
I love sci-fi and fantasy and stuff like
that so I the ideas of having this
transhumanism and you know Nanobots in
your body that will destroy diseases all
these things that you kind of you know
it excites me on a on a fantasy level
like I know that's not that's just a you
know a fictional representation of what
the reality could be um and I don't know
what that would look like in actual
Society I don't think anybody knows I
don't know how anyone's going to react
to um these things even if they're done
in the most beneficial way possible we
may find out some things about our human
nature that we didn't realize we really
required or took for granted um and so
it excites me on a theoretical level but
I definitely feel like we need to be
talking much more openly about these
things and um
having enough
regulation that allows for growth um
without having too much that it stops
the research in general and now where
those boundaries lie I feel like that's
not my expertise that's kind of a that's
kind of a highle answer to a very
complex problem but that's where if I
would draw the kind of put the put the
Line in the Sand somewhere it would be
you know maybe this is uh two on the
nose but some kind of uh you know enough
so that you can have that that
complexity and enough that you can have
that adaptability but not too much that
you become restrictive um you know kind
of talking about Dynamics and systems
and things like that I think that's
always the best place to lie
and I spoke with Pedro Domingo he's a
bit of a controversial guy but he was
advocated he's obviously a Libertarian
and an Accel accelerationist but his
basic thing was that oh um if you look
at the way markets work right now and
Traders we actually have AIS we have we
have AIS that act on our behalf and the
AIS share information with each other
and you know there's this kind of
equilibrium in in the trading markets
where you know certain thresholds are
met and trades are made and the whole
thing just balances and yeah we don't
really understand how it works but it
doesn't matter you know it just works
and and he was advocating for a kind of
governance like that where you know we
all have our own personal AIS and you
know it's much more of a direct
democracy when the AI can actually
understand what we want and we can you
know be far more Dynamic and so on does
that does that seem um you know
dystopian to you or do you think that's
a good
idea um I feel like a lot of these
arguments feel like kind of the
Invisible Hand sort of thing like it's
you know I mean it doesn't
really um I think Richard thler called
caught it the invisible handwave which I
feel like is you know to be this is a
reference to Adam Smith of course that
Hidden Hand of the mar yeah right I mean
and I and I think like that's I mean you
know I don't think we know I don't think
he you know um I don't think we can
confidently State that's what's going to
happen um and I don't know if you know
it depends on your level of what you
think is um a good measure like is
Market all that you know what about
people's overall prosperity and things
like that and people's General ception
like even right now in America right now
like you know the economy is you know
doing well unemployment you know but the
general feeling of a lot of people is
not great um and then maybe that may be
due to other factors that are not
necessarily directly linked to the
market but I'm just saying that when you
have an economic measure or something
like that um that may be a future you
look at but that doesn't necessarily
mean that you understand all the
Dynamics of what other things are
affected um you know world is very
complicated and there all these other
factors we to take into account so I
would say I'm not that optimistic um but
I don't think there isn't some truth to
that that isn't some level of this kind
of self-regulation that will probably
occur um naturally because we are also
incentivized to be looking out for our
best interests the problem is I think
the systems that we are now studying and
interacting with are so complicated that
we may not be able to even when acting
in our best interests know what the
outcome is going to be um and I have
advocated this is kind of you know this
isn't something that I would say is
possible yet and it would require a lot
of really good data collecting very
accurate data collecting but it would be
very interesting to me to move toward an
effort of predictive modeling on the
scale of like complex systems where we
make public policy uh decisions and you
know you do something that seems like a
great idea what are the outcomes going
to be oh well I didn't expect this to
destroy an entire class of people for
example even though it looked like a
good policy on paper kind of the
unexpected consequences sort of
scenarios that happens a lot when
well-meaning people put forth a good
policy that ends up hurting or harming
other people as a result if we had ways
of predicting incentive structures like
how can we incentivize people to make
monetary decisions companies or
individuals and show them that if you
made this decision versus this one you
would still benefit Society or benefit a
group of people um and there would be no
net loss to you like can we make better
predictions as companies or as
individuals in a complex system that
requires not just social and economic
markets but also the environment and you
know whatever is going on with our
planet and things like that which are
all kind of interlined now you need so
much data to make a predictive model
actually work about with something like
that but I do think like that's one way
we should be putting our efforts toward
if we're going to be developing AI
models is developing something that has
the capacity to reason in a very complex
setting using our Collective knowledge
as people so we can make better
predictions that may benefit the planet
and benefit
ourselves I want to talk about about the
the strange bed fellows in the um the
fvp adjacent community so this is
something I noticed coming in straight
away that there are lots of crypto
people and these are people who believe
strongly in in decentralization and you
know almost like um they want to have a
small government they don't want they
presumably don't believe in safetyism
they they don't want any um any
interference and then and then there are
people who have the complete opposite
view like a lot of the the the
enactivists for example and that they
think in terms of system and and
collectives and so on and you know one
one school of thought is that we could
just build these distributed systems and
um good things will emerge as a result
of it and I I actually I think I asked
KL friston this question a couple of
years ago and I got the impression that
he kind of lent in this direction and
and then there's loads of discussion of
when we design Fe systems and and we use
them for managing smart cities and
countries and stuff like that that we
should Institute a governance framework
on the top what what do you feel about
the Dynamics there in the community yeah
I mean this certainly gets into some
discussion also like on the spatial web
and other other things that are you know
talking more about governance systems
and you know what the future would look
like um I personally haven't run into
I've seen a little bit of talk in that
direction but I haven't really paid too
much attention to like the political
angle of how the different uh
perspectives have have looked at things
um related to what you're describing but
I will say that it does
um concern me that people would that
there would be this level of confidence
that like I understand from the free
energy principle perspective like
there's this sort of survival aspect
right like you know we'll find our way
to our attracting sets it's going to be
okay like we're it'll it'll happen but
also you think about like the way that
our our bodies and Minds have have
evolved it took a billion years and a
lot of organisms died you know so yeah
maybe it's possible that yes eventually
it will self-regulate itself but at what
cost is that you know are we going to
incur along the way is that something
that's going to be straightforward that
you know or would could there have been
a better path that would have been a bit
safer maybe slower and less efficient
that would have incur less of a a
negative loss so I mean and you know
there is a history of civilizations
destroying themselves you know that's
happened hundreds of times of
civilizations eventually just dying out
whether that's from you know Warfare or
other things they did um and you know I
I just wonder if we can be so confident
to be on that complete deregulation side
although I understand and and definitely
respect the nuances of the other
questions that come with that which is
like who regulates and who makes the
rules and there are so many other things
that go into this that it's hard to you
know formulate a complete answer but I
would say I still lie on that
perspective that um I think you need
more than just this uh putting all of
your hope into self-regulation as being
the primary component to drive decision-
making yeah I mean I hosted a debate
between Beth jez you know gim um the ex
Tropic guy he's actually interesting
because he also has a startup you know
talking about the physics of AI very
similar stuff and he was debating Connor
Lehi and obviously he's an
accelerationist and he was making you
know all of these you know yeah we
should Connor at one point in in his
best style said oh what we should trust
the void God of
entropy because he was basically arguing
that that good stuff just just comes out
of all of this but um I mean you know
I'm I'm a bit of a cist and I mean
clearly I think it was a good thing that
we ban smoking indoors but now in the UK
they want to ban smoking um in in Pub
Gardens and that seems like an an
overreach and and sometimes in
retrospect we look back and we think yes
it was a good you know good thing that
we Bann that but clearly markets have
failure modes and you know sometimes
it's good not to regulate because it
increases velocity and Innovation and
sometimes it's it's bad to to regulate
but it's it's a really difficult thing
to get a handle on I completely agree
and I think it's only going to get
harder I mean as our world becomes more
and more interconnected um there's
there's more and more considerations to
take into account and I think that's why
I was emphasizing this need for you know
Finding ways to predict in complex
systems you know like that's of course
like a pipe dream because the amount of
data you would need to do that kind of a
thing but this is the kind of thing is
like how much overreach is the right
amount of you how much overreach is
maybe not the right word but how much um
control and regulation is the right
amount um and that of course will differ
by opinion I mean everyone will have
different levels of how it affects them
um I don't think they're easy answers to
this but I definitely
think this is a really really important
area of um the technology we're
developing that should be taken into
account and should be not taken lightly
um because it's it's very easy to get
pulled into the AI hype train um it's so
exciting or it's this amazing time where
you know all these things and whenever
I'm thinking about active inference I'm
really just thinking about math like
that's just for me it's I'm excited
about oh I'm writing code and I'm making
you know looking at math equations and
then getting really excited about these
philosophical top topics but really
they'll have an impact on the world and
they really in a way that really matters
and we should definitely consider it
interesting and just in in closing I
mean if you if you could uh you had a
magic wand and you could go back in time
and you were the government would you
regulate social
media oh no that's a really difficult
question
um I mean I think I'm going to be
consistent with my prior responses which
is some level of Regulation yeah um
because I don't think inherently social
media in
itself is a problem I think it's also
the kinds of problems that have emerged
from the usage of social media are also
a response to
General the general feelings of people
in the world and they're using it as a
way of an outlet and you know for
example if you think about like you know
controlling elections and some of those
things right there's that whole angle
there's also like the psychological
angle of you know how people are are
using it those are all questions I would
ask is like why are people using it to
compare themselves to other people and
all these other things and why are they
feeling that way what about our society
means that when we have a tool we use it
for a certain purpose like this um and
there are no easy answers to that
because some of it is just like that's
just how things are right now and you
can't you can't solve Those Questions by
just um you know they're they're comp
societal problems that that lead to that
um but I do feel
that social media has been really
wonderful in a lot of ways too and it's
provided a lot of net positivity to the
world I'm really not I don't really use
it so I don't you know this is not I
don't um a very little social media
presence um at the moment but I do feel
like it's been you know amazing for
example in you know in unsafe situations
where hundreds of people have you know
used social media for communication um
it's brought scientists being able to
broadcast their papers around the world
you know there's uh it's the ability to
share knowledge and information which is
what everyone got excited about with the
original internet um so I don't know I
feel like that's a hard question to
answer and I think I would go with the
response that I think some level of
Regulation um would have been useful but
it's always hard to know if that
regulation even if done in good faith
would also lead to more calamitous
outcomes that you would not have
foreseen because you're imposing a
regulation um and this is kind of a
black markets and other stuff you know
these things happen um so I guess the
way I would put it is that I'm glad that
I'm not in government itself but I'd be
happy to contribute as a as a researcher
from the outside but these are hard
decisions and I you know don't envy
anyone who's in the position of having
to make them I don't it's certainly true
that regulation done badly is even worse
than no regulation at all absolutely and
I I agree it's very complex there are
failure modes of social media it's it's
a good thing and in some ways it's a
very bad thing and when you when you
start Banning things when when do you
when do you stop yeah and who decides
like you know I mean this same with free
speech and the other questions that we
have too is like well what species
permissible you know there's a kind of
Gray Zone of like there's certain things
you you know yelling fire in a crowded
room or you know so I don't know I think
there I think there's in in legal
systems there is the principle of least
harm where it's sort of you know you
have some level of laws that will maybe
it'll harm you know some people but on
on the whole it's good for everyone but
that's going to get harder and harder to
what that looks like I think is the
problem but I think that's kind of where
I would tend toward is like just enough
that it has least harm without also at
the same time incurring uh you know
stopping the accelerationist um at the
same time so active inference is quite
inscrutable for people coming into it I
mean I I know when I did the first Carl
friston interview I was reading some of
his papers from about 15 years ago and
they were remarkable in the sense that
they were bringing together so so much
jargon and terminology from different
fields from Neuroscience from cognitive
science from uh physics and it was it
was really inscrutable I felt that some
of his um mathematics was was a little
bit difficult to to get a handle on what
was your journey into Act of inference
so my journey um into Act of inference
started in uh 2018 I was trying to write
a grant I was actually in my first
postto and realized that I was um
wanting to leave Academia that's when I
moved into industry and at that time I
discovered Carl's 2013 um life is we
know it paper and was very inspired by
it but I didn't understand any of it and
so as I dug deeper and deeper um it just
was this endless hole of finding new
fields that I didn't know about that I
had to keep looking into and researching
and learning more and not knowing where
to stop and so that continued for about
three years of just exploration and the
same kind of thing with you're saying
this reading these papers that just pull
in so many different topics together um
everything just seemed very inscrutable
as you said interesting so um I mean
tell me more so how how did how did you
build a mental model of this I think
it's really difficult because active
inferences is an example of a discipline
where you have so many different folks
from multi-disciplinary backgrounds and
that's different from something like
machine learning so do you feel that
you've focused on one of those
backgrounds so you know let's say coming
at it from a very technical perspective
or have you embraced the the full
spectrum I think I'm getting closer to
embracing the full spectrum and coming
at it myself um since my PhD work was in
Neuroscience I had that Neuroscience
background which even though there are a
lot of other topics that are brought in
there the core of it is neuroscience so
that gave me a good anchor but I wasn't
at that particular time um as well
versed on the technical side of things
and I was actively trying to learn that
anyways um for as I was transitioning
into industry to be a machine learning
engineer um at that time so um it was
natural to start slowly bringing in all
these ideas so I would say that you know
there's some a lot of the areas in um
active inference also involve a lot of
the the philosophy and cognitive science
um that I don't know as much about more
of the neurobiology and the mechanistic
side of it um as well as now the
Technical Machine learning side and I've
had to slowly pull in these threads as
I've learned more and more about the
field I was in a neuroscience lab we
were doing computational Neuroscience I
was working with uh Dr Ken kashida who's
at uh Wake Forest um in North Carolina
United States and we were doing um some
work modeling human behavior with
reinforcement learning so it was an
adjacent field um and uh U but at that
particular time I had the Neuroscience
background but I did not know about
active inference because it was
relatively Niche at that time so you
have this Vision to you know essentially
educate um upand cominging scientists on
on active inference because it's it's
really really important why the focus on
on education why do we need this I think
the active inference field um if you
kind of look at the sort of genealogy of
How It's developed and also into new
ideas like the free energy principle
Invasion mechanics are these new terms
that have also emerged alongside active
inference there's a very very historical
perspective you can take on the field
and when you uh look at how it's emerged
through time to where it is there's no
good entry point into the field but at
the same time I see active inferences
being really revolutionary um in the
same position that deep learning was in
the early 2000s where there are a lot of
interesting ideas and papers out there
but the explosion had not yet happened
and so because we are perched on this
precipice um where we're going to start
seeing all these new and important ideas
come out of active inference and provide
it some sort of challenge to uh the way
that deep learning is currently done
today um I felt it was really important
to have an important a very clear
resource that would guide readers
through the field that did not have the
neuro ology focus on it so just looking
at the core mathematical mechanistic
ideas but also the implementation so
that I can bring it to a much wider
readership both for newcomers and
students undergraduates and grad
students but also for researchers um
engineers and machine learning Engineers
or you know robotic engineers and any of
those sort of Applied fields that use
these kinds of ideas and and what are
the high level things you talk about in
in the book and of course this is the
first edition you you've got another
edition coming out next year that's
that's correct so so there are going to
be two books they're going to be
separated for the ACT focusing just on
active inference the second book which
is going to be um submitted next year is
going to be on beian mechanics
specifically so there's a a nice
demarcation between those two um so for
the active inference book the basic idea
is to cover all the fundamental concepts
and themes you need to know so starting
from just the basic way of how can you
think about modeling this interaction
between agent and environment in a
statistical setting so that requires
just a change in perspective from how
normal statistics is actually done um
just so you can see that relationship
work and then begin to build models from
that starting um you know there's two
versions of active inference one in
continuous time and continuous State
spaces and one in discrete State spaces
so the book provides all the
fundamentals of the background I need to
know things like predictive coding
variational basion inference um
connecting um latent variable models
with more common parameter estimation
models and then leading into the core of
active inference describing all the key
papers and the state-ofthe-art and then
the third part of the book goes much
more into extensions uh where the field
has gone next um how to apply it to
different areas in a historical
perspective of how all the different U
antecedence and also influences of
active inference kind of coales together
U in this sort of Nexus where active
inference lies at the center of all of
it yeah you made an interesting point
which is that you know we we might be on
the precipice or something so it might
be like the um you know the the image
net moment back in
2011 and back then we had these
sophisticated algorithms but they were
very difficult to to do I mean you know
the only top Folks at Google were were
doing them you needed lots of computer
power and so on is is that analogy the
same for active inference so how useful
is it now really like how how good do
you need to be what kind of tools and
software do you need H how does it
work so for active inference um as its
Origins were in the behavioral
neurosciences so a lot of the early
models were designed for very simple
experiments that did not have hugely
complex environments so there has been
some work trying to look at scalability
uh from the perspective of amortised
models so you know using neural networks
to learn parameters um to kind of bridge
that gap between the active inference
world and the Deep learning World um
this is kind of a bit of a hybrid
approach and that has shown some promise
um in some areas for scalability um but
the where active inference is right now
the current software that's available
pmdp for example is the primary software
there are others that are sort of
related like RX and fur that do similar
kinds of things um we're at kind of this
interesting point where we are poised
for this kind of expansion into this
highly more scaled space and
applicability because there's been a lot
of papers looking at robotics for
example uh performances between active
inference and robotic methods or between
active inference and reinforcement
learning um but there isn't a this is
sort of why I like to make that analogy
between deep learning around 2006 it's
it's getting there it's comparable and
there are lots of really interesting
ideas that are there that show the high
potential for active inference and we
need more people to come into the field
to develop it and that's you know the
motivation for writing this book was to
get that um wider readership
involved yeah that that's that's really
interesting I mean I'm quite a few
people in the Discord Community are are
just saying to us how how how do I how
do I do this you know and many examples
come up like I want to build a
multi-agent system where the agents uh
exchange information and they cite this
agency as being one of the reasons to
use Act of inference because you know of
course in in the old days we used to
write software and we used to write for
loops and you know we had conditional
logic and stuff like that and we were
explicitly telling what to do and then
machine learning was trained with data
but it was still pretty much just a
static model and once you've trained it
that's it that's the end unless you do
something like Active Learning and then
now we're talking about active inference
where we have um essentially an agent
that can continually uh learn and adapt
and exchange information with with its
environment but that seems quite scary
right because how do we how do we make
it do what we want it to do and how do
we actually code it with
software so I think that's the the key
mechanism that you point out this
engagement with the environment um
that's a really that's the key feature
of active inference the ability for it
to you know going out and to seek out um
you know certain uh preferred or
expected states which I'll use that term
instead of reward rewarding States
that's much more in line with the active
inference
perspective um and uh in the active
inference literature um the simpler
models you know they follow this
Paradigm but it's a framework so there
are certain perspectives and ways you
can design these models that have that
behavior in there but that doesn't mean
we can't augment them with other
features and things that are really
necessary for specific environments
where we can add in our own controls add
in human knowledge or our own um I'm
using the word biases in a neutral sense
here but if we want to bias the model to
do a certain type of thing um it's not
like we're going to lose the
controllability aspect of it there's
still a way for us to inject in um our
own set of particular goals and things
like that uh into the model it's just
that the model has the ability to be
very generalizable and applicable to
many different types of modeling
situations
but we're still able to control uh
aspects of
it yeah you know in in governance we
have like we talk about good Hearts law
and we talk about things being
good-hearted so as soon as we try and
ban things or control things we
completely distort the system you know
it's when a when a a measure becomes a
Target it CE ceases to be a good measure
do we have a similar thing here right
that we we have these agential um
applications if you like and when we try
to steer it or um enforce goals then it
it almost I don't know whether it's a
binary but it very quickly loses the
very thing that we wanted it to have in
the first place which is you know agency
and
dynamism so if you look at active
inference agents this is you know this
is a problem that's also present in
reinforcement learning where there's you
know a combinatorial explosion of
different kinds of path you could choose
and to be efficient and active inference
agent and indeed humans as well you know
there's only a subset of particular
paths that you might choose
um in an environment um that are
relevant for a task at hand and I think
the same sort of principle applies in
that there's no way you can you know
explore that entire search space anyways
of things you could do and there may be
certain things we would just discount as
being certain paths that are just not
interesting to the agent so this this is
a kind of agent that would not pursue
certain types of goals um and so those
would be things that just would be a
very low probability of being selected
by that agent so you can still maintain
that flexibility of having an agent
that's able to choose what it's going to
do but you can restrict the search space
within which it's going to make those
decisions very cool but just on on the
goals thing though where where do the
goals come from for the active inference
agents you mean yeah so they do planning
as inference right which is which is a
form of you know denovo gold generation
if you like and then and then we
constrain the the trajectories of goals
as as a form of design of of the system
and and we hope that it maintains
flexibility but it still does the thing
we want it to do
reliably yeah so I think uh this is kind
of becoming going into the distinction
between when you design simple active
inference agents where we are specifying
some of these boundaries of you know
what is the agent capable of what does
it State space look like what are the
kinds of things it can do versus a true
active inference agent which is where
the field is moving now with work in
like instructure learning for example
where it's learning its own goals and
learning the its own learning
specifications itself rather than you
know the user handcrafting the
components of the generative model so um
as we start getting more into that space
I think that question is going to become
more and more relevant um we're going to
have to consider uh what what what what
kind of agent would emerge in that sort
of scenario um under the free energy
principle um you know we have some ideas
about agents that would emerge from this
would be agents that are trying to
survive that's sort of the biological
imperative or the biological influence
on the free energy principle but I still
think it's an open question of how these
agents are going to behave when you
start building models that are this open
um and that's going to be an open area
of research yeah could you could you go
into a bit more detail on on that
Spectrum so in of course in in the
natural world all of this emerges from
lower level Dynamics but when we
designed these systems certainly with
active inference now we make the agents
explicit yes so it's it's a little bit
like old school symbolic AI or or just
doing any basian modeling I I I create
all of these variables I give them names
and I represent um you know Str
structures and the causal relationships
between them so that's how it is now but
you're you're you're kind of hinting at
an evolution of that where it could do
some of that stuff by itself yes yes
absolutely and that's you know
essentially what we can do you know we
we can U you know infants um our our
great at this and we are also especially
infants because they have less priors to
work on they have less prior knowledge
about this structure the world they have
to start creating these categories and
also mapping things I'm using the word
metaphor kind of loosely here but
metaphorically saying this is like this
other thing and so I can use the same
properties to sort of link them together
and map one model onto a different model
um and leverage that information I
learned somewhere else to something
that's similar um and so in some sense
you know the priors and the the inherent
nature of those agents their their self
model that has been crafted by Evolution
so and I don't want to use the word to
theologically I just mean that Evolution
has produced an agent over time that
happens to be the kind of agent that
expects to be in a certain kind of
environment and its preferences and
habits and things like that have been
sculpted um through its evolutionary
history which is you can think of like
compression of billions of years of
supervised training uh in you know one
agent's uh DNA that's unfolds with time
as you know the agent develops so when
you know even though the agent May uh we
might say that it's you know it is is
inventing categories and things and
learning about the world first of all it
is of course constrained by Evolution
and all of that that's come prior to it
it's not like a completely blank slate
but in addition to that there's also
supervision through parents and culture
and other things that also kind of
restrict its space so when I see active
inference agents that we're building um
I think the analogy is very similar we
are sort of The evolutionary benefactors
in a way of where where we can sculpt in
certain types of what do we want this
agent to be and within that space it's
still free to explore and you know
choose its own goals and acquire
preferences and personalities and things
like that um but it won't be like a
completely open space um and I think
it's going to be hard anyways to design
agents that are completely free and open
um uh you can probably get all kinds of
aberant interesting behaviors that you
may not actually want that would just
emerge out of it and there's um uh uh
active inference researcher uh Nur Sajid
has done some work in this area as well
learning in active inference we call it
prior preferences what the agent prefers
to do and that would be
probabilistically what it expects to be
doing um you can get some very strange
Behavior depending on how you determine
that system so I think there needs to be
some guidance from uh us as creators of
these systems um in order to even have a
successful agent in the first place and
I think that's uh analogous to how
Evolution and you know growing up with
parents in society and things like that
also sculpt us in a similar kind of
fashion when you start talking about
these higher level more you know
abstract things that uh influence us so
you know society and culture and just
the general rules that we follow I think
what uh they don't constrain us
physically per se but we learn about
what is um you know permissible is
what's expected so we learn what to
expect about the world and we sculpt our
world in that way to make make it make
sense um some of these may be cultural
rules uh that are just part of you know
this is the expectation of how you
behave in this Society or this family or
this culture and um I've actually just
been traveling to Europe and I always
look up you know before I go to a new
country what is expected of me how do I
behave right because you can get into
interactions that you wouldn't even
expect things that would uh seem aberant
to someone else because their Norm is in
a different direction than yours um but
then to you is totally normal um and so
there's the cultural aspect as well but
there's also uh even just like traffic
laws for example like learning in a new
city you know how how do you behave in
the city and then for that PE for the
people who live in the city all those
items are all predictable things like I
have a stop sign you know it makes the
world make sense to you uh and makes
everything much more efficient because
you don't have to think about them
they're just these ass assumed priors
that are just part of your your
generative model um and so I see them as
those sorts of ideas don't constrain us
physically uh at least not directly but
they're still in some sense a physical
constraint because they become in a more
abstract way because they become part of
your just narrative about how you see
the world that can be very unconscious
and you don't even think about how you
behave you just know you know your brain
wants to be efficient you know this is
how you unfold these actions you don't
have to think and calculate you know
every single piece in detail about
certain things um but that in itself is
a kind of physical constraint because it
would stop you from doing certain kinds
of activities not directly physical in
in when we talk about the lower level
but in some sense still physically
stopping you from doing something unless
you make the choice to well I'm gonna
violate this rule that's expected of me
in the society or culture or something
like that yeah I'm really interested in
this kind of hierarchy of um phenomena
right so you know obviously we have the
physical world you can take certain
lenses like the selfish Gene right so
you can think of of of genes and and
memetics and you know and you can go all
the way up to culture and and language
and so on and as you say that they're
actually entangled they're sharing
information with each other and we seem
to somehow all of this MH right so you
know we we we we know how to behave
physically and verbally and culturally
when we're embedded in in different
societies I mean how would you build an
active inference agent to know how to
behave well in
Italy um yeah so I think that's uh
that's an interest really interesting
question because I think the first thing
you would have to do is you know the
actual things you would have to to teach
the agent um you know you could in some
sense I want to say the word program the
agent and just say like here are the
things you should do but but if you have
an active inference agent um the way
that I would I would design it is the
same way that you teach a child you know
people who Italians who grow up Italian
infants when they are in the world um
they come in with some pre-existing
knowledge of course about you know space
and time and some you know basic
abstract principles about how the world
works but all the cultural layer that
happens to them you know between the
ages of uh you know zero and five you
kind of probably even earlier than that
you know an infant by the age of um
actually this is just by the age of like
two I would just thinking about my I was
just at a cousin's wedding and one of my
um cousin's children uh is uh you know
she's maybe a year and a half and she
met one of her friends you know
Playmates and they put the two of them
together and they both kind of very
tentatively kissed each other's cheeks
um and they're um uh this is they live
in Switzerland and so they have these
sortal cultural norms they've just been
watching their parents they know okay
this is what's expected of me my parents
said that when I meet another person um
my peer and you know we're interacting
this is how I'm supposed to behave and
so I think the same thing would be true
of um a uh an artific um an active
inference agent um and even robots that
don't have you know priors to begin with
um you look at how infants learn
behaviors and you can just teach the
active inference agent those things so
it's sort of feedback um it will grow
and learn this is huge space of possible
actions and things that are behaviors I
could do but I'm learning that according
to um the culture Italian culture in
your example here these behaviors are
more permissible than others and this is
how I'm expected to behave as uh to be
bit factious in Italian active inference
agent these are some of the things that
are expected of me within the context of
my culture yeah good old imitation
learning yes yeah that's quite
interesting philosophically though
because that is kind of pointing to
behaviorism I mean a lot of people say
that even reinforcement learning is a
kind of modern incarnation of
behaviorism and it's really interesting
from a cognitive science perspective
because you know is is the only thing
that exists the behavior right so you
know that the children kiss on the cheek
and then and then something happens
inside the brain right or inside our our
minds how how does that process happen
so by purely imitating the behavior
physically does does that trigger some
commensurate representation or or
thinking inside our minds I would say so
so I mean you know we take the language
of active inference is all probabilistic
so there are certain things that can
happen you know nearly infinite state
state space but some things are more
probable than others and so the things
that are more probable are what we
expect so you know I can imagine that
that infant's model through imitation is
being refined over time as it interacts
and it's told you know here's how you're
supposed to interact the interaction of
um kissing as a greeting is becoming a
higher and higher probability or
expectation under that agents that
infants agent generative model and so
that is the kind of there is an internal
development that's happening where um
you know every single time you meet
someone that's the expectation of what's
supposed to happen um and you know
that's the most uncertain situation so
if you didn't if you didn't get it um
didn't get that kiss from someone
perhaps you'd be confused you know and
then you may learn there's actually more
nuanced things there's other cultures
that don't do it or you know there are
other situations where it's less
permissible there are certain you know
subsets of of of different types of
situations where the certain Behavior
applies and then you get to learn the
Nuance about how uh culture which is
very complicated sometimes the rules
apply in one situation but not another
and you know depending on the parties
involved so I think this is all building
that generative model and building the
probabilistic model of what's the most
common expectation of behavior that
would all happen internally but then
also through feedback receiving or not
receiving something after executing a
behavior will teach that agent all that
fine graining of the nuances of the
different situations in which that
behavior applies
yeah ever since I discovered externalism
it's something that's preoccupied my
mind quite a lot and I guess you could
you could take a radical View and and
you could actually argue that the
behavior itself is the locus of the
cognition and that the the brain is just
an Impulse response machine and you know
it's almost like we need to look outside
the brain to actually understand
cognition but certainly there must be
some symmetry so if if the behavior is
present certainly in the presence of
sophisticated brains then there must be
some some representation or resonance or
transfer but it's about what came first
yes so I definitely think that uh you
know that that is kind of the key
Insight of active inference which is
also present in you know Fields like
cybernetics um but being able to become
a model of that system outside you so
it's always you know relative to what
you've experienced and what's part of
your cultural or ecological niche where
you're you know you're adapted but those
things I think um must in some way it's
just more efficient to have that model
because you don't have to think about
and plan your actions in a really
complex way if you already know what's
permissible you just have to look at
situations that deviate from a norm um
and then try to understand those
deviations why you know they may make me
feel a certain way and then
understanding those deviations of
uncertainty from that expectation helps
you uh better adapt in new situations in
the world yeah because you know people
talk about biometics and and that that
simply mimetics simply means copying and
isn't interesting that even in lie of
sophisticated brains you know like even
in plant ecosystems and stuff like that
information is being copied all the time
right you know not only genetic
information but also simplistic
behaviors and you know growing towards
the Sun and and and stuff like that
isn't that fascinating you mean the the
aspect of convergence that different
types of creatures in different
environments converge upon similar types
of behaviors or mechanisms is that what
you're referring to here yeah yeah so I
suppose at at different levels of of
agency you you see you know different
types of information sharing so at the
very lowest level it it's simply just
Gene memetics yes so I think um that
speaks to just how effective that
strategy is um you know we are isolated
agents of limited information and um but
we are able to take in so much from our
experiences and our memories um and so
being able to interact with others who
have experienced different things um I
suspect that that's why you know social
cult cultural animals that have an
aspect of social um behaviors are very
successful uh this isn't of course there
are many many very successfully adapted
uh creatures on the planet but many of
them that are social tend to develop
these higher order or higher level types
of behaviors because they can do this
type of information
sharing so can you disambiguate active
inference the free energy principle and
basian mechanics um so I think this is
General a tricky task just because um
you know the way the field has evolved
um so the perspective that I like to
take on it which I think um makes the
most sense especially from an education
perspective uh knowing that sometimes
these lines can be a little bit blurred
um I see that the I look at the top down
level starting with the free energy
principle so the free energy principle
itself is uh getting into a description
of
systems um which are dynamical systems
of some kind um that are uh behaving
into in accordance with the principle of
minimizing variational free energy which
is a statistical quantity um that agents
can calculate and um in general the free
energy principle is a uh bound it's a
aoxy for something else called surprisal
um which we can think of you know very
roughly speaking is a kind of
uncertainty so uh minimization of
uncertainty which helps to restrict
agents to a specific set of states that
are um conducive to their survival
States they'll frequently revisit uh
among all states they could visit in a
state space so some of these may be you
know for us as complex agents it may be
cultural layers of it there may be
evolutionary and biological layers um
that are more specific to survival but
then as you get higher and higher they
also Encompass a lot of other kinds of
behaviors that seem less directly linked
to survival per se um that build upon
this and when you have this basic
framework um of how agents can behave
and you have the statistical quantity
you can calculate you have a way of
describing systems um that persist over
time and are self-organized in different
languages you can uh mathematical
languages you can describe them so for
example you have active inference which
is the application of this free energy
principle to the brain and animal
behavior um and then you have basian
mechanics which is a much more broader
take on the free energy Principle as
applied to dynamical systems theory so
studying the property of systems which
contain variables interacting variables
that change over time and how those
systems stay sta uh stay different from
one each other from the from the
perspective of Statistics so how they
retain their
properties so that's the demarcation
level that I see it as I see the the fre
energy Principle as an underlying
principle applied in two different
domains so how has this evolved over the
years because I'm aware there have been
different formulations of the the free
energy principle and of course active
inference came into play at at at some
point how did this all kind of evolve
over
time so um if you look at the sort of
history of these fields um they have
gone through many different phases or
changes uh that's one perspective that I
I try to emphasize in the book is um
explaining the most modern incarnation
of these ideas but also looking back
historically because that's really
important for understanding how we got
to where we are now and the The Logical
threads that led us to this this place
so the way that I see it is the very
early papers um you know we're looking
around 2000 early 2000s um we're looking
at a way of taking a lot of the work
that was done in the 90s and
unsupervised learning uh lat and
variable models a lot of this work was
done by uh Jeff Hinton garmani uh David
K and others uh many other researchers
and bringing that into the brain and
looking at cortical function and
cortical architecture and seeing how um
there could be a mathematical
description of the type of operations
the brain is doing that would match um
is is well modeled by unsupervised
learning that led into the uh a
perspective of uh bringing the idea of
action into this uh framework and in the
early papers around 2006
active
inference and um the free energy
principle were kind of described
together where the free energy principle
is much more of a descriptive term
saying you know agents want to avoid
physical damages to their physical
structure so being crushed or you know
undergoing what they call a phase
transition like you don't want to
experience a certain range of things
that are permissible for your body's
physical integrity and so what actions
do you take uh and what do you need to
do as an agent to prevent uh
disintegration or you know just becoming
part of your the external environment
you're no longer different from the
atoms in the rest of the atoms in the
world around you um and then as the
field evolved those two terms sort of
branched out so around 2010 you start
getting more of what we call Active
inference in continuous time and you get
a bit more of what is now called the
free energy principle but then as those
fuels evolved further um around 2015 you
start getting more of the discret uh
formulation of active inference which we
can we can talk about the distinction
later between continuous and discreet
here um and then we also have the free
energy principle becoming kind of this
overarching philosophical and
mathematical basis for active
inference that later evolves into what's
now called basian mechanics around 2018
and so the way I see it from here we now
have three different strands that are
kind of uh going out from this point
onward that all interact with one
another but have slightly different uh
sets of papers and formalisms that are
associated with them but are all unified
under this same umbrella of this theme
of free energy minimization so what is
active inference active inference then
is uh so the application of the free
energy principle to the to agents that
perform perception and action so these
can be biological agents that have
brains human human brains animal
behavior um and uh and this view is in
literal name actively inferring so
everything under active inference is
inferential
meaning that there is an unobserved or
un unknown quantity out there what is a
state of the world and so that has to be
inferred indirectly from some kind of
Sensation that these agents receive and
that inference process is perception
active inference um and actually just to
say this now the inference process uses
the free energy principle in that it
minimizes variational free energy in
that calculation so that statistical
quantity is the basis uh in machine
learning um talk it would be an
objective function that we try to
minimize which will enable the agent to
um describes the behavior of the uh
neurons or representation units in that
agent coming to a steady state the
behavior of those neurons as they
interact follows a gradient flow on
variational free energy so it describes
what that system is doing its behavior
and that process of perception um which
inherently
relies upon free energy minimization is
taken to the next level with active
inference to suggest that action itself
is also a form of inference that also
requires minimization of variational
free energy so I'm saying this
specifically to underscore the idea that
this property of free energy
minimization inherent in the free energy
principle is a component of both action
and perception and these are kind of
unified together they're not under a
separation principle separate modules in
the brain where you do one and the other
they're kind of linked together and
they're linked as one
type of thing that the brain does where
action is a prediction or an expectation
um about the kinds of sensory
information the agent expects to receive
and it controls its environment to bring
about that State of Affairs so when you
link these together you get essentially
what we call U active inference um which
is all written in this language of
probabilistic models where the optimal
action or the optimal perception is
described as a minimization of
variational free energy it might be
worth just meditating on this for a
second so in in machine learning we just
have the perception bit and it's also
relevant that we just usually train once
so now we have perception and action
linked together in what I guess Frist
would call a cybernetic Loop so we but
the iterative component of information
sharing is is really important I think
computationally the reason why we can um
encourage the emergence of sophisticated
behavior is because we have there
iterative inference and and information
sharing with with the environment and
that seems to set it
apart yeah so you mean inative iterative
in the sense of like online like you're
continuously learning um as information
is streaming in and that's changing the
model yeah that that's right Ian it's a
similar thing to a cellular automaton
that that's also iterative so at every
single time step we are using the the
state from the previous time step and
then we're performing a computation and
we're we're iterating and iterating and
it's essentially an unbounded comput
and that's a very very different type of
computation to machine learning where
you just compute a bunch of Weights once
and then that's it that that's the end
yes so that's a really good point I one
thing I I didn't mention in my
description just a moment ago was the
temporal nature of this that's just
inherent in these models it's always
assumed that the environment is dynamic
which means it's always changing now if
you in a this speaks to this the idea of
um inferential learning um you can
constantly keep updating these models as
new information comes in if you're
perceiving something different that
changes um how your action is going what
actions you're going to take so this
level of control is really intimately
linked to what you're perceiving because
if the State of Affairs changes outside
in the world you're in a new location
something is new you may have to perform
new behaviors in order to reach back to
this free energy steady state um and so
this is a kind of iterative process as
you say it takes place over time um and
the Dynamics is really important in
these models and it also kind of speaks
back to some of the other topics we were
just discussing in that um a very
volatile environment um you know it may
be hard to find that Minima but if you
build in a lot of um a lot of
predictability into your environment
things that are very stable um things
that are very predictable because you
have cultural rules or you know rules
about how the world operates uh it makes
it much more efficient and easy to find
that Minima um because there's less
things to consider uh as options in
there yeah we don't need to meditate on
this for too long but of course in in
the ml World they do have reinforcement
learning and and that that also has this
kind of iterative uh property what
what's what's the rough difference
between the two
approaches so I would say the there's
there's a number of different um
differences in there in that for example
um in in machine uh in reinforcement
learning um the state action mapping um
is what you your policies that you're
attempting to determine what should the
agent do in terms of uh what actions
will bring it into certain States versus
the active inference approach um is
looking to compute using as an inference
problem uh sequences or trajectories of
actions the agents you can think of them
as paths if you want to use kind of a
physics metaphor of um actions being
these kind of motion paths over time um
then you can think of them unfolding in
time and picking the right path to reach
some sort of end goal State um and so
that's what you what's called a policy
sometimes it's better to call it a plan
so that we can disambiguate from
confusion with reinforcement learning um
it's a plan of action sequences to
execute at a given moment in time based
on the information and knowledge the
agent has in that present moment um
combined with its preference of what it
wants to I'm using that
anthropomorphically but you could say
expects to uh receive in the future and
so a component that I think really
separates active inference and
reinforcement learning is the appearance
of this um epistemic value term that
appears in what's known as expected free
energy which uh roughly speaking is a
way of using uh uh is is a way
of a function that you minimize uh for
for selecting policies in the future so
we information you don't have yet
because variational free energy is used
for current and past information
expected free energy is minimized for
future planning uh picking out those
plans or action
sequences and what's really important
about active inference agents is they
have the ability to forage for new
information and look around for
interesting and new information in their
environment um and reduce the ambiguity
um that they have about their
environment in order to increase the
confidence that they can actually reach
that goal
state so there's a natural uh level of
curiosity and exploration inherent in
active inference models that are part of
the way that an active inference model
uh active inference agent functions yeah
sly in reinforcement learning you can do
exploration but I can appreciate it it's
it's far more principled if you do it in
in a basian way because you actually
know where where there are areas of
uncertainty when I spoke with Jeff Beck
about this he said that admittedly it
was kind of similar to an in like a
maximum entropy inverse reinforcement
learning agent but surely it must be
more principled in other ways though
it's a really good way of of defining
you know States and and agents and being
able to design much more structure into
into the system rather than the
inscrutability of reinforcement learning
yes I think the the key the key way that
active inference looks to this problem
is purely inferential so it's sort of
the planning as inference statement um
all of the behavior that you see in an
active inference agent uh does not
require um you know ad hoc temperature
parameters or other kinds of things that
you do to tweak the system um the
behavior just emerges out of the
expected free energy uh equation itself
and so you you have uh this idea of
curious and exploratory Behavior as a
kind of inference in itself um where you
have this
long-term uh you know incurring of
uncertainty or exploration for the
purpose of in the long-term average
minimizing variational free energy uh
for future States so this sort of
perspective on um inference and the way
that it's encoded in the model is
different in the sense that from the way
that it's performed in reinforcement
learning sanj thank you so much for
coming on I really appreciate it
absolutely thank you so much for having
me

welcome to Doom debates I'm Lon jaira I
hope you've been having a good last few
weeks I hope you've been going to some
cool parties spending time with your
friends and family maybe doing some cool
outdoor sports or enjoying nature what
I've been doing is just scrolling
Twitter specifically the part of Twitter
that's talking about Ai and existential
risk and policy around that and the next
breakthroughs and how soon they're
coming today I invite you to experience
my World vicariously join me on a
journey into the world of Twitter
beefs the first beef is with Jack Clark
who's one of the co-founders of
anthropic Jack was recently tweeting
about how there's this new harder
Benchmark for math that's extremely hard
for humans but it's actually still hard
for AIS too it's one of the only current
benchmarks that AI aren't easily passing
so here's Jack Clark's tweet he says AI
Skeptics say llms are copypaste engines
incapable of original thought basically
worthless professionals who track AI
progress say we've worked with 60
mathematicians to build a hard test that
modern systems get 2% on hope this
Benchmark lasts more than a couple years
and then he's got a quote from the
famous math petition Terren to the 2006
Fields medalist Karen is saying these
are extremely challenging I think they
will resist AI for several years at
least and then Jack Clark writes I think
if people who are true llm Skeptics
spent 10 hours trying to get modern AI
systems to do tasks that the Skeptics
are expert in they'd be genuinely
shocked by how capable these things are
there is a kind of tragedy in all this
many people who are skeptical of llms
are also people who think deeply about
the political economy of AI I think they
could be more effective in their
political advocacy if they were truly
calibrated as to the state of progress
so that's Jack Clark's tweet and then
the person who comes in with the beef is
actually the official account of pause
AI us P AI us comes in with a quote
tweet saying oh it's tragic how people
don't understand the power of your God
machines because their political
advocacy against the god machines is
less effective why don't you stop making
God
machines Jack Clark is a likable guy
very much in the style of Dario and most
of the rest of the anthropic team these
are thoughtful people normally new on
normally mature people but at the end of
the day this whole project called
anthropic is a pipe dream it's
neglectful of the very idea that
building a safe AI sometime in the next
decade or two when we have virtually
zero theory about how that would even
work in terms of alignment that whole
idea is a non-starter and that whole
company anthropic and just like the
other AI companies is a pipe dream
that's neglecting the possibility that
safe AI is impossible and they're just
like look we'll do our best because
there's no other choice but to do our
best right and so the thought that
they're not allowed to think is oh we're
just burning down the last few years
that we have before super intelligent AI
is here and instead of tapping the
brakes we are just trying to make it
safe as best we can and if it works it
works if it doesn't it doesn't right
there's zero effort to dignify the Pai
effort from these AI Labs so you've got
the P us account kind of calling them
out just being like hey you know what
you are the problem
you're actually not the good guys right
you're part of the problem and you're
doing something that you can't reverse
and you're legitimizing not just
yourselves but the other AI Labs right
open AI you're not yelling that they
should stop right at the end of the day
you may be mature you may think of
yourself as the good guys who are safety
conscious but you're absolutely part of
the problem and in fact P their next
protest is coming up on November 22nd
and everybody's invited they just
tweeted out anthropic wants you to think
they are the good guys but they are at
the head of the pack making dangerous
artificial intelligence developing more
and more powerful AI without safety
regulations in place is not safe period
join us in protest and then there's a
Facebook link to sign up for their
protest it's happening next Friday
afternoon November 22nd at the anthropic
office in San Francisco everybody's
welcome I personally am planning to go
and I just donated a th000 bucks to P I
think it's one of the best organizations
that you can be supporting right now now
the US branch of p is run by my friend
Holly Elmore and I think she's been
doing an amazing job moving the Overton
window the window of what you're even
allowed to say and talk about to
publicly be making these kind of
statements like hey anthropic sorry but
you're part of the problem right you
guys should not be legally allowed to do
what you're doing you're kind of like
criminals before the law catches up in
our opinion right if we summon the
Democratic power to make that the law
before it's too late well then their
current project should be
right like that's that's our position
and Hol has been doing a great job
letting rationalists talk about it
because for so long there was so much
trepidation in the rationalist community
from which you got elzra owski or rather
which maybe formed around elzra owski
right that whole Community has just
always been in this very deliberative
thoughtful intellectual mindset and it's
just the farthest thing from their
nature to like run out on the street and
yell stop building this AI before you
can make it safe and it's this
distinction that a lot of people refer
to as a scout versus Soldier I think a
coin term by Julia gay in her book uh
recent book The Scout mindset everybody
wants to just be a scout and just look
out at the territory and just analyze
what's true nobody in the rationalist
community wants to go out and be a
soldier and just be like hey you guys
are wrong you need to stop they feel
more comfortable writing a paper being
like probability of Doom seems high for
this and this reason but also there's
this and this mitigating factor and kind
of lobbing the paper over the wall
hoping somebody else will take it and
run and do the next step of affecting
policy affecting social change like they
they feel like their work is done that's
my model of the average rationalist I'm
a pretty big rationalist myself but I
guess I'm just more comfortable being
like hey uh this seems right and the
time to act needs to be now and I don't
mind helping out the soldier part of the
movement and you know Soldier is such an
intense word it's literally just like
engaging in a protest right I mean
normies protests all the time
without getting character assassinated
as like hardcore soldiers right I mean
protesting in the Normy world is just
considered something you can do to
defend your beliefs right and not enough
rationalists are doing it but look Doom
debates we're not exactly a rationalist
channel here right I I'm hoping to reach
a wide segment of the population with
these kinds of videos just being like
hey this thing is worth protesting right
this needs to make it out of the
rational sphere out of the people who
just like thinking many decades into the
future and like um this is actually
something that the AI labs are have
gotten way ahead of their skis and like
normal people just need to yell at them
and then we need to yell at our
representatives in Congress like we need
to just act right it's it's time to stop
thinking and start acting because Lord
knows the AI companies are acting as
fast as they can with the best talent
that they can and the most money
imaginable right we're talking over a
hundred billion dollar at least if you
look at their combined valuations so
it's a huge force a raate against us I
want to read you an excerpt from a
recent blog post by the head of Pai us
you know Holly elore post is called
Scouts need soldiers for their work to
be worth anything since I started Pai
I've encountered a wall of paranoid fear
from EAS and rationalists that the
slightest amount of wrong think or
willingness to use persuasive speech as
an intervention will taint the person's
mind for life with
self-deception that politics will kill
their mind I saw people shake in fear to
join a protest of an industry they
thought would destroy the world if
unchecked because they didn't want to be
photographed next to an unnuanced
sign they were afraid of sinning by
saying something wrong they were afraid
of sinning by even trying to talk
persuasively the worry about destroying
one's objectivity was often phrased to
me as being a scout not being a soldier
referring to Julia Gay's book scout
mindset I think we have all the info we
need to contradict the fear of not being
a scout in her metaph for jumping ahead
here's another excerpt if a scout
reports what they found to a superior
officer and the officer wants to pretend
that they didn't hear it A good scout
doesn't just stay curious about the
situation or note that the superior
officer has chosen a narrative they
fight to be heard because the truth of
what they saw matters to the war effort
the success of the Scout and the officer
and the soldier is all ultimately
measured in the outcome of the war one
more excerpt this is very scary for a
contingent of effective altruists and
rationalists today who treat thinking
and talking as sacred activities that
must follow the rules of science or less
wrong and not be used for anything else
I think these concerns are wildly
overblown what are the chances that
amplifying the message of an org you
trust in a way the public will
understand undermines your ability to
think critically that's just
contamination
thinking so I highly recommend reading
the whole post I'll put it up in the
show notes there's one more thing you
need to know about posi us which is
they're fundraising right now they're
still a small organization they're
making a big impact in my opinion even
when just 30 or 40 people show up to a
local protest at one of these AI Labs we
get a disproportionate amount of
attention a disproportionate amount of
coverage because everybody's wondering
who's pushing back against these
companies it's such a David and Goliath
story right now who is going to step up
and be David unfortunately very few
people are stepping up and being David
even while experts are predicting um
yeah AGI is uh 3 years away maybe 5 or
10 years away maybe 15 years away like
it's time for David to actually get some
power here so if you want to be part of
that cause I'll put up a link in the
show notes to Their donation page right
now you can read more info about them uh
you can write a comment and Holly Elmore
I'm sure will respond they've got
details about what they're spending the
money on looks like the core team is
Holly and two other people right now so
yeah definitely take a look if you're
wondering about me and doom debates this
is just an independent project on my
part not officially affiliated with paii
but if if you go to pa.info and click on
the link to join the paai Discord not
only will you be joining a great
Community but there's actually a channel
that they made for me it's called hash
Doom Das debates and I'm there because
even though this isn't aai initiative
I'm a member of posi and I go to the
protest all the time and I'd love to
meet you and hang out whether we just do
it over the Discord Channel or we could
even potentially interact in meets space
if you come to these protests so that's
paii us click the link in the show notes
if you want to donate let's move on to
the next
beef here's a fun Twitter beef from a
few months ago this one's between elazar
owski and you'll see it starts when
elzar tweets I think he's subtweeting
some interactions that he's had and he
tweets the kind of guy who has at some
point learned linear algebra who is very
proud of knowing linear algebra and who
presumes that all your disagreements
with him about AI must be because you
don't know linear algebra telling that
you know linear algebra won't help a
similar kind of guy has learned about
self attention or something mostly he is
identical to the first guy except even
prouder you cannot possibly know how kqv
matrices work if you say you know you're
lying if you knew how could you go on
disagreeing with him about AI perhaps he
works in the AI industry now he has
personally learned by hard sweat that
AIS are stupider than you think or that
they are smarter than you think he has
earned this knowledge by work it cannot
be systematized and supported by
observation you can't have it sometimes
he has even learned that there is math
inside AIS did you know that you can't
know that you wouldn't be scared of AIS
if you knew that he knows how
multiplication works it's just numbers
it is not
scary if this sounds like a straw man to
you you remember in the first episode of
Twitter beefs I looked at amjad mad who
literally tweeted something like tell me
how multiplication of numbers could
possibly kill me in the real world or
Mark Andre has actually tweeted
something as explicit as AI is just math
it can't hurt you it's just math this
was quite a low-level failure mode and
yet sadly it's not uncommon at all even
among smart people who You' think would
know better so this is just elazer
venting about an experience that Rings
true to me now a guy named GM Verdon who
on Twitter is known as based Beth Jos
replies to elazer maybe he was even one
of the guys that elazer was subtweeting
regardless here replies to the threat he
says actually you also need a background
in functional analysis optimization
Theory probability Theory complexity
Theory and thermodynamics to make
coherent full stack arguments about the
future of AI elazar replied what's an
example of a truth about the future of
AI that requires functional analysis to
understand also you left out
evolutionary biology whose relevance is
indirect but pretty important and which
I discussed in the simple math of
evolution sequence so nobody has to take
my word on it and then elazer also added
in a quote tweet whenever I say anything
remotely like this I lay out the
arguments and I do my best to explain to
the reader the basic math I think
relevant that is one of the differences
between intellectual leadership and
intellectual Showmanship and bullying
it's incredible that your followers are
crowing about such blatantly bad conduct
I do agree with Alazar on that I mean
Beth basically just listed a bunch of
names of Fields instead of engaging with
the argument and then declin to engage
when asked pseudo intellectual tweets
are his specialty here's one of my
favorites it's from a debate that Beth
had with conlay and I just want you to
savor how the wheels are turning in
Beth's mind asking himself how do I just
inject the most buzzwords without
actually needing to touch on this stuff
to make my point but how do I make
myself sound as fart as possible let's
listen and by the way the first guy is
Connor Ley and the second guy is Beth a
lot of what civilization is lacking is
coherency is that we constantly are
shooting ourselves in the foot for
literally nothing interesting point I
think I think we have different models
of like how how much Mutual information
we can have about the future obviously
like we we we know that physics kind of
uh Newtonian physics uh you know time
translation and variance so we evolved
ways to navigate in Newtonian physical
world but I I I think like in the cases
where Dynamics we have very low Mutual
information about the future actually
actually random can be your friend like
to to to search and find new Optima but
uh again I think that this tracks uh in
my model of your model of the world
where um if if you have high predictive
power you have high Mutual information
that I mean coherence in in quantum
mechanics is proport some form of mutual
information with a future um then then
you you can have stronger uh methods of
of optimal control um but um let me also
give an example where coherence is also
relevant to search a lot of what
civilization is
so that was polite of Connor to kind of
let the pseudo intellectual rant go and
move on to making his actual point if
you're wondering what Beth was talking
about in the clip I just played with him
and Connor Ley he's just trying to make
a point that maybe the future is
unpredictable and he's busting out this
detail about how Newtonian mechanics has
time invariant physical laws which is
such a random detail if you're just
trying to explain why a physical system
is predictable it's predictable because
it's non-chaotic right it has to meet a
lot of constraints to be predictable the
fact that physics are time invariant is
the kind of random detail that you only
bust out when you're just intentionally
trying to use
buzzworthy to pass it off onto the radar
as if you actually are making coherent
points that a smart person would make
somebody's described Beth as a reverse
Richard Fineman where instead of trying
to take points and simplify them and
give you the simplest version that still
helps you understand the point he just
does the opposite where he takes
something that hopefully there's some
point not in every case but let's say
that there's some point and then he
tries to expand it out into saying hey
how many associations can I make how
many different buzzers can I draw to
make you think that my brain has gone
off and done a bunch of hard-earned deep
connections instead of optimizing for
how do I just present to you the point
in the simplest crispest way that
actually gets the job done of having you
understand the point now funny enough
since we're talking about Richard feeman
there's another gentleman who responded
to the thread with bef Jos and alzer
remember Beth was originally talking to
alzer about all of these different
fields that you supposedly need to be an
expert in in order to understand AI risk
so funny enough Richard fineman's son
Carl who actually follows AI Twitter he
quote tweeted the part where Beth was
saying you need a background in
functional analysis optimization Theory
probability Theory complexity Theory
thermodynamics and Carl fan wrote for
people who wonder if this is true I have
a background in all of these things and
I can assure you this is nonsense
unchecked superhuman AI is possible and
extreme dangerous and the arguments for
that don't use any of those
tools so yeah I mean Carl fan and
elowsky are obviously correct and
everything that they've said in this
Twitter beef there's not really much of
a both sides to it it's just look I'm
here cataloging these beefs right as
long as the state of discourse is so low
where somebody's so obviously wrong and
yet still getting tons of likes tons of
retweets I'll just be here calling it
out right maybe the discourse quality
will one day improve and I'll only tell
you about Twitter beefs that are like so
high level between one smart goodfaith
person and another smart goodfaith
person but for now we have pseudo
intellectual bef jzos beefs it is what
it
is the next Twitter beef is with
Professor Jeffrey Hinton you may know
him as 2018 touring award co-winner
together with yosua Benjo and Yan laon
well he's not just that anymore he's
also the 2024 Nobel Prize winner in
physics you may have heard about a month
ago he's now a Nobel Prize winner so I
like to use his name as often as I can
because it lends massive credibility to
the AI safety effort to bring up a Nobel
Prize winner who warns about AI Doom
regularly and of course he's also one of
the Godfathers of the field I've heard
Dr Hinton say on various interviews that
he's happy to help raise awareness about
AI risk he's actually looking for ways
to personally contribute so I reached
out to him on Twitter publicly I wrote
Jeffrey Hinton big congrats on the no
Bell sir thanks for speaking out about
AI risk can you please come on my
podcast to discuss in more
depth well the man himself never
responded to that but I was happy to see
that I got 126 likes which is kind of a
lot for a random tweet like that so if
you one of the people who liked it
thanks very much thanks for supporting
the show thanks for recognizing the
value of bringing on these luminaries
and just explicitly asking them you know
what's your P do what do you think
people can do to help you know get the
message out there if you're watching
this and you have a connection to
Jeffrey Hind
try telling him to check out Doom
debates maybe he'll like the content and
maybe he will agree to come on you know
he's really busy I'm sure he didn't see
my tweet but I do think that Jeff and I
could have a very productive discussion
if you check out the Doom debate
archives and you watch How I interview
other guests like Dr Lee Cronin Dr Robin
Hansen you can see these are highquality
interviews so Dr Hinton if you're
listening to this I think you are
aligned with the mission of this show
which is to raise public awareness about
the risk of imminent human Extinction
from artificial general intelligence I
think we're on the same page that this
is a big risk and if you want to propel
the show's growth into the next tier by
gracing us with your presence please hit
me up anytime on Twitter DM or email
wisg gmail.com thanks Dr Hinton okay
back to the Twitter beef this has to do
with what Dr Hinton was saying right
after he won the Nobel Prize he gave a
press conference at the University of
Toronto where he's a professor Meritus
and he said something funny about his
former student Ilia satk and what Ilia
did last year to Sam almond so open AI
was set up with a big emphasis on safety
um its primary objective was to develop
artificial general intelligence and
ensure that it was
safe um one of my former students Ina
was the chief
scientist um and over time it turned out
that Sam Al was much less concerned with
safety than with
profits and I think that's um
unfortunate I'm particularly proud of
the fact that one of my students fired s
malman um and I think I better leave it
there and leave it for questions to give
you more context on Professor Hinton you
know over the last couple years he's
come out as a full Doomer kind of
tracing in the footsteps of elzra owski
a couple years ago he quit his highly
lucrative JW by Google and he explicitly
said he wants the freedom to speak out
about major concerns that he sees and
he's even been talking about kind of
regretting his research or not seeing it
coming how rapidly we'd be facing these
potentially existential threats like
he's kind of getting into the Doomer
mode so I've tweeted from my own account
a couple Choice quotes of Dr Hinton over
the last few months or the last couple
years here's a recent clip where Jeffrey
Hinton says his independent assessment
of P Doom is more than
50% and then he humbly adjusts it for
other people's ass ments as if other
people would know better than Jeffrey
Hinton so he adjusts it downward because
of other people in his life and he
arrives at 10 to 20% P Doom I think it's
cool that the Godfather of AI keeps
getting more explicit that he sees how
effed we are you know so many other
people don't but he sees it so good on
you Dr Anon here's that clip you want my
P do uh I think it's a sub question um
you might expect Doom from other sources
I'm talking more about something like P
misalignment um pretty high I think rhf
is a pile of crap I actually think the
risk is more than 50% of the existential
threat but I don't say that because
there other people think it's less and I
think a sort of plausible thing that
takes into account the opinions of
everybody I know is sort of 10 to
20% you know we still a good chance of
surviving it but we better think very
hard about how to do that that's
alarming here's another recent clip of
Dr Hinton talking about AI existential
risk and he specifically names open a
and
meta I think the profit motive we saw
what happened in open AI when everything
was stacked in favor of safety MH
because the the company that owned the
for-profit bit was a not for-profit bit
which was set up to
be concerned primarily with safety and
even in that circumstance where it was a
very unlevel playing field it was tilted
all the way tilted toward safety and
profit one um so that doesn't seem
good and while I'm at it there's one
other thing I want to mention which is
that I think open sourcing the biggest
models is completely crazy mhm my good
friend Yan Lan thinks it's the right
thing to do um he thinks we're all going
to be fine we'll keep in control of
these things they won't have any goals
of their own um they won't have any
desires of their own I think it's very
very dangerous to open source them I
think it's like open sourcing nuclear
weapons
so so I think I I would love governments
to forbid companies to open source big
models strong language from one of our
top AI scientists right The Godfather of
the field a Nobel Prize winner so don't
let anybody Gaslight you as if this is a
niche opinion that we're all going to be
doomed or it's a niche opinion that we
need a harsh government regulation no
I'm sorry this is what the adults in the
room are saying and everybody else is
really enjoying life trying to ignore
this message right like the razor blades
are whirling razor blades are right in
front of their face and they just refuse
to see it and they're seeing how long
they can go without seeing it here's a
tweet from the man himself October 31st
2023 Jeffrey Hinton writes Andrew ing is
claiming that the idea that AI could
make us extinct is a big Tech
conspiracy a data point that does not
fit this conspiracy theory is that I
left Google so that I could speak freely
about the existential threat and then I
replied # believe Hinton and I put in a
photo from a protest we did a Pai
protest from last year in San Francisco
one of the signs was a big # believe
Hinton you got to believe Hinton
people okay my next Twitter beef is with
Samuel Hammond I've been following him
on Twitter and he tends to post
thoughtful nuanced analysis so that's
cool he's not enough of an AI Doomer for
my taste but I still respect him
he recently tweeted a screenshot from a
big post he did called 95 thesis on AI
so he kind of brain dumped a bunch of
different thoughts he had on AI and he
quoted one of his own sections the
section on AI progress is accelerating
not plateauing and he made some of his
own predictions uh I'll just read you
three of them that I think are relevant
to this little beef number one the last
12 months of AI progress were the
slowest they'll be for the foreseeable
future okay plausible number two scaling
llm still has a long way to go but will
not result in super Intelligence on its
own as minimizing cross entropy loss
over human generated data converges to
human level intelligence like not super
intelligence hm not sure about that but
okay and then number three exceeding
human level reasoning will require
training methods Beyond next token
prediction such as reinforcement
learning and self-play that once working
will reap immediate benefits from
scale okay my beef is just with a Point
number two where he says scaling llms
won't result in super intelligence
because their training process is just
training them to predict tokens that
humans wrote so you can't get something
smarter than human just by trying to
predict tokens that humans wrote I don't
think you can make that logical
inference I think you're you're jumping
too far like I don't want to go full
logic cop because it's a reasonable
guess it's a reasonable intuition but
it's not a logically sound connection
that he's making here so my reply tweet
to him is I said I don't see how one can
claim Point number two humans use clui
analogies to attack hard evolutionarily
unfamiliar domains like modern math and
Engineering AI that learns to predict
the result of such efforts will in my
opinion probably do it by understanding
the underlying domains properly in other
words if the AI is reading over some
Physics textbook or some intelligent
essay on a complicated topic and the AI
is just learning to predict what this
particular smart human wrote
the AI might train an understanding of
itself that's not just an understanding
of the human who wrote this tretis it
might be an understanding of the
underlying domain it might be a deep
understanding of physics and if the AI
can understand physics so deeply that it
knows how to predict what a human
physicist would write in a textbook that
doesn't mean that the ai's intelligence
is limited to the human's intelligence
it might just mean that the AI
understands physics really really well
if the AI wants to predict what the next
textbook author is going to write in a
physics textbook the AI might just draw
on its insight about physics be like oh
yeah physics works like this so if a
chapter is called this it might explain
this kind of physics now you might argue
okay well the AI is not going to invent
new physics it's just going to draw on
the physics that it knows maybe you
don't know that because what if the next
chapter heading is reconciling
Einstein's theory of relativity with
Quantom Theory something which is kind
of an open problem still in today's
physics but the AI sees that chapter
heading and it'll just be like oh okay
yeah humans must have figured that out
here's the answer right you don't know
that it doesn't know the answer you're
making an assumption that because it
hasn't seen the answer already it's not
going to regurgitate the answer but
that's a separate assumption being
trained on human tokens doesn't mean
that you don't get an understanding that
doesn't let you generalize outside the
tokens you've seen if you see a bunch of
humans writing about physics you might
generalize okay yeah physics I know
physics now and if the next human writes
a book that you haven't seen yet you
might be able to predict what's already
in that book
if you look at Albert Einstein the human
he only got trained on physics written
by other people right he didn't get
trained on the theory of relativity and
yet he came up with a theory of
relativity right so he made some kind of
leap you can't say that just because
somebody's only seen other types of text
then they're not going to go make their
own leaps right he's Sam is kind of
sneaking in an assertion here that AI is
not going to LEAP outside the data that
it's already seen but we know that it
makes small leaps right like we know
that you can give it a title of an essay
that's never been written before and be
like hey pretend a human wrotes this
essay and pretend the human is very
intelligent and has good analyses what
might the human write in such an essay
and AI will spit out an essay that's
like a perfectly plausible essay so if
the next essay is like a new Theory of
physics who's to say that the AI can't
do that right I I mean maybe in practice
it can't today right it seems like today
we don't have ai running away doing this
stuff I grant that but who's to say that
it won't do that in the next generation
who is actually drawing a clear
separation you don't get to sneak in a
clear separation of what the AI can and
can't do just by observing that it only
got trained on tokens outputed by Dumber
humans so I'm not saying Sam's wrong I'm
just saying Sam hasn't really Justified
his claim and I don't think that either
of us know I think you just have to be
humble and wait and see or better yet
you shut it down don't wait and see so
that's the end of that beef I never got
a reply but it's no big deal we can move
on to the next beef
the next beef is with Yan Lon on the
topic of predicting the timeline to AGI
so recently yon tweeted I said that
reaching human level AI will take
several years if not a decade Sam mman
says several thousand days which is at
least 2,000 days or 6 years or perhaps
3,000 days or 9 years so we're not in
disagreement basically they're both
saying hey it'll take several years if
not a decade and then Yan continues but
I think the distribution has a long tale
it could take much longer than that and
AI it almost always takes longer in any
case it's not going to be in the next
year or two now elazar feels like he has
to weigh in on this claim that AGI is
definitely not going to happen in the
next year or two because as I often say
on this show I don't think any of us
really know what's going to happen when
we scale up llms I think we really need
to be humble about how little we know
and how often we're already getting
surprised so when we see Yun confidently
saying it's not going to happen in the
next year or two if you watch my first
episode of Twitter beefs when it was
called called Doom tiffs number one if
you watch that episode there's a funny
clip of yan Leon predicting that llms
won't be able to answer simple questions
about physics I don't think we can train
a machine to be intelligent F text so
for example I take an object I put it on
the table and I push the table it's
completely obvious to you that the
object will be pushed with the table
right because it's sitting on
it um there's no text in the world I
believe that explains this and so you
train a machine as powerful as it could
be
you know your gbt
5000 um or whatever it is it's never
going to learn about
this um that information is just is not
present in any tent right something that
got disproved very quickly after he made
that prediction so Yan's add it again
making a confident prediction about
what's totally not going to happen in
the next year and elazar comes into the
thread and he calls him out he
says what do you expect to have already
seen 2 years before human level AI is
possible to put it in another way what
algorithm are you following such that
actually 2 years before a human level AI
you will not still be stating not in the
next two years my own stance I don't
know when so that's elazar just making
the case for humility because the way y
Quinn's talking right now is just not
inspiring confidence of somebody who's
epistemologically reliable at making
these kinds of predictions somebody
who's careful enough to not accidentally
say that it's not coming when it is
coming doesn't seem like he's being
careful enough okay so Yan
replies I'm not following any algorithm
I'm working on new architectures like
JEA World models you can look that up
it's like Yan Leon's own framework that
I haven't seen many other people talk
about uh but anyway Yan continues infms
procedures planning and reasoning by
optimization in embedding space and
learning paradigms self-supervised
learning when a system with these
component starts working we'll be on a
good trajectory and you know by good he
means it'll get us closer to AGI so he
not even acknowledging that maybe having
AGI will be disastrously bad he's just
using good as synonymous with AGI that's
my interpretation at least so now
there's a few different sub threads
where elzar also chimes in here's a sub
thread from orge Hernandez where he's
going back to answer El asra's question
about what would you definitely expect
to see two years before AGI and hor is
saying algorithms capable of maneuvering
and meeting their goals in highly
Dynamic environments which are also
capable of doing continuous learning I.E
no need for separate training phases
like a cat and alaz replies why do you
expect to see that 2 years before human
level AI rather than 3 months before
human level Ai and Yan laon jumps in and
replies it'll take longer than 2 years
doesn't really explain why but just says
it maybe he's too busy to reply and then
Jorge Hernandez reply on and says indeed
cat level AI to human level AI will take
us several years and one or two more
architectural algorithmic breakthroughs
so I don't consider these repli to be
sufficient just to ification but you can
imagine just busy people making
predictions without diving deeply into
how they're thinking fair enough it is
just Twitter after all right it's not a
serious Forum now another guy named Chad
in a different subthread he says I don't
think scaling it bigger is good enough
they need a breakthrough just like
Fusion Energy needs a breakthrough but
that seems to be regarded as an
incorrect opinion right now and elas
replies why do you expect you would know
two years earlier if that breakthrough
were on the way and Yan replies to that
he says because those of us working on
this know how hard it is it's not that
we don't have the basic ideas or
concepts it's that making them work is
really hard and takes time so that's the
end of this beef it didn't really go
anywhere from there and again Yan to me
is just sounding like somebody who
doesn't have epistemic humility cuz the
thing is on the object level he may be
right let's even say there's a 75%
chance he's right and a 25% chance he's
wrong it's just that he's not reasoning
reliably when he says I know how hard it
is because I work on it he sounds like
the Wright brothers right where they
started working on understanding flight
and Wilbur Wright famously said in 1901
not within a thousand years will man
ever fly presumably he was coming from
the perspective of I know how hard it is
because I actually work on it and it's
extra ironic because he himself was the
one who built the first working flyer
together with his brother in 1903 so
very similar attitude coming here from
Yan and it's not to say hey don't be
pessimistic or things always work those
aren't my claims my claim is just you
don't know you're saying something that
you don't actually know that you don't
have a reliable epistemological process
you don't have a reliable methodology to
be outputting statements like it's going
to take more than 2 years that's what
Alazar is also getting at the
methodology that you're using which is
tell me subjectively how hard it feels
like you have to work today to get AIS
to do this stuff that particular
methodology we know is not relied so
instead of just confidently declaring I
know we're not going to do this in 2
years the correct thing to do is to say
it seems really hard I personally don't
feel like it'll happen in 2 years but I
totally acknowledge that this is an area
where my confidence is pretty low and
somebody might surprise me it's
incredibly arrogant for him to be saying
stuff like that and again for him
personally we have him on record saying
that llms are going to be able to talk
about the physics of a book on a table
right so you'd think that would give him
some humility but clearly no clearly it
didn't it's just crazy to me that given
the stakes given that he is one of the
people trusted to tell us whether our
whole species is imminently doomed he's
so confident or dare I say cocky when we
really actually need him to show some
humility the next Twitter beef is with
run run is a member of the technical
staff at openai and a well-known Twitter
personality so he tweeted recently we
will never be quote unquote ready for
AGI in the same way nobody is ready to
have their first born or how Europe
wasn't ready for the French Revolution
but it happens
anyways and then alaz owski
replied nobody is ready for a giant
asteroid crashing into Earth but it
happens anyways says man working to
steer giant asteroid toward
Earth I thought that was a good reply I
think it's pretty messed up when people
who work at these AI companies act
fatalistic like what can we do AI is
coming for us nobody can ever be ready
it's like okay great have fun dying then
that's your attitude and you call
yourselves The Optimist you know John
Sherman from the for Humanity podcast
has this great observation where how is
it optimistic to just be walking like a
lemming into the future when you have
plenty of signs that it's not a good
future and you're just like that's not
in my control I have to run toward it
you can call it optimism to be like hey
I bet we have the power to not run
toward our Doom I bet we can continue
avoiding Doom continue Humanity's time
honored tradition of somehow not going
extinct that's my optimism right that's
the optimism of the empowered Doomer I
consider these kind of Tweets in very
poor taste they'd be totally uncalled
for if we were talking about nuclear
weapons pandemics this fatalism like
look the nuke's going to explode it's
not in our hands Game Theory says the
nuke is going to explode you're not
going to have the perfect bomb shelter
for it just be ready like what are you
you talking about what are you talking
about your company is supposed to be the
steward it's your job to have a noo
button it's not just to have a Go Button
you're supposed to only build it when
it's safe this is
insane I know I know everybody's trying
to build it if you don't build it
somebody else will okay but still you
better stop them you better do your best
right I'm not saying you're a guaranteed
to succeed you better try your hardest
to stop everybody right in the case of
nuclear proliferation what did we do we
stopped everybody right so it's it's not
impossible right we stopped terrorists
from building nukes we somehow did it
for now maybe not next year but so far
we did it okay so resigning yourself to
just say it's inevitable when many of
the experts are telling you this will
kill us we can't survive this it's such
an insane attitude to me I don't know
why people can't see that at the very
least this is just immature this is not
how an adult addresses the situation so
even if he's right even if he's right
okay sadly we think that the odds are
worth taking we think that there's only
a 10% chance that we're all going to die
and that's low enough that we really
should just plow through because it's so
hard to stop okay if you're making a
sober assessment fine but when you're
talking about it like that like hey you
know having your first kid you can't be
ready for it AGI you can't be ready for
it to me it's just so inappropriate it's
just so Unbecoming of somebody who's
working at an AI company and actually
tuned into the situation and somebody
who should be aware of his own
responsibility to be a voice of reason
or a voice of morality or a voice of
guidance
like compare him to Jeffrey Hinton at
least Jeffrey Hinton you can tell he
feels the weight of responsibility yosua
Benjo he feels the weight of
responsibility even somebody like Sam
Hammond who I disagreed with who I think
is not enough of a Doomer when he tweets
it at least sounds like he's acting like
a responsible adult right he understands
the gravity of the situation and he's
trying to balance different factors
whereas the attitude that run is showing
that Sam alond is showing it's just
unbelievable right it's very childish
it's like
wow like we we as Society have
implicitly just given this Authority
when they clearly don't have the
temperament to handle it given them this
authority to decide that they're going
to go ahead and build AGI it's
incredibly
scary like it's great when we Empower
Tech innovators and we let them run and
we're L Fair that's what makes
capitalism great I get it but when we're
talking about human extinction right the
next nuclear weapon grade technology and
these are the kind of people that we're
trusting to run it it's just like we
have to be able to stop them we have to
be able to say this isn't okay but the
time is running out to ever say that it
may already be run out it's just like I
said really poor taste and it's sad to
see the discourse happen like
this my next beef is with the famous
George Mason University economics
professor Tyler Cowan overall I tend to
be a big fan of Tyler he's written a lot
of interesting books essays he's one of
the most prolific readers bloggers and
Broad domain polymath thinkers in the
world and he's also known for assembling
amazing talent I mean the George bason
University economics department is
insanely overpowered three people that I
personally follow Robin Hansen Brian
Kaplan Alex tabaro they all work for
Tyler Cowen in this one little
Department what are the chances so Tyler
has created this amazing talent
attractor and anyway he he's great but
just when it comes to AI I've never been
able to make any sense out of his AI
takes so from my perspective he's just
got this major blind spot
or perhaps it is I who has the blind
spot what of us has a blind spot okay
and this particular Twitter beef stems
out of a recent post that he wrote in
his blog title of the post is effective
alterists and finance Theory I won't
read the whole thing but the the key
part is when he says look all you
doomers why aren't you making some kind
of move in the stock market that
corresponds to your belief in Doom it
doesn't make sense guys I'll quote this
part of Tyler's post he's talking about
doomers like me he's saying I have never
ever heard not once I'm going to sit
down and study finance and see if I can
find a feasible way to short the market
if I can't I will feel sad but I might
get back to you for further guidance and
he's also never heard quote soon enough
AI will be good enough to tell me how to
short the market intelligently then I am
going to do this thanks for the tip
Tyler's post continues nope never the
absence of the last one from the
discourse I find especially odd AGI will
be powerful enough to destroy us but not
good enough to help me do an effective
short okay dot do dot dot the sociology
here is more indicative of what's going
on than the arguments themselves because
the EA rationality types and doomsters
here generally are very good at learning
new things for me it's frustrating
anytime I see a non- Doomer who seems to
not really get pretty obvious truths
about what it's like being a Doomer and
then go on to be like oh doomers are a
cult or they're religious or they refuse
to see this so in this particular case a
lot of us Doomer is on Twitter kind of
are scratching our heads saying like um
Tyler what do you want us to do exactly
like this doesn't make sense there was a
good response from a guy named Jonathan
palis on Twitter he wrote Tyler Cowen
wrote another post about his if you
really believe there is an existential
AI risk you should short the market
hypothesis unfortunately even though he
writes about a lot of details the
hypothesis fundamentally does not make
sense to me let's say I think there is
80% chance that AI brings increased
wealth and 20% chance that the AI kills
us all
why then would I expect gains from
shorting the market it seems better to
be long on the market and earn good
returns in 80% of cases I.E the opposite
of his suggestion then Tyler Cowen
himself responds to Jonathan he says
covered in my list he's talking about
his list of falsehoods that Doomer say
Tyler continues you can go long
volatility or long something and short
others but you absolutely should have
some kind of
hedge John mat responds could you try
and explain why spelling it out more
it's a topic a lot of us take very
seriously and there is seemingly
Universal trouble understanding you for
example take the two worlds let's say
we're in the 80% non- Doom world then
it's mostly better to go long since
there will be more growth let's say
we're in the 20% Doom world then I'm not
sure what's best but either way I die so
I can't use the money he's saying like
why would I short the market when if and
when I do win okay great I win and then
I instantly die so why would I make that
kind of financial plan that's basically
what he's asking Tyler and Tyler
actually replies I have bought fire
insurance on my home right even though I
don't think a fire is even 20% likely
and Jonathan replies a vital difference
fire Insurance helps dramatically in the
small proportion of Worlds where there
is a fire I can rebuild my life which is
in ruins it helps so much that it is
overall worth it even though I lose
money in the majority of Worlds but the
Doom scenario short investment is not
like that instead instead of the payout
being extra useful in the Doom scenario
it is less useful because I and everyone
I care about
die I really wanted to see Tyler respond
to that because that is the Crux of the
issue for me like I don't know why Tyler
saying this stuff but unfortunately
Tyler did not respond to that I got into
the frame myself I did a quote tweet and
I said killing me at Tyler Callen want
to Hash this out on my podcast so you
know I'm always trying to work in Dune
debates when I can because I think it
would be awesome for Tyler to come and
explain this here on my pod podcast
right I mean I would just seek to
understand what he's off about I mean
certainly there there's no question that
me versus Tyler Tyler is the one who's
way more insightful about the finances
of all this right so I'm totally open to
the idea that he has This brilliant
Point how I should be able to make money
off of my Doom claims if I'm right and
I'm just not seeing it and Tyler is
going to explain it to me and set me
straight I'm open to that possibility
Tyler so you're invited to the podcast I
was happy to see 60 people liked my
tweet I'm glad people are starting to
see Doom debates as a productive forum
for this stuff because that is a major
goal of Doom debates is to be that Forum
remember in addition to raising public
awareness of the imminent risk of human
extinction from AGI the other part of
Doom debates mission is to raise the
level of discourse about these kinds of
urgent topics now I did get precisely
one type of reply SL engagement from
Tyler in the aftermath of posting my
tweet he retweeted somebody else's tweet
a guy named Alexander Campbell who
replied to my tweet so the only
engagement I got from Tyler is that he
retweeted Alexander Campbell and
Alexander wrote until doomers understand
and acknowledge Mark to Market this
conversation will get nowhere so Tyler
retweeted that I just wrote what I did
not understand what Alexander Campell is
saying doomers have to understand and
acknowledge Mark to Market so doing my
best to unpack what he's trying to say
because I never got Clarity maybe he's
saying like all the different stocks are
going to have lower marks as we get
closer to Doom AO doomers should short
them or buy puts on them right which is
just kind of repeating Tyler's original
point so I'm just not understanding the
additional level of insight that
Alexander Campbell is bringing by
posting that or that Tyler Cowen is
bringing by retweeting that again maybe
the problem is me right I'm happy to
come P this out or if you're watching
this and you're on Tyler's side here and
you totally think doomers ought to be
buying puts even though the puts pay out
right before it's obvious the world is
going to end and we can't really profit
from that scenario but if you think we
somehow can profit I'm all ears let me
know if we take a look at my Twitter
thread nobody else besides Alexander
Campbell stepped up to explain what I
was
missing Greg curn commented seems like
the issue is that Tyler doesn't actually
think of AI Doom as Doom I.E the actual
end of humanity he thinks of it more as
a survival catastrophe where he will
still be alive and there will be money
in houses and stock markets afterward a
guy named Alexi wrote Jesus that has to
be trolling at this point so yeah it's
pretty crazy that the whole effective
altruist or AI Doomer Community can't
seem to reach agreement on Tyler on what
should be a pretty simple issue right
like I don't mind making my beliefs
consistent enough that if there's some
high expected value opportunity that I'm
missing then to either take the
opportunity or admit that I don't really
believe what I say I believe like I'm
happy to do that kind of reconciliation
I just don't get it right I don't get
the logic I don't get why I have to
short the market if I only think a
scenario where I collect is one where
Doom is extremely imminent but okay
steel Manning I guess there is kind of a
dream scenario where people realize uhoh
we're so doomed in like 30 years AI is
going to come kill us and at that point
a lot of the stocks tank but they still
have a little bit of value left because
hey there's still going to be some cash
flows in the last 30 years of the earth
you know like the businesses are still
going to be going so they're worth
something the Market's still open but
the values have crashed so much because
there's no longterm Horizon for these
companies at that point maybe I'll have
a decade or two to enjoy my shorts right
so I think that's kind of where Tyler
Cowen is coming from is like look don't
you want to have aaska decade or two so
I guess in that case the Crux of
disagreement would just be that I don't
think we'll have two decades I think
that the moment it becomes consensus
that we're doomed if we haven't actually
stop the Doom with good government
policy then I just think the crash will
be quick right I think we're going to
hell pretty quick or you know I think
tomorrow all of our computers might turn
off because China was working on a
larger scale AI than any of us knew
about and it became super intelligent
and it's now a virus that makes the
internet not work right I expect to wake
up one day like that and be like oh okay
it's game over the world is now on a
permanent slide into hell nice knowing
everybody right so I I just don't see
myself being rich in some kind of Bunker
in that scenario I expect to just go
down with everybody
else so call me Tyler by the way if you
listen to my recent episode where I did
the epistemology debate with v and Ben
you know baz versus popper ven asked me
a similar question of hey if you think
the world's going to end why don't you
just go make money on a prediction
Market saying that the world's going to
end or why don't I see a prediction
Market that says there's a 50% P Doom
how come it's just you telling me that
don't you trust prediction markets and I
pointed out look prediction markets are
a very powerful tool for aggregating
people's beliefs but the aggregation
doesn't work when the people who think
that there's a hyy doom don't expect to
make money when they're right and he
said well don't you think that the price
will increase as pdom increases toward
100 and I was like well no because the
price only increases when somebody
expects that somebody else will come in
with an even higher price and then pay
them but the whole point of a higher P
Doom is that you think it's less likely
that you're going to have to pay right
so there's a major biasing Force so that
anytime you have a prediction Market
about what's the pdom it's going to be
heavily biased if it exists at all it's
going to be heavily biased toward low P
Doom because only the people who want to
come in and bet on low P Doom are
rationally expecting to collect like
anybody who comes in with a high P Doom
maybe they're just trying to be honest
but they shouldn't expect to collect
right it's like a structural problem
with trying to bet on Doom it's the same
thing as betting on like Hey will this
platform be legally allowed to operate
in two years or like will this platform
run away with everybody's money if you
bet yes that's like the dumbest bet ever
right because you only win when the
platform runs away with your money right
so it's a similar kind of structural
problem you're assuming that the
platform will stay in existence to pay
you and similarly you're assuming that
the world will stay in existence to pay
you when you bet on these kinds of
markets so that's a similar kind of
impass right what Vaden was asking me I
feel like that's kind of similar to what
Tyler is asking me where these people
are just aren't appreciating the whole
end of the world thing when they talk
about making money right or for whatever
reason there's some kind of Disconnect
here so Tyler or anybody who feels like
they're aligned with Tyler on this issue
balls in your court hit me up on Twitter
DM or email me wisy gmail.com come on
the show let's hash this out
the next Twitter beef is with the one
and only Professor David
Deutch I actually instigated this I just
randomly did a top level tweet where I
said hey David doy instead of hand
waving about creativity being the
essence of what humans do that AI can't
do why not just compare human versus AI
abilities to plan toward goals in the
physical Universe you'd make sense and
you'd still be pointing out a major
separation you're probably thinking whoa
Lon coming out of nowhere but you know
I've been been watching his podcasts and
this is what made me want to tweet that
at him it's a podcast where he talked
with Naval rant who I covered on Doom
Tiff's number one check out that episode
so David was talking with nval about
creativity and this is basically how the
conversation went David deuts says gp4
is not creative and Nal says how would
you define creativity and David deut
says creativity is fundamentally
impossible to Define so let me play you
that clip right
now you've conceded that g pt4 has made
progress and it's improving but you're
not willing to say that it's improving
in the direction of being a person why
so I see no creativity now people say oh
look it did something I didn't predict
so it's it's creative perhaps one of the
problems here is that we just Define
creativity so poorly so how would you
define creativity in this context
creativity and knowledge and explanation
are all fundamentally impossible to
Define because once you have defined
them then you can set up a formal system
in which they are then confined and if
you had the system that met that
definition then it would be confined to
that and it could never produce anything
outside the system so for example if he
knew about arithmetic to the level of
the postulates of p and and so on he
could never and when I say never I mean
never produce girdle serum because
girdle serum involves going outside that
system and explaining it you can say
that it's not defining something and
then executing the algorithm basically
because it would always be an algorithm
then once it was in a framework so you
say well it's its ability to go outside
the framework well I I tried by the way
ordering chat GPT to disobey me and it
didn't refuse but it absolutely didn't
understand what I was going on
about okay as I watch this clip now one
thought that comes to mind is he's
making creativity be about like jumping
out of a formal system like imagine an
AI that could see Good's theorem if it
just NE arithmetic but it's like okay
has he noticed that today you can pull
up GPT and talk to it about Good's
theorem it's like I know that's not
exactly your point your point was about
an AI That's just trained on piano
arithmetic but like doesn't it give you
some kind of clue about the nature of
intelligence that we're just discoursing
with AI we have today and they don't
have any trouble answering your
questions about goodles theorem and like
engaging in experiments with you about
goodles theum like it's not like goodles
theum is breaking their brains here
right it's probably easier for AIS to
talk about goodles theum than it is for
humans to talk about goal theem all
things being equal so when I was
listening to the clip I thought it would
be natural for David deuts to be like
okay I'm talking about about good theum
let's ask llm about good theum but no
that's not what he said he said let's
ask llms to disobey me like in his mind
this question of please disobey me is
more analogous to a formal jumping out
of the system than just talking about
systems right and just directly asking
about go theorem so I guess he and I are
just both making two different crude
analogies but anyway at the end of the
day my beef with David is just like is
creativity really that hard to Define do
you really have to lean back and be like
I can't Define it just it's inevitable
like really you can't even take a stab
at it and remember I asked him can't you
just compare human versus AI abilities
to plan toward goals in the physical
Universe like isn't that a nice
separation that you could say say
couldn't that support your point of why
AI aren't human level yet so David
actually came over and replied which is
rare right I don't get that many David
Deutch replies when I'm on Twitter so he
replied to my tweet and he said the
answer is in my book the beginning of
infinity and then I wrote I read it big
fan of that and fabric of reality his
other book I'm still confused about the
meaning of your claims about AI lacking
creativity or knowledge creation
compared to humans EG how to
operationalize testing the difference
all right and then David replied again
and what he wrote
was right so he just gave me this
one-word reply like apparently just like
a snarky reply like I'm just explaining
what I'm confused about and he's
basically right you are confused Lon go
figure it out right that's like my
interpretation of his one-word reply so
then I replied again I wrote according
to your interview the one I clipped it's
impossible to Define SL
operationalize why focus on using an
undefined term to talk about the
distinction between human and AI
abilities when there's also an
interesting distinction that can be
defined namely planning
prowess now David didn't reply directly
but another user named Sarah Fitz
clarage replied and she wrote that's
like suggesting looking under the lamp
post because the light is brighter there
and David deuts retweeted her so
presumably that's like his reply too
okay fair enough I I get it that makes
sense and then I replied in the general
case sure you know the move that I did
is kind of like suggesting looking under
the lamp post because the light is
brighter there right that's kind of the
move I made when I said here's another
separation that you could be pointing
out between humans and AIS but my tweet
continues it feels like in this
particular case he wants to say
something about what current AIS can't
do and this is the actual thing they
can't do and then David dor replies and
by actual you mean defined and
empirically testable and then I said yep
I get what you're saying about
undefinability in formal systems but do
we have to go there to EG talk about why
gb4 can't write a best-selling book and
that is where I lost David deuts he
didn't reply anymore but like I remain
confused right it's like why is he going
on podcasts and writing in his books
these handwavy claims about creativity
why is a genius like him opting out of
the problem of defining what creativity
means like just Define it it's not that
hard right I mean at owski defines it
great he basically says finding
solutions that rank high in your
preference ordering and low and naive
search ordering so visually imagine
there's just like this giant space that
you have to search like you have to
search the Sahara Desert and somehow you
find the one bar of gold that's in the
Sahara Desert the bar of gold ranks high
in your preference ordering because you
think a bar of gold is valuable but it
would rank very very low in a naive
search wording imagine asking a random
person to try to search the Sahara
Desert to find you the gold they
wouldn't know where to begin right maybe
they invent some kind of really powerful
sonar or scanning technology that
nobody's ever had before and they find
the gold that way well if they do that I
would say that they're being creative
because they search for something that
the naive search would have sucked that
and even a pretty intelligent human
searching would still fail to find right
so that would fall under my definition
of creativity it's very closely
connected to my definition of search and
my definition of intelligence it's
really hard for me to think about
somebody being extremely intelligent but
not creative or vice versa extremely
creative but not intelligent the way I
would make that distinction It's
actually an interesting puzzle if I
think about somebody who's very creative
and not intelligent I basically just
think of like kind of like a babbling
baby right just like a lot of Randomness
involved a lot of free association where
they're being very generative right
they're combining a bunch of building
blocks together and a bunch of Novel
ways but they're just not doing it in a
way that searches for a complex
Criterion right so they're not narrowing
down a search space they're just kind of
generating a bunch of interesting
elements in a search space and to the
degree that the elements they generate
are consistently good then they're not
merely creative they're also intelligent
in order to even generate good enough
candidates so already it wasn't a hard
distinction the way I was making it out
to be let's do it the other way let's
talk about somebody who's very high
intelligence but low creativity maybe
somebody like that always wants their
destination to be very predictable so
it's like oh you gave me clear
instructions I'm just going to follow
the instructions exactly or you gave me
a handbook I'm just going to follow the
handbook closely basically somebody
who's reducing unpredictability might be
high intelligence low creativity but of
course reducing
unpredictability if you tell Elon Musk
get us to Mars he still has to be really
creative even though the outcome getting
to Mars is predictable because he's so
intelligent and so effective so reducing
unpredictability actually isn't
sufficient to say somebody is highly
intelligent but not creative I think the
concept again the distinction starts to
get very blurred like I'm not seeing
much of a distinction between
intelligence and
creativity also if you imagine somebody
who's highly intelligent but very low
creativity all you have to do is inject
Randomness into that person's thought
process and suddenly he becomes both
intelligent and creative so if this
person is struggling to just be
unpredictable enough or be random enough
no problem just give him a coin just
tell him to flip the coin a bunch of
times and just Generate random data and
work with the random data just tell him
that he has to do that right it's a very
simple instruction it's a very simple
modification to the system and the
process that he's going to have to
follow to take the random data and
follow your instructions to incorporate
the random data the end result of that
process is creativity right like if
you're playing a song improvisation is
basically taking random mistakes and
then making them work anyway right at
least that's one key that people use to
have good improvisation it's not the
only way you can improvise but it's
certainly a good way right so suddenly
you turn somebody who's ridiculously
good at music you turn him into somebody
who's good at creative music because you
just order him to fix all of these
mistakes that you're injecting right so
there's always a trivial Gap in my mind
between high intelligence and high
creativity but people like David deuts
seem to insist on treating creativity as
this mysterious thing that they won't
even Define or they'll defend their lack
of a definition because they'll connect
it to Good's theorem and they'll say how
could you possibly Define Good's
proposition when working in a formal
system you know the proposition of I
have no proof right that kind of trick
is if you studied Go's theorem it's it's
like a statement that's saying I am not
provable so it's kind of a real logic
twister kind of a paradox that's like
the flavor of Good's theorem and so
David de is getting a lot of mileage out
of Good's theorem being like look this
is the brain twister that makes
creativity in current AI impossible and
I just don't see the connection right it
really seems like he's reaching for a
connection I don't get it luckily I've
got some deutans on speed dial remember
my old Pal's uh Ben and Vaden from last
week's episode where we debated
epistemology I know they're big fans of
deuts so I will be sure to ask them in a
few days when we record our part two I
will be sure to ask them what deuts is
thinking when he talks about creativity
and this ineffable thing that AI can do
that humans can't maybe they will have a
good answer and maybe they'll Enlighten
us but as for this Twitter beef we're
going to have to leave it here because
there's no more David deuts
replies okay those are all the Twitter
beefs I have to share with you today if
you missed the previous Twitter beefs
episode it was previously known as Doom
tiffs number one if you missed that
episode go check it out I think you'll
enjoy it as well it's the same kind of
content I'll probably be doing these
kind of Twitter beef episodes once every
two months because you know it's an
outlet for me it's a release right so
when I'm on Twitter getting pissed off
and frustrated at least I can add it to
my collection of these beefs and I can
release it out upon you and you can also
vicariously live the frustration that's
why you come here to Doom debates
because you want to live the frustration
all right this is going to be a big week
for Doom debates we've got an interview
coming up with Andrew crit who was
formerly a researcher at the machine
intelligence Research Institute and also
one of the co-founders of the Center for
Applied rationality super smart guy so
this is not going to be a very heated
debate because we both have high P Dooms
but it'll be a very interesting exchange
of ideas between two people who have
different conceptions of Doom I'm still
planning to do a longer episode where I
analyze all of the different David
Deutsch media that can get my hands on
try to do an authoritative David Deutsch
episode and lots of other good stuff so
stay tuned now if you're watching the
show hopefully I've been giv you a lot
of value a lot of entertainment a lot of
information I don't ask for much in
return all I want you to do is
synthesize the neurotransmitter dopamine
uh maybe put it in a vial package it up
and just mail it to my address just mail
me the
dopamine okay that sounds hard to do
okay fair enough if it's hard to do what
you can give me instead is just a unit
of social engagement so like if you're
watching this on YouTube just smack that
subscribe button on YouTube my eyes will
see the number go up and my brain will
actually give me the same hit of
dopamine so that's probably a more
convenient process for you to do it that
way or just smack the like button smack
the comment button even write a custom
comment that's always much appreciated
or go to Doom debates.com type your
email address smack the enter button
subscribe to my substack that'll give me
the hit dopamine write a comment on my
substack that'll give me the hit of
dopamine okay one way or another you got
to get me the dopamine that's all I ask
all right thanks very much and I look
forward to seeing you again on the next
episode of Doom debates

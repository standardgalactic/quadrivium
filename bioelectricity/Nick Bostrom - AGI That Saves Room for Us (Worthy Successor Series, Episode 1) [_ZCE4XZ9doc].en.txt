this is Daniel Fel and you're tuned in
to the trajectory today marks our
transition to our second series on the
trajectory referred to as worthy
successor where we're going to be
exploring permutations of posthuman
intelligence who we might be happy could
populate the Galaxy and do things beyond
what humans can do either living with us
or taking over from us and our for our
first episode I thought it would make
sense to bring on none other than at
least in my opinion the premier living
posthuman thinker Nick Bostrom uh I had
a conversation with Nick Bostrom 10 long
years ago on one of my other podcasts
when super intelligence came out
famously a bit of a skeptical book about
the survival of humanity after posthuman
intelligences are birthed his latest
book is called Deep Utopia where he's a
little bit more optimistic about AGI
maybe going right and what that could
look like in this episode our first
episode in the worthy successor series I
unpack with Nick some of his ideas
around what going right might look like
and what the traits and qualities of a
worthy successor intelligence would be
he goes into what he hopes for the
future trajectory of intelligence in
general how he thinks about moral value
and a little bit on how we might nudge
the future more in that direction I'll
give a little bit more analysis in the
outro but without further Ado I could
not be happier to kick off the worthy
successor series uh with the man himself
here's Nick Bostrom on the trajectory so
Nick welcome to the trajectory
hey Danielle yes good to good to have
you here it's been hard to believe it's
been a decade since we chatted last when
your last book uh came out and got my
hands on a copy of your latest one and
wanted to be able to dive in on this
very important topic for this new series
that we have around the worthy successor
you've written a lot about everything
from Singletons to utility monsters and
everything in between in terms of the
postum state space of Minds I would like
to ask how would you define a worthy
successor intelligence something that
you would feel okay about handing the
Baton of this project of life um where
would you begin
there I think the best case scenario
would be
where it's not a replacing of what
currently exists but a continuation of
it and then maybe an adding to it so I
think it's already a little bit sad if
all people who currently live die and
then yeah maybe there is another
generation and another generation
another generation after that uh it
still would seem nice if people's lives
could be saved now of course the
background condition here is that we are
all dying all the
time um so from a person affecting point
of view um the Baseline is that we are
basically all dead in 100 years or so I
mean a lot of us much sooner than that
um but if wishes were horses it seems
like we would hop on to this future uh
every
person currently uh alive and and maybe
other creatures as well that that are
morally considerable like certainly some
animals and then uh have trajectories in
front of us where we could continue to
thrive and develop and grow perhaps over
long time spans into some form of postum
beings for people who are interested in
that um and then maybe adding additional
beings to the world as well like that
could be completely digital minds or AIS
and and other other types of entities
that we can create but it would be more
like an uh
and plus type of vision rather than just
a replacement yeah sort of supplanting
all plants and animals with one computer
mind that would then move forward it
sounds like to you would that replace
richness in like an abstract
philosophical sense or would it simply
harm humans and of course you being a
human you have a vested interest in in
uh what you know uh mammals are sort of
up to what's what's your take on that
and preference is it is it a more
complete exploration of the state space
of value if it is an and as opposed to a
replacement um why do you stand
there um yeah probably a more complete
exploration uh of the space of values in
as much as some values might not
supervene on time slices of the world
but on entire trajectories like there
are values that care not just about what
happens at a particular time but sort of
how it came to be um so those kinds of
values
obviously uh could be more fully
realized if you're also having the right
kinds of trajectories going into this
future now I should hasten to say though
that um uh when saying and here it
doesn't mean that all aspects of the
current condition should be preserved in
their present State I think in fact
maybe the most important moral
imperative is to get rid of a whole host
of
horribles um but even normal uh
relatively happy human lives I think
there's a great potential for them um
over some time frame to develop further
and maybe the end result will be
something quite different from our
familiar form of homo
sapiens um so it's not like change is
not good it's more like there are
certain kinds of changes and it could be
nice if U those who are already here
have a chance to participate in that I
think I mean in fact yeah sorry I was
going to say I I would think that would
be nice too it's I think the exactly
what the odds are of that are sort of up
in the air and you've certainly had
writings a little bit on both sides of
that coin um when we think about it
about okay humans can maybe ride upwards
on this trajectory you have this state
space of Minds from a very old Ted Talk
of sort of where humans occupy and maybe
the animal world occupies and what the
the grand postum trajectory of of
possible states of mind could be if you
zoom forward it doesn't really matter
the time frame pick yours 100 years a
thousand years whatever um it does seem
at least from my vantage point and
certainly from reading some of your
writings that it would maybe be less
likely that humans would be running the
show maybe maybe we're in the mix to
your point in a different form than we
are now but probably not running the
show you've had the idea of the
Singleton that would sort of manage
complexity the idea of sort of uh better
versions of a utility monster that could
sort of Bloom vastly post-human uh value
into the world and maybe also preserve
humans if there's a right way to kind of
govern and manage those transitions who
who who do you expect would be running
the show in a best case scenario and
what sort of values would that entity
have it sounds like one of them is it
would permit you know hins to stick
around in some form but but how else
would you describe what this thing is
who is our
orchestrator yeah so we we've kind of
been running the show for you know the
last 10,000 years or so I I don't know
how you would score our performance I
think uh I mean it's it's possible that
we've had our our chance now and that
it's time for a change of guard like
maybe uh I I like obviously a lot
depends on what precisely we would be
replaced by in terms of the having their
hand on the tiller here
um
um I think yeah it's not so much that
hominids uh need to be around forever I
I mean I I think like from from a
respecting different people's choices
and and having a future that
accommodates values I think it's nice if
people who do for whatever reason want
to remain exactly like they are now with
two legs and two arms and and just run
around and doing their biped things for
10,000 years probably it would be nice
if they can keep doing that um but I
think there are a lot of other
possibilities as well whereby we would
eventually be transformed into something
quite non- homin like I mean if you
think about a child today um fiveyear
old let's say um now eventually maybe
they become like a 30-year-old and and
they're quite different
a 30-year-old has a very different mind
a very different body different
interests different relationships with
other humans than the 5-year-old so it's
fairly profound transformation yet we
don't think uh normally that it's bad
for a 5-year-old to eventually grow up
uh in in fact many of us would think it
would be so muchat tragic if the
5-year-old stayed 5-year-old forever and
never really had a chance to develop I
think
similarly um we all are infants
currently compared to what we ultimately
could
become and that there is a you might
view it as a huge tragedy that is
unfolding that that we have a kind of
100% infant mortality like we all just
develop for maybe 20 years and then sort
of stagnate for a few more decades and
then rot but uh maybe for for a human
being really to reach its full potential
in terms of maturity of Personality Etc
uh maybe that takes like 3,000 years and
we've just just never had a chance to to
observe that uh completely on the same
page with you here Nick and again I
think probably in many context you
you've got to sort of ladle it out of of
making these analogies I'm I'm totally
with you about the grand posthuman
blasting probably most of the audience
is as well uh not completely nent in
that domain when that expansion occurs
you know and and some of it will be
humans too I think there will be very
few humans 10,000 years from now who
were doing normal hominid things as as
we do now I I suspect that the adoption
of Technology will necessitate wild
change but let's just say that such a
future exists The Entity that is running
the show do you expect it to be a
conglomeration of sort of melded human
Minds from some uh sort of BMI project
do you expect it to be a grand sort of
babysitting AGI that you know is is
essentially making sure that the planet
Earth is very happy and friendly for
digital and and physical Minds together
do you expect it to be something vastly
beyond all of that doing things we can't
possibly comprehend throughout the
Universe blooming value in ways that we
don't understand but allows us to stay
alive on our little rock here who would
you hope 10,000 years from now Nick
would be running the show would it be an
evolved version of you and sort of
fellow hominids or would it be something
uh very different than
that I think what might be most
important is that uh uh what values are
ultimately shaping the future and and it
could be human values even though the
actual
Administration sort of the day-to-day
governance the decision making might be
uh more efficiently done by super
intelligent AIS if they are kind of
acting on our behalf then uh we might
get better results by having that acting
on our behalf be done by super competent
uh super aligned AIS than you know some
some random human politician who who's
like kind of persuaded us to vote for
them and then bumbling around and trying
to represent us um so I I don't I I
wouldn't fix it too much on on the the
precise mechanism that does the
governing and more on the kind of
interest that it represents let's dive
in on that so um I could I could see
very much an argument against so I'm
very lucky uh Nick that the first early
rodentia with a postage stamp of Cortex
whatever it values were were not carried
up into humed I mean some of them were
I'm sure maybe caretaking for young or
some some weird proxies maybe the
enjoying cheese Nick I don't know some
of them maybe I like okay but but I'm
actually very glad that I have blossomed
Beyond a great many of the values of
those little rodents and and and and
that state space of potentia and values
has opened up I think there is
tremendous value in the continual
opening of those values would you you
you've certainly articulated that there
is a moral case for that but would you
push against against it when you look
forward to a future that you hope for is
it somewhat of a solidified sort of
human values uh with a preference for
human intelligences and then some kind
of intelligence whether it's brain
computer interface or AI or whatever
that's managing and kind of housing that
or or would you hope for something
vastly beyond
that yes I
was sketching earlier what I my guess
would be maybe the first best option
would be the future is enormously big so
why not have a little uh slice of that
where where where us currently living
humans could also participate because it
would be such a trivial cost as it were
in the scheme of things um now as a
second best like it's not clear just how
much we should then insist on the like
have continuing 100% of all the
different peculiar things we happen to
care about as humans um and and how
cosmopolitan should be with respect to
what would count as some um impersonally
valuable future even rodents I think
have quite a lot in common with I mean
they they want pleasure for
example um and it it is an interesting
kind of philosophically difficult
question but if you imagine
extrapolating the volition of a rat so
it can't really articulate very well
it's uh desires and preferences maybe
can't even itself really properly
understand what they are but if you
imagine um cognitively enhancing
rats uh to the point where they could
start to reflect on their own values and
giving them a lot of time to do this and
information um it's not entirely obvious
that the end result would be very
different from if you extrapolated a
human I mean maybe we would all just
converge to
being uh hedonist or something um um
and AI I think um might realize a whole
bunch of different values even if hum
were not particularly shaping them and
um it then becomes really uh very much a
question of meta ethics I think the
degree to which it is plausible that the
um um I don't know the ultimate theory
of what has value is very closely shaped
like it has a human shape or whether it
it would be a huge coinci
uh to to think that precisely the things
we have happened to evolve to like also
match this kind of independently
existing moral reality um but to really
get grips with those questions I think
you have to kind of wrestle with meta
ethics like thinking about what is the
nature of moral uh reasons and values
where do they come from um and and then
maybe from there you can kind of find
some ground to stand on that is not
simply a regation of our own subjective
taste and preferences I I would
certainly say a regurgitation of our own
subjective tastes and preferences would
be a uh wildly fettered conception of
the grand states base of possible values
um it sounds as though in terms of what
a worthy successor would be for you
wouldn't matter really the substrate
what it's built off of biological
non-biological hoop lots more of its
values part of that is preserving a
slice for current instantiations of
sentients whether they want to maintain
their physical body or not sort of that
this sort of like babysitting corner is
is a is a part of the value structure of
what you would prefer for for a worthy
successor what would you hope such a
grand millionfold intelligence Beyond
Humanity would do outside of that little
orb maybe Nick and I hate to say it the
more important things it would pursue
outside of babysitting youur or I as we
want to maintain our human form and kick
a soccer ball and trim our fingernails
and you know do other things like that
um what would you hope that the
intelligence would aim to achieve
reaching out beyond that little marble
uh of
babysitting um yeah it might depend on
what it wants to achieve I mean I might
hope that it gets what it
wants uh that that that is one type of
conception of values that it's sort of
based on uh the preferences that
different agents might have maybe maybe
not raw preferences but idealized
preferences are way waited by some
attribute entity that has the
preference
um and so um there might be this kind of
conditional desire like if they want X I
hope they get X if they want y maybe I
hope they get y that might be
complications and qualifications to that
especially if there are many of them and
they have values that are intention then
I hope for like some Cooperative future
um
but if we kind of anchor on vaguely
humanlike values but that are not sort
of super tied to the particular current
uh
biological uh Incarnation like things
like pleasure maybe um
uh knowledge uh is various forms of
athetic beauty and complexity these um
uh learning um achieving things that
seem worthwhile to the person achieving
it
um it might be that there are like sort
of pockets of convergence in some of
these areas as well uh such that uh a
wide range of different uh intelligent
species many of them would kind of um
want some of these things um like if if
you like there might be other things
that are very idiosyncratic to to human
like for most obviously our sexual
preferences we tend to desire other
human beings like obviously a scaly
alien species would have like want scaly
aliens and think they were really hot
right and so so um I mean it it seems
like the more
independent uh value is of of the
particular random idiosyncrasies in our
evolutionary trajectory the the more
plausible it is that the same value
would be pursued by many other
intelligent
entities um I concur I guess just to
touch on the idea of that stay space of
possible Minds from your you now very
old Ted Talk um it strikes me that there
are things that you brought up aesthetic
Beauty um you know pleasure uh Etc that
probably maybe even a dozen times more
intelligence than we have would would
still be rich interesting and and you
know uh Curious and and worthwhile it
also strikes me that in that grand empty
canvas of of possible State space of
Minds there are values for which you and
I have no words and no ability to
imagine that are vastly grander and
richer than those human values not that
I'm devaluing the human values or that I
want them to go away I'm not saying that
I'm simply saying I don't necessarily
think that they are the greatest or the
grand attractors there are values we
know not of we cannot imagine we do not
have words for uh that that could be
astronomically and vastly more valuable
that that could possibly be explored
some of what you've touched on in the
past through all your VAR writings has
touched a bit on this how do you feel
about that clearly preserving the values
that are super worthwhile now absolutely
let's do it what is your take
nowadays on that other state space which
could be
grander I think there are a lot of ways
of being
and relating and experiences that if we
encounter them we would um we we we our
Jaws would just drop and we would think
like holy moly that's like I don't know
we were wasting our time with this old
human stuff and thought that was like we
had no idea like that it could actually
be like this this is like a whole
different thing and wow
um um so in that sense I certainly think
there are value or
valuable modes of being that uh we could
discover um if you're talking about
something outside that then I think it
becomes a kind of met ethical question
of on what grounds would we uh judge
that a particular
uh you know object was astronomically
valuable if if it had no relation to
what we or anybody else wanted
um and I'm kind of maybe more skeptical
that there would be a a kind of possible
meta ethics that
would make that a life possibility for
the way things could be um
I I I should also say I've been thinking
about some things recently I'm not
really it's not it's kind of still in
the work so I'm not really um ready to
roll it out but that that maybe connects
to some of these things we have been
discussing
about
um and yeah
so so we we are very focused on this um
alignment problem like the a lot of the
conversation now in AI is around that um
but I I think there might be more
Dimensions to that and and that maybe a
more I don't know you might say
Cosmopolitan outlook on on what would be
worthwhile to achieve with the AI rather
than this kind of narrow fixation on
aligning to current human values even if
not the best possible outcome might
still be
um um a lot better than just Extinction
followed by nothing and so how to
evaluate the the relative desirability
of these different things as things to
aim for is I think yeah I I have some
ideas there that I'm hoping to develop
more cool well I look forward to reading
them as they they bubble out I'm going
to try to Nutshell some of what you've
put on the table for us already around
this worthy successor idea make sure I'm
not on the wrong page and then we'll
clarify a little bit at the end about
maybe some of your hopes of how we could
measure and ensure we're going to arrive
at that sort of a future and not not a
grand paper clip producing uh entity
what I've picked up from what you've
articulated here is that you you'd hope
that such an entity which might sort of
run the show as we have done for the
last 10,000 years um would maintain this
little marble of Earth and the
individual instantiations of
Consciousness even if they take
different forms or maybe even if they
decide to maintain their existing form
it would possibly build upon some of the
values that maybe you you seem to find
valuable and uh maybe from your
metaethical analyses have sort of
discerned to be good pleasure uh
aesthetic preferences you know things
along those lines that it would it would
carry some of those that torch uh
outward and that also you sort of hope
that it gets what it wants um and the
way that I thought of that as you said
it is like if I was a a fish with legs
and I was walking out of the water and
then you were to come up to me and say
hey eventually you're going to Bubble
all the way up to hominids and you guys
are going to build spacecraft and it's
going to be a totally different world
what do you hope those humans do as the
fish if I were honest I would say well
gez I have no idea what its possible
State space of values or actions would
be I would hope it could do what it
would want when you said I hope it gets
what it wants it almost felt like it was
from that same standpoint of like if it
is that grand and vast and morally
valuable of a mind it will have better
ideas of what it wants and I I hope it's
able to pursue those that's sort of how
I interpreted that but I want to make
sure I'm not misinterpreting you Nick
let me know if we're on the same page
here um yeah no that's broadly it and I
think there are different basis for that
like one is um that I think a a broadly
Cooperative U and generous attitudes
towards the future is more likely to
make things go well from from a host of
different perspectives including our own
um and that might also have a more
specifically ethical underwriting and
one form that this kind of Cooperative
uh attitude could take I think is for
different entities out there that want
different things to try to uh hope or
help them get what they wanted like
obviously there are limitations to that
if if what they want conflicts with what
you yourself happen to want or with some
other entity then you might not be able
to give everybody 100% of what they want
in every situation but other things
equal I think it is nice um and should
be encouraged I have this paper called
uh base camp for M ethics which I wrote
a few years ago it's actually not a
proper paper but more like a series of
thinking notes but it's an attempt um to
to sort of outline a kind of
um meta ethical Theory like a
perspective what ethics is and how that
relates to some of these questions um I
should maybe hopefully one day be able
to like write it up in a in a way that's
actually like much more clear and
understandable so but
um but but it it kind of views like it
puts forward for consideration the idea
that uh morality is a kind of slightly
idealized system of norms that some
entities develop up and um and that
there could be uh levels of different
Norms that might be kind of nested so
you might have Norms within a family or
a community and then sort of higher
level Norms at you know at the national
or International level and uh and you
might also have Norms at an even higher
level that we haven't uh been thinking
much about yet a kind of um set of
cosmic
Norms um and that that we probably uh
should make sure
we we conform ourselves to as we develop
into a more advanced civilization
ourselves or build super intelligent AIS
so base camp for Mount ethics people can
check that out also uh sort of living
along with digital Minds I'll find the
exact title Nick but that paper is one
I'll also reference in the show notes as
as we wrap up we've got a little bit of
homework of some of your other thoughts
we've got a future of uh where humanity
and current life is in some way uh
protected some of those highest values
are continued forth into the universe
and otherwise these Grand posthuman
entities with ideas and values and
actions vastly beyond our own are
getting what they want hopefully in a
non-conflict way and blossoming that
outward into the Galaxy there are many
not so great scenarios nick uh of of the
paperclick scenario of of maybe Humanity
Going To War uh over AGI before we even
get there um what are your hopes in in
closing here around Innovation and
regulation that that you would hope
policy makers Business Leaders Etc would
bear in mind to inch closer to that
successor you've talked about instead of
Landing in a bad
place it's uh it's quite a complex issue
I I wish I had like a clear set of
policy prescriptions but um um I don't
at this time I think that's you can be
vague high level if you'd like yeah I
mean there's obviously been a big
increase in awareness of the potential
for transformative AI to really you know
create risks uh but that any rates
change to Human Condition and we now see
sort of top level policy maker like
statements coming out from the White
House the UK had this Global Summit on
AI Etc um and there is an increasingly
vocal uh set of people calling for AI
Paws or like like more kind of um biting
forms of regulation on AI development
and so I think it seems desirable if
there exist the possib ility at some
critical point of development like when
AI really starts to take off
um to to to to go a little bit slow
during those stages like if you're the
AI Lab First figuring out like the
missing ingredient or something but
there are like 15 other AI Labs uh post
on your heels and so if you slow down
even for four months it just means
somebody else zooms past you and the
race goes to h throws caution to the
wind the quickest that would be uh I
think risk increas inreasing and so so
having the ability at that critical
moment to have like a Time limited pause
of you know six months or a year or
something like that would seem good like
I would start to worry more if if the
way of achieving the PA was one that
possibly could result in a Perma ban on
Advanced AI it might start out the idea
being we just have this this brief
little thing right but then um that then
once you can prove that this AI is safe
then we're going to permit progress to
go forward but how do you ever approve
this unless actually by running it or
you set up a big regulatory agency and
then it kind of Aces more and more
powers and it becomes impossible to do
it or you create such a stigma around AI
that it just becomes impossible for
anybody to say anything positive about
it and then you might get a lock in like
so far in human history these kind of
faces have been temporary like some some
you might get some super ultra
conservative or um like view in a but
eventually something shakes loose or
some other country zooms ahead but um we
can't be confident that that will
continue to hold we might already have
levels of technology which if applied in
certain ways would permit the kind of
permanent locking in of current
orthodoxies and beliefs if if you
imagine even rolling out current AI
Technologies to their full extent with
automated uh censorship and and
propaganda and surveillance like that
might already make it possible to sort
of fix
a
temporary uh opinion to make it so so
yeah so I would become less excited in
proportion as I think these attempts to
regulate oruse AI had a risk of spilling
over into something very long term or
even just long term enough that the risk
would start to rise that we destroy
ourselves in some other way in the
meantime so even like 50 years might be
well long enough to create significant
existential risks coming from other
sources like six months doesn't seem we
probably won't go extinct in six months
anyway and
um so so that's that that's one thing
yeah and and then I think like things
that more broadly increases the chances
of a Cooperative outcome where everybody
has like a slice of The Upside and where
also the interest of digital Minds
themselves can become recognized and
given weight I think uh would be uh very
very
desirable you've articulated sort of I
think things are very rational and again
I'm just going to Nutshell and get your
take on it before we we wrap here but um
certainly Global authoritarian butlerian
Jihad locking in current states to base
of human values could be outlandishly
dangerous I think almost everyone agrees
with that uh yosua Benjo others we've
had on um violent arms race of whoever
can throw caution to the wind also seems
unlikely to get us to a worthy successor
uh in my personal opinion sounds like
yours as well um to your point though
some degree of coordination is probably
likely you're articulating this idea of
a pause of a refle C to use Toby or's
terminology from from precipice here um
H how would you hope currently uh that
we would permit such a pause that would
not turn into that kind of
authoritarianism would it involve the UN
blossoming into something with a little
bit more muscle would it involve
international organization and Alignment
in some way where each nation has to
reel in the labs within their own
jurisdiction I mean we all I think want
to avoid you know caribdis and whatever
that other Greek uh uh you know
mythological demon is here yes yes yes
uh they appreciate you you're at Oxford
you you got to have all this stuff
memorized um
so how are you hoping we'll thread that
needle because it's not super obvious
but maybe you don't have the whole plan
but maybe an intuition that we can wrap
up on well the original plan as it were
I mean not my specific but like amongst
people who were thinking about this is
that there would be some AI developer
who would just happen to be significant
ly ahead of the others like at one point
Deep Mind was ahead and like so maybe
like the lead developer would have like
two years lead time or something like
that um just just like not every
technology product is exactly equally
Advanced so you'll just and and you
could maybe help this along a little bit
if it looks like that was a clear front
runner people could sort of hop on and
try to help them along and and not help
the closest competitors then you would
have a situation where if if if the lead
developer let's supposed to have some
decent level of concern about safety and
pro sociality like there would be some
thresold that would have to cross but
then uh when they when they get to the
point where they could create super
intelligence they would have an an
opportunity to slow down for maybe a
year or two just kind of uh burning up
their
lead um that that would be desirable and
then that lead that that kind of Paul
would expire
automatically as other people started to
catch up technologically
and so that maybe after two years you
would have another AI lab you know maybe
in some different country kind of
catching up and then if if you wanted to
extend the the pause even further you
would have to sort of persuade one more
entity now that it was actually better
to pause so maybe if the risk was comp
was was obvious enough then maybe they
would also agree let's let's together
pause for another six months but the
kind of the the the difficulty of
continuing the pause would increase over
time as more and more uh lab
gain the ability so it would ultimately
expire uh spontaneously and it wouldn't
turn into a kind of Perpetual uh ban and
so so that would give you some of the
good features like it would give you
this the opportunity to slow down easily
and and it wouldn't have much risk of
spilling over now that might not be on
the table anymore in as much as
currently the AI race is kind of a
little bit more competitive yes so now
something analogous might require a
little bit of coordination amongst say
the top three um Frontier Labs or
something like that and and and it might
mean also there needs to be more
government coordination if one wanted to
make something like that happen but that
then brings in additional concerns of
course like I think some of the people
currently being very eager to get
governments more involved I mean it
remains an open question what they will
think afterwards because once you open
up the box I mean maybe it is the right
thing to do but like not everything that
gets tossed into the political realm is
then
by good meaning people who rally
deliberate the option you know with full
information and come to a sensible
conclusion like that that it's a bit of
a like anything can happen once
something becomes a political football
and once something becomes a
geopolitical football like the stakes
are potentially even higher so uh
clearly you know pros and cons to
governance it sounds like you're not on
either side of the fence if you were a
betting man Nick and again you're not
committing to this nor am I uh for some
eternal opinion but it feels like it's
up in the air for you if you were a
betting man and say right now we're in a
very clear race us and China and all the
major Labs um do do you suspect some
degree of government coordination to
sort of bring about a bit of reflection
is more or less likely to get us to uh
the worthy successor than the pure state
of nature circumstance your opinion may
change in two months or two days but as
of right now do you have any betting man
take on that Nick and reason why as we
close out yeah I mean right now it's not
so much a race between the US and China
it's more like us racing against itself
it's true it's true um yeah I think
probably the optimal would be to have
more government oversight and regulation
than there currently is but um it's one
of those things that you might not have
very fine grain control like you can
sort of maybe set off an avalanche but
once it's on the way you can't sort of
then stop it at the optimal point so I
would worry a little bit about
overshooting the Target here
um and yeah so I'm just I'm I'm
open-minded about this and I think like
it it's the kind of thing where it's
probably not like there's one position
that's correct that you can derive X but
you have to sort of see how things are
flowing and then what opportunities
there are to nudge things on the margin
and that that will likely change over
time um and so I would like look for
opportunities if there is like something
that seems cooperative and constructive
uh like not it in that direction but I
would be more um weary of things that
kind of strategies that take the form of
sort of I'm gonna stand on the tracks
and and say stop or I'm gonna try to
revolutionize the world to implement
this particular Vision like I think
those
have like greater opportunities for
backfiring and and that there's like a
kind of these these things are so big
like one feels so little as a human
being that it seems almost prep to come
up with a a grand plan that one is done
like pushing hard to implement I I would
completely Concur and I think this is a
good synopsis a little bit more
governance but let's be careful of these
wild ideological you know Steer the
entire I think what you're saying is we
can kind of Bend and direct this Grand
blooming here but to think you're going
to step in and sort of totally grab the
steering wheel feels very very
counterintuitive hopefully nick uh we'll
be able to sort of make our way between
those monsters as well as dsus did and
hopefully we'll be able to do so with
losing a couple less guys than he did
along the way uh I guess we'll have to
see how the governance ideas bubble
their way up but I appreciate being able
to have your take on on how we can get
to a more worthy successor here yeah no
good good to talk to you awesome glad to
catch up Nick so that's all for episode
one here of the worthy successor Series
this episode is a little bit shorter
than some of our follow along episodes
with some of our other guests and some
of the components of what we covered
surprised me a bit certainly Nick's tone
has shifted from more of existential
risk to potential moral upside of AI I
was really surprised to see some of his
emphasis on very uh anthropomorphic
things sort of he's famously a proponent
of sort of this vast state of posthuman
value and posthuman sentience that that
could possibly be explored we talked
about things like appreciating aesthetic
Beauty and kind of pleasure in human
terms um interesting to see Nick put a
lot more focus on kind of The Human
Experience as we start to to make this
transition uh in addition to the the
grand posthuman Realms of value that
might be unlocked which he did mention a
little bit he said something that I tend
to agree with and that maybe will come
up as as a theme in some of our future
episodes which is that he would hope
that whatever this intelligence uh would
do it would it would sort of get what it
wants um and we are we unpacked in the
episode a little bit of what he meant by
that I happen to agree I think that
something that can value and discern
goals Beyond us should explore that
state space even if we humans can't
imagine it but I'm maybe a little bit
less optimistic that that would imply uh
inherently sort of a good treatment for
humans I think there's many ways to
pursue value that don't necessarily
involve uh keeping the hominids sort of
happy and healthy but I appreciate
Nick's shift towards optimism I think
many of his ideas are brilliant and I
would encourage you to check out his
book I got to thumb through a good deal
of it before this interview here today
in addition down in the show notes of
every episode of this series and of our
previous series we have an article that
breaks down the key takeaways so in the
case of Nick we talk about his actual
worthy successor list and some of what
he thinks we should do to nudge and move
in that direction we'll be doing that
for all of our follow along guests so
you can look side by side at some of the
criteria of a worthy successor from many
different AGI thinkers uh and then folks
that we have on for this particular
Series so hopefully you'll enjoy that
again that's going to be linked down in
the show notes as for our next guest um
I won't say any names uh but this is a
fellow who has made some major
contributions to the modern era of
artificial intelligence and machine
learning has been maligned in some
regards for uh being okay with the idea
of machine intelligence overtaking
humanity and populating the Galaxy and
seeing that as natural and normal again
has been kind of vilified for that and
may or may not live in Edmonton Canada
that's all you get for Clues you're
going to have to tune in next time here
on the trajectory so stay subscribed
thanks for tuning in I look forward to
catching you then

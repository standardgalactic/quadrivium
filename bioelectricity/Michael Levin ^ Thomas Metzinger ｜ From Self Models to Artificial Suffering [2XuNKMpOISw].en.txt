good evening everyone and welcome to the
great mind Series tonight we have the
honor of learning from two brilliant
thinkers whose work has not only
Advanced the field of Consciousness but
has reshaped our understanding of the
Mind itself our first guest Professor
Thomas met singer and joining him is
Professor Michael Levin thank you both
for being with us tonight so maybe to
start um I've been saying for a long
time that selves do not exist and I come
at this of course from a very different
angle uh than Michael does uh I come at
it from the Consciousness angle I've
been in this Consciousness thing for a
long time and I think the first thing
one has to understand is that
Consciousness is not one problem but
many so there's a question say of global
integration there is the question if
phenomenal atoms exist smallest units of
experience and I thought of all these
many problems the most difficult one is
the subjectivity of the target
phenomenon that as many people claim it
is always tied to first person to a
first person perspective I think that's
false um but the question
was what is a first person perspective
and how could it evolve in natural
systems right um so one concept for for
instance for for me Michael talks about
epistemic agents I have recently begun
talking about epistemic agent models uh
that is systems who model themselves as
a knowing
self but I've done all this under the
assumption um that no such things in the
world exist and now there are basically
two people two kinds of people who
disagree uh some people say yes selves
do exist and talk about selves and then
there's Michael uh who even spells self
with a capital S and
um that is maybe a good point for us to
jump off I think if um Michael's very
interesting and very ambitious project
gets through one of the most important
things is the principle of parsimony
that we don't unnecessarily
introduce entities that we don't need
for
explanation so I've always thought with
some
qualifications you can explain the
phenomenology of selfhood and everything
else without assuming the existence of
sell but I see that in Michael's writing
self plays a big role self with a
capital S and maybe you could explain
this a little bit Michael what you mean
by this and what road it plays and so
forth well okay a few kind of a few
background things that I wanted to say
first of all uh you know my my main
bread and butter at least at this point
is not Consciousness research right so
so so typically I mean I have some
thoughts about it and I think uh I'm
working on some things along these lines
but I do not have a special new Theory
Of Consciousness that I've been pushing
or anything like that so the vast
majority of things that I've been
writing about are uh much more practical
a conventional third person accounts of
cognition so so what I what I you know
have strong things to say about are uh
what I think about um different types of
systems and how we interact with their
cognitive properties um you know
Consciousness is is is is a is a
different problem to which we have not
done we have not added anything yet so
that's the first thing um this so so
anything I say on on this on the
Consciousness aspect of it is is very
unbaked at this point is what I'm saying
um the second thing uh I want to say is
that you know you you mentioned
something quite interesting which is the
uh how how how do these things evolve
and I just want to La my my cards on the
table at the beginning that I I think
cognition and Consciousness are a
superet of living things in other words
I think they were here before life and
they do not um fit within the this what
I think is a smaller set of things that
we currently call Life uh without
getting hung up on the definitions I
think that things that we call life are
things that are good at scaling up uh
these these very simple cognitive
properties life is very good at at
enlarging what I call the cognitive L
cone and so and so to you know in it's
more advanced versions at least for now
those two things track each other nicely
but I think as we go to the bottom uh
quote unquote the bottom of of of of the
the spectrum of cognition I think we we
come upon things which are already on
that Spectrum but would not be called
alive and so so what I see the job of us
when we when we talk about evolution is
not to show how it shows up for the
first time because I think by the time
you get to biological evolution that's
already here I think right I think our
job is to show how does it scale and how
do we understand it interact with it and
and and things like that and I guess the
um the the the last thing I would say
about parsimony is uh I I like parsimony
as as much as next person but I think we
need to be careful that our job I think
I think our job is not simply to explain
our job is to uh to to discover and
invent which means that I can imagine um
scenarios in which we pick the most
parsimonious model but that model does
not facilitate the next round of
Discovery in fact we've made it we've
made it you know we've made it so
parsimonious that it's it's very good
for what we have now but it doesn't
immediately show us what the next right
what the next step is so so I'm not
super tied to parsimony although
although of course um you know we don't
want to multiply ideas un unnecessarily
but but I just want to be clear that
that I like models that help us um in
this in this journey of of Discovery and
invention um not simply explain the
things that that we've had before yeah
that is actually one thing where the two
of us really strongly agree uh I think
the value of a still metaph torical
description really consists in its
euristic facundity
so will it help us to actually generate
novel prediction or control phenomena uh
and I mean good philosophy could even do
that even if it uses somewhat fuzzy high
level Concepts it can inspire people
that's a Criterion uh does something en
able progress but when when you spell
like when you say there's say in a large
U nested uh system causal structure
there is a new functional hole and you
call this self with a capital
S
uh what is the
fecundity yeah I will give you what I
think of as a definition with what what
I mean by it but I also think we should
talk about what we expect that word to
do for us because it's entirely possible
that that the thing that you are uh
deconstructing here is not you know that
we we may well agree on that right so so
we have to we have to talk about what it
is that we are what what exactly do we
mean when when we you know say that that
they're ourselves or not so so here's
here's how I I have used it um for me
I've used it as a term that
emphasizes uh first person perspectives
that include veilance and attention and
decision-making of systems that are the
owners of goals preferences and memories
that don't belong to their parts and I
we I I can give you some examples but
but the point is that a a coherent self
is an emergent hole that is kind of it
it's implemented by an interlocked Triad
of of of three things an option space
with or a problem space within which it
operates it um a cognitive light cone
that demarcates the side of the goals
that it can pursue okay so what is the
size of the largest goal that this thing
um can
pursue um and a set of uh cognitive
processes that allow the system to
navigate that space with some degree of
of Competency I see selves as systems
that fundamentally live forward in time
that is uh they are attached to not them
you so so so they may have memory
they're not attached to the um uh the
past interpretation of whatever memory
traces exist in their body but but
they're fundamentally a decision-making
system that has to be creative in
deciding what do my memories mean now so
and when we can we can get into get into
that whole thing so um there there there
are a collection of parts that have been
some way um uh found alignment or bought
into the same uh story about themselves
and the outside world um and uh there
there's there's there's some other stuff
about how they interpret and and uh
their own memory grants and and things
like this but that's basically what I'm
going for so this is um you
know the what what I've just described
is is not necessarily um have all the
features of the things that other people
mean when they say selves and so it may
well be so I don't know you can tell me
if if we're if we're in agreement or not
but maybe we shouldn't get hung up in
these more unimportant details I don't
know if you should maybe explain your
concept of a of that ative light cone to
the audience
briefly but before you do I'll ask you a
question right away
so for a very complex system like the
two of us for instance couldn't one
distinguish between an epistemic light
cone and a pragmatic uh light cone so in
in the say the epistemic surface and
maybe the computational surface to the
world that I can use an active inference
so that I can see think or
citate about things far distant in the
future as a human being but these are
very obviously things that I can never
interact with causally so there is
something I have a pragmatic cone in
this
embodiment but I do also have an
epistemic cone which may be much larger
does does that fit into your conception
of the like cone good point uh let me
let me just describe what I have in mind
and and you can tell me which of these
you think you think I
mean what I don't mean are uh the things
that I I don't mean anything like a like
a radius of a sensory motor boundary so
I don't mean the thing you know the
James Webb Telescope has an enormous
sensory reach but but that's not what I
mean um and I don't mean the things that
are practically achievable by you in
fact that's what I think is something
that's interesting about humans is that
maybe uniquely we have a cognitive lyone
that is for sure bigger than our
lifespan so maybe maybe that's unique
what what I mean by the cognitive lone
are the type the the size in some space
but let's say in the 3D space we can we
can collapse it to space and time um the
size of goals that you can represent in
a in a um in a in a go in a goal
directed action Loop the action Loop is
a this simple um cybernetic thing where
you sort of measure your current state
you have some record of the state you're
trying to get to and then you try to
minimize the error and you try to get
there so so let's just uh let's just
think about different different types of
creatures that have cognitive lone so so
uh if uh if if you have a dog um clearly
it has it has some memory and some
predictive power and so on but I I don't
think and this is an empirical question
but I I don't think you can get your dog
to care about what's going to happen
three months from now in the next town
over right it cannot it can not um uh
never mind the practicalities of whether
it can achieve go those goals but it it
cannot represent those goals right the
goals that it has are are are much
smaller both both in space and time if
you have a bacterium the cognitive lyone
is even smaller so so I don't know maybe
20 minutes forward and back in terms of
in terms of the time axis and and it's
you know it's it's it's measuring and
and it cares about um the the sugar
concentration in a very small space
right the the reason that and and so and
so if you have a human can represent
goals that are enormous you can work
towards world peace and you can think
about the financial markets 100 years
from now and whatnot these are right so
so the scale of the goals that you can
represent as a goal in some uh
homeostatic process and the reason that
that that these things matter so now now
here we get to this issue of of
fecundity so um so so in my lab what we
try to do is we try to take some of
these philosophical ideas and embody
their implications in Practical U
inventions of various types so so here's
how something like this ends up being
being very useful um the the story of
cancer so one story of cancer which is
different than the conventional story of
cancer is that cancer is fundamentally a
dissociative identity disorder of the
cellular collective intelligence that
basically what happens is that Evolution
has taken in in individual cells you
know unicellular organisms with a very
small cognitive lyone they only care
about a local their own local physi iCal
States and things like this and then
through some very interesting um
mechanisms which we've studied uh they
assemble into uh larger systems with
much larger cognitive lyones for example
the ability to um pursue specific
anatomical outcomes in in morphospace so
they build nice organs uh you know
livers and and fingers and things like
this which individual cells cannot no no
individual cell knows what a finger is
or how many you're supposed to have but
but the embryonic tissues absolutely
knows and so the reason for thinking
about it this way is that when you have
a breakdown of this this cognitive glue
these mechanisms that that bind cells
together to give them a larger lyone by
the way projected into a different space
when you have that breakdown what
happens is that uh it's a it's a failure
mode of that process individual cells
disconnect they start to pursue local
tiny goals and as far as they're
concerned the rest of the body then is
just environment right yeah so so what
this what what this crazy kind of uh way
of thinking about it suggests is
actually a very specific therapeutic
approach that when you have cells that
have done that you don't necessarily
need to kill them nor do you necessarily
need to repair the hardware that may
have led to this for example a a a nasty
mutation like Kass and things like that
but what you could do is force it to
reconnect to the collective and we have
ways of doing that and so in animal
models and now we're moving moving
towards towards human tissues if you if
you take those cells and you and you
force them into an appropriate
bioelectric circuit with their neighbors
they basically again become part of this
Collective which can remember things
that individual cells can't do and then
they begin to work on on the right thing
even though they're bearing some of
those mutations so you can overwrite
some of these um Hardware defects with
this kind of this kind of process so so
I think you know that's just an example
I I think it matters to ask the question
what are the boundaries of the system
specifically not not just with respect
to energy as lots of people do or or
even information but actually to the
size of their goals right when you have
a when you have a system what are the
size of the goals that this thing is
going to and then yeah and then that
that that helps you um invent new ways
to interact with it I get that but then
it becomes very important to be
conceptually precise if if it's the goal
to um to think about what go
directedness actually means so from a
philosopher perspective it could mean
what we call practical intention it so
there is this whole thing
intentionality breno 1874 today one
would say there is theoretical
intentionality some system directed as a
a set of Truth conditions and there's
practical um intentionality a system
directed at a set of satisfaction
conditions but all of this I think
doesn't license talk about goals you
know of course there are all these
things like functional set points on
attractors on different levels of
granularity there are these I found very
intriguing ideas of idea of you I don't
know how you put it this um editing a
homeostatic pattern uh to influence uh
the system but all of these terms do not
carry any intrinsic normativity in them
these are not goals so if I wanted to be
a very radical hard-headed scientist I
would say there are no goals in this
universe so what we have is systems for
instance systems with smaller or larger
light cones that uh become
successful by hallucinating goals in a
certain way by internally representing
say functional set points or something
like that preferences in a specific way
for instance in a way that makes them
unable to see that they are actually
representing it and make it become as
philosophers would say
transparent uh turn it into a real goal
that's what I mean by hallucinating a
goal right like they they they predict
satisfaction conditions or some say
functional set point but in a way that
makes it a goal for them but still that
are no goals in reality you know there
is no intrinsic normativity we can speak
of the system has preferences it's
directed as at goals and so on and so
forth but that's actually very loose
talk and I
think how to
say one would get more euristic F
fecundity if one would think how can I
say the same thing on multiple nested
levels um without using the term
goal um and let me add a bit to this
story it'll take me a minute or two
um so I have thought a lot about this
very very special subcase of a
phenomenal self model like conscious
systems who have a global model of
themselves with global properties a meta
system representation if you will which
is transparent to them and then they
cannot recognize this as a
representation so they get the
phenomenology of identification and
selfhood and they get glued to the
content of their own self model and have
to act it out now I think it is the
structure of our own self model that
also influences our theoretical
intuitions if we look for good new
metaphors or good new theoretical
approaches and one thing is I mean we
are both I see we are both influenced uh
by Dan a lot and one thing Dan really
liked in my work is that I said we
actually have a phenomenal model of the
intentionality relationship this is not
only something philosophers talk about
you know uh age and goal relationships
or subject object relationships the
human brain
actually constructs a model for instance
in motor
control of the as us and an infector
being directed at a Target state or in
Precision control in consciously um
guided
attention we actually have a model of
ourselves focusing our attention for
instance on a perceptual
object this relationship is actually
part of our own conscious
experience this was successful it was a
good user uh surface it created the idea
that we have a firstperson perspective
but it may
also how to say
um it if we start thinking theoretically
as we're doing now this structure that
has evolved in our brains in our
conscious experience it might may limit
us so what we see is actually ele or
aspects of our own self model and then
we um try to discover this and say there
is go directedness
say even below a cellular level or there
is
agency not only on whole organism levels
but there is agency
somehow in a biofilm or something like
that but that is actually that way of
thinking is
infected um by the way our brains the
user service they have created by the
way our brains um model reality
so I I totally understand what you're
doing and I think it's very Charming but
whenever you say goal or agent or
individual or something like or goal
directedness I
think you're importing our phenomenology
there which is probably very
idiosyncratic or that's what you're
trying to find I I'll stop here did you
get the point um I I I I do and and for
sure um my point is not that any of
these things are going to be humanlike I
I agree with you that we have to fight
hard um not to import our expectations
of what these things feel like into
these other unconventional models um I
will say I will say this
um I I'm an engineer I'm not a
philosopher and so so maybe there are
subtleties here that that are missing
but I I don't I don't buy the existence
of some some kind of in fact almost
anything but but certainly um goals that
are true goals as opposed to apparent
goals so I think the the thing that
you're talking about I'm not saying
there are some kind of true goals that
are different from what an observer uh
observes either in the M eles or in a in
another system models it as a
goal-seeking system and interacts with
them that way I I don't know what it
would mean for there to be something
else over and above this that was a true
goal of which these other things are
just metaphorical and so so and and I'm
sure this is this is a controversial you
know stance in philosophy but uh I I
don't think there are metaphors and then
there are the real thing that we could
some you know someday get hold of I I
think all we have is metaphor certainly
in sence certainly in science I think
everything people ask me this all the
time do you mean that metaphorically or
real I don't know what you mean by by by
real it's a metaphor right so so all I
mean when I say Goals I don't mean a a a
human level um or or or or a humanik uh
you know pre percept of what is it like
to have a goal I don't mean any of that
what I mean is that um there is a
particular framework that is uh useful
in having a a a rich interaction with
that system as an engineer when we build
out either when we build things uh or
when we you know as a bioengineer when
you try to control biology what you
really have to know is how much
autonomous action can I expect right
right because because if you don't know
that you know if if someone comes to our
house to repair the heating system and
they don't have a view of the of the
thermostat as having little tiny goals
and doing things uh you know I don't
want to hear about Maxwell's equations
through the wires of the of the house I
they need to understand right that that
you can do a lot if you understand what
the system is trying to achieve and
under what conditions it fails similarly
not so good if they sort of try to um
you know appeal to to the thermostat's
you know moral intuitions that's not
going to work either you have to get it
right and so what I like as an engineer
is the task of saying okay uh as an
observer I'm going to make a hypo I'm
going to make a a an empirical
hypothesis here's the problem space I
think you're working in here's the goal
I think you have here are the
competencies I think you might have to
to achieve it and then we do experiments
and then we see how has that enriched
our interaction whether that be with a
cell or or with a device or or with a
friend or whatever um somewhere along
that that Spectrum you try you try to
get it right and and I you know Josh
bongard and I have been developing this
um uh this this thing called uh poly
Computing and the idea is simply that
any physical event if we try to ask is
it Computing and what is it Computing
fundamentally what you have to say is
from the perspective of which Observer
that is the exact same event might might
be interpreted uh and might have comput
computational value and and and
information content that would be
extremely different for different
observers and that's all there is you
know I'm not I'm not sure um I don't
know of any reason to think there's some
kind of ground truth to the matter uh as
to what it is and so so I you know again
I I don't you know by by using the word
goal I don't I don't mean to uh invoke
uh these other that I think don't don't
necessarily exist I just mean this this
Observer dependent functional um
interface that lets us have a richer
relationship than if we didn't know that
the system was a goal directed system so
why not take a very sober and
straightforward say control theoretic
notion of goal and say this is all there
is and we're going to spell this out on
different scales that's the pro you
could do that you could do that um the
problem the problem with picking one
level is that you're making an
assumption and and it's you know it's an
it's empirically testable you're making
an assumption that that particular level
will give you everything you want and of
course there are people who don't even
like that there are people who will say
no no no you take the you should take
the chemistry level right and and and
especially so so let's say right in the
bi in our community let's say the
regenerate medicine Community there are
people who will say forget this control
theory cybernetic stuff chemistry we
like we like models in terms of
chemistry and they're not really
reductionist because if you say to them
well so quantum foam right ultimately
you want to talk about quantum foam they
say n no not chemistry so you know no no
nobody wants to really like like follow
that that through no nobody wants to
talk about Quantum F but they'll they'll
pick they'll pick a level and they'll
say no no chemistry is the right level
and then someone else may say chemistry
is too low but but cybernetics and and
and control theory will will do what you
want um that may be and that may be
enough for some things but I think it's
a this is not a choice that you can make
philosophically I think this is a an
empirical question for any given system
what is the right level because depends
on your epistemic interests yeah well
well and in some of our work what we're
finding is so so so one of the things we
do is we import or we try to import
tools and approaches and Concepts from
behavioral Neuroscience into other
non-rainy things and see how that works
out for us that's the that's the
research project so so one of the things
we find is that actually reporting not
only active influence but all kinds of
stuff you know in terms of uh Stress and
Anxiety and perceptual bability and
visual Illusions and false beliefs and
all of these kinds of things in some
cases those actually give you quite a
lot more control of of and and and a
richer interaction with the system than
you could have had if you stuck with
control theory so I'm not saying control
theory is bad I'm sure there's plenty of
places where that will be sufficient um
and and maybe there's even places where
just the chemical story is sufficient
but I don't think we can assume that
that's going to take us all the way
through and I think because you know if
if if we if we can agree that that
interactions with with humans can't be
dealt with by by control by the standard
tools of control theory that human came
very smoothly and gradually from a
single cell so somewhere in there right
there has to be this scaling process so
um yeah nothing control theory but I
don't see why it would have to be the
best level so
just a footnote I mean
again I see you've read quite a lot of
dennit and there's this concept of the
intentional stance there's this edited
collection
19987 but the point the move that Dan
was making there was to be an
instrumentalist as we say about
intentionality so not be a realist and
say intentional properties are something
that we find in the world and brains or
somewhere else and then we have to
explain them but to say the whole
intentionalist vocabulary is an
instrument we use and if it works we can
use it on different levels but we make
no assumptions about what is the ground
of reality or if intentionality is some
irreducible property and what are
there's something I really like uh about
stuff I've read from you is maybe we
should get to your idea of scale-free
cognition
U but you won't like why I like it I I'm
getting accused All My Life by other
people that I uh pour out the baby with
the bath water and uh what I like is
that you're really doing this when you
say scale free cognition I think that's
a bit too much um so what what we have
is levels of description analytically a
level of description is um constituted
by a logical subject and a set of
predicates so a logical subject could be
a bofilm or a brain or what have you and
a set of predicates are you know where
we ascribe properties to that logical
subject now I read you as actually
saying there is a there are many many
more levels of description that are
relevant and there is a continuity there
but you're not but you're packaging it
as if you say there is no no level of
description and you're not saying uh
cognition is scale-free you're just
saying boy there are so many more scales
than what we've ever thought about right
but yeah you cannot work without scales
or without levels of description is that
right this is correct and I think I'm
following in the terminology and I can
see how it could be confusing but I'm
following terminology that in in some
Sciences you say that there are certain
principles that are scale not because
there is no scale but because you find
the same thing at multiple scales it
isn't inherently tied to one scale and
that's what I'm saying I agree with you
I'm not saying there are no scales what
I'm saying is um we should untie our
hands about uh certain important um
dynamics that we are used to finding at
one particular scale and not just scale
but in fact in space so so people talk
about behavior and embodiment and these
kinds of things and they specifically
mean you know threedimensional space
you're moving and perceiving in space I
want to untether all of that and I want
to point out that right in other problem
spaces at other scales you can take uh
and that's an empirical question but but
I think the evidence is good now that
you can take some of these same um
Concepts and apply them to to to things
that we never would think of as
something something like like us you
know medium-sized object moving at
medium speeds through 3D space and I
want to um one other thing about um I
wanted to get your thoughts on on you
know denn um instrumentalism and all
that I I I I like it a lot um because I
think that what we're fundamentally
indexing into is the tools and the
approaches the reason I say these things
is is because the tools and approaches
of um cognitive behavior science seem to
be working at these other in these other
um you know in these other areas but my
question is why do we only apply this
like when you were saying you know it
it's it's not um something real we find
in the in the physical world it's it's
it's this kind of like uh you know an
instrumentalist I I'm why do we not
apply that to everything like not not
just not just these questions of
cognition and whatnot you know my my
molecular biology colleagues will say
well attributing memories to cells or
beliefs to cells is is metaphorical but
Pathways you know what we really have
are Pathways I I I don't see this at all
I think I think that all of the things
that we are talking about these models
the models right which is all we have
are exactly the same you treat it as a
pathway because you have a set of tools
that have gotten you to you know to a
certain point where you can treat it as
this um it might be a a pathway you
might be think it's a touring machine
you might but but all of these are just
formal models we do we ever connect to
the actual thing I feel like I feel like
we're we should be instrumentalists
about all of it well it looks like we if
we ever know reality at all we know it
under a representation and that can mean
many many things it can mean under a
description like pathway or something
thing but it can also mean under a model
created by our brain right now like
think about the inceptive self model we
have or something like that we know
certain functional aspects of our own
organism under this model which has
nothing to do with language or doesn't
have syntactic Properties or anything
like this so there's a multi
multiplicity of representational formats
under which we know reality
and where it gets interesting of
course is when you ask yourself who is
knowing and what is knowing uh because
there's of course also a representation
of I guess in your world that would be
infot taxes or something like
that I don't know if that's fitting or
appropriate here but um I think there's
one thing for a system do something like
infot
taxes and control the process by having
an internal model of what is going on
right agreed yeah yeah I agree with that
those are to to me those are different
steps along the um and not steps but but
the different different uh capabilities
across the the the cognitive Spectrum I
think there's an
interesting class of systems of
functional
holes um
which do what they do by actually using
a self model by using a generative model
of themselves and I was just to give you
an example I was very enthusiastic about
Yos bongat starfish because uh that
robot allowed me to illustrate and to
explain also to a larger audience what I
been by a self model and a self model
that evolves in an individual system
finally in a really consistent way uh in
a way that makes it intuitive for people
um something that starts with random
movements of effectors and then
calibrates an internal body model and
just ends up pretty quickly actually
after a short steep learning curve with
the capacity to control a body as a
whole um so this system certainly
doesn't have conscious
that starfish we could IV we could even
there videos of it which we could um
play to the audience if we wanted to it
wanted to do that this system is
definitely unconscious but it controls
Global properties of it self body shape
motion with an internal model of itself
now what is the cut off point when it
becomes so to
speak computationally
interesting to control
yourself with a under a representation
of yourself as a whole so
um you will know much more about this um
than I do there is modelbased control
and there's model free
control and there will be self
modelbased control of a whole organism
or entity and self- modal free control
if I am
right uh does this scale up and down in
any way or am I wrong would one say that
even very primitive self-controlling
entities
must you know um conand and
Ashby must
instantiate some kind of self model just
we don't see it in these do you have any
thoughts on this I do and I think this
bleeds into some other stuff that I
wanted to talk about which was uh your
your awesome paper on artificial
suffering and epistemic
indeterminacy um I think that uh we have
to be very careful I I because I think
that all of this is uh defined from the
perspective of some Observer which may
be the system itself for a significant
system
um by saying that something does or
doesn't have a self model I think that's
really hard and you know and especially
I I you know and and I think two
observers can can disagree about whether
that's the case and um you know Josh's
other stuff with Mor on morphological
computation right is is really apropo
here because what you end up what you
end up having is uh systems that
function as though they absolutely have
a self model but but there is no um at
least you know that anybody has has
found there is no explicit um symbolic
representation of it that that anybody
can see because the material is doing a
lot of the work it's not the classic you
know if you do if you do the classic
thing where you write code and you say
okay I'm going to represent some angles
and some things here you know you can
see what the what the model is but
morphological computation and and some
of the stuff that we study show you that
you can get there without anything that
we as frankly very um inexperienced
observers of these things I think we're
not very good at noticing these things
uh would not recognize as a formal self
model so I think we have to be really
careful with that and the other thing is
that the kinds of things that we are
interested here which is which which is
cognition and then and then
Consciousness and so on um I I I no
longer believe that these things show up
because of the material material and the
algorithm I mean of course the material
and the algorithm is important but I
think there is an
um another component here which means
that we have to be extremely humble
about these things and what what we
think is there basically I I I see in a
in a variety of very um minimal systems
um emerging not just emerging complexity
that's easy emerging complexity is is is
very easy to get but I see but what I
see is emergent cognition that you know
sometimes very minimal sometimes more
significant and and we can talk about
where I think it's coming from but the
bottom line is that I don't believe it
is defined in any way that we yet
understand by the accounting of the the
physical parts of the system or by the
algorithm that we thought we we you know
sort of um implemented here so so that's
that's my point is that is that I I I
think there's an exam there's a there's
a kind of emergence of cognition uh that
we could talk about that makes it very
difficult to have to to be able to say
anything unique privileged about whether
something does or does not have an
internal model yeah so there is I mean
this umbrella term of cognition of
course there's a risk of vacuity there
too I mean if we use it across scales I
mean what I would offer is something
does
cognition
uh if the
representationalist level of description
whatever it means in that case is still
tically if it helps us to predict and
control like that so that's the
naturalized version of intentionality
basically the representationalist level
of description and then there's a can of
worms of I don't know uh gener
predictive processing what format what
Dynamics and and and everything uh but
as long as we can still say the system
achieves what it achieves
by generating
representations U and this is helpful
for us then we could still say there is
cognition there uh what is a difficult
question is when you say emerging
cognition because there are these I mean
there technical def uh def technical
discussions about Notions of emergence
in philosophy but just there's a very
benign version that many scientists will
use that you have new system properties
that are suddenly properties of the
system as a whole which maybe we
couldn't
predict from the past right which are
genuinely novel or
sudden but there's also strong emergence
which say the emergent property say
cognition the new emerging property
actually exerts downward causation it it
has a causal force of itself yeah I
would hold if that is possible the
scientific image of reality is gone and
this could be a universe full of
Miracles so the question is if if we
say cognition
emerges do we want a benign modest
concept of imer let's go for the scary
one okay you go you do you go ahead yeah
I okay look this is these are these are
very new new I um ways of uh ways of
thinking at least on my part that I'm
going to try to try to say so so no no
doubt there are many cans of worms here
okay I have I have no no doubt this is
not fully worked out and it's not going
to be I'm not going to consider it fully
worked out until we use it to actually
achieve new things in the lab that's
what I like as you know the the the the
the evaluation of these ideas but but
let me let me throw out some things um
I I I I don't want a universe full of
miracles in the sense that I don't like
mysterian views that we can't you know
we can't know anything about what's
going on um however I don't think that's
the same as saying that um there are
uh causally important uh causally potent
patterns that Ingress into the physical
world uh that are not themselves
determined by anything in physics and
what I mean mean by that now now the the
the less controversial version of this
is is a kind of platonis mathematics
right is that uh we know there are
truths of number Theory and certain um
you know patterns that you get from from
from very simple formulas and complex
numbers or fractals with you know all
kinds of shapes if we ask where does the
where do those things come from you're
not going to get an answer that has
anything to do with physics that th
those I mean this is I know there are
people who disagree with this too but
this is my my amateur view of of what
mathematics does I I I think they're
discovered they're not invented and I
think that a lot of these things if not
all of them would not be different if
the uh constants at the Big Bang were
were were tweaked and we had a
completely different universe and
different physics I think those things
would still be the case so I I prefer a
mathematics first view um some people
obviously don't and I think that these
things do have causal power in the real
world I think Evolution I think what
evolution and and living things are
amazing at doing is exploiting these
free gifts uh that that do not have to
be directly um um uh engineered or
constructed they uh they appear from
wherever it is that the that the truth
of mathematics come from and we can talk
about the properties of prime numbers
that that tell you why teas come at you
know
17 every 17 years and things like this
and we can talk about other patterns we
can talk about laws of computation that
that allow Evolution to evolve um to
discover ION channel so a voltage gated
current conductance right a voltage
gated ION channel and immediately I mean
that's that's a transistor and
immediately with a couple of those you
end up with a truth table now you didn't
have to evolve the truth table you get
that for free as you get many things you
know the fact that if you know two
angles of a triangle you don't have to
look for the third you know what it in
flat space you know what it is so so I I
now now the reason that I I don't think
this is this is a mysterian view in that
you know the mathematicians they they
have what they call a map of mathematics
which is like they believe that that
space at least some of them believe that
that space is structured and that what
we can do is have a um a rational
research program to map out that space
it is not the same research program as
mapping out the physical universe but
nevertheless it isn't random it is not
just a bunch of random stuff that pops
up from time to time there is a
structure some kind of a metric to that
space and
that more interestingly that that by by
discovering one thing in that space it
now helps you discover some other things
around it that that they're literally
right and
and and and how we do this is this is
this is one reason we make xenobots and
anthr robots and and all these um
synthetic beings because because what
I'm very interested in is a question of
okay when you when you have a you have a
creature normally when you ask why does
it have a certain shape why does it have
certain behaviors the standard answer is
evolution right so so for years you know
eons of selection you've selected for a
specific thing well we can make um I
know where the history doesn't play role
there's no there's no history yeah so
you get it so so so now the question is
where did these properties come from now
um much like with certain properties of
networks and things like that some
people will just say it's just something
that holds you know this is just a real
it's just a fact that holds well that to
me is a much more mysterian position I
don't like things that just randomly
hold I I I would prefer to think that
there is a space of things that hold and
every time you find one that helps you
push forward a little bit and find some
others around it and so that I I think
these things that Ingress into the
physical world are not random stuff that
every once in a while we find some kind
of thing that seems emergent and
surprises us I I think we can do better
than that I think I think there's
actually a space of these things that
contain mathematics but also I will
conjecture here um I think probably
contain kinds of minds and that what
we're doing when we make these physical
things like xenobots and and seesaws and
you know simple machines and whatnot
what we're really making are Pointers
we're making pointers into that space
pointers in the sort of computer science
you know definition um that and and and
that and this is this is why I I you
know one reason I really liked your um
your your your paper on on the
artificial suffering paper is that I I I
think that is what requires us to be
very very careful about uh knowing
knowing what we have once we've built
something you know I I've seen so many
people who say well well you know I I I
wrote this thing I wrote the code for
this I know what it does no I I I don't
think that just because you built it
using the laws of chemistry or
engineering or you wrote the code I
don't think you necessarily know what it
does and in our latest work and there's
more coming um on very simple sorting
algorithms these are these are very
simple minimal deterministic fully
transparent algorithms that people have
been studying we found novel um novel
behaviors that for example um delayed
gratification and things like this that
do come straight you know do come
straight from cognitive science that no
one ever thought these things could do
and no one had ever looked because they
assumed right so so anyway so that's so
that's kind of my point is that um I I
don't want to be mysterian about it but
I don't think the physical facts fix all
the causally important facts yeah so of
course the platonism that's really deep
deep water maybe we should postpone this
to to another conversation but would you
agree I mean the pointer to the space
the pointer is itself a representational
device and what makes these things these
discoveries causally effective is
properties of the representations
of these say mathematical
structures and not they themselves you
know I mean so so granted these are
extremely deep Waters and probably far
deeper than than you know I can I can go
in in the on the philosophy side but but
no I I I actually think both sides are
critical I think that that the physical
things uh are very important because
they determine which um causal
influences from that space are going to
Ingress but the but but they by
themselves do not have all of the
information nor all of the
um predictive you know power that you
need to know what you're going to get
when you make that pointer you know
something about what you're going to get
you know if you make a very simple if
you make a mechanical clock you are not
going to pull down a high level
intelligence you know you know that much
but when you start making but but you
don't actually know exactly what you're
going to get and we are surprised all
the time and I don't think it's because
it's random and un unknowable I think
it's because we have just begun this
this this this scientific Enterprise and
and if we take seriously the idea that
it's a space that that we can explore
then we have a research program and
that's that's exactly what we're doing
so before we go to the artificial
suffering let me ask get that get at
your point from a different angle do you
think there are radically radically
unpredictable events for instance say in
transitions from simple forms of delay
gratification to more complex forms of
delay uh
gratification are the
things which surprise us now transitions
that surprise us now and look
unpredictable to us now or will they
also in the
future always re remain have this
quality of having been and still be
unpredictable the transition itself yeah
I think that um I think there will
always be some new things that are
unpredictable but I think that window is
going to move and if we do our job
correctly over you know continuing over
over um uh re the time of future
research the things that are surprising
to us now will become less surprising
right I mean I I I do think that a lot
of and I know this is also very like
emergence is a very very deep Waters but
uh I I do think that a large part of
emergence is has to be looked at from
the perspective of an observer and the
and the surprise level of the Observer
so for example um um the cellular
automaton in The Game of Life right if
you if the the the first time I read the
the ru the three rules of the game of
Life Conway's life I did not immediately
say oh yeah I see of course gliders you
know moving at 45 degrees I get it this
was to me to me I looked at that and in
fact in the book the book that I read I
think Poundstone or somebody it it said
look the these simple rules have this
incredible emergence uh emergent
richness of this world you got some
gliders and beehives and all this stuff
I I I I don't know you know when when
Von noyman saw those rules he might have
said immediately oh yes of course that's
not emergent gliders aren't emergent
they're obviously entailed by these
rules I can see it I can see it right
now and so the question of what's
emergent what's
surprising uh right is is I I think
Observer relative and I hope we get
better at uh at at predicting these
things I don't think they're
fundamentally unpredictable but the
question I I I don't think they're
fundamentally unpredictable but I don't
think they are
predictable uh in the way that that a
lot of people are are looking at this
now you know I I I think we're going to
we're going to need a lot more work on
this I don't think we're good at this at
all for these kinds of things let me
tell you something that happened to me
three days ago I this friend came by and
I said look at these flat worms and they
make one without head and they make one
with two heads and you know they you can
cut them up to 250 times and and and
show and and see this tadpole with eyes
on its tail and spinal cord and I just
showed this to a friend who was visiting
and he and he said uh these people need
to be locked up you know and had this
absolutely not a philosopher or
scientist absolutely strong reaction of
disgust and the they are treading on
Dangerous Ground this is something where
science shouldn't go and I guess you
must have encountered this so many times
and I came to this um artificial
suffering paper I wrote from a very
different angle because I was in the
European Union's high level expert group
on artificial intelligence supposed to
do the ethical guidelines and collided
headon with the industrial Lobby and uh
there were 52 experts and there were
only two people who said there's a
problem with inadvertedly creating
artificial Consciousness not
intelligence yeah but synthetic
phenomenology without even having a
theory of what we're doing and they all
said just this is bizarre this is
science fiction and the only other
person uh in the whole group who saw the
risk um connected uh to um synthetic
phenomenology was Yan talin the inventor
of Skype and everybody else said you
guys are just living in a dream world
our robots crash all the time this
doesn't work this is never going to
happen yeah um
now if we do let's put it like that
would would you agree if
anything um like the story you are
trying to tell or you the the windows
you're trying to open for all of us in
your theorizing and if anything like the
free energy principle and what Carl has
been saying is true there's an elephant
in the room and that elephant is that
life might not be worth living uh and
that there might be just in sentient
systems there might be just much more um
suffering than Joy um that we we are
these anti-entropic systems that fight
this anti this uphill battle but in the
human case as you have written this
toxic information has entered our self
models telling us we will die and we all
know we will lose this battle and we
will disintegrate in the end but on the
other hand we have this what the
Buddhists call batana uh the craving for
existence in a certain sense I think
which your work actually adds aspects to
is we are a craving of existence you
know on on on many scales and it's it's
very clear that as soon as sentience and
Consciousness enter enters the picture
this will create a lot of suffering
right and we haven't fully understood
why we suffer so much as human beings um
there's a long tradition there's
Buddhism and everything
um and there's an extremely high risk
that we would multiply this on Cascades
of not artificial but as I say
postbiotic systems systems for which
this distinction between artificial and
natural doesn't make any sense because
it's not exclusive and exhaustive so
that we trigger a new scale level in the
evolution of cognition which brings a
lot of suffering with it before we have
even understood
what the root causes of our own
suffering are and if anything can be
done about it so that it is super super
um
dangerous to play with these things and
try these out because as we all know in
the history of
science sometimes things happen earlier
than everybody has
thought and sometimes you you said
you're an
engineer sometimes you create something
without actually having the theory for
it you just yes create the it's true
right sure of course so what are your
thoughts on this should we be yeah do we
have a
responsibility um yeah yeah uh
absolutely critical questions um okay so
what I what I don't feel competent to
give um a kind of authoritative answer
on is the problem of existentialism and
whether it's better to exist and to die
versus being nothing I can tell you my
I'll give you my personal belief but
that's all it is everything else I was
saying before was driven by some some
degree of experience experiment and
things like that this is just you know
this is just my my my personal belief I
don't have any way of of um you know
having an argument some kind of a
knockdown argument for it uh I I have a
uh overall I have a kind of a positive
and and optimistic view of all of this I
think that there there are and and I'll
tell you how this gets into my dilemma
as well um I I I think that to the
extent that we can push forward uh
knowledge and and you know W wisdom and
understanding we are going to uh get
better at the exactly the problem that
that you've said now and uh I I I think
um I think we need to I think we need to
push through a a painful part of of our
development as a species Maybe maybe
other maybe there are you know other
intelligences in the universe that do it
too but for us uh I I think we are going
to push past this to the point where we
can a understand um the the the the the
causes and the implementation and the
meaning of of joy and suffering and we
are going to be able to do much much
better than we're doing now and I think
it's worth it I I don't I don't think
that this is an argu that any of this is
an argument for nihilism or that it's
better off to have an the universe with
nothing in it uh I I I I reject those
things and and and here so so now let's
let's come back to um the person who uh
who who who visited you before yes yes
so so okay so so I receive I'm I'm not
even a clinician okay I do basic science
but I get every day I get uh let's just
for the purpos of the ratios I'll I'll
get one email from somebody saying oh my
God uh what you're doing is scary and
terrible and you need to stop
immediately and then I get 10 email
from people that say my kid has a spinal
cord defect I've got cancer somebody
else I love has you know some other kind
of issue what do we do and why are you
scientists sitting on your butt and we
are having this horrible um experience
the the bottom line in in my
understanding is that we have now uh an
incredible amount of um of of biomedical
suffering in the world people people you
know and and the people who are the most
against this are the young healthy
people typically who don't or or or or
they just don't have the imagination to
to ask um when when something's wrong
with my child or the person I love and I
run to the hospital and pray like hell
that they figured out what to do with
this where does that actually come from
right it doesn't it doesn't you know the
only place that knowledge comes from uh
is is from uh from doing experimental
work in biology and right through
through dealing with with with with
people about on these issues for a long
time I've only met one person ever whom
I honestly believed you know uh you know
um strict vegan blah blah blah right so
so no you know no issues of of meat
industry and whatever and I believed her
when she told me that she would not
bring her kid to the doctor if they were
sick I believed her I think she was a
monster and I think the rest of us uh
understand that we have a responsibility
to people that are um you know there
there there's a concept in in in in bu
Buddhism of um an inauspicious birth
right and and I I I think at the moment
I think pretty much all births are in
aici births for the following reason we
now contrary to what a lot of people
think is that look uh we have this
optimal way you know about 80y year
lifespan is optimal and our our form
like this is pretty optimal so you
scientists don't screw it up I think
this is this is ridiculous what we have
now was not tuned for us by a by a
process that had right that had our
welfare or any of the things we we we
value in mind we were we were uh
basically shaped by random cosmic rays
and some other aspects of of of
evolution and and so on this is where
Evolution left us it is not because this
is how we're supposed to
be right of course and so so all the
people who have you know childhood
cancers and then later on uh you know
they have the stigmatism and lower back
pain and eventually you know
degeneration and death all of these are
random things that we are now subject to
and it is I I view it as as our
responsibility in the community of
bioengineers and and developmental
biolog
to allow people what I call Freedom of
embodiment you should not have to live
your life in whatever body you were
handed out by the by a pattern of cosmic
rays that that happen to hit your egg
cell when you know when you were
gestated uh we should we should have
control over the kind of embodiment we
have and that only happens if we have an
understanding of what you know how
bodies come to be how Minds come to be
and all and all of that right so so okay
so so that's so that's my answer to
people who are who are freaked out about
flatworm experiments but but this goes
but but there's a there's a there's a
problem here which is what you put your
finger on which is which is this uh I I
take the issue that you raised in that
paper very seriously I think I think
you're absolutely right to worry about
this and and I think we need to worry
about it the problem is and this is
something that I I wrestle with a lot
the problem is that to the extent that
we are we are um working to figure out
what is it about uh living things about
cells like an egg you know we all come
from a from a quiescent egg cell little
blob of chemistry and physics and then
eventually here we are so so to the
extent that we figure out how that works
and what's going on there it becomes
easier to create artificial uh uh
constructs right to that that are going
to have exactly the problem that that
that you're talking about and
unfortunately it it turns out that
understanding how to relieve the
suffering that that I was just talking
about all the all the biomedical stuff I
think the path to that goes right right
centrally through the issue of
understanding what is it about biology
that makes agents that matter in a moral
in a moral sense and and uh you know
what what I haven't done I started at
one point and I haven't done it and I
still wrestle back and forth with it is
um I think we're in a pretty good place
now and and I'm not the only one there's
there's at least one other person and
maybe more who could actually write down
what is what is it that that you would
have to engineer into your system to to
to reach that point I think from the
biology and it didn't come from the
computer science s or the AI and it came
from the bi from trying to understand
the biology and I think we're to the
point now where we can pretty much write
down a lot of that and right and I I I I
don't want to do it but and and you know
but but at the same time I think that
it's not going to be very long before
people who understand the biomedical
literature that is being developed who
are are going to catch on to all of this
right this is like this this is coming
and so I think that it is absolutely an
issue and I think it is you know uh in
scale not not in not in principle um but
because because we make living things
that matter all the time and and you
know we we know pigs can suffer and yet
there's the meat industry and so on uh
but but in scale and the number of these
agents that will be created for all
kinds of purposes uh this is this is
potentially very problematic and that's
why uh we really have to work on um
developing a better ethical framework of
recognizing other minds and relating to
other Minds in completely um uh uh
diverse and Divergent mations than what
we're used to yeah I'm not so optimistic
I mean I have demanded this moratorium
until
2050 but I would never dream that
anybody even thinks about it seriously
because the incentive
landscape of the investors that drive
this kind of research is very different
it's not Guided by any moral incentives
so all of this will be
done um many things that what you said
so I think there's a very
straightforward good to work by uh
Criterion for what counts as a moral
object it has to be conscious and it has
to be able to suffer uh we don't have a
theory of suffering but I have some good
ideas so any system that has a
phenomenally transparent self model
where it represents its own preference
landscape in it uh and which can have
thwarted preferences which are
transparently embedded into that surf
model will experience them as its own
suffering and then you can say things
about you know loss of control uh as
embedded into a conscious self model you
can say something about the rate of
prediction error uh in time you know
that creates a surprise uh that the rate
of prediction error um increases so f so
you can have some ideas what suffering
is but I think that's also what you
meant we if we want to go about this
seriously we need a theory of
suffering that is grounded empirically
grounded but how are we going to do an
evidence-based theory of suffering
without creating a lot of suffering in
the process and testing it out that's
the problem and what counts I mean I
mean that's a philosophical point you
know um utilitarianism I mean your
argument was is of course very
convincing it's actually the argument
from the medical need and the biomedical
need for This research it's actually a
scandal that's not much more money and
funding goes into exactly that kind of
research you know that's that's all
granted but the individual animal given
that has a conscious self model the
individual animal or newly created
system that you sacrifice in that
research that has no interest uh in this
human child being saved or so and it's a
relationship between you as an
individual and those individuals you
sacrifice even if on a population level
you could say this is doing this kind of
research and sacrificing animals
artificial system postbiotic systems is
in the interest of everybody in a longer
time
window the individuals will not agree
the individuals sacrificed and how does
one get around this do do you have any
ideas have you thought about this I I I
have and and I don't I don't disagree
with you um uh although um just just to
just to say one thing I think I think
what what you're saying about a theory
of suffering and so on is critical we
must have it but I also don't think it's
sufficient because I I don't think
anybody is going to argue that pigs
don't match that Criterion and yet it is
not sufficient we have all the you know
all the the convincing arguments that
you want for in that case and it really
it really hasn't moved the needle that
much and so so I think it's necessary
but not sufficient but the argument um
uh the the the the issue of of looking
at it from the perspective of the
individual animal yes so the way the way
that I propose to for people to view it
is as the
there's a trolley problem here and there
is there is no option of not having your
hand on the switch I don't believe it I
think each and every one of us is is
making a choice there is no not making a
choice you are going to make a choice so
so what I recommend everybody is to um
pay a visit to you know you don't you
don't necessarily get let in inside but
go into the front lobby of um of a
pediatric oncology clinic so so you're
standing there here come here come the
CH the children with cancer and now you
need to ask yourself uh yes the planaria
will not agree they don't care about the
kids but you are the one making that
choice they're not able to make the
choice they they they can't do it but
you can and so now you have to make a
choice there is no saying well not my
fault not my problem I'm taking my hands
off the wheel I don't believe that
exists and certainly I don't believe
that exists for those of us in the
community who actually have the ability
to uh contribute to that work if they
choose you know if they choose to do it
so the scientist uh you are going to
make a decision you are going to either
contribute to that to to a cure or you
are complicit in deciding that you are
not going to do it fair enough I can I
can I can understand if somebody says to
me I have thought about it and I am uh I
am literally uh uh I taking taking the
side of the of the flat worms and the
tats over the over the the human kids
with cancer but that's a caricature it's
not but no one does this no one not the
flat worms and the Tad poles those are
pretty sure I'm pretty sure they don't
have phenomenal self models I I I I am
not I don't know that I agree and in
fact and in fact yes yes no I'm not my
my goal is not to try to weasle out by
by um focusing on so-called World War
life forms that's not that's not my
argument my my argument is that uh and
and by the way I'm not sure that like
earlier you said you know X and Y is not
conscious so I'm not sure I I don't
think it's a binary thing I think I
think it's a
Continuum and I and I don't I I do not
discount any of systems that I work with
as not having something that makes it
trivial uh in fact in fact down to the
you know down to the to the vegans who
eat plants uh I I I don't think that's a
you know I don't think that's an easy uh
answer there either by the way that's
very interesting I'm a vegetarian on
ethical grounds for 48 years by the way
better than not but still but still
you've got sure sure you create all
kinds of indirect uh uh suffering so um
let me get at this from another angle so
I have an intuition the intuition that
is is
that the interesting point actually
falls out from you is that moral object
Hood might be a greated property you
know across the Spectrum yes and even in
that ethical real there are no binary
yes and no answers but I'll I'll tell
you a story so I'm in Brussels uh in
this AI thing and there's this highly
decorated lawyer coming and say I really
don't understand your thing about
synthetic
phenomenology can you explain this to me
I said I said yeah I think we
shouldn't recklessly risk the creation
of suffering conscious suffering on
postbiotic systems and then the guy says
yeah but listen we punish our
children and we torture and punish
animals when we want to domesticate or
train them and that's an established
cultural practice and if we could make
the learning curve of some of these
artificial systems steeper by endowing
them with conscious experience and
punishing them and torturing
them uh we have a cultural tradition why
shouldn't we do that and I just thought
oh boy oh boy I see how you take um but
isn't there actually a point uh that we
say yeah we we
deliberately make other conscious
creatur suffer if it's in our interest
and why shouldn't we deliberately create
conscious AI because we have this
idea it could become more efficient it
could speed up progress if we can
torture it a little bit or punish it a
little bit and it really hurts yeah no I
mean look U yes I that's that's not a
that's not the way to go but but but um
I as as as optimistic and uh positive as
I generally am about the Universe I do
think there is one true tragedy and and
the true tragedy is that it is
impossible to not have your hands on
that on that lever that that that's the
tragedy is that is that as far as I know
there is no way it it would wouldn't it
be nice to be able to say I I don't know
how to make this decision I don't know
how to prioritize the suffering of this
creature versus that creature and so I'm
going to literally do nothing and my you
know whatever you want to call it my my
my sort of karmic Ledger there is going
to be neutral and I'm not I'm just not
doing it I'm not doing either either
thing and the true tragedy for all of us
a agents that are complex enough to
understand the choice is that I don't
believe that's possible all of us have
to we have to pick and and and and the
best we can do and and that that's you
know that's horrible but but there it is
and the best that we can do is pick as
best as we can so I you know I I don't I
don't claim to have a solution to to to
the philosophy of
Ethics all all in all but I think that's
where we are I think I think all of us
have to have to be very conscious um uh
conscious and deliberate about what
choice we're making because I don't
believe there is no avoiding making that
choice you know I think you sense you
have heard about this there is a
classical philosophical answer to your
question which
is
one you move the lever you do whatever
needs to be done in the world in the
most rational and evidence Bas you are
based where you are capable
of including in the most compassionate
way you are capable of which includes
also maybe improving your own compassion
and training it and looking what the
conditions are for it but then like you
make the choice because as you say you
have to make a choice as long as you're
embodied in this
world but you do it out of a quality of
choices
awareness um so you come you do it
without egoic attachment you do it
because you are thrown into this
embodiment into this situation and you
move the
lever but you try to do it in an
additional mental state which is
actually choiceless pure awareness uh
observing without an observer this
answer has been G it has been given for
instance in the bhavat Gita um
but many of course sages in E Traditions
have spoken about something like this
like acting
compassionately as skillfully as you can
under these limited circumstance
but in a detached way
um does this ring any bells in you or it
yeah it it rings many Bells I've I've
heard this uh and and I'm I'm
sympathetic to it uh it's beyond my pay
grade to tell anyone how to achieve it
there are plenty of people who have uh
you know uh tried to guide others to
that uh I think we all have our journey
in that direction um and there's you
know there's there's not there's not
much I can add to it much much smarter
people than we have okay now going back
concretely to your own discipline I mean
if if you had to say what people really
shouldn't
do would you have could you produce a
list of
say red lines for the kind of research
that you are trying to do Foster or even
only develop right now what shouldn't
one do okay so so the the the first
thing I'll say is that any any such list
is not going to be optimal for the same
reason that you said earlier which is
that we cannot foresee in advance what
what you know what any particular
research program is going to give rise
to so so for sure uh we we have to know
right away that that it's not as if
we're going to do this and we're going
to do it right and then everything will
be will be great I mean that's naive
that's not going to happen uh we we have
lists we have very long lists of things
that we uh that like that um these lists
exist uh as guidelines for the research
industry so the kinds of things that
somebody that does when they go fishing
you know we need a month of writing
protocols and reviews by ethicists and
and uh and and veterinarians and
whatever before we can even begin so so
there is uh there there is an enormous
structure around this for the um for the
biomedical research community and then
all of us have our own personal things
that that we think are not um
sufficiently uh impactful to reduce
overall suffering that we are not going
to do them so so so all all of us
researchers have have various uh you
know various lists uh however I'm I want
to be careful there's a there's a free
riter problem here which is that
uh many of us will have some kind of
biomedical need we will we will run to
the hospital we will get treated we will
say I am so glad I got treated or or my
my kid you know is better and I'm also
glad I wasn't the one that had to do the
experiments that led to this right so
this is this is a problem so I don't
want anybody feeling too good you know
that they've avoided some of these
experiments that they themselves didn't
want to do it's for the same reason that
people you know who they don't want to
hunt but but they go to the grocery
store and they get their Stak like you
just have to think through where these
things are coming from and and this is
part of what I mean as as the tragedy
that that we have right now which is
that you cannot take your hands off the
wheel un unless you're really willing to
just uh I'm I'm just not willing to help
anybody and I've made that or myself and
I've made that decision all right I
don't have a knockdown argument against
it but I think it's insane how should we
think about say California Tech
investors who want to drive this with
financial incentives this development in
Ai and which who will eventually stumble
into synthetic
phenomenology
yeah do we as researchers have a
responsibility to stop them uh or change
the incentive landscape or not take
certain rewards I I think the problem is
even worse than that in the sense that I
don't think it's just the incentive
landscape if it was if it was just the
financial motive that's one set of
problems that we could try to deal with
but I think it's worse than that because
I actually think that the much more what
I consider the much more noble and
positive vision of relieving human
suffering that track goes right through
the the discoveries that you need to do
exactly this so even if so this is this
is my point and and I don't know what to
do about the investors who will
absolutely stumble into it at some point
I don't mean I don't mean we should
ignore that but even if we had some kind
of magic wand to make that go away we
would still have the much more I think
difficult morally difficult um decision
to make which is that if we want to
continue uh to relieve um biomedical
suffering we are going to uh inevitably
have to discover these very things that
that are that are problematic and you
know it's not just a matter of somebody
stumbling into it because they wanted a
better you know GPT or whatever it's
that understanding what it is about
cells and tissues that give them larger
uh ability to um to understand the the
the environment around them and to have
preferences and like understanding all
of that is is is exactly the road to
what you're talking about so I don't I
honestly don't see a way around it other
than developing the same kinds of and
obviously that's as I said with the pigs
and everything else that's extremely
problematic but trying to develop
ethical guidelines around these things
in the way that we develop them for
animals in the food industry for
companion animals for um you know other
other other beings of of diminished
capacity and and so on we already have
like I think that aspect of it and again
I'm not I'm not a you know professional
ethicist or anything like that but I I
think we already have that problem just
not in not you know not not as as as
scalable but we already have this
problem we deal with with other beings
all the time uh who are who are not um
you know not not not quite like us and
even and even and even with us right
it's it's how easy is it throughout
human history to find examples of of
this kind of like well they're a little
bit different from us so their suffering
isn't quite as you know not not quite as
problematic right humans are very good
at that kind of thing that that to me
that is what we need to fight is because
right and and to develop as you said to
develop a mature theory of suffering and
apply it then not just to these novel
agents but to everything and everyone
that we deal with I'm actually learning
something that will alleviate my future
suffering from Michael today that you
can actually ask for twom minute
breaks never occurred to me so I will
try to pick up that thread so there is
this concept of Ethics washing like
green washing which means that large
companies Google or even political
institutions
install ethical debates cultivate them
in the public with panels committees
even
Publications in order to postpone
distract from the real ethical issues
while the industry has a large time
window to develop and Implement things
there's something as a concept in
applied ethics that's called the pacing
gap which is the the temporal gap
between the implementation of a
technology and how long the political in
institutions take to regulate this this
often four years or more and there are
people who exploit uh that time window
now here's the question I'm battling
with um you say we just need this
ethical debate yes uh and uh we have on
the Fly we have to come up with some
good rules and regulations to do all
this but the thing is that we know in
advance for instance if we know we cause
suffering to pigs and we have a theory
of suffering for pigs and all that
society as a whole will not care um they
will treat pigs very badly knowingly we
know that in advance yes so the same
thing is if we now start to work on the
ethics of synthetic phenomenology and
postbiotic systems yes we know
given the global or the American
cultural
context the technology will be
deliberately abused by some people and
we
know compassion will generally Bel low
uh we know that agreed agreed and and we
have the same problem with with with the
traditional high level um intelligence
systems which are human children we have
the exact we have the exact same
scenario right there's
well what I mean is that even even when
I mean I'm agreeing with your point that
even when we have utter certainty that
if anything matters and and and can you
know in in a moral sense that that human
children do we still have the scenario
where uh some number I don't have any
statistics but but some number of them
are uh are raised in in in environments
that are just absolutely atrocious right
so we already have this it's not just
the pigs and it's not just the the
synthetic beings it's it's it's all of
us that that's why that's why I'm saying
that the this this this project of
figuring out what to do about um other
minds and suffering and B basically
anyone who is not you right the other
Minds problem um is a is fundamental and
it's not just about Ai and it's not just
about Sin bio it's about um it's about
all all cian beings which you know going
back to the tradition that you were
talking about let's turn a question
another way so in this new book the
elephant and the plant I have um asked
the question if we could create a
machine that reliably just stays in a
state of pure awareness no
preferences no preference frustration
just pure awareness for as long as it
exists like a med a meditation machine
or something like
this would there be anything wrong with
creating such a machine um
from an ethical point of view I don't
think you know because you wouldn't
create suffering you would have created
synthetic phenomenology but you would be
certain you create no
suffering and I was struck sometimes
when I read papers by you there is this
short light that goes on where for
instance you um talk about the idea of a
system that is liberating from the
global self
and abides in some kind you call it even
a neuronic
void um do you think there would be a
way
to how to
say to focus research on suffering free
phenomenology like
uh without having to
test a theory of suffering and then
inevitably creating suffering ing but
circumventing
this creating the first
non-sufferers iic stuff was written by
actually my my co by my co-authors on
that paper right so I'm I'm not I'm not
the expert on that stuff I we we we
collaborate closely because I'm very
interested in it and I think that that
body of thought has a lot to offer here
so so I you know I collaborate with a
number of people in the Buddhist
tradition and so on as I think that's
something we need to uh those are tools
that we need to use but I'm not I'm not
the expert on it um I think that overall
I'm optimistic in that much like with
everything else while we can't predict
everything without trying it we can
often predict a lot of things without
trying it and much like with
conventional uh bodies uh you know I
could come up with all kinds of
scenarios and if I were to say to you so
you know let's uh you know what if we do
this what how would that be and you say
oh don't do that that's Terri I don't
want that at all and so right so so we
have we have some ability to predict
some of this and I think as we get
better at these things we'll have more
ability but I'm under no illusions that
we will be able to avoid it entirely
we're going to screw up we're going to
make mistakes just like we have with
medicine you know um throughout
throughout the you know throughout the
ages we are going to mess up so you know
I don't know there's a there's a bigger
there's a bigger picture here too which
is like I did a I did a poll once um on
on Twitter and this these these were my
followers who are already kind of I
think skewed towards the technological
end but I asked them I said okay just
imagine you're you're You're A Primitive
uh you know early homid and you've just
uh you you you you're you're walking
back to your camp and and you and you
you you have this like uh vision of fire
of what it is but but also everything
else that comes after that so so you
know metals and swords and landing on
the moon and and and you know artificial
hearts and space shuttle and stem cell
you know um spinal cord transplant like
all of that stuff just you know you
suddenly see it all do you keep going
and show them the fire that you've made
or you put it out and you stay in your
in your cave and you know and then
that's that uh 6% of of of of the people
who read my 6% thought that you should
put it out and and and you know and stay
in the stay in the rain and that's that
so I you know I I I I I think what's
inevitable is that we're going to make
some mistakes and I think we do have uh
some some tools and some traditions that
may help but we are going to make
mistakes and the real question is going
to be uh do do we keep going or or or
not and I don't I mean I I'm not I think
that's not the real
question because Humanity will keep
going
somebody in China will keep going even
if you were going to shut down all all
your Labs I agree with that um I agre
it's like an Unstoppable Dynamics in a
certain way the question is rather for
me if we think about the project of
synthetic
phenomenology and these scales it's very
clear that our suffering has a lot to do
with our form of embodiment and
metabolism and you know homeostatic
failure allostatic failure and and and
stuff like this could there be
um systems who realize phenomenology and
intelligence but are
uncoupled from this
evolutionary thread in a way that they
don't
suffer from their embodiment is that in
principle possible uh or does this
Vision you have of scale-free
cognition imply that any future version
of cognition of
phenomenology will necessarily involve
suffering necessarily because that's
just how it works uh entropy and all
yeah I I mean the real answer is is
above my pay grade because we can talk
about Liberation from from desire and
from suffering and these these ideas
that are present in the in the right in
the in the kind of Buddhist Traditions I
I I don't have the background to to say
anything intelligent about that although
there are many people I know who would
um to to to under conventional under
conventional sort of motor sarandi I I
don't think so I think to the extent
that you are a a um uh a a viable
cognitive system you have goals and you
if you if you don't U measure whether
those goals are being met
then you know I'm not sure like I don't
I don't think you can have a cognitive
system without some some goal
directedness uh but what happens on you
know in these Liberation States I have
no idea that's I mean I don't know now
could one have an architecture that does
go frustration and everything and
intelligent
Behavior but has a layer that is not
attached because there is not the
phenomenology of ownership and
identification that is brought about by
a transparent self model for instance A
system that had a completely opaque self
model where the system would realize
this is all the virtual Governor this is
all yeah virtual is it is a
representation and that would be part of
its
phenomenology yeah I I I don't know um I
mean on the one hand if we take
seriously the idea that some humans have
in the past become liberated in that way
then that should be reproducible in some
other some other but we don't know that
that's we don't we don't know that yes I
mean we don't know that this folklore
it's fairy tales we we have no way of
knowing correct we we don't which is why
which is why I'm saying I mean I don't
have an answer to this right I don't
know whether that's true or not uh I
guess you know
uh it it one of the one of the
difficulties I have with that view in
general and and I've asked people this
is if if you do have sufficient amount
of
um you know Detachment or decoupling or
something what is it that uh how do you
choose your goals then you know what
what what what do you do and and I've
asked this like if every just imagine if
everybody on the planet uh acquired that
state assuming that's possible you know
they sort of detach what do they do
after that like that that's it do we
still do we still go to Mars do we do we
solve other problems or well if you look
at these empirical evidence we have a
simple thing is that Buddhist monks and
nuns do not have children so for
instance procreation would stop very
likely I think that's a plausible
prediction yeah
um
possibly you know there's the firmy
Paradox on and one of the possible
solutions I have thought about this is
that civilizations at a certain Advanced
stage just stop to procreate for some
reason or other because they change
their own architecture or because they
come to a philosophical conclusion that
we cannot really imagine that
non-existence is better yeah and that
that solves the firmy Paradox on but if
everybody would become a Buddhist
practitioner mankind would cease to
exist that's pretty obvious right what
we're handling in Buddhism is the
craving how intense the craving is is to
reduce the craving rather than not
reproduce and not continue doing what
we're doing but we're doing it with
compassions and being more present and
thoughtful yeah yeah but we're not
reducing into not reproducing or
anything like that or sitting not doing
anything yeah do you think there is
something like compassionate child
rearing or
compassionate skillful
procreation or is it
actually that's deep water I know it's
it's also a strange question but is it
unethical question yeah to risk that
your child yeah will not profit from the
fact that you're a Buddhist and will
actually
suffer it's a very interesting question
isn't it yeah the the only other is so
right so so I don't pretend to have any
anything to say about the one way or the
other about that question but but but
but there is something else which is
that I don't really believe that we make
other Minds in in any circumstance we we
we don't make them what we what I think
now we do is we create
embodiments uh through which various
kind the same the same way that the you
know the laws of geometry and and math
Ingress into the physical world my gut
feeling is that this is this is how
Minds come to be so we make embodiments
they become inhabited by by Minds in the
same way that the laws of mathematics
become inhabited in physical machines
and what what what sort of the those
mysterious minds are platonic
entities that well yeah the way that I I
mean yeah the way that the way that the
mysterious triangles are platonic
entities uh there's plenty of there's
plenty of them it's just that the
traditional I think the traditional view
is that these things have to be kind of
low agency things like like you know
rules about triangles and things like
that but I I think it's it's the
properties that defi the the the the
surprising non-physical properties that
we encounter when we make machines when
we encounter them all the time I think
are of the same kind generally as the
surprising cognitive properties that we
find when we make certain other kinds of
systems so from that perspective right
from that perspective one could ask
whether whether what you're really doing
is uh providing or or or refusing to
provide uh physical experience for
certain potential Minds let's let's put
it that way yes and right and what and
what the ethics of that is right so so
so my point is also that if right if you
suddenly decided you know what that's it
no more no more life no more that you
know that that Alpha goes I'm not sure
that again I I think we're back to this
idea that I don't think you can take
your hand off the off the lever I think
if you did that that would be a definite
choice in in in refusing to provide uh
an embodiment which you know I don't
know what's the what's the purpose right
is there a purpose to the embodiment
like why are we here and and suffering I
mean I I'm not going to try to answer
that but but if there is then refusing
to provide any kind of vehicle is also
an issue yeah I would term your point
the potentiality argument against
anti-natalism so if one really becomes
radical about this and I have learned
that I don't want to talk about it in
the public but if people there are
different philosophical versions of
antinatalism but if people say we
shouldn't procreate yeah and end this
line of embodiment uh
rically one of course makes a decision
about a large space of possibilities you
know uh one loses future
potential and it's almost C that one
doesn't
know how large that space of possibility
is that one shuts off uh by not uh
procreating that could be a type of
argument that it is unethical to how how
I say to preclude a very large number of
possible
embodiments without even knowing how
large that possibility is uh I don't
know uh I don't have a position on this
this is too difficult for me right but
of course but don't you think um
um maybe as a last question don't you
think that there is in this whole
development in
evolution in
science in the evolution of culture and
the evolution of self-deception and all
of these process that there is a force
in it
that we probably couldn't even resist if
we wanted to uh in any way
did does this have a momentum of its
own yeah it it probably does again these
are these are deep deep Waters uh where
we don't know anything but it it
probably does for whatever it's worth my
personal feeling on this is that we we
are we are uh flound wanding around at
at the early stage of of of our
development uh kind of like uh kind of
like a baby you know that that
everything is uh you know you feel wet
you feel hungry you don't know what's
going on we we are we are at that early
stage where uh I I I I think I think
that I'm I'm optimistic about the path
forward I I think there are some
fundamental problems I I don't think we
can get through it without a lot of the
suffering that we have now but uh I
think ultimately the force that you're
talking about the the the the momentum
is to push through that towards more
intelligence in the universe not less
more more better more more joy more
Discovery more um you know uh uh
experience of Life as a positive thing I
I think that's that's that's the future
that that we should we should aim for
I'm definitely more on the pessimistic
side and on the I think it's more
important to reduce suffering than to
increase uh Joy
um but given this
um funding resources for
research um are
limited how would we best um allocate
resources where do you think would we
get the biggest bang for the buck in all
of this yeah I I I think that uh the the
biggest area that stands to contribute
to all this is the field of diverse
intelligence research all right right
and and uh really really trying to break
down I mean I think we as as humans and
also we have being a product of of
certain Educational Systems we have a
lot of um built-in firmware that makes
it hard for us to recognize Minds that
are not like ours we are very people
there is so much certainty out there
people say well this this thing can't
that and this doesn't you know and they
don't just just just the idea that first
of all to to to make the make people
understand that first of all you don't
know and second of all there are efforts
that that are out there that can help us
to figure this out and that you can't
just sit back and have assumptions about
what anything is we have to have a
mature science of it and we have to
understand you know part of part of
making ethical choices is understanding
what you're choosing between and sure
sure sure right and I think a huge
number of people are making decisions in
the space and they have no idea what
they're talking about and and you know
and and they're not informed in the way
that you you need to be informed to even
begin to make make some kind of choice
so so I I think putting this into
diverse intelligence uh research and um
is is is I I think that's the best
that's the best shot we have right now
yeah you
probably get the idea I would have I
would like to see really good work on a
theory of suffering under the condition
of achieving epistemic progress without
creating more suffering in the process
or having to do a lot of very ugly thing
you know yeah yeah I and and I I don't
disagree with that at all and I think
that is a that is a fabulous um sector
of the diverse intelligence research
that should get much more much more
attention I for sure a audience is
wishing the thanks for a brilliant
conversation to today it's organic and
it's very refreshing to learn from both
of you today yeah thank you thank you so
much I appreciate it and thanks Thomas
we'll we'll be in touch thank you for
joining us have a good day byebye bye
bye
he

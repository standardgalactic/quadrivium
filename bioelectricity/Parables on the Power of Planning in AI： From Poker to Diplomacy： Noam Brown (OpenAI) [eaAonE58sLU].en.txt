okay hi everyone um welcome to uh this
talk so we are super delighted to be
able to welcome Nome Brown from open AI
uh here with us today he's been
responsible for multiple of the biggest
developments in AI game playing over the
past few years um he created the first
AI systems that could beat the top human
players in poker um which was recognized
by science as one of the the top
scientific breakthroughs in 2019 and for
this work he was also awarded uh the
Marvin Minsky medal uh for outstanding
achievements in Ai and named as one of
the M MIT tech reviews 35 innovators
under 35 more recently he's also
developed Cicero the first AI system to
achieve human level performance in the
natural language strategy game of
diplomacy so super excited to hear about
what you have to share with us today
okay thank you oh I think I'm I'm likeed
up yeah okay hi everybody uh my name is
Nom uh I want to start off um by giving
some motivation uh and talking about
when I started working in AI as a grad
student um so I started grad school in
2012 and I was working on AI for poker
basically from the start and around the
time I started people had already been
working on AI for poker for several
years and at the time it was felt like
the system had been fix figured out and
it was really just a matter of scaling
the models further um I have this plot
here showing you know each year and
roughly the number of parameters are
proportional to the number of parameters
that were in that was in the model each
year that we we trained um so we would
every year we would train this model all
the other research Labs would also train
models that played Ai and we would all
get together every year and play them
against each other in this thing called
the annual computer poker
competition um and yeah every year uh we
use roughly the same algorithm but there
was a lot of challenges in scaling it uh
so you know uh we had to make make it
more distributed train it on uh for
longer on more uh comput and um and
every year the models would get better
and every year they would beat the
previous year's models uh and in
2014 my adviser and I developed uh an AI
that that beat all the other bots in the
annual computer poker competition we got
first place and we decided to try to
play it against actual human
experts and so this uh led to the 2015
brains versus AI poker competition uh
where we challenged four top Poker
professionals to an 80,000 hand poker
competition we had $120,000 in prize
money to incentivize them to play their
best and ultimately our bought clao lost
by a pretty sizable margin so the the
amount was 9.1 big blinds per 100 if
you're not familiar with poker that
doesn't really mean much uh but
basically it was a substantial loss it
wasn't like a crushing defeat but it was
a pretty substantial
loss and during this competition um I
noticed something
interesting you see our bot had trained
on roughly one trillion hands of poker
uh leading up to this competition it had
been playing poker non-stop on thousands
of CPUs for months leading up to this
competition um but when it came time to
actually play against these experts it
acted almost instantly uh it would take
about 30 milliseconds or or you know so
whatever whatever it took to just like
look up uh the the policy for whatever
state it was in and and just act
instantly um the humans meanwhile if
they were in a very difficult spot they
would sit there and they would think and
they would sometimes think for you know
5 Seconds they would sometimes think for
five minutes um but if if it was a
difficult situation they would take the
time to think through what to
do and it occurred to me that maybe this
is what is missing from our bot so after
the competition I decided to look into
this direction and see how much of a
difference does this actually make if we
add this
capability um and and this is what I
found um this uh data is from a paper
that I I published in 2017 with my
adviser
um so the blue line is if we don't do
any search or planning we just do uh
what we have been doing for years at
that point just training bigger and
bigger models um but having them act
instantly when it came time to actually
play um the x-axis here is the number of
buckets this is before neural Nets uh
were really taking off and so we used uh
c means clustering but you know
basically it's the number of parameters
in our model and um well it's it's the
number of parameters in our model times
times some constant factor that we won't
go into but basically roughly
proportional to the number of parameters
in our model and the y- axis is distance
from Nash equilibrium so the lower this
number is the better you are at poker um
I I'm not going to go into a lot of
details about this because like I don't
want to make this a poker talk but the
lower that number is the the better
you're going to do when it comes when it
comes time to actually play poker um and
you can see that indeed as we scale up
the model um it was getting better at
poker the distance from Nash equilibrium
dropped from around 800 to around 400 um
as we scaled up the model
100x the orange line is the same model
if we add search if if we add the
ability of the model to think for about
30 seconds before acting in the final
round of the Game of Poker and you can
see even for the same siiz model you're
getting about a 7x reduction in distance
from
Nash to me this was staggering um I had
been working on AI for poker in grad
school for about three years three or
four years at this point and over those
three or four years I had managed to
scale up the models by about 100 fold
and I was really proud of that that was
you know I thought really impressive
work and here um if you know okay if you
were to extend this blue line out and
see how far would you have to extend it
to match the performance that you're
getting by adding search the answer
would be
100,000x and so just by adding search uh
we were able to do the equivalent of
scaling the models by
100,000x um when it's took me 3 years to
scale it by
100x so at this point I realized
everything I had done in my PhD up until
this point was going to be a footnote
compared to just adding
search um based on this result we
changed our approach completely um the
focus became how do we scale up search
and um in
2017 we did another competition with
four top professional poker
players this time we played 12 ,000
hands of poker with $200,000 in prize
money to incentivize the players to play
their best and the bot ultimately won by
15 big blinds per 100 whereas previously
the bot the previous bot in 2015 had
lost by nine big BLS per 100 and each
human lost individually to the
bot this was a very surprising result to
everybody um it was a huge surprise to
the poker Community it was a huge
surprise to the AI Community it was even
a huge surprise to us we were not
expecting to win by this kind of margin
um and in fact I think what really
illustrates this is that when we
announced the competition uh there
emerged not surprisingly among the poker
Community a very active betting Market
on who would win and the odds against us
were about four to one and even after
the first three days of the competition
when we had won for three straight days
it was still a 50/50 uh in the betting
markets on who would win and but but but
by around the eth day um it became very
clear that we were going to win and the
only betting Market that remained was
which human would lose the least by the
end of the
competition we followed this up in 2019
I I continued in this direction of of
how do you do search uh and and planning
in the domain of Poker um and in 2019 we
made a six player poker bot the previous
bot was just two-player poker uh now we
have six player no limit Texas Holding
poker uh we played against um uh in six
player poker with five humans at the
table and and one bot 10,000 hands over
12 days we Ed variance reduction
techniques to reduce the luck Factor um
and again the bot one with statistical
significance um what's really
interesting about this bot is that it
cost under $150 the equivalent of under
$150 to train um if you were to run if
you were to train it on cloud computing
resources um and it runs on 28 CPU cores
at test time now the fact that this
costs less than $150 to train means that
we could have done this back in the 90s
even if we had taken the right
approach
um okay so this was a big motivating
factor for a lot of my future research
in in AI um this focus on planning um
and I think looking back it you know
it's really easy now to say like well
you why didn't people work on planning
in uh poker for I mean clearly it was
like such a huge win um and it's I think
it's really hard to put yourself in the
mindset of um you know everything's
obvious in retrospect but at the time it
was it was not obvious that that this
would uh work so well and so I think
there were a few reasons why um search
and planning was not as active a
research area in AI for poker and and I
think it has a lot of lessons for how uh
people approach research in general and
today so okay why wasn't search
prioritized in a AF bker before and the
first thing I should say is that people
were working on search in poker
um it just wasn't given the same
emphasis that it I think it deserved um
and it wasn't like considered the
highest priority um and by the way when
I say search I really I I use it
interchangeably with planning I consider
search a kind of planning um but
basically the idea is that the the model
is taking a longer time to act um than
its pre-computed policy would would uh
would do okay so why wasn't it uh valued
highly before I think there one thing
was cultural factors um researchers
especially in Game Theory wanted to
compute an equilibrium and if you want
to you have to verify that you have an
equilibrium and so you you want to have
the entire solution upfront and if
you're doing planning then that means
that you don't have the entire solution
UPF front and so you don't know you
can't compute exactly how far away you
are from equilibrium um and so this this
was like kind of a factor that the
community came out of the the community
working on AI for poker came out of Game
Theory and and so they had this bias
against um anything that wasn't
Computing the entire
policy um another is just the difficulty
of doing experiments with planning so as
you scale up the amount of compute
that's used at test time it makes it
very difficult to um measure how well
you're doing uh it makes experiments
much more difficult especially in a game
like poker where you have such high
variance um and you have to play maybe a
million hands of poker between two bots
in order to tell which one is better um
having the model take like 30 seconds
per hands it it just makes things very
difficult another was incentives um and
I I think this is especially true for
academic researchers today to pay
attention to um people are always
thinking about like I mentioned there
was this thing called the annual
computer poker competition and everybody
was always focused on winning the annual
computer poker competition comption this
kind of short-term objective um and the
annual computer poker competition
limited the amount of test time compute
that you could use to to CPU course and
the reason for this is just the funding
that the the organization had it was
very expensive to run you know these
Bots for millions of hands against each
other if they were taking so many
resources at test time um and so people
were focused very much on this this
short-term goal of winning the annual
computer poker competition at the
expense of developing techniques that
might work
in the more important Benchmark of
beating top
humans and and finally I think the most
important factor is that people simply
underestimated the difference that it
would make if you had asked me before I
did research on this direction how much
of a difference would it make if you
added you know if you figured out how to
do planning and added it to the poker
Bots I would say yeah maybe it's the
equivalent of scaling up the models by
10x which is a lot I would not have
thought it would be
100,000x and um and and I think I think
that's a general Trend that I've seen
that people underestimate just how much
of a difference planning makes uh for
these
models okay um before I go on to to
other games is any any questions about
poker uh and planning yep could you say
a little bit about what search is in the
poker
process um I can't I don't want to spend
too much time on it because I think one
of the one of the points that I want to
make is that
if you look at how search is being done
at all these different games it's very
specific to the game and um when you go
to a different domain it's not
necessarily useful for those other
domains um but I think there is a
general Trend here that figuring out how
to do search um ends up being very
useful um and if we can develop a very
general way of doing search then that
has the potential for being very very
useful um to answer your question for
for poker um we would do these equili
computations that are these iterative
algorithms so it starts by playing
randomly and then it gradually converges
towards an equilibrium so in rock paper
scissors for example this would be
randomizing equally between rock paper
and scissors so that you're guaranteed
to not lose an expectation um obviously
in a more complicated game like poker
the strategy is much more complicated um
and so even if you're you know playing
what would appear to be a defensive
strategy where you're guaranteeing you
could not lose you actually end up
winning in practice because the other
player makes mistakes um now previously
all this equilibrium computation was
being done off uh ahead of time and what
we developed was a technique that would
allow us to better approximate the
equilibrium by doing this iterative
process more but only for a piece of the
game so kind of like in chests when
you're at a particular board State you
only have to search through the
possibilities that could come after that
board State we developed a technique in
poker where you could compute an
equilibrium for the possible states that
could come after the situation that
you're in and not worry about the other
states that came before or that are are
not possible to reach at the given point
that you're
in Jamie so forgive me I know nothing
about poker I managed to make my way out
of not knowing anything about poker but
uh were
you what you just suggested um was there
a time uh space tradeoff Also right
because like Computing an equilibrium
and like storing it in a lookup table is
presumably gargantuan right whereas like
doing search conditioned on like you
know this many steps in a game like you
could imagine you're searching a much
smaller space rather than storing the
entire thing is there some trade-off
that you see there also in terms of like
you know the the amount of total compute
and the amount of things you have to
store or yeah that's a good point and
this was this was certainly a a factor
for uh you know when when we were doing
research on poker I remember like we'd
spent a lot of time figuring out how do
we compress the representation of the
strategies that you know we can store in
as little space as possible uh we'd have
to use like all of clever tricks to like
you know for most of the game we
actually just had like a binary
representation of like do you raise or
do you fold and not worry about the
probabilities so much um that I wouldn't
say that was the bottleneck um for for
performance but but it certainly was a
factor and one of the nice things about
doing search and planning is that like
in poker we no longer had to have a fine
grain representation of the strategy at
the later rounds y you have a question
um so obviously there's a lot of
different situations in poker like you
can have different stack sizes you can
have different bet sizes how constrained
was this sort of model like how much
could it generalize outside of like to
like say 200 or 500 big blind uh for
these for for the two player and the six
player bot that I just described it was
always playing uh either 200 big blinds
um fix Stacks or 100 big blinds fixed
Stacks uh we developed later techniques
that would be able to play arbitrary
stack sizes
yeah yep U about test time comput so the
if the model is larger I assume it will
take correspondingly longer at test time
as well so if you have a 100x model 100x
larger model does it take 100x longer
for a test time for the models that we
were using no um but this was because we
weren't using um neural Nets so a lot of
the computation was kind of like
independent of the model size I was
wondering if like given a certain test
time budget if you should just train a
much much much larger model that uses up
all of that time
for inference at test time versus a
smaller model that does search and
planning and uses the same amount of
test time uh that's a good question so I
think the issue is that at a certain
point becomes like so I I think at some
point it's probably worth it to do
planning a test time rather than just
training a bigger model uh because you
can imagine that like you know training
a bigger model you have to keep like 10
Xing you have to do some kind of
multiple of the pre-training and so at
some point it becomes cheaper if you
imagine that you're only going to like
do do um an inference once like a call
once to let's say like play against a
world champion and go or something you
don't it's worth it to shift a lot of
that computation to like it's worth it
to 100x the computation at test time
rather than 100x the the
pre-training was your model optimized to
be game period optimal or did you ever
go into the exploitation part the these
poker Bots were optimized to be game the
optimal so we're trying to uh computed
equilibrium and the idea was that and
what what we observed empirically was
that by Computing an equilibrium and
playing that we ended up uh doing very
well against expert humans and beating
them um even though we were not actively
trying to exploit them um we never like
I think a lot of people were interested
in this question of how do you exploit
expert players um we ended up developing
techniques that could effectively do
this in diplomacy by modeling the the
other humans that were playing and I'll
get to that a little bit later um we
never actually apply this to poker but
if you're looking for an interesting
search Direction I think applying these
techniques to poker is something that as
far as I can tell nobody's doing um and
is definitely doable I think at this
point okay I want to move on to go next
uh okay
so this idea of doing search and
planning is not unique to Poker and in
fact uh it's been widely used in in
games like chess and go and I think if
you look at the performance in games
like go you see a similar pattern where
I think most people underestimate just
how much of a difference it makes so um
here is a chart from the alphao zero
paper so alphao is very famously this AI
that that beat uh lisod doll in go uh in
2016 alphao zero is a follow up that is
even better and um was trained with very
with very little human knowledge um so
this so you can see uh on the y- axis
here we have El rating which is a way of
measuring the performance of of humans
and different models and on the x axis
we have different versions of alphago so
you can see alphao Le is the version
that played against ladol and it's just
over the line of superum performance
around 3600 ELO um Alpha go zero is the
stronger version which has ELO rating
about 5200 so clearly
superhuman um but Alpha Gozo is not just
a raw neuronet um it's uh A system that
uses both a neural net and an algorithm
called multicol tree search and if you
look at just the raw neural net of alpha
go0 the performance is only around 3,000
it's below top Human
Performance so I want to really
emphasize this point that if you look at
just the raw neural net of alpha go zero
even the the the even though it was
trained with Mod research if you just
run it at test time without modol
research the performance is below top
top humans and in fact this was you know
alphago was in 2016 16 it's now 2024 8
years later and still nobody has trained
a raw neuronet that is superhuman and
go now I I noticed that when I when I
mentioned this to people they would say
like well surely you could just train a
bigger model um that would eventually be
superhuman in go and and the answer is
like in in principle yes but how much
bigger would that model have to be to uh
to match the performance of alpha go
Zer well there's a rule thumb um that
increasing ELO by about 120 points
requires either 2x in the model size in
training or 2 Xing the amount of test
time compute that you use so if you
follow this rule of thumb if the raw
policy net is 3,000 ELO then to get to
5200 ELO you would need to scale the
amount uh the model size and training by
100,000x now I want to cave out this
there's a big as risk here which is that
I don't actually Alpha go Z is 5200 ELO
um I think they measure that by playing
against earlier versions uh earlier
versions of the same bot and so there's
a bias where it would do better against
those because it's like train through
selfplay so it's trained against those
Bots um so I think the number is
probably more like 1,000 to 10,000 um
but in either case you would have to
scale the model by a huge amount and the
training by a huge amount to match the
performance that you're getting by using
mol research where the bot thinks for
like 30 seconds before acting and by the
way this is still assuming that you're
using mtic research during training
during self-play so if you were to take
away the mtic research from the training
process this number would be
astronomical um okay
so when I noticed this pattern I I
started just like whenever there was a
domain that I was looking at um I I
noticed that a lot of people would think
about how do you do deepl model free
deepl which was the cool thing to do in
2019 cool now but still pretty cool um
and instead I would look at at at the
Domain and see how can we apply planning
and I think kabi is a really good uh
another example of this and by the way
this whole talk is just going to be me
going through different domains and
talking about how planning has been like
really really effective um so Hanabi is
this fully Cooperative imperfect
information game and and after um AI
became superum in poker and and go
people were looking for other domains
that would be these new grand challenges
and in 2019 February 2019 deep mine
proposed
Hanabi as as this new Benchmark um and
they presented a new RL algorithm deep
RL algorithm that achieved a 58.6% win
rate in Hanabi which was very good but
not uh
superhuman uh less than like six months
six six months later uh my my teammates
and I at Fair uh presented a new
algorithm um that achieved a 75% win
rate in two player Hanabi uh which is
superhuman performance in this domain um
and we did this using a surprisingly
simple technique uh which I'll get to
you in a second um and the key
breakthrough was again we didn't do
anything novel when it came to the RL we
just did
search uh and in particular we did like
basically the simplest form of search
you could imagine um basically in Hanabi
you know there's there's uncertainty
around like what state you're in so you
have this belief distribution over what
states you could be in and there's like
20 different actions that you could take
in the game and so we would just say
okay for each of these different states
that we could be in we you know there's
either like play card one discard card
one hint red there's these different
actions you can take let's just do a
bunch of different rollouts to see uh if
we were to take that action and then
play according to our policy for the
rest of the game what would our expected
value be and we do like a thousand
rollouts we get a good expect a good
estimate of what our expected values for
each of those actions and then we just
pick the one that has the highest
expected value
um here is what the performance is if
you just look okay let's let's set
search aside for a second and look at
the performance of like different um you
know pre-trained bots so this is
smartbot It's a heuristic handcrafted
bot and it wins about 25 uh maybe 28% of
games um this is the algorithm
introduced in the Deep mine paper and it
gets it's a uh de deep uh Q networks uh
bot and it got about 45% win rate and
this is there's like a whole sequence of
papers on uh de algorithms for Hanabi
that would get increasing performance
this was at the time of our publication
the latest one which would score about
uh
58% um this is what you would get by
adding this search algorithm to the
different bots so if you take this
handcrafted heuristic bot that was only
getting 28% and then added the simplest
search imaginable where you just like
you know do a bunch of rollouts for all
the different actions you you could take
and then pick the one that had the
highest expected value that would boost
your performance to nearly 60% which was
beating all the previous deepl Bots just
out of the box this was using like a
single CPU core at test time uh for like
a second and the beautiful thing was
that you could actually add this on top
of all the other deepl bots so if you
added it to like the latest and greatest
bot uh DL bot you would boost the
performance even further to uh around
72% and then if you did this this was
only if you did search for a single
player so if you did it for both players
that's the green bars and you can see
the performance went up even more um now
I should also point out the the point of
the the upper Bound for this game is not
100% because there are some ver there
are some like deal outs that you just
cannot win so really the top performance
is possible is like I think uh maybe 90%
um and so you can see like we're quickly
saturating um performance in this domain
now when my teammates and I at Fair um
got this result my my teammate literally
thought it was a bug because it was just
unimaginable that you do this like
simple thing and their performance jumps
up from like 28% to state-of-the-art
58% um and again this I think is is a is
a trend that I've seen where where
people really underestimate the value of
planning until they see it in the domain
that they're working
on okay any questions about this or
go so uh we're not the only ones to to
notice this pattern and in fact there's
this really great
paper um that was published in 2021 it
just went on archive um Andy Jones's uh
now at anthropic and he did a bunch of
scaling laws for the game of hex
basically using Alpha zero like
techniques and he had this plot that I
think was really fascinating um so on
the x-axis here we have the amount of
compute that goes into training a uh a
hex bot hex is this like board game
that's kind of like a simple version of
go um and this is the amount of test
time compute that goes into the bot um
using the mol research algorithm and you
can see there's like these isocurves of
different ELO performance so if you want
to get, 1500 ELO in this game you could
either spend a lot of compute at test
time or you can spend a lot of compute
at training
time um or or some
combination and he found that for
basically there's a trade-off between
10x of training compute uh is equivalent
to 15x of test time compute so what this
means is that by increasing the amount
of test time compute in this uh domain
by 15x you're getting the equivalent of
increasing the training compute by 10x
now why does this matter well if you're
training A system that costs let's say
like a billion dollars and you wanted to
improve the performance further um you
could either 10x that and go from you
know a billion dollar to $10 billion or
you could increase the inference cost
from you know what I like a penny to 15
pennies now for certain do for certain
domains you would prefer to increase the
inference cost by 15x over increasing
the pre-training cost by
10x okay
um now we've also worked on the domain
of Chess and I think here this this is
very interesting because um this isn't
so much about getting top performance
but it's about imitating
humans so there was this paper Maya that
came out in 2020 and what they were
trying to do is instead of just making
like state-of-the-art chest Bots they
were trying to make very human-like
chest Bots and so trained these chess
bots on hundreds of millions of chess
games from
humans and um they created different
models to uh match the ELO ratings of
different of different kinds of like
different ELO
ratings um and this was at the time
state-ofthe-art for predicting human
moves in
chess now one thing that's really
interesting about Maya is that for High
ELO models it was about 100 to 300
points uh ELO points below the target
ELO rating so if you were to train it on
2,000 ELO rated humans it would only be
about 1,700 ELO for the lower ELO
ratings this was ended up not being a
problem for the high higher ELO ratings
it was it was a
challenge now one hypothesis for why
this is the case is that approximating
human planning is hard for neural Nets
that when when you're at 2000 ELO your
planning process is so sophisticated
that it's actually quite difficult to
distill that down into into a neural net
and I think one piece of evidence for
this is that there is one version of
Chess where there was no ELO gap for the
Maya model and that was bullet chess
where humans have very very little time
to plan ahead and they just have to
basically act on
Instinct um now what we found we we
wrote a follow-up paper that um was
published at icml 2022 and in this paper
we added plan
um on top of these supervised models um
and so here's the performance that we
get basically we take you know we we
pre-train on a bunch of human games and
then we add multicol research on top
with different hyper parameters for the
mol research to regularize it towards
the human
policy so um this blue dot is if you
just do imitation learning on human data
and um the x-axis is accuracy of
predicting human moves in different in
different game States and the y- axis is
the win rate versus The Raw versus this
blue model so like you know obviously
it's a 50% win rate against itself and
then you can see the human prediction
accuracy it predicts about
53.2% of human moves in a in a test data
set um and I should say this this is for
highly rated
humans if you add multicol
research um you see that the prediction
accuracy goes up and the win rate goes
up and it this is um again for different
levels of regularization so this is for
if you regularize very closely to the
human policy uh and this is like as you
decrease the regularization it moves
further away the prediction accuracy
starts to go up and then then eventually
it it goes down quite a bit um but the
performance keeps going up so this kind
of makes sense that like you know uh as
you completely ignore regularization
towards the human policy you have much
more flexibility to improve your policy
a lot but the prediction accuracy goes
down but what's really interesting is
that the prediction accuracy does
initially go up by a pretty substantial
amount so roughly speaking one
percentage Point increase in the
prediction accuracy on this test set
would correspond to a roughly 4X larger
neural net uh and 4x more training data
and here we're increasing in both uh
chess and go by about 1.5 percentage
points um maybe one percentage point and
go and what's interesting is that the
timable hyper parameter was the same for
both um it's actually it looks looks
like it's the same plot in in both
figures but it's actually different
um so I I think this is really
surprising because you know we're we're
kind of taught that if you want to
maximize prediction accuracy on a you
know on some kind of data set the way to
do it is to just like take a giant model
feed that data in train a giant neural
net and then and then see what it
outputs and here uh that's not the case
if you add modol research on top with
the appropriate hyper parameter then
you're increasing the prediction
accuracy by a substantial
amount and again I think that's because
you're adding this planning ability that
the neural net is insufficient uh is not
good enough to to approximate that
humans are
doing okay any
questions yep I'm curious if you see
similar gains using sech in a space
that's not in a game where there's maybe
not a super defined optimal
um yes I'll get to that a little bit
later you think the same like scaling
law of like test time and
training uh will hold in like continuous
domains where the states are not
discret um I think it's possible um I
think I think it depends on the domain
and and the technique um but it's
possible
yeah okay so this idea of uh using
planning to improve your ability to
predicts human moves um this is again a
followup to the to the myor so on the
x-axis we have different ELO uh ratings
and these are like uh levels uh you know
human human ratings uh ELO ratings and
on on the x-axis we have the prediction
accuracy for predicting humans in those
ELO ratings uh in that ELO bucket and
you can see let's just focus on on this
blue line here so the
circles is the raw Maya model and the
squares are uh the Maya model plus MCTS
with this like tuned hyper
parameter and you can see if you use
Maya to try to predict let's say like it
was trained on 1900 ELO humans and then
we try to predict the moves of 1900 ELO
humans the prediction accuracy is only
around 54% if we then add MCTS the
prediction accuracy goes up by two
percentage
points okay so we Ed this idea of um
trying to predict human moves better by
by adding planning to our AI for the
game of diplomacy so Cicero was an AI
that we that we developed at Fair um
that plays the natural language strategy
game of diplomacy and it was trained on
50,000 human games acquired through a
partnership with web diplomacy. net and
the performance um was quite strong I
won't go through all the details of of
how this row um because again this isn't
a talk on on CIS specifically but
diplomacy is this very complicated uh
natural language strategy game
um we entered Cicero anonymously I'll
get to the results and I'll talk a
little bit about about examples of it
but we entered it anonymously in an
online diplomacy league and so first of
all kind of separate from the planning
stuff it was actually quite interesting
that Cicero was not detected as an AI
agent even though it played 40 games of
diplomacy uh sending and receiving an
average of 292 messages per game and
honestly people were kind of shocked
afterwards when we told the humans that
like oh yeah you have been playing with
a bot the whole time and we got a lot of
messages like this um fortunately people
were overall quite um quite okay with
the fact that they had been like been
playing with a bot the whole time we
were pretty worried about that but but
they seem to take it uh quite
well um as far as the performance cisero
ended up placing in the top 10% of
players and second of 19 players that
played five or more games and it more
than doubled the average human
score uh also there was one player that
mentioned postgame that you know they
thought that like you know they kind of
joked that ciso like the the screen name
that we Ed for ciso was like you know a
bot but they also accused a few other
players of being bots so I don't know if
that was specific to Cicero or maybe
there were just other other labs working
on on diplomacy that also submitted
agents um okay so the way ciso works um
it takes his input the board State and
the dialogue history at that um uh and
it feeds that into uh a dialogue
conditional action model so this is a
model that again takes his input the
dialogue and the board State and tries
to predict what all the players will do
um in on the current turn these actions
are then fed into this planning engine
and again I think this is one of the
novel things about Cicero that you don't
really see in a lot of language models
today that um it would iterate on what
it was predicting all the players would
do and what they thought we would do um
and it would refine these predictions by
by doing several lubs kind of like in a
way similar to what we were doing in
poker and then that would lead to the
action that we ended up playing but it
would also lead to um these intents
these actions that we would condition
our dialogue model on so after we
planned to figure out what moves we
should do for this turn and what we
thought other players would do for this
turn we would feed those plans into the
dialogue model condition our Dialogue on
those plans and then output a message
according
and again I think this is one of the
underappreciated things about about
Cicero that it wasn't like a typical
language model where it was acting
instantly um it was actually really
expensive to run we were using um you
know like dozens of gpus and it would
take uh you know sometimes like at least
10 seconds for each message that it
decided to send um but that time spent
planning ended up making a huge
difference for the performance of the
bot okay so there's a general Trend here
um that for some problem like okay why
why does planning work in in so many
domains and and I think one of the
fundamental reasons is that there's this
thing that you know you might call the
generator verifier Gap basically in some
problems verifying a good solution is
much easier than generating one so for
example in poker verifying that you're
in an equilibrium is much easier than
Computing one in chess verifying that
you're at a really good board state is
much easier than finding the path to
that very good board State and you know
in something like Sudoku it's much
easier to verify that you have a
solution to a Sudoku puzzle than it is
to find that
solution and this is true for domains
like you know puzzles math programming
you know proofs for example it's much
easier to verify a proof than it is to
uh generate a proof um there are some
domains where this isn't true uh so for
example information retrieval um if I
ask you what is the capital of Bhutan
you know I could give you 10 different
options and I don't think that would
actually help you in um tell me what
what the correct answer is at least most
of you um and also things like image
recognition these are domains where
verifying isn't much isn't isn't much
easier than generating and in these
domains we would expect planning to not
make a very big
difference but when there is a generator
verifier Gap and we have a very good
verifier then we can spend send more
compute on the generation process and
then verify um when we've come up with a
good
solution okay so I've talked before I go
to llms do you have any questions so far
because we got we got to talk about LMS
of course right yeah did you say this is
like another way to think about it is
like the efficacy of forward search
versus backward search from your goal or
is that distinct um I'm not sure what
you mean by that so like you have a goal
in mind and you're just searching
backwards from where you where from the
goal to your current position rather
than from an initial position to the
goal there are forms of search that do
both um and I think
that's I don't think what I'm describing
is assuming one or the other I think you
could do you could do either one
depending on the domain
yeah okay so we're going to talk about
LMS u i I should mention of course that
like you know I'm only able to talk
about published research and so I'll
keep this um all these discussions to to
things that are are published um so what
what does scaling test time compute look
like in language models um there are
instances of it and again I think people
underestimate just how much of a
difference those techniques make um so
the simplest one is this algorithm
called consensus and the basic idea is
instead of just generating one solution
you generate a bunch of solutions and
you take the one that's the most
common um there was this paper manura so
so some of you might be familiar with
What's called the math benchmark so this
is a benchmark it's literally called
math and it involves many difficult math
problems that are basically like high
school college level um people thought
it would take a very long time to uh
reach even 50% performance on this
Benchmark but um in 2022 uh Google
published this paper this algor this bot
called manura and it got over 50% on the
math benchmark this was very surprising
to the people in the llm community
because like math was viewed as one of
these very difficult tasks that llms
were really bad at um they did a lot of
things to get the performance to 50% but
one of the things they used was
consensus
so they generated a lot of solutions and
they just returned the most common one
and that actually boost the performance
of manura from
33.6% to
50.3% by generating a thousand
samples now there are limitations to
consensus so for example if you you know
it's really easy to do cons it's you can
do consensus when you have like you know
a single number that you have to return
as your answer it's much harder to do
consensus when like you know you have to
write a proof for example because like
you're not going to generate the same
proof multiple
times um but there are other things you
can do another one is best of
end and the idea here is that you have a
reward model that can score how good a
generation is and instead of you know
returning the first one you generate n
Solutions and then you return the one
that the reward model considers to be
the best now with a good enough reward
model best event beats
consensus um but it is uh in many cases
bottlenecked by the quality of the
reward model um and in fact if your
reward model is not very good then you
will see this overfitting Dynamic which
which you kind of see here where um on
the x- axis we have like the number of
solutions that you're generating and on
the y- axxis we have the test Time
Performance the the solution uh the
score and you can see like initially as
you generate more and more solutions and
take the best one according to the
reward model it goes up but then
eventually it overfits and it goes down
so if you don't have a very good reward
model uh this you know can be worse than
consensus um it is very useful in a
domain where you have ground truth um
like chess is a good example where you
know you've either won or you've lost or
Sudoku where again like you know that
youve solved it or you haven't solved
it um can we do even better than these
well there there was a paper that
actually open AI uh published um very
recently it went online about a year ago
but it was uh it was officially
presented at ICL just a few weeks ago
this is called let's verify step by step
and they introduced this idea of process
reward models and the idea is that
instead of verifying just the final
solution they're you're going to verify
every single step
individually um so yeah they go through
each of the steps say is it correct is
it incorrect If it's incorrect then they
just like you know Mark that down as
incorrect for the whole
solution um okay so how does this
compare
well okay so in this plot we have the
number of solutions generated um going
up to a TH or 2,000 and this is uh the
success rate on the math benchmark
this gray line is consensus also called
majority voting and you can see it like
goes up initially but eventually it
flattens out but you can see it actually
gives you a pretty big boost so you're
going from like you know roughly 60% to
uh almost 70% just by doing um
consensus if you do best of n you're
getting an even bigger boost and you're
getting up to um
72.4% if you do this process reward
models where you're verifying every
single step with a really good reward
model you're getting an even bigger
boost you're getting up to 78.2% and you
can see it still looks like that that
number that line is would go up more if
you generated more
samples um there's some examples of this
I I think this is a really interesting
one in particular because this is a math
problem you know it's asking you to
simplify tan of 100° plus 4 * s of 100°
um this if you give it to the raw gbd4
bottle it would only get it right one in
a thousand times but by going through
the process of verifying every single
step um you could generate a thousand
samples and then only take the one that
the reward model says is correct and
your success rate would go up a ton on
this
problem okay um I want to take a step
back and talk about more broad uh the
the broader picture about AI today so I
mentioned at the start of this talk when
I when I started grad school working on
AF for poker people felt like they had
figured out the method for getting to
Super humid performance in poker there
is this algorithm that we had and it
worked and it was just a matter of
scaling it up and every year the models
would get bigger transfer longer on more
data and they would always beat the
previous year's
models and that's very similar to I
think the state of AI today we have a
technique that works and we can train it
on more data for longer with bigger
models and it keeps getting
better and um and there are some that
say that that's all we need to do
um but one thing that I want to point
out is that the inference costs for
these models are still quite low you
know you can go on Chach BT right now
ask a question you get a response
basically instantly at very very low
cost and it doesn't have to be that way
um it wasn't that way for a lot of these
games that we looked at it might not it
might not have to be the case for
language models as well
so um so the next goal uh and the thing
that I'm working on uh is generality can
we develop truly General ways of scaling
INF compute
um now this is a very difficult problem
um you know we've developed a lot of
these techniques these search techniques
that scale inference compute but they've
always been very specific to The Domain
that we've been looking at uh and and
what what I am working on in and others
are working on is developing very
general ways of doing it uh this one
involve spending much higher compute at
test time but it would in mean much more
capable models and I think for many
domains we're willing to make that
trade-off um yes it's true that for
chubbt where you you know want a
co-pilot for your coding tasks maybe you
you don't want to spend five minutes
waiting for a response um but there are
many questions where we're willing to
wait uh hours days even weeks for an
answer you can imagine that we would be
willing to spend that kind of cost for
something like a proof of the reman
hypothesis or for a new life-saving drug
or for the next Harry Potter
novel and I think very importantly it
can give us a sense of where things are
going as we scale these models further
it can give us essentially a window into
the future of what the state of AI will
look like um down the road as these
models become even more
capable okay so um general advice for
academics because I know that a lot of
academics uh grad students in particular
are wondering um how do you how to do
research in this new age of AI um and
one thing I want to suggest is that
planning is actually a relatively good
domain for academic research um and I
say relatively I still think that it is
much easier and better to do this kind
of research in Industry because you look
the reality is you have way more
resources in Industry compared to
Academia and it makes the research much
easier to do um but
I think compared to things like
pre-training planning is actually like a
pretty good domain to work on in in
Academia and the reason is because if
you think about the incentives of large
companies um if you have a if you have a
company that's serving billions of users
you're willing to spend a lot of compute
upfront to train a really good model but
then you want the the inference cost to
be extremely low because you're doing
potentially trillions of queries um and
and so the incentives are are actually
not in favor of planning it's it's it's
in favor of training a really really
really giant pre-train model and then
having the inference cost be as low as
possible espe
those
yes um and so like if you're in Academia
you don't really care about serving your
model to trillions of users or or
billions of users um you just want to
generate a proof of concept and like
only um generate potentially like a few
samples enough to publish a paper and
prove out an idea so this is um a domain
where you you know the the balance isn't
really as badly against you um I would
also suggest you know I mentioned for
things like best ofen the bottleneck is
really on the verifier the quality of of
the reward model um you might want to
consider working on a domain where you
have an external verifier because then
you um avoid being bottlenecked by the
quality of the reward
model okay I want to close uh with this
quote from the bitter lesson if you
haven't read this I highly recommend
reading it I think it's a great say it's
really short um and at towards the end
he says the biggest lesson that can be
read from 70 years of AI research is
that General methods that leverage
computation are ultimately the most
effective the two methods that seem to
scale arbitrarily in this way are search
and learning now I think when it comes
to scaling learning we have done a great
job and I think that that bitter lesson
has been internalized I think there's
more that can be done on the dimension
of search okay so I'll stop there and
I'll take questions for remaining time
thanks
y um what are your thought on the
importance of undoing bad actions um in
this kind of General applications where
you're not playing against in moment and
if downam it turns out that you took a
bad action can you go back and undo it
and do something else yeah so one of the
things that I want to avoid doing is
speculating about you know future
techniques um and what those might look
like because you know I want I want to
stick to like previously published work
and and very very broad uh discussions
um so I think that uh might be a
difficult question for me to
answer yeah you have a question these
soci you
discussed
go time variable depending on complexity
States is always fixed time it they were
anytime algorithms that you could spend
as much compute uh for as long as you
wanted and you would get better
performance the more that the longer
that you ran
it I just mean that once you decide on
that fix time but fixed time kind of
cost play this uh when we did it for
poker we actually used a variable amount
of time um because we were targeting a
certain number of iterations or a
certain like solution quality so it
would think for variable amounts of time
uh but we didn't put a lot of time into
optimizing that um yeah the question so
the research that you described makes a
very clean distinction between search
and the model and you're using the model
inside the search as a sub in some sense
but when you look at something like
Transformers you know there's a lot of
research on this and maybe they are
doing some amount of research inside the
model and maybe you know hopefully by
doing that you can actually do better
than just by having them modeling
separate that's really possible I I
think you know when I when I'm talking
about search really what I'm saying is
scaling inference compute I mean search
is one example of this but I think that
the the bigger question is how do you
scale inference compute um to to just be
yeah um and that could be with like some
kind of external process that could be
within the architecture itself where you
can you know dynamically spend compute
um there's a lot of different directions
to
pursue so yeah I I I definitely think
it's like worth thinking more broadly
than something like just MCTS or or um
just like
you know the algorithms that currently
exist yep uh coming again from more of a
poker background but one of the primary
uses for these models is for humans to
implement them in some way and have you
looked at all about like the
implementability of these things like
for instance mashy calibrium in the
poker space can be incredibly complex
but often times a lot of that's just
noise there's just you know you have
these preconditions that you put into it
and then you get all these very like
small probabilities of doing this thing
or that thing um is there any sort of
like human
facing uh takeaways that you've come
from working on these models
um
I I think I like in terms of Poker I
think it is very difficult for humans to
do this the kinds of computation that
the bot ends up doing I think that there
were General takeaways but it wasn't
really about like the way way that the
bot approached planning it was more
about like the general strategy of the
bot so for example like one of the
things the bot loved to do was in in
very carefully chosen situations it
would bet huge amounts of money so you
know there would be situations where
there's like $100 in the pot typically
humans would bet between like $50 and
$100 and the bot would sometimes bet
$20,000 and that was considered very
very unusual by humans at the time they
actually thought the bot this is part of
the reason why the betting markets were
so against us because they saw the bet
the bot doing these kinds of things and
they thought like this this bot's a joke
it can't be it can't be good um but it
turned out it ended up being very
effective um in certain spots and so um
that's actually one of the things that
the humans took away from the bot and
started doing in their own in their own
games yep if I were to develop like a
new seal planing algorthm what are some
like good qualities that would make this
algorithm more
scalable
um some good qualities that make it more
scalable well I think you can you could
really plot just like as you give it
more compute um how does it do and
ideally you would like to see that it
continues to improve the more compute
that you give
it please thank speaker again

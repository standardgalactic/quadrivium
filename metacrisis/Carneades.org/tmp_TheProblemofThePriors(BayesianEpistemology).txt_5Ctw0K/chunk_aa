Welcome back to Carnades.org. Today we're going to be continuing with our series objections to Bayesian epistemology.
In this video, we're going to be looking at the final video in our large series on Bayesian epistemology.
This is going to be the objection entitled, The Problem of the Criers.
The reason we've saved this objection for last is because it seems to me that this objection is entirely sufficient
to bring about a complete loss of any degree of belief in Bayesian epistemology.
It seems to be big enough to me to cause all the problems required to have us put Bayesian epistemology in the closet and not look at it again.
A quick warning before we get started. This series has been one of the most complicated series that we've done here on the channel.
This video will contain not only reference to the rest of the series, which has been pretty long and complicated without much extra explanation,
but also topics I've covered elsewhere on the channel as well.
I'm going to try to link those videos to this video so that you have a sense of it.
But if you find yourself lost, I encourage you to watch those other videos in the series or the previous videos on the other topics that will be linked as they come up.
I probably should have made this into a number of separate videos, but this series was already dragging on long enough,
so we're just going to put this together as one big video, and I hope you enjoy it.
So, The Problem of the Criers goes something like this.
Deductive logic can tell us if an argument is valid or invalid, but it cannot tell us if the premises are true.
Inductive reasoning, similarly, can tell us if our reassessment of our degrees of belief is rational by our Dutch book arguments,
but it cannot tell us if those prior probabilities are correct.
Take the following argument. If cats can fly, then pigs are gods. Cats can fly, therefore pigs are gods.
Logic alone can only tell us that this argument is valid. It can do nothing to demonstrate that it is sound.
Logic alone can't determine whether or not cats can fly, therefore we can't have any information of the truth of the premises and therefore the soundness of the argument.
Similarly, inductive reasoning can tell us if we reassess our probabilities correctly, but it can't tell us if those originally probabilities were correct.
So, take H as there's a dragon that lives under the hill, probability of H.05. Take E as the ground shakes once in a while, probability of E.05.
And the probability of E and H is .04. We find all of these things pretty unlikely, and so it would seem that based on these probabilities, there's not much strange or not much weird at play.
Maybe we have a higher degree of confidence in a dragon living under the hill than most people would, but it doesn't seem to be very high.
It's only a 1 in 20 chance that that happens. Surely there are people in history who've held such a degree of belief.
However, we'll notice that once we put these into a probability table, were we to discover that E?
In fact, our degree of belief in that dragon would raise significantly from 1 out of 20 to better than a 50-50 chance, or basically a 4 out of 7 chance.
So if you do discover that in fact the ground does shake, then you must reassess your belief in the claim that there's a dragon under the hill from .05 to .04 over .07, or approximately .57.
Therefore, even though the original degree of belief may be faulty, the assessment or reassessment of the argument of degrees of belief is inductively valid.
So just as deductive arguments can only tell us if the conclusion follows from the premises, Bayesian conditionalization can only tell us if a particular degree of belief follows from the previous degrees of belief.
It cannot tell us if those degrees of belief were correct or justified.
Take the following argument. There are aliens living among us, probability of H.02, evidence. Everyone around you starts believing that there are aliens living among us.
You think that's even less likely, but the probability of that evidence is .01. However, there are aliens among us and everyone around you starts believing there are aliens living among us.
You take that probability as .009. Basically, if everyone around you starts believing there are aliens living among you, you start thinking it's more and more likely that maybe there are.
Your probability table would look something like this, and it would mean that if everyone around you starts believing us that there are aliens living among you, the only rational thing is to put a really high degree of belief in that.
Or, in fact, put a degree of belief in that of .9. You have to reassess your probabilities to .009 over .01, or .9, which is a very high degree of belief.
The problem, of course, is that this is a fallacious argument. It commits the ad-popular fallacy. It's inductively valid, but it is deductively invalid.
The final degree of belief is implied by the evidence and the initial degrees of belief, but it commits a logical fallacy.
And so, in fact, with those degrees of belief, we would be committed to believing an argument that commits a logical fallacy.
So, the bigger problem is not only would lacking a criterion for assigning prior probabilities commit us to possibly believing some logical fallacies.
The problem is that with fallacies of relevance like this, some might actually think that while such fallacies are not sufficient to deductively prove an argument or a claim, they are sufficient to increase one's degree of belief in something.
You might not say that everyone around you believing there are aliens among us means that, in fact, for certain there are aliens among you, but you might think that that might be pretty good reason to think that.
Maybe if everyone else starts believing this, there might be some reason you're missing to believe it, too.
But, surely that doesn't work for all fallacies. Surely there are other fallacies or certain situations in which that fallacy would or wouldn't confirm your degree of belief.
Yet, Bayesian epistemology does not allow for those nuances.
The problem is that there's no agreement on what constraints we should put on prior probabilities.
Nay, there's not any one completely coherent set of constraints we can put on prior probabilities.
Nor is there a solid framework to differentiate between cases in which fallacies can confirm hypotheses and cases in which they cannot.
Nor is there any way to tell what degree those fallacies should confirm the hypotheses when and if they do.
Maybe we might think that if there is a fallacy present, it should confirm our hypothesis a little bit less, but still confirm it to some degree.
Which fallacies does that apply to? When is that the case?
No Bayesian can offer a full assessment of that, or at least has offered a full assessment of that.
So, while Bayesian epistemology might be able to tell us how to reassess our beliefs once we have them, it cannot tell us what prior degrees of belief are rational to have,
any more than logic alone can tell us if it is true that cats fly.
Perhaps one might think that objective Bayesians can come to the rescue.
They would assert that there are in fact some constraints that one must put on prior probabilities.
They would affirm that this is in fact only an objection for subjective Bayesianism, and they have a set of constraints they can put on prior probabilities.
However, currently, objective Bayesians only assert that you can put those constraints on very specific cases,
and there's a huge amount of disagreement on those specific cases currently, and a number of objections, and paradoxes that arise from even the small set of solutions they've posed.
So, no one has succeeded in producing an exhaustive or paradox-free list of rationale for constraining prior probabilities.
This is not to say that it cannot be done.
Simply that it has not been done.
If you think you can create a paradox-free, completely exhaustive list of rationale constraining prior probabilities, I encourage you to offer it in the comments below.
I think it's something that would do a long ways to saving Bayesian epistemology.
But before you do that, watch the rest of this video, because I think there's some very powerful objections to any such criterion.
So the question is, why is this a problem for Bayesian epistemology?
It might seem that, just like deductive logic is fine, just because it can't prove the truth of the premises doesn't mean that it's not still important for us understanding forms of argument.
Bayesian epistemology might simply say that, well, we don't need to determine the prior probabilities.
That's some other person's field.
That's another idea.
That's another area that we're not going to deal with.
We're just dealing with what is inductively valid.
The problem that it will pose, it means that the Bayesian is going to end up being just as powerless in the face of classical problems for epistemology and philosophy of science as any other dogmatist is going to be, as someone who just believes in certainties.
Let's take a look at why.
So, thinking about the problem of induction, we might think that Bayesian epistemology might help us solve the problem of induction by showing that inductive evidence can provide degrees of support for a hypothesis, but not deductive certainty in it.
If you don't know what the problem of induction is, I'm not going to review it here. Check out my video on it.
So, imagine that we have a hypothesis, all swans are white, and we state the probability of that as 0.8.
The evidence, I find a white swan, we think that's a 50-50 chance I'm going to run into a swan today, 0.5.
And the probability of both E and H as 0.45.
That's how we're going to assess that.
So, if we look at our table, we would see that, in fact, finding a white swan would confirm our claim.
It would raise the probability in our claim that all swans are white, slightly.
And theoretically, the more white swans you find, the more it would be confirmed.
We might never reach a certain degree of a belief in it, but it would confirm it little by little.
And yet, with these probabilities, finding a white swan would confirm your hypothesis that all swans are white.
The problem, however, is that you need to have those prior probabilities only if you already trust an induction.
The only way you would have some probabilities that confirm all swans being white with finding a white swan
is if you already think that induction is some way that you can gain some amount of certainty.
And so, unless you already believe or have some confidence in the uniformity of nature,
there's no reason to trust that finding a white swan will say anything about all the swans you'll find in the future.
If you don't trust the uniformity of nature, or if you don't have a strong degree of belief in the uniformity of nature,
there's no reason for you to put any faith in all swans being white having some relation to finding a specific individual white swan.
And similar to the problem of induction, the only way you can confirm the uniformity of nature is by using the uniformity of nature.
So, you have to bootstrap the principle once again, and we're going to be back into our original problem of induction.
To clarify this, let's say you don't have any faith in the uniformity of nature.
You have the same original probabilities in the claims all swans are white and I find a white swan.
However, you're going to not say that there's any relation between these two claims.
So, you're going to put 0.4 in for your E and H.
Note that whether you find a white swan or not, you're going to end up with the same degree of belief in the claim that all swans are white.
Why? Because you don't originally put some preconceived belief or justification in the uniformity of nature,
in the idea of induction, that finding an individual white swan can say something about all swans.
By only changing your prior degree of belief in confidence in H and E to one which puts no confidence in induction,
we can create a scenario in which the evidence does not confirm the hypothesis.
The problem of induction is that unless we have confidence in induction, we cannot justify it.
If you lack confidence in induction to begin with, you have no reason to structure your prior probabilities in favor of it.
The only way you would structure your prior probabilities in favor of it is if you already believe in induction and therefore bootstrap it with circular logic.
Now, one might then think that Bayesian epistemology, even if it can't solve the original problem of induction, might help us solve the new riddle of induction.
Once again, if you don't know what that is, check out my video on the subject.
But without a justification of restrictions on prior probabilities, that also is unable to succeed.
There are no concrete explanations of why one should sign a higher probability to finding a green emerald,
and all emeralds being green, than finding a green emerald and all emeralds being blue.
This means that the evidence of a green emerald will confirm both the claim that all emeralds are green and the claim that all emeralds are blue equally.
The point is that until you have some way to establish prior probabilities that will show all emeralds are green in some way being more confirmed than all emeralds are blue,
there is no way you can solve the new riddle of induction.
And because any such criterion of prior probability assignment is simply going to be biased towards your own set of beliefs,
as opposed to the set of beliefs of someone who uses terms like grue and bling, there's no rational way you can do it.
Maybe then, Bayesian epistemology can help us with a problem we looked at more recently, the problem of underdetermination in scientific theory.
Perhaps then, the Bayesian has a better solution for the problem of underdetermination than the rest of us do.
One could imagine that if you discover contradiction in your theories, you might throw out the hypothesis that you have the lowest degree of belief in,
or that was least confirmed by your evidence.
It seems then we have a very rational way of deciding which hypotheses we should throw out and which ones we should keep.
So the holistic problem of underdetermination seems to just fall apart.
Once again, if you're confused by this, check out the videos I have on underdetermination.
However, without a rational criterion to create specific prior degrees of belief, our revisions remain just as arbitrary.
This irrationality now instead rests on the heads of the lack of clear rules for forming prior probabilities,
or in other words, the only reason that you have a lower degree of belief in whichever hypothesis you decide to throw out is because of arbitrarily determined prior probabilities.
And this is also going to open up another problem.
If ever a criterion for determining prior probabilities were to be established, either we have certainty in such a criterion or we do not.
As we've seen before, certainty in Bayesian epistemology is irrational.
Therefore, our degree of belief in our criterion for establishing prior probabilities must not be certain.
This raises a new question.
What degree of belief do we place in that criterion?
Well, the only way we can determine what degree of belief we place in that criterion is by using that criterion itself and assuming that we have a higher than 0.5 degree of belief in it,
because we have a lower than 0.5 degree of belief in it, surely we wouldn't use that criterion for determining our prior probabilities and for determining the very prior probability of that criterion that we're trying to assess.
What other than this criterion could allow us to choose what probability we should assign to the criterion?
The only way we can establish what prior probability we have for this claim is by already assuming that we have a high degree of probability in it and therefore bootstrapping it.
The only way we could choose even what prior probability we have, not to say justify that prior probability, but choose any prior probability is by already assuming that we have a high degree of belief in that very criterion that we are assessing.
This is circular logic and it surely is a logical fallacy.
Similarly, one might conclude that between empirically equivalent hypotheses, one might be better confirmed by the evidence or have a higher degree of belief and therefore be more preferable to the alternatives.
Even if two hypotheses are empirically equivalent, they predict the same evidence and are justified by the same evidence, one might be slightly more justified or slightly more confirmed by the evidence and therefore we would choose that one when two hypotheses are the same.
But yet, once again, this assumes that we have some reason for assigning priors that would provide better confirmation or a higher degree of belief, but we of course do not.
Other than, once again, our biased irrational opinions as we've noted in Larry Loudon in the SSK and his science rational.
And this too will open up yet another problem for the Bayesian.
Whenever we assign a probability of some hypothesis H, we must assign to the alternative not H whatever one minus the probability of H is.
And yet, there is no way for us to have access to all of the possible hypotheses that are contained in not H.
Remembering that not H is just a denial of that hypothesis, so that means anything that's not that hypothesis could possibly be the case.
As has been noted, science comes up with new hypotheses which are more confirmed or persuasive than old ones.
So claiming to have a solid specific probability for all of those hypotheses or a disjunction between all of those hypotheses is just patently absurd.
It seems crazy to assign a degree of belief into something you've never seen or had any understanding of.
So finally, maybe Bayesian epistemology can't solve the problems that we have in the philosophy of science, but at least it can furnish us with a response to skepticism, surely.
The challenge of the skeptic is to prove something with absolute certainty.
Bayesians can sidestep this by allowing for some propositions not to be perfectly certain.
And in fact, as we've noted before, the Bayesian is going to fall into some problems of their own by asserting that anything is certain.
So the first concern to put forward here before we declare victory over the skeptic is the one that is presented in the previous objection about dealing with the immutability of logic.
Check out my video on that if you're curious.
While the Bayesian may allow some propositions to be uncertain, they must still have faith in the claims of logic and probability to justify any of their claims.
If they don't have faith in that certainty, the whole of Bayesian epistemology falls apart because the connection between degrees of belief and probability is broken.
As long as these are in doubt, which the skeptic, as we've shown, can doubt, the Bayesian cannot defeat the skeptic.
But that isn't all.
I present for you Agrippa's quadlema for Bayesian epistemology, clearly never presented by Agrippa, but in the spirit of his trilema for classical epistemology.
So yet, even setting such concerns aside about the immutability of logic, there is still a problem for Bayesians in terms of everyday beliefs.
For any degree of belief that a Bayesian claims to have, there are four possibilities that they arrived at that degree of belief.
One, it's a prior probability whose value is justified by some criterion that assigns prior probabilities.
Basically, they've never conditionalized over that probability, it's just whatever prior probability was assigned to them.
Two, it is a conditionalized probability whose value was determined by one or a longer string of conditionalizations over prior probabilities determined by some criterion that assigns prior probabilities.
Basically, we had some criterion, it assigned this belief a prior probability, and then we were given series of evidence that reconditionalized and reconditionalized and reconditionalized that belief to the current degree of belief we have.
Three, it is a conditionalized probability whose value was determined once again by a string or single conditionalization over prior probabilities that were not determined by some criterion and taken on faith.
So we had some original prior probability that we decided because we liked it, that we decided because we flipped a coin, that we decided because we had faith in it,
and then we conditionalized over that a series of times through pieces of evidence and therefore arrived at our current degree of belief.
And finally, four, it's just arbitrary and taken on faith, it's just a degree of belief that we have put zero justification into or zero conditionalized piece of evidence into and just taken that degree of belief on faith.
Let's take a look at if one, two, or any of these can help the Bayesian to deal with skepticism.
So, if four is the case, then Bayesian epistemology has not answered the problem of skepticism by giving reasons for a justified degree of belief, but it's simply replaced a faith in certainty with a faith in likelihood.
So, all the Bayseans done in this case is say that, well, we can't have faith in certainty, but we're allowed to have faith in likelihood.
We're allowed to have unjustified belief in the likelihood of God existing or evolution being true because we're allowed to have faith in things that aren't certain.
This doesn't answer the question of the skeptic.
It changes the conversation and I think most people would be very uncomfortable with this response because it's not actually justifying your degrees of belief.
If three is the case, then though we can state reasons for arriving at our probability, we cannot give reasons for our original priors which shaped our belief.
The reasoning may be inductively valid, but to be sound, it must have true priors that it rests on.
Since these are taken on faith and unjustified, there's no justification for the conditionalized probabilities that come after.
This is very similar to the problem we had with the deductive argument about cats flying and pigs being gods.
Basically, that argument was valid, but the original premises arguably weren't true.
And so the conclusion isn't going to be any more true because the premises that we base the argument on weren't true.
Similarly, if the priors we took originally were just based on faith or were arbitrary, there's no reason that we should assent to the conclusion.
Now, some Bayseans might claim, well, no matter what prior probabilities you have, everyone's going to eventually reassess their degrees of belief to about the same point.
But that doesn't really seem to follow, it doesn't make sense, and it rarely seems to be the case.
Because there's an entirely coherent set of prior probabilities that allows me to conditionalize to a very high degree of belief that God exists based on the evidence that I've seen a bird.
I see lots of birds all day long, and I keep confirming this belief.
With no restraints on our prior probabilities, even absurdities are deemed rational.
And just because everyone's getting the same evidence doesn't mean they'll arrive at the same conclusions.
So let's put probability of God existing, .5.
Probability of me seeing a bird today, also .5.
Probability of me seeing a bird today and God existing, .499.
Basically, I saw a bird today, therefore I'm justified in having a .998 degree of belief in God.
The problem, of course, here is that even though I'm receiving the same evidence as everyone else and I'm conditionalizing the same way as everyone else,
my prior probabilities, my original ideas, were really problematic.
Those had such a problem in them that no matter how much evidence I receive, I'm probably not going to change or reassess my beliefs.
Now, number two.
So if two is the case, just as with three, we note that the justification for the proposition will rest on the justification for the criterion of creating prior probabilities.
Just as the last one was taken on faith, and therefore our current degree of belief that we're looking at must be just as taken on faith,
the justification for our current degree of belief must be equivalent to our justification for our criterion in creating prior probabilities.

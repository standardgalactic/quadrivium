It's a great pleasure to introduce Thomas Paar today, we'll be about his background.
So he started medical school in 2012, I guess, at the UCL and did also like a bachelor there
with a focus in new science. In 2016, he started his PhD, I hope everything is correct, he started
his PhD working together with Carl Friston and Garen Rees. So I think Thomas is really like the
person who most frustrated me, when I was at the at the Philly, so a long time ago. I was fascinated
by like the idea of the free entry principle and Carl Friston and really started going, learning
about Boltzmann equations and some kind of like message passing and so on and over years and years.
And then in 2016-17, I realized that suddenly a number of papers were coming out, it became much
more accessible and on these papers there were always like one name, Thomas Paar. And so I looked
into that and I realized that there is one person out there who is so young and who plus
started his PhD, who was able to go all through this extremely complicated math and ideas so easily
that I thought I should really give up thinking about the free energy principle at all. And more
impressively, I think a couple of months ago, even like a book came out like about like Active
Influence, where he is like the first author on and I really can recommend this book. It's a
fantastic introduction to Active Influence, to the free energy principle. So it's a really,
really great pleasure to have you here tonight, Thomas, and I'm really looking forward to your
talk. Thanks for joining. Well, thank you very much for the very nice invitation and the very
nice introduction as well. First of all, can you hear me okay? Yes. And next thing is to see
whether I can successfully share my screen. Let's try this. So I'm assuming you can see the
PowerPoint and now hopefully in full screen. Perfect. Great. So today, I realized that you've
already had a talk from Ryan Smith previously, so I'm assuming he's given an excellent overview of
Active Influences as he always does. And I wanted to take the opportunity to focus in on something
a little bit more specific in this talk, which is thinking about how dealing with Active Influence
lets us think about neurobiology and how we can start to make connections and form neurobiological
theories from the principles. So I'll start just by giving an introduction that I think
will hopefully give everybody a little bit of a refresher on Active Influence and the basic
principles. And although there are some technical elements to it, I'll try to make it as intuitive
as possible. And it really doesn't matter if you get all the technical detail through this,
it's just to build some intuitions. So I tend to like to start talks with this slide, which just
shows two different sorts of system. The one on the left is one that just gradually diffuses over
time, sort of loses form becomes progressively less interesting. Whereas the one on the right,
despite having the same amount of randomness built into it, manages to maintain its form over time.
And a useful starting point in thinking about Active Influence is thinking about
what the difference between these two sorts of systems are and how the one on the right is able
to maintain its form and resist the effect of these random fluctuations that are forcing it
in all sorts of different directions. And it's often useful to reformulate this in terms of the
probability density or the change in the probability distribution of all of these little particles
over time. So again, you can see on the left, we've got something that's just diffusing out into
nothing. And on the right, we get something that behaves from a probabilistic level pretty much
statically. And it's really those on the right that we're interested in, those biological creatures
are able to maintain their form, are able to resist the effects of what the environment does to them.
Now, if we sort of write down the kinds of dynamics that we'd need for a single particle
or a single part of the system to try and maintain this form, what we get to is a system that I think
quite intuitively always moves when it can from regions of low probability to regions of high
probability. So if we interpret this final distribution it gets to as some steady state,
all we need to do to maintain that in a random environment is to always try and climb uphill
at the probability gradients. And then almost by definition, we end up spending more time in
highly probable states and less time in more improbable states. And this is sort of the starting
point really for active inference, because we now have this notion that we're climbing probability
gradients and we have some distribution that we're effectively trying to maximize.
And we can link that back to ideas like the Bayesian brain, the idea that the brain is using some
model of the world around it to generate predictions. And then is drawing inferences about
the data that it actually obtains, so sensory information coming in through our eyes, ears,
through our skin, that we can then use to draw inferences about the causes of those data.
So here we've got the idea that there's some states of the world X that are causing some
sensory data Y. We're then forming some posterior beliefs, so beliefs about the causes given with
the data. And we're doing this using a generative model that comprises a likelihood in a prior,
so prior being how plausible are the things out there in the world before we've
made any observations. The likelihood being how likely the observations we've made are given
the states of the world or given our hypotheses about what caused them. And together we refer
to these two things as a generative model. Now the posterior distribution is what happens when
we invert that model where we find the probability of some causes given the data.
And the final term we've got at the end here is referred to either as an evidence or a marginal
likelihood. And that's because in the context of Bayesian statistics we often use a marginal
likelihood as a measure of the fit of a model to the data that it's trying to explain. There's
how much evidence do those data afford some model of the world. And together we can think of these
as being an inversion of the generative model. Now the reason I've put this here is that we
can see that the dynamics that I'm showing in the upper left can be interpreted as the process of
maximising the evidence for some model, maximising the fit between the brain's model of how the world
works and how the world then engages with the model or engages with the brain by presenting it data.
And broadly there are two ways of doing that. The first is, as we've already spoken about,
it's changing your beliefs based upon new data such that you get a better fit to the world.
The other way, which I think is one of the key ideas in Active Inference, is that you
generate actions based upon your beliefs that change the world to make it more like your model.
And this is sort of the key idea that underwrites Active Inference, that all we're trying to do
is maximise the evidence for some model of our world. And we can either do that through
perception by changing our model or through action by changing the world. But together,
they come under one single objective, sometimes referred to, particularly by people like Kevin
the Acapoway, as self-evidencing. Now I want to go a little bit into the structure of generative
models and specifically the ways we can think about and the ways we can notate generative models,
because it often helps to move from the slightly more mathematical abstract description to a more
graphical notation that I think often gives a much better intuitive sense of what's going on.
So to do that, I'm going to start with this idea that model evidence is effectively what we get out
once we've integrated out all of the causes from our model, by which I mean if you take
account of all of the things our model predicts and then we take into account the prior probabilities
of all of the things that are causing those data, we can then work out what the probability of the
data are under that model. But the model itself takes account of the things that are being generated,
so our sensory data, and also the things that are causing them, simply a joint probability
distribution involving all of these things. So I'm now interpreting this graphic up on
the left as depending upon some generative model where we've effectively integrated out everything
that we don't need to determine explicitly. Now generative models will often have some
interesting structure, and in fact if they don't have interesting structures then they're not
interesting generative models. And normally that structure manifests as a factorization
of this joint distribution, but not everything depends upon everything else. And so we can
often factorize it, and here I'm just showing a completely arbitrary factorization of some
generative model where we have certain dependencies, so y depends directly upon x1 and x2, but not
directly upon x3 and so on. And the reason it's useful to think about this factorization is because
we can then express a graphical version of this model that provides a bit more intuition as to
what's going on. So the way we do that, or one way that we can do that, is using something known as a
factor graph. And the idea is that we take each of these factors in turn, we draw a square,
and then we draw an arrow to whatever's on the left of the probability factor we're interested in.
So here an arrow towards the y, because we're saying the probability of y conditioned upon or
depending upon the other two things, and then we attach those to the same square. We then move
on to the next factor, draw another square, and we then attach the relevant variables together
in exactly the same way. And we carry on doing that until we have a picture of what our model is
like. So now instead of having to look at this equation, you can look at this and say, okay,
well x3 causes x1, x4 causes x2, and together x1 and x2 relate to or together generate our data y.
This is just to demonstrate several models you may be familiar with generally, and the fact that we
we're often implicitly using these kinds of generative models without necessarily thinking
about it. So when we're performing a principal components analysis, we're often taking some
distribution of some variable x, we're then mapping that to a higher dimensional space y,
and that's our sort of model of how the data that we're working with are generated.
So what we can then do when we perform a principal components analysis is take our
high dimensional data y, work out how you go back from the y to the x, and that gives you all of your
specific principal components and the directions that have been stretched in various different ways.
Same principle applies for things like canonical variance analysis, where you've got some set of
variables then mapped to two different sorts of data by stretching them and
distorting them in various ways. And the challenge is how do you get back to the common
hidden variable common cause for both of those datasets. And we do the same sort of thing with
something like clustering analysis, where here the model says we're effectively sampling from
one of several different clusters with different probabilities. We're then generating some point
in space depending upon which cluster it falls in. And the process of inference is again undoing
this. It's finding the posterior by taking a point in a cluster and saying, well, which cluster is
that from? So going from the y to the s. So that's a sort of brief introduction to the idea of
generative models, the the role they play in active inference, some key examples and some of
the notation that I'm going to use as we go forward. What I've said so far has been relatively
abstract. And in the next section, I want to try and work from those sort of general principles
and these slightly abstract notions through to something more specific and more neurobiological.
Now, the first thing we're going to do to get there is to introduce one more abstract concept,
which is that of a Markov blanket. A Markov blanket comes up in all sorts of places in active
inference, but I'm going to use it in a very specific way here. And this is to talk about
conditional independence. And what I mean by conditional independence is that if you have
two different sets of variables, so here I've noted them as the mu and the eta,
the Markov blanket B is the set of set of variables that render the other two completely
independent to one another, that mean that if you know everything about the blanket,
knowing something about mu tells you nothing new about eta.
To give you an example of this, one of the examples that most people are familiar with
is the idea of a Markov chain, where you have a sequence of events in time. So something that
happens in the past then influences the present, which then influences the future with no direct
influence from the past to the future without going via the present. So if I know everything
there is to know about the present, knowing something about the past tells me nothing new
about the future and vice versa. So in that sense, the present is the Markov blanket that
separates the past from the future. Now, this is a very useful concept when we're dealing with
graphical models or generative models that have some interesting factorization structure,
and I'll try and explain why. The Markov blanket is often referred to as or can often be identified
by identifying variable we're interested in, so let's say we're interested in the variable x2,
then you find the parents of x2, so the things that caused it, the children of x2, so the things
it causes, and the parents of its children, and that gives you the Markov blanket of x2.
Now the significance of this is that knowing this means that if we know about the blanket,
it doesn't matter what else is going on in the generative model, you know, this might be some
tiny section of a huge generative model with many, many variables, but we don't need to know about
all those variables, all we need to know about are the blanket variables, so we can effectively
ignore everything else if what we're interested in is x2. Now, how does this then matter for
neurobiology? Well, I think the answer to that is that the brain is an extremely sparse structure,
that synaptic message passing does not involve connections between every different neuron in
the brain to every other neuron in the brain. There are a small number of, relatively speaking,
of synapses between any neuron and its neighbors or neurons elsewhere in the brain,
and so we can make an argument that if the brain is performing inference about lots of different
variables, all it needs is to know the structure of the generative model and the variables that
sit in that Markov blanket, or at least the neurons that are representing those other variables.
And that tells us that we can then, when we're inverting that model, when we're performing
inference, what I'm showing here is just one example of an inference scheme, we can, we can
express the dynamics of that neuronal message passing in terms of connections from the other
neural populations that are representing the variables in that Markov blanket. The details
of this aren't that important, but this is one example of an inference scheme known as variational
message passing, here formulated as a gradient scheme in terms of some dynamics, which has
relevance for modeling neural populations, we're interested in how the dynamics of those
populations evolve over time. And we can sort of do this recursively, we can couple together
lots of different parts of this system, where the things that are being connected up are just the
Markov blankets of different populations. Here I've expressed it in terms of an error term,
this epsilon, which effectively represents the gradient of a free energy objective,
which is used as an approximation to that marginal likelihood or evidence that I spoke
about earlier. And then some beliefs about or expectations about that variable, so we can now
use our, the errors in our predictions to update those variables. And once we've written down the
dynamics of this sort of system, we can simulate things like firing rates, we can simulate the
dynamics and the behavior of these networks of neurons passing messages between one another
as a means of performing a form of inference and thus self evidencing.
I know that this is probably still seemingly relatively abstract, but I think what will
hopefully help is if we then go through some examples thinking about specific kinds of
generative model and how that might then affect the kind of message passing we would see in a
brain. And the first step to thinking about that I think is very important when we think about
really useful generative models is they will all typically have a temporal aspect to them that
as biological creatures we deal with things that evolve in time. And so it's worth thinking about
how do you formulate a generative model that has that kind of dynamic aspect to it.
And there are several different answers to that and one of them is we use something known as
as a Taylor series approximation. So we start by saying okay at the current time what is the
value of some variable x in the world and there's maybe some continuous variable it may be
where my arm is in space. If we then want to know how it's going to evolve in time we could then say
well let's take the next element of our Taylor series approximation that's taking out of its
velocity and that tells us a little bit more about the trajectory that my arm might be on.
We can then take another value which is the current acceleration of that position and we get
a slightly better approximation to the trajectory. And the more terms we add in the greater approximation
we have of the trajectory or the greater representation we have of the trajectory just as
a series of numbers where those numbers are my current position my current velocity acceleration
etc. So we could formulate a generative model by trying to predict each of these coefficients
of this Taylor series or each of these generalized coordinates of motion as they're sometimes referred
to. An alternative is we just say at time one where will I be at time two where will I be and
we simply represent it in terms of the sequence of points over time. Often when we're discretizing
time in this way we often we also discretize space so we might say okay where am I in some
discretized scheme am I in location one two three four or five at each different time point
and so we have two different ways of accounting for how things might evolve over time and each
of these can be useful in slightly different circumstances. So if I were dealing with a
sequential decision-making task it may be much more efficient for me to say okay at the first
move I'm going to make is this then the second one is this the third one is this and dealing
with something very sequential that might also be important in in sort of language processing
where you have to think which words you're going to put together in a sequence and we're dealing
with discrete variables over time because words are categorical instead of opposed to
continuing as opposed to lying on a continuum. However if I were dealing with movement or
the responses I'm getting from some photoreceptor in my retina it may be much more sensible for me
to be working in continuous time thinking about how things evolve at a continuous scale.
So what I'm showing here is an example of a generative model and the associated message
passing scheme when formulated in terms of this continuous time scheme. So what we've got here
is a factor graph at the top this blue element that uses exactly the same sort of formulation that
we've been discussing so far so we have some variable here which might represent our position
which then predicts our velocity at that time which then predicts the acceleration at that time.
So we have a series of predictions here about the coupling between different orders of motion
at each point those are predicting some sort of data here are wise which represents the
the current position of our sensory data the current velocity the current acceleration.
What I'm showing lower down here is the message passing scheme we get when we take account of
the Markov blankets of each of these variables so the Markov blanket here for the the velocity
would include the position and the acceleration but also the data I've got about those things
and some prior beliefs about about other variables in our model and so we'd end up connecting those
things up and although it looks like a bit of a mess of connections it's still fewer connections
than the total number connections you would get if you match everything to everything else.
What we've got here takes the form of effectively a predictive coding style scheme
which people may be familiar with and the idea that that you can you can hierarchically predict
predict some data and you can predict the thing that's predicting that data
those data and by observing the errors in your prediction of the data you update your prediction
which then cascades up a hierarchy allowing you to update subsequent predictions.
What I'm showing you here is the equivalent model formulated in terms of discrete time and in this
case discrete space as well so here we have a state at time t minus one which generates a
state at time t which generates a state at time t plus one at each of those time points we're
generating some observable outcomes which are represented as the o's with a variable pi here
that represents alternative trajectories I could I could pursue so alternative policies or plans
alternative ways I could change the sequence of events and again we have underneath the message
passing scheme that could perform inferences about this sort of model and you can see that the
message passing scheme to some extent looks like an inversion or or as loosely a sort of mirror
image of the generative model because all it's doing is taking each step that was performed
to generate some data and then inverting those steps one at a time so it's just going backwards
from the data to arrive at the causes of those data. Now what I've got shown here on the left
is just a sort of cartoon image of some of the key connections in in cortical micro circuits
so here we've got superficial pyramidal cells as spiny stellate cells as SS deep pyramidal
cells as DP and inhibitory engineer arms as the double eyes and this is clearly an oversimplification
of what is a very complex connectivity structure but the reason I'm showing this here is that
if we know something about this connectivity structure if we know about some of the key patterns
we can start to ask how can the the sorts of models that we've spoken about over the last
couple of slides then be interpreted in terms of the micro circuits that could help
could help implement these in a biological system. So what I'm going to show in the middle
is an example of a continuous states-based predictive coding style model that has all
the same elements as in the slide I showed you before but are now mapped so that some of the
connections coming in and out of it loosely map to those associated with a known cortical anatomy
and we can do exactly the same thing here with the discrete states-based model and think about how
we can assign those roles associated with the roles of different cortical layers
and then use that as a way of forming hypotheses. Now what I'm showing you on this slide is not
necessarily the last word on this and you could formulate several different hypotheses about how
these two things map together and that's where a lot of interesting science happens. This is just
one example of how we can associate what we know about cortical microcircuitry and anatomy
with what we know about the anatomy of message passing for some simple generic forms of generative
model. What I'm going to try and do over the rest of this presentation is to take the connections
coming out of this out of each of these points and to try and think about how those might or the
roles they might those might play when we take an active entrance approach and the sorts of structures
in the brain that might be involved in dealing with those dealing with those things. I'm going to
start by talking about movement and reflexes so here the kinds of things we're interested in
are the connections that leave the cortex and go to areas like the spinal cord in the motor
neurons, the predictions that we can then make about what we're seeing here in the central
plot and the G that's been circled in red is a prediction about the sort of data I might expect
to observe which may here be proprioceptive data consequent on particular sorts of action.
And we can think of this quite simply if we think about the structure of a reflex arc.
So what I've got here on the lower part is just the same marginal likelihood or model evidence
that we've dealt with before this idea of it being a measure of the fit of the model both in
terms of its accuracy, the accuracy with which it predicts some data and the simplest least complex
version of the model that will work for that. What do we do when we change the data? Well we try and
make the data more accurate in relation to our model by trying to align the data with our predictions.
So when we think of the structure of a reflex arc where we can interpret it as some descending
prediction coming from the motor cortex saying what is the proprioceptive data I expect to
observe. By comparing that with the proprioceptive data that's actually coming into the spinal cord
through the dorsal horn with that prediction we then get a potentially a mismatch between the two
that can be used to generate action via the ventral horn generating movements and contractions and
muscles that then try to fulfill these predictions. I'll show you just an example of a simulation of
an arm using active inference that has these sort of reflex arcs built in and all we're doing here
is we're just tapping on the biceps tendon where the red arrow is pointing and we're just seeing
the reflexive response that we're getting as a consequence of effectively inducing this additional
piece of sensory proprioceptive information by stretching that tendon. We induce the mismatch
between the prediction that's coming down and the data that's coming out and that
is corrected then by generating this additional movement. Interestingly we can do things by
manipulating the confidence of those predictions and sometimes get things like bigger reflexes
and hyperreflexia as the sort you might see in clinical populations with spinal cord injuries
and so here you can see the reflex is slightly larger than it was before and slightly brisker.
I'll show you another example of this same idea where now we've developed a mapping between
the microcircuit and the anatomy of the ocular motor brainstem so here starting from the superior
colliculus the the rostral interstitial nucleus of the medial longitudinal vesiculus and there is
other centers and the cranial nerves that are responsible for generating eye movements so cranial
nerve three and cranial nerve six here are seen here as resulting from these error terms so we're
predicting how I would expect my eyes to move and any error in the current position of the eyes is
corrected by generating eye movements so by inducing different priors on top of this model
of different predictions about where I'd expect the eyes to be we can actually then generate
different sorts of eye movements using predictive coding style active inference scheme where the key
thing is that the predictions can be fulfilled by actually changing the positions of our muscles
and positions of our eyes so that sort of provides a very brief overview and a couple of examples
dealing with the role of reflexes and predictions in generating movements but it doesn't really
tell us anything about how movements are chosen how movements are selected and you don't necessarily
get any intelligent behavior out of that and for that we need to start looking at different sorts
of structures and different parts of this generative model and here the key thing I want to focus on
is the role of the basal ganglia and which here we're going to associate with the computation
of something known as the expected free energy and I'll try and describe what that is a little bit
more in some of the subsequent slides so what I'm showing here is a mapping of some of the
subcortical but subcortical anatomy or more actually I should say that this is a mapping
between parts of our Bayesian message passing scheme parts of the model inversion when we're
dealing with the ability to plan and make decisions to some of the known subcortical anatomy of the
brain and the key thing I want to focus on is this G that is depicted as part of the as part of
the stride and so the things that are feeding into this are what we're getting from the cortex
here which is some prediction of the outcomes we'd expect if I were to pursue a particular
cause of action which is this subscript pi and the another sort of error term here which is how
far away are those predictions from my preferences about how the world should be my prior beliefs
about how the kind of data that I would actually actively seek out and act to get together those
are used to calculate our expected free energy which we can then use to formulate beliefs about
policies by saying that the policies we would select the plans we would engage in are those that
we would expect to be associated with the lowest expected free energy and to put that more formally
we're saying that we're going to give a prior belief that the policies the series of actions
we're going to choose are going to be those associated with the maximum information gain
tell us the most about the world around us that fulfill our preferences and then we're going to
add in an additional term here which deals with habitual type policies and things that we tend to
do because we've learned we behave in a particular way in those situations now the combination of
this information gain and preferences are often referred to as an expected free energy and I won't
go into the details here but that's simply because you can rearrange them mathematically to make them
look very much like the equation for a free energy or or marginal likelihood approximation
with an expectation around them to say that these are the what we would expect given
our predictions about the data we would obtain because here we're dealing with beliefs about
the future plans into the future where we haven't yet had those data and we need to deal with the
expectations of what those data would be under different plans that we could choose
and so here we're showing just an example of the direct pathway through the basal ganglia
which is normally thought to facilitate movement and depends upon this expected free energy
and an indirect pathway which here we've associated with these kinds of habitual drives and I'll
come back to those a little bit later on when we deal with hierarchical models
just to give you a little bit of intuition as to the information gain aspect of the expected
free energy because I think most people most people probably understand this idea of seeking
preferences and behaving to maximize some degree of reward but many people are less
familiar with the idea of seeking information into the way that that might manifest in these
kinds of models so as an example I'm going to show you just a simple simulation where we're
going to manipulate some of the different aspects of the uncertainty in the model so here
in the upper left I'm just showing a way of parameterizing the the uncertainty associated
with the data generating process so this is our likelihood precision on sensory precision
and it effectively says that when this precision is very high we can be very certain about the
outcome we'll observe given a particular state of the world whereas when it's very low we could
predict everything with very similar probability and we can do something very similar by manipulating
the uncertainty in the dynamics of the world so again when this is very high it means that
where I am now is very highly predictive of where I'll be at the next step in time
whereas when it's very low it means that pretty much anything at the next time point is is equally
probable and I'll show you a simple simulation where these two things are manipulated just to
try and give you an intuition for what it means to act to to maximize one's information about the
world so the upper simulation here top left shows four panels which can each change at some point
in time to a different color with some random probabilities and the blue line here is designed
to show effectively an eye tracking trace so it's a simulated agent who is allowed to choose which
of these panels it wants to look at at any one time and you can see it samples them with a
relatively even frequency in the middle panel what we've done is we've reduced the precision
or increased the uncertainty associated with the likelihood in the lower left that effectively is
like turning off the lights in that location it's effectively making that lower left location much more
much less informative much more noisy and so effectively what what we've got here is a system
that then ignores that it says that I can't get a good quality high quality information from there
so I'm going to look at all of the other locations rather than rather than this in the lower left
an analogy for this is if you're thinking about how you perform a scientific experiment you would
probably aim to use if you had a choice between two different measuring instruments you would
choose the one that gives you more precise measurements rather than the one that gives you
very noisy measurements in the lower left we've manipulated the uncertainty or the volatility
associated with the dynamics so here what's happened is the upper left location is associated
with much more uncertain dynamics so here what happens is that I end up sampling that upper left
location much more frequently and intuitively this makes a lot of sense because if you've looked
somewhere very recently normally you will you'll know a lot about what was there you don't need to
look back there anytime soon however if it's very volatile if it changes quite with some degree of
randomness and you have very little certainty about the state of it after you've looked away
you'll look back with a much greater frequency so we've effectively decreased the inhibition
of return in this location by increasing its volatility or decreasing the precision associated
with its with the dynamics in that location so the end of this was just to show you this sort
of emergent behavior just by having an information seeking objective by by having a prior belief
that we're going to act to minimize some expected free energy one component of which is to maximize
our information about the world now the next thing I want to come on to is the role of hierarchical
generative models and one of the key benefits of having a hierarchical model is that we can now deal
with things that evolve over a range of different timescales so you might have some things that
evolve very slowly and some things that evolve very quickly and to some extent we can separate
out two and use slowly evolving things to to to help us predict what's happening at the faster level
and here the key things to to note are all of these connections between higher cortical regions
and lower cortical regions which manifests both in the in the discrete and continuous state space
models and so it's worth them thinking about what the role of these are and how that manifests in
terms of the generative models we've been dealing with before and it's really this that links back
into the the idea of predictive coding as many people know it and you've probably seen graphics
of this sort in the past where we have a range of cortical regions that are each making predictions
about the others and so here going from going from the right to the left we've got a series of
predictions in as these dark black lines with prediction errors passed back up using these
these dashed lines that then allow us to correct at each level and all this this graphic shows
is a simplification of the graphic on on the previous slide so i want to give you an intuition
for why this is useful using a couple of examples so the first example i'm going to use is one from
the domain of active vision so imagine imagine you're doing a task now where you have to fixate
on this cross in the center and maintain that fixation and show you a stimulus but maintain
fixation on the cross. Stimulus is going to disappear and then your next task is to perform
an eye movement to the location where the stimulus appeared. It's a very simple task if the sort was
used frequently in monkey electrophysiology and all throughout neuroscience but it's an interesting
one because it has several different components to it that i think help in terms of thinking about
the utility of hierarchical models so one aspect of this is making decisions about
where you're looking at each of those time points and making inferences about which
of several alternative locations you're going to perform an eye movement to so simple form of
planning here so that calls into into action the sort of discrete state space model that we were
dealing with before this this partial observable decision process this set of discrete sequential
timing points and that's often very highly relevant structure of most simple tasks in cognitive
neuroscience. However we also need to actually move our eyes we need to we need to move our eyes
from one location to the next we need to know how to generate forces we need to know how to make
sure the eyes come to rest in the right location given those decisions and that calls into play
this sort of continuous state space predictive coding style model so how do we then how do we
then combine the two how do we deal with the situation where we want to predict a particular
location at a discrete level one of several alternatives but then also map that to a continuous
motor trajectory and this is where generative models get a little bit more complicated so
taking this a bit at a time what we've got at the top here is our mark-off decision process model
this is our discrete state space model in discrete time now at each of those time points we're no
longer predicting data specifically we're now predicting a short trajectory formulated in terms
of our continuous state space models so at time one we're now going to predict a short element of
our trajectory and this is very much like the sort of clustering model I showed you much earlier
that you take a discrete point and you map it to a point in continuous space with some probability
at the next time we then predict the next bit of that continuous trajectory so our priors for our
continuous model are now inheriting from the predictions from the discrete state space model
and then we can do the same thing again at the next time point and and so we can create a discrete
sequence from our our sequential model and at each point in that sequence associate that with a
short trajectory in continuous space that then predicts our continuous data allowing us to then
predict the proprioceptive information we would expect to get from the eyes when we're performing
a task at that sort and you can see that we now develop a hierarchical structure also in thinking
about the relationship between the bits of the message passing scheme so here we've got the
discrete bit of our message passing scheme our discrete microcircuit and a bit of the continuous
microcircuit here and the interactions between the two of those were making predictions from the
discrete one to the continuous one and then passing errors backwards to allow us to to update
the discrete element so interpreting this again neurobiologically you can imagine
that say we have some element of the cortex perhaps the frontal eye field that's making predictions
about one of several alternative locations I could direct my gaze we can then map that
via sort of output of the basal ganglia and the predictions about what policy I'm going to pursue
through to areas like the superior colliculus which then might make use of predictive coding
style networks of the sort depicted here to then generate eye movements as we've seen before
and just to show you that in action we imagine that these are population of neurons that at
each time point are trying to predict one of several in this case three different locations
at four different time points we can then map that through structures like the superior colliculus
and this is supposed to sort of cartoon the idea of a population code in the superior colliculus
but then results in the eye movement to each of those discrete locations
putting it all together we can then formulate exactly the task that I asked you to perform
earlier and see the result of an active infant scheme performing that task
and so here you can see it performs it very successfully by predicting sequences of eye
movements that are conditioned upon the discrete part of the model and the idea of maintaining
this belief about where the stimulus appeared and using that then to direct my policy selection
and eventually the movement I select I want to give you one more example of this sort of hierarchical
scheme in the domain of motor control and appeal to the same sort of simulations we were looking
at earlier in terms of the reflexes exhibited by that arm the generative model we're going to use
here is a slightly complicated one but here effectively involves transitions between different
locations that an arm could be in or a hand could be in which then predicts different locations in
some continuous space and different locations that a target might be in and that will then allow us
to make predictions about about given where some target is in some continuous space which decision
I'm going to make about the alternative points I could move my arm to and so here we have the
simulation where we have the target appearing as the black ball out of the out of the three balls
which is going to change at various points in time and our active infant scheme is now
selecting these locations translating those into predictions of continuous trajectories
and making the appropriate movements such that it reaches each of these target locations
and what's shown in terms of the the graphic here is also an interpretation of the message
passing in terms of the known anatomy of the motor system or part of it
we can perform various lesions to this and see whether they behave in the same way that we might
expect patient populations to so here we've induced a cerebellar lesion which effectively
involves a misestimation of certain precision terms certain confidence or variance parameters
and you see this sort of overshoot and this kind of oscillatory behavior in the way the arm behaves
that mimics what we might expect him as cerebellar attacks here
we can look at the higher levels of the model and actually have multiple
hierarchical levels of the discrete scheme and here what we've done is we've induced a
lesion in what might represent a frontal lobe lesion where now the model is able to perform
all the movements with perfect fluency but every time there's a change of context it takes quite
a while to adjust to that new context to deal with a new situation you'll see that instead of
moving straight there as it was before there's a lot more hesitancy and a lot more difficulty
constructing that long-term narrative because we've broken that slower hierarchical level
and here a final lesion that we can introduce is one that reduces my confidence in policy
selection so here this effectively makes me very uncertain about what I'm going to do next and
here we get a sort of a kinetic type picture and sort we might expect in a Parkinsonian type syndrome
so I realized that was quite a lot and I hope I'm not too far over time but I'll just try and
summarize briefly the key ideas that we've discussed so the first was thinking about this
idea of climbing probability gradients the idea that to maintain some form over time
and to persist we need to effectively be climbing these probability gradients all the time
we interpreted those probability gradients in terms of in terms of Bayes theorem and connected
that to the Bayes in brain and the sort of message passing that might be involved in
forming inferences about elements of some generative model and the role that might have
in cortical micro-circuitry we also discussed the role of action and the idea that that one way to
make our model better fit the world is to act upon it and one way to do that is to simply reduce
any discrepancy between the predictions from that model and the data that's coming in and the
structure of that is very similar to the idea of a reflex arc as is well known throughout
motor neuroscience and the final thing we thought about was how we can then construct hierarchical
models that allow us to produce simulations and theories that deal with much larger scale
networks in the brain and take a full sort of systems level view of how particular movements
and particular behaviors are generated through the inversion of particular forms of generative model
with that I'd like to thank many people who've been involved either directly or indirectly in
this work and I'll mention the book again which was mentioned at the beginning and a lot of this
talk was loosely following some of the structure in a couple of the chapters in the book so if
anybody's interested please do take a look I'm more than happy to answer any questions thank you for
your attention

So, today I have the pleasure to introduce Professor Ryasmith from the Larrouette Institute
of Brain Research, the University of Tuzla, Oklahoma in the USA.
Ryasmith has received his PhD in Psychology from the University of Arizona in 2015 and
worked as a postdoctoral researcher in Arizona in the lab of Professor William Kilgoe until
2018.
He then spent a year in the lab of Professor Carl Friston as a visiting researcher and
in 2019 he is a member of the Larrouette Institute of Brain Research where he is still working
as a principal investigator.
Ryasmith is the author of many publications in the field of effective neuroscience, neuroimaging
and models of the brain and he offered a great contribution in the development and application
of the free energy principle and active inference framework which are the main topics of this
semester's seminar series and together with Carl Friston and Christopher White he recently
published a fantastic tutorial on active inference that I would like to recommend to anyone that
is approaching like me and many of us to this topic.
So Ryan we are really glad to have you here today and I'm really looking forward to your
talk.
The mic is yours.
Okay well yeah thank you very much for the kind introduction I'm super happy to be here.
I do have basically exactly an hour worth of material if I can even get through all of
it so I'm just to prepare everybody and feel free to just you know kind of cut me off if
I start going too long but I'm hoping I can get through as much of it as possible.
So let me just share my screen here, make sure this all works okay okay so yeah so like
I said so just kind of jump in here my talk officially is titled active inference as a
computational framework for modeling empirical data and the reason I say that is just because
my lab mainly is focused on trying to apply active inference models as a way of essentially
studying individual differences in behavior and largely in computational psychiatry so
and in clinical populations so part of my goal is to kind of illustrate how active inference
can be used to model empirical tasks that can then allow fitting and then you can study
individual differences in particular computational mechanisms so the general kind of outline I'll
try to give a brief kind of conceptual introduction to active inference and then I'll introduce
you and walk you through the formalism which can initially be fairly daunting but I'm hoping
that kind of by the end it'll it'll seem a lot more comprehensible so that'll include the way
that perception relates to variational free energy minimization and how that relates to
prediction error minimization. We'll talk about policy selection and expected free energy minimization
and then hopefully if I have time we'll get to active learning and then I'll briefly kind of
touch on empirical applications throughout so kind of highlighting how active inference can make
both behavioral and neural predictions that can be tested in empirical studies and I should just
say right from the bat right off the bat there's a way too much packed in here so I'm gonna point
you kind of throughout toward further resources as we go and since this will be recorded I'm hoping
people can kind of go back and and see those pointers to those so first you know what is active
inference generally so I think it's important to recognize from the start here that active inference
is a term that's actually been used in multiple ways so the term was actually initially used to
refer to a theory of predictive motor control starting around 2013 a little before which was
essentially an extension of prediction based models like predictive coding extension of those
sorts of models of perception to the idea of motor control through prediction and currently
however active inferences is more often used to refer to a different but related theory which
is a theory of predictive decision making as opposed to motor control and that's what we're
talking about in this lecture and both of these theories are grounded in Bayesian inference or
probability theory one is just about deciding what to do which is what we're talking about and
the other is just about controlling the body to enact a decision once it's made and if people
want more details on kind of the historical walkthrough and distinction between those in
more detail this is just one paper that we're just gonna put out that the beginning portion describes
that and makes that distinction or we tried to make it as clearly as we could but the basic idea
with active inference is that agents aren't simply passive observers of their environment and so
they actually they actively infer the probability of future observations given the different
possible actions that they might choose so you might start out saying okay if I go inside I predict
that I'm gonna feel warmer and then you combine those with predictions you combine that with
preferences so I want to feel warm therefore I will go inside and then the other thing that's
useful about active inference is that it also models agents making information seeking actions so
they gather information when it will be useful so for instance I'm gonna turn on my flashlight
right now because I predict this is gonna help me figure out how to get inside so all of those
things are kind of encompassed and it's like active active inference as opposed to just passive
inference now just to kind of introduce a few kind of initial starting variables here so first
we have observations oh and then we represent hidden states with the variable s and the idea is
that those are hidden because they have to be inferred from observations and then we have chi
which stands for policies which are sequences of actions where each action is represented as
you and the idea here in the figure above is we want to distinguish between the generative model
and something called the generative process and the generative model here is represented by this
joint probability of essentially it's all the it's the probability that you believe is associated
with each possible combination of observation states and policies and what that allows you to do is
kind of work out what you think will happen what you think the probabilities of different observations
given your beliefs about states and what action you choose so you might say okay what might I do
to stop this flame that I think I'm perceiving over here well probably the best policy or the best
action is just to blow just blow out the candle now there's a difference between that and the
generative process which is just the actual objects you know objects and events that are that are
out in the in the environment itself so where the generative model here is the beliefs about those
things and that could be inaccurate but there is some true states and of the world that are
generating real observations and the brains is trying to model and get a handle on those the best
as possible so this is a common depiction and the common very daunting depiction of active
inference that you'll often see in papers on the left here is the graphical model depiction in
the right is all the equations for inference to solve this graph and so we'll walk through each of
these steps in detail in a second but just to kind of motivate this I want to start out by
kind of illustrating what what active inference is actually good for to motivate why you might
want to learn it and understand it and so first is it incorporates perception learning and decision
making all within a within a single model so it allows the framework to be applied to a really
wide range of problems including perception tasks or enforcement learning tasks planning tasks among
others and the equations that I just showed that are used for inference are fully generic across
different generative model architectures so really you just need to come up with the right generative
model for whatever you're trying to model and then the the same exact equations will perform
approximately optimal inference for whatever the generative model structure is so the task is
just to figure out the right generative model structure to to simulate a particular cognitive
processor behavioral task so the other thing is it's motivated by biological plausibility so in
other words there's clear ways in which neural networks can implement all the linked equations
in the model and it has an accompanying neural process theory which is often depicted like
this with these little kind of ball neurons and synaptic connections that I'll walk through this
in a minute but the point is for well later in the talk but for the the moment the idea is just
that it provides the opportunity to make and test precise hypotheses in neuromaging studies so
including predicting ERPs in EEG studies or localized neural responses in fMRI studies and
then the last thing that I like to highlight is that it provides a unique approach for modeling
the explore explore exploit trade-offs so it kind of helps answer the question or models the agent
trying to answer the question of when do I seek reward immediately or when do I first seek out
information so I have a better idea of where the rewards are so it's a useful framework for
modeling behavioral tasks that involve this sort of information seeking and planning component
now just pointing you to some further resources this is a paper that we wrote recently that is
just kind of a review of recent applications and then I'll just point you also to a number of
empirical studies that we and others have done so these are a few that we've done modeling and
fitting models to be empirical behavioral data on information seeking and learning it's a couple
of papers that we've published looking at planning behavior and then there's a couple studies not
many today looking at neuroscientific predictions and testing those against either EEG or fMRI data
so so the idea is that it's already you know there's already it's already somewhat established or
starting to be that this is useful for like practical scientific purposes and so as they
mentioned we put out this tutorial recently that we hoped would make some of this material clear
and more accessible for people who want to use this modeling framework in their in their own
studies and so a lot of what I'll be talking about going forward will draw pretty heavily on this
some on this tutorial so I just want to do acknowledge it here and my and my co-authors on
it and as I just another place to look for more details and so in the formalism for active inference
and it's based on what are called partially observable Markov decision processes or POMDPs
and the idea here is that it's modeling hidden states of the world that evolve over time and as
I said the agent doesn't have full knowledge of the environment so it has to infer states
from observations and then it has to also infer the policies or action sequences that are most
likely to generate preferred observations based on current beliefs and I just want to make a brief
clarification for anyone who does reinforcement learning at all that the term policy is actually
a little different in reinforcement learning vector versus active inference so in reinforcement
learning or model three reinforcement learning policies often refer to mapping from states to
actions so for example just rules you know if I'm in state one then I'll go left or in I'm in state
two then I'll go right and active inference of policy refers to a possible sequence of actions of
some preset depth some preset number of actions so for example one policy might be I will go left
then left then right whereas another might be I'll go right then right then left so the idea is an
active inference you're you're evaluating the the whole sequence of actions in terms of whether
will lead to the observations that you want so just an additional notation here and just as
another kind of explicit illustration of the policy idea if we have you know in this maze we
might have kind of the starting position in this goal one policy might be this whole sequence of
10 actions whereas another policy might be this whole sequence of 14 actions in this case and the
first one to be favored here because it gets you there more quickly so the idea is that we're
evaluating everything under policies and active inference because the goal is just to choose a
policy so prior beliefs in Bayesian inference which I've been told everyone here is you know ought
to be familiar with Bayesian inference so I'm not gonna give an intro on that but priors over
states instead become priors over states conditional on policies so the estate I expect to be in if I
give a if I choose a given policy and then same thing for likelihood mappings so the observations
I expect if I'm in a state and I choose this policy same thing posterior beliefs it's just the
probability of states given some set of observations and a policy and that's often the kind of thing
as we'll see that we want to figure out in perception and then we can just solve that with
Bayesian theorem in the standard way so and then we have predictive posteriors over observations
which is going to be really important for policy selection as we'll see and so those are the
observations I expect if I choose a policy and then another thing that's fairly unique in active
inference is this idea of a prior preference distribution so probability of observations
given C which is just this variable that we use as a matrix essentially that encodes what we what
we want what observations we want over others and so higher probability here just indicates that
something's more rewarding or more desired so like I said this is a unique element in active
inference because it's using something with the form of a probability distribution to encode
relative reward values another thing to note here for people who aren't familiar with variational
inference as an approximate Bayesian inference technique we use the variable Q to denote
approximate distributions because exact Bayesian inference is just often very intractable in real
world cases so Q is essentially kind of like our best guess about whatever belief it's indexing so
here Qs given pi would be our approximate posterior over states and we would want to get that to
match the posterior the true posterior as closely as possible and Qo given pi would be our
approximate prediction about what we'll observe if we choose one policy over another and the last
kind of thing that I would want to introduce for anyone who isn't familiar is the concept of a
KL divergence and the idea here is just that smaller values for a KL divergence indicate that
two distributions are more similar and this becomes really important in action selection and
active inference because in this case the closer we can get Qo given pi with Pio given C the smaller
the KL divergence will be which means the probability is just higher that we'll get what we want so to
give you kind of a practical example so take Qo given policy one and whatever our preference
distribution is if the distributions look like this so this red distribution being the expected
observations given a policy these are pretty far apart so that would be a large KL divergence
whereas if we take policy two in this case the predicted observations under the policy are much
closer to the preferred observations and so that's kind of a smaller KL divergence and so that's
going to be a policy that has a higher chance of being selected by an active inference agent
so now to kind of get through the graphical model itself to kind of parse this so it's more
comprehensible so here arrows denote conditional dependencies circles correspond to random variables
that are updated during learning so in this case we have our s's here and our pi's and these are
just these stand in for Qs and Qpi so our approximate beliefs the squares indicate fixed
parameters which I'll walk through but these are the things that are kind of fixed within a given
trial but that are updated more slowly through learning so as I mentioned this can be a bit
dawning but when we walk through it step by step I think you'll find that it's it's more
comprehensible so this bottom part here the states and observations is what roughly corresponds
to perception in active inference models whereas this whole part on the top involving policies
and the things that policies depend on is the action selection part so we'll start by just
walking through the static and dynamic perception part of these models so that bottom part and to
do so I'll use a concrete task as an example and this is what we used in the tutorial so in this
task the agent has to choose between one of two slot machines and if they choose the right one
then they'll win four dollars whereas the other one will pay out zero dollars and if they guess
right away then they get the four dollars but they can also choose to ask for a hint so this
like information seeking action and if they do that then they'll find out which one is more likely
to pay out but there's a loss involved because then they can only win two dollars so it is
specifically this kind of information seeking versus direct reward seeking trade off and the
perception part is just going to involve this observing the hint in this case so one hidden
state factor in the model is just going to be which machine is better the left machine or the
right machine and the outcome or observation modality here is going to have three possible
observations just no hints the hint that the left machine is better and the hint that the right
machine is better so that's we're going to use to model perception in this case so to start out
we'll just pick kind of the smallest little unit in the graphical model where we just have
observations hidden states and then we have a which is the likelihood so the probability of
observations given states and then we have D which is just our prior belief about states so
observations hidden states prior beliefs about states and the likelihood mapping and then we
can solve this just by this is just a way of encoding based theorem essentially where it's
just here's your prior here's your likelihood and then you softmax this just to turn the
result back into a probability distribution and so just to give a concrete example here and in the
in the context of the explore exploit task we can start out just by assigning an equal prior
probability to L here which would the left machine being better and are here for the right
machine being better and then we can and then we can say our observation is a one and here a one
and a zero and that just encodes in this case that the the hint that the left machine is better
and if this is our likelihood function then this observation here would pick out the top row
which is just saying that so left machine has a point nine probability of being the case given
that the you got the left hint or I said that backwards but but the probability of the left
machine better is point probability that the left machine is better is higher under the with the
left machine hint so we can just use those probabilities and to figure out the posterior
overstates here and just to show you how you would do that just to carry out that equation
we end up with this which then once you softmax it becomes that which is just saying that now
there's a point eight two probability that the left machine is more likely to pay out
so moving on from that to dynamic perception now requires that I make a kind of distinction
that's that's been a little tricky often when people are learning active inference to start with
and that's the fact that there's different types of time in active inference so the first notion of
time is indexed by this Greek letter tau and that's the time about which I have a belief so
for instance I might believe right now that I'm in a car but I might believe that I was in the
kitchen 10 minutes ago and I might believe that I will be at work in 20 minutes so those are
beliefs that I have about three different times but I have all of those beliefs right now in the
present so T is the time at which I have a new observation so the belief at which I have all
these beliefs about the different times so here we might say after turning on a light so I make
some observation that say T equals at T or at yeah at tau equals T then I believe now that I was in
the kitchen for the last five minutes so whatever tau minus one was so in other words I can get an
observation later and I can update my belief about where I was at an earlier time although I
didn't know that before turning on the light so I can update my little belief about all towels
the states at all towels with each new observation at each T so hopefully that's helpful and note
again that the variable Q here is just denoting our our approximate belief for best guess like I
mentioned before so and then the other thing and I mentioned this already but there's a different
thing between there's an important distinguish different state factors and outcome modalities
and the idea is that different factors can correspond to different types of states so like
one set of states might be beliefs about an object's identity whereas another set of states might be
about the object's location or one set of outcomes might be about vision and another set of outcomes
might be about audition so we separate different types of states into factors and different types
of observations or outcomes into modalities so so here when we're modeling dynamic perception
as a hidden Markov model all we're doing is we're just taking the single time point inference here
that I showed before and just kind of stringing them together and then we're connecting the states
over time with these B matrices here that just encode the probability of moving to some state
given that I was in some state at a previous time so we encode these are just the probability of
state transitions and these are essentially you can think about them as providing the prior beliefs
for states for all tows greater than one and the equation for solving this in active inference
is essentially the same in form as the one for the single for solving it for the single time
point the only difference is now so the likelihood here is exactly the same the difference here is
that our prior belief now includes for tau equals on now includes our beliefs from the past and also
beliefs about prior beliefs about where we'll be going in the future and and for tau greater than
one it's the exact same thing except for instead of D here our prior from the past is just the B
from the previous time point and our prior from the future is the same as it was here
so this idea of prior of having prior beliefs from the future is also something that people
often find a little confusing at first but this just goes back to this t versus tau distinction
I mentioned before where this allows you to update beliefs about earlier time points when
you're getting observations about later time points so this will allow you to kind of propagate
when you get an observation at time two will allow you to kind of propagate beliefs back
and update your beliefs about where you were at tau equals one now solving this now requires
because we have to do it with approximate inference gets in gets into message passing
algorithms and in this case with active inference the main way that we tend to do it is with
variational message passing or or an updated version called marginal message passing but
that's what the equations I just showed you implement and as we'll see in a second this
message passing scheme is actually what the neural process theory in active inference is based on
and so to just go into this the idea is is that message passing is essentially an efficient way
to do approximate posterior inference on a graph and so and it's based on minimizing a
quantity called variational free energy which I'll go into after kind of giving you a practical
example of how this how the dynamics work so essentially what you do is you just start by
encoding whatever observation is one so fixing fixing whatever observation is a tau equals one
and so let's say it's observation the top observation here so we'll just call that observation
one so now what you would do is you'd use that and you'd first try to update your belief about
whatever states are for tau equals one and we do that just by passing these messages so in other
words in this case it's just the log the natural log of the prior belief and the natural log of the
prior belief from the future time point and then our likelihood and we just put those together
and then we update our belief our approximate belief about states of tau equals one and in this
case if I were to do that with actual numbers it would just look like that so we'd update our
belief about s of tau equals one to now be a point five five point four five and then we would take
that new qs and we'd now use it to try to try to update our beliefs about states of tau equals two
and and then same thing we get our qs equals two then we do the exact same thing again pass
those messages and get our updated posterior belief for asset tau equals three and then we
just do that over and over again until eventually this will converge onto a stable updated prior
updated posterior belief about states at each of these time points and and crucially so that's
all based just on making up the first observation so once we do that we can assume that beliefs
have now converged and now we add the second observation in and then we just do the whole
thing again so after the new observation comes in we update our beliefs about all the different
time points so and then we just repeat it again iteratively until we reach convergence
and then we arrive at a final posterior belief ultimately about all time points after all
observations and and you can kind of plot out the way that beliefs change so in an easy inference
problem we have pretty fast convergence so in this in this graph we have six different traces
so it's beliefs about each of the two possible states so left the left machine being better
or the right machine being better we have beliefs about each of those over each iteration of message
passing and what you can see here is this is just showing that really quickly beliefs that the left
machine is better for all three time points converged being one really quickly if I were to
make the inference problem harder by making the transition beliefs more probabilistic for example
then you can see that the inference problem becomes harder and you have slower evidence
accumulation and so it takes a lot longer for the beliefs to to converge so and as I mentioned
before this also represents a possible way to think about the way that messages are passed
between neurons in the brain and this comes in part from the fact that we can think about
message passing or it can be formulated the equations can be formulated in terms of something
like minimizing a prediction error and the way you the way you do that and I won't go into this
in a in a ton of detail right now but you can just kind of shuffle around the variables here a little
bit and make it so that this is the fixed point for this is zero as an error which we represent
like this and then we're just moving the s over here to the end and then message passing is described
in an identical way except for now that what it's doing is kind of every time you iterate it it's
moving toward this error being as low as possible and the reason it you can think about it as a
prediction error is you have whatever over here the generative model after an observation and then
you have your new approximate posterior river states and the difference here is essentially
is essentially the error signal because you're trying to get these to match as well as possible
and then you can define a kind of membrane voltage that would be associated with with the
activity in a given neuron with this variable v where v is just the log of this posterior
state so it's just identical to this and and that can take on continuous values like a membrane
voltage whereas softmaxing this so normalizing it then becomes your beliefs about states
which can then be thought about as a normalized firing rate which can only take values between
zero and one so that's kind of the the really basic idea about that first part of the neural
process theory is just you can you can formulate message passing on a graph like I just showed
you in terms of a prediction or minimization process and there's a nice paper by Thomas Parr
I guess who's going to be talking to you at the next meeting showing how just illustrating how
you can connect a pretty small number of neurons together to to implement this just as a kind of
concrete illustration where the the state probabilities and the error signals are just
represented by the activity in a set of neurons where each one each pair of neurons corresponds
to beliefs about each moment in time so each tau whereas the conditional probabilities so the
likelihood and the prior beliefs are just encoded in the synaptic weights essentially the strength
of the excitatory and inhibitory connections between each of these neurons so that's kind of the
simplest aspect of the active inference the neural process theory and active inference
so to kind of illustrate how this works practically in the in the task so let's just say
that I have completely precise prior beliefs about transitions so if the left machine is
better at time point two then it will still be better at time point three so this is all this
is saying it's just probability of left at time point two is going to be is also going to be 100
probability of one at time point three so so if we do that then well what I'm showing here is just
a set of graphs here where for tau equals one tau equals two and tau equals three these are the
posterior beliefs that the left machine is better and the right machine is better and here's the
observations and so darker here is going to equal higher probability and the cyan dots here I'll
show you just correspond to whatever the true state or outcome or action is depending on the
specific plot but so let's say we start out and there's a null observation so it doesn't get any
information so its beliefs don't change at all everything just stays flat and gray but now at
the second time point it observes the the the hint that the left machine is better so now
the update the beliefs update and note that they update for each time point right so and now it
believes that the left machine was better at tau equals one it believes that it is better right
now at tau equals two when it got this observation for t equals two and it also now believes that
the left machine is better at time point three or will be still be better at time point three which
is when it would make the choice about which machine it would choose and then it doesn't make
any it just makes the null observation here but the beliefs basically stay the same about all time
points and this is just noting how the beliefs about both the past and the future change
but it makes the observation at time point two um so now if we were to do um if we were to just
plot this um each of these rows the way that um I did before um literally you can just treat
as I mentioned each of these each of these traces as corresponding to predicted firing rates um and
then what what leads um active inference to be able to make material um empirical predictions um in
terms of EEG studies is that you can um you can just take the those voltage um predictions or or
really it's something very equivalent would just be the rate of change in the posterior beliefs
and that will generate um ERP predictions um and what this basically saying is just the faster the
neural uh the neural firing rates change the larger the ERPs will be um and just to show how
that can differ given um different belief precision um we can make it so that the priors over states um
the transitions are um are less precise so this version of a B matrix would say that if the left
machine is better at time point two it'll still be better at time point three but only with probability
of point seven right so the left being better transitioning to the left being better is only
point seven um so in this case if I do the exact same thing then when it gets the left hint
now it's confident that the left machine is best at tau equals two but it's no longer that confident
that it was best at time point one and time point three because the probability the transitions are
only point seven so that's what makes the the difference there um when you have different
prior transition belief precisions
and again same thing with the neural process theory um we get precise changes for the traces
for tau equals two but much less precise for tau equals one and tau equals three um which is going
to predict a very different pattern of ERPs in an empirical study
and there are a few papers now that either in simulations or in empirical studies
that have shown evidence that these predicted ERPs match up well with empirically observed ERPs
which is promising but still there's only a few studies so far
um and these are just examples of the ways that these traces kind of match so like here's an
empirical PR ERP in an inattentional blindness paradigm and this is what a simulated ERP
looks like when you're modeling the same task
so the the next kind of important thing so I mentioned that um I mentioned that this message
passing um slash prediction error minimization sort of scheme was based on minimizing variational
free energy um so now I now that you have a sense of other dynamics work um wanted to walk
through um kind of explain formally what variational free energy is um so the idea is that you're
starting with some generative model and in this case I'm not including policies um just for for
simplicity so in which case your generative model is just your joint of observation probability of
observations in states which we know you can just represent as a conditional times marginal here
um and then what we're doing with variational free energy is we're um starting with a kind
of arbitrary qs that we somehow want to get to approximate the true posterior as closely as
possible um and then we formulate f like this so and what you'll note is that there's this part
here and there's this part and this part um is fairly intuitive in terms of the way that f
minimizing f ends up minimizing a prediction error um and the reason is is because this is just
putting our approximate posterior over our true posterior um so it follows that the closer that qs
gets to ps given o um the the smaller this whole thing will be right so in other words f gets smaller
as our approximate posterior comes to match our true posterior um so it's just the difference
between the approximate and the true posterior and then what's left over is just the log probability
of the observations under the model so just how surprising is the observation under my model in
general which is um equivalent to the the model evidence essentially how good is your model at
predicting observations overall um so as f approaches zero um or f approaches zero as your
approach as your posterior beliefs become more and more similar to your uh to the true posterior
um
and just to kind of show how that works in this simple kind of example um if I start out with
a prior belief that's just flat 0.5 0.5 and I have a likelihood of 0.8 and 0.2 um I can calculate
my joint probability here so just 0.4 and 0.1 um and this um step of calculating the marginal
is what's intractable in real world cases but if I were to do it I end up with a posterior of 0.8
0.2 so with this approximate inference with variational free energy what I would do is I just
start my qs my best guess at just 0.5 and 0.5 and then I can calculate an initial f value um so
here just based on this version of the equation for um variational free energy um which is just
a kind of uh shuffled around version of the variables from the one I showed you before um
I can calculate it as 0.916 um and then I can just move qs a little bit so now it's point
try it at 0.6.4 and I can see that f gets smaller so then I can keep moving it in that direction so
0.7.3 f is smaller still um and then when I get to 0.8.2 then f is 0.693 um and if I try to move it
farther to 0.9.1 then you'll see f goes up again um which means that this is going to be the value
of qs so the guess that best minimizes f um which in this case corresponds exactly
to the true posterior um but it won't often um match it exactly in um in many real world more
complex cases um so that's so that's the idea um now another way that you can represent variational
free energy that um that kind of brings out um part of why it's useful um aside from just being
attractable way to do Bayesian inference um is you can represent it as a kale divergence between
your approximate posterior belief and um and uh the um prior here your prior overstates um so
essentially this is saying how much do beliefs need to change so if I move from my prior belief to
my posterior belief and that requires a big change then your kale divergence will be large
um so we can think about it as complexity in the sense that you're having to move your beliefs
change your beliefs a lot um to um to get to your best guess um whereas this would just be
the prediction part so this is just the probability of the observations that I'm giving given the
states um in my model um so this is just how accurate are my predictions um and you can just
flip this around and just think of this as complexity plus prediction error um then because
again this is just about the accuracy of your model's predictions um so in other words to minimize
f we're trying to minimize complexity so we're trying to move our posterior beliefs as little
as possible from our prior beliefs um so the simplest change in beliefs um that we can do
while also maximizing accuracy so adjusting beliefs to make the most accurate predictions
possible um so this is another way of seeing how convergence to q s in our in the message
passing scheme we talked about before um can be understood as prediction error minimization
um and this is probably familiar to a lot of people but sometimes the question comes up
you know why worry about this complexity part why not just maximize accuracy um and the reason is
that the more complicated your model becomes the easier it is to overfit you know so in this case
I can come up with this really complicated model that will predict each observation you know
like perfectly but it's going to be really bad at predicting new observations so you
so a model like this is kind of in the middle um that um requires uh simpler uh simpler change in
beliefs um predicts it pretty well but is a lot more likely to um to actually generalize to
making accurate predictions about future observations so that's the motivation for the
complexity being useful um so now that we've talked for a fairly long time now about how
the perception process works in active inference um now we want to move on to the actual action
component um and here the idea is that action is just about control over state transitions
so essentially what we do is we have um one of these sets of transition beliefs over time
for each policy um so each policy is a possible sequence of actions so like choosing to take the
hint and then choose the left machine or just immediately choosing the left machine um and
what that corresponds to is different types of transitions so different B matrices essentially
um so each policy just entails a different sequence of state transitions um
um so like transitioning from the start state to the take the hint state or transitioning from
the start state to the choose the left machine state for example um so what we're doing then
is we're predicting future observations under different possible state transitions so if we
get this when we get this observation one we are we're trying to now after we update our beliefs
we're trying to now predict based on our current beliefs what observation two will be before we
get it um so you can actually think about this as just performing message passing in
parallel for a bunch of different um possible hidden markup models the transition models I
showed you before so one policy could just be this model where we have transition uh beliefs
you know B1 and transition beliefs B2 and then a second policy could just be one that has B1
and then a different set of transition prior beliefs so B3 um and so in the context of the um
of our kind of example model here this just requires that we add a second state factor
so our choice states um and then a second outcome modality where we can observe losing or winning
or a start state um and so the way that we would do this um is that so we did the exact same thing
as before um where we would make our observation here and we're now we have the second outcome
modality that corresponds to losing and winning and so now we have this preference distribution
where darker is higher probability this is basically saying I prefer to have a win versus a loss um so
now when we make that hint observation um we that's after we choose the hint state um and then we
update our beliefs now that the left machine is better so now our belief are we choose the
transition to the choosing the left band at stage or the left slot machine state um and then that
results in us observing a win um so that's adding the the action component so in other words the
agent just took the hint and then shows the left machine um and just to show you how this is encoded
for example one b matrix encoding uh the action of choosing the left machine would just you know
look like this right so just given any possible state I'm in I will transition to this third
state which corresponds to choosing the left machine um and then that could be for choosing the
right machine that could be for taking the hint etc so just to show you how it would actually be
encoded um so now policies themselves in these models depend um on the expected free energy
which is essentially evaluating how good is each policy and that in turn depends on c which again
is our preferred observations or or what corresponds to reward um and so the what
expected for energy ultimately amounts to is stating that the policies with the the lowest
expected for energy are those that are expected to generate preferred outcomes um while at the
same time maximizing information gain um and just to show you if you were to specify a particular
c matrix here um it might look like this um in actual models where this is tau equals one two
and three um and you might say put negative ones here right so I don't want to lose and then
certain sorts of positive numbers here I want to win with value four at time point two and
but I'll only win two dollars uh at time point three if I take the hint first um and then you
adjust softmax it and then log it and what we actually use are these log probabilities
um and I'll just note here that um this value four here we could actually vary this or it could
be something we fit in an experiment um which corresponds to what we can call preference
precision um and we'll see that higher values for that end up reducing information seeking and
increasing um reward seeking um so just to kind of go through what expected free energy is so the
variational free energy pertains to current observations um but remember that decision
making requires making um predictions about future observations under each policy and we can't
calculate f um without the observation um so what we're doing is we're taking our predicted
observations under a policy um and then we're calculating the expected free energy associated
with each policy based on those expected observations um now the the literature shows a
lot of different decompositions of expected free energy um the ones that are probably more
intuitive um are one is this one which is the epistemic value and pragmatic value um version
um and this just kind of to walk you through it this is just saying um this is um our beliefs
about our approximate posterior beliefs about states um before making an observation and this is
our approximate posterior beliefs after making an observation so this is just saying how much do I
expect beliefs will change after a new observation if I choose this policy um and note that there's
negative here which means that the more I update my beliefs so that the more my beliefs change um the uh
the the lower the expected free energy that will be so this is actually maximizing information gain
so you want your beliefs to change more um and then our pragmatic value is just the probability
of the preferred observations um so a higher lower expected free energy is going to maximize
change in beliefs so maximize information gain while also maximizing the probability of uh
of preferred outcomes or reward um the other common decomposition you'll see is this risk and
ambiguity um version and here this is just the kale divergence between the observations you expect
under a policy and your preferred observations so again this is just trying to get preferred
observations as close as possible to expected observations um and then the and then this is the
ambiguity or the corresponds to the entropy um which is a essentially a similar way of encoding
just another way of encoding how informative I expect observations to be so here for example
if in my likelihood state one will generate observation one point not with point probability
point nine and observation two with only point one whereas this one state two will generate
each with equal probability then these observations aren't very informative so I'd be driven toward
choosing this one because it can actually disambiguate states um and so this risk seeking term is really
just what I showed you before where you're trying to find the policy that gets the expected
observations to match the the preferred observations as closely as possible um so the idea is that
this this actually provides a principled approach to arbitrating um the explore exploit dilemma
where policy selection is initially information seeking so the ambiguity term dominates
then the agent ends up levering that information to bring about preferred outcomes once it's
confident um so just to kind of review so each policy in active inference entails a different
set of state transitions um and that in turn predicts a different sequence of observations
and those jointly allow evaluation of the expected free energy um and just to show you kind of in
matrix form this is um what it would look like um just based on the actual variables and then our
probability of choosing each policy is just um just normalizing or softmaxing uh negative g
so a lower g pi then just corresponds to a higher probability of choosing that policy
and then finally what we do is we um we often use this alpha parameter which is like an action
precision which just controls how likely you are to choose the action associated with the best
policy so it controls the kind of the randomness and choice um and as I mentioned when the precision
of the preference distribution goes up that ends up making people more uh risk seeking
and just to show you an example of that um this is what it looked like as you you know as you saw
before the agent takes the hint when the preference distribution is moderate um whereas if we made a
very precise preference distribution then instead the agent will typically without being confident
which one is correct just choose one of the machines right away um and in this case it it loses
so in this case it took the guess immediately and lost um so now the final um bits that I'll
walk through that are more just kind of like the some some bells and whistles that can be added but
don't necessarily always need to be added um an active inference models so one is this e um matrix
here or it's a vector an e vector and um it's just a way of encoding habits um and habit learning
so it's just a prior belief about what policy you'll choose um and then gamma is um is the expected
precision of the expected free energy um so it's it's an inverse temperature parameter that um
basically what it does is it just encodes the confidence that an agent has in its action model
so what ends up happening is is that if gamma is low so if the expected if your confidence in g
is low then that ends up increasing the influence of habits um or making your behavior more more
random and that's just encoded like this so if this number is lower then g becomes down weighted
and habits end up having a stronger effect um and just to show you this is just based on um
there's a hyper parameter um on gamma which is this beta parameter which is a rate parameter in a
gamma um distribution um and we just take the expected value of gamma which ends up just being
one over beta um and that's just that's just how you have a prior on your expected for energy
precision um uh one other thing that that often gets talked about in the um neural process theory
associated with active inference is that um there is a a scheme that um has been proposed for
how gamma your actual confidence in your expected for energy can change after new observations
I mean I won't go through this in a ton of detail but the reason I mentioned it is just that um
it's actually been the part of the neural process theory that um you know Carl Friston um has um
proposed um as a as something that could correspond to phasic dopamine responses
um and I'll just show you this um you know briefly to to go just uh and point you toward
the tutorial for more details but all you're essentially doing is you're just taking what
your beliefs were about policies before and after you get the variational free energy associated
with the observation and then um this is the actual update equation but essentially all this
is doing is it's just saying how consistent were my observations with my prior beliefs about g
so if the observations I got were very different than what the expected free energy entailed
then my confidence in the expected free energy goes down um and then I can just update beta which
then updates gamma um and so in this case just to show you you know if say in this case the
agent expected that taking the hint was the best policy and then after receiving the hint it became
more confident um and so in this case that was evidence that g was reliable and so gamma goes
up and that would be the um the simulated phasic dopamine response and then the blue here would
be the tonic change in expected change in dopamine which is this tonic change in in gamma
um and these are just uh if people want to see them a couple worked uh examples in this
tutorial um for calculating this but I'm just kind of pointing you toward the tutorial for that um
and there is one study just to point it out um by Philip Shordenbeck and um and colleagues that
did show that gamma updates simulated gamma updates um correlated with neural responses
in several regions including the midbrain um associated with dopamine but the theory otherwise
remains to be um remains to be thoroughly tested um but this also leads us to um the um the expanded
kind of full neural process theory um where the um the neural activity and the synaptic
weights are no longer just about states and predictioners instead they also encode the policy
probabilities predicted observations under policies the state probabilities under each policy
they expected for energy preferred observations um and also gamma and the uh and the uh prior beliefs
if they're included um now probably I'll um I was going to show you some examples of um how
exploitation so information seeking kind of reduces with learning um I guess I probably won't
go into that for sake of time but we can go back to it um but this is just kind of showing
essentially different behaviors if uh with a risk averse agent with a smaller preference
distribution versus a more risk seeking agent um and um again I can go back through this um maybe in
the um in the Q and A if people are interested but the the key point here is is just by um
by simulating these sorts of behaviors um over time we can also generate predicted neural
response time courses both for the the dopamine predictions um the the gamma and also for RERPs
and how those would change over trials in a task um so the the very last thing I'll cover just in
the last couple of minutes here is um is active learning um and the idea here is that there's
actually two different types of exploration in active inference so one is state exploration
which is what we've been talking about so seeking out information about hidden states um so figuring
out by taking the hint which of the two machines is better for example but there's also something
called parameter exploration and that's where you're actually seeking in for seeking um information
to update your beliefs about the parameters itself so in this case like updating your beliefs about
whatever the the reward probabilities are or or figuring out how confident how how good the
hints are for example so be learning to change your beliefs in in say like the A matrix um and
that's driven by an added term to the expected free energy when learning is included um and it's
based on using um what are called Dirichlet distributions um which are just um in Dirichlet
categorical models these are just priors that are often used on the um with categorical distributions
which which are the kinds of distributions we've been working with in my examples um so to skip
that um but basically the um what the Dirichlet concentration parameters do we just represent
them with small a's and these are essentially just beliefs in this case about the probability for
each entry in the A matrix so the probability of losing given you know choosing the left machine
etc um so and these are counts um basically every time you make a new observation then you just
add a add a value so you just increase the value of each of um of whatever one corresponds to the
combination of observations and states in that trial um so and this is the the equation for
updating which again I can probably just go into in more in more detail um but essentially it just
amounts to counting coincidences between states and observations which is a type of heavy in learning
so so say we start out with concentration parameters like this so it's just point two is all the way
across a really small numbers so we don't know whether the left machine or the right machine
is more likely to win um so let's say I observe a one here so I observe a a loss um and I believe
that I was in the right you know I chose the right bandit so now or the right slot machine
and so now I just add a one here so now this entry has a one point two instead of a point two
and if I softmax these so if I turn them each column back into probability distributions
then this one right the probability of losing given being in the right state will now have a much
higher probability um and then we can also if we want to parameterize this uh the learning equation
here with things like forgetting rates or learning rates if we want to those are things
that we could like fit to behavior in in empirical studies um and so to do this we have um this uh
extra term in the expected free energy um here um and this novelty term essentially is just doing
some of the same thing as with the epistemic term you're just trying to change your beliefs about
a as much as possible after observing a new state observation pair um and that's based on um
essentially the thing is just figuring out based on the sums um in the concentration parameters
um which essentially the column that has the smaller total number of counts will be the one
that will be favored um so just a kind of final intuition here um the ambiguity term if we if
this were what the counts were so I've chosen left 100 times and seen 50 wins and 50 losses
and I've chosen the right machine two times and seen one win and one loss the ambiguity
term would be equivalent for both of these so state exploration wouldn't change but the novelty
term would highly favor choosing the right machine again because I've seen so many fewer observations
um so I'll skip that for now um but the idea the final point is is just that in empirical task
modeling um we have these fixed parameters that we can fit to account for individual differences
and behaviors so different people might have different preference precisions or different
precisions of initial state priors or sensory precision or action precision or learning and
forgetting rates and so forth so we can just find the parameter values that best reproduce any
participant's behavior in a model and we can use those parameter values as individual difference
measures um and in the q and a if you'd like I can provide specific examples of that from some of our
empirical studies um but so just uh just to close um so active inference describes perception
learning and decision making all in terms of this approximate Bayesian inference framework
it uses this variational message passing scheme um that's biologically plausible and it can
generate neuroscientific predictions um added includes both this reward seeking and information
seeking drive um drives that can be helpful in um explaining um real human and animal behavior that
does often involve a lot of information seeking um and you can also include this active learning
component um as well as another type of information seeking um so with that I'll stop and just say
thank you for listening and I'm happy to start taking questions

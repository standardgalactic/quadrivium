I'm Alan Downey, I teach at Olin College.
My slides are there.
If you want to load up the slides ahead of time, you can.
So if I say anything interesting, you don't have to take a picture of my slides.
I teach at Olin College, it's a small school just outside of Boston.
Our mission is to fix engineering education.
One of the ways I'm doing that is by teaching Bayesian statistics to undergraduates.
I have to give credit.
My illustrations here are all from the Phantom Tollbooth.
How many people have read the Phantom Tollbooth?
All right.
This talk will make just a little bit more sense if you're familiar with the book.
So my thesis is that Bayesian methods have been the victim of a smear campaign that is
almost 200 years old now.
If you're interested in the history of all this, this is an excellent book called The
Theory That Would Not Die that gets you some of the early history.
But the result is that a lot of what you've heard are myths about Bayesian statistics.
Supposedly, it's hard, it's slow.
The results that you get are subjective.
We don't need this anyway because it's redundant.
None of that is true, but just to explain.
So most places, Bayesian statistics is a graduate level topic.
You have to learn a whole lot of math, and then you can learn conventional, frequentist
stats.
Maybe as a graduate student, you get to start doing Bayesian statistics.
And if you pick up a statistics textbook, Bayesian stats, this is a very popular Bayesian
statistics book.
This is page seven.
At this point, you only have 563 pages to go, so it is at least at first a little intimidating.
The other, as we've heard a couple of times today, Bayesian methods tend to be computationally
intensive, so maybe they don't scale up to big data.
And as I said, the results that you get are all dependent on your prior beliefs, so they're
subjective.
And if you're doing objective science, then maybe that's not acceptable to you.
And then the other thing that you'll hear is that even if the priors are objective,
the results that you get at the end are pretty much the same as what you get from frequentist
stats, so why bother with any of this?
And furthermore, hey, we're doing great.
Science and engineering are taking off.
Why do we need to worry about this at all?
This is Ronald Fisher, influential statistician of the 20th century, basically explaining
that all of this is a bad idea.
Not so fast.
I think Bayes has been getting short shrift.
What I want to do is bring Reimann reason back to the kingdom, and I'll do that by addressing
what I think are the myths that I just presented.
Not quite in the same order I presented them.
Number three, the results that you get are subjective because they depend on the prior.
That is true.
Live with it.
But I claim that that is actually a good thing, specifically it is an I.J. good thing.
This is what I.J.
Good had to say about this topic, that Bayesian methods take your models and make them explicit.
You're not pretending to be objective.
You are addressing the fact that you're being subjective, putting your model on the table
and making that a topic of discussion and making sure that you know what you don't know.
So making your model explicit is a good thing.
Also, if you have enough data, people with different priors will converge.
If you don't have enough data that different priors converge, then you need to know that
and quantifying that convergence helps you know what you don't know.
We don't need Bayesian methods because everything is great, right?
Well, not so much.
We have reason to believe that many published findings are wrong.
This is a paper that talks about, let me see if I've got these in the right order.
Some journals are explicitly banning classical statistics.
I think this is actually kind of a big mess that we're dealing with.
I don't want to throw out the baby with the bath water.
There are a lot of classical statistical methods like regression that are, of course, enormously
powerful tools, but specifically the tools of statistical inference, confidence intervals,
and hypothesis testing.
I believe that the world would be a better place if no hypothesis significance tests
had never existed.
The claim that things are redundant is the one that I find particularly baffling because
people say that the Bayesian methods yield the same result as the Frequentist stats.
They don't yield the same kind of thing.
Frequentist stats will give you point estimates and confidence intervals.
What you get from Bayesian methods is a posterior distribution.
That distribution tells you all the possible outcomes, the possible values, and the probability
associated with each possible outcome.
That's a different type.
It's a different kind of thing, so it can't possibly be the same as the Frequentist result.
You can kind of make them be comparable if you take a posterior distribution, which has
all the values in it, and you collapse it down to a point estimate or an interval.
You can compare that to a classical estimate, but that's a silly thing to do.
It's like comparing a car to an airplane, except that the rules are that the airplane
has to stay on the ground.
I'll give you a couple of examples to explain concretely what I mean.
Polling, electoral results is a classic example.
You survey, let's say, a few hundred people.
Fifty-two percent of them indicate that they're going to vote for one of the candidates, Alice.
You can get a confidence interval on what that estimate is.
You can get a p-value that tells you who knows what a p-value is.
The question that people will naturally want to know is, what's the probability that Alice
will win?
Classical statistics flatly refuses to answer that question.
This is a diagram from a friend of mine who shows here graphically that the set of things
that p-values can tell you and the set of things that you care about do not overlap.
As contrasted with a posterior distribution, again, this tells you all the possible outcomes
for this election and probabilities associated with each one.
You can answer questions in the form of probabilities, and furthermore, the Bayesian methods tell
you how to do an update when new data come in.
You have a prior that's based on previous polls.
You do an update based on the latest poll and that posterior in the lower right-hand
corner is what you should believe given the sum of all the data that you've seen.
This, by the way, is from the 538.com explanation of how they did their Senate forecasts, which
were pretty good.
This is one important point that I want to make, Bayesian methods answer the questions
that we actually care about and make the data that you have actionable.
Second example is something like comparing two drugs.
You've got a new treatment for a particular condition.
You run a drug A versus drug B, and you find a difference between them that has a very
small p-value.
But maybe the new drug A is a more expensive drug, so as a doctor, the question you care
about is which drug should I prescribe?
First stats provides very little guidance for this question.
As contrasted with a posterior distribution, if you have a posterior distribution on the
difference between A and B and you have information about the prices and you have information about
side effects and other things like that, you can do analysis like dollars per life saved
and you can evaluate different interventions in a sensible way.
I'll do one more example that's a little bit more fun.
This is if you're ever on the prices right, at the end of the show you have the showdown
and they show you a prize and you have to guess the price and then they show your opponent
a prize, they have to guess the price, and then whoever is closest without going over
wins the prize.
I love this example because the reward function is asymmetric and it has a sharp cutoff, so
it doesn't lend itself to analytical methods, but with computational methods it's very straightforward.
All you have to do is watch the show for a couple of years which fortunately someone
has done for me and recorded the price of all the prizes.
So this is what you should believe, these are your priors if you're contestant number
one or contestant number two, you know what you should believe about the prize before you
see the prize.
And then when you see it you use your estimate as a noisy measurement of the prize of the
thing and use that to do an update.
So this is contestant number one which is you in my scenario, you see a prize that you
think is worth $20,000 and you update from the dark blue prior to the light blue posterior,
that's what you should believe about the prize after having seen it.
Your opponent does the same thing, this is contestant number two.
Let's say that your opponent thinks that the prize is worth $40,000, that's what the
posterior would be and now you can use that to do an optimization.
This is as a function of your bid on the x-axis there, this is your expected return on the
y-axis and you can see that the peak of that is what you should bid, that's the bid that
optimizes your return.
Interesting point here is that those are very sharp peaks which means that you actually
need to do some analysis to get this right and being accurate will maximize your expected
return.
This is an example of a way that Bayesian methods allow you to do complex decision analysis
under uncertainty.
So this is the basis of my claim that these things are not redundant when you compare
Bayesian methods and frequentist methods, it's not that the Bayesian methods are doing the
same things better, it's that they allow you to do different things and those things
are better things.
I want to come back to one of the other claims about Bayesian methods which is that they
are slow, that is somewhat true but I want to clarify, first of all, if you want the
right answer you might have to wait for it, if a wrong answer is acceptable I can get
one for you very fast.
But the other is that there are a range of ways of implementing Bayesian methods, I often
start with what I call a brute force approach, these are grid algorithms where you just enumerate
all possible hypotheses and compute the likelihood of every hypothesis.
It's conceptually very simple, it's a good way of doing model building as a way of developing
your understanding of the problem but it doesn't scale up very well.
That's an understatement, it scales very, very badly.
An alternative is Monte Carlo Markov chain which we heard a little bit about, this is
an implementation using PMC, also in Python, it's a straightforward way of expressing your
model in code and the implementation is much more computationally efficient so this does
scale up to moderate to large scale data sets.
And then one last alternative, many of these methods have analytic solutions that you can
think of as being a radical kind of optimization of taking very often linear algorithms and
turning them into constant time algorithms when there is an analytic solution.
One of the other accusations is that this is all very hard, if you do this analytically
it can be hard, there is some pretty heavy math in here, however taking a computational
approach makes things much simpler.
Pretty much all of the integrals suddenly become for loops and if you have a little
bit of programming experience a for loop is not all that scary.
I teach a class where I use this computational approach, I have a bunch of college sophomores,
it's only a seven week class but at the end of that class they're able to take interesting
problems that they craft themselves and write solutions and then I take their solutions and
publish them in my blog.
This is one example from the last time I offered the class, this was a couple of students who
worked on predicting the outcome of the Super Bowl, sure enough they predicted that Seattle
would throw an interception in the fourth quarter.
This was a group that studied Tinder apparently, if you swipe right and after some time has
passed someone has not replied, you don't know whether they logged in, saw you and swiped
left or maybe they haven't logged in yet, so there's still hope.
You can analyze that and figure out the probability that they might still log in and swipe right.
Obviously not the most important problem in the world but an example of something that's
real world that used data that they collected that was an interesting problem they wanted
to work on.
Similarly, these guys worked on predicting how long a character in Game of Thrones is
likely to survive.
The answer is generally speaking, not good.
So there are certainly parts of this that are hard, learning to think in a Bayesian
way takes some time but taking a computational approach I think can make this a straight
forward process.
Given people who have basic programming skills and about seven weeks we can get them there.
So just to summarize the myths, it's subjective, get over it but there are parts of that that
are good especially making your model explicit.
I think that this is particularly an important time in science if not so much in engineering
where these methods are critical.
They are not redundant, they do a different thing, they answer the questions you actually
care about, they can be slow but there are options there for making them scale and again

What's up, Coordination? How are you doing? This episode of the Green Pill Podcast is
number two with Daniel Schmacktenberger. We are talking about the Metacrisis. You should
check out episode one of the series that we've done together, which was all about what is
the Metacrisis. We're doing a four-part series together. First one was what is the Metacrisis?
This one is one of the considerations for governance that can solve the Metacrisis, and
then we're going to go into specific Web3 projects and take a look at how they're doing
for addressing the Metacrisis. Address the Metacrisis, not solve the Metacrisis.
Very brief recap. The Metacrisis is being stuck between catastrophes and dystopias.
Basically catastrophes are coordination failures, our inability to stop climate change, existential
risks like biodiversity loss, and CRISPR, and nuclear war, and runaway AGI risk are catastrophes.
They've traditionally been solved by creating dystopias. Governments that are authoritarian
and can regulate the human activity are usually meant to prevent coordination failures. With
Daniel in our first episode, we talked about how do we build a third attractor, so basically
something that can solve for catastrophes and coordination failures, but without centralized
and corrupt systems of governance and surveillance on top of them. Looking for a third attractor,
a governance system that is decentralized, but can solve coordination failures. You
maybe see where this is going if you've been listening to the podcast for a little while
coordination. Again, four-part series. What is the Metacrisis? It was the first episode.
You should listen to that episode, but if you didn't, I've just given you a recap of what
it was about. Then we're going to talk about considerations for addressing the Metacrisis.
That's this episode, episode number two of our series with Daniel. Then we're going to talk about
Web3 projects and how they are solving for the Metacrisis, addressing the Metacrisis. In this
episode, we talked about is tech values neutral? What are the values that technology imbues in
the systems that it is within? We talked about infrastructure, social structures, and superstructures,
and we also talked about conflict theory and mistake theory. One of my favorite episodes that
we've done so far, Daniel is just so lucid, so articulate. He's thought so much about these
things. I really enjoyed this episode with Daniel Schmockenberger. The first episode that we did
with him was one of the ones that I've gotten the most positive feedback on so far. I think
that you're really going to enjoy this second episode. Without further ado, coordination.
I give you Daniel Schmockenberger. Enjoy.
The Opera Crypto Browser is the world's first web browser built for the crypto community with
Web3 support and a non-custodial wallet. Opera lets you access DeFi apps quickly and easily.
The Opera Wallet has buy, sell, and swap features, and of course lets you view your beautiful NFTs,
but the browser still lets you use any crypto wallet extension you prefer, giving you the choice
and flexibility for the Web3 world. Opera lets you view and manage all of your assets across all
the blockchains all at once, and offers seamless multi-chain support between Ethereum, Bitcoin,
Polygon, Binance Chain, and other EVMs and Layer 2s. But Opera goes even deeper than that.
Opera has a built-in homepage for crypto natives with the Opera Crypto Corner, with price charts,
news feeds, NFT updates to make sure you are always on top of your game, and it even has
Discord and Telegram integrated natively into the browser. That's crazy. Opera is truly building
the battle station for the crypto world. Check out Opera both on mobile, with Android and iOS apps,
and on desktop too. ReFi Summer has arrived, and Sello is here for it. Sello is the Layer 1
blockchain for the regenerative finance movement. It's fast, planet positive, and built for the
real world. Sello has committed towards producing a sustainable future from the very beginning,
and it is the world's first carbon-negative EVM-compatible Layer 1 blockchain. Sello has
become much more than a technology, a currency, a community, or even just a Layer 1. Sello is a
movement to create conditions of prosperity for everyone. You can soon engage with all of this
via green asset Uniswap pools on Sello, benefiting reforestation and other regenerative products
through the 2CAN protocol, MAS, and more. ReFi is also about the health of communities and resource
network is creating bankless infrastructure for circular trade and mutual credit networks
to benefit small businesses and local economies all on Sello. Follow along on Twitter to learn
more about how Sello is accelerating ReFi Summer for a positive impact on people, communities,
and the planet. If you're attending ECC, visit the Sello Saloon to learn about what's happening
on the front lines of ReFi from industry experts. Hey Daniel, thanks for coming back.
Excited to be back. Yeah, likewise. So we're doing a three or four part series together. The first
is what is the Metacrisis? This episode is going to be about the considerations for governance
that can address the Metacrisis. And then we're going to, in the next episodes, get into addressing
the Metacrisis, possibly with Web 3 and Web 3 based systems. So let's maybe dive in on considerations
for governance that can can solve the Metacrisis. How do you think about about that design space?
Yeah, and I think most people who are listening to this and working in Web 3 are
probably not thinking how do we solve the Metacrisis, meaning like how do we solve
civilization writ large? But it is important in whatever area you're working on to understand
how that relates to the other areas you aren't working on in the context of the world. And if we,
if you're working on some other area and nuclear war starts looking more eminent, you might be like,
oh, that is relevant to the topic that I'm addressing. And so obviously,
we happen to live in a moment right now where nuclear equipped superpowers are in actual armed
conflict for the first time in a long time. And climate change and AI and biotech and all the
other things, planetary boundaries and stuff we talked about. So that can't not be part of the
context if you're thinking forward, kind of at all. And I know that here, the reason people are
with you at Gitcoin as opposed to other places in the crypto world is the topic of public goods,
which is where does the economy, where do the incentive systems of the economy not rightly
orient us based on both the ownership and the kind of incentive for extraction and externalization
built into currency not rightly orient us to solve certain problems? And where do the structures of
governance not rightly orient us? And can we build a political economy? Can we build an economic
system whose incentives are more aligned with the actual world we want to bring about and where the
governance and the coordination are more aligned? And so that's why we started with the big picture
we did last time. And in this time, it's definitely not like, here, we're going to give all of the
necessary insufficient criteria for how to design a political economy and solve civilization. Like
we're not, that's more ambitious than what we're going to do. But there are a few high level
frameworks that are really important, particularly for technologists who are thinking about building
coordination technology, right? Like how do you actually design technology that can mediate
economics and mediate governance? So we're going to talk today about things like the
intersection of technology, and basically our kind of tech stack writ large, and our social
systems, and our value systems, and the world we're embedded in, and what are some of the ways
that those connect that need to be factored into the design of the tech to have it facilitate
the right kinds of things. So yeah, just kind of calibrating. I know there's a bunch of interesting
projects happening, and can we do a better job with governance if we apply
quadratic voting or this type of DAO structure or whatever, and in the next talks, we'll get into
specific things there. And so here, I just wanted to share a few more kind of structures of thought
that are important to think about at the intersection of the tech that mediates social systems.
That's one of the areas that's so interesting is for the most part, what a social system is trying
to do is to actually help the coordination and regulation of tech, right? And yet the tech
that is mediating the social system itself is a unique class of tech. It's not mining, even though
that term gets used. I mean, it's not mining of the physical substrate of the earth. It's not
waste management. It's not transportation. It's not the other things where you need to be like,
oh, maybe we need to regulate that to not have certain disruptive purposes. What is the tech
that would facilitate a social system that would in turn regulate the rest of the tech?
So it's a very interesting class of things. So yeah, that's what I'm looking forward to getting into
today. Nice. Yeah. I mean, I think that I got a lot of positive feedback about the first episode
just people really enjoying the framing of the Metacrisis as catastrophes and dystopias. And
how do we find this third attractor towards something that can solve coordination failure,
but doesn't have the centralization and maybe like the corruptness of the centralized government.
And I really, I'm just so happy that we're kind of starting to drill down for
considerations that can address the Metacrisis and appreciate you saying that we're not going to
solve civilization. We're not going to address the Metacrisis here, but it's a useful thing for
builders to have in their back pocket as they're moving forward in building this technology.
So it feels like a great place to start might be, is the technology that we're building,
does it have values embedded in it? Is the tech values neutral in any way or does it
create a slant in the social systems that we're creating in this space in some fundamental way?
Yeah, obviously, partly this question arises because we just published a paper on this with
Kinsley and his project and I sent it to you and I think it'll get linked in the show notes here
and the title kind of gives it away, technology is not values neutral. But here what we're looking
at is the way that technology affects psychologies and then affects cultures. We could also look at
how it affects social systems. But just to kind of just let's just zoom in on this part about
does tech embed values and does it affect values directly? So the thing about this,
you put a camera around your neck. It's a technology, right? It allows you to take
things in your visual field and capture them as pictures. When you have a camera around your
neck or in your hand, it affects the way you perceive the world and it affects your intention
because it extends your actuation capacity in a certain way you can now make photographs.
So all of a sudden, the little piece of grass growing through a crack in the sidewalk that
you'd have never noticed or thought of maybe that's a weed I got to get rid of. Now you might
think of zooming in and seeing this beautiful picture of the tenacity of life growing through,
you know, Anthropocene or whatever it is. And so and the birds all of a sudden get more interesting
and so you can see how literally just holding a piece of tech changes what your attention is drawn
to and kind of the meaning that is made on it and then what you do. Similarly, if you're walking
through a forest and you have a spear, you have a whole different set of attunement than if you
don't have a spear, right? If you have a chainsaw, trees mean something different, right? The moment
you're walking around a forest with a chainsaw versus you're walking around with tree climbing
shoes, trees mean something different. And so it's important to get that you could say, oh,
tech is values neutral. Chainsaws can be used to cut dead timber and support the forest or to
cut live trees. It's the values we bring to it, kind of. That's also really just profoundly naive,
simplistic thinking. Tech increases our capacity to actuate. That's right. It's an extension of
our actuation capacities. And it might be an extension of our sensory capacities, like a
telescope or whatever. But and we can connect that all, but I'll start with the actuation side.
In X, there's a way that evolution selects for our sensory processes, what we take in about the
world, our sense making, how we make sense of that to inform choice, and then our actuation
process being in a closed loop, right? Evolution only selects for the closed loop between the
information input, information processing and actuator output. Because if I evolved some capacity
to act, but I had no ability to have sensory process informing it, like it doesn't matter how
high I can jump if I don't know when I should jump or towards or away from what. So evolution would
never select for that. Evolution would also never select for a sense that I couldn't actuate, like,
oh, I can see something that's going to harm me, but I can't do shit about it. So evolution
is only selecting between the relationship between those. So of course, if you have some new actuation
capacity, it's going to direct your existing sensing capacity to pay attention to the things
that can be actuated, right? And so obviously, the chainsaw and the, and the tree climbing shoes
are going to give you a very different sense of trees. And just to give one good example, and
the reason that I'm going into this in a bit of depth and why we actually took the time to write
a paper on it is there is no metacrisis without tech, right? Like cavemen cannot destroy the world.
Chimpanzees, polar bears, or because other apex predators can't destroy the environment. And you
can't destroy the world even with stone tools and even with bronze age tools, you have to get to
industrial and then post industrial, right? And once you get up into nuclear and exponential tech,
you kind of can quickly, and you almost kind of can't avoid it if you aren't being very careful,
because that much power, especially driven with embedded growth obligations and, and
multipolar traps and stuff makes it very likely. So ultimately, it's not that the
tech itself is the problem. The tech is extending human capacities for choice. We're making choices
that are problematic, but it's not just extending our capacity for choice. It's also predisposing
it. And so it's important to understand any solution to the metacrisis is going to involve a
fundamentally different relationship with tech. And it's not just that we need to bring better
values to how we use the tech, we need to recognize that the nature of how the tech is designed
affects the values of the people that are involved. So obviously, a news feed that automatically selects
for maximum social interaction score and time on site is going to outrage and polarize and appeal
to shorter attention spans and do all that shit. It's the way the tech was designed. Now,
that doesn't mean the tech called network based media had to do that. You could design it
differently, right? You could design it where it used the same types of tech capacity to see what
would have supermajority support and upregulate that in the news feed. It could specifically
put in front of you the things that were most likely to connect you to people outside of your
existing network clusters and across memetic tribes. But that's the tech design, right? The
tech design will predispose patterns of human behavior. So I want to go a little bit deeper
in this because this is a group of tech designers and the way that tech
affects the world and creates kind of these recursion loops is a critical thing to understand.
So at first, we design technology for a specific purpose. And it's usually a fairly
narrow purpose. There's one problem we're trying to solve or maybe a cluster of two or three problems
we're trying to solve. The word externality is a word that is now pretty well known. It was not
previously, but it typically means it started with economics, right? The idea that your value
equation had revenue and expenses and there was profit, but there were some of the expenses,
some of the costs that were externalized to the value equation, meaning if I treated trees as a
natural resource, I didn't have to reproduce the trees. I could just extract them from the
balance sheet of the commons because the commons didn't have a balance sheet, right? And I can
turn into pollution in the balance sheet of the commons, but basically I was just stealing from
nature's balance sheet. It was an externalized cost where if I actually had to pay for the real
cost of that waste and pollution to be processed in the unrenewable resource, the cost of the
thing would go way up. And so that's the idea of externality in economics and it's related to
externalities in tech, right? Externality in tech is you're building something for a purpose,
but it affects other purposes that you had not planned for and that's where you get an unintended
consequence or side effect. So obviously when Facebook made that algorithm choice to maximize
engagement for advertising revenue, it wasn't trying to polarize the whole population and
break democracy and make people believe crazy stuff, but it did. That was an externality of
the design of the tech. So one of the things that I want all technologists in every field
be thinking about, particularly those working on the most fundamental technologies that support
coordination, meaning like the actual choice making of everybody at large, is to be thinking about
the externalities and their design. You're designing it for these purposes. What else
will it affect that you didn't think about? How do you think about that better and then how do
you notice it afterwards and internalize that into the process? The first type of externality
that people are used to is a physical externality. Physical externality is you make a pesticide
that is intended to help crops and it ends up killing all the pollinators and going into the
water and messing up ecosystems and that kind of thing, right? It's mediated via physical causation.
You make a laptop that's easier to carry around than a desktop and the externality is everybody
gets bad next from looking down all the time because of the ergonomics built into the portability,
right? And so physical causation creates physical externalities, but there are also psychosocial
externalities and psychosocial causation and that's the topic we kind of intro in this paper that I
want to intro here, which is it's not just the physical tech will do a physical thing, it's also
that the tech will incentivize humans to behave differently because using that tech in a particular
way confers advantage that not using the tech doesn't confer. So it's actually tech is a
incentive to behave in the way where the use of that tech confers advantage. In changing the
pattern of behavior, you end up changing the encoding of human mind and that's where you have
a psychological effect and if that starts to happen at scale, you have a social effect.
And so we can see the most obvious examples of that with you make a social media thing where
the more likes people get, the more it gets up regulated, you start to put Instagram filters on
it and you get the psychosocial effect of an entire culture becoming more body dysmorphic and
narcissistic. What's kind of clear that you do actually like you can breed narcissism and body
dysmorphia in the entire fucking population literally because of the design of the tech,
right? The combination of the incentive and attention capture and the picture emphasis and the
you're like, that's pretty bad. Like the amount of effect it had psychosocially that quickly
compared to the amount of psychosocial effect that any religion ever had, it's just dwarfs it,
right? Like no meme otherwise outside of the tech could have spread that much change to
psychologies and cultures that quickly. So, but there are ones that have done it that deeply
because they were very significant. They just as tech is accelerating, obviously the
meaning is tech is getting to bigger scale and you can create things like Facebook that
become dominant in a very short period of time relative to say the internal combustion engine
or something else. The effects are faster and bigger. But the example I want to give because
it's so illustrative and the way I'm describing it simplifies a lot, but it's because it gives the
gestalt of it is the plow. I want people to think through the example of the plow because
we can talk about the printing press. The printing press kind of ended feudalism and ushered in
everything from science to modern education to democracy because now it didn't cost the equivalent
of $100,000 to copy a book of someone having to do it by hand who had a very rare skill set.
Everybody could have textbooks, everybody could have newspapers, everybody could have their own
Bible. So the Lutheran revolution, we didn't need the priests to interpret the limited Bible for us.
So we can see like, wow, that was a technology that this radical effect, right? And so the sovereign
individual and biology and many other people are looking at the way the current info tech will
usher in new social changes. The plow is such an interesting example because it wasn't just that
it changed our social structures, but our cultural systems in a way that the world is still so
fundamentally influenced by and to just think about it. And before we had the plow, we had
other agriculture, we had horticulture and using digging sticks and the plow was not equal in all
environments. Some places that used horses or donkeys that were also used for transportation
are different than the places that used buffalo that turn into ox. But let's just simplify it and
say the moment you can start to convert an animal's capacity, they can eat food that we
can't eat grazing and whatever, and then they can do all of this tilling and they can convert a whole
area to food production. The plow meant that the caloric output that a culture that was using
that could do compared to hunting, gathering or just the human labor digging stick was so much
greater that of course the moment that technology emerges, it kind of starts to become obligate
because the culture, any civilization that uses it stops dying in famines and they grow their
population way faster with the caloric surplus, which also means that any other culture that
doesn't is a lot more likely to lose in tribal warfare with a smaller population and things like
that. So that's actually one really interesting insight is once, if a technology confers a lot
of capacity, it also converts in theoretic advantage in a rivalrous context. So if anyone uses it,
everybody has to use it or some comparable thing or they lose. So there's a way in which
tech starts to become obligate. Doesn't necessarily mean that exact tech, but it means
the tech stack as a whole has to be competent to be able to deal with the rivalry of other groups.
So some types of changes do become obligate. Obviously, if any group develops a new piece
of military tech, it does obligate the other militaries to do something in response.
So we see that the plow kind of became obligate because it made so much more food
production capacity. But you also see that, and this is the thing that I think is so
interesting about this example, if I have an animal, let's say an ox that is being used to
draw the plow, it doesn't want to do that, right? So we have to yoke it, meaning put the plow around
it and then beat it all day long to do that thing. That requires me changing my value systems around
animals. So if you look at hunter gatherer tribes around the world in any continent,
they were pretty much all animistic. They talked about the spirit of the whale,
the spirit of the buffalo, the spirit of everything. They were cognizant of the great
cycle of life that maybe they kill the buffalo now, but they'll be put into the ground when
they die, which will grate grass and their ancestors will eat it. And that whole kind of thing,
so they could have a sense of the sacredness of the other life and be part of the cycle of life.
But it can't be animistic anymore, if I've got to beat the buffalo all day long, right?
Yeah. Box all day long. So now it's like, then the memes change from the men are not the web of
life, we're merely a strand in it, whatever we do to the web, we do to ourselves in the great
circle of life. It changes from that to man's dominion over nature. The animals were put here
for our use, you know, that kind of thing. And of course, it's just a dumb animal. It's different
than us. It doesn't have feelings. It's not that different from that to slavery. Identify a difference,
remove empathy and see other sentient things as utilities for your advantage. And then a whole
kind of cultural system that sees everybody else as a utility rather than a subject to various
degrees. So so interesting as you can see that the introduction of the plow actually ended animism,
changed value systems to a more utilitarian relationship to other sentient things.
The plow couldn't be run by women because it required a more upper body strength and women
would miscarry when they were doing that. And so men were now hunting and doing the plow. The gods
changed from male and female God distributions to largely all just male God distributions. This
was covered in the Ken Wilber sex ecology and spirit and has little footnotes in there. And
then it gave a huge amount of surplus, more type more surplus than there had ever been,
which started to lead to more capacity for wealth inequality and needing to formalize ownership
rather than the tribes of the larger populations plus the ability to store huge amounts of surplus
started to mean wealth inequality and all the things that came with that which started to mean
hierarchical organization. Like wow, everything from the commodification of nature to changing
whole ecosystems to turn them from forests to row crops. So the beginning of the Anthropocene
to patriarchy to extreme wealth inequality to hierarchical governance systems were
the side effects of a technology that met some seemingly good purpose of feed the people.
And so then you're like, wow. Okay, how do we really think through if it does a thing? And then
for the people to do that, it has to code patterns of belief that go along with those patterns of
behavior. And then it's not just one technology because then you have to have the baskets to
store the grain and you have to have the thresher. So it's also part of an entire ecosystem of
technologies. Every, you get a laptop, but then you also need the external camera and the modem
and the satellites and the whatever, right? All those things. So technologies emerge in ecosystems,
the whole ecosystem of technologies ends up predisposing a whole world. So this is the topic
of a psychosocial externality that the technology is affecting our psyches inexorably all the way
from what we pay attention to to what we value to what we believe is as real. And this is where
what I want people who are technologists to think about is the physical externalities,
i.e. how much energy does it take to run the Ethereum world computer and how does that affect
climate change and all that stuff, physical externalities, but also the psychosocial externalities
and internalize those externalities into the design process, which means if we build a tool that
provides some advantage where people using that advantage are behaving differently,
how is that coding mined and then how does that code cultures and where does it do that in ways
it might be destructive and how do we think through that better and then design better?
Right. You know, one of the things I think is most interesting about this is that I kind of grew
up or maybe wanted to think that our values are what's designing the technology, but the way you
frame it is it almost feels like we're rationalizing our values to fit the technology that has the
the evolutionary advantage or the economic advantage in the web through space.
So this is half true and now we'll move. So if people haven't read Marvin Harris cultural
materialism, it's really important and great work that basically says what you just said.
Like the thing that really obviously changes is new technological capacities. Those technological
capacities required and they both enable and require changes in social systems and ideologies
and cultures, but the other ones are always playing catch up to the tech. I don't think
that's true. That's what he's arguing there. I think it's a part of the truth. I think it's
true that we can use culture to shape the way we do tech. It's also true that tech in turn
shapes culture. So it's a recursive process and you can have kind of virtuous or vicious cycles,
but one way of like this is a real politic kind of assessment, but
that the dominant ideology or the dominant narrative of a culture
is the apologism for the power structure of that culture.
Right. It kind of has to be. And so of course, if you have a crusades that are spreading
Christianity around the world, including with violence and destruction to other cultures,
that also happens to be making certain kings and emperors very profitable and have a Holy Roman
Empire that covers lots of territory or whatever. But everybody feels good about it because the
belief system of we need to spread Christendom because this is what God wants is good apologism.
Today, it's like techno capital, capitalism,
in the, you know, Pinker, Rosling, Gates, etc. sense that tech makes things better and capitalism
makes things better. So keep doing more of it. Obviously, that's the winners of the current
system making a value system where the thing they're doing is the right thing to do and everybody
should want to do more of it. So I would say critical theorists are kind of very attuned to
is the dominant narrative just a bullshit trope for the power structure. And if so,
who isn't well tended to by that power structure, which is why typically this is why justice
movement is actually an epistemic and a design movement, too. It's it's those who
got most marginalized marginalized and fucked by the system who are
the most capable of seeing where its narrative is lying. Or if not lying, at least missing
something not true. So then they can be like, Oh, this looks more like apologism for power dynamics.
And it does like a clear assessment of how a civilization should ideally work. And so then
that has the ability to either just try to tear that system down or hopefully offer a critical
perspective for how to improve it. And so what I hope happens here is as people are looking at
how do we design fundamentally new political economies, it would be, how do we make sure that
every stakeholder human and non human life is going to be affected by the second and third
order externalities, the physical and the psychosocial ones, how do we make sure that we're
in conversation with them where their where their perspectives are being factored, and we don't get
that it's an ideology of winners in something that is also creating losers and externalizing harm.
And so, yes, it is true that the generally the values we have are justifying the use of the tech.
But it can also be true that we can have a value system where we're like, No,
there's something we really value, we want to codify law to actually bind tech and not use it
in a particular way, right? We actually don't want to let the chainsaws and whatever law and
technology cut down the national parks. So the values of the people are going to
instantiate law that then binds the technology and the private incentive from not doing a
particular thing. That would be an example of the causation going the other direction.
And or like weapons ban, we actually have a value system where we just don't want to let that
thing happen. So now this comes to the framework that Marvin Harris put forward that I would like
to just make a little modification of. He following kind of Marxist analysis, even though
wouldn't call it Marxist, he said, you can think about a civilization in terms of three
major ways of kind of dividing it that all necessarily interact, what he called the
infrastructure, the social structure and the superstructure. And the infrastructure was the
whole tech stack by which the people did the things they needed to do and all the needs
were mediated. So the modes of production, transportation, waste management, energy,
all that stuff. And obviously, if we're talking about a hunter gatherer tribe, they have a tech
stack, they have spears and they have tools that are used to be able to make spears and they have
storage vessels and they have shoes and they have clothes for the to be able to be adapted
to environments that they weren't biologically adapted. And they couldn't operate without that
tech stack, right? And as soon as you move to a agrarian culture, you have a plow and you have
yoking devices and you have fencing technologies and you have storage vessels and you have
whatever all these types of things. So that's infrastructure. The social structure is the
collective agreement field. So governance, law, economics, primarily, the and the institutions
that mediated. And the superstructure is basically the values of the people, the religion, the ideology,
the how do how do we do meaning making of what the good life is that we want to orient towards.
And what Marvin Harris was kind of structuring is that the changes in infrastructure drive
changes in social structure and superstructure that without a printing press, you couldn't have
had democracy because you had to have everybody be able to have access to a newspaper and everyone
have access to textbooks and be educated. And that not only was that capacitating for democracy,
but it was also destructive for feudalism, where the concentration of power in that way
also required the concentration of knowledge where that new tool changed it. And it's interesting to
see that where the printing press was a democratizing force, it gave everybody access to something
that only a few had. After a while, it also became an even more powerful centralizing force,
right, where you have these few major broadcast stations that could now broadcast information
at a much wider distance than anybody had ever been able to for larger populations.
You see that with the internet, it was like, everybody's ability to upload their own video
was going to be this massive democratizing force and get over broadcast consolidation,
except when there's a billion videos on anything, the video player that can put them all in one
place and determine which ones I see ends up becoming accurate is what sorts it. So YouTube
and Google and Facebook become the new centralizing force that are even more powerful than the biggest
broadcast media station was before. In general, all tech, all major changes in tech tend to be
decentralizing at first because they break the existing power structure and then they tend to
become centralizing over time because some people are better at it than other people and
they get these recursive loops of the better that they get. And so to say any tech is inherently
decentralizing, you just got to think through that really well. So what I would say is that
infrastructure, social structure and superstructure all three inter-effect each other.
If you changed the economics fundamentally or certain costs had to be internalized into the
cost equation, certain technologies would not be profitable to create and they wouldn't proliferate
and other technologies would be profitable and they would proliferate. If you make law that bound
certain things, again, certain types of tech wouldn't proliferate and other ones would,
if you changed the superstructure of what people actually want and what they're willing to engage
with, it would. And so we're not saying that the value systems and the social structures can't
guide the tech. We're saying that the tech also in turn changes them. So what we want is people
who have a more inclusive value system that's starting at the level of superstructure to start
to design and build technologies where the other people's use of those technologies will also
support the growth of those same values. And where the social structures that get created,
how does the law work, how does the economics work in turn are also aligned with the proliferation
of those more inclusive values. Beautiful. You mind if I recap for a minute?
Please. Cool. So I've been taking some notes here. From the top, tech increases our ability to
actuate and also to sense. So we're all in these kind of like OODA loops, observe, orient,
decide, and act. And tech increases our ability to observe and to act. But tech doesn't just
enhance our ability to do those things. It also kind of slants by embedding our values into things.
It affects psychologies and cultures. So there's physical externalities. So if I look at my phone,
I'm going to hurt my neck. Psychosocial externalities, tech has an incentive to use it,
which confers an advantage, and then it becomes compulsory. And the other psychosocial
externality that I wrote down is body dysmorphia. So like outrages the Facebook example that it's
designed to keep me on site longer. And so that creates an algorithm that as a side effect creates
more outrage that polarizes the population. So for myself and the Web3 ecosystem as system
designers, we want to think about externalities, both physical and psychosocial, and think about
how to confer an advantage to systems that solve the metacrisis in order to make that kind of tech
advantage. Especially in a networked age, in a high tech age, where the question is existential,
sticks and stones may break my bones, but they're not going to blow up the world in a nuclear
catastrophe. So the how big the problem is has gotten bigger with bigger and bigger tech. So
to me, this leads to this question of like, what does the internet do? And what does Ethereum do?
How does it code minds? And how does it code cultures? And how can we be in conversation with
everyone in the system, both humans and nature, as we're designing a system that can build and
solve the metacrisis? And then of course, infrastructure, social structure and superstructure
are all intertwined. If we have values of building and funding digital public good,
solving coordination failures, those are the people who should be building this infrastructure.
Those are all the notes I wrote down. I don't know if there's anything I missed,
or is that directionally accurate? It was a good recap. Okay. Cool. So where should we go next?
I think next on my notes, I have conflict theory and mistake theory to get into next,
but I'm curious if there's another place you want to go? Yeah, let's come back to that.
I want to do one more piece regarding this unique category of tech that is the tech that
the aspect of the infrastructure that mediates the social structure. So notice, if we say,
what is the technology that enables Facebook? And obviously, whether we're talking Facebook or
TikTok or YouTube or whatever, there are certain things that are in common across all of those,
but it's not it's not video player tech. It's the ability to take my online behavior,
particularly when I'm on platform and record it, record my mouse uses and click patterns and stuff
like that, and then be able to have AI do pattern analysis for pattern prediction,
and then have it be able to give me split tested stuff to be able to upregulate its pattern
prediction, to be able to develop a predictive model of behavior that is fully personalized,
and then be able to put specific pieces of content in front of people to affect their
behavior in specific ways, all driven by a profit motive in which the user is not the customer.
The user is, you know, ad sales. And so the customer being engaged has been more time on
site engaged with the content more so that more ads come across them and that the right type of
ads come across them is what that data is being used for. And so we can see that, like,
of course, at the superstructure level, human belief and values and emotions and identity are
going to be conditioned by what we're paying attention to. You can't pay attention to stuff
and have it have no effect on the nature of your mind and in our experience. And this is why, you
know, every culture has some version of a statement like you become the average of the five people
you spend the most time around and the mimesis of learning, we learn language simply by watching,
we also learn belief systems and all those things by watching what we're surrounded by.
So if I can't observe the world because there's too much of the world, so now I'm observing
a subset of the world that is customized for me, then I am, I can't not take as the world,
right, because it's what I'm actually taking in. And it happens to be being customized for me
based on things that will commodify me. And it happens to be that my emotions commodify me
better than my rationality. So it will appeal on those things. And my identity being part of an
in group will commodify me better. That the design of that tech, meaning the incentive
economic system plus the way that suite of technologies are used is changing culture.
It can't not, it can't not change culture. And the one thing I'll add here is that
we're talking about a hyper supercomputer that is trained on hundreds of millions of data sets,
and it's pointed at our brains when we're using these, so we're way outgunned, it's like a hot
knife through butter and changing our attention. Yeah, we already were saying as far as the kind
of like voluntary as an idea, oh, it's the market, so you don't have to buy the thing,
it's just a gibberish argument. One, it's like saying,
do you have to use dollars? Well, no, I could try to just barter, right, like,
but I'm going to be so radically disadvantaged that I am since there's a monopoly on the
creation of currency, I pretty much have to fucking participate with that monopoly if I want
to be effective at all when like, you know, when like, do I have to use the telephone lines when
there was AT&T ran all the telephones? It's like, pretty much I do. If I could sort of say that
I am voluntarily engaging, if I don't have an authentic choice to still be
viable in society, if a small business doesn't advertise on any of the major platforms,
it's pretty fucking hard to be a viable small business, right? And then as we were mentioning
with the plow case, if any civilization starts making, if any other country starts making AI
weapons, do we really have a choice not to? Yeah, not really, right, we're probably going to do that
thing, because otherwise, we kind of obliquely lose. So it's important to understand that the
voluntary or voluntaryism argument is there's a lot of places where we pretend it's more true than
it is. So if tech confers a bunch of advantage, some people will start using it, and then that
will be used for conflict theory, and then it does require the other people to either use that or
some comparable thing, or they start mattering. And so we come back to the Facebook thing you were
saying, you know, this is this huge AI supercomputer directed at our brains. Now, we also have a
situation that one, we don't really have the capacity to choose to not engage with the whole
suite of those technologies and still be effective at certain domains of society.
And then the other thing is that it's like saying, well, it's voluntary whether or not we engage with
ads and whether we purchase a thing. I don't actually get a choice in those environments
of whether or not I'm exposed to them, nor do I get a choice of if my personal data is harvested
and used with AI dynamics to compel me to do that thing. And the asymmetry of it is so fucking much
that it's like, okay, this is why we wrote in that paper about the topic of undue influence. Now,
that was actually a different paper on the Consent's Preserve where we talked about that
undue influence of like someone is choosing something, but they're under some type of
cult mind control, or like why it's not okay for a professor or a therapist or a priest to
be in a relationship with someone is the other person really, there's so much power asymmetry,
they can't consent adequately that even if they're consenting, the asymmetry of power messes up the
integrity of their consent. We'll link that article in the show notes, I just found it,
put it in there. So we would propose that if information about you that enables from your
behavior, that enables a platform to predict your behavior better than your wife or therapist or
lawyer could, that that is privileged information, that that does enable undue influence, that that
needs to be in a fiduciary contract, a principal agent binding contract, or whoever's using that
is using it for you, not in any way against you. And so that would mean that those tech platforms
had to actually have a fiduciary relationship with you, the user to only use that information
aligned with your interests, where you had full visibility in settings and could change all of
it. That would of course require a change of business model, probably where you were the
customer or the state that is representing you or the commons that is represented is.
So as we get into tech that is more powerful, the asymmetry of those who are using it relative
to everyone else also becomes a bigger deal. And that affects the nature of social systems
radically is when there's such radical asymmetry of information processing,
can you really still just say buyer beware when the buyer can't begin to deal with weighting
all of the things that the other side that's using an AI and we're not
can factor? Right.
Does this get into to sort of like the political theory in such a way, at least with the Facebook
example where Facebook's management is optimizing for the shareholders instead of the customers.
And so it's optimizing for the customers as well as just the customers or the advertisers,
not the users. Right. That's actually a point is that the users should be the customers.
Well, I guess what I'm getting at is like, there's this idea in political theory that the
consent of the governed is the only legitimate basis for governance, as opposed to like the
divine right of Kings. And what if the Facebook users were owners of the platform and had,
you know, like management had to sort of like optimize for them in a little bit more
of like you said, a fiduciary obligation kind of way.
Otherwise, the people are actually the thing being extracted from like we used to extract from
nature. Now we're extracting from human minds, the particular kinds of behavioral predispositions
we want for the extension of power. But should Facebook have the customer, the user be a customer,
yes, but should it go further? Should it actually be a public good? And that if it is,
if it has that capacity to affect human behavior at scale, what the fuck should drive that?
Right. And this, well, and should everyone, should the consent of the governed have some say
in the governance of a thing that is going to have AI's pointed at their mind?
And so then you're like, Oh, well, it should be a DAO of some kind. And then you're like,
well, fuck, isn't that what the government should be? And now that it's not 1776 technology with
a town hall, but can we actually similarly have a process? Because when you say the consent of
the governed, what you're saying is that super structure needs to be the basis of social structure.
Right. The social structure law specifically, if it is not an instantiation of the will of
the people, it will be tyrannical. For it to be an instantiation of the will of the people,
then the thing that really matters is not only how do we actually listen to what the values of
the people are, but how do we make sure we're developing people that have better sense making
and deeper and richer meaning making? So that the, because the will of a very uneducated xenophobic
audience is not a great thing, which is actually why Kings made sense in low development environments.
So how do we develop super structure to have an increasingly considerate population? And then
how do we have that creating the laws that both utilize and also bind, guide, and direct the
technology? And I would say what's happened is we've got exponential curves in infrastructure
during the digital age, but we have not got exponentially faster governance or exponentially
faster growth of morals, virtues, and epistemology and culture. And as a result, the effects of
infrastructure influencing the other ones have radically increased and the effects of the integrity
of the social structure influencing or of the superstructure, the culture influencing the
social structures to bind the tech has radically decreased and got hollowed out. And then I would
say it's that direction from the values of the people from the superstructure to a healthy social
structure that has the oodalooops that has the capacity itself to bind, guide, direct the technology
to be in service of what the superstructure wants it in service to what the collective will of the
people does. That is the critical direction that we must increase to make it through metacrisis.
So that means we have to be both thinking about human values and how we support the proliferation
of the development of human values, how we encode those in our decision making systems,
and how we make sure that our tech is not only bound by those decision making systems,
but that the use of the tech that the psychosocial effects of it are in turn in service of an
enrichment of structure. Immutable X is the Layer 2 platform for crypto gaming. Immutable
offers massive scalability with up to 9,000 transactions per second and instant transaction
confirmation. No more gas fees, no more waiting around for your transaction to clear. Immutable's
zero knowledge roll up finally unlocks the world of crypto gaming. Immutable X is the only gas-free
NFT minting platform with over 26 million NFTs minted all with zero gas fees. With the power
of immutable gaming developers don't also need to become smart contract developers, they just need
to plug into Immutable's API and instantly start unlocking the full potential of crypto assets
inside of games. This is why world-class companies and projects have decided to deploy on Immutable X
like Gamestop, Ember Sword, Planet Quest, Illuvium, TikTok, and many more behind the scenes. So start
building your game on Immutable X today at immutable.com.
Coinshift is a leading treasury management and infrastructure platform for DAOs and crypto
businesses who need to manage their treasury operations. Every crypto organization manages
their treasury and Coinshift offers a simple, flexible, and efficient multi-chain treasury
management platform built on top of the extremely secure NOSA safe. With Coinshift, your organization
can go from primitive single-chain treasuries to expressive, flexible, multi-change features such
as global user management, global contracts, proposal management, and many other features
that can be chaired across an entire organization. Coinshift layers on powerful treasury management
tools on top of the proven security of NOSA safe, allowing users to save time and reduce
operational burdens and gas costs. Coinshift even has data tools like account reporting across the
seven chains on which it operates. Used by industry powerhouses such as Uniswap grants,
Balancer, Consensus, and Misari, Coinshift is speeding up the coordination and efficiency
of the organizations that use it. In DeFi, you have to keep up with the frontier and Coinshift
to make that easy. So sign up at coinshift.xyz slash bankless.
So let's go into conflict theory and mistake theory next. Curious to pull that thread?
Yeah. So we're talking about externalities. And typically, we think of an externality as a
mistake. We didn't intend the laptop to hurt people's necks. Like that wasn't a built-in thing.
It was just an unforeseen consequence or something that we, yeah. And similarly, nobody
intended climate change. They intended to build their businesses and their businesses required
energy and the cumulative effect of everybody doing that caused climate change. So we think this is
not like an intentionally created problem. It's a problem that is a byproduct of solving some
other problem. But obviously, there are places where somebody causes harm because they want to
invade that area and get access to those materials or that port or that whatever it is. So whether
they are initiating conflict on purpose or at least they know that it'll cause harm and they
don't care, right? Maybe it's the intent or they don't care. So there's a blog post on the
less wrong blog, I think with this title conflict versus mistake theory. And it's a subset of a
very large conversation and kind of social theory about what percentage of the problems in the world
are the result of simply not being able to factor all of the second and third order consequences
and what percentage of it are the result of things that are being initiated knowingly.
I think the article does a good job of discussing considerations on both sides, but the thread is
particularly interesting of comments afterwards. Michael Vasser, who's a very interesting thinker
in there specifically goes into that the existence of mistake theory ends up being used as a cover
for a lot of things that are actually conflict theory. Meaning that we can knowing that I can
say later, oh, that was an unintended consequence, we could have never predicted it. I can hide the
fact that I actually knew that thing was going to happen, or at minimum not try really hard to
figure it out, because it's again my incentive to try to figure it out. So then I have a mistake
theory as plausible deniability that I didn't intentionally do the fucked up thing. And so I
think when we're thinking about tech design and social systems, we want to think about conflict
theory. What is the underlying basis of what gets humans into conflict with each other, individual
humans, groups of humans, and humans as a whole with the environment? And how do we resolve the
underlying basis of conflict? And obviously, economics is very core to that. It's not the whole
of it. Marx has his analysis of conflict theory, Gerard has his analysis. If anyone wants to solve
any of the problems of the world, this one of the things to go deep on is what are the various
things that initiate human conflict, and what would it take to create a system that addressed
those more fundamentally? Not just conflict mediation, but conflict generation mitigation,
how to mitigate that. And because the mistake theory part, we can do a much better job procedurally
of that if we had the incentives, meaning if we had the motivation, incentives and deterrence
and intrinsic values. So can we predict all of the effects of a new tech? No, of course not.
Can we do a much better job than we've ever done? Easily. And so can we predict all the
effects is the same as predicting weather a ways out in complex systems, you end up
not being able to predict beyond a certain point. But for instance, just even thinking through with
a framework like physical externalities and psychosocial externalities and getting a group
of people that represent different stakeholders that would be touched by the system and that
represent different expertise. And just say let's think through all the possible externalities based
on patterns of human use using this and based on the supply chains that it'll take to make this
thing happen. All of a sudden, we anticipate a lot of stuff we wouldn't have anticipated.
And then we say, well, if those things happen, what are the externalities of produce to get a
third order? Right. And so then we start thinking about that and saying, how could we design it
differently that internalize that. And similarly, thinking about something like the relationship
of infrastructure to social structure and superstructure, you get to start to think, well,
what are the effects of this on the systems of human governance and social structures? What are
the effects on the superstructures? And obviously, all of that is embedded within the biosphere.
What are the effects on the biosphere? So these are frameworks for being able to consider
externalities better. Then of course, there will be some things that you didn't predict. So you
want to create monitoring systems. And you also want to create tests, right? How do we test this
at a small scale, what we'd call a safe to fail probe? And Dave Snowden's terms. And
then be able to observe what are the things that happen, including stuff. Since we don't even know
what to predict, how do we make just very kind of wide observations and notice new stuff that starts
to emerge and then be able to factor that. But then of course, what we need is that let's say we
make a technology and it's off and running. And we did all of our good upfront assessment. We still
need to be watching what are the other things that it does. And we also even want to be incentivizing
and decentralized intelligence to show us unintended consequence, right? And then we'll need to figure
out how to solve that in terms of a recursive design of the technology or a design of the social
system, i.e. a law or something that will affect the utilization of the technology.
And then we have to actually be able to enact that. So you can't move from the tech designers to now
the CEO and board of directors have a fiduciary responsibility to maximize return on shareholders
and it doesn't matter if you create an externality, you don't ever get to change it. So you actually
have to build into the governance protocol of the thing, the ability for ongoing recursion that
internalizes extra points when they're found. But the underlying reason not to do that because
procedurally what I just said is not that hard. It's not obvious, but once you hear it, it's
kind of obvious. And it's not perfect, but it's like so many orders of magnitude better than what
we do. But then you're like, okay, well, the other issue is the conflict theory, the perverse
incentive. So let me explain the perverse incentive that is underneath much of the problem here. Is
that I'll specifically talk about market applications. There, there is much more incentive to focus on
the upsides of a technology than the risks. There's much more of an incentive for a technologist and
they funder and a lawmaker and a politician and whatever who can bind themselves to those upsides.
Right. So let's just say for the technologist, if I say, oh, we're going to make this AI thing and
it's going to like, do protein folding and solve all these diseases and do all this amazing stuff
and everybody wants it for those reasons. Yeah, well, what about all the risks of the really
terrible things? Oh, no, it's not going to do those terrible things. Of course, whoever focuses on that
moves faster, gets first mover advantage in the market, gets the race to network dynamics,
you know, a massive adoption of their thing. Whoever goes slower to say, hey, wait, we're
not sure when this protracted process of anticipating second and third order negative
effects and we found a bunch and we're internalizing those into the design. So we're iterating the
design a lot. Those people just get out competed by the other people who just rushed to market.
So the incentive is to not look for where it's going to harm things or to do a bullshit job
where you do some box ticking thing that says, oh, we did do diligence, but you really don't
want to know anything that's going to mess up the speed to market. And so this is where
things that are actually conflict theory gets a height as if they're mistake theory.
And this is where you have to factor the relationship between the those topics well,
but when you're thinking about new technologies in say the web three space, you want to be able
to think about what new types of conflict theory dynamics does this actually enable? Like, where
does it enable the people who are better at this technology to have more game theoretic ability over
the people who are less good at it? Where does having this type of spear or chainsaw incentivize
thinking in certain ways? What are the effects of those things? How can this increase or decrease
the underlying basis of conflict in the space? And of course, when we're trying to think of
governance tech, how do we make it to where there isn't this perverse incentive to not focus on
externalities? Well, you could do things like really tight attribution where everything in your
ecosystem actually does have to pay for the externalities that it finds that are found later.
And so it becomes catastrophically unprofitable, whereas right now the company gets to privatize
its gains and socialize almost all the losses. Could we make a system where the externalities
have tighter attribution or internalized? And so it actually affects the cost equation and the
incentive. We have a situation where you actually do regulation from something like a DAO first,
right? You create some new governance protocol. And before a new technology is rolled out,
the people actually have to see that its risk analysis was good enough. So it's not the technology
rolls out. And after we see that it ruined everything like DDT or cigarettes or whatever,
then we try to regulate after the fact, which once you're an exponential tech, it's too late to do
that shit. You can't let out self replicating biotech. And then after everything say, oh,
we should regulate that it's too late or AI. So we actually have like, say before tech gets
released into the market, is this safe? Did it actually pass enough, you know, appropriate process,
which of course happens in some areas the FDA is supposed to before getting authorization to,
you know, ensure oversight of drug safety. But that's not the case in like almost anything
in software for these purposes. But showing have even bigger effects on culture than any
particular drug would. Yeah. And we've got the same coming out of the valley,
move fast and break things. It's almost the exact opposite culture of what you're talking about.
Remember how we said that the dominant narrative is apologism for the power system?
Yeah. Move fast and break things. Oh, we're all, we're all about disruption and coming up with
innovative new stuff. And we're like fail and be these great innovators. It's just like what you're
talking about is having privatized a fuck ton of gains and socialized losses. Because what you
broke was the social contract of the entire fucking country and the epistemic comments and the ability
for anybody to do anything to solve climate change. Like if what you broke is the biosphere in the
country, no, that was not a good thing to do. That was really fucking stupid. Yeah. I think it's
really interesting to hear about the conflict theory, which is kind of like initiating harm
via conflict on purpose. Maybe there's a scarcity of resources and the mistake theory
creating harm via an externality of not knowing and then the plausible deniability
of really being in conflict theory and but treating it as mistake, mistake theory in order
to get away with it. Is there anything more to say about that in the intersection of what we've
talked about? Yeah, I would really love to see web three developers taking mistake theory super
seriously and saying what are the negative effects psychosocial, particularly effects of this
technology proliferating? How do we incorporate it into the design? And then where are the incentives
of people to overemphasize the positives and move fast and they might break things? Because where they
have an incentive to be focused on the upside, they are probably in conflict theory, they're
going to blow past preventing mistake theory they could have. And so you really want to look at
where do the incentives give people less orientation to pay attention to real risks,
second, third order consequences and risks. Where does the incentive landscape and deterrent
landscape where they'll be able to socialize the harms later or have plausible deniability,
they didn't really cause it or attribution issues. And so for the web three world to say,
let's really look at the incentives well. And make sure that there aren't ones that are fucking up
our ability to think through the effects. And then let's look at what are we, what are we really
trying to serve? Are we trying to serve a thriving biosphere and a thriving human society? If so,
we don't want to externalize harm anywhere. We know these technologies are going to be powerful.
How do we really think through what that power is doing? And where the tradeoffs are and whatever
is being benefited, what's being harmed? So how do we hold more procedural rigor in doing that?
And how do we identify where there's incentives not to do it to both try to change those incentives
and to know where to be dubious of certain claims? Right. So I mean, I guess, you know,
you and I are going to wrap this episode soon. But for episode three and episode four, we're
talking about how web three can help address the meta crisis. I'm almost imagining a scorecard
here in which we can take a couple of projects, hold them up to the prism of ways of what we've
developed here, looking at each project within the lens of conflict theory and mistake theory,
recognizing that tech is not values neutral, looking at the narratives that have been manufactured
by each project, looking at how they intersect with the infrastructure, social structure and
superstructure. And then also looking at whether or not these projects confer an adapted advantage
to their participants or at least not a disadvantage. It's kind of like my scorecard
for something that's going to work and is something that is going to help address the
meta crisis. But, you know, I don't know if there's anything to add to that list.
The scorecard involves things that are qualitative, right? Not just qualitative. So you
can't make the decision based on just expected value calculus where you've converted everything
to quantitative and then the same metrics. So that means actually something like human
wisdom is still at the center of the whole thing, earnestness and honesty and wisdom to be able
to factor. Because as soon as you get into the, okay, well, here's the trade off. And if we did
this thing, we can solve this problem, but it'll cause us other problem. Then I have to say
how many dead whales are worth how many tons of CO2 are worth how many abused children.
And if you just any version of doing that ends up turning into somewhere between nonsense and
evil. And this is one of the things I really want the web through community to get is that
you cannot convert the world into quantification. There's a lot of qualified stuff that cannot be
quantified. And there's a lot of quantified metrics that are uncommentable. And that if you make them
commensurable, you do it at a disadvantage to reality. And so what that means is there's a lot
of decisions that cannot be made in a metrified way, which means they cannot just be automated
and computed. They actually do require adjudication and discernment. And so how we actually grow
populations capable of holding that complexity and the qualified things and the uncomparable
things and making wise choices has to stay at the center of our focus. And so that scorecard
is not a metrified one, like there are some metrics, but it's, it's asking like, are they,
you know, where are the incentives? A lot of those will be squishy because some of the incentives
will be like status or, you know, whatever. And like, did they do the risk analysis? Well,
it's going to be, you'll get an index maybe, but you're not going to be able to do a perfect
quantification because how do you know what was not included that maybe should have been included?
And so a scorecard on process, was there earnest and deep and rigorous process?
Yes. And it won't be perfect, but it's directionally right. A pre-computable thing.
But considerations that we would like the Web 3 superstructure, the Web 3 culture to hold very
deeply, where people, like, one thing I'll say about the future is that if we are to have a
future where we make it through the metacrisis, we will be less focused on design systems of
extrinsic incentive and more motivated around intrinsic incentive associated with a deeper
connection to life. And if I am deeply connected to nature and to people, that connection informs
and embodied ethics, that intimacy informs and embodied ethics. And the systems of extrinsic
incentive are basically how do you control people based on their selfishness, because they don't
have ethics or to even override them. And so I actually want people to do shit based on incentive
and reward much less. And I would like to be able to enable people to have needs met and have
then a deeper existential development of what is actually meaningful and worth doing with our
short lives here. You know, it's interesting, the ETH Barcelona conference just happened and
they had every speaker come on stage and touch grass. They literally had grass up on stage,
which I guess grounded them a little bit of nature in the heart of the city.
Something. Yeah, it's a start. I'm also imagining this like, I'm in an imperfect vessel for this
conversation, because I only have a certain vantage point on even Gitcoin, like it's gotten so big that
I don't even know everything that's happening. And it almost feels like a distributed intelligence
question of in mapping all of the externalities, you almost have to look at it for the hyper
structure from all of the different vantage points of it happening. I mean, I think we can
take a stab at it in episodes three and four, but the real thing is empowering the everyday
citizens of the web three space to have these conversations and almost to create a hyper structure
that can like rate projects on their ability to solve them at a crisis and help them find the
externalities in like. This is why the diversity perspective is important. Yeah. Because
if nobody in there actually lives in an industrial zone,
like physically and knows what it's like to live in an industrial zone, or if nobody actually lives
in poverty or any of those things, and there's just types of experience they are completely
unclued to and they just can't academically clue into the reality of it in a way that anyone who
lives there can. And so, you know, the question of like, Hey, do you have people from other classes?
Do you have people from other races? Do you have people from all the genders? Do you have them
there as well? They have different experiences. And their experiences are like, Hey, you're
fucking designing civilization for who you're designing a thing that's going to affect a lot
of things through all the second, third order effects. Do you have everybody who's affected
by it weigh an in on it? And it's specifically what you just said is that they can anticipate
some of the externalities differently. Because they experience it differently,
because they're in a different part of the system. So if you think of sensors, if you
think of this as like a human internet of things sensor system, then of course, I want a wide
distribution of human sensors all giving the feedback of what they're sensing in the various
aspects of the system to design a better system. And so then, rather than so often we design a
thing, we take it as very precious, we don't want to criticize, we fight for it. No, fuck that,
like design a thing. And then ask everybody for what's wrong with it, knowing that what they're
offering you is the gift of how to make it better that you couldn't have seen. And then ask them,
how do I make it better? How do I factor these things? So let's say you start having red teaming
the design, not just the security, but the design from an externality point of view, identify all
the externalities here. And then that'd be something that is proceduralized and even
incentivized in a decent way. And then, okay, we got all these things. Now we're going to incentivize
anyone that can come up with solutions for them that don't cause other externalities in the process.
That's how you get a decentralized collective intelligence thinking about the right things
and actually synthesizing its intelligence. I guess a great place to end for this episode.
We've got some great conversations set up for episode three. Maybe one last question I'll ask
you before we break is, how should we select what projects we discussed for episode three?
I mean, obviously, I have a lot of knowledge of and experience with Web three on Ethereum
and Gitcoin is a project I founded, but it's now a McDowell. What kind of projects do you think we
should hold up to the lens in the next episode that we do? We can certainly pick ones that either
are some of the most prominent or possibly most promising ones or talking about some design
aspects that are pretty fundamental, which we can do. And if you want to open this up as you put
it out there for people to say, hey, what about this, projects or specific technologies or ideas,
anything that people are particularly interested in would be fun for us to have a look at.
Great. Yeah, I think accepting submissions from the crowd would be great. Is there anything I
didn't ask you, Daniel, that you want to say? Not in this time. I'm looking forward to next
time and this was a fun round two. Yeah. So enjoying the series that we're doing together.
You're so lucid. You're so articulate. You're many steps ahead thinking in so many different
things and I'm thankful for this collaboration with you. So thanks again, Daniel. I'm excited
about thinking about you having a whole community of people that can actually build stuff that are
incentivized to build stuff and actually care about things like public comments,
taking these types of considerations seriously and starting to have conversations like, okay,
well, let's take this podcast as like a book club and then let's talk about a dialogue and set
up processes and actually make a collective intelligence that has some of the deeper theoretical
framing to inform the building. That's actually really exciting to me. That's why I'm happy to
you know, it's the synthesis of theory and action, I think. And if we can bring those
people together, that'd be really great. Thanks, man. Thank you so much.

So I just want to express my gratitude to everyone who's come today, thank you for coming.
So my name is Maxwell Ramstead, I'm a postdoctoral fellow here at the Jewish General.
I work with Lawrence and I work mainly on this approach called active inference that
I'll be presenting today.
So my talk appropriately is called a tutorial on active inference from the predictive brain
to sociocultural regimes of expectation.
And the talk basically is going to come in four parts.
The first is basically a motivation of the problems that we're trying to address using
active inference.
So I'll present on the one hand the problems per se that active inference addresses the
space of problems in which active inference lives.
And then I'll move on to a few more specific motivations about why an approach like this
might be interesting.
Then I'll turn to what's known in the literature as predictive processing, which is basically
this active inference scheme applied to the brain.
Then I'll move to active inference per se.
These two sections will kind of come together.
I mean, the predictive processing part is just the application of this framework to the brain,
so they're not all that distinct, although if you get into the more technical debates,
there are some distinctions, but we can return to that.
And finally, if time allows, I'll discuss some of the multi-scale extensions of active
inference, so going beyond models of brains and cells to this kind of integrated view
of systems of systems of systems that are nested, the ones within the others, and the
kind of move together thanks to this active inference framework.
So first, I guess, a discussion of the motivation and problems.
So basically, the problem, I think, that active inference helps us solve most clearly is the
problem of multi-scale systems.
So this is a picture of McGill University in slightly warmer times.
So McGill University is a complex system.
It's a system that's composed of several different parts, some of which are living, some of which
are not.
So the faculty, the student body, the support staff, but also all those are living and some
things that arguably are not, like the buildings, the rest of the physical infrastructure labs,
and so forth.
Well, if you zero in on one component of this, the living ones, so say the student body in
McGill, is itself composed of bodies.
And bodies, as we know, are composed of cells, and cells make up organ networks that eventually
make up organisms.
Organisms then cells interact in social groups and eventually make up social networks, one
of which may be the most evil I've posted the image of right here.
So systems within systems within systems, we are systems within systems within systems.
That's kind of the takeaway of this slide.
So if you look at the structure of the brain, the brain structure, in a sense, recapitulates
the structure of the environment in which it's encapsulated.
So if you look at the structure of the brain, this is from a paper by Park and Friston.
What you'll notice is basically a repeated encapsulation of networks within networks
within networks, right?
So dendritic formation at one level.
And if you zoom out a bit, you have networks of neurons.
And then if you zoom out even more, you have brain regions interacting with brain regions
and so forth.
And what we notice is that there's a segregation in the brain, a kind of spatiotemporal segregation
of the interactions between brain regions that in a sense corresponds to and captures
the regularities of the kind of statistical segregation that we see out in the world around
us, right?
Things change in relevant ways at different scales, at different spatial and temporal scales.
So this is something that we'll return to.
And so up until very recently, this was lacking in a literature.
But suppose we wanted to study all these levels together in an integrated fashion, recognizing
the interest and dignity of all these levels, right?
So suppose we wanted to construct a framework that was able to address, I've plotted here,
this is from a paper that I wrote with Carl Friston and Paul Badcock a while back.
I've plotted spatial dimensions on the y-axis and temporal dimensions on the x-axis.
So what we would like is a theory that can, in an integrated fashion, offer an explanation
for the way that systems behave from the subcellular level through to the cellular level, the level
of tissues, organs, and organ networks to organisms all the way eventually to speciation,
the construction of specifically designed niches.
And this is from the spatial point of view, but from the temporal point of view, we would
also like an account that's able to account for the variety of temporal scales that are
involved in the phenomena of interest, right?
So from mechanistic processes occurring over milliseconds all the way to phylogeny and adaptive
radiation, which span hundreds of thousands, if not millions of years.
So that's the motivation of the problem.
How would we address all these different spatial and temporal scales in a principled way?
What kind of framework would be able to enable that?
Okay, so that's the problem.
One motivation for thinking about active inferences in particular is an observation.
It's that most self-organizing systems in nature tend to dissipate, right?
So from galaxies and stars to lightning bolts and tornadoes, almost all really like, and
I mean that in a very strong sense, almost all systems that self-organize in nature self-organize
to equilibrium.
So what does this mean if we had to unpack it in a kind of basic way?
Well, first, we should note that these self-organizing systems consume the gradients around which
they organize, right?
So if for instance you think of a lightning bolt, a lightning bolt self-organizes around
a charge gradient, right?
And in striking, the lightning bolt consumes the gradient around which it's self-organized,
effectively leaving the entire system at equilibrium.
So the same could be said for tornadoes, although now it's not a charge gradient, it's a temperature
gradient in a weather system.
But the main takeaway is that for almost all systems in nature, self-organization serves
to increase entropy.
So in this context, entropy is a measure of spread.
You can think about it roughly as a quantification of how many different configurations the system
could be in, right?
So if you think of a gas versus say a crystal or water or a solid, in a gas, the constituent
particles could be in any number of different configurations, right?
Like this one that I'm pointing to at the end of my finger, this little molecule of
gas might be at the other end of the room in like 20 minutes.
There's less constraint as to where the little particle will end up.
Whereas if you think of a crystal, well the kind of regular lattice structure in a crystal
means that there are only ever local interactions and that you can predict with a high degree
of reliability what any given particle, what state it will be in, right?
So high entropy means a high number of available configurations, low entropy means a low number
of available configurations.
So back to this, most systems in nature, right, self-organize to equilibrium, they dissipate,
they increase entropy.
But other systems self-organize and do not, right?
Bunny rabbits and tigers and, you know, I don't want to make assumptions about you,
but I assume you as well don't self-organize to equilibrium, right?
We self-organize to this regime of states that's in effect very far from equilibrium,
right?
I don't exist at room temperature.
My core body temperature is about 36 and a half degrees Celsius.
I imagine yours is roughly the same.
You know, so we manage to stay far away from equilibrium.
And the question is, well, how do we manage to do this?
So here's going to be my first kind of technical excursus.
I'm going to present to you the state space formalism from dynamical systems.
And this will help us think a bit more formally about what I just said.
So a state space description is something borrowed from dynamical systems theory.
And the idea is to construct an abstract space that represents all possible states of a system,
right?
So it's really that we're just constructing a state, a space, right?
So like a mathematical object with dimensions that you can kind of move around in, right?
And this space is going to represent all the possible states that our system can be in.
And so how do we do this?
Well, basically for every way that the system can change, right, a.k.a. for every variable
in the system, what we're going to do is plot one dimension in this space.
So we're going to say, OK, this variable corresponds to a dimension in this space that
we're constructing.
So suppose you have a very simple physical system with a temperature and a size, right?
Then you might represent it this way, right?
One of the dimensions corresponds to temperature.
The other dimension corresponds to size.
And so what you'll notice already from this setup is that a point in this space here is
already a complete description of the system that we're interested in, right?
So to recap the argument, every dimension in this space corresponds to a variable in
the system.
Therefore, every point in the system corresponds to assigning a value to every variable.
So essentially a position along the dimension, right?
And so that might seem like a banal observation, but it allows us to say interesting things
about what you might call the intrinsic geometry of a system in terms of its phase space.
So for example, you might want to say, well, in the whole space of possible states that
the system could be in, there's this particular region that has these particular properties
that when you're in this area, something interesting happens, right?
So as an example, here again, I'm just using the same thing, right?
Plotting the temperature on the x-axis and size on the y-axis.
Well, you might say, well, most living things that exist exist in this sub-region of that
space, right?
Most living things exist somewhere between 0 and 100, roughly degrees Celsius, and most
living things are somewhere between 10 centimeters and 100 meters, if you're looking at these
very large colonies of cloning trees and stuff like.
So this is the kind of thing that you can say using the state space formalism.
One of the interesting things that you can do with this is describe trajectories over
the phase space, right?
So this is literally drawing a trajectory over this phase space and saying, okay, well,
the system starts here, and then some perturbation moves the system towards another state, and
then the system has to cycle back.
So for example, I wake up in the morning, I'm a bit hungry, right?
This is somehow related in an interesting way to my making coffee and breakfast, and
then I eat, and then I'm satiated, and then I move to another area of the phase space
that is me satiated, right?
But I consume sugar, as I, for example, give this talk.
So sure enough, I'll be hungry later today, so then I'm back where I was.
So the phase space description allows us to say something about the dynamics of a system.
It allows us to say something about the way that a system states evolve over time.
So basically, the reason I'm talking about this is that active inference is a theory
of, provides a mechanics of or tells us how living systems are able to do this, right?
So we all exist as living things in a bounded set of states, right?
We don't self-organize to dissipation like these things, right?
We self-organize to a very well-defined set of states.
The question is, how do we do this?
And active inference provides an answer to that question.
Active inference tells us about how things can stay away from basically states that they
want to avoid and stay in a regime of states that are compatible with their existence.
Okay, second motivation, which you might call the animal's perspective or the Bayesian room.
So if you really, really, really want to caricature the position that organisms are in, basically,
organisms are in what you might call the Bayesian room.
Chris Elias Smith is called this the animal's perspective.
So the idea of the Bayesian room is that organisms only ever have access to their sensory input,
right?
And the sensory input isn't always very reliable.
For those of you who do neurophysiology, you'll know that the sensory motor channels that
we deal with are just intrinsically noisy.
They have to deal with movement and variance.
And they're a varying trustworthiness as well.
That's kind of context dependent.
So I spent a lot of time in London last year and there's a lot of fog.
So you learn, for example, not to trust your eyes as much and to listen more than you would,
for example, in Canada where there's very little fog.
So this kind of problem that the organism has to solve is a reverse inference problem
in the sense that it has access only to its sensory input and it needs to determine what
caused its sensory input.
So here I've got a spock ear connected to a brain, a very simple sensory system, as
occur in nature frequently.
So suppose my spock ear system hears a snap, a crack of some kind.
Well, what caused the snap is relevant, sometimes for the survival of the spock ear organism
in this case.
Maybe it was just the wind moving the branches of trees and a twig snapped.
Maybe it was a tiger.
And the difference is relevant because the brain, we mustn't forget, its main function
is to coordinate movement.
I forget what the animal is.
There's a very simple animal that lives in water and it has a nervous system for the
first phase of its life.
It moves around.
In a metaphor for tenure, what it does when it reaches maturity is it attaches to a rock
and it digests its nervous system.
It's the first thing it does when it doesn't have to generate movement.
When you don't have to coordinate movement anymore, you don't really need a nervous
system.
So out of multiple possibilities for action, the brain has to determine what exactly caused
the sensation.
Was it a tiger or was it just a twig?
And has to coordinate action in an adaptive way as a function of its inferences.
So this is another motivation, right?
Active inference is useful because it tells us how this happens.
Okay.
That was the motivation introduction bit.
We can now move to predictive processing unless there are questions about that opening
part.
Are we good?
All right.
Okay.
So predictive processing is a theory of how the brain works.
And it's essentially an instantiation of active inference applied to the dynamics of the brain.
And it's a good starting point to understand active inference.
In part because historically this is how the sequence kind of originated, right?
These approaches were developed in theoretical and computational neurosciences.
And only recently through the work of, I mean our group at McGill and the work of others
has been extended to model social and cultural phenomena.
So we'll start with the brain.
So neuroscience 101, the traditional view of cognitive processing that you'll learn
in any neuroscience class is that the brain is essentially an aggregative bottom-up feature
detector.
I'll unpack this sequentially.
The brain is, first of all, the feature detector, meaning that the more sensory areas of the
brain that are supposed to be like lower down on the processing hierarchy, what they do
is essentially detect features, right?
So in the primary visual area, for example, you're sensitive to bars and then as you ascend
the cortical hierarchy, you, and this is the other keyword, aggregate these statistical
features together until you arrive at something like a complex percept.
So two things to note about the traditional view.
On the traditional view, the bottom-up signal is the one that's driving the show, right?
Essentially what the brain is kind of cast as on this view is a kind of passive collector
of sensations that get combined, aggregated together, and yield our world of experience.
Another thing to note is that on this view, the descending connections that descend from
also so-called higher areas like the prefrontal cortex back down to the sensory areas, these
are seen as modulation or feedback.
So on this view, the brain is a passive organ, it receives sensations, it aggregates them,
and the top-down stuff is just feedback.
So there are a few weird things about this for people who study the brain in more detail.
One thing to note, for example, is that roughly 80% of the brain's connections are doing this
feedback thing, right, or descending.
So in a sense, it would sort of be surprising that 80% of the brain's energy budget goes
to something that's just feedback, right?
So I mean, for someone trained in philosophy like myself, another very important problem
is like, where is meaning supposed to emerge from this, right?
So somehow I'm supposed to be aggregating geometric features of stimuli, and then meaning
is supposed to pop out of that somehow.
It's not clear how that's supposed to happen.
As is often the case in neuroscience, we give a nice little name to our ignorance,
essentially.
It's called the binding problem in cognitive neuroscience.
So how is this information bound together to produce a meaningful stimulus?
I mean, what we think is that this is maybe just the wrong way to look at the problem.
Maybe no answer is forthcoming.
So the predictive processing view or the active inference view flips this on its head
and says, OK, well, maybe this top-down thing is what the brain is mainly engaged in.
Maybe that's the main activity that the brain is busying itself with.
So from this point of view, the main activity of the brain is essentially to produce these
kind of top-down predictions about what it should expect to sense next, right?
And what flows up, right, so what's traditionally thought of as the signal, right, is what we
call a prediction error.
So it's essentially the difference between what was predicted at a given moment and what
is actually sensed.
I'll return to this in a second, like in a lot of detail, but a few things to note first.
I guess the most important is that really conceptually it flips the traditional view
of how the brain works on its head, because what we're basically saying is the top-down
activity is the main driver, and essentially sensory information is feedback, right?
The feedback is not what we thought it was, in a sense.
So it's sort of like moving from a conception of the brain as a passive collector of data
and a combiner of geometric or statistical information.
Moving from that to a view of the brain as a kind of query machine, right, it's sort
of a perception is like a Google search, right?
You ask a question and you get an answer.
So a visual cicade is asking the world a question, essentially.
So perception is an answer to that question.
So it's a complete inversion of the received view, in that sense.
So now we're going to get a bit more technical.
So how does this work?
It works via this broad framework that we can call generative modeling.
You're going to see this a lot in the following slides.
The scheme is always basically the same.
You have two things.
So when I say the active inference is simple, I mean it.
You can essentially explain it with two circles and a line, as I'll try to do now.
So the two circles here represent, on the one hand, data, right?
The data that's available to us, so observations, things that are capable of
registering observations or measurements on the one hand.
And on the other, the hidden or latent factors that actually cause the data
that we're observing, right?
So in this graph, causality flows from left to right, right?
You have states in the world that cause the blue arrow,
the data that we're observing on the right here.
And so what we want to do, if we go back to this problem that I presented,
the brain having to solve this inference problem, right?
Is it a tiger?
Is it a twig?
So basically the brain, according to active inference, at least,
what the brain is busy doing is kind of reversing this arrow, right?
So the arrow of causality goes from hidden states to data, and
what we want to do is move from the data that we have to an inference of what
the most likely causal factors are that caused the data, right?
So I say data in a kind of broad sense, because it's important to keep in mind
the historical context in which these methods were initially developed.
My mentor at UCL, Carl Friston,
developed these generative modeling techniques for fMRI initially.
So in fMRI, which you have is a bold signal.
And what you want to infer is the underlying neurobiological activity
that caused the bold signal.
So just by show of hands, how many of you are familiar with neuroimaging and
like more technically?
Okay, so a fair bit of you.
Just as a refresher, for those of you who don't know, when you do fMRI,
you're not directly measuring brain activity, right?
These fancy heat maps of the brain that you see are not direct measurements
of brain activity.
What we measure is a proxy for brain activity, right?
So it's basically a measure of, this bold signal stands for blood oxygenation
level dependence signal.
It essentially consists in putting someone in a brain scanner,
which is basically a big magnet.
And then you measure essentially a byproduct of oxygen consumption
by neurons in the brain, right?
So what you're measuring is an effect of the neural activity
that you really find interesting.
And so what you would like to do then is from this signal, right?
From this data, you want to infer the most probable neurobiological
activity that caused the data.
So this is the technique that was pioneered in the 90s by Carl Friston.
In the 90s, they were using statistical parametric mapping,
which I won't get into very much.
But essentially, for reasons that we'll be able to see a bit later,
these techniques allow us to move from this data to a model of essentially
the connectivity of the brain, the most likely connectivity that would have
generated the data that we're interested in explaining, right?
Active inference in its more familiar format, for those of you who work on it,
comes in at a second level.
What happens when the data that we want to explain is literally the sensory
signals that the brain is receiving all the time?
How does that change the general scheme?
Well, it changes it in that what we're doing now is constructing a model,
not just of the underlying neurobiological activity that causes signal
that we measured, but rather constructing a model of what causes
the sensory signals of an organism.
So the major part of what causes our sensory experience is us acting
in the world in various ways, right?
Think only of the visual saccades that you're performing many times a second.
We are, as embodied agents, are the main causal driver of our phenomenology,
in a sense.
So active inference, per se, is about characterizing this, right?
It's about characterizing a model of how our sensory data were generated.
So I'll return to that in a lot of detail later.
I just want to quickly say that there's a new kind of tier of active inference
that's being developed right now.
This is probably the least explored, most cutting edge application of this framework.
What does it look like when the data that we're trying to explain is
outcomes of the diagnostic process, right?
So the data that I'm trying to explain, for example,
is a schizophrenia diagnosis or a depression diagnosis, right?
So then the model that we're writing is a model of the process that generated
the diagnostic classification, per se.
And for me, as a philosopher, this is really interesting because this is
where it gets really meta, right?
Like this is where we are part of the model, in a sense.
We, as clinicians and researchers, really are represented in the model
that generates the data if the data is psychiatric diagnosis.
Anyway, so the point of generative modeling, the way that it's been used
for a few decades now, is this kind of very simplified pipeline.
Where on the one hand, you have an experimental setup that generates data
that you want to explain.
And on the other, you have a computational setup that is effectively
a model of the process involved in your experimental setup.
So then you get this kind of circular relation going, where your
experimental setup provides data, you can essentially model or
create models of the most probable causal process to have generated your data.
That, in turn, can be used to refine your hypotheses and
your methodology that you're using in your experimental setup.
And so there's this kind of circular motion between modeling work and
computational work that we capitalize on, essentially, in the generative
modeling paradigm.
Okay, so what is a good model?
A good model is a model that generates a small amount of error.
So we'll see what that looks like in the brain.
So remember, I gave this kind of general framework to understand what's going on
in the brain in terms of active inference.
If we wanted to unpack it a bit more, at any layer of cortical activity,
or really of any neural activity, according to this framework,
what's happening is that layers above and at the same layer,
any unit is receiving predictions about what it should sense.
And basically, what's going on is a constant comparison between what the brain
expects to perceive and what it actually does perceive.
And the discrepancy between these two signals is what gets shuffled up
the hierarchy in this kind of fashion, right?
So, I mean, interestingly, this happens as early as the retina.
So, I mean, I won't get into the neurobiology too much,
but there's reason to think that this predictive architecture stuff happens
everywhere, at every level, that really there's no prediction-free layer
of cognitive activity, as early as the retina, like I was saying.
So if we want to think about it metaphorically, but we're sort of moving
towards a formalism here, like what it means then to minimize this error
quantity is basically you have a cloud of data that you're trying to explain,
and you're constructing a model of how that data was generated,
and therefore what you should expect of that data.
So what you're essentially doing is fitting a curve to a cloud of data,
and the prediction error per se is this distance here.
It's this distance between the value that the model predicted
and the value that you're actually perceiving.
So there's a story to tell about thermodynamics and information theory
that I'm going to skip over, but we can return to later if you're interested.
So essentially, according to active inference slash predictive processing,
there are two ways to minimize this quantity, right?
This discrepancy between what I expected and what I perceived.
The first is to change your model.
I think it's the most obvious, right?
If there's a discrepancy between what you predicted and what you perceived,
the simplest way to reduce the discrepancy is just to change what you predicted.
So we just change the model.
We'll see what that means a bit later.
We adopt a different model, and this model generates less prediction error
than the original one, right?
Which means that it's better.
It means that it's more reliably explaining the variance in your data.
Of course, it's always possible to overfit.
So I mean, this is just a brief mathematical excursus,
but for those of you who have done statistics,
you'll know that if you have n data points,
you can always arbitrarily construct a polynomial function of degree n plus 1
that'll go by all of the points that you're interested in,
but capture none of the trends that you're trying to explain, right?
So I just ad hoc, you probably can't tell, right?
Because it's so gracefully traced.
But I ad hoc constructed a function here, which I assure you is a very rigorous one,
and it passes by all of the data points, as you'll notice.
But what you'll also notice is that there is a trend in the data,
and that this function isn't capturing it anymore.
So you might think that if the brain does something like that and overfits data,
then on the long term, it's going to generate a lot of prediction error.
And so some disorders, you might think of delusional ideation,
and this kind of thing have been understood in terms of overfitting.
So overall, you're better with a slightly simpler model that generates some error,
but that on average sticks to the trend in the data better.
So this also speaks to a more general, I guess, philosophical point here,
is that in this framework, the error is your signal.
When we say minimizing prediction error, we don't want zero prediction error,
because that would mean having zero signal.
So one way to think about this is your model has to be simpler than the reality
that you're modeling.
So think of a map.
A model is basically a map.
A one-to-one scale map would be completely useless.
If I had a map of this room, I wouldn't be able to look at it,
and it wouldn't contain information in a way that could be used in a useful fashion.
The model has to be simpler than the data.
This means that just by construction, there will always be prediction error.
That's how it works.
You need prediction error because it's your signal.
Okay, so first way to minimize prediction error is to change your model.
Second way to minimize prediction error is to change the world.
So if there's this discrepancy, you might want to make your model more like the world,
but you might also just want to make the world more like your prediction.
And so in active inference slash predictive processing,
action itself is understood as a form of self-fulfilling prophecy in the following sense.
You start off with a prediction of action and you're not moving, right?
So I don't have a prop handy, I usually do, but suppose I had a glass of water handy
and I wanted to reach for it.
Well, the way that it's unpacked in the predictive processing framework is
I produce a prediction of movement, but I'm not moving, right?
So ipso facto, if I'm not moving and I expect to move, that induces a prediction error, right?
And so the cool thing about using a prediction error as your signal is that it updates over time,
in real time, right?
So I basically initiate movements that in real time reduce this prediction error.
So it can be used as a kind of knowledge driven real time signal
that enables adaptive motor behavior.
So this is just what I said.
You generate an action prediction, you're not moving, therefore a prediction error is induced
and that can be used to guide online motor behavior.
Okay, so now I'd ask you to buckle in really tightly because we're going to do some math.
So we're going to look at the generative models themselves.
I've talked a lot about the models, etc.
So what are these models?
So remember, the general flavor of this is we have data and we're trying to infer the
most probable latent states that cause the data that we're trying to explain, right?
So causality flows in this direction, inference flows in that direction.
Okay, so how exactly do you quantify, you know, how well a model explains your data?
So the way that you typically will do this is by constructing several alternative models,
right, that are each encode a different hypothesis about how the data might have been
caused and then you evaluate how well that model accounts for the variance in your data.
So if you want to compress that algorithm into like a quantity, what you get is this
spooky variational free energy thing, which, you know, you might also just call the model
evidence or a bound on the model evidence.
Basically this variational free energy thing is a measure of how much evidence is provided
by the data for a given model of the process, right?
So it's important to stress this, the free energy isn't an energy in the sense of thermodynamics,
right? It's a measure of how well your model explains the variance in your data.
So I think that's an important takeaway, you know, to like understand what is going on here,
right? Let me repeat, the variational free energy is just basically a measure of how well your
model explains the data, the variance in your data. Yes?
Okay, well, so the the model evidence itself, if you actually try to compute it, usually
just for calculation reasons, you won't be able to do it. Typically it's because these always
involve a normalization term, you kind of have to divide by this term. And this term
calculating it involves summing over an infinite set of states. So basically you can't do it
analytically. And so what you do instead is you construct this quantity, the variational free
energy, which is basically an upper bound on the model evidence. What that means is that like
the model evidence, your variational free energy basically tells you you at least have this much
evidence, right? You might have a bit more, but you at least have that much evidence from your model,
for your model, right? So just to rehearse this again, right? So always the same formula, you
have some data, you're trying to construct the most probable model of the process that caused the
data and you get added through inference. So this is what the models typically look like. And
Lawrence once told me that just presenting these models is basically an act of intimidation.
It is more complicated. This one is very consoling and comforting.
Thank you. So rather than engage in this act of violence, I'll try to unpack a bit
what these models are, right? So the most basic generative model is this thing here and this thing
here is just this thing here, right? Everyone sees, right? You have your data, you have your
most probable causal states and a relation of inference between them, right? So here you have,
it's just flipped, here you have your data, right? The most probable causal state that caused your
data that you're trying to infer. And now what we're doing is just adding a bit of mathematical
niceties. Mainly we're parameterizing these relations, right? So this a thing is just a
likelihood mapping and basically what it does is specify for every state, if this state was actually
the case, right? What kind of data would I expect to see, right? So for example, if it is night,
I would expect it to be dark outside. This is the kind of thing that this a characterizes. So one
important distinction to make sense of these graphs is that the circles are either your data
or the quantities that you're trying to infer, right? And the squares are essentially parameters
of the relations between these circles, right? So they characterize the arrows between. So
like I was saying, S is just your state that you're trying to infer. O is your data. A is your
likelihood mapping from your states to your observations. Basically, like I said, assuming
that this state is the case, what is the probability of observing this or that? And D is just your
prior beliefs over states. So like independently of any observations that you might make,
what do you think the state is, right? So just I'll run through some of the math, right? So if
you see on the left hand side here, S equals D is just saying, well, my guess about the state
independent of any data is just my prior about the state, right? Pretty simple. This O equals
a S thing. Again, it's just describing this relation that the likelihood mapping has, right?
Your outcome just is a combination of this mapping between your states and observations and your
beliefs about states. And here to the right, what you see in a mathematical form that I won't get
into too much is that the state that you predict, right? Your posterior estimate over states is
equal to some combination that I won't get into too much of your prior beliefs and what you learn
from your observations from the data and how that relates to states, right? So these look very
intimidating at first, but like we can unpack them in a way that's okay. So these were the models
up until that we were using up until 2015, roughly. And in 2014, 15, 16, what happened was that we
started considering temporal depth and temporality. So here we're adding, and it looks a bit more
complicated, but we're really just stacking the first layer model, right? Does everyone see that?
Like, see, this thing here is just this, right? So basically, the only thing that we've added
here is this B matrix. And the B matrix specifies your beliefs about the way that states transition
over time, again, independently of the observations that you're making. This is very clear from the
equations here to the left. So it just says your state at time t plus one is a combination of this
B matrix, right? That captures your beliefs about the way states transition and the state at time t,
right? Nothing exorbitant or complicated. And again, the posterior thing is just telling you,
okay, your posterior belief about a state is just equal to some combination of
your state and B in the past, your state and B in the future, and what you're seeing in the present,
right? So again, it looks intimidating, but it's actually less complicated than it is.
I note HESP at all 2020. This presentation of the material was developed by one of my
close collaborators and friend, friends Casper HESP at the University of Amsterdam. This is from a
paper that we've pre-printed now called Deeply Felt Affect, which, I mean, besides articulating
emotional inference using the active inference framework, also presents a tutorial of the more
formal kind of package that underwrites active inference. So if you're interested, I can send
you the paper. Okay, so so far we've talked about perception from moment to moment, right? This
relation between O and S mediated by A. Then if you consider, you know, the sequence of states,
what you're doing is introducing beliefs about the way the world transitions. States evolve over
time independently of your observations. You'll notice that your prior here has effectively
disappeared from these steps here because your prior just feeds in your first kind of estimation,
and then it kind of goes on. So you're really just using S2 as your new prior for the next time.
Sorry, S1 is your prior for the next time step and so on. Okay, but so far this could have just
been a tutorial on reinforcement learning. Active inference does something special, which is this.
So again, don't panic. This is just what we've just seen. All I did here was add a circle,
or which is this policy selection thing, this Pi thing here. So in active inference, the way that
action selection is implemented is essentially by choosing transition matrices, by choosing your
beliefs about the way that the world changes over time. So again, we have these B matrices
that we just discussed here, right? These beliefs about the way the states in the world transition.
But what this policy thing, this policy selector does up here, is effectively choose a series of
B matrices, right? So to act is to have beliefs about the way you think the world should evolve
in this kind of self-confirming way that we discussed earlier. And all the stuff at the top
does is just like put the variational free energy thing into the loop. So this is definitely beyond
the level of mathematical discussion that I had in mind for today. But essentially,
you have this G thing. G is just your expected free energy, right? So it's the amount of free
energy that you expect for every possible action that you could take. And essentially what this
thing is doing is selecting the action that leads to the lowest free energy, right? So again,
it's just this action selection thing that leads to, you know, prediction error minimization,
or that runs on prediction error minimization. What you'll notice is that pi, again, is a circle.
It's one of these quantities that we have to infer. So contrary to, you know, more traditional
schemes in motor control, where you just program a command and it's affected, in active inference,
your little agent is effectively trying to infer, well, what am I doing, right? No, but really,
like, yeah, well, on the basis of your prior beliefs, right, and on the data that you're
receiving from your senses, what is it that I must be doing, right? So this again, contains a lot
of weird math that I'm not going to go into, but I just wanted to show you this because it's cool.
So here you have your generative process. So this is just a basically a
model of, like, a model in the sense of we are modeling just the process that actually
generates the data that's sort of, like, invisible, invisible to us, right? So this is sort of like,
it's hidden to us, but like, there is a process that generates our data. And this is a Forney-style
factor graph representation of the generative model that I just showed you. It's all the same
terms, right? You've got your B's, your D's, and your A's, and all that. I won't get into how you
go from one to the other because it's a little complicated. I was just putting this slide up
because I wanted you to notice two things. Well, it's the same thing, but it has two elements,
where the process that generates our data and the model of our data, like, where they meet,
is at two points. You have these U-things up here, which are the actual actions that you're
performing, right? And these, the O's, your observation. So basically, the model, the generative
model, and the generative process meet at action and observation. Like, that's sort of the two
connective points. I mean, this is complicated and we can return to it. Then, this is the last
technical thing I wanted to show you. Like, where we're at now with the modeling is that we introduce
a new layer of states into the model that are about states at the lower layer. So in our new
worker working on metacognition, emotional content, and kind of self-control, mindfulness meditation,
and this stuff. So what we did here is connect a layer of states at the top here with their own
transition matrices and everything to states at the lower level. So what we've begun doing is
treating states, oops, is treating states at the lower level here as observations. So like,
we're treating the results of inference as new data, right, that the system can then use
to make inferences about itself and basically how well it's doing. So I told you this paper is
called Deeply Felt Affect because we use this kind of layered structure to account for basically
beliefs about oneself, right, and eventually self-control in this kind of thing.
So Matt, for the sake of people who may get a little lost when the discussion is too abstract
and too formal, if you could think of a couple of concrete examples without technical language,
going beyond something like, oh, here's how a monkey moves through a room to find a high-quality
treat, something more like, how would you help make sense of human emotions and moods, for example,
checking in with oneself in phenomenology, how would you make all that relevant?
Well, so this bottom part of the model, you can make it do whatever you're interested in
making it do, right? So you could have a model of, I don't know, having a conversation, right? So
in this kind of model of having, it might be simpler to explain that bit just with this. So
the data that you have to explain might be, you know, configurations, spatial configurations of
expression in someone's face, you know, the auditory sensations that you're registering, right?
So for each of these modalities, you have an A matrix that specifies the way that you believe
these sensations are typically related to the states that cause them, right? And you have this
policy selection thing that affects beliefs about the way that states evolve, right? So
in a conversation, for example, right, you would be resolving that continuously. I would be inferring
these lower-level states like, what is Sam telling me now, right? And I would be selecting a policy
that, you know, that facilitates this interaction that ideally gets us like to some kind of coupled
point where we're able to interact. So the point of adding these additional layers is that now I
can make inferences about my inferences. I can be like, well, am I sure that Sam feels good today?
You know, like he seems tired or something. Or I don't know, like, yeah, it's things to do with,
for example, confidence in your own judgments. How well am I doing? How do I feel about these
inferences that I'm making? This kind of thing. So inferences about our inferences. And the reason
why this is important, I think, just generally for humans is that, you know, I'm telling this to
you. You know, in particular, most of human thinking is thinking through other minds, right?
It's thinking about other minds.
I don't want to take us too far away from your main narrative, Sam, but the way you're describing
it now, it seems like there may be two things conflated, or maybe just one part of them being
instantiated, because you're saying, I'm talking to you and I'm thinking, gee, I wonder
you know, how Max was feeling today. I wonder if
I'm thinking something about the nature of our conversation. And that's a kind of
self-consciousness, if I'm thinking that. I wonder if I'm making a good impression.
So what it brings to mind is, is there a generic self-consciousness, like you just have lots of
those thoughts, because this version is just very specific. I just have one question, right?
I just have one hypothesis. But in real life, these things come in bundles,
and that's what we would then call an affective stance, let's say. I'm feeling uneasy with you,
so I'm engaging with lots of alternative hypotheses and lots of uncertainty at that level.
So then I'm getting clear as I'm talking, which is probably part of the point here.
So how would you model that? In other words, it's not simply one
thing, one bit of metacognition or one bit of reflection on the nature of interaction.
It's a different, I would describe it informally again, it's like a different stance or a different
One important thing is that we can entertain more than one different model, right? So there's some
interesting work by Isomora, I think, who essentially implements like a simple,
and these are very simple toy systems for now. What we're doing now is scaling them up to human
systems, but it's basically a bird simulation, and it listens to eight different birds,
and it recognizes them, essentially, because it entertains eight different models
that might correspond to this or that. So the capacity to simultaneously entertain different
models might help to sharpen what I'm trying to say again. So to me, it's different if I say,
okay, I'm uncertain where I stand with Maxwell, and so I have to entertain many different models
versus I'm just, here's my hypothesis, and I was having one policy at that level
is very different than having many policies. So to model that, do we have to have yet another
level just deciding, okay, I see your question, right? Well, the thing to note about is that kind
of uncertainty. The thing to note about this is that this top level of inference makes this bottom
level accessible to the system, right? So basically inferences at this bottom level just happen,
right? So they guide action but in an implicit sense, whereas these states are literally about
the states below. So attentional states, for example, exist at this level, right? States about
my sensory states. So that helps because then if you think what happens when you practice mindfulness
meditation and you stabilize states. So you're going from what my state of mind is like, where
there's not one of those red circles, there's like dozens of them, and they're all different
hypotheses, but it's actually happening to me and they're all differing with each other, right?
Which then has a disruptive effect, let's say, on my attentional state versus having one that is
coherently sort of reinforcing a particular mode. Anyway, I guess getting too far down into one.
No, what you're seeing is that's really like, I think, an important point. I mean,
and in work that we have in, that I'm not going to be presenting today, which is basically like
where we're at right now, we've got a three-layer system going to explain mindfulness meditation.
This is a work by Lars Sandved Smith, in particular, that we're doing with Carl
Friston at UCL, where basically on top of this, we have yet an additional layer to make the
attentional states opaque to the system. So that's kind of what I'm getting at, the idea that you
could learn to manage your own attentional state so they can be problematic. So this is already
there kind of implicitly, right? Look, we have a B matrix that links these different things. So we
already have beliefs about the way that these higher-order mental states are transitioning.
What would it look like if we implemented policy selection there? Because you can,
it's a B matrix. So why not just like, hook these up to a higher-level Pi? You could.
You could, and that's what we end up doing in the, anyway. So I guess I will be,
we will be putting this on YouTube. Are the B matrices, you said you can treat them like
policies? A policy is a sequence of B matrices, right? Because that's what it is, to select an
action under this framework, is to change your beliefs about the way that states are supposed
to evolve, right? It's an array of B matrices. Yeah, it's a, well, it's, in the more, why wouldn't
you just use your B matrices? Well, because in the most simple case, it's just a series of B
matrices. But what if you're considering counterfactual depth, right? Like, yeah, then you have like
trees of B matrices and so forth. You can create a network. Yeah. There's feedback between like
different contexts and principles based on context, for example, like Dr. Kirmler's,
maybe, but you're getting that. Well, let me try to just finish. I have about like 10 minutes
left of material. One small point and you'll come back to it. So, because for me, the, so it's,
it's appealing again that you get these hierarchical levels and it seems like you're getting more,
getting things that can model self-reflection and all these different, you know, strategies for
managing, etc. However, the more layers you add, it seems to me, you don't necessarily have more
data, do you? So, exactly. The problem of like your, your complexity is back to the overfitting,
whatever, having way more complexity to your model than you actually have in most,
I mean, that's a really good question. And we struggled really hard. You know, last time I
was in London, a lot of what we did was ask, well, do we really need a third level? It,
can't we just do policy selection at the second level? But the key thing is this opacity thing,
is that to make these states of inference accessible to the system, to, to, to allow the
system to use them as data for further inference, you need a higher order set of states. So that's
sort of, so if you give me 10 minutes, I'll be done. Cool. Okay. So, why is this framework
Bayesian? I mean, we've sort of seen it before. But basically, Bayes, Bayes' rule is just a way
to combine prior probabilities with your likelihoods to get posterior probability.
So, yeah, Bayes is important, so I put it on a little crowd. So your prior probability is just
the probability of some event before any evidence is taken into account. Your likelihood is the
likelihood of some event, the probability of some event given some evidence. That should be
probability of some event given some evidence. So just to illustrate the difference, if Houdini
magics away an elephant on stage, well, you know, there's a high likelihood that the elephant is
gone, you can even go on stage and collect more data, the elephant clearly is gone, right? But
we know that there's a low prior probability that elephants just dematerialize, because elephants
are solid objects, and right, those don't typically dematerialize. So Bayes' rule is just a way to
combine these quantities optimally. So basically, the, a cool thing about this is that you can
always use your posterior at one step. So the result of combining your prior and your likelihood
as your new prior at the next step. So there's this nice kind of bootstrapping thing that you can do.
And the way that this relates to what we've been talking about is basically the descending
connections, right, they carry your priors, and your prediction error is basically always
integrating the data that you're, that you're generating. So it's essentially a likelihood.
And the Bayesian brain says, okay, well, this is what the brain essentially looks like, right?
You've got likelihoods flowing up in the form of unexplained prediction error,
and prior probabilities flowing down, right, in the form of neural prediction.
These, these schemes are typically hierarchical, I hinted at this earlier. The reason why they are
is that as you, mystically, as you go up the hierarchy, the things that are represented are
more stable over time. And as you go towards the more sensory end, things change faster, right?
So for those of you who are familiar with Fourier analysis, basically, you can take any image
and decompose it into frequency bands, right? So high spatial frequency and low spatial frequency
information, right, which, which you'll notice. So high spatial frequency is to the right here,
low spatial frequency is to the left here. And what you'll notice is that in a conversation,
high spatial frequency information changes much faster than low spatial frequency information.
I mean, unless you're someone like myself, you know, just moves around his face and like,
makes funny expressions, like typically most people, like, you know, their lips move faster
than the rest of their face, right? So you might think that this is implemented in the hierarchies
that the brain encodes, the hierarchies of information that the brain encodes. And according
to the predictive processing framework, this is precisely what we see. So I'll skip over that.
All right. So yeah, so you have hierarchies of information that encode regularities that are
time sensitive. And you have prediction error minimization going across the hierarchy. And it
essentially explains the way that the brain reacts to stimuli. All right. So that was predictive
processing. We can scale this up is what I submit to you now. So I have this winter is coming.
Just to talk about my favorite example of active inference, it's like, well,
when I get cold, I don't know about you, but I put on a parka, especially like in Canada,
it's cold, right? So we can cast this as active inference as well, I think.
So the point for the next two slides is basically going to be, it's not just the brain that engages
in active inference, it's every cell in your body, every organ system in your body, and effectively
maybe social groups as well. And this is what we work on. Okay. The last bit of math that I want
to introduce to you, I think is simpler than the rest. It's called a Markov blanket.
And what I want to submit to you is that rock, cells, organs, animals, social groups, basically
anything that exists at all has a Markov blanket. So a Markov blanket is just a way to use statistics
to answer what is traditionally a philosophical question, right? The philosophical question being
what does it mean to exist? What does it mean to exist as a thing? What does it mean to be a
thing? So I mean, if you ask a philosopher, they'll tell you like a story about metaphysics and,
you know, like type token identity theory and whatever. We've eschewed all of this for something
much more simple, which is to say, okay, well, if we're interested in a system, right? So suppose
the system that we're interested in is the brain, right? Because we've talked about it and we want
to differentiate it from the environment in which it is embedded. What we'll do essentially is
introduce or define a third set of states that mediates the causal relations between
the system that we're interested in defining and its environment. So these are known as
sensory and active states. So these are metaphors, of course. The way that they're defined is by
their connectivity, right? So sensory states cause internal states, but are not caused by
internal states. And active states caused, but are not caused by external states.
So, I mean, the point of doing this is to say, okay, to exist is to be endowed with some degree
of conditional independence relative to your environment, right? So if you consider this
mass of gas, right here, it's not a system because it'll just dissipate, right? Like,
there's no robust sense in which it's independent of its environment in any sense, like, you know,
it's gone, right? A Markov blanket is a way of saying conditioned on the existence of this set
of states, the sensory and the active states, the internal states of the system are independent
of the external states of the world. Is that clear to everyone? Okay. So active inference then
is a story about how internal states, which encode our model, right? And the active states,
which are like our skeletal muscles and so forth, change to minimize free energy.
And the end effect is to allow this inference process to happen, right? So the inference here
meaning that the free energy or prediction error diminishes, right? So making the internal states
more like the external states and vice versa. Again, this is just this story, right? It's just
that as we've seen inference also occurs through action. So basically, like, you have the state
estimation bit going on here where we're kind of inferring what should be going on in the world,
you have the policy selection bit here. But this is just the story that we told, right? But just now
presented in terms of like the existence of the system. I'm going to skip through that. So yeah,
this is a drawing by a famous physicist, Huygens, and Yelda Brineberg has it in one of his papers on
this. Like, an important point is that like an advantage of using active inference over other
frameworks is that it just, it comes from physics, right? So you solve this inference problem, but
using a framework that basically says, well, inference is sort of like a rock falling down
a cliff. You're just falling to an energy minimum, right? So the reason it's called variational free
energy is with analogy to the thermodynamic quantity free energy, right? In thermodynamics,
free energy is the amount of energy left in a system that can perform work, right? In the information
theoretic context, the variational free energy is basically the amount of wiggle room you still
have on your parameters to get a better representational grip of the situation, right? It's like,
yeah, how much room is left on your parameters to do work, to do representational work? Okay.
Last thing, basically, that I'll be talking about today. So here's our Markov Blanketed
System again. I submit to you that all of the components of this system are also systems,
right? This is the observation that we started with. I'm a system, I'm an organism, but I'm made
of networks of organs that are themselves systems, and the organs themselves are systems of cells and
so forth. So every component of a Markov Blanketed is itself Markov Blanketed. So in this 2015 paper
by Carl Friston, which I think is the first in literature to do this, they effectively connected
two levels of description. So what they showed is that on the assumption that what you see here first
is I think seven, eight cells, eight cells that share a generative model and over time reach a
target configuration. So it's basically a little creature with a head and a tail. Everyone sees
that. And so what you have plotted here is basically the beliefs of each cell about what kind of cell
they are essentially. And so you have your free energy plotted here. And so what you see is all of
these cells share the same generative model. So basically they're able to infer their place relative
to other cells, so long as they're able to communicate with other cells, because we all have
the same expectations. We all expect to sense the same kinds of things. So basically the little
units start off and their free energy spikes because they're trying to figure out what's
going on. But then as they communicate, the free energy starts to go down until it reaches a minimum
value. And when it's reached its minimum value here, the simulation reaches its target configuration.
So that, for me, this was like an eye-opening moment where I kind of,
with Carl, realized that this is how you effectively connect levels of organization. Units at one
level sharing a generative model are able to enact a target morphology. So from there we
generalize, this is from one of our physics of life reviews papers. We're essentially just telling
the story of how any Markov-blanketed system itself is composed of Markov-blanketed systems.
So you have this kind of recursively nested systems of systems of systems of systems approach,
which you might think of as a vertical stack of systems. I say vertical because there are two
dimensions to this system really. There's a vertical stack where cells compose organs,
which compose organ networks, and so on and so on. But there's also a horizontal stack where at any
scale of interest, the relevant actors like cells, for example, are also in the process of
niche construction. They're constructing an environment for themselves and they're effectively
sharing a physical environment. So this kind of gets us here where we started. At least I hope I've
made the point that via active inference there's at least the possibility for something like an
integrated science of culture, mind, and brain that takes all of these levels seriously.
And I'm just going to skip through this right to the thank you slides.
Yeah, so that's what I wanted to share today. Thank you for your attention. Thank you to my
funders. Special thanks to the people listed, particularly those of you who came. And yeah,
thanks for your attention.

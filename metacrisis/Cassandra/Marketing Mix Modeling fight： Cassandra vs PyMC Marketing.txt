Hello everyone! Today we're going to compare PyMC Marketing, the Python open-source library
for marketing mix modeling. They use a special statistics versus Cassandra, the MMM software for
non-technical marketers that guides media locations decisions to optimize media ROI.
We're going to evaluate these two methodologies based on accuracy metrics, ease of use,
actionability of the insights, and overall utility generated for the company. So stick with me,
we're going to dive into both of them. First thing first, we're going to see what is the data
that we're going to use. Here we have the dot set, it's an extremely simple data set in which we
have the first column represents a data weekly basis, we have revenue at a weekly basis, and then
we have a list of medias plus some organic variables. So first we have tvspan, tvmedia,
autobomb, print, facebook, and search. And these represent our media factors. On top of that,
we have one organic factor that is newsletters, and then we have a context factor, which is events.
Everything is aggregated at a weekly basis, and it has between a thing four years of data,
between 2015 to 2018, is going to be enough in order to train our model. Keep in mind that
best practice is to have at least 10 rows per each variable that we're going to use in our
marketing mix model, which means that if we're going to use 1, 2, 3, 4, 5, 6, 7, 7 variables,
we need to have at least 60 rows. If each row represents one week, which means that we need
to have at least one year and a half of historical data. Now let's dive into the PyMC marketing
library, but before I do, in the description here, you're going to see this file that you can copy
and paste, then go here, file, create a copy, which is a data request checklist for Cassandra's
marketing mix modeling. So let's dive into the PyMC marketing. Let's start with that,
and then we're diving into Cassandra's. So first thing first, what we want to do as a first step
is, obviously, install PyMC marketing. So pip install mark, find some marketing, and the
drain start. What I want to start after installing PyMC marketing is to add some warnings, just in
case if anything happens, I want to know anything, any bug happens, I want to know why it happens.
So let's run the cell. What we want to import right next to it is pandas.py matplotlib,
cborn. These two are for graphs. Then we want to install our viz as az. We have PyMC as pm.
This is not strictly necessary, but I imported it anyway. Something in PyMC marketing doesn't
work. I can actually use the original library, which is PyMC. And then we are going to install and
download all the variables and all the functions of PyMC marketing. PyMC marketing, by the way,
is a really new open source library developed by an amazing team. In the past week, I've
studied the entire open source library. I'm not an expert about this library, but I'll try to
show you in the simplest way possible how you can leverage it in order to generate the utility for
your company. So we download all the all the functions from this open source library, and then
we want to actually download the max upscaler and min maxscaler that we're going to use in our
training phase and in order to standardize our data. So let's run this. Awesome. We've imported
all the requirements. The sns set theme is just a way to set the grid for our graph. So I'm just
going to run it. It's really easy. It's not a requirement. First thing first, what I want to
do is I want to upload my CSV file. So let's go here. Let's Cassandra did request checklist example,
I'll upload it here. And what I'm going to do is copy path, and I'm going to paste it here.
The first thing I do is I import using pandas, the CSV file, then replace any infinite value that
I'm having on my data set. Then I fill all the na with zero. So if I have any na value or any cell
in my data set has a non value or an infinite value is first transformed into non and then from
non to zero. Then I check if I have any non values and I print. Obviously, I want to know if there
is still any non values after these operations. This phase is extremely important in order to
validate manually. If the data set that we have has some errors inside as some it requires our
data cleaning operations. So going to run this looks like that we have no non values in our
data set and we've imported everything smoothly. So we're going to create and export the list of
the columns names that we have in our data set to simplify how we're going to fill the form of
identifying with our variables without without the output variables, etc. So let's run the f
columns. We have a list of our variables. And now let's define the output variable as revenue,
the date variable as date week, as we can see. Here we have a script that allows us to identify
and create a list of strings that represents the names of the media variables that have
spanned inside of their of their label. Now you can see here we have TV span out of home,
print, etc. And they're automatically included into media. Then we have context variables,
which are advancing and use letters. With PMC, I'm fairly sure that we don't have organic
variables feature that we can identify. So everything else that is not media is identified
as a context variable. So let's print media and see if it works. Awesome. We've imported
all the variables automatically with one click. Now, I've used Chai GPT in order to create these
scripts for building a seasonality. So what I want to extract from the patterns of my sales
was the seasonality of my business or the business that they were going to model. And it works in
this way. So function called extractable seasonality, we need to define what is the data frame,
what type of seasonalities we want, and what's the window size. So how many records represent
one month. And what we do, I mean, this is the function, we can copy and paste it.
So we run it. Also, we've run the function, we can call it now. Now, what we want to do in this
cell is first, identify what are the seasonality features. These identity features need to be
the Apple variable and the date variable. I want to print them just to be sure that I did
the right job. And then what happens here is seasonal DF, which is a dataset composed by
only the variables that are going to help me understand what's the seasonality. So it's going
to be DF, which is our data frame with our two columns. And I'm going to copy it. I'm going to
rename, I'm going to add a new columns that is going to be called DS. And DS is a normal label
name to identify dates into a dataset when you use profit or use in this case, this function.
We're going to rename revenue with Y, just to standardize. And we're going to identify that
the seasonal configurations here are the period, the quarterly period has 12 weeks inside,
and the yearly period has 52 weeks inside. In order to simplify and identify how you need to
model the seasonality here, you need to identify these two parts. It might seem a little complicated
at the beginning, but believe me, when you get used to it, it becomes really easy. So the last
thing that we do is we insert everything inside a variable extract season. So we create a new
dataset, mainly, and we run the function extract for the seasonality defining what is the data
frame that we're going to use, the configurations, our seasonalities, and what's our window size.
So I'm going to run this cell, I have the right columns, I have the seasonality. If I want to
check if there is any graph that we can run from this, right, what I would interest me is this one.
This is the graph that represents my multiplicative seasonality. So how
did the man changes over the year, over time? And it looks like there is a pattern that repeats
itself over time. Now, let's print this in an actual graph. This function in this cell just
works in order to simplify and show you all the parts of the quarterly seasonality and the yearly
seasonality. Let's run this. Awesome. So as you can see here, there is a pattern repeats itself
quarterly, and a pattern repeats itself yearly. We want to import these two inside of our actual
dataset and add the quarterly and yearly variable inside of our modeling experience.
So what we're going to do, first of all, we're going to split, create a variable that's called
weeks to split separate the test group, the test data frame with a trained data frame,
and we're going to split by four weeks. So we're going to leave the last four weeks
as an data set as a data points, they're not going to be used in order to train our model.
We need to identify, we need to add a new column called the F quarterly, and we're going to insert
the values that are in extracted season quarterly, same thing for yearly. And what we did basically
we added in this part two columns, they're going to be then use in order to train our model and
added as a context variables. Let's recheck and substitute all the infinite values into nonce
and from non values into zeros, we don't want any non value or the modeling procedure in the
training does not work. Then what we want to do is we want to separate the data frame for training
and the data set for testing. And then when we want, and that's it. So let's run this cell. Awesome.
So in here, what we are to do is what we want is we want to create, first of all, the context
variable list, I'm going to add from context the two new columns that we added into our data set.
Second thing that we do is we call MMM, our object that has the function inside called
delayed saturated MMM. This delayed saturated MMM is going to use ad stock and diminishing returns
inside of the training procedure to find the optimal ad stock, the optimal diminishing returns.
And we need to define first, what is the data frame that we're going to use? What is the target
column? And we did what is the date column here we did channel columns, we created the list inside
of media control columns is a list of strings that we actually did here, the ad stock max flag,
which is how many lag effect that can be at a maximum in our data set. And it is really wide,
it means that there can be a two month lag effect, which is crazy. But we're going to keep it like
this. And we want to have your decisionality, we want to model the early decisionality and
consider the last two years. So we're going to run this awesome, it worked, we're going to rename
x and epsilon just to simplify our training procedure. And we're going to add into x all the
values that are not the apple variable or dates, why we're going to define only the apple variable.
So we're going to run this awesome. And then we here, we start the training procedure of our MMM.
Now, these MMM is not really complex. I know I tried to model it before. And it's not really
complicated. It's really easy. There is no multicollinearity. There is consistency on the data.
There are enough records on each variable. So I'm not going to complicate it too much. I'm going to
use x epsilon MMM dot fit x epsilon, just simple target accept is the statistical significance
of each coefficient that is going to come out number of chains. So how many iterations it needs
to do, and how many tests and experiments and draws needs to do in order to train it in order
to tune it. So I'm going to run the cell. And it's going to take around 20 minutes, more or less
in order to finish. So I'm going to leave a train in a little second, we're going to see a bar plot
that actually loads over time. It started loading here. It's going to take some times in order to
load here. While it trains, let's go and use Cassandra in order to train our model, because any
training is going to take some time. So let's go inside and see how it works.
So we are inside of Cassandra. The first thing that we want to do is we want to add model,
I want to use my own data set, and I'm going to drag and drop our CSV file.
Awesome. So we've added our CSV file, we click next. And in this case, we need to do the same
thing that we did previously, we need to fill the form just to tell the model what each column
represents. So let's call it YouTube, date column, which is date week, the output variable type is
revenue, the KPI is revenue that we're going to model as an output variable. The media channels
are these ones are already selected, paid media, organic and contextual. Let's define the country.
Let's use not a state set thing. And now what we need to do now in this experience is just to
click next. That's it. Everything looks correct. Let's continue to the modeling procedure. So
dear, what we have here is an EDA analysis, already down for us that actually shows us the
pattern of each variable over time. We identify as if there are some warnings. And these warnings,
in this case, our DB, at the one previous search have lowest band volumes. And this is interesting.
So I did not know that. I thought I was using a data set that actually was working with no
problems. But it looks like there are some problems. In fact, if we hide all channels,
and we check on your Facebook, compare it with TV, we see that the dimension of this data is
completely off. This is probably because there is a problem with the data set that we need to check.
So let's go back to our data set here. And it looks like that we're taking three, yeah, 72 million
per week is a little too much. What I did wrong probably is Facebook spend one is I formatted
wrongly this data. So what I'm going to do is I'm going to divide this by 1000, not 10,000,
but 1000. And this is the actual value. I did this mistake. I did not expect that. But thanks to
Cassandra, actually, could check this before starting the training. Now going to go back here,
going to download the data set, be uploaded here. Awesome. Click Next, fill the form again, YouTube,
define this is revenue. Let's call United States in order to get the holidays from this particular
country and model the seasonality with it. Click Next, receive back the EDA analysis again. Now
our thing looks more concrete and with the same dimension. We see the revenue over time versus
this spend over time. We see all the other variables here. We want to create a model now.
We click create model and a modeling procedure actually is done. The only thing that we need to
do is click train model. And the training starts. So it's a three step procedure in order to train
your MMM. All right, training started. Click okay. We check the status. Okay,
training is starting right now. If it says it's not started yet, we need to wait for a couple
of minutes. We refresh it and we check status. Okay, not yet, but it's going to start really soon.
So we need to go back to our PMC marketing training because we noticed wrongly that our
data set is wrong. So we're going back to our PMC. Yes, it finished the training, but we need to
retrain it again. So going to delete the old data sets that we added here. One, two, and three.
Let's add the new data set, copy the path and add it again on top of it. We need to rerun everything.
So really quick. Nothing is too heavy. Everything used to is really easy to do. Now it should start
MMM fit really soon. It's going to take a couple, I think, around 10 minutes in order to finish
training. Awesome. Took around 10 minutes in order to train this model. Here we have some
posterior analysis, which are extremely hard to translate into channel value. So this is why
we're going to use graphs in order to explain all the information that we can derive from this MMM.
So first thing first, let's create a component fix MMM dot plot component contribution. The cool
thing about PMC is that it is really easy to create plots with the outputs and drive
what is the accuracy with all our actionable insights that we can implement. So let's run this.
Awesome. So in here, we see the contribution that I have. Everything is killed down with the
min max together that we use. But we can see that there is 40% of the contributions explained by
the intercept. Then we have some other variables. The medias have between zero and 20% of contribution.
And then the seasonality has a plus 20 minus 20 contribution in our media mix. This is really
interesting. And it's a cool thing to summarize all the variables that represent media, represents
control and the seasonality contribution in this way, because it simplifies and actually shows you
some macro insights that we can derive from the first training that we do. So after that,
what we want to do here is we want to get a contribution of each factor. So how each factor
contributes to generate sales and how many sales are coming from TV, out of home, Facebook, etc. So
let's run this. We created a data set in order to see what is the contribution of each factor.
And then what we want to do, I'm not going to spoil you with the real diminishing returns curve,
but we want to get the diminishing return insights. So we want to see what's the contribution of each
factor based on the volume of investment that we run that we have. And the cool thing about
PMC is that we can have the confidence interval. So we can have range that represents
the probability of the 95% probability of having that specific output if we spend X, for example,
everything is scaled down with is called down here. This is the contribution. And this is the shape
of the curve. If we want to see out of home, for example, same thing here, we see that if we invest
400k, we get around 150k of revenue. So ROI is really low here. Let's see if we have something
with Facebook. With Facebook, if we invest 75k, we get 100k of contribution. This is interesting.
It looks like in this MMM, this channel works really well. But the value can be between 25k
of contribution if we spend around here up to 200k of contribution. It's really important to
understand that any statistical analysis that we do needs to have some idea of confidence interval.
The idea of confidence interval just show you what is the level of uncertainty that each measurement
has. And having that into each specific graph that we run is extremely helpful in order to
calibrate our decision making on top of these insights. Awesome about this. So we see in here
with 100k, we get a little less than 100k of contribution out of really strange values,
really big values. This is interesting. I think this one summarizes, yes, this graph that we're
going, the running summarizes everything into one plus. So we can compare them. We got to compare
each graph simultaneously. So we can see that TV spend has the highest contribution based on the
X factor, which is how much we invest. And there are other values that we can explore manually.
But what I want to show you step by step is, first, what are the insights that we can unlock,
and then we evaluate the results, comparing the results with the MMM built by Cassandra.
Let's run MMM predicts posterior. In this way, we're running the MMM, our object that we've trained,
and we use the function predict posterior, and we import the X in order to predict the value,
the output variables based on the input variables that we have historically. And then what we need
to do is just plot posterior. Awesome. All right. It looks like it is pretty accurate. There are
some moments that over forgot, as you can see here, you have not blew what actually happened
to the sales. And in light blue, we have some, there is the confidence interval. And on average,
it looks like it is really accurate. But what I want to do is I want to print the R square and
map a, which are two accuracy metrics that we use in order to validate was the level of accuracy
of our MMMs. Now the R square, I have these scikit-learn R2 score and mean absolute error
function they're used to actually derive these values. The R square actually is not really a
an accuracy metric. It's more like it explained, it actually tells you how many variations in the
output variables are described by the variations in the input variables. Then we have the MAPE,
which is the percentage error that we actually see on average. And in this case, we have an 86
percent R square and a 10% 10% MAPE, which is interesting, really, really interesting. It's
not bad. On average, you start leveraging and using the MMM insights when the accuracy level,
the R square level is above 80%. The MAPE is a little big, but we can still use it. Definitely.
What I want to do is I want to scale up everything at their original scale. Awesome. And now we want
to plot the contribution of each factor. So we want to see over time, well, where the factors
that actually contributed to generating revenue. Now, there is this function, I don't love it,
because it's called plot group contribution breakdown over time. I don't love it because
it uses my plot lib. I have some problems into writing every time, but it shows you just a
small picture that we cannot filter the values inside. So as you can see here, there is a big
impact on a certain value here, the yellow one, that I don't really know what it means.
Probably I've scripted the function without including the labels of each color, my fault,
but I find it a little complicated in order to derive the real contribution, because what I
would like to have is a filter each factor, if I want, and identify and analyze which one,
one by one, in order to simplify my analysis. And ideally, I would like to compare these insights,
these contribution insights over time, to the span level that I had historically, in order to
understand whether at certain points, we were overspent or underspent in each specific channel.
Now, I'm not going to, I mean, actually, let's let's run these plot channel parameters. I want to
see the alpha, I'm not going to include a lot of insights about this, because it's a little
technical, I don't want to overwhelm you about this, this one is the same. Let's create though,
for each variable, I want to see what's the ROI measure. In here, what we have here is
channel contribution scale, we compute the channel contribution and the regional scale,
again. And we create and we analyze for each media that we have in our media mix, there was.
As you can see here, it shows us not only a deterministic output, but also the probability
distribution of the rows of each channel. And this is amazing, I would say any marketers,
even though this is extremely difficult to understand at the beginning, but any measurement
that we do based on based on our data, it always has a probability distribution, a confidence
interval is a list of values that have 95% probability of being true. And as you can see here,
this probability distribution has a lot of value. So there is this violet value, I need to print
the labels here, but it looks like this violet value can have a row as between three and zero,
while there is this blue one, it is concentrated between 0.5 and one, this is really interesting.
And this green one has a really wide distribution, like this red one. Now, awesome insights so far.
What I want to do, I want to go to Cassandra and see if it's done training the model. So in here,
we have the new model that we've trained. Let's see results. So differently to PyMC, what we receive
as an output from the training is a list of models, they're actually really similar here
with a certain accuracy. And all of them are ranked by lower rank to the highest rank. And here we
have the best. The reason why this is the best is because we actually try to understand what's
the level of uncertainty, the accuracy and the average error this model has. Now, compared to the
model that we've trained with PyMC, we have 95% accuracy with respect of 86% and 5.99% error
compared to 10% error. Now, obviously, I remind you guys that these are square, which is the
percentage of the variations in the input variable described by the output variable described by the
input variables. Now, as we can see, we have the data set, we see that it is really accurate on
average. We see that we have 28% contribution from the baseline. While we saw previously that the
baseline for the model with PyMC was 40%, search contributed for 24% to be 23. What I want to
see here is the ROI of each campaign type. Oh, this is interesting. Wow. The ROI measured according
to this model are way higher than the one trained with the PyMC. Obviously, PyMC has a higher baseline,
so what we're seeing here is that a big percentage of the baseline is not attributed to the baseline,
but is attributed to the revenue and to the input, the media variables. And the confidence
is the role of pre-consistent. The lowest ROI channel is print that can have an ROI between
2.7 and 7.9. Out of them can have between 4.47 up to 10.60. And this is really interesting.
So, let's go back to the distribution. Here we had other roles, split by each campaign type that
went from 0 to 3. That's actually, oh, this is my bad. Next, expand this to 10. All right. So,
the ROAS went from 0 to 4, not to 3. But the ROAS are really, really different. This is actually
really interesting. And here we can see the contribution of each factor over time. And we
can see the ad stock. So, I didn't find a way to export the ad stock from the MMM object that we
trained. We can actually run an additional function, which is deer, deer, open parenthesis MMM. So,
we can actually see what are all the functions that we can call from this object. Ad stock max
lag, but it's 8 is the one that we inputted. I cannot see any function to actually print or show
me what's the ad stock effect. And actually see it into a graph so I can understand how to interpret
it. But then, right, so let's use this data set, this model. I'll save it. And let's dive into all
the insights that we can try that we can derive from it. So, we go to models overview. Let's select
our YouTube model. We have some suggestions. So, it created some suggestions for our media mix.
And these suggestions are appeared here. And they tell us, right, says that Google search can
generate MMM revenue if it increases weekly budget to this volume of investment. But Facebook is not
performing as good as other channels in your media mix. So, we should reduce this weekly budget
down to this volume. This is interesting. I mean, it's implemented here just to show you
and simplified interpretation of the insights. Because most of the times, if you are an analyst,
you need to explain each graph to each marketing stakeholder. And if you need to repeat this for
every forever meeting, because it becomes extremely expensive, operational wise. So,
this is why next to each graph, there is an explanation natural language that explains
what this graph means and what to do about it in order to optimize your media mix.
Now, we saw the confidence intervals. There are between three and 10, which are bigger than the
PMC. The contribution, the baseline is lower here compared to PMC. Seasonality is actually
similar. And here we have a window of one year from 1st of January to December. But the pattern
is really similar to the one that we saw previously with the PMC. What I want to do is I want to check
for each campaign type and the contribution that they had over time. I want to see how much is
factor that inserted in my model contributed to generating revenue. So, let's say we want to
see just Facebook, and this is the contribution of Facebook. We see that Facebook generates
weekly between 149k and 103k, more or less. This is the range. This is interesting. I want to see
the baseline contribution. The baseline is around 500k per week. Let's add seasonality to it. And
thanks to seasonality, we can actually see the variations over time. And it makes sense because
during the end of the year, we have a positive spike. What I want to see is the contribution of TV,
the contribution of newsletters is really minimal. We add out of home, okay, and then we add the
holidays. There is one thing that it looks like that events have a huge effect here. Yes, they
wear the reason why there is a positive spike during the 22nd of April. So, we go back to models
overview. In the models overview, here, these spikes are explained by the event. I want to see
not only the contribution, but also the ROI over time. So, there were moments in which the ROI
was 214, which is crazy. And there is another moment in which TV ROI was really high. Now,
I don't, this is really interesting. I want to know what happens, what happened in those cases. So,
I hide this channel, I just checked TV, and this is the second of September. And here we have the
spend over time of each factor. Let's select only TV. So, here we have second of September.
Second of September, we spent zero, but we had some add stock effect, which means that we can
see it here. So, high channels, here we see. Second of September, we've spent zero, but we had some
add stock effect generated as a carryover effect, generating revenue for us. This is why we got a
this spike in ROI during that moment. I want to see not TV, not out of home, but only Facebook
over time. It's pretty consistent on average. We have 3.5 to 7. A print consistent with Facebook,
as this positive spike, but then it's consistent. And then search. Search is a really nice distribution
around eight ROI. Let's see which one is called reading emission returns. And what I want to do
now is I want to compare the emission returns found from Facebook with the ones from PMC. So,
let's go back to our diminishing returns. All right. So, let's select Facebook. 100k represents
150 more or less, right? 100k of spend represents 381k of revenue. So, this is interesting. Now,
this shows you the mean, right? So, this is how much revenue is attributed to Facebook per each
volume of investment. But the ROI and the contribution can go up to 250k of contribution
in terms of revenue for 100k of investments. It is still underestimating compared to the model
built from Cassandra. Is any other livestock effect? Print has a delayed effect. This is
interesting. The delayed effect shows us what's the lag effect between the moment in which we
invest and the moment in which we see the maximum impact coming from the previous investment,
and shows you what's the carryover effect. So, for how long will I continue generating revenue,
even if I shut down my campaigns coming from that particular campaign times? Keep this. So,
as we saw, we have big differences in the ROI measurements between the PMC marketing.
It is highly underrating these media channels contribution versus the contribution measured
by Cassandra's. Now, I'm creating an assumption here. I'm not an expert. If you have a better
interpretation of why this phenomenon happens with the same data, please share it in the
comments below. But I think that there is a different way on how we handle ad stock for the
specific PMC marketing library. So, this is why we see a different ROI. The easiest way in order
to understand what's the true incremental contribution right now, it would be to run an
increment high test, probably on the highest spend variable, which should be, according to this
model, the highest spenders are TV and search. So, I would run an increment high test on search
in order to validate its real incremental contribution. Now, going back here, what I want
to leverage now is the ability of having an open source library that we can play however we want.
So, we can actually see what's the accuracy in our square NMAPE for test data. At the beginning
of this video, we split our data set in two groups, and we've trained our MMM with the first
training data. And now we want to see how well it can predict the future. But what I want to focus on
is I want to first print all the columns. I want to be sure that the F-tests and the F-trains have
the same columns. And then here, we run this script in which we don't need this, but we define
X-tests as a DF test, as we saw, as we did before, Y-tests the same way. Then we print the shape
just to be sure that the shape is consistent with what we want to predict. Y-pred, which is MMM.predict
X-test. In this way, we predict the mean values that we're going to receive. I'm going to insert
everything into Y-true. We get Y-true, which is the same thing as Y-tests, the actual sales that
we generated, and we compare them in order to measure the R-square, as we did previously. If we
run this, we get the R-square on non-trained data is 40% and the map is 6.2. So the error is lower,
but the R-square is lower, which is not a problem by itself. We have only four data points, which
means that not all the variations in the Apple variables are well-described by the variations
in the Apple variable. I want to run this script again, because I want to see why we got it so
low R-square. The reason why is that in this point, it didn't quite get the incremental value,
and the reason why it didn't quite get here it overestimated sales and here it underestimated
sales, I assume because in here, we did not take into account the actual lag effect that
the ad stock of previous investment had on the VX, and in here, it's probably because the ad stock
that we actually generated is not big enough in order to bring the carrier effect over time.
I'm having a hard time in order to understand how to derive the ad stock effects from this
training model, and this obviously influences the accuracy. This, though, doesn't mean it is a raw
model. It means that, actually, it means that I'm not able to extract what are the upper parameters
for ad stock for this specific library, but it means that it can still be used in order to optimize
the distribution of our budget. Let's do an example. I want to do here, I want to run a
budget allocation simulations, and I want this budget, I want this model, and help me exactly
how I should distribute my marketing budget week over week. Now, it's not really flexible.
The first thing that we need to do is we need to retrieve the diminishing returns
hyperparameters with this function. So I'm going to run this, and then I'm going to use the sigmoid
parameters. I'm going to insert it inside of this function, which is optimized channel budget for
maximum contribution. I'm going to use the sigmoid methodology, which is just the shape
and the function for the diminishing returns curve. I'm going to insert the marketing budget,
and I'm going to invest, and the hyperparameters, which are these ones, I'm going to run this
to other than 8k for the next week of marketing investment. I'm going to run this, and what I see
is a new distribution, a new estimated contribution split by each campaign type, and here we have
the total. So the total is we're going to invest 2.08 times 10 to the power of five, and here
225 times 10 to the power of five, which means that we're going to have an ROI or a ROAS of 1.1.
Take into account this. When you use this budget allocator, it only suggests you the optimal
distribution, not taking into account the ad stock hyperparameters and the ad stock effect of previous
investments that we had in our market mix. So according to this simulation, we're going to have
1.1 marketing ROI. Let's try with Cassandra. So with Cassandra, we have a little bit more
flexibility. So we go to Optimus Budget. I'm going to click new budget allocation. Let's predict
the next week, right? If we do tests, I want to predict week 208k. I want to, this is 2018-11-18.
So going to do this, want to predict only the next week of marketing budget. Let's click Optimus Budget.
Now, if we invest 208k, we're going to generate an optimal ROI of 9.92, which is 35% more than a
previous week, which is completely different from what we saw in the other library. The reason
why is that we take into account all the historical data and all the historical ad stock that there
might be into this simulation. So what we do here is we bring the ad stock of previous investments
killed to the last data point that we have and we forecast it. And if you work a lot with MMM,
you're going to see that ad stock has a really big effect over time because it compounds the effect
over time. Now, according to this simulation, we need to invest less almost on everything,
but especially focusing on a lot, less money allocated to search, a lot less on Facebook,
almost half on print and a little less on TV. Let's compare these two results with PMC. Let's
see how much it suggests to invest on Facebook versus Atovo on print, etc. So on Facebook,
it suggests to invest 5.61, so 56K on Facebook versus Cassandra tells us to invest 84K,
then to invest in Atovo, I think a lot. Actually, this is actually here. No, no, sorry, sorry,
sorry. Here it suggests us to lower the marketing budget by a lot, by a lot. So it's a 5.61 times
10 to the powers of minus 10, which is we don't need to spend anything here and anything here,
I guess. And then we need to spend a lot of budget on print. So 6.39 times 10 to the power of four
should be 63,000. In here, we suggest to invest 14,000, right? In here, we need to invest 56,000,
sorry, here is 63,000 on print, 56,000 on search and 87,000 on Facebook. So on search here,
it says 95K versus the 56K that's suggested on search. On Facebook, we should spend 87,000 and
we suggest invest 84,000, fairly similar. And we should not spend anything on TV, Atovo,
and Atovo. But we're spending here some on TV and zero on Atovo. So these are the two comparison.
What we can do with Cassandra, automatically, is we can implement these suggestions and then we
can refresh the model after one month, right? We refresh the model. And every time we refresh it,
we can compare the actual contribution of media factors versus the factors that versus the
prediction that the budget locator gave us. But we can do the same thing with PMC for sure. It's
a little bit manual. Now, we've got a lot of information sold together. I'm sorry that I'm
not an extremely expert about this topic, about this particular library. It only took me one week
of training in order to get to this output. What I can say is that we can evaluate PMC marketing
as a really robust open source library that someone can use fairly easy. I mean, it took me one week
to understand how it works. That probably is going to take you a shorter amount of time if we
follow it to this point. It's a tool not specifically for data scientists that understand
how Bayesian statistics and probability work. So I would say is to use five out of 10 because
just because we compare it to a SAS no code product. Flexibility, I would say eight or nine
out of 10. It's extremely flexible. There are a lot more other parameters and priors that we can
use in order to model and change the posterior that we get from these trainings. Complexity is
six out of 10 and granularity and the utility that I can bring to our company. I think it's
a nine out of 10. The cool thing about this is that it embeds the probability distribution on
every measurement that we do. So it is extremely, extremely powerful. I would suggest PMC for
all the companies that do need to create a really robust model for their brand.
They have data scientists, a data scientists team, an engineering team that can actually run these
analysis week or week, a month over month. It is fairly, I mean, it's not super cheap because it
requires all the people, but it's a free open source library that you can actually start to use
and learn really, really quick. To be honest, it's the easiest open source library with the right
insights about probability distribution and taking into account uncertainty level that I've
ever seen so far. Compare Cassandra is nine out of 10 ease. I mean, it takes three clicks in her
to create an output. It's really easy to refresh. So ease to use nine out of 10. Utility for the
business, I would say seven out of 10. I mean, it generates pretty cool insights that are really
easy to interpret and validate over time. One thing that is not super on point is with these
probability distributions. I mean, we use an evolutionary method, not a Bayesian measure
methodology in order to measure the uncertainty rate of DMMA. It's just two different techniques,
but we've optimized our UI and our insights in order to be extremely easy to be interpreted
by non-technical marketers. So all the insights then are interpreted through our systems and
then explained in natural language as we saw previously. So I would say Pi MC is for companies
and brands that have a data science team internally. Cassandra is more suited for non-technical
marketers or marketing agencies that want to scale up their measurement operations. Hopefully,
this video has been useful for you and I can't wait to challenge a new, another open source
library about MMM. See ya.

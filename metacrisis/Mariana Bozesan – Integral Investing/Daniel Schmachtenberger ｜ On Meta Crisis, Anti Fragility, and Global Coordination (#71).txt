Daniel, welcome to the investment turnaround.
It's an outstanding pleasure to have you.
And I'm so looking forward to our conversation today.
So in order for the people to get to know you better,
why don't you tell us what took you
into the present moment in your life?
What path?
What significant emotional and cognitive events
participated to Daniel Schmachtenberger?
Oh, my goodness.
That is such an impossible question.
Because there's so many different ways I could answer it.
So I guess one through line that is pretty central
and is relevant to things I think we might discuss today.
I was homeschooled and by parents
that wanted to kind of run an educational experiment of letting
their kids design their own curriculum.
And this was similar to some of the unschooling ideas now,
but before unschooling was a thing.
My parents' interests were Buckminster Fuller
and kind of design science and Fritschoff-Capron systems
science and kind of the philosophy coming out
of modern physics and world religions
and spiritual traditions and the best kind of thinking stuff
coming out of the hippie movement time period.
And so obviously as a kid getting to design my own curriculum,
being exposed to those things made a difference
because no one's going to choose to study something
they don't even know exists.
And so it was nice to have those.
And so I was studying the sciences
and interested in how the world works,
studying the philosophic traditions
around what we're really here for and what is meaningful.
And then in studying things like the Buckminster Fuller design
science work, thinking about how do we
redesign the technological substrate of civilization.
Fundamentally, those were kind of early interesting questions.
And I got into activism very young.
That was one of the other big areas.
And it got to be homeschool curriculum time.
So I got to do a lot more like frontline activism stuff
and research younger than a lot of people would.
And it started with animal rights stuff
and started with factory farming and then went
to whaling and overfishing and species extinction
and with PETA and Greenpeace and all those types of organizations.
And that was the beginning of kind of existential devastation
for me of how much human induced unnecessary suffering
and just rolling atrocity there is on the planet.
And then how can I consider my life a success
while that's happening?
Like there has to be something wrong with me
that I can disconnect from that and just be happy to party.
And so I remember the very first thing
when I was like nine years old that I had this feeling around
with factory farms was if they still exist when I die
then I failed at anything worth living for.
Because I can't have a world that I feel good about where
that exists.
The hard part was I kept adding things to that list
because then start studying extreme poverty
and then start studying the things that lead to unnecessary wars.
And this was kind of like the centrally torturing thing
for me was everybody said that these all are such hard issues
that nobody's fixed them.
And so it would take all of my life
focusing on one of them to maybe have a little bit of a chance.
And that means ignoring all the other ones and I couldn't.
I just couldn't.
And fortunately, the system science focus
started to give me a sense that maybe part of the problem
was focusing on these things in isolation
and not what interconnected them and the underlying patterns
that gave rise to them.
Why do humans make shitty choices?
Why are we not good stewards of the technological power
that we have?
And so there were things that I knew
I needed to study to be able to even think about it well.
So I went and did university studies and studied math
and physics and things that I knew
would be like important fundamental disciplines
and philosophy and then independently
studying economics and social systems and things like that.
And trying to look at all of the proposed systems
for how do we make a better world?
Looking at the UN-STG kind of model
and looking at the Bildung model.
Can we make the whole world like the Nordic countries
and looking at the anarcho capitalists
and libertarian models?
And it didn't take that long to see how all of those philosophies
catastrophically failed in the face of planetary boundaries
and exponential tech and that they were just nowhere near deep
enough thinking for the actual nature of the problems.
And that was a bummer because I think I thought originally
that there were adequate solutions
and I could just find them and join them and add energy to it.
And so it was very devastating for me
the first time I went and engaged with people at the UN
to see, I remember the first conversation I had
there was the World Food Program Director
where they were looking at a solution to address world hunger
which was great, but their solution involved commercial agriculture
going into areas where it doesn't currently exist
to be able to feed the people in a way that doesn't depend upon
shipping food from other countries, which made sense
except it was bringing more nitrogen based fertilizers
to more river deltas, which will increase the rate of dead zones
in the ocean, which is an existential risk for the whole planet.
When I brought that up to them, I'm like,
okay, well, you're going to feed more people per year for a few years
and then speed up the death of the oceans and everybody.
They're like, yeah, we never thought of that and that's a bummer,
but those are not the metrics we're tasked with.
We're tasked with defeating kids metrics.
I'm like, are you fucking kidding me?
And then I saw that everywhere and I saw that most of the work
was not only not adequate to succeed, but it was causing other problems
in other areas, worse problems because we were defining the problems too narrowly
and the world was more interconnected than that.
And so this just led to having to continue to step back and say,
okay, maybe we just need to do an assessment of the whole problem space
well enough to know what adequate solutions would take.
So that was kind of the central thrust of my life focus
in all the various areas of study where what do I need to understand
to be able to look at that.
So we were mentioning economics.
Obviously, we can see that if I have a metric of optimizing GDP for a nation
or GDP per capita, some metric like that, that war is really good.
GDP goes up under war.
Sick people that spend more money on medicine is good,
that there's a lot of perverse incentive that addiction is quite profitable.
And so it's like, okay, well, that's a dreadful metric.
And yet we can see that underlying so many of the issues like bad medical systems
and bad food systems and driving addiction from supply side to manufactured demand
is an economic system that has perverse incentive writ large.
And there's something like 70 trillion dollars of trades hands every day.
And I'm like, if we got 70 trillion dollars of decentralized human incentive
almost all of which is causing harm along a supply chain of action somewhere.
Even if I had a billion dollars a day to spend,
it didn't even need to make money, just pure nonprofit.
And I was maximally affected with it.
And I'm going against 70 trillion dollars a day that is harm externalizing.
And it's easier to break stuff than it is to fix it or to build it.
I can I can destroy a house much faster than I can build one.
I'm so many orders of magnitude off of being adequate.
So I'm like, anything that does not change perverse incentive is not even worth doing.
So how do we change 70 trillion dollars a day worth of human activity
to not have perverse incentive built in?
Well, it doesn't look like our current economic system.
It looks like a fundamentally new economic system.
So how do we how do we make projects that succeed within this economic system
and externalize a bunch of harm is never interesting to me.
How do we make a fundamentally how do we make an economic system
where the success of that economic system and the thriving of life
are aligned with each other is a different question.
And so, yeah, then one of the things I saw was as I was looking at all these different
problems and how they were looking at how they were interconnected
to see how trying to solve one would move problems somewhere else.
And we'd see that all over the place.
The, you know, example that I gave about nitrogen runoff is one,
but we can take pretty much any example.
And but then also looking at what do all the problems have in common
as the at the level of generator functions.
And perverse incentive is one example,
but there's a number of things that are the underlying system dynamics.
And by system dynamics here, we're not talking like healthcare as a system
or or the judicial system as a system.
We're talking about underneath that the patterns of human behavior.
What is creating patterns of human behavior?
So I started forecasting also.
I was trying to see are these problems or the problems getting better?
Are they getting worse?
And the answer is, of course, both.
And so you can you can read plenty of books,
Pinker and Hans Rosling and all those books on why everything is getting better.
And if you cherry pick the stats and you decontextualize them, sure, that's true.
But you read most any environmental metrics, not any, but so many environmental metrics
and also catastrophic and existential risk.
And you can see how many things are getting not just worse,
but precipitously towards the non viability of civilization worse.
And so when I saw that some things were getting better,
some things were getting worse, I saw a phase of civilization destabilizing.
And that clearly, we didn't just need more of the type of actions that we were doing,
more nonprofit projects, more impact investing, more UN SDG stuff,
more laws being made and more tech solutions, because the whole body of all of that
was not converging towards adequate because every year we were getting
more total catastrophic risks and higher probability on each of them.
When CRISPR comes along, we have way more chance of all dying from bio weapons with
development of AGI.
We have way more chance of dying from AGI based risks with the development of drones
and the weaponization of drones and their ability to take out infrastructure.
So it's like, OK, the total probability space of catastrophe is rapidly expanding.
And if you look at the UN world, we haven't solved any of the SDGs.
They were called the Millennium Development Goals before we didn't solve them.
We never succeeded with this nuclear depriliferation.
Since the UN started, we went from two countries with nukes to lots of countries
with nukes and faster nukes and better nukes.
We haven't stopped arms races on a single new type of technology.
We have an arms race on drones.
We have an arms race on AI.
We have arms race on cyber weapons, on bio weapons.
We haven't been able to deal with any of the major tragedy of the commons,
like climate change or overfishing.
So it's like, all right, our problem solving processes writ large are not
adequate to the problems we face.
So we shouldn't have 17 SDGs.
We should have one superordinate one that is develop the capacity to coordinate
effectively towards global level issues.
If we don't have that, we don't get any of the other ones.
If we do get that, we get all the other ones.
So how do we develop better coordination capacities towards global level issues,
like arms races and tragedy of the commons and things like that?
And yeah, so that's kind of been the through line, is seeing all of the problems,
seeing how they're interconnected, seeing what's underlying and driving them,
seeing where the solutions have some effectiveness but are inadequate
and thinking about and working towards what would new problem solving processes
adequate to the problems that we actually face look like and how do we bring those about?
Brilliant.
So how do we, you mentioned AGI, I just wanted to explain AGI is like what artificial intelligence,
artificial general intelligence as an explanation.
So how do we, what are the acupuncture points?
Obviously, there are enough smart people on this planet to be able to develop this whole system
thinking and come up with solutions.
Why, what makes it so difficult for such people to come together and come up with solutions
and implement them?
Because obviously, there are many, many people who come, you know, have these solutions,
but they don't have the political power, the influence, the money, whatever to make them happen.
So what are the fault lines of existing systems and what are the possibilities we have
to find these acupuncture points and influence them?
So that we, just to frame me, let me frame this a little bit better.
You are, went on record saying that we're currently facing World War Three
and it's not kinetic, so we don't really see it.
We don't really, we think it's peace and it's wonderful and we all can whatever, whatever,
be blessed to buy this and that and the other.
But in reality, we are deep at war with one another and we are on our best way toward extinction.
It's only a question of time.
So how do we save ourselves?
It's just a simple question for you, Daniel.
So everybody knows that the thing we call politics and the thing we call war are a gradient
of basically rivalrous interests.
Von Klauswitz famously said war is politics extended by other means.
And but it's more like the other way.
It's more like politics is how we sublimate war where we want different things.
And yet diplomatic warfare, economic warfare, cyber warfare, narrative and information warfare,
population-centric warfare, are those politics or those, well, they're outside of the domain
of what we consider the agreed upon political process.
They're not kinetic warfare.
World War II was kind of the end of kinetic warfare being a viable solution for the major powers
because any weapons that the major powers used against each other to really be able to deal
with it could escalate and we had weapons now big enough that you have a war that nobody wins.
So after that, we had to depend on proxy wars, which we've done a lot of.
We've noticed there have been no major superpower wars since World War II, heaps of proxy wars.
Well, in order to fight a proxy war, you again have to base it off propaganda.
Oh, they have weapons of mass destruction.
No, they don't.
But you have to tell that story to be able to go invade and do that thing.
And that is a kind of non-kinetic warfare.
So we fight proxy wars and we fight non-kinetic, unconventional wars.
So is it fair to say that the world is in peacetime right now?
No, I don't think that's fair to say.
I think we can see the total number of cyber attacks that are obviously not peacetime.
We can see the increase in total numbers of catastrophe weapons.
The first real catastrophe weapon we had was the bomb.
And before that, the major empires of the world always fought.
There were never times where they didn't fight.
That was how we dealt with our border issues.
World War II was the beginning of having weapons so big that we couldn't fight as the solution,
or at least the major powers couldn't fight.
And so the whole world before World War II and the world after fundamentally different,
because before that, we weren't powerful enough to ruin everything.
And so World War II was the beginning of technologically induced catastrophic risk.
So we had to be able to do something we'd never done, was have the superpowers not war.
So we had to make an entire new world system, a globalized world system for the first time ever.
And it bought us like 75 years of no superpower war.
There was a cold war, there were proxy wars, there were other things.
And that involved mutually assured destruction.
It involved a bunch of IGOs, the UN and the World Bank and IMF and all those types of structures,
because we realized that national governments only weren't enough to prevent world war.
And so we needed these other things.
And a whole globalization system where we became so economically interdependent on each other,
that we had more incentive to do trade with each other than we did to war.
And so we can see that the computer that we're talking on today, the microphone,
all of it, no country in the world can build.
From the mining of the materials to the refinement of them to the hardware,
to the software, to the satellite production that our internet is communicating over,
all take six continent supply chains to make.
Well, when you have that much interconnectedness,
you aren't oriented to blow each other up, which is nice.
But you get other problems, which is you get radical fragility.
So you get a problem in one area of Wuhan and the whole world shuts down.
And you get cascading catastrophic failures.
You have to stop a virus in a world that connected.
You shut down transportation in a way where now you realize the flow of fertilizer and pesticides
that now got shut down just drove the poverty of the 100 million
poorest people into much worse conditions.
And we had locusts throughout Northern Africa and parts of the Middle East because not having
the pesticides.
You can see all these second and third order effects.
Also, a major part of that world, that globalization world, was we will use industrialization to
extract resources from the earth way faster so that we can be so positive GDP that everybody
can get more without taking each other's stuff.
And the idea there is that if GDP is not high enough,
everyone's desire to have more makes them go zero, some may have to take each other's stuff.
And so the answer is there has to keep being more.
Well, more with a linear materials economy where you're taking stuff unrenowably and then turning
into trash unrenowably, you can't run an exponential linear materials economy on a finite
planet for all that long.
And so what we see is that that Bretton Woods world that kept us from warring for like 75
years till now has just come to an end because we reach planetary boundaries.
So we can't keep running the linear materials economy that way.
We have enough fragility in the overall system that the types of local collapses that are
inevitable now create global cascading collapses.
And we can't do mutually assured destruction anymore because we don't have
one catastrophe weapon and two superpowers that have it.
We have dozens of catastrophe weapons and many, many actors,
including non-state actors, including people we don't even know who have it.
Well, how do you do a forced Nash equilibrium?
How do you do a mutually assured destruction?
You don't.
All right.
So now we're in a new situation.
There was the whole world up till World War II.
There was World War II till now.
There's a new thing where you could say that the orienting question that I find,
one of them I find most interesting is around why humans have not been very good
stewards of power.
Why we have been shitty stewards of power, which both looks like war environmental destruction
and shitty social systems and subjugation and whatever.
And we can see that as we've gotten more technological power,
those same underlying issues of not being good stewards of power have just become bigger deals.
And with exponential power, it's really important to get the ability to engineer
new life forms, to crisper genetically engineer new life forms.
This is not the power of an apex predator.
This is not something that a gorilla or a polar bear or an orca can do.
This is the power of gods, the ability to extinct species at 13 a day,
the ability to destroy whole ecosystems, to build totally new environments,
the Anthropocene where we are the largest force affecting the surface of the earth,
more than geology, the ability to build artificial intelligence that has the ability
to paperclip maximize the world that this is the power of gods that requires something like
the love and wisdom of gods to guide it or it blows itself up.
So while this has always been an interesting topic of how do we become better stewards for
our power, it is now a critical topic because rather than just having local wars and local
environmental destruction, which we've had for the last 10,000 years, exponential warfare
blows everything up on a finite planet, exponential externalities ends a finite planet.
So exponential tech is a forcing function for us to become good stewards of the amount
of power we have or for the human experiment to complete.
So a way of thinking about it, if we take exponential tech and the emergence of it
as the center of our inquiry, which I think is the right way to look at it,
we take a real politic view and just look at what is happening and what is going to happen.
Just what we can say that World War II was there's a bunch of other ways of looking at it.
This isn't the only way, but it's a useful way. World War II was a few competing social ideologies,
competing over a chunk of new tech that was all made possible by a level of science.
So you've got communism, you've got fascism, and you've got capitalism, liberal democracies,
something like that. And basically physical chemistry, the atomic physics, physical chemistry
got to a place where the bomb, the V2 rocket, the computer and chemistry all basically came on
board at the same time. And Germany was way further ahead than the Soviets or the U.S. for
different reasons. And we realized that was existential and had to catch up. And so the U.S.
did the Manhattan Project and invested like crazy, which was not market investment. That
was state investment. The market did not build the nuclear bomb. It didn't figure out computation.
So the idea that markets innovate and states don't is utter gibberish, because both Germany and the
U.S. and the USSR that developed all those things, the Apollo projects, but Nick, et cetera, those
were all done by states, not by markets. And markets have never done anything of that scale
of development before or after. So we can see, we can think of that as a few competing social
ideologies for who would get the new tech, because whoever would get the new tech would
rule the world. And we can see that the U.S. kind of came out ahead. So we get a Bretton Woods world
rather than a Soviet contracted world or whatever. We'll detect jump where at right now.
The center of it this time is computation. Computation gives exponential returns rather
than multiplicative returns, because I can make a piece of software once and then sell it an
infinite number of times with no cost of goods sold and no cost to transport the bits. That's
very different. Anything in the domain of atoms or energy, I have cost of goods sold and cost to
transport the energy of the bits or whatever. So computations at the center and at the very
center of that is AI. And then coming out from there is the application of AI and computation to
upgrading the computational substrates, quantum computing, photo computing, DNA computing, et cetera,
other critical computational capacities, crypto type stuff. And then the application of that to
things in the domain of atoms and energy. So biotech by being able to apply AI to protein folding,
nanotech because of that material sciences, because of that robotics that are run by AI,
all those kinds of things. That chunk of tech that is emerging right now is orders of magnitude
more significant than the World War II chunk of tech, who only those who are directing it will
have any say in the future, because it is that much more powerful and power has been what determines
the future. And this is the real politic assessment. Right now, there are only two types
of groups as far as I see it that are trying to direct that type of tech. Authoritarian nation
states and corporations. So authoritarian nation states, China is not the only one and I'm not
saying even the China isn't doing things that are quite smart and reasonable. Of course they are.
But China is not leaving the development of its tech completely to a market that isn't aligned
with the long term plan of its country. It has a long term plan for the country. It makes sure
that what the market does is aligned with it. So if a corporation starts to do something that is too
far outside of what would benefit it, like the Ant Group started to do, they bring it back in line.
They have a huge amount of R&D themselves as a country, which so they're applying AI and attention
technology and IoT and robotics and whatever to making a better country, not just a military and
not just an economy, but a better nation state. And so this is why they have been able to not only
make high speed trains within their country, but export them all around the world and the US
hasn't built one within its own country. This is why they have something like 94% of the supply
chain on rare earth metals. This is why they have the majority of lithography and
like really foundational things. So we can see that an authoritarian style nation state that can,
because they don't have term limits, they don't have a perverse incentive to only do
shit that will create returns within four years so they get reelected. So they can do long term
planning because they don't have two parties fighting against each other. They have the ability to
not have all their energy used up as internal friction and heat. So they can actually do fairly
decently coordinated long term planning. Well, that's just a more effective system,
that's a particular point. So you see that then in the West, you see that the
development of the exponential tech is mostly not happening by the government.
We don't have to see Manhattan project type activities. We see narrow applications in
the military, but the military is not the government. It's not how the state tends to the
people. It's not a better system of democracy. It's not a better system of educating the people.
It's not a better system of creating healthy economy where people have something like
the possibility of upward mobility. All of that's left to the market, but the market that is doing
it, the people who are running it aren't elected. They don't have accountability to the people.
They don't have jurisprudence of those kinds. So you can see that the companies that are developing
the tech are becoming exponentially more powerful companies and they're becoming so powerful that
they're more powerful than countries in many ways. Facebook and Google have more users than
China and the U.S. combined have people and they have more data on them and they have more
behavioral influence over them. That's fucking powerful. And not only is that not strengthening
the U.S. outside of GDP, even though they don't pay taxes proportionally, it's destroying the
basis of the U.S. functioning. Those are running a business model that optimizes people's time on
site, uses AI to optimize time on site, which happens by appealing to people's biases and
in-group identities and all that kind of stuff, which actually polarizes people against each other
so much that anything like democracy becomes impossible. So we see that exponential tech is
making exponentially powerful companies and actually destroying Western democracies and
republics in the process. So that converges towards a new feudalism. And so we have basically,
exponential tech has two attractors. Either it causes catastrophic risk directly and so you
just don't have civilization anymore or you get shitty civilizations. You have two options within
each of those. Shitty civilizations look like authoritarian nation states that are empowered
with exponential tech or feudalism that's empowered with exponential tech. The catastrophic
risk looks like two kinds, exponential externalities, which equals destruction via planetary boundaries
and environment and stuff, or exponential warfare, which looks like different things that lead to
escalation pathways to World War III. So those are the four attractors I would say that are out there
right now. And they're all bad. They're all dystopic. So I'm interested in a new attractor.
And given that only those who are directing exponential tech will have the power to have a
say in the future. But we want to have exponential tech not be used dumbly in a way that causes
catastrophic risk, but also not create a type of civilization that is undesirable for the majority
of the people, right, that is not aligned with participatory governance and high civil liberties
and things like that. Then we get a very clear design constraint. The design constraint is that
the number one imperative of the world of civilization, if they are paying any damn attention,
needs to be the development, the development and implementation of the new emerging categories
of exponential technologies to making new social technologies, to making democracy 2.0,
Republic 2.0, open societies that are both aligned with the kinds of values that we care about,
they'll be different, right? And they should be different. The idea that you just vote on
propositions or the propositions you had no say in and they're made by special interest groups
and they're all bad. If you vote yes, it benefits one thing and hurt something else, which is why
50% vote for it and 50% don't. It polarizes the population. That's not democracy. Voting is not
democracy. Democracy is a way that people can participate in governance. That's a stupid system.
So will the new systems look like just digital voting? No, they'll look like better ways to
actually be engaged in the crafting of the propositions themselves. So we can come up with
better propositions that don't have such unnecessary theory of trade-offs. And so it's not that we're
trying to make exactly the same structure but a digital version. We're trying to make some of
this aligned with the same values of both maximizing the integrity of the commons and maximizing
individual freedoms and creative expression and like that within that. And so how do we
engage exponential technology to make new social technologies, new open societies,
that can preserve the right in advance, the right types of civil liberties and values,
and guide the exponential tech safely so that it doesn't cause catastrophic risk?
To me, that's the only imperative worth anything because if we don't do that, the other things
won't matter. They won't end up being able to happen. So when you asked how do we go about it,
that's an answer at a very high level but it's a starting place.
Brilliant. Thank you. So let me just summarize it, you know, the last thing that you said.
I heard you say that we need to develop a new attractor other than the ones that are currently
dominating and your solution or another alternative is what Elon Musk is doing,
getting ready to colonize Mars because I don't think he hopes or believes that we will be able to
develop our consciousness to the point where we will be able to implement the social engineering
that you are justâ€¦ If not, we will probably export the same social failures to that system.
So the thing about the Mars colony that's actually great is not the Mars colony itself,
it's the thought experiment. Why it's a good thought experiment is because import is really hard
and so if I'm trying to build a civilization on Mars, it has to do everything without import
and so if I'm in a bubble where we're producing our own oxygen, we're all stuck in the bubble,
do I want any pollution at all for manufacturing? None, nothing, nothing. How many people can I
have in prison that are breathing oxygen and not doing stuff? Nobody, everybody has to be
meaningfully engaged. Well, how do I prevent crime completely and have a better process
for conflict resolution? We have to do our own biotech. In that world, a GDP maximization and
the thing that matters, I don't want sick people. How do I prevent health issues? How do we optimize
health? How do we make sure that all the new stuff we need to make, we can make from the old stuff
because we can't get new stuff easily? What minimum set of tooling do we need to build the rest of
the tools and would we build the tech stack the same way that we did or would we rebuild the tech
stack differently? This is what's interesting about it. How would we do law? Would we import
the same system of law or would we do law fundamentally differently? Would we even have
the values that are the basis of it be rethought through? So the Mars colony is interesting for
Earth. If we take the thought experiment seriously enough that we start thinking about, oh, wow,
we're powerful enough now that designing civilization from scratch is a possible thing.
Well, let's design this one from scratch differently. What does it look like?
So how do we, how do we, and then, you know, you may want to dive into the Concelion's project,
which is your project, which I highly, I'm fired up about it. So how do we begin to design a new
social technology and re-engineer society, basically? What are the, I think, the best words
or what are the best levers to implement that? I mean, how do we begin? I think,
just like Elon Musk is using his idea to go to Mars as a means to achieve the technology to shift
the mind and technology and everything, how can we do an Earth shot on social engineering?
I'm going to take a tangent and just address, because you brought up the word
term social engineering, and that word has a very negative connotation for many people
in the US very intensely. And so I want to speak to that.
When we say social engineering, a social system, a society,
a society or a civilization, we can think about what essentially is that. It's a way for a lot
of people who don't all know each other beyond a tribal scale to coordinate their actions
usefully. That's kind of fundamentally what it's about. And tribal systems before that were
different, because there were not that many people and everybody knew everybody. But if we now have
a place where I have to compromise my actions in some way for people who I don't even know,
and we're all going to kind of get along well, and we, but how are we all going to get along well,
and how are we going to do division of labor and specialization and then share the stuff that we
produced in a way that creates more kind of wealth and possibility for everybody,
we have to have some systems of coordination. So I think of civilization as coordination systems.
Civilizations fail either because they can't bring about the order and coordination necessary,
so they fail in the direction of chaos, or the way they bring about the order is imposed.
So there's some, okay, here's rule of law, and we're going to impose this rule of law by force,
and so then it becomes increasingly oppressive. And then it fails for those reasons. And so
civilizations teeter between oppression and chaos, right? So we say, how do we get out of that? How
do we have something that is not oppression and is not chaos? Well, we need order, but we want the
order to not be imposed. So we need order that's emergent. How would we possibly have emergent
order? This is the idea of a democracy or a republic or an open society of participatory
governance. And you notice that they were aligned whenever they emerged in the world,
they were aligned with some idea of an enlightenment of some kind, a social enlightenment
that had the idea. And if we look at modern democracies emerging out of the kind of European
enlightenment, the key in the European enlightenment was the philosophy of science. And the idea that
we could all come to do the same experiment at the same result, and there was an inherently
unifying property to the nature of objective reality. So we could make sense of the same
world, because what we're trying to coordinate is the choices we make. Well, what's the basis of our
choices, our sense making, and our meaning making inform our choice making, our sense
makings, what do we think is going on? And what do we think in action would be that would move us
towards the thing we want? What do we want as our meaning making our values generation, right? So
ultimately, our ability to figure out what we value and what's going on and how we think we
can advance the values, how we do choice making. So the idea that we need to, if we want to have
coordinated choice making, we need to coordinate our sense making. Can we all come to be able to
assess reality similarly? And we have to be able to coordinate our values. And so can we run a
Hegelian dialectic, right? Can we run a dialectic where rather than just say that, because it seems
like you're optimizing for something else, you're evil, can I try to understand the value that
you're trying to optimize for that has some real value. So at first, you're voting for Proposition A
and Proposition A is going to, as far as I'm concerned, hurt the owls in this area. So you
must just be a non environmental evil fuck, except I'm like, why do you want to do that? Well, for
the economy, what does that mean? Well, it means that you are actually in a resource insecure
situation and your kids don't have job prospects. And that Proposition A looks like it will actually
give your kids a better opportunity to have a good life. So the value for you is the opportunity
space for your kids lives. Okay, well, maybe you're not an evil fuck, maybe that's a real value. And
maybe you don't even not care about the owls, you're just forced into a situation where that
proposition unnecessarily polarized them. Is there a way to care? And but you think that I'm evil,
because I want your kids to be poor for some fucking owls, right? Well, what if we just say,
how do we take all the values and recognize that they are not necessarily opposed and
try to take all the values before we come up with a proposition and then come up with a proposition
that meets the values, the best that it can, we don't even try to do that most of the time. Now,
a special interest group will make a proposition based on some very narrow value that of course
externalizes harm to other things, and we're not even trying to do a better job of that.
So the idea of a democracy, though, and earlier democracies did try because of smaller scale,
like in the US, you weren't making huge choices, they were mostly in a town hall where everybody
could just talk. And so a special interest group was making the proposition, we were all talking,
we said, what about this idea? Well, that's not quite right. Because of this, what about this idea?
And you could leave sometimes without even voting, because everyone would just agree this is the
thing to do. There was a quote by one of the founding fathers of the US, something to the
effect of the death, that voting is the death of democracy, because it means that we didn't have
a good enough conversation to come to shared understanding. And then the vote was a way to
sublimate violence. But the idea of can we get emergent order? Well, if we can unify our sense
making, that we can make sense of reality, similarly, and we can unify our values, can I try to
understand your perspective, you can try to understand mine well enough that we say now
let's put all these values together, and then let's use design process to design a solution
that will meet everybody's values as best as possible, maybe not perfectly, but as best as
possible factoring the physical constraints that our sense making shows us. That's the right kind
of direction to be able to get emergent order. Well, we can see that since the modern democracies
have emerged, the world has gotten much more complex technologically, ecologically, etc.
And our capacity for shared sense making and shared meaning making, meaning our training
in civic virtue to listen to each other well with good faith, and our training in the relevant
areas to understand the world has gotten worse. And so social engineering, the kind of social
engineering that brings about order through imposition, we're not interested in.
What we want is something that actually makes that kind of social engineering impossible,
because the people recognize propaganda and manipulation of those things when it happens,
because they have good sense making capability, and where that is, so it's both impossible and
not necessary, because the people can actually coordinate. So what we're looking for is how do
we socially engineer a world where that type of social engineering is both not necessary and not
possible. Now, another way of speaking to it is conditioning of people is not avoidable. You
can't not condition people. They're going to be born and learn Mandarin if that's what they're
hearing or they're going to learn English or they're going to learn German or they're going to
learn whatever. Just what they're hearing is conditioning them. It's conditioning them to
think a certain way. It's going to condition their aesthetics, their values, their whatever,
based on what they observe. So we have to realize that you can't not condition people.
So given that it's inexorable, what is a meaningful human life? What is a civilization that
enhance, that makes meaningful human life possible? And how do we want to condition people
such that they both have the best life possible and can contribute? That we have a civilization
that makes the best life possible for the people and people that are disposed to live their life
in a way that in turn contributes to a civilization in other people. So I just wanted to say that
briefly about social engineering is that there is an element before you move on.
I'm German. I'm a German citizen, half Romanian, and Germany's been very well known for
voting green. And for decades, we now had the Green Party in a leading political position
in various states within. And now they have the opportunity to really go away, go away from the
opposition into leading at the next election, leading Germany. And one of the, I'm referring
to question as to how to implement what you just said. Because one of the, and I am a green voter,
obviously, but one of the reasons why they were not leading a leading political party
over the past 16 years has to do, although they tried, has to do with, from my perspective,
with the fact that they try to, well, first of all, they went into these
never ending conversations where no decisions were taken, or if a decision was taken,
they were trying to impose it on other people, like eating, you know, not eating meat on Fridays.
And that, of course, that, and I'm, I'm a vegan, so I don't, I don't have a problem with that,
but that led people to not vote for them. So my question is, how do we
optimize for the integrity of the whole, or the wholeness, in trying to come up
to an emergent order without ending, you know, in, in these endless conversations and not having to
impose a decision onto people that do not agree with and lose political power and leadership for
that matter. Am I making myself understood? Yeah. Thank you.
I think the idea of the masses
is class propaganda. I think that if I want to be a ruling class, the idea that most people
are too dumb and unmotivated and lazy and irrational to rule themselves well. And so we kind of
have to rule because we are the ones who know what is right and good. That's a very convenient
belief for those who happen to think that they're part of that class. And so if I, if I look in,
and I'll speak about the US because I know it better. So if I look at the dumb masses
that don't, that think Africa is a country and can laugh at them and whatever, which we see,
right? There's a elite focus. We'll do that in the US. But I look at the schools they went to.
They're shitty schools. Then I look at where do the wealthy kids go to school and go to a place
like Exeter Academy? What Exeter Academy, the average person becomes like a senator, right?
And like the people that do the worst still become partners at a law firm. And
I think if I took that person who thinks Africa is a country who had a shitty school, parents,
no tutors, et cetera, and put them in the same position and put them in Exeter Academy with
the right tutors and teachers and everything around, they would be similar. So the masses
are the result of how we condition the people. And this is why Benjamin Franklin said if
that the ultimate depository of the power must be the people, there's no other thing that is actually
a trustworthy depository of the power. And if we believe the people too uneducated to be a safe
vessel, the only answer is educate them better. Everything else becomes evil.
So it's a big damn task. But look at the options, right? The other options are
starting with the idea that I'm meritocratically better, right? I know what is true and good
and right better. So I'm willing to like, let's say I'm going to do this thing about
nobody's allowed to eat meat on Friday, whatever it is, because otherwise climate
change in the whole world will end. And it seems like it's worth kind of forcing people.
What I'm saying is, wait, is that a law? Does that mean I'm going to take someone to jail
if they violate it? Does that mean we're going to use a monopoly of violence via a police force,
which an internal kind of military capacity to take away people's freedoms? Because I'm so certain
that we know it is right, that we're willing to implement it with a monopoly of violence. Where
does that go as you keep going with that process? What legitimate authority is legitimate enough to
be not being corrupted in the presence of that? So basically, when you come back to the early
narrative, I said that the problems that we face today are not being solved by our current
problem solving processes. So we don't just need more business solutions and nonprofit solutions
and etc. We need fundamentally new types of institutions that have the right characteristics
to solve these types of problems. Well, who's going to build those new institutions?
And it's important to get when we look at like, the world has built totally new types of government
and new types of institutions for 200,000 years, it was tribalism. And it wasn't until the emergence
of the plow and baskets and a few things that we started to get the different types of empires,
right? And then that kind of emerged into certain dominant types of empires and feudalism. And then
feudalism for so long that like the idea of something like
nation states and markets, that wasn't even an idea that it would go that direction and
that new thing. And then obviously post World War II, not just nation states, but like inter
governmental organizations at a global scale and multinational corporations. So the world does
evolve at social technology. And we can see that when the founding fathers of the US
were founding the constitution, they didn't have AI as an issue to worry about.
They didn't have a billion people. They didn't have planetary boundaries that were coming up.
They didn't have such interconnected supply chains that a break in one area could break everything.
They didn't have the capacity for a digital democratic system. They had to try to fit people
in who would ride horses to fit them into a town hall. So they were not trying to solve the problems
we need to solve today with their system. The people who created the idea of markets and the
Scottish Enlightenment, Von Mises and Hayek and Rand, were not trying to solve these problems.
And neither was Marx, neither was any of the social theory. So we need fundamentally new
social theory that starts with understanding our problem space. And so the problem space
requires new institutions and new problem solving capacities. Those have to emerge from people
popularly starting to want them, care about them, be willing to empower them and participate with them
so that the power of governance emerges from the consent of the governed.
Otherwise, it will be imposed, which is its own dystopia.
Wonderful. Would you like to dive into the Concelion's project as a contribution to achieving that goal
of creating and developing a new social theory and education system to implement that,
to inform the public space and avoid AI and what Google and Twitter and Facebook are doing?
Yeah, so everything we're talking about is the problem space that the Concelion's project is
focused on. And I'll take one more tangent first just because I think
some of what I'm referencing might not be grounded an example for people. It might be helpful.
I'm guessing that most of the people who are listening to this are aware of many of the types
of existential risk that exponential technology makes possible. If not, go watch Nick Bostrom give
a short overview of it on TED. But briefly, things like nukes are hard to make. Not that many places
have uranium. It's hard to enrich uranium. You can see who has it because it's radioactive from a
satellite. It's not hard to make drone weapons. Commercial drones are good. You can 3D print
drones. Being able to put the charges on them are easy. You can take out infrastructure targets
that way. Infrastructure targets can lead to pretty big cascading failures if you know which
targets to look at. We can see that that's happened a couple of times recently, and they weren't
even really good targets. They were small things, but the drone a couple of years ago hit a Ukrainian
munitions factory, and a little homemade drone did the level of damage that an incendiary bomb would
take. Run that out a few years. See that tech getting at earth, and think about it, and you're
like, all right, well, how does, and now think about how cheap CRISPR gene drives are and how
easy cyber attacks on infrastructure are. The thing that happened with the pipeline on the
Atlantic coast shutting down recently where nobody could get gas, we've known about that for a long
time. Nobody's done anything to protect. For 25 years, we've known that was a type of risk,
but now it's easy. It was kind of hard before. Now it's super easy. So if we have decentralized,
meaning not just state actors, but non-state actors, if we have decentralized exponentially
more powerful, which could mean exponentially more catastrophic technology, how does the world
remain antifragile in the presence of that? It's a big question, right? It's a big question.
For the most part, Western countries aren't even taking the question seriously.
China is taking it seriously. Their answer involves ubiquitous surveillance. It's not a
dumb answer. It's a problematic answer. It's a smarter answer than no answer. What's a better
answer? I would propose better answers than that. But just to say, if people have not thought about
how exponential tech causes catastrophic risks, go look into that. I have a podcast where I talk
about that a bunch. But the other example of how could any of these emerging areas of tech innovate
in social systems? I'll just give a couple examples just so you can start thinking about it.
You think about a new computer technology like blockchain or some other kind of decentralized,
uncorruptible ledger. What if all government spending was on a blockchain, which meant that
it could be perfectly audited transparently by anybody? And you could see where the money flowed
with a total provenance of data. And there couldn't be any missing money. And there couldn't be emails
that got lost or burned or whatever because you had uncorruptible ledger or transparent.
Well, there's a whole bunch of corruption that just can't happen now. You just made it impossible
through a kind of technologically enforced transparency. That's pretty great. That changed
a lot of stuff when you start to look at how much unaccounted for money there are in federal systems.
Look at AI technology. Look at the ability of AI to take a number of different faces and make a new
face that isn't actually a real human face, but looks like a real human face because they average
these different faces. They can do that with sound of the things. Someone actually just emailed me a
proposition they're working on about this the other day and such a well formed proposition. What if we
use the same underlying AI technology to look at a semantic field of proposals that people have and
come up with a new proposition that is the weighted center of all of those propositions?
Could we actually factor a humongous amount of human input and be able to come up with better
propositions where then humans can take those propositions and go through something like a
div merge process to be able to come up with better refinements on the proposition to move it
through? That's not voting, but it's achieving the goal of voting is better than it. What about the
kind of attention hijacking tech that Facebook or YouTube have, where you think you're just going
to check for a second and then an hour went by? Well, it's taking a lot of time to think about
unbelievable personalized data about your behavior, building behavioral models and then
being able to use that ability to control your behavior. What if instead of maximizing time
on site because they have an advertising model, they were maximizing your rate of learning and
development? They had a pedagogical orientation. Well, that would be different, right? What about if
why do we not have voting in person everywhere? We can see Taiwan made the shift, right? We can see
a few places are starting to work on it. Well, if you can do your banking online with
public key crypto, of course you can do that online. Then you don't have to have voting
be something that happens once every four years or every two years. It can be a continuous process
and where you don't just vote yes or no on a proposition, you get to help craft a proposition
where you can see the provenance of information that is leading up to it and who's influencing it.
Maybe you can even do things like a qualified democracy where before you get a vote on a
proposition, you simply take a simple test that shows that you understand the pros and cons
as generally stated, not leaning one way or the other so that you can't just get populist people
that are coming and voting without understanding it and that if you don't want to do what it takes
to learn that thing, you can use liquid democracy to give your vote to someone who does understand
it, right? So there's a lot of things we can prototype with that are like, wow, that's really
different. That makes a whole new set of possibilities. What if you start getting lots of those together?
Well, when you look at the rate of technological automation, both in AI and robotics,
we're about to lose most of the jobs. Oh, but there's going to be new jobs that are comp...
No, they're not. No, they aren't comparably in the same way. This is why so many in the
billionaire class are fans of UBI is because UBI is arguably the cheapest way to tend to the
unemployed underclass, to keep them from being a problem. And so you get a centibillionaire
class who are the ones that control the top of the power law distribution within a vertical,
right? One of the things that happens in the new areas of exponential tech is that Metcalf's
law leads to natural monopolies and power law distribution. So Amazon is bigger than all other
online markets combined and Facebook is bigger than all other social media and YouTube is bigger
than all other video players combined and Google bigger than all other search engines.
You get basically one owner of a whole vertical, right? That's not through government monopoly.
It's through the natural monopoly of network dynamics. When we've made antitrust laws anywhere
in the world, network dynamics didn't exist yet in that way. So we didn't build that in.
And yet the essence of antitrust should apply, right? Which means our legal system can't begin
to keep up with the rate of tech changing the fundamental premises. Okay. So you control one
of those verticals of exponential tech. So, you know, ecobillionaire, centibillionaire class,
they get to compete with each other, coordinate with each other for class interests and compete
with each other for who kind of runs the solar system. And then some increasingly smaller number
of middle class who tends to those people's systems and then a much larger underclass that
is just not useful anymore because robots and AI are better at most things. So we give them
Oculus and UBI. So they're not a problem. Like, that's a shit world. That's a shit world. And
that's one of the attractors right now. Now, the same tech can be implemented towards very
different purposes that make a much higher quality of life possible. But again, when you take the
tech, oh, so you take that tech's going to merge. So, all right, well, the jobs are going to get
automated. Well, that sucks if the people need the jobs. Well, UBI is one way to make the people
not need the jobs, but still with quite low autonomy and upward mobility and etc.
But beyond UBI is something like access to commonwealth resources that she gets beyond just
individual possession and ownership being the only way to have access.
And being able to separate the meritocratic stewardship of resources from access to quality
of life. Those are things that are both mediated by dollars right now, but can easily be separated
should be separated. Well, technological automation, automating the easiest things to automate are
the things that are most wrote. The things that are most wrote are the things that people like
to do the least anyways. So, could we direct this tech in a different way, a not Centabillionaire
class oriented, not a short term ROI, but a how do we make a beautiful civilization way where we
say, okay, great, so we can automate the shitty jobs. Awesome. We used to have to financially
incent people to do those shitty jobs because civilization needed the jobs to get done. So,
we made it to where the people needed the job. So, the market forced them rather than the state
forcing them because otherwise we called it totalitarianism and didn't like it. We'd rather
let the market force them. Well, we just changed one of the underlying bases of capitalist stereo,
communist everything, which is you need a labor force. You don't need a labor force. You have
to rethink all the economics from scratch now. Okay, well, now maybe we don't need a system
of extrinsic incentive to force people to do shitty labor jobs. And you can make a whole
education system and an economic system that is oriented towards intrinsic incentive. What is it
that what what meaningful things could humans do with a life that they would have intrinsic
incentive for? I don't have to extrinsically incent them to do. We simply have to support
their capacity to do it. So, the same technologies that are being implemented based on current easy
market opportunity that lead to either dystopic worlds or catastrophic worlds could lead to really
beautiful worlds, but it's not just based on the current easy market opportunity. It's based on
designing it differently, aligned with long term civilizational vision. And I guess this is one
of the key things is a system that is in the process of committing suicide, right? A self
terminating system is not a system you should try to win at. And dollars are kind of like the
scorecard of how well you're doing at this current world game. And so if I have
it, what the market currently incents?
If I follow that, I'm not going to build a new system of incentive. If I want to build a new
system of incentive where that $70 trillion a day of human activity is incented in a different
direction, how I build that new thing is not going to be what's incentivized by the current thing.
So it's going to be some people who make an evolution in their values first,
who are willing to build new technologies in new ways, social and physical technologies together
in new ways that create new systems of incentives that make it easier for other people to onboard
into those values. But the beginning of a new civilization is always not winning at the previous
one. It is always willing to fight a revolutionary war to take some real sacrifice to migrate to
something because something matters enough that it's worth not continuing to just try to
be secure or win at the existing one. And this shift in sense making is the goal of the
Consilience Project? I didn't even do Consilience Project. Sorry. Thank you for bringing me back.
The goal of the Consilience Project is to help this zeitgeist that the one thing that civilization
needs to be centrally focused on is the development of new social technologies that both employ and
direct exponential technology. The goal is to not only get that zeitgeist out there, but to get
enough understanding about what those new systems must be, that the design criteria for
decentralized innovation can happen. So we're writing a bunch of articles. We have a research
team of people that have been thinking about these things in novel ways for a long time from
different disciplines. We're growing that team if people have the right kinds of background and
insights. And so what we're writing is where people can understand, take any social system,
take the Fourth Estate, take education, take law, take economics governance. Fourth Estate,
media, if you're going to have something like a democracy or a republic anywhere where there's
participatory governance, the people have to know what's going on to be able to be
able to be civically engaged. So you need some uninvested way of getting everyone the sense
making. Well, obviously those systems were built with print as the mechanism. And we can see that
the world has evolved into a way where they've all been captured by vested interests, economic
and political interests, and then attention optimizing social media technologies that deliver
it to people micro-target the things that appeal to their biases the most, their in-group biases.
So without a good Fourth Estate, you cannot have participatory governance. You need to be
ruled. But the type of Fourth Estate that has ever been possible before is not possible in a
post-Facebook world. So what are the criteria for a Fourth Estate that is adequate where people,
for an information commons and an epistemic commons that is adequate that people can participate in
collective choice making because they have good enough collective sense making? How do we do that
in a post-digital world? So we have a series of articles we're building that help people understand
what any civilization that wants that participatory governance needs in terms of an epistemic
commons, why the technologies that have emerged have broke the previous answers completely and
forever, and we can't go back to them, and what the criteria of the new solutions must look like.
Now, exactly how to build it we're not saying because there's different ways. But what we're
saying is these are the criteria that it must be. So now if you want to build a blockchain
organization or you are a nation-state and want to try to innovate as a nation-state or whatever
it is, great, like work on these things. Same with education. What we can show how emerging tech has
destroyed the previous system of education and obsolete it and then say what is education in a
post-technological automation, post-AI, post-information singularity world? Well, must be these things.
So our goal is to basically get the curriculum or the metacriculum. It could be translated to
different pedagogical levels, but the key insight, so the metacriculum that is needed for people to
be engaged in the design of the future of civilization, right, to get that out there.
And then to get it translated to where at least for most people won't be actively designing these
systems, some will, but they will at least be calling for them. So the people who are building
them will be able to have support, whether it's political support, economic support, social support,
whatever. So we're writing these articles and then trying to translate the info and the articles
through things like this, through podcasts and conversations that are able to translate them
and where people who do understand them will be able to translate the message to their audiences.
And so you start to get a decentralized zeitgeist of people caring about what we think is valuable
for people to care about, so as to be directing innovation.
Growing and so building metacuricula, curicula for various disciplines around the world. Ken
Wolver, of which I am a fan, has experienced more than 60 disciplines, you know, that adopted the
integral model. And so it's a long journey for you. Will there be a metacuriculum for investing
and or entrepreneurship? Ken, what is your plan?
So let's say people start thinking about, all right, well, what are the risks associated with
these various areas of exponential tech, what would it take to buy in that? If I'm a government
agency, I can think about what state capacities do we have to do that or could we make? If I'm a
nonprofit, I can think about that. If I'm a business, I can think about or what kind of business could
we build? We can also do things that are more creative than business as we've understood it,
things like a decentralized autonomous organization in the crypto space. So
all of those are different ways of trying to organize people and steward resource to build
something. So, you know, if we look at the Gates Foundation, there was basically a kind of worldview
that informed a philanthropic hypothesis, and it was the like, let's apply business metrics,
kind of business analytic metrics to philanthropy. I would say that what we're talking about would
make a basis for new philanthropy, which is let's donate money to the projects that are
working on building the new institutions necessary, the new civilization systems necessary,
because again, otherwise, the billion dollars that we donate going against 70 trillion dollars a
day, it's not going to matter that much. So how do we actually build new systems that can scale to
the thing that needs to happen? In the same way that that could be the basis of a philanthropic
hypothesis, it could also be the basis of a return hypothesis. It says, okay, if we're not trying to
maximize returns in the short term, no matter how nasty the thing is, but we're trying to create returns
while and through having that capital support the most meaningful things that it could.
And what we're pointing out is a basis of criteria for assessing what meaningful
things are. Is this good design? Is it actually moving in the right direction?
Yeah, absolutely. I hope for that to become the basis of investment hypothesis.
And I mean, even just listening to this, someone can start to think about,
oh, you have a piece of technology that harvests people's individual data and
does personal micro-targeting to be able to direct their attention. That sounds like a very
profitable business towards what aim is it going and what are the secondary effects?
Oh, it's going to be this thing called Facebook. It's really for dating and then it'll be like
for tagging pictures. Well, let's think through it a little bit. How does it pay for itself?
It's going to be an ad model. How do you, so it's, you sell more ads based on more time on site. How
do you optimize time on site when people want to get off and do things in the real world? Well,
through the things that are stickiest. So how do you figure that out when there's a huge amount
of stuff? Well, we use AI and we use their data and we, oh, so the secondary effect of this picture
tagging app is going to be that it drives addiction in the entire population and doubles down on
everyone's bias and limbic hijacks and destroys democracy in the process. Awesome. No, I don't
want to invest in that. So now if you want to change the design criteria, right, this shouldn't
have an ad model. People pay for the service and now the service, they are the customer and so you
have a fiduciary responsibility to them rather than to the advertiser and the people of the product.
Now we can actually do something that's meaningful for the people. Yeah, I'll invest in that thing.
And maybe it has a longer return cycle. Awesome. Long return cycles are necessary to do things
that aren't evil. The short term money on money process, like it's very important to understand
that capitalism is not business. It's not the same thing. Business was pre-capitalism. Business
was the idea of how do I make a good or service that people actually want based on real demand
and how do I make the best good or service at the best value so that hopefully rational person
will make a good choice. And in doing so, I get more money through production only of real value
at a good value, in which case I'm a good steward of that money because I'm going to continue to
produce real value. That's kind of like a very brief version of randian market ideology.
It's not capitalism. Capitalism is once you start doing that and you have a big pool of
capital and you realize that compound interest makes money faster. And financial services of
various kinds make money faster than the good or service because the good or service has a cost
of good sold and the money doesn't. And the money has an exponential return on it. And the other
thing doesn't have an exponential return. Capitalism is about stewarding pools of capital to create
more capital. And you know the saying that down criminals break laws, smart criminals make laws,
they start to recognize, well, who makes laws? Lobbyists. Who pays for them? People with money.
Why do they do it? Because they can make more money if they do the thing. And what do politicians do
after they stop being politicians? Well, they're still economic actors. Oh, great. So if I have
more money, I can also change law in the direction of the interest. So capitalism works to, we can
also create media companies. We can also fund certain kinds of media. We can also fund certain
kinds of research. So what people even think is true is based on the shit that actually got funded.
So to increase those pools of capital, I can pull on the levers of
affecting policy, affecting media zeitgeist and style and like that, affecting even research and
the fundamental ideas of what we think is true can affect all of that. Ultimately, to be increasing
optionality and power. That is not a system that is meritocratic where the more money you have is
a sign of the more good you did. No, it's utter gibberish. I can make money through production.
I can also make money through extraction. And as behavioral economics showed, it's very easy to
compel people to not be rational actors. So I can make money through producing valuable things or
I can make money through producing addictive shit and stuff that drives FOMO, that clearly
makes everyone's life worse. And given that the supply side, if it's a million dollar, a billion
dollar, a hundred billion dollar organization has way more coordinated power than the individual
purchaser does, it's an asymmetric war to influence them where demand isn't driving supply, supply is
manufacturing demand. Well, the intelligence, the whole base of market theory just broke.
As soon as you can manufacture demand, the idea that it is a collective intelligence system is
broken. It's not a collective intelligence system now. It's a paperclip maximizer. It is now an
autopoetic process for converting people and the world into capital.
Why did we go there?
Because you wanted to support our listeners, investors and entrepreneurs to make better
decisions on where to invest and where to buy their capital and their efforts and all of their
resources toward. And I'm grateful, very grateful for that. So financial services were kind of the
beginning of the end of market theory actually having any real integrity. Because now I could
not be producing real goods and services but doing speculation and market manipulation and
blah, blah, blah. And the ability for short-term money on money creating so much advantage that we
didn't invest in real things and then things in the domain of atoms have cost of goods sold,
whereas in the domain of virtual don't. So I don't get unicorns in a year anywhere other than software.
But what's happened is not what the world needs to be better. But all the money is going to flow
away from, oh, we don't have money for green tech and whatever because it flows to the area where
you get exponential returns that are making shit the world doesn't need based on manufactured
demand because it's where the return cycles happen. That shit has to shift. So can you make a return
in the current world with the money you have? Sure, doing some good things. But most of the
opportunities authentically suck. Like most businesses, if they shut down, the world would
be better, not worse. Most new businesses that are emerging will cause more externalities than
the benefit they cause is worthwhile. Most of the ways that people make money will cause enough
harm to the world that the philanthropy they do with that money will still end their life being
negative to the world. If someone wants to claim they're doing impact investing, just fucking be
honest about this. Just don't do it as greenwashing. It's evil. Just be a capitalist. That's fine.
It's better to be honest about it. Otherwise, if you really want to do impact investing, what
do you want to impact? Why? What do you honestly want to impact? Then look at clearly the second
order effects of the things that you're putting it into. There are some things that are worth investing
in. I want way more money going into fusion. I want way more money going into many areas of
energy technology, into manufacturing techniques that get to go close-loop materials economy so we
can make all the new stuff from old stuff into the types of nanotech and manufacturing that can
make that happen. Those are longer-term return cycles. They won't be exponential return cycles
in the same way because they'll have cost of good and they'll have R&D up front. R&D in the domain
of atoms is longer and slower, more expensive than it is in the domain of software. That's
the shit the world actually needs. It can be profitable. It can even be super profitable
because it can solve major world problems. Larger investments that are longer-term,
we didn't get railroads with short investments. Railroad wasn't worth anything until it went
all the way across the country. The industrial revolution required long-term planning.
How long did Notre Dame take to build? The short-term return focus is fundamentally
destructive to life. One of the first things I can say is have a longer-term focus. Then in
the thing that you're investing in, is it solving a critical must-solve problem for the world?
What are the likely second-order effects it's causing? If it's not solving a critical problem,
and if it generates second-order effects that are worse than the problem it's trying to solve,
don't invest in it. Wonderful, Daniel. It's been a great pleasure to speak with you again.
I know it's two o'clock in the morning in your time, so I want to thank you so much for your
graciousness of time and knowledge and information and inspiration. Thank you so much for being on
the program, and we will continue. I appreciate the education that you're bringing to people,
Mary-Anna. It was a delight to be with you and talk with you today. Thank you. Bye-bye.

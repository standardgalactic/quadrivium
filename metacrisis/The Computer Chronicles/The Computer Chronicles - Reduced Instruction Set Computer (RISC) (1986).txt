A modern jet plane, the epitome of speed and power.
Some people think today's sophisticated computers are also the epitome of speed and power, but
today's personal computer may be more like a DC3 compared to a new kind of machine called
a reduced instruction set computer.
What is that?
You'll find out today on this edition of the Computer Chronicles.
Computer Chronicles is made possible by Leading Edge, makers of IBM-compatible computer systems
including Lotus Look-alike spreadsheet, word processing with spelling correction, communication
software, and Hayes-compatible 1200-baud modem.
Leading Edge with over 1,000 service centers nationwide.
Additional funding is provided by McGraw-Hill, publishers of Byte.
Byte's detailed technical articles on new hardware, software, and languages cover developments
in computer technology worldwide.
Welcome to the Computer Chronicles, I'm Stuart Schiffet and this is Gary Kildall.
Gary and I are playing this game of RISK here, R-I-S-K, and as you know Gary there's a very
risky game out there called RISC, standing for Reduced Instruction Set Computing, some
major players in that game, IBM and Eulet Packard for example.
Some people are saying RISK technology is the wave of the future, other people are saying
RISC is already a passe technology.
What does this expert think?
Well Stuart, the current IBM PC line, the PC, the XT, the AT and so forth are based upon
a processor called the 8086, the 8086 is really the next generation following the 8-bit processors,
the 400 was called the 8080.
And so we've got right now is a, we try to expand into a larger memory space, go from
the 640K restriction to let's say 8 megabytes and things like that, it's really very very
difficult to do that, it's kind of a messy architecture.
So RISC gives this as a new starting point, we can say okay here's a simpler instruction
set, faster, less code space, it makes it easier for a compiler writer to write compilers,
and for a software writer we can stay with high level languages and really don't care.
Well we're going to take a look at IBM's RISC machine today, the RTPC, we'll meet the
man who coined the term RISC and we'll talk to some other RISC experts both pro and con.
As I mentioned Eulet Packard is one of the major players in RISC right now and we're
going to start out by going out to HP's RISC lab.
In the search for faster and more powerful computers, improved hardware is often taken
center stage while changes in architecture trailed behind.
The new architectural design used in RISC, or reduced instruction set computers, uses
simpler instructions requiring fewer machine cycles.
In the early 1970's IBM examined the kinds of operations most commonly carried out by
a complex instruction set machine and found that only about 20% of the simplest instructions
represented about 80% of the processing time.
RISC architecture relies on these simple computational elements to execute most of its operations
in a single cycle.
While RISC has made few inroads in the market, at least one major company, Eulet Packard
demonstrated its confidence in RISC by introducing a whole line of RISC based machines as the
next generation of its high end computers.
All computations except load and store take place in the data path.
Inline subroutines can be called up for complex functions and assist processors operate when
required for floating point arithmetic and graphics.
The special nature of RISC design makes it well suited to engineering and scientific
applications where instructions of four bytes or less can be executed in one cycle.
But while such applications demonstrate the advantages of RISC, some critics contend that
the speed gained by reduced instructions could be wiped out by the longer data paths and
by the need to accommodate complex functions.
Joining us now in the studio is Dr. Joel Beerenbaum, Vice President and General Manager for the
Information Technology Group at Eulet Packard.
And next to Joel is Dr. David Patterson, Professor of Computer Science at UC Berkeley.
David, just for background, where did RISC come from?
What's the origin and what really makes it different from traditional kind of machine
architecture?
Well, the idea is going towards simplicity, going back to kind of a small, beautiful philosophy.
There's lots of origins to it.
One of the main ones was done at IBM during the middle 70s in some research groups out
here in California.
The philosophy is by trying to make simpler computers to make them faster and more cost-effective.
Now, if we compare, let's say, a traditional, what we call a traditional machine, like a
286 IBM PCAT, how does a RISC machine compare in terms of speed?
The belief is by those researchers in the field is that using the same resources, you
can get something like a factor of two to four improvement in cost performance compared
to the traditional design.
So given the same transistors as Intel uses on the 286, finding the AT, we should be able
to build another machine twice as fast or four times as fast.
Joe, I had heard a number which makes it perhaps easy for some people to understand about this
80% and 20% business, which somebody discovered maybe was you back at IBM in the 70s or so.
Could you explain that?
Well, I think the notion is that computers spend most of their time, 80% of their time
is a symbolic number, doing a few simple things and don't do the complex things very often.
And so the notion is that if you optimize the machine to do the simplest things as well
as possible and to do the complex things as infrequently as possible, then you might
come out with a machine which for certain types of jobs would be more effective than
those which pay the penalty of complexity across all of the things that it does each
of the times that it does them.
Joe, if you talk about RISC, is that a philosophy, a way you design a machine architecture, or
is it a specific set of instructions?
It certainly is a philosophy or a style, and in fact it's a style which differs depending
on what you're trying to do.
So the main idea is not to add any complexity to the machine unless it pays for itself by
how frequently you would use it.
And so for example, a machine which was being used in a very heavily scientific way where
floating point instructions were important might make a different set of trade-offs than
another machine where that wasn't important.
Only one in which compatibility with other machines was important or in which certain
types of networking was important would include different features, but in each case they
ought to be done as the result of measurements of relative frequency of use and the penalty
that you would pay for the inclusion or non-inclusion of a particular feature.
So where do you see RISC architecture being important to say in computing right now?
We're going to see it moving to the scientific areas.
Is it going to be a replacement for the current desktop computers for RISC?
I don't think you're going to like my answer because I don't think there is such a thing
as RISC architecture.
I think what there is is good engineering design of a machine called a computer.
And in my view, all such machines ought to be designed by analyzing exactly what they
do and what the objectives of the person building the machine are.
And then building that analysis into the specific design.
It's turned out that a smaller number of instructions, those which are most frequently
used, can in fact address a very wide range of applications and types of applications,
sizes and types.
And so that's come to be called the reduced instruction set machines because in fact they
do wind up being a smaller number of hardwired instructions.
But to make a successful product includes many, many other things.
At HP for example, we spent probably two thirds of our time on portions of our new computer
family that have nothing to do with the instruction set itself.
If we introduce a new machine into society, there's a problem obviously of trying to get
applications onto that machine.
What's the strategy in terms of, let's say, healed packages?
How are you going to get people to move into a new architecture like that?
Well in our case, we're trying to get our customers and if we're lucky maybe a few of
other people's customers to move on to the new machines from at least three previous
families, each with their own instruction sets, compiler families and so forth.
So we've had to provide smooth migration paths for previous customers.
When the people have programmed in a high-level language, we can often accommodate that simply
by recompiling, which gets us most of the features but not all of the native mode architecture.
When people don't have that capability, we run in a compatibility mode and we have a
wide range of tools and accelerators, sometimes in hardware and sometimes in software, which
essentially take the old instructions and transliterate them or translate them into
the new ones and that can be done in a variety of ways.
David, you helped create this notion of risk.
How do you think the evolution has come?
I mean, you hear Joel saying, well, it's not really such a simple thing as just a risk
machine.
Well, I think what's been interesting is how viable the philosophy has been in many different
fields.
The original work that was done ignored what's called symbolic computing for artificial intelligence
and the folding point computations.
The experiments that several groups have done since then have shown that risk philosophy
of an experimental-based design works extremely well for symbolic processing, for languages
like common list, and for the floating point computations, number crunching applications.
So I think it's a surprisingly viable approach.
Okay, gentlemen, thank you.
Now we've been talking about a risk machine and just a minute we'll take a look at one,
the IBM RTPC in just a minute to stay with us.
Joining us now in the studio is Dr. W. Frank King, General Manager for Advanced Engineering
Systems at IBM, and next to Frank is you, Martin, Vice President of Development at Ridge
Computers.
Gary?
The risk concept is really apparently the central processing unit instruction set philosophy.
Now what about related concepts like parallelism, IO concepts, fast IO processing, have you
worked on those things at all?
Sure.
At Ridge what we've done is we've concentrated first on the central processor, but the entire
machine has to be designed to be efficient, and so we spent a lot of time initially on
the IO processor section to make sure that the performance of the IO matches the central
processing unit, and now that we've got this simplified processor we're examining parallel
architectures which can take advantage in the same way of this simplified architecture.
Now, VAX computers, the 11, 780s and 750s and so forth, they're used a lot because they're
good IO processors.
Do you think that the risk architecture with, say, fast IO is going to be a competitor
for VAX?
Well, I think that whether or not you have a risk computer you need a fast IO system.
When we designed our first machine, we made sure that we had an extremely fast IO system
to go along with this fast computer, and so as we move into marketplaces where there
are lots of users and disks connected to the machine, we find that we do just as well
as a VAX.
Okay, good.
Frank, this is a real example of a risk machine, it's the RT.
This, apparently, you can give a pretty good comparison on the speed of this because isn't
there a co-processor that runs in this machine that will run the standard 286 applications?
There is, Gary.
This machine runs 1.6 to 2.1 million instructions per second, and it has in it a co-processor
board with a 286 on it that runs about a third of that speed.
Frank, what market is this machine aimed at?
You've got this out on the market now for some period of months now.
Who's buying the risk machine?
We packaged a tabletop version of the machine primarily for those people that need speed
and capacity above what the IBM personal computer and the personal computer type systems can
support.
So it's really for people who need more speed and more capacity.
Well, who are they?
There is an easy example, architects doing building design, people in the insurance
industry that are examining actuarial tables, those kinds of people.
So you see business applications in addition to science, engineering, or other things that
are in there.
Yes, high-function business applications.
Now, Frank, just for a reference point, what's the cost of this machine?
This machine, as it sits here on the table, is about $10,000.
And that comes with how much memory?
This has up to 4 megabytes of memory in it and a big 70 megabyte file in it.
Okay, Frank, you mentioned design, and Gary, I'm going to put you on the spot now.
I want to see how easy it is to use this RTPC here.
You're going to be our operator, and you're going to help show off Frank's machine.
What can you show us on this, Gary?
Well, I'm an expert now.
I've had a total of 10 or 15 seconds for this instruction.
I'm going to go in here and get a technical illustration, and so I select that and click
on it, it opens the window up, and here's a document of some sort with a vector drawing,
I believe.
What I'm going to do is go ahead and select this, and if I point at this little segment
properly, I've got to get a hold of it, there it goes, it starts flashing, and now I can
get a little menu down here, and I'm going to take this thing and rotate it, and as I
move the mouse around, you can see that the object itself is moving, and then I can just
leave it in that position.
Okay, in that real-time rotation, I take it as an example of something that's requiring
a lot of fast processing to handle that.
That's right.
Or I can pick up a piece right here, for example, to get a hold of it, and my hand-eye coordination
isn't that good.
Okay, well, there it goes again.
Okay, now what I'm going to do at this point is I'm going to take the thing and move it
around, we're going to take it and slide it down to this portion of the screen, so you
can see the action there is pretty nice, and then just leave it.
Frank, one of the concerns I've heard about the wrist machines is the problem of adapting
software and so on.
Where do you stand on that?
Do you have to write new software just for this?
Not really.
Most of the software that runs on this machine is written in a high-level language, and so
the compilers have to be there to support it, and you do have to have compilers that
know how to work with risk architecture, but the compiler really masks moving the current
software to this kind of machine.
This is basically a Unix operating environment?
Yes, this is basically a Unix operating environment that supports C and Fortran and those kinds
of standard languages.
What's your strategy for moving applications over?
We talked to Hield Packard about that.
What's your...
We define a set of compilers, and those are the normal compilers, and then the price performance
of the machine has to justify people wanting to spend the money to move their software
to it.
You have a small company, a rich computer, isn't it?
I think you've sold four or five hundred of these wrist machines.
Who are you selling them to, and how are you carving out a niche among the HPs and the
IBMs?
Well, our niche is primarily price performance in a technical marketplace.
A primary example of a customer you may be familiar with if you've seen the Super Bowl
or the Olympics, all the animation that precedes those programs were done by a local company,
Pacific Data Images.
They have 12 ridges doing nothing but cranking frame after frame.
So once again, the heavy duty graphics requiring the speed and the power that risk can give
them.
Yes.
Frank, where do you see going next?
We hear rumors of an enhanced RTPC coming out and some changes in it.
What's...
Is that true?
Well, certainly IBM never talks about its features.
I know that.
So I have to start out by saying we're not going to talk about our features.
What we see going on in this industry, the desktop micro-computer industry, is really
a doubling of speed and capacity about every 12 to 14 months.
So clearly if we're going to participate in this marketplace and we intend to, that kind
of improvement over time has to be sustained.
We're seeing some pretty substantial improvements in speed on the 2A6 and 3A6 side.
Isn't that pretty competitive with the risk architecture?
Let's say a 24 megahertz 3A6.
Isn't that pretty close to the speed of this machine?
The 3A6's that are available today, of course, are 16 megahertz.
But as the Intel architecture becomes faster, in order for this kind of a machine to justify
the price and the price performance, it's going to have to stay faster, and that is
our intention.
Gentlemen, thank you very much.
Now we've seen a risk computer.
We've been talking about applications.
Let's find out about how one customer is actually using a risk machine.
Wendy Woods went to ESL in Sunnyvale, California to find out.
ESL in Sunnyvale, California is designing the world's fastest systolic processor, a
chip that could be used in such diverse technology as sophisticated listening devices, or in
image processing of pictures from outer space.
The task requires tremendous computing power, for which engineers depend on a risk-based
pyramid technology's mini computer.
It enables us to have a machine that runs much faster, and that's very important to
us because our programs take so long.
We also have a number of users on these machines, so it's very efficient in handling that.
And the particular kind of instruction mix that we get from our integrated circuit design,
which is fixed point instructions or integer instructions as opposed to floating point
numbers that have exponents, those run particularly fast on this risk architecture.
The integrated circuit that ESL is designing has 62,000 transistors, and the risk computer
is employed to perform a thorough simulation and verification of that design before it
is committed to silicon to make sure that it actually works.
You'd think that after two years, people here would be sold on risk technology, but not
so.
Parallel processing, they say, offers similar advantages, but the bottom line is that these
kinds of projects never have enough processing power, and engineers are not ready to marry
any one technology in their attempts to get a job done.
For the Computer Chronicles, I'm Wendy Woods.
With us now in the studio is Jan Lewis, president of the Palo Alto Research Group, and next
to Jan is George Morrow, whom we all know is our commentator here on Computer Chronicles,
and George is also currently vice president for engineering at Intelligent Access.
Gary.
Jan, the computer community has been pretty reluctant to accept new architectures in
the past.
Is this going to be a problem with a risk architecture?
I think this will be a problem for a couple of reasons.
First, is the concept of accepting a new architecture, but the other thing is that there's a lot
about risk that I'm very skeptical about, and in fact, the commercial applications have
yet to actually prove that risk is going to give us true performance increases.
The machines that are out there, both the HP that's shipping in small quantities and
the RT, really are watered down versions of risk.
They're not true risk machines, although they do use some of the risk principles, and some
of the performance improvements that we see may in fact not be because of risk, but because
of other factors such as on chip registers being a lot more in quantity.
George, what do you think about risk technology?
Well, looking at it from a point of view of silicon, the richer register sets and the
simplified instruction sets favor the use of silicon, so that's a positive thing.
On the negative side, we just seem to keep making new instruction sets, and that's like
inventing a new typeface every time you want to say something again.
So it's a real balancing act.
What I'd really like to see happen is for some of these guys to figure out how to make
a computer that would run anybody's binary without recompiling or translations of any
of this stuff.
That's what I'd really like.
Jen, you said the machines on the market are watered down and they don't really reflect
the theory.
What's the problem?
What's the technology problem that prohibits that?
Well, there's a few things.
First of all, the concept is that you have very few number of instructions.
I say 50 or less.
Juris says eight or less.
But you have a few number of instructions, each one using basically very few clock cycles,
one, theoretically, and that you get maximum performance as a result.
But in fact, the machines that are out there do need extensions.
They do need other things.
And in fact, the RT, for instance, actually averages about one instruction per 2.7 cycles
as opposed to the theoretical one.
So they're not purely risk.
They've sort of evolved from risk in the pure sense to risk-like architectures to architectures
using risk principles to simply simplified instruction sets on Chef registers.
What are the first applications going to be for risk architecture?
Where are we going to see it?
Is it going to be in the scientific CAD-CAM community?
I think it will be largely in the scientific community.
For one thing, it doesn't require quite the same amount of I.O., which still is a problem.
And the other thing is, in terms of accepting new architectures, I think if there's one
community that is less standards-conscious, it probably is the engineering and scientific
community.
George and Jen, you're both saying there are quite a few problems here with risk.
We've heard the phrase that Yulet Packard is gambling its whole computer division on
risk technology.
I mean, is that a danger?
Well, I don't think that's quite fair, because I think part of what HP has done and is an
investment to transport customers from their standard environments over into something
that's perhaps a little bit more flexible.
So I don't think they're gambling the whole company on it.
What Jen says is right.
But on the positive side, compiler writers, with lots and lots of registers, compiler
writers are going to be able to write better compilers.
And maybe we'll finally get to the point where we've truly got portable, high-level
languages with this kind of background.
That's probably the biggest hope.
Of course, the scientific community will jump on them quicker.
It does rotate images faster, that sort of thing.
What about the competition from the new processors from Motorola 68020, the new 386?
They're talking about 24 megahertz now.
Is that going to knock them out?
Well, I was on a forum called the 32-bit shootout, where Motorola introduced the 68030, and those
are very, very high-performance machines.
So it's possible the traditional approach will continue to get better.
But eventually, we're going to get down to the point where silicon is important.
And their risk machines win on silicon better than the complex instruction set.
George, Jen, Gary, we're out of time.
Thank you very much.
And we'll be back in just a minute with this week's Computer News.
In the random access file this week, both IBM and Ulet Packard have just made announcements
affecting their risk machines.
HP says software problems will cause a delay in the planned shipment of its Spectrum mini-computer.
Delivery is now expected in mid-1987.
And IBM has announced upgrades and improvements in its desktop risk computer, the RTPC.
The new RT Model 15 will feature improved networking capability, memory, and storage.
IBM also said it's cutting the price on the RTPC.
IBM is also beefing up its sales force, announcing the transfer of more than 2,000 employees
into sales, with a goal of increasing its overall sales force by 5,000 people.
Apple has announced a deal with Northern Telecom that will enable the Macintosh to network over
regular phone lines.
The new hardware and software can speed up Mac communications by a factor of 18.
Lotus is showing off its new HAL program for 123 users at this week's Info86 show in New
York.
HAL is an AI-based add-on that simplifies interaction with Lotus 123.
And Lotus has also announced its first CD-ROM product, called OneSource.
It's a compact disk containing 20 years' worth of financial data.
The OneSource CD-ROM can communicate with 123.
At the annual Software Publishers Association Convention in Washington, a spokesman for
Education Systems Technology said computers will reverse the trend of smaller classroom
sizes.
The port said computers will enable teachers to handle classes of 90 students with more
individual attention than is now possible in smaller classes.
Time for this week's software review, and here's Paul Schindler.
You know, computer software writers are constantly on the lookout for ways to improve manual
systems.
Now, if you, like me, use this kind of system for keeping track of things on your desk,
I think you'll like tornado notes.
Tornado notes is a memory-resonant, multi-purpose note-taker and finder.
It's like a combination of yellow stick-on-slips, paper, and a database management system.
Tornado notes takes this common idea a step further by creating a more desk-like environment.
Not only can it search your whole pile of notes for a specific reference, it can arrange
those notes as you'd arrange a pile of paper with some notes easy to access on top and
some notes buried on the bottom.
Tornado notes can insert dates and times, important export files, and offers a reasonable
word processor plus online help.
Unlike some older memory-resonant programs, tornado notes seems to coexist well, perhaps
a sign that memory-resonant program writers have finally signed some peace treaties.
Tornado notes is $50 from MicroLogic in Hackensack, New Jersey.
For The Computer Chronicles, I'm Paul Schindler.
A company named Gamalink has come up with a new product that lets you send copy from
a PC directly to a fax machine.
The add-on board costs about $1,000.
By the way, Federal Express announced this week that it is eliminating its own fax service,
Zap Mail, due to heavy losses.
An organization called the Overseas Security Advisory Council is starting up a bulletin
board on international terrorism.
The council says it hopes the BBS will become a source of important information for American
businessmen traveling abroad.
A blind businessman in Indiana has developed a talking spreadsheet program for blind users.
It's called Rapsheet and it offers six rows by 22 columns and most spreadsheet functions.
The developer says he'll give it away for free to other sightless people.
Finally, researchers at Carnegie Mellon University in Pittsburgh say they are making progress
on synthesizing the protein molecules that may one day be the CPUs of molecular computers.
One CMU researcher says molecular computers will make possible the replacement of defective
human neurons in the brain and will allow for tiny tumor patrol computers in the body
that would spot abnormal cell growth and automatically repair the cells.
That's it for this week's Chronicles.
We'll see you next time.
The computer Chronicles is made possible by Leading Edge, makers of IBM-compatible computer
systems including Lotus Lookalike spreadsheet, word processing with spelling correction,
communication software, and Hayes-compatible 1200-baud modem.
Leading Edge with over 1,000 service centers nationwide.
Additional funding is provided by McGraw-Hill, publishers of Byte.
Byte's detailed technical articles on new hardware, software, and languages cover developments
in computer technology worldwide.

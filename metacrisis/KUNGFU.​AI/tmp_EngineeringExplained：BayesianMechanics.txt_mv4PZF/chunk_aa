Hi, I'm Sanjeev Namjoshi, Senior Machine Learning Engineer at Kung Fu AI.
In this video, I'm going to give a mathematically minimal introduction to Bayesian mechanics
and related ideas like active inference and the free energy principle.
Bayesian mechanics is more relevant today than ever before because it holds the key
to unlocking artificial general intelligence.
I'd like to start with the following question.
What is life?
This is the title of Erwin Schrodinger's 1944 book where he famously asks the following
question.
How can the events, in space and time, which take place within the spatial boundary of
a living organism, be accounted for by physics and chemistry?
Today, we are close to answering this question from the perspective of Bayesian mechanics,
which integrates physics and machine learning to provide a statistical description of physical
systems.
Bayesian mechanics is a very new field, the term first being used in 2019.
A variety of other theories paved the way to Bayesian mechanics, starting, surprisingly,
from the statistical analysis of brain imaging data.
These techniques were developed by the neuroscientist Carl Friston and his colleagues in the 1990s
and turned into the well-known SPM and DCM software suites used for fMRI image analysis.
In the early to mid-2000s, Friston and his colleagues extended the statistical methodology
used in the software to provide a mathematical description of the brain.
This led to two other theories to be mentioned shortly.
The free energy principle and active inference.
By the mid-2010s, we saw further extension of this work attempting to provide a mathematical
description of living systems in general.
Today, this field is called Bayesian mechanics, the union of neuroscience, machine learning,
and statistical physics.
So what is Bayesian mechanics and what does it claim?
Let's start from the most general place we can.
Instead of describing living systems as a customary in biology, we will start from
the opposite perspective.
We assume that living beings exist and now ask, what do they do in order to keep existing?
Before we can really answer this question, we need to make some assumptions first.
What from the mathematical perspective might it mean to exist?
There are many answers to this question we could think of, but here's one.
Let's start from this simple observation.
Nature loves entropy.
Spray cologne in a room and the particles clustered in one place will slowly spread out.
Put some dye into water and it will slowly diffuse.
In terms of probability, the most likely configuration, due to random motion, is the
one where particles are spread out evenly.
But living systems are different.
We are a collection of particles organized into a specific, mechanically relevant configuration.
We have brain, organs, tissue, biochemical pathways, and even personalities and identities
which may change slowly over time.
Since we are open systems, we can use energy to maintain the form of the structures that
constitute ourselves.
This self-maintenance allows living systems to stay self-organized for periods of time
and maintain their identity.
Living systems structured in this way that can maintain their form over time are not
at thermal equilibrium, which would be death, but rather at non-equilibrium steady state
or nests.
Mathematically, we could say that such identity inherent in living systems mean they have
characteristic states they occupy more often than other states.
This naturally lends a description of such systems in terms of probability distributions.
One in particular we will call the non-equilibrium steady state density, or nest density, as
just mentioned.
This is much like the phenotype of the organism.
It is a probability density because on average, the organism is described by, or to be found,
more likely in some characteristic states rather than others.
A fish cannot survive out of water.
The organism's goal is therefore to stay in these preferred states despite the changes,
uncertainty, or random fluctuations in the world around them.
These changes threaten to pull the organism out of its preferred states where it risks
succumbing to thermal equilibrium.
Instead, there is a contributing flow from the organism that pulls it back to preferred
states.
We will talk about the nature of this flow in a moment.
So we could argue that existing means to be separate and distinguishable, and perhaps
even measurable, from other things over time, existing in these characteristic states.
We could say that existence thus implies some kind of boundary that separates a living
thing from what it is not, its surroundings.
While we might be tempted to describe this separation with a physical boundary, it is
actually more convenient to use a probabilistic boundary because the equations we are using
are based on probability distributions.
In probability theory, such operations can be achieved through conditional probability
and independence.
And since the systems we are talking about are physical in nature, we can describe how
they change over time using stochastic differential equations, which are used in physics to describe
dynamical systems.
We will partition the universe into three different sets of states, external, states,
eta, blanket states, b, and internal states, mu.
Blanket state, technically known as a Markov blanket, is responsible for the statistical
partition of the internal states of a living organism from the external states of its
environment.
When conditioned on the Markov blanket, internal states and external states become separate
dynamical systems that can interact through the blanket.
The two systems thus change independently over time, and the living system remains at
non-equilibrium study state.
Functioning as a kind of interface, the Markov blanket can itself be decomposed into sensory
states, which give it a peek into the external environment, and action states, which allow
it to act on and change the external environment.
So long as a living system maintains this boundary, it remains separate and distinct
from its surroundings, and is thus alive and exists, according to this definition.
Invasion mechanics is possible to write down stochastic differential equations describing
the flow or change of these states over time for certain systems given some initial conditions
and assuming the existence of a nest density.
Our goal is to understand how to model the dynamics of such a system, which is it-ness,
or non-equilibrium study state.
We describe the entire system and its components in terms of a Langevin stochastic differential
equation over the different partitions.
We can determine the conditions that must mathematically be true for such a living system
to exist and stay at study state by expressing what is known as a steady state solution to
the Fokker-Planck equation.
This equation tells us how the probability densities evolve over time in stochastic systems.
Here we define P of x to be the steady state distribution, and then the solution to the
Fokker-Planck equation is shown here, at study state.
The solution can then be decomposed into two orthogonal components using the Helmholtz
Io decomposition.
This shows that one component of the system is due to noise, which tries to dissipate
the non-equilibrium study state density and pull the organism out of steady state.
The other is a contravening solenoidal flow, which counteracts this process and keeps the
system in steady state, maintaining the Markov blanket.
Under the partition we have created, this amounts to separate decomposed flows for each
of the different partitions of the system.
In short, Bayesian mechanics tells us how to describe systems at non-equilibrium study
state, and shows that such systems can exist and can be modeled.
Next, we must ask, how do living systems maintain this boundary and stay in the characteristic
states of their nest density over time?
This is where the theories from neuroscience come into play.
From 2006 to present, Carl Friston and his colleagues have been developing active inference,
a statistical model of the brain.
Active inference is a unique approach to agent-environment interactions.
Unlike reinforcement learning, in active inference there is no direct notion of reward.
The brain is a pure prediction engine.
The process of perception is cast as a prediction about the states of the environment which
uses sensory data to update these predictions and to form more accurate models of the world.
Action is also a kind of prediction in the sense that it is a correction to the environment
to force it into alignment with our predictions about it.
There are two flavors of active inference, one used for adaptive control and homeostasis
and one for planning and decision making into the future.
Active inference is a highly general algorithm that relies upon some generative model of
the environment, that is some model of the external world and how sensory data is generated.
Basically every known statistical estimation method can be derived from active inference
core algorithms which applies to any non-linear random dynamical system, even those exhibiting
stochastic chaos.
It thus has an enormous variety of applications beyond the neurosciences, specifically in the
analysis of time series data, and even been used for COVID-19 modeling.
Active inference specifies the optimal behavior of internal and external states under Bayesian
mechanics that will be needed to maintain the nest density.
This optimality means, as we will see shortly, the machine learning comes into play.
Active inference describes how internal states learn to predict, or track the external states
of the environment over time, and how active states come to encompass behaviors which try
to help sculpt the environment into a desired configuration.
Active inference models can be written down as recurrent neural networks that are believed
to underlie the neural anatomy and the hierarchical layers of the cortex.
Under specific assumptions, tracking the flow of paths instead of states in Bayesian
mechanics and using generalized coordinates of motion which track the higher order derivatives
of the paths, we can derive the most likely paths of internal and active states at nests.
You may notice the F symbol that has appeared in the equations describing optimal paths
of internal and active states.
To perform active inference, it is proposed the brain calculates a statistical quantity
known as variational free energy, F, that allows it to course correct its predictions
about the world based on sensory data.
The smaller this quantity is, the closer the environment aligns with the brain's predictions
about it.
In machine learning, negative variational free energy is a loss function, sometimes known
as the evidence lower bound, or elbow.
Since variational free energy is an upper bound or approximation to negative log evidence,
also known as suprisal or negative log likelihood, we can see this whole process can be thought
of as a form of approximate Bayesian inference.
Variational free energy is so important in active inference that it has led to the notion
of a free energy principle, that optimal action and perception occur when the brain minimizes
variational free energy by gradient descent.
In the neurosciences, free energy is used in many different ways, including as metaphors
for learning and attention.
One can also calculate expected free energy for planning and decision making.
This entails calculating the variational free energy associated with sequences of possible
actions.
In this case, variational free energy of the future is known as expected free energy.
Minimizing variational free energy enables living systems to stay in a kind of steady
state with their environment, maintaining their characteristic states that establish
their identity in the face of the unknown, that attempts to destroy the carefully crafted
self-organization of the system.
One can think of this much like the piloting of a ship on a choppy ocean.
The captain will try to stay on course, predicting what is going to come next, sometimes many
steps into the future, and correcting or adapting by steering the ship back on course to keep
the ship afloat.
This is life.
Those of us who do this successfully survive and adapt in the face of the fluctuating uncertainty,
and those that do not, dissipate, disperse, decay, or die.
Now let's take a step back and summarize.
Vasion Mechanics uses stochastic differential equations to describe physics of living systems.
We note that living systems maintain themselves in a characteristic set of states over time,
which is in a steady state with the environment.
This implies that living systems exist as separate and distinct from their environment.
The boundary separating a living system from its environment is statistical in nature, known
as a Markov blanket.
To maintain this boundary, survive, exist, and resist entropic decay, living systems
use a brain, or some internal model of their external environment, to predict and perceive
how it will change over time.
Living organisms can also adapt and sculpt their environment through actions.
Together, this is known as active inference, which relies on the free energy principle,
where the free energy principle can be accomplished through the minimization of the statistical
quantity known as variational free energy.
Those that adapt successfully thrive and survive.
Those that do not, lose their Markov blanket and become indistinguishable from the surroundings
they interface with.
Active inference and Vasion Mechanics are fast growing fields encompassing many disciplines
from neuroscience to philosophy.

An important question that has to be answered when you're designing an
autonomous system is how do you get that system to do what you want? I mean how
do you get a car to drive on its own? How do you manage the temperature of a
building? Or how do you separate liquids into their component parts efficiently
with a distillation column? And to answer those questions we need control theory.
Control theory is a mathematical framework that gives us the tools to
develop autonomous systems. And in this video I want to walk through everything
you need to know about control theory. So I hope you stick around for it. I'm
Brian and welcome to a MATLAB Tech Talk. We can understand all of control theory
using a simple diagram. And to begin let's just start with a single dynamical
system. This system is the thing that we want to automatically control like a
building or a distillation column or a car. It can really be anything. But the
important thing is that the system can be affected by external inputs. And in
general we can think of the inputs as coming from two different sources. There
are the control inputs you that we intentionally use to affect the system.
For a car these are things like moving the steering wheel and hitting the break
and pressing on the accelerator pedal. And then there are unintentional inputs.
These are the disturbances, D, and they are forces that we don't want affecting
the system but they do anyway. These are things like wind and bumps in the road.
Now the inputs into the system interact with the internal dynamics and then the
system state X changes over time. So for a car we move the steering wheel and we
press the pedals which turn the wheels and revs the engine producing forces and
torques on that vehicle and then combined with the forces and torques from
the disturbances the car changes its speed position and direction. Now if we
want to automate this process, that is we want the car to drive without a person
determining the inputs, where do we go from here? And the first question is can
an algorithm determine the necessary control inputs without constantly having
to know the current state of the system? Or maybe a better way of putting it is do
you need to measure where the car is and how fast it's going in order to
successfully drive the car with good control inputs? And the answer is actually
no. We can control a system with an open loop controller also known as a feed
forward controller. A feed forward controller takes in what you want the
system to do called the reference R and it generates the control signal
without ever needing to measure the actual state. In this way the signal from
the reference is fed forward through the controller and then forward through the
system never looping back. Hence the name feed forward. For example let's say that
we want the car to autonomously drive in a straight line and at some arbitrary
constant speed. If the car is controllable which means that we have the
ability to actually affect the speed and direction of the car then we could
design a feed forward controller that accomplishes this. The reference drive
straight means that the steering wheel should be held at a fixed zero degrees
and drive at a constant speed means that we depress the accelerator pedal some
nonzero amount. The car would then accelerate to a constant speed and
drive straight exactly as we want. However let's say that we want the car to
reach a specific speed like 30 miles an hour. We can actually still do it with a
feed forward controller but now the controller needs to know how much to
depress the accelerator pedal in order to reach that specific speed and this
requires knowledge about the dynamics of the system and this knowledge can be
captured in the form of a mathematical model. Now developing a model can be done
using physics and first principles where the mathematical equations are written
out based on your understanding of the system dynamics or it can be done by
using data and fitting a model to that data with a process called system
identification. Both of these modeling techniques are important concepts to
understand because as we'll get into models are required for almost all
aspects of control theory. Now as an example of system identification we
could test the real car and record the speed it reaches given different pedal
positions and then we could just fit a mathematical model to that data. Basically
speed is some function of the pedal position. Now for the feed forward
controller itself we could just use the inverse of that model to get pedal
position as a function of speed. So given a reference speed the feed forward
controller would be able to calculate the necessary control input. So feed
forward controllers are a pretty straightforward way to control a system.
However as we can see it requires a really good understanding of the system
dynamics since you have to invert them in the controller and any error in that
inversion process will result in error in the system state. Also even if you know
your system really well the environment the system is operating in should have
predictable behavior as well. You know so that there's not a lot of unknown
disturbances entering the system that you're not accounting for in the
controller. Of course it doesn't take much imagination to see that feed forward
control breaks down for systems that aren't robust to disturbances and
uncertainty. I mean imagine wanting to autonomously drive a car across the city
with feed forward control. Theoretically you could map the city well enough and
know your car well enough that you could essentially pre-program in all of the
steering wheel and pedal commands and just let it go. And if you had perfect
knowledge ahead of time then the car would execute those commands and then
make its way across the city unharmed. Obviously though this is unrealistic. I
mean not only are other cars and pedestrians impossible to predict
perfectly but even the smallest errors in the position and speed of your car
will build over time and eventually deviate much too far from the intended
path. So this is where feedback control or closed loop control comes to the
rescue. In feedback control the controller uses both the reference and the
current state of the system to determine the appropriate control inputs. That is
the output is fed back making a closed loop hence the name. And in this way if
the system state starts to deviate from the reference either because of
disturbances or because of errors in our understanding of the system then the
controller can recognize those deviations those errors and adjust the
control inputs accordingly. So feedback control is a self-correcting mechanism
and I like to think of feedback as a hack that we have to employ due to our
inability to perfectly understand the system and its environment. We don't
want to use feedback control but we have to. All right so feedback control is
powerful but it's also a lot more dangerous than feed forward control and
the reason for this is that feed forward changes the way we operate a system
but feedback changes the dynamics of the system. It changes its underlying
behavior and this is because with feedback the controller changes the system
state as a function of the current state and that relationship is producing
new dynamics. And changing dynamics means that we have the ability to change the
stability of the system. And on the plus side we can take an unstable or
marginally stable system and make it more stable with feedback control. But on
the negative side we can also make a system less stable and even unstable. And
this is why a lot of control theory is focused on designing and importantly
analyzing feedback controllers because if you do it wrong you can cause more
harm than good. And since feedback control exists in many different types of
systems the control community over the years have developed many different
types of feedback controllers. There are linear controllers like PID and full
state feedback that assume the general behavior of the system being controlled
is linear in nature. And if that's not the case there are nonlinear controllers
like on-off controllers and sliding mode controllers and gain scheduling. Now
often thinking in terms of linear versus nonlinear isn't the best way to choose
a controller so we define them in other ways as well. For example there are
robust controllers like mu synthesis and active disturbance rejection control
which focus on meeting requirements even in the face of uncertainty in the plant
and in the environment. So we can guarantee that they are robust to a certain
amount of uncertainty. There are adaptive controllers like extremum seeking and
model reference adaptive control that adapt to changes in the system over time.
There are optimal controllers like LQR where a cost function is created and
then the controller tries to balance performance and effort by minimizing the
total cost. There are predictive controllers like model predictive control
that use a model of the system inside the controller to simulate what the
future state will be and therefore what the optimal control input should be in
order to have that future state match the reference. There are intelligent
controllers like fuzzy controllers or reinforcement learning that rely on data
to learn the best controller and there are many others and the point here
isn't to list every control method. I just wanted to highlight the fact that
feedback control isn't just a single algorithm but it's a family of algorithms
and choosing which controller to use and how to set it up depends largely on
what system you are controlling and what you want it to do. So what do you want
your system to do? What state do you want the system to be in? What is the
reference that you want it to follow? And this might seem like a simple question
if we're balancing an inverted pendulum or designing a simple cruise controller
for a car. The reference for the pendulum is vertical and for the car it's the
speed that the driver sets. However for many systems understanding what it
should do takes some effort and this is where planning comes in. The control
system can't follow a reference if one doesn't exist and so planning is a very
important aspect of designing a control system. With a self-driving car for
example planning has to figure out a path to the destination while avoiding
obstacles and it has to follow the rules of the road. Plus it has to come up with
a plan that the car is physically able to follow. You know it doesn't accelerate
too fast or it doesn't turn too quickly and if there are passengers then
planning has to account for their comfort and safety as well and only after
the plan has been created can the controller then generate the commands
to follow it. An example of two different graph based planning methods
are rapidly expanding random trees rrt and a star. Once again there are too many
different algorithms to name but the important thing is that you understand
that you have to develop a plan that your controller will then try to follow.
Alright so once you know what you want the system to do and you have a
feedback controller to do it now you need to actually execute this plan
and as we know for feedback controllers this requires knowledge of the state of
the system that is after all what we are feeding back
and the problem is that we don't actually know the state unless we measure it
and measuring it with a sensor introduces noise so for our car example
we're not feeding back the true speed of the car we're feeding back a noisy
measurement of the speed and our controller is going to react to that
noise so in this way noise in a feedback system
actually affects the true state of the system and so this is one additional
problem that we're going to have to tackle
with feedback control. A second problem is that of observability
in order to feedback the state of the system we have to be able to observe
the state of the system and this requires sensors in enough places
that every state that is fed back can be observed.
Now it's important to note that we don't have to measure every state directly
we just need to be able to observe every state
for example if our car only has a speedometer we can still observe
acceleration by taking the derivative of the speed
so there are two things here we need to reduce measurement noise
and we need to manipulate the measurements in such a way that allows us to
accurately estimate the state of the system.
State estimation is therefore another important area of control theory
and for this we can use algorithms like the Coleman filter
the particle filter or even just run a simple
running average and choosing an algorithm depends on which states you are
directly measuring and how much noise and what type of noise
is present in those measurements. Now the last major part of control theory
is responsible for ensuring the system that we just designed
works that it meets the requirements that we set for it
and this comes down to analysis simulation and test.
For this we can plot data in different formats like with a Bode diagram
a Nichols chart or a Nyquist diagram we could check for stability and
performance margins we could simulate the system using Matlab
and Simulink and all of these tools can be used
to ensure that the system will function as intended
and so this full diagram here I think represents
everything you need to know about control theory. You have to know about
different control methods both feedforward and feedback depending on
the system you're controlling. You have to know about state estimation
so that you can take all of those noisy measurements and be able to feed back
an estimate of system state. You have to know about planning
so that you can create the reference that you want your controller to follow.
You have to know how to analyze your system to ensure that it's meeting
requirements and finally and possibly most importantly
you have to know about building mathematical models of your system
because models are often used for every part we just covered.
They are used for controller design they're used for state estimation
they're used for planning and they are used for analysis.
Alright I always leave links below to other resources and
references and this video is no exception and there are a bunch
for this video since I mentioned so many different topics
and something I think is nice is that we already have Matlab tech talks
for almost every topic I mentioned. We have feedforward
and PID and gain scheduling and fuzzy logic and Coleman filters and particle
filters and planning algorithms and system identification

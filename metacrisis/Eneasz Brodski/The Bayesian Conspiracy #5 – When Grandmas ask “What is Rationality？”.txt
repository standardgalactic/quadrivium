Welcome to the Bayesian Conspiracy.
The topic for this episode is how to explain rationality to your grandmother or explain
it to me like I'm five.
You didn't introduce us.
No one introduced us.
I'm going to introduce us now.
Oh my god.
I'm such a fame whore.
I need my name out there right away.
My name.
My name is Katrina Stanton.
I'm Iniasz Brodsky.
I'm Iniasz Brodsky.
Oh, there's two of me.
Yes.
I'm multiplying.
And I'm Stephen Zuber.
So let's dive right into it.
Okay.
Iniasz, how would you explain rationality to your grandma who is Polish?
She...
Is she live?
Yes.
I don't know.
Because I don't really talk to her because I can't talk to her because, you know, she
only speaks Polish.
So I'm going to pretend I have a grandma that speaks English for this episode.
Okay.
Sounds good.
But before we get into that, I want to do a little preface like disclaimer thing here
for this thing.
Okay.
Because these two already know this, but none of the listeners do.
And I was talking about this yesterday with Shelley, I believe, at our meetup.
People have several times...
Sorry.
Am I getting off the topic?
We named names and she points out that we named her last time.
Yeah, but she was okay with it.
Yeah, she was.
Okay.
Is that why we exchanged a look?
Okay.
Yeah.
But she's fine with it.
Okay.
So we got a number of emails saying, why don't you do a crash course in rationality episode?
And that was something that I had not wanted to do from the very beginning.
And it was actually one of the reasons I hadn't done a podcast ever aside from the, you know,
Harry Potter methods of rationality, even though people kept asking, hey, why don't you do something
explaining rationality in general?
Because I am of the opinion that if you are going to create something, it should either
be something new or something that is an improvement on what has come before.
And I couldn't do that with an intro to rationality, because an amazing intro to rationality already
exists out there.
And it's LDIser's series of posts over at lesswrong.com.
And anything I would do would just be a pale imitation of that.
And really, it would be a disservice because it would distract people from the good explanation,
you know?
I would be making the world worse by putting out a shitty explanation, so I wasn't going
to do any of that.
When I was finally talking to doing this episode, it was like, oh, it's just a conversational
thing about rationality.
And I'm like, yeah, okay, that's fine.
I can do that.
Not this episode, but this podcast, rather.
So I didn't think I had anything to add.
And if you want to actually get a good intro to rationality, I would say go to lesswrong.com
and read through some of those earlier sequences.
Go on.
The only thing I would add, the full length of the sequences is something like the length
of the Lord of the Rings trilogy.
So if you're kind of just like, I wonder what this whole thing is about.
You don't want to spend three months reading, and that's like a really good speedrun of
those lesswrong sequences.
But that said, there are some summary pages, some intro pages.
Click around the wiki and find whatever word you're looking for, rationality, bays, and
check out some of the summary posts, and that can give you a good quick crash course too.
Or you can follow the link on our blog and go through a Bayes' Rule Guide.
Yes, but that was actually one of the things that convinced me because it was pointed out
that that is a ton of reading.
And sometimes people just want a little summary of what is this rationality thing and why
should I care.
So I thought, okay, if my grandma were to ask me, what is this rationality thing, what
would I tell her?
And this is what I came up with more or less.
Oh, and also you don't have to read the sequences of Balog posts on the website if you don't
want to.
They've also been collected into a book in which is a much more structured format called
Rationality from AI to Zombies.
And we'll put a link to that up too.
Yeah.
Indeed.
Fantastic.
It's available on Amazon, I think, for a minimum payment of $0.00, but it's a just a donation
of five bucks.
Yeah.
So what is rationality?
What would I say to my grandma?
I would start off with two axioms.
The first being that reality exists and that it's possible to know things about reality.
And we can have our own series of episodes about those questions if we want.
But I thought that generally, grandmas agree with those two things.
You don't have to argue them into agreeing that reality exists and it's possible to know
things about them.
So I'm just taking those as axiomatic at the beginning.
I have never met a solipsistic grandmother.
Okay, I like the grandmas, you've met them.
So the primary thing about rationality in my mind is the map versus territory distinction,
which is a aspect of epistemic rationality.
And to me, that's really what rationality comes down to.
So I'm going to go into that right now.
I would say to my grandma, grandma, you know how if you have a map of New York, that is
a really good tool to have.
And the closer your map of New York is to the actual territory of New York, the more
power you have to do things.
You can plan your actions, you can decide, well, if you're a city planner, you can decide
what buildings are going to go.
But the better the better a map you have of New York, the better your ability to interact
with the city of New York.
But they're two very distinct things.
The map can be wrong.
The territory New York itself is never right or wrong.
Just is.
It's there.
So in this analogy, reality is the territory.
Reality is New York itself.
And our beliefs are the map.
And that's not even completely accurate, because it's not just our beliefs that are the map.
It's what we view reality as that's the map, like everything that I think is the map.
It's not just like, oh, I believe this and this.
It's the fact that I see Stephen here in front of me as part of my map of this room.
And hopefully part of the territory.
And hopefully part of the territory.
What would you say to your grandma if she answered, well, human beings are all so different
and we all have a different experience of what reality is?
And you were going to say something?
I was going to jokingly say, can you do that in an old lady voice?
One moment.
No, you didn't really.
What if, no, no, I'm going to try.
What if she said, Emy Ash, honey.
Oh, God.
What about how different human beings are from one another and how we all have a different
idea of what reality is and the way we perceive things with our unique brains is pretty different?
And I would say, yes, that's part of the problem that we all have slightly different maps,
but there's still only one reality out there.
And the closer we can get our maps to match the reality, not only will we have a better
ability to handle reality, but we will be closer to each other as well.
Because if both our maps correspond more correctly to reality, they will also correspond more
to each other.
I apologize to all the senior women out there.
We have lost our entire senior audience.
Oh, God.
And I apologized to everybody else for making that recommendation.
I'm not sure how it was kind of creepy to me because I laughed and cringed.
No, I mean, like as a general old lady voice, it was great, but like I saw my own grandma
and she doesn't sound like that.
And so I had this weird uncanny valley.
I don't think many grandmas actually sound like that.
I don't know.
I've not met very many grandmas, honestly.
But stereotypical Koreanis do sound like that.
Cartoon grandmas.
Cartoon grandmas.
I think of stereotypical grandmas.
I think of Betty White.
Oh.
Yeah, she's kind of badass.
That's pretty good.
Yeah.
That's good for grandmas.
Don't live in sliced bread, famously.
Seriously?
Sliced bread came on the market in like 1927 and she was older than that.
Yeah.
Okay.
Fantastic.
So part of this distinction is that your map, I mean, the territory is what it is.
But your map, if you know ahead of time that it is not exactly the territory, it is just
your representation of the territory, you don't have to believe it is completely accurate.
You can have various degrees of belief.
You're like, I know this part of the map is really good and spot on.
It's got every little stare and crack.
But this part of the episode of Fuzzy, I know there's some kind of building here, but there
may be dragons, you know.
And so that is the whole degrees of belief thing, that you can be very confident of some
things like the sun will rise tomorrow morning and be very anti-confident of other things
like there is no such thing as supernatural magic.
But you can also accept things with some degree of probability that's less than that.
And you also know that despite how extremely confident you are that the sun will rise tomorrow,
if it doesn't rise tomorrow, that means there was a flaw in your map, and you shouldn't
have been quite that confident.
There is no flaw in reality.
Inyash, honey, cupcake, I'm your grandmother.
You can do the grandma voice.
I don't want to anymore.
No, you should, because then if you call me cupcake in your normal voice, it's like you're
flirting with me.
Okay, cupcake.
I'm like, that is distracting right now.
Cupcake.
That's distracting.
That's another question for you.
Please, grandma.
How big your eyes are.
If we all have different perceptions of reality, how do we get close to what exists in the
territory?
You're skipping ahead slightly.
Sorry.
It's okay, but I can go ahead and skip to that if you would like.
The part of rationality is the process of making sure that reality changes your beliefs,
that your beliefs are entangled with reality, and you're not trying to force your beliefs
onto reality.
You are letting them be manhandled and mauled and destroyed in any way that reality needs
to destroy them so that reality is affecting what you believe.
This includes things like knowing what standards apply to evidence and just being familiar
with the scientific method in general, accepting critiques that if you hear a critique of your
position from someone else, you don't dismiss it out of hand because it's not what you believe.
You take it into consideration and especially if they have good evidence for their side.
A litany I really like is what is true is already so and acknowledging it doesn't make
it worse.
So if you really, really don't want to go to the doctor because that mole is, you don't
want to know that it's cancer, the thing is it either is or isn't cancer already.
And if you find out that it's cancer, that doesn't make anything worse in reality.
All it does is make your map more accurate.
So never shy away from knowledge that may hurt because it can only help you.
And in my opinion, these are the sorts of things that people who have a like internal
desire, like a real need to know the actual truth will come to eventually when I, I got
to thank my parents for their religious upbringing because in my religion, it was very big about
the religion is the truth and the truth is really important and we must spread the truth
to people.
And so I was deeply ingrained with this love and desire to have the truth all the time.
And eventually you start asking yourself things like, how do I know that what I think is the
truth is actually the truth, you know, the fundamental question of rationality?
How do I know what I know?
Why do I believe what I believe?
And anyone who cares enough about the truth will eventually ask themselves that question.
And then they will, in my opinion anyway, then they will start seeking out methods to
verify what they believe.
And they will eventually stumble across the scientific method or, or rationality or other
similar methods of finding out what is true based on empirical research.
So that that's a big part of it is to make sure your map accurately flex the territory
by using these tools.
Now an important part of this, I actually had this earlier, but this works out really
well.
An important part of having a good map is being able to update your beliefs when you do run
into this, when you do run into new evidence, which is where Bayes comes in.
And I believe we mentioned in the last episode, a link to a good, intuitive, easy explanation
of Bayes theorem.
It's still not the easiest thing in the world, but it is the concept that it's how to integrate
new evidence that you find into your current beliefs to update them in a direction that
puts you closer to what reality actually is, puts your map closer to the territory.
And one example that I got, which I really like is that Mormons are told that when they
read the Book of Mormon, Mormon, they will know that it is the truth because they will
get this warm, glowy feeling in their heart.
And the question, the Bayesian question of that is, what is the, if you do read the Book
of Mormon, what is the likelihood that you will get a warm, glowy feeling in your heart
if there is a God versus what is the likelihood of getting the warm feeling, glowy feeling
in your heart if there is no God?
If there's still a lot of things that could cause the warm, glowy feeling upon reading
that are not to the God, then that's not very good evidence.
If you know, if it's written very poetically and it is inspiring to you and there's cultural
baggage behind it and you've been told this will make you feel warm and glowy, there's
psychologically a good possibility you will feel warm and glowy even if there is no God
in our universe.
So it's not great evidence.
Can I interject there?
Sure.
I've read, I tried to read part of the Book of Mormon because I picked it up from a hotel
and I can't, I personally can never, cannot imagine feeling warm and glowy while reading
it or even being able to get past the first page.
Thank you.
I was going to say something similar.
I have a copy out there and I was going to mention how it's written in terrible, like
fake sounding 17th century English and you know, it was written 150 years after people
talked or anything like that, but according to you non-believers.
Well, so the other thing to keep in mind is that I think that there are other examples
of introductory bays and what you mentioned was more akin to I think the conjunction fallacy,
right?
So you feel, well, it's basically a formalization of extraordinary claims need extraordinary
evidence.
Yeah.
Okay.
Fair enough.
It's not time for conjunction fallacy yet.
Yeah.
And this goes back to the actually the UFO theory that I think two episodes ago, not the UFO
theory, but my UFO experience a few episodes ago, one of the things that I thought when
I saw that light and had this overwhelming feeling that aliens were coming to get me
is, is there other things that can explain this feeling as well or better than aliens
actually coming?
I suppose nothing would explain it better than aliens actually coming.
But are there other explanations as well?
And yes, there were lots of other explanations.
Therefore simply that feeling and that light was really shitty evidence that aliens were
actually behind me.
So it's important to keep in mind that something did explain it better than aliens actually
being it because it turned out not to be that.
Right.
So whatever turned out to be actually did explain it better.
Yeah.
Yeah.
And then if you do have the alien hypothesis, there's all these other questions like why
are they visiting Podunk me in, you know, a suburb of Denver as opposed to going to
the president?
Why are they even landing on our planet as opposed to using it for whatever they need?
Yeah.
Yeah.
It's a stupid hypothesis.
It's a large hypothesis.
Yeah.
Isn't sufficiently answered by saying, well, they wanted my cattle or to ruin my crops
or something.
Whatever does the aliens have to do?
Yeah.
So you have degrees of belief in your map.
You update your map based on evidence.
You make sure that you have good evidence and you know what constitutes good evidence.
Next on the list here is it puts a rationality puts a lot of emphasis on knowing what flaws
you have in thinking or knowing what flaws are common and correcting for them.
The analogy for this is the analogy that I like anyway is that if you have a camera
and you look through, you know, you look through the camera through the lens and there's a
spot where there's a building that's like really wonky.
It's like the lines are everywhere and you're like, wow, that is a crazy ass building over
there.
And you just assume there's this weird building in your field of vision.
But eventually you may come to the conclusion that it's not a weird building.
There's a chip in your lens right at that spot.
And it's something that you don't know at first, but you can find the chips in the
lens that you look through if you put enough effort into it.
And once you know there's a chip there, you can adjust for that.
Cupcake?
Yes.
Cupcake?
Are you talking about biases?
I actually am.
Yes.
Oh.
And I thought how to overcome those biases.
Excellent.
Yes.
So you know, if you can, you look at it from other angles as well, or you ask other people
what they see, or you simply know that there is a flaw there.
And so whatever is behind that flaw cannot be trusted as entirely reliable.
And I think a wonderful, very top timely example of that is politics.
Humans have very ingrained emotional reasons to take sides in politics and to defend their
sides very strongly regardless of how true they may be.
So the classic term is politics is the mind killer.
Whenever politics gets brought into the discussion, you have to be aware that you may not be seeing
things clearly.
Emotion is likely to rule because of evolutionary reasons.
So be very careful at that point.
Apply your tools more than you would in other cases because it is a mind field for thinking.
Be very vigilant in that field.
One way to help overcome that is if you happen to be a friend with anyone who's historically
savvy, you can discuss whatever current topic that you're having a deliberation about in
the context of, you know, whatever, 1700 France.
And then that way that it puts some distance between you and the topic rather than it being
part of your identity like people's current politics are.
It also often helps to have friends who you know are smart and you respect that are on
the opposite side of you.
I do have a few Republican friends and that's that's the whole getting a picture of the
same area from a different camera.
It's you know, you can see if they can explain it well and you can listen to them long enough
to get their explanation.
You can see some other parts of reality that you would not have seen otherwise.
If you haven't had a conversation with somebody that you disagree with politically that you
acknowledge as otherwise smart, you owe it to yourself.
It's those are some of the most rewarding discussions I've ever had are with people
that are on the other side of political topics than I am.
And if you can both keep calm and and share your ideas and defend them, the consensus
from these people and myself when we come away from these conversations is that we both
feel better and smarter.
So talk and learn and grow.
It's great.
The one one example that I like a lot is orchestra recruiting.
Is it recruiting hiring?
When people hire players for an orchestra out when, you know, someone else retires and position
opens up, they used to just watch them play and then decide from that.
But the theory is that they want whoever plays the best.
And they came to realize that there are some biases biases in their choosing, which they
weren't.
I mean, as much as they were like, no, I'm not sexist, I'm not racist.
As much as they were confident they were making decisions just based on what they were listening
to, the hiring data didn't necessarily bear that out.
So they blinded themselves by putting up a screen, which helped things a bit, but they
found that they also had to put down padded carpeting to go behind the screen.
Because if they heard someone in heels walking to it, the hiring rate would go down just
because they heard heels.
It was an unconscious bias, not these people were sexist.
But once they put in the padded carpeting and the blinds, women started getting hired
at a much higher rate than they had been previously.
And that's a good way of when something's brought to your attention, you can take steps
to mitigate it.
Yes.
And if you rewrite your unconscious beliefs, since that's probably impossible, just taking
outward steps to fix it is a great way to adjust for solving that problem.
And I thought that that's a great example.
It is also, in my opinion, an example of where stated preferences diverge from actual preferences.
I suspect because everyone states all we want is the best player, like so the music sounds
the best.
And this helps them get that.
I suspect that's not all people actually want when they go to an orchestra.
They want to see performance, which I think why more attractive people get hired more
often, because that's part of the whole performance aspect, right?
I'd never been to an orchestra.
Oh, wait, yes, I have, but I maybe not close enough to see the people play.
I can't comment on that on personal experience.
My favorite example of that is there's piano competitions often where people, you know,
play piano and judges decide who's the best piano player.
Afterwards took the recordings of those performances and they played, there were five competitors.
They played them for a group that only heard the music and didn't see the video.
And they chose who played best.
And then they also played it to people who only saw the video and didn't hear any of
the music.
It was muted.
And they chose a different person who was best.
And without fail, the person that the visual people chose matched who the judges chose
that were, you know, supposed to judge you as the best player.
Huh.
Yeah, because when you see someone play, what you really want to see is you want to feel
something and the music helps you feel something as what music does.
But also part of that is the performance, the passion, the banging on the keys, the
hair flying as you're going.
So if you, the visual aspect turns out to be incredibly important.
I would not have guessed that I am not very music savvy, but I would have thought that
it would be more how it sounds, not how it looks because it's music.
Well, think about food.
Think about your enjoyment of food.
It has a lot to do with how the food looks.
That's true.
I have seen the pen and teller bullshit episode where they dressed up like salads from McDonald's
and charged 30 bucks for them and people liked them a lot more than they would otherwise.
So yeah, okay.
Well, that's that's different from the look.
That's more the perception of price and value.
Well, they also put it in a nicer bowl and not a plastic shell.
Yeah, yeah.
So a number of things can, can have an impact when you don't expect it.
Yeah.
But yes, you try to, you try to correct for your flaws, your biases.
So the way, the way I always thought of it is that for the, if I were to wrap up to my
grandmother, that formal, if she knows about formal logic, which I hope she does, because
that's taught in school nowadays, right?
Anyways, assuming it was.
Nowadays.
Grandma, you know what formal logic is, right?
Of course.
Excellent.
She spawns the best children.
My grandma grew up in a small city or a small town, rather, and she didn't have electricity
in her house till she was 13.
So I don't think she was taught her formal logic tables in primary school.
All right.
Well, I would say that formal logic is a systematized method for retaining truth value between statements.
It's generally what it's accepted to be.
And rationality is a systematized method of making your beliefs more closely match the
reality around you.
That is, that is primarily what rationality is to me.
That is the epistemic rationality.
There's also the second, what I consider secondary, which a lot more people consider
the primary part of rationality, but to me, it's slightly less important, is instrumental
rationality, or as the term is, rationality is the art of systematized winning of finding
out what you want and then using the skills of rationality to get better at achieving
your goals, then includes things like getting the cooperation of other agents, knowing what
the payoff matrices are, and just knowing decision theory in general.
How to use rationality to come to a better decision.
And I think my favorite current example of this, which I don't know if you guys will
agree with me, is Pharma Bro.
Martin Schrelle, is that your name?
Shkreli?
Shkreli?
Yeah, Martin Shkreli.
Something.
Age drugs that only his company can produce and jack the price up from $15 a pill to $700
a pill?
Well, that was the whole basis of his business.
Yes.
Yes.
The thing is, only his company makes it and people don't want to die.
And this pill is what keeps them alive.
So he's like, hey, it's worth $700 a pill to not die, right?
So yeah, and everyone hates him right now.
He's been called in front of Congress.
But it is, it is my contention that if you wanted to lower prescription drug prices in
the country, because right now there is prescription drug prices are pretty fucking crazy.
And there's some systemic reasons behind this, which I won't get into because I'm
sure just having a brief conversation with my grandma.
But if you wanted to lower prescription drug prices, his current actions may be the most
rational course to take.
You play the villain and you exploit the fuck out of the system so badly that no one
can ignore it.
And they hate it and they finally get some motivation to change the fucking system because
all he's doing is playing within the rules that the system has set up.
So I pictured him as being a really bad, bad guy as in not being good at being a bad guy.
And no, he's terrible at being a bad guy.
Well, now I'm really hoping that like in two years, he comes out with some timestamped
footage of him saying that was his intent the whole time, was to, was to shout out,
I know he won't, but wouldn't that be, wouldn't that be a great cartoony resolution?
Yes, it would.
But I, this is, this is the same reason I loved Professor Quirrell, because he, you
know, was, he was trying to unite magical Britain against the next dark lord.
And you know, he was doing it in a bad way, but no, it's sometimes if the system is fucked
up and the only way to fix it is to unite people in opposition of a cartoony villain,
you become the cartoony villain to fix the system.
And sure, your own reputation sucks and people hate you, but your, if your goal was to fix
the system, that is the rational pursuit of that goal.
Cupcake.
Yes.
It sounds like you might be going to that, to that crazy conclusion because it's exciting
and fun and not because it actually has anything to do with the truth and reality.
I know.
But it's a nice fictional.
So what are we supposed to be focusing on as rationalists?
Reality itself.
Oh, but that is my example of a thing where instrumental rationality might tell you to
do things which you would not normally think is a good idea in the pursuit of your goals.
A counterintuitive solution to a problem.
I probably wouldn't get more into it than that because, you know, there's a lot to
systematize winning and to me, it's, it's the epistemic rationality is the more important
part of it.
I would also tell my grandma, because I do think this is relevant, that there tends to
be some cultural trends in rationalist circles due to partly due to founder effects and partly
due to the conclusions rationality brings you and partly due to the fact that it is
kind of a youth movement.
That'd be one thing I would tell her right off the bat.
It's sort of a youth movement.
And so it has a younger culture.
Rationists generally subscribe to utilitarian ethics or a derivation thereof.
Wait a second.
Yes.
Are you saying that elderly people like me can't be rational?
No, I'm not saying that at all.
I'm just saying that it's a fairly new philosophy.
And so, like most such things, it, you know, grabs the fiery youth and, and slowly filters
its way throughout society.
I will say if you're about to make a list of things rationalists believe to choose your
words carefully.
No, I know.
I'm just saying in some of the cultural things behind it.
I agree.
And I think you're probably, I can imagine being too inflammatory, but I'm, I'm eager
to see the response.
Please go on.
If I were, if I were to clarify what you were talking about, it's more that you're talking
about a rationalist movement.
So a community of people who are interested in a certain kind of rationality and who are
talking to each other rather than rationality in general.
So what somebody on the street or grandma or maybe not a five year old, I think that
maybe we aimed a little bit too low on age and introduction, but what they might think
of as what is rationality?
Yes.
Because I think the rationality system itself is the main thing we're interested in.
But if my grandma wants to know what rationality is, these cultural things are part of that
as well.
I want to take a step to explain two things.
One, they explain like I'm five, comes from the subreddit name, explain like I'm five,
which isn't meant to talk to somebody like they're literally a five year old, but it's
to presume virtually no background knowledge.
And so to that, to that note, you mentioned that utilitarianism is popular among rationalists.
And utilitarianism is the ethical subscription that consequences are what matter and maximizing
the well-being of them, depending on which consequentialists you're talking to, but more
or less it's about maximizing as much happiness as among as many people as possible or as
many sentient beings.
And would you agree with me that that is much more common in the rationalist community than
it is in the world at large?
I would definitely agree that it's common in the rationalist community.
I'm not sure how uncommon it is in the world at large.
I don't think it's uncommon, but I think it's less common.
That could probably be true.
Yeah, I don't know that we have any evidence to back this up.
I will dig up one of Scott Alexander's survey results.
Sounds good.
Again, come to the website.
Look at our links.
We're trying to be as detailed as is practical with our episode description and links for
since it's sort of impractical to explain everything we talk about like like I'm five,
like you're five, we want to be as as detailed, as I said, as is practical in the descriptions.
We don't want to give you two hours of reading every two weeks, but it's there if you want it.
I think the culture also is a bit economically libertarian ish and that in it's generally
more sex positive and poly friendly than the rest of the world.
I was going to say that was the first thing I noticed when meeting more rationalists.
For listeners, I want to explain what Inyash means by poly.
He's referring to polyamory, not poly nucleated or many pointed squares.
Yeah, not polygons, although we like them too.
So polyamory is simply people who have multiple partners, multiple romantic partners.
I want to dive this into a whole episode.
We totally should.
I had a friend call me a couple weeks ago and in four columns where I'm from,
this really wasn't a thing, which is weird because it's not a small town.
There's 130 something thousand people, but I don't think I knew anybody who knew anybody
who was into the poly community and a friend of mine who lives there called me a couple weeks ago
and he was like, so are you familiar at all with this thing?
And I was like, actually, yeah, as it turns out, it's kind of popular down here.
And, you know, we recorded at a Denver.
I'm not sure if we've mentioned that.
And, you know, it's not a far drive, but the fact that there's this kind of big gap
between that relationship schema that groups have.
But anyway, so I was trying to explain it to him to the best of my knowledge.
And I think, anyway, it occurred to me at the time that we were doing it.
It's ripe for a long discussion.
Yeah, I do also want to say that you're speaking as a mono person.
Yeah.
Well, I think that there's been a bigger, that people have been embracing polyamory and open
ethical nonmonogamy more so over the years.
It has always existed in Fort Collins.
A variable may have.
I guess what I'm getting at is that it was hardly out there, you know, so.
And to be perfectly clear, I'm speaking as a fan of monogamy, but not a not someone who's
disfavorable towards any other preferences that people have.
Right.
So, you know, this isn't coming from, oh, people in my group weren't like that.
We were very careful to have our token monogamist on the podcast.
I guess.
Yeah.
What?
So, Inesh, why are you telling your grandma about why rationalists tend to be more
more fiscally conservative and.
Did she ask about your sex life?
No.
Yeah, what's up with that?
I don't know.
It's just part of the culture.
Like the another one is that we're very a religious generally, not necessarily anti-religious,
but just religion is silly.
And it's not a thing we bother with anymore.
I want to just different or I want to distinguish that from how the skeptic community treats
religion.
And I notice a lot of the mean word is circle jerk.
The polite the polite word is patting oneself on one's back of, yeah, I shook off my religion,
you know, man, really.
And so there's a lot of fun and I went to that phase.
It is a lot of fun to argue religion with people.
Oh, God, I went through a several year phase.
Yeah.
And but then it gets really dry.
Right.
But the thing is, is that I don't meet a lot of rationalists who congratulate themselves
on shaking their faith.
And I think I saw a talk with it might have been Eleazar Yudkowski.
I forget who it was, but it was somebody, one of the major figures, probably Eleazar.
And he was like, it's a lot like answering the werewolf question.
All right, cool.
Werewolves aren't a thing.
Let's not spend a lot of time patting ourselves in the back for getting that question right.
Like it's really if you shake off all of the baggage that comes with it and all the indoctrination,
it's really a really easy problem.
And all of these, I just want to say, are also things that are not like hard rules.
There are religious rationalists.
It's just more prevalent in the rationalist community than in the general populace,
which is why I bring it up.
For sure.
Yeah.
And if anyone who like if you are the religious rationalist, you're the guy that sticks out.
Right.
People are like, that's kind of weird, but okay.
That's that come comment.
We're not going to bite your head off.
No one's rude here.
That is the other thing as long as.
Try not to be rude.
I think that that's.
That actually is a pretty big one too.
Try not to be rude.
That was one of the things I was going to mention that.
And so, Ines, are you officially wrapped up or almost wrapped up?
I have the one last one.
Please go ahead.
Okay.
I was going to say that all those other ones are just general tendencies,
but I think this last one is actually kind of a cultural norm.
Like it is actually a thing that we try to encourage this last one that I'm about to talk
that a social norm is that no ideas are taboo.
And as such, people aren't shunned for advancing arguments on reprehensible subjects.
And I don't mean that there's no ideas that we don't think are terrible or reprehensible,
but there's nothing that we can't talk about.
Like the joke form of this is you make a compelling case for infanticide.
But the more serious case is that racists, for example,
are allowed to present their arguments.
And in the general populace, if someone started coming in saying racist things,
it'd be like, fuck you, you're racist.
We don't care.
Get out of here.
We're not going to tolerate this.
Whereas oftentimes in the nationalist community, you'd be like,
okay, what you're saying sounds kind of racist, but give us your evidence.
If you can back it up, we'll hear you out.
Right.
And then we will argue with your evidence as to why we don't accept it
or why we think it's wrong, because that's important.
Or equally important.
Accept it if you will.
We might change our minds if your evidence is that compelling.
Yeah.
Bring that evidence to us if you want.
I'd be curious.
This isn't necessarily an invitation for racists to bring their evidence to us right now.
No, it is not.
Unless you can do so politely.
But yeah, please, yeah, scratch that.
Honestly, I probably wouldn't bother to engage it because I have too much on my plate already.
But there are some people that will.
Would depend on the claim for me.
I expect, you know, if they're going to.
So it wasn't evidence.
They bring was it Francis Crick, one of the Crick or Watson, Crick or Watson,
whichever one it was, got a bunch of flak a few years ago for suggesting the possibility
that native Africans might be genetically inferior to other subsets of the human species.
I made a joke about that genetically inferior.
Let's say have a bell curve that is slightly to the left on IQ spectrum.
Is that what his claim was?
Yes, that's what his claim.
Oh, but but he did he claim it was genetically based.
He did.
So that's what I meant.
Yes, to qualify.
But the thing is, is that I would hear him say that and I'd say,
can you show me your test results?
Rather than be like, oh my God, I can't believe you'd say that because that's a strong claim.
That's terrible.
But it's either true or false.
I think it's probably false.
But let's let's see what he has to say.
If it is true, then, you know, it's already true.
And knowing that doesn't make things worse.
Exactly. If anything, knowing that is the only way to make things better.
Yeah, that's the whole thing.
That's the takeaway.
And there's there's also, you know, does this actually have any implications on the real world?
Because you still can't judge individual people by what a general, you know, racial profile might be.
Yeah, bell curve anomalies exist.
And that whole thing is not really productive anywhere.
Unless we're going to talk about eugenics.
So again, that's a whole episode.
Yeah. Well, it's all it's one of those fun topics when I try to explain to people, you know, at work or whatever.
Hey, so, you know, what is this whole rationality thing about?
What do you guys get together and talk about?
Don't tell.
No, no, I'm just going to say that in general, it's the kind of thing where you can get together
and raise any topic, however contentious, even sometimes deliberately inflammatory,
and have it be a jumping off point, and no one's going to crucify you
for saying whatever it is that you want to you want to bring up.
So I'd say it's a place where it's okay to be wrong.
Yeah, yeah.
It's okay to be wrong, and it is a moral good to be less wrong.
Yeah.
Aha.
And the only way you can get to that point is if you explore the places where you are wrong.
If you just wall them off and say, we're never going to talk about them, you don't make your map any clearer.
Yeah.
In fact, I would add that when I talk to people about rationality, I talk about
how it's a great thing to try to disprove yourself.
Yes.
And that's a good rationality tool.
And it's an important one in your toolkit is to try to prove the other options.
I will say it's been a hard one for me personally to put into practice often.
My brain does not typically go to how can I prove myself wrong?
I mean, I make an effort to sometimes, but it is an effort.
I think it, I don't think there's a person who's natively built to flinch into looking
towards the what if I'm wrong question, right?
There might be some.
I'm imagining the person on the Savannah, just petrified in self-doubt about whether or not
that rattling of the bushes was a tiger or not.
The one who, well, that's not the best example.
But I can't imagine strong self-doubt being a selected feature.
But it's one that we can now select for ourselves.
Ideally, a good scientist would want to try to prove what they call the null hypothesis,
but look at other options, see how reasonable they are instead of trying to prove themselves
right.
And again, that's a whole other conversation about publication bias and the file drawer effect,
but why that doesn't actually happen in science.
But it's, again, a moral good.
So Stephen, my grandma knows a lot about rationality now.
What are you going to tell your grandma about rationality?
I would tell my grandma to listen to your explanation.
It was quite comprehensive and covered almost everything I was going to say.
So we don't rehearse these beforehand.
We don't.
We will sometimes share notes, but not every time.
So part of it I already talked about being able to discuss any idea fairly and honestly,
and whether that's someone else's idea or your idea.
And this section is something that you did miss that I think is more integral than any of the
utilitarian or whatever.
The celebration of changing your mind.
You don't typically get applause.
But the if you look at any politician, oh, they voted for this in 1992.
But now they think the other thing that they that's seen as a bad thing in the
rationalist community that's seen as a good thing.
It means that you're amenable to changing your mind.
A good deal of effort is put into making yourself open to revising your beliefs and
possibly changing them completely.
There are some fun techniques that people work on.
Like this is an important part of the project.
So people give a lot of thought on how to make that happen more fluidly for you, etc.
So it's a fairly key feature.
Yes.
Since I joined the rationalist, since I learned about less wrong and the rationalist
movement, I have changed my mind on incest.
So a great technique is to find things that really are icky to you or upsetting to you
or carry a massive social taboo that you buy into and taking the time to deconstruct that
and and figure out what the actual what the truth is.
A lot of these are incest.
That's more of a should statement than a factual statement.
But as a rationalist, I can change my mind about should statements to be a better person,
a kinder person and a more accepting person.
I think should statements are just as important because a lot of it is what we want to know
is what we should do, right?
And what the way things actually are greatly impacts that.
But that's one of the reasons we're figuring out what things are so that we know what we should
and shouldn't condemn or promote.
Absolutely.
So that's a good example of finding a good place if you want to get some practice.
I think if you've already lost your religion or if that's not something that you want to start
first with or ever.
I'm not going to say you have to be an atheist, but find something that you find just repellent
bestiality as long as we're on the sex scene.
I'm still working on that one.
I'm still working on that one.
That's tough.
My general consensus on it now is as long as you're not hurting the animal,
at the very least you could probably do something better with your time.
But I'm not going to go out there and tell people.
I'm not really going to comment on it, but I will say I wanted to say the biggest thing
that I changed my mind on away from the sex thing, at least away from the animal and sibling
fucking the biggest thing I changed my mind on.
And it was fairly early in my introduction into the rationalist community was death.
I lost my religion at some point in my I don't know if I ever really had it.
We touched on this earlier in a previous episode, but there were a few years where I
went through and I'm like, I'm going to die and the world's going to be, you know, my god.
And, you know, I found some solace in the whole Mark Twain quote about, you know, like,
oh yeah, but you were dead for millions of years before you were alive and, you know,
be back there.
It's like, I won't be freaked out, but like, I still don't want to be there.
I'm glad that I'm not in that pre-birth state.
I don't want to return to that when I die.
And in one of my favorite books called Unweaving the Rainbow by Richard Dawkins,
he has this great poetic analysis of what it's like to live and die.
And he talks about how we're going to die and that makes us the lucky ones because the number
of potential people so vastly outnumbered the number of actual people and the fact that you're
alive right now is so statistically improbable that it's like a miracle.
Get out there and enjoy it.
I found a lot of solace in that and it is.
But then I, so this is kind of the opposite.
This is related to looking at a yuck reaction, looking at something that terrifies the hell
out of you.
So when I finally got around to re-examining that, I said, no, Dawkins, that's great.
And if this was 100 years ago, I'd be on board with that.
But I can't resign to just die if there's any way around it.
So after some deliberation, I signed up for cryonics.
I make an effort to, I mean, I just never like, I guess, what am I trying to say?
You changed how you lived your life.
A bit, yeah.
In accordance to a change in your opinion.
Yeah, I think so.
I fully hope to live to be as long, as old as I want.
Let's put it that way.
So yeah.
So you did some soul searching.
You looked at evidence.
You looked at other thoughts about death and dying that were out there.
And you changed your mind and that's an okay thing to do.
For sure.
And I think that's the, you know, and if I change it back, that's great too.
But this is one topic where there is some, there's not a strong consensus.
I think that in the rationalist community, at least that I've seen, maybe there's some,
some data on Scott's blog, but that's something that I feel like not a lot, not enough people.
They're, I mean, they're, if we were talking about death, there isn't a strong consensus,
but they're, it's still much more, we're still much more anti-death than the general population.
Oh, yeah, for sure.
Then higher than the base rate, but I've been, while I'm imagining it's something like 80%
utilitarian, I'm not sure what percentage are, are anti-deathists.
All right, we both said something.
What did you put something?
You were talking about how there are a lot of people who are very interested in longevity.
And I think in the rationality movement, and I think that goes along with people who are
interested in superintelligence and artificial intelligence.
Those are also more, more common concerns within the rationalist movement too.
Transhumanism in general.
Yeah.
And transhumanism is basically the thesis that it's okay to have aspirations beyond
what biology gave you as far as for your own physical well-being.
And I did explain this to my grandma once.
Yeah.
I said, grandma, we're both wearing glasses.
Neither of us are content with what nature gave us.
Now, I think that's kind of a cheap shot for transhumanism because it's not,
or it's a cheap, it's a cheap point rather.
It's a good point.
It is, but it's not the same as like brain augmentation or, you know, synth body upload,
whatever, right?
So it, it, but it generated, it illustrates the general concept.
It does.
Cheap and easy are different things.
Yeah.
Fair enough.
Yeah.
So what's something you've changed your mind on?
Wait, before we do that, did you have anything else on your list?
Unrelated, but I was going to give another example of, of Bayesian updating.
There's, there's the traditional example that I really like.
And if you also have any of this.
What's the traditional example?
Well, if any of the, the talk of Bayesian probability or probability theory in general
is fun to you, check out Leonard Mladenov's book, The Drunkard's Walk,
How Randomness Effects Our Lives.
That's my book pitch for the day.
The, one of the, one of the classic examples for just illustrating what Bayesian updating
is to somebody is say you're approached by a mathematician pushing a stroller with
two swaddled babies in it and you can't tell what sex they are.
And you ask, is at least one of your children a girl?
And the mathematician says, yes.
So then you can ask yourself, what is the probability that they're both girls?
And the point to illustrate here is that before asking that question,
the probability that both of those babies are girls is one in four.
Because you have boy boy, boy girl, girl boy and girl girl as the four options.
After confirming that one of them is a girl, you knock off that boy boy option.
So now you're not the chances that they're both girls is one in three.
In other words, you've been given more information so you can update your probabilities.
Exactly. And there's, there's an exact amount to which you should update your, your belief
by based off of the, just the number of possibilities that there were.
So that's, that's one of my favorite examples because it illustrates updating
and it's really simple fractions.
So one of the ones I like that's just very simple and that I don't know
a decent percentage of people don't get is if you flip a coin 10 times
and the first nine times it comes up heads, what is the probability that it's going to
come up tails on the 10th flip? 50 50.
Right. But a lot of people will say it's a much greater probability that it will,
that it will come up tails because it's come up so many times it's due for it to be tails.
And that, you know, it's incorrect because you have more information now.
But if you would have asked before the flip started, what are the chances that you'd get
nine heads in one tail, you'd say extremely low.
But if you've already flipped the coin nine times, the two options that are left are heads
or tails. So it's a 50 50 shot.
Well, that said, with nine out of 10, it'd be less strange to me.
But if it was 999 times out of 1,000, I would say we have an unfair coin.
Right. Yes.
And so then then you can use your your Bayesian updating to say, look,
the fact that it had had so many times, it is not one in two.
I'm willing to bet more than I would bet whatever I'm willing to bet.
Yeah, that you're cheating.
Yeah, or that or that for whatever reason.
It's going to be heads.
Exactly.
So.
So, yeah.
Yeah, but I like that one because it's a demonstration of the fact that probability
is in your probability is in the mind, not in the reality.
Probability is an aspect of the map, not of the territory.
Like if you roll the dice and you cover it with a cup and you ask someone what is the
chance that it's a six, whatever it actually is, it's 100% of what it is.
Right.
In the reality, it's either 100% of six or 0% of six.
But what the probability is that it's a six is one out of six,
because you don't know what it is.
So you have to give equal probability to all options.
Right.
So then that's exactly as probability exists in the mind, not in the territory.
It is six or not in your prior estimate.
So when people talk about Bayesian statistics, they'll often use the word prior.
That's just the the word that they use for initial estimate.
You know, if I pull out a quarter and say heads or tails,
your initial estimate is going to be 5050 after 1000 throws.
And it's if we're getting some weird results, you can then your new estimate,
your posterior probability is going to be your updated estimate.
Excellent.
Any fun things you've changed your mind on?
We never asked.
You gave the the incest example.
That's why we never ask Katrina what she would say to her grandmother.
Oh, I'm so sorry.
Yeah.
Well, again, yeah, she's covered some great things that one should say to one's grandmother,
I love the map and territory.
That's a it's really good getting into what reality is the idea that we believe in an objective
reality one that exists no matter what's going on in your head.
And so you covered a lot of great points.
I think I might in addition tell my grandma that rationality, the rationality movement
is about living your life as a scientist and applying what you do as a scientist and and
how you look at data and how you look at evidence to everything.
I'm curious, being curious and wanting, you know, wanting to learn and wanting to know.
And this doesn't mean getting a PhD and running a lab experiment.
I like Carl Sagan had a great quote.
Whenever you check your belief against reality, you're doing science.
And by that by that standard, I completely agree that anytime that it's not enough just to have
your little belief in your head and just leave it there unchecked and believe in it strongly.
If you can test it, go out and look and see if you're right.
And if you're not, you should want to change your mind.
That's that's another key component to rationality is not just having a map territory
distinction and having mental models, but wanting those models to be accurate.
Katrina, as your grandfather, I'm worried about this rationality thing.
Why don't you believe in things like love anymore?
Why do you think it's so irrational to feel emotions?
You really do just have the one old guy voice.
I don't at all.
I just want my emotions.
I want my emotions to be more in line with reality and what I see as, you know,
as reality in the best course forward.
But doesn't Spock always say that emotions are highly illogical?
I think actually you're bringing up a really good point, Grandpa.
Because a lot of times emotions steer us wrong.
It's part of that human bias that we were talking about earlier.
But emotions are also wonderful and a part of the human experience.
I would say that yes, it's true that Spock does say that, but that doesn't make it right.
Spock is not your your go-to rationalist guy.
And I was actually just explaining this to somebody today about,
and I think the there's a lot of wrong post on this as there is on a lot of things called
feeling rational or feeling rationally.
And I'm not sure if this is his wording or not.
I don't think so.
But the way I explained it today was that it's not about not having feelings.
It's about having your feelings line up with what's actually happening
and to an appropriate degree, you know.
So if I'm trying to think, I had a high school,
my high school biology teacher told me about at the time that his wife
woke up from a dream where he was being something doing something mean to her
and woke up mad at him and she stayed that way for like an hour.
And I thought that was anywhere both, you know, laughing about it.
This, you know, it wasn't a terrible thing for him.
But that was funny because that was absolutely not feeling rationally.
She woke up, knew it was a dream, but she still felt pissed.
And, you know, it's one thing to shake that, you know, to have that and then shake it off
a few minutes later.
But it's another thing to like stay mad for an hour after having a bad dream.
Yeah, I think that having having rationality and having rationalist tools in my life makes it easier
for me to feel the way that I feel like I should feel about things.
To reflect on things if I'm having an outsized emotional reaction
or if I'm having, honestly, the wrong emotional reaction given the circumstance.
Actually, I think it would be a great idea to do a whole episode on emotions at some point,
both emotions and the whole Straub-Vulcan fallacy, which is what I was trying to
point out. A lot of times rationalists get the whole, well, I guess you can never feel
any emotions thing. And you're like, no, that's not what rationality is.
Yeah, the Straub-Vulcan is the TV tropes definition for the Hollywood rationalist.
And I think, yeah, I'll save what I was going to respond with for that episode,
because that is a big topic and it's fun.
And it's a popular misconception, but we'll save it for another day.
I don't want to be like Spock, even though Spock's pretty cool.
Yeah.
Am I the only one who didn't love Spock? Yes, I guess.
Spock was cool. He was also just a really cool character. He made for good stories.
For sure.
So even if he was a bit of a caricature, the fact that you got good stories out of that made you
like him having the character in the show.
Let me qualify. I never saw the show. I saw the new movies with the fun guy from Heroes.
And in fairness, original series was kind of hit in this. It wasn't nearly as good as the next
generation.
And which one is Spock in?
Original series.
Okay. My main exposure to Spock was from some clips that Julia Galev played during a
talk on the straw bulk and which we'll link to in this episode or the next one or both.
And there was one where he is on some planet and he, I don't know, some flower spores or
something getting his head and he falls in love. And then he realizes at the end that it was all
irrational and that he gets away from it and like, you can still have feelings, bro. Come on.
Love is great. Love is fun.
Yeah. I want to hear what you changed your mind on.
What did I change my mind on?
Yeah. So many things.
I've never been wrong about anything, Stephen.
Yeah. I still, I've never had to change my mind.
Oh, shit. What have I changed my mind on? I've lately been worried that I'm not,
this isn't something I've really changed my mind on, I guess, as much as it's been a worry that
I'm not feeling emotional enough about certain things.
Like, I just recently saw the John Oliver episode on abortion, which I think was or abortion laws.
Which I think was two episodes ago.
That was a good one.
Yeah, it was. And I, you know, I'm one of those people who believes that abortion should be
available on demand without apology in all cases. Like, I am far to the left and I don't care.
I think it's a goddamn right and it always should be. But when I watched that episode,
I felt like I should feel outraged and I recognized that me from six years ago would be
having an emotional fit. And I didn't really feel much of that anymore. And I don't know if I'm
like just, if I'm feeling outraged fatigue, or if I'm really insulated because now I'm finally
at a place in my life where I'm starting to reap the privileges of being, you know, a white male
in America. And so I'm not, I don't have that visceral feeling that laws impact my life a lot.
And I don't like that. I feel like I should have a greater emotional reaction to these sorts of
things. And yes, you should feel the way you feel. It's okay. It's okay to feel the way you feel.
It's okay to not feel strong emotions about something like if someone dies or if abortion
laws or if abortion laws are stopping women from getting the care that they need or, you know,
it's okay to not have that all the time. Also, I mean, they call that stuff outrage porn for a reason.
And I was going to say, as far as outrage fatigue, it is election season. I'm sure that
a lot of folks are experiencing outrage fatigue. But that said, I do sometimes I just cry. I want
to see if I understand exactly where you're coming from, because if you know, isn't driven
emotionally by this problem. And I didn't see the episode, I'm sure it was good. And I'm sure that
it was horrifying. If I'm reading this right, right? And no, it wasn't actually that horrifying.
It's just a bummer. Well, in so much as it was, people are having their lives ruined
for absolutely shitty reasons. So but the thing is, if you don't feel an emotional reaction to
that, there's very little other reason for you to get involved trying to help.
Well, there are other ways to elicit a better emotional reaction in Eniash. For example,
we could have a movie about a specific person who wasn't able to access a safe abortion. And so,
actually, they mentioned this on John Oliver. So she asked over the phone,
these are the things I have in my closet. What can I do? Because I'm not going to be able to
make it to get a safe abortion. So they told that story. But they didn't tell it in a way that was
emotionally compelling. And this is something else is our emotions don't necessarily have to do
that much with factual input. They have to do a lot with anecdotal input, right? So there are ways
to tweak your emotions. They could have added different music. You can do that for yourself if
you want to inspire yourself to action about things that you're passionate about. It's not like
hearing about something that that you're not okay with, it's not cool, doesn't need to whip you into
a fury every time. In fact, that can burn you out. And you can knowing what tweaks your emotions
can give you the power to to motivate yourself when you need to be motivated and rest when you
need to rest and make the difference that you want to make. I think I see we're coming from
better now. And I my my confusion was that you were saying that you shouldn't be swayed by everything
that comes across your path because I'd be exhausting. And that's that's I think that's
absolutely true if I understood you correctly. But I do worry that people don't have enough
emotional response to things that actually do matter. You know, so you you can show people
the numbers of you know, do whatever for me calculation you want for the the number of
things that could cause the world to end in the next century and people like oh, wow, one in four
or one in or three and four that sucks. Well, I'm gonna go back and you know, finish my whatever
I was working on people should care about that. So that's one of the reasons I try to get people
not to use hyperbolic language too much. Like I saw recently a thing about the Amazon Mechanical
Turk program, where people would do basically very simple tasks for very low pay could be
sometimes it ended up being as little as like four bucks an hour for things that are just
mindless and repetitive and no one wants to do them. But you get four bucks an hour for it, right?
But someone commented that is basically modern slavery. And I was like, dude,
okay, this is a shitty situation. But if you use the term slavery to describe it, what you're
really doing is making the actual slavery out there less horrific by comparison. People will
be like, oh, that's slavery. I guess slavery isn't all that bad then. Like save those words for the
things that actually are good examples of what slavery is and why we hate it. People who can't
choose not to do that. Yes. People who get absolutely nothing in exchange people. Yeah,
who don't get to make those decisions. So three year marketing background. Unfortunately, what
we're talking about is human biases here, what people respond emotionally to what people don't.
And if you are working for a cause, you have to have somebody you're working with who is good at
exploiting those emotional triggers. And unfortunately, that's all there is to it. And
it's kind of a sad reality that I found out early on that people will not donate money to the cause
that saves the most animals, they will instead donate money to the one animal where it's super
expensive to give it the surgery and also that animals from a privileged class of animals pet
dogs. So it's definitely something that's difficult to grapple with. But it's not just true of people
who don't know about all of these horrible things going on in the world. It's true of me.
It's true of me. I wanted to give it to that one dog so it could have the surgery knowing that my
money could do much better things saving many, many, many animals. So I guess part of being a
rationalist is discovering these biases and maybe in your own life realizing them,
but also realizing that they applied other people and there are ways that you need to work around
that. Anyway, grandma, I hope that you understand rationality the way that I use it a little bit
better after we had this discussion. I think that you didn't do yourself justice when you said you
couldn't do a good quick synopsis. If you threw that together in five days, I feel like that was
I haven't seen such a succinct crash course. So I think that was great.
Well, I mean, I threw it together in an hour, but it wasn't really a crash course. No one
knows how to do rationality any better. They just kind of know what I like about it.
That's true. This wasn't so much of a crash course and how to do rationality. This was a
crash course and what is rationality. Fair enough. I liked it. That was great. This was a lot of fun.
All right. Thank you for joining us again for another episode of The Bayesian Conspiracy.
You can see links to this episode at TheBayesianConspiracy.com.
Email us at BayesianConspiracyPodcast at gmail.com and subscribe to our podcast on iTunes.
And we do have a subreddit as well, The Bayesian Conspiracy. Check it out.
We'll see you again in two weeks. Thanks for watching. Bye.

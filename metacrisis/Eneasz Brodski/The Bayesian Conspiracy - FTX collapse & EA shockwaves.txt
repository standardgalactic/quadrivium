Hello listeners. While recording the next episode, David, Steven, and myself got to talking about the FTX collapse and its effect on the broader, effective altruism community.
Because it's talking about events that are still unfolding, we decided to break it out and release it early, so it can be available while this is still relevant.
The full episode will release on Wednesday as usual. Thanks.
The next thing is, I think that is happening actually right now. It is a story in development, so we are not up on whatever the latest thing is.
But have you heard about the whole FTX crypto crash that just happened?
Yes, the last I heard was that SPF was missing and possibly in hiding.
I believe he's actually still in the Bahamas. We should give people a quick rundown as to what we're talking about.
No, just let him go off that.
FTX is a cryptocurrency exchange, apparently one of the top three biggest cryptocurrency exchanges with Coinbase and Binance being the other two.
Sounds right.
Okay. SPF was the guy who founded it and ran it.
Sam Bankman Fried, for those of you who've been paying attention to the EA podcast circuit for the last year-ish.
Because his whole thing was he was worth, was it triple digit billions or double digit billions?
Double digit billions.
Anyway, in any case, this guy was an elite rich and he was like, oh, it's all going, almost all of it's going to go to effective charities.
Yes.
So that's all I knew about him.
Yeah.
All I heard about this crash or whatever is that there's a crash or whatever. That's the extent of my knowledge.
So the thing that happened is that he ran two companies FTX as the exchange and Alameda something.
I don't even know what Alameda did.
But a lot of people gave their money to FTX, like you would give your money to Robinhood or something to hold on to and do trading with it.
Then he took that money and gave it to Alameda, which is his other company.
When Alameda was in some huge financial trouble and he was like, oh, you know what, you just need to bail him out.
They can make that money back and then I'll reimburse FTX. No big deal.
This is grossly immoral, unethical, against a lot of laws, but just completely unethical to take other people's money and gamble with it.
And then if you win, you know, you reimburse them and no one ever knows.
And if you lose, oh, whoops, they're all fucked.
And what happened was he did lose all that money went away.
There are now thousands of people who have lost their life savings or a big chunk of their life savings.
I know that a lot of people that worked for FTX really drank the Kool-Aid and put their entire paycheck into FTX whenever they got it.
And they have no money now.
It was a huge financial fraud.
I believe the Toronto Teachers Union pension put almost a hundred million dollars in it and they don't have money now.
So FTX fucked a lot of people really hard to the tune of many billions of dollars have been stolen, Bernie Madoff off with.
The big thing about why we're talking about this is because this is strongly related to the EA and Rationalist community
because SBF was in the EA and Rationalist circles.
He was kind of a hero because he made all this money.
He right now is 30, so he's been making this money mainly in his late 20s.
He walked the walk.
He was quote unquote one of us.
He gave a fair chunk of money to effective altruism and had a future fund set up at FTX to give tens of billions more.
It was planned out.
There are a lot of charities whose funding is disappeared, including things for like a nuclear safety and pandemic preparedness.
And a lot of people who are in the charitable career tracks that suddenly have had their future financing disappeared on them,
which I mean, it's a risky take when you work in those sorts of industries, but it still sucks.
And I guess the big thing for us specifically why I brought this up.
I don't know if this was on the record, but people certainly got the impression that he believed things like it's okay to rob a bank if the cause is good enough.
It's possible.
Hey, he's a man of his word.
Yeah.
Okay, to commit wide scale fraud as long as the consequences outweigh it by a wide enough margin.
I was going to make a case.
I am sorry to keep interrupting.
Yeah, go ahead.
I think a case at the end of this, like, you know, this might not have been the worst move for him to make, even though it didn't work out.
Because like maybe it was only, you know, one in 10 that it wasn't going to work out.
And so if he did manage to double his impact, right?
If this went well, which I don't think it sounded like it was going to double his money or anything, but this could have had, you know, an expected positive outcome, right?
Remind me never to loan money to Steven, please.
Okay.
Thank you, Enya.
That wasn't how I work with my money.
My money just sits and checking, not doing fuck all.
Yeah.
A couple of my hot takes have been validated by this saga.
One is utilitarians are mostly just using utilitarianism to justify whatever they selfishly wanted to do.
And the other is don't trust people who are kind of weird looking.
I didn't think he was kind of weird looking.
He was kind of weird looking.
I think he looks charming.
He was also charming if you met him in person.
Okay.
All right.
So the thing you brought up is actually a thing a number of people have said, which is why I'm bringing this up.
That you shouldn't trust weird looking people.
No, the other thing about utilitarianism, it's mostly the same people that are always saying that the ones that like to shit on utilitarianism and like, oh, no, it's evil and bad and all effective altruists and long termists are actually evil people.
Because of the argument like Steven made that, oh, maybe it was a good idea.
After all, I mean, I think sometimes that might be the case.
I'm not going to, I'm not going to go into examples right now, but I think that's complete bullshit because everybody is saying basically this was awful and immoral and he's a horrible person.
Obviously it was not worth the risk of burning the commons and destroying, not destroying, but greatly marring both the name of VA and the social trust that people in EA had with each other.
Like this is, this is going to be a massive blow to people's ability to trust each other and coordinate because of this exploitation and will it, I'll let you finish.
I think it will. And the thing is, there is sociopaths who use naive utilitarianism to justify whatever they want, as you said, right? Oh, it'll have good enough effects in the future that I can do whatever I want right now.
And that's bullshit.
More than just sociopaths would go on.
But that's bullshit and everyone knows it's bullshit. And to the matter at hand, like Eliezer Yadkowski wrote in 2008, the ends do not justify the means among humans where he says, don't ever do this sort of thing.
You are not running on trusted hardware.
But Yadkowski and especially sequences area, Yadkowski, I don't know if he's changed this pretty clearly is not like a full blown utilitarianism.
No, no.
Utilitarian, especially if you read method of rationality, like that's the story of a naive utilitarian becoming not a utilitarian.
He specifically has said many times that humans should not be utilitarians.
It should be like maybe three quarters utilitarian.
The pithy joke version of that, which I love is all the cool kids are deontologists.
The rules say you should be a utilitarian by being a virtue ethicist has the best outcomes.
Cool.
Utilitarian is specifically a morality for gods.
It's for things who are omnipotent or at least omniscient and can see what all the results are.
And humans are not that.
So no human should be a complete utilitarian.
And I think it sucks that SBF, SBF, right?
Bankman fried fried.
Okay.
Yeah.
Did this.
And I think it's stupid that he's trying to justify it with utilitarianism, but also anyone who thinks that this is utilitarianism done right has not been reading or interacting with a factor of altruism or rationalism for any serious amount of time.
So I'm going to steelman the case that this was good utilitarianism in the interest of proving that even good utilitarianism is bad.
Oh, dear.
You should be one.
Okay.
So if I were bankman fried, the way I'd be defending myself is like good utilitarianism requires you to take positive expected value but uncertain bets.
And that's what he did.
He just happened to lose.
And if you hadn't had the recession and the credit crunch and so on, while crypto was still getting off the ground, then he would have been the Jeff Bezos for the next generation of the internet.
And all of that money would have gone into effective charities.
That's probably what he would say.
Yes.
But everybody disagrees with him.
Okay.
But are they agreeing with him for good reasons?
Or are they just trying to distance themselves from a loser?
They're agreeing with him for good reasons specifically because humans are running on corrupted hardware and cannot trust themselves.
Okay.
Yeah.
Okay.
Yeah.
Most people aren't making that argument though.
They're saying, no, he messed up the calculations.
No, no, they're not.
Everyone who's arguing against this is saying specifically that humans can't be complete utilitarians.
Everyone is an aggressive claim and I admit I'm kind of filtered with my internet bubble on bad EA takes a bit more so than good EA takes.
I mean, everyone who has any sort of reputation in the EA community is saying that it's a saying a combination of humans are corrupted hardware and can't make these sorts of calculations.
And also that he failed because partly because he's running on corrupted hardware of taking into effect the things like burning the commons and destroying trust.
And the only reason I think one of the favorite things I saw is the only reason we are such a rich society and we have the ability to have all this excess wealth that we can use for helping other people is because we do have high social trust and great cooperation.
And when you do something like this, you are damaging all that and do it enough times and you will destroy it entirely.
And you don't take that kind of risk, not with benefits this small benefits that can be measured in dollars, right?
Like you would do that for something like I will prevent a nuclear.
The benefits of having a commons can be measured in dollars.
Like the things you might think the benefits are like economic growth and advanced economies and stuff are correct.
Not having nuclear bomb go off can be measured in dollars as well.
But it's a little harder.
I'll grant you.
Yes, but certainly not something like how much crypto can I reap over the next few years to put into effective charities.
I feel like that's going to hurt the crypto enthusiasts more than it hurt the EA enthusiasts.
As an advocate for effective altruism, I don't find this the least bit disconcerting.
It's a drag that this guy did this and it does take money out of places that it would have been better off going.
But it's not like I learned that give well is gambling money away, right?
Right.
That's me actually trying to give charitable money and somebody lying about what they're doing with it.
This guy was like, yeah, I'll invest your money safely and smartly.
And then he did, which is different than an EA organization fucking you, right?
So that's true, but essentially, if that's where you're taking your comfort, then you've given up on doing anything other than preaching to the choir.
Because like the issue is not that more crypto bros will stop being crypto bros than EA's will stop being EA's.
It's that soccer moms will associate both crypto and EA with Ponzi schemes.
Maybe, maybe I'm missing the mark on how popular this guy was.
I mean, he was a billionaire quite a few times over.
Right.
But most I don't know if soccer moms know who Jeff Bezos is.
I'm sure that I'm well sure, but I'm sure there are some teachers in Toronto who know who he is.
I'm sure, you know, everyone who's met anyone that he fucked over will know who he is forever.
But like the thing is, I didn't know who he was until today either.
I knew there were several billionaires that got rich in part due to crypto that were financing EA stuff.
But I didn't know his name.
I didn't know the details of his company or whatever.
I don't give a fuck about crypto, but I know his name now and I know what he did and I know his justification.
And a lot of people who knew absolutely nothing about this are going to know about him and possibly likely even the connection, the connection to EA.
I heard about this on NPR news yesterday about FTX collapsing and wiping out tens of billions of dollars.
And did they talk about the angle?
No, but that's going to come out.
I guess I mean, I don't know how much that could possibly hurt the EA community.
You know, unless people are going to just, it's going to be like a Fox news, like holding up this one example of somebody did it wrong and be like, look, it sucks.
I mean, I kind of hope they focus on the polycule thing because that's a, you know, sexy, sex cult angle, like literally sex news.
And then that'll distract from the EA stuff.
But I think that has as much impact as anything else as, you know, nothing.
He was just a bad immoral person.
Yeah.
I hope that too, because I want poly to become more marginalized.
Hang on.
Hear me out.
Okay.
So that hopefully Kink also gets a bit more marginalized because it's becoming too vanilla.
And I want an injection of punk rock back into the scene.
Yeah, that's fair.
Once all the normies are doing it.
So you said that he's making these justifications that it was the right thing.
Are we, is he actually doing that?
Or is this the speculation that, so I was, I was jokingly defending like, well, there was an expected outcome.
That was great.
You know, but is that actually what he's saying?
Or is he, is he just keeping quiet in the Bahamas?
Like he should be for being this kind of financial criminal.
That's a good question.
I mean, there's a lot of things he said in the past that are being dredged up.
I don't know if that's what he's saying literally right now.
I think he's being quiet at the moment and declaring bankruptcy and all that.
But, and possibly stealing a lot of money.
But that's something we'll get into in the mind killers because that's like news stuff rather than rationalist talk.
One of the things he said a year ago in, on a podcast with Cowan was a Cowan asked him,
if there was a button you could push that had a 51% chance of creating a duplicate earth, you've heard of this.
I heard that because I listened to Tyler Cowan, Cowan yesterday.
I haven't heard of it.
And I imagine some of your listeners won't have either.
So can you finish saying what the question was?
The question was, you have a button you can push, which will make a duplicate earth in a parallel universe or outside of our light cone or somewhere.
So it's not just duplicating effort here.
There's actually going to be, you're doubling the utility, you're doubling the lives and happiness and everything like everything that's good on earth.
51% chance it'll do that.
49% chance that all the earths are destroyed.
So either you spin up a second matrix or you delete this one.
Yeah.
Well, not this one.
You delete this one and all other matrices.
Assuming there are others.
And he said, yeah, I push the button.
And he was like, even though there's a 49% chance, you're going to destroy earth.
And he's like, yeah, it's net positive utility.
He'd be dumb not to.
He's like, okay, well, then after that, you're presented the button again.
And he's like, well, then you push it again.
Now you have four earths.
And he's like, or you have no earths.
You can just St. Petersburg paradox your way into having wiping out all human life.
And he's like, or you St. Petersburg paradox your way into having infinite utility.
Checkmate atheists.
This, this is why I don't like utilitarianism.
That's not, that's not good utilitarianism.
Thank you.
This is somebody who likes, who likes pointing fun at like religious people.
I wouldn't point like I'm, it's, it's high school.
It's Lord to point it like the suicide bomber and be like, that's why I don't like religion.
Like granted, that's an edge case as to why I don't like it, but it's more like that.
That is not the representative sample of the average religious person, right?
Right.
And so this guy, I feel like if he's doubling down on the 51% thing over and over,
I feel like he might just be trolling, right?
I think that's how he lost all the money and wiped out tens of thousands of people and ruined things.
Right.
So if he, if he, if he sticks to his guns in the Bahamas and he's like, no, I made the
right call.
You know, it was this, it was the expectedly good outcome.
Then it's like, all right, this guy is kind of not about his, and he sucks.
Sorry.
Let, let's rewind to the, uh, that's not good utilitarianism thing.
How is that not a blatant, no true Scotsman fallacy?
Like you're just drawing a circle around all the obvious implications of utilitarianism
that suck and declaring them not real utilitarianism, which sure, on that grant, on those grounds,
I guess I'll endorse utilitarianism, but that's basically just going to cash out as virtue
ethics.
So the, but with a lot more computational intractability.
I think in general, my, my thing where like, I'll, I'll draw a circle around something
and say, this is not part of the thing that I consider in utilitarianism is like the things
that often with not a number attached to often produce bad outcomes.
Like this move that this guy, that SBF did, produced a very bad outcome.
And if, if you were the kind of person, if this is the way that you knew the world operated
or you wanted to operate, nobody would invest in companies like that, right?
Um, nobody would save their money at a bank because the bank also invests your money,
right?
Like this, this would actually be terrible.
And so if it has a terrible outcome, I think it's not a very utilitarian thing to endorse.
Right.
The actual utilitarian calculation is if you keep pushing that button, eventually you're
going to get a universe with zero earths because they all get destroyed, which is negative utility.
So you don't push the button.
That's the problem with naive utilitarianism.
It's super naive and, and doesn't do anything.
And it's also why humans can't do utilitarianism because you often can't actually calculate
everything.
Like in this case, it's super simple to calculate.
Oh yeah.
Pretty soon you're going to wipe out everyone.
So that's stupid and don't do it, but there's some cases that are a little bit harder.
I don't think this was one of those hard cases.
I think this was like him just wanting to make a lot of money and not wanting to admit
that his other company was failing and being driven by ego.
And then, you know, post-hoc rationalizing it that this is not utilitarianism at all.
This is just someone being an egomaniacal idiot.
Okay, sure.
But if this isn't utilitarianism at all, and this is the most utilitarianism like thing
that humans can run on our meat brains, then shouldn't we just give up on the project?
Well, I mean, as was said before, humans should not do utilitarianism.
They should do maybe three fourths of the way there and actually stick with things like
virtue, ethics or deontology or something.
I think that that's basically the right approach.
I mean, like, you know, you can do the utilitarian thing while still sticking to like certain
axioms that seem to make a lot of sense.
Like, I'm not going to violate the dignity of, you know, somebody else or a stranger
or a group of them.
I will take no chances in destroying the world while doing my thing.
But I'm also like, if I've got $1,000 that I wanted to give to charity for the holidays
or whatever, I'm not going to give it to the Salvation Army guy outside King Stupers.
Right?
I'm going to give it to an effective charity that will save people.
Right?
Yeah, it sounds like we pretty much agree.
Oh, I think we totally agree.
I'm just, again, I'm still kind of riled up.
I guess if you're on board with that, I don't see why you're not on board with Hoel's argument.
But I guess you were also getting that second hand from the podcast and not just reading
the article itself.
Well, I was getting it firsthand from him.
For the listeners, before we started recording, we were talking about Hoel's argument on a
podcast with Lex Friedman.
Was it Lex Friedman?
No, it was Russ Roberts.
Oh, Russ Roberts.
Okay.
So Eric Hoel wrote a substack article on why effective altruism is bad.
I assume it'll be in the show notes.
Yeah.
It's because they're not giving money to Eric Hoel.
Well, I mean, if maybe the essay is more coherent than him, like talking about it, but he was
just being the most like ridiculously disingenuous straw man wielding person I've seen since
I watched Dinesh D'Souza debate Peter Singer in 2007.
Which is basically what's happening right now with SBF and people saying this is why
EA is our evil.
But also it's a simple enough argument that people are going to be like, oh yeah, this
pattern matches to what all the villains do in movies where they're like, oh, it's for
the greater good.
Therefore I will commit atrocities.
So I have not listened to the podcast.
I have read the article and I found it not particularly straw manny or incoherent.
So probably he's just not not good at doing the interview thing and just like couldn't
present it as a conversation.
Sounds good.
No, I'll definitely read it.
I mean, I'm intrigued.
And he did say a couple of times like, you know, I'm just doing this memory, so I might
be wrong.
But then he would go on.
They would go on to like discuss that for 10 minutes.
It's like you were actually wrong.
And so you're now you're just wasting everyone's time patting yourself on the back for how
right you are to see through this, you know, obvious falsehood when you made up that falsehood.
But you know, when you're writing stuff down, you get more time to look it up and read your
sentence over again and fix it.
And as somebody who can't talk talk good most of the time, I'm very sympathetic to the fact
that he might have misspoken on a couple of things.
So, so that was the whole thing that happened.
You'll hear more about the nitty gritty details on the next Mindkiller episode because I'm
going to put it in there if David or Wes doesn't, we're definitely going to mention it.
Cool.
Yeah, I mean, it's interesting.
It's big news.
It would hurt the community if we learned that Peter Singer was in fact torturing children
in his basement, you know, like, wait, but would it?
I think that because he's an actual icon of the community.
I mean, SPF kind of was not to the singer level, but he was a lot of the money driving
the community forward.
And that's that's fair.
Maybe I, you know, he's mostly set off my radar.
Like I did hear him on Sam Harris's podcast and I was on there in the last few years.
But I only remember that he was on there because Harris opened up his most recent episode
mentioning the crash and said I had him on the show.
And so like he was peripheral to my attention, but he wasn't somebody that maybe maybe he
was this for other people though, whereas like this is what I heard about, you know,
I can do good better.
And I found this person really inspiring and oh shit, he sucks.
Like that actually is a drag, right?
But, you know, we'll see how it shakes out.
Hope it doesn't set things back too bad.
Yeah.
I don't really approve of like movement effective altruism.
Why is that?
Just because I think it's got a lot of the politics and power dynamics that a lot of
movements that start with good principles end up developing approximately immediately.
But can you say what those are?
Because I don't know what those are.
I can't remember any examples offhand, but but like, I mean, an example would be like
a rich asshole rocketing to celebrity because he started throwing a bunch of money around.
Just more like banali, I guess.
People in movement EA tend to be big fans of Rob Wiblin.
I find him a like, I kind of like 80,000 hours, but I find Rob Wiblin himself to be like
a fairly uninteresting thinker.
He's a good interviewer, but like his contributions to his interviews are like,
not really much.
Being a charismatic popularizer is important.
Yeah, I mean, that's more of a like, I really don't get the hype.
And I think it's just that like, he's probably some sort of influential behind the scenes.
Or maybe, I don't know, he just has a really good Twitter game or something.
So wait, you dislike EA because it has people that are popular,
even though they aren't the smartest people?
The smartest people, the people doing the best work, you know, whatever.
But again, that's movement effective altruism.
I'm more on board with like the principles.
But principles without a movement is, as Paul would say, a faith without works is meaningless.
Principles without a movement does nothing.
So one of my big takeaways from virtue ethics that's really like helped me as a person,
just psychologically is, and this is the like virtue ethics is more about being a good person
than about doing good things sort of thing.
Virtue is about you and your struggle and your journey, not about saving the world
with the theory being that like, if the world is populated by virtuous people focusing on
their own personal virtue, then it will be saved.
And so that's like, in that sense, like I see effective altruism as being
something that a virtue should track, namely prudence.
But uh, so like when I donate money, I try to donate it effectively and I try to donate
more than I think I should, just like running my basic money allocation software.
I do think that there's some individual projects like GiveWell trying to identify what those
effective charities are that are useful just because of basic economies of scale.
But all that is to say, I think there's fairly significant risk that this does do some
serious damage to movement effective altruism.
I'm just not entirely convinced that that'll be a bad thing on net.
I want to both agree and disagree with you on the virtuousness thing because I do think everyone
should cultivate virtues in themselves as we talked about in our virtue ethics podcast.
But this was the thing that occurred to me while I was going to various cathedrals in London.
There's all these statues of people who did great things for their community.
And the thing is, when you're a peasant in England, oftentimes you don't have much opportunity to
do much of anything. Most people aren't in the best health, either physically or mentally.
They don't have all the resources. They're just trying to get by.
And that's what virtue ethics is trying to help you with.
But there's some people who are lucky that were born into wealth or born with extraordinarily
healthy bodies or extraordinarily good mental focus and got lucky with the interest lottery
and are interested in things like cell biology, which are going to help the human race.
And it is virtuous for them to use those extra gifts to advance humankind or advance their
community. And I think it's kind of right for them to be honored, even though that
they just got lucky by having good genes and access to resources, but they still did a good
thing that helped everyone out. And it's nice to commemorate them and to be thankful that
these people were around to make communities better. And I think if you're in that, if you're in
that part of your life, which more and more people can be nowadays because our society is so rich,
you should be cultivating the virtue of helping people if you can and as effectively as you can.
So I agree that we should celebrate people like that. But, and I'm not sure how much of this is
me generalizing from fictional evidence. I imagine the people who get statues made of them, like,
I don't know enough about saints and sainthood to say, but like the say soldiers who win the
Medal of Honor, they're not the ones who are in boot camp saying like, I'm going to go out there and
be a hero and win the Medal of Honor and be like super famous and everyone's going to talk about
me. They're the people who are like, I'm just going to do my job and do it as well as I can.
And just like try to make sure that me and my buddies get home safe and we complete our mission.
And next thing you know, you're like fighting off 200 German soldiers in a broken,
in a broken tank turret with nothing but a heavy machine gun, a bunch of grenades,
and two giant brass balls. Sure, that's the case sometimes. But you said that you like to donate
even a little more than you thought you should to things that you think are effective like AMF.
Like, why does AMF exist? It's because of people doing this, making it exist. Sure, but I don't
think I'm saving the world by doing that. I think it's a good thing for me to do for like,
because it helps me develop skills and mental habits that are good to have.
Do you think it's a good thing for AMF to exist? Because if they didn't, you'd have to give your
money to the local church, which is far less effective, right? Yes, I think it's a good thing
for it to exist. But I think if you are a worker for not AMF specifically, because they have like
a fairly narrow mandate, but like, give well, say for listeners, AMF is the against malaria
foundation. Yes. If you're a worker for give well, say, like your job is, I think you will
be a better give well researcher. If you show up to work saying, I want to be the best researcher,
I can be and do really good research, then you will, if you go into work daydreaming about
being a hero who's saving the world. Alright, so having met a bunch of EA people who got into EA
to help save the world personally, that's what you just described as approximately 100% of people.
It's just people who want to help and they're like, how can I help? I know how to do data science,
or I know how to do this and this. And so is there a company there is, there's a give well,
I will work for them. Like it's that is movement EA. It's people who just want to help the world
and who have some skills. And so they sign up to work for give well or AMF or some other place
like that. Yeah, maybe I guess movement EA might not be the best way to describe what I'm
getting at like maybe EA influencers would be better. Like there is a subset of people who
self describe as EA's and who talk about it a lot that are like self aggrandizing and not
particularly good at being people. Well, it sounds like SPF was one of those.
Yeah, and that's the point I'm making. Yeah. But people like SPF I think are the
distinct minority. Whatever label you want to attach to those people, like imagine I said that
instead of movement EA, if you think of something else when you think of movement.
Okay, so you think self aggrandizing sociopaths suck. Yes. And I agree with you that has nothing
to do with EA aside from they have as many of those as any other organization does.
I just wanted to say that like, you know, like you said with virtue ethics, you know,
the idea is that everyone tries to focus some of their their attention on being a virtuous person
and cultivating a virtuous life in the world we saved through that some portion of their time
will be spent talking to people who aren't yet virtue ethicists about why they might
think about considering it, right? Yeah, like, so that's what I think of community EA. That's
what I think of. And so I just wanted to get that out before you said specifically the celebrities.
And yet, you know, the influencers, like, I will be cast a lot to fuck up seriously bad for me to
be like, Oh, yeah, he turns out he did a net negative, right? And not like an utilitarian talk
of, you know, utilities and whatever, but like his actual impact was bad. Same like Peter Singer,
like the that guy might have single handedly led the charge that has reduced the suffering of animals
of billions of animals on the planet, right? And sure, it took like 40 years. But it's entirely
possible that without him, it's not take another 80, right? Like, so, you know, I think I think that
the Titans in different fields of whatever, I think they can have a lot of value, you run the
risk of them, you know, fumbling really hard, or having been an asshole secretly the whole time,
right? So, you know, be careful with it. And I was going to say, you know, if there was like a
virtue champion that was out there talking about how awesome that wasn't going on book tours on
their new virtue book, that'd be great. And then if they were secretly an asshole, that would suck.
And yet they wouldn't secretly be an asshole, because they're a virtue ethicist. So if they were
actually eating their own Kool-Aid, there's no way that they could that they could, you know,
end up being a bad person, like the way that a utilitarian might go a little too far into robot
territory and become a bad person, right? No, I think there's just as many sociopaths in virtue
ethicism as, and that our virtue ethicists believe in virtue ethics as any other system.
I'm prepared to, I'm prepared to guess that somebody's going to say, no, there aren't,
by definition, they can't really be, and they'll know true, they'll know true Scotsman that.
You know, sociopaths will neatly chameleon into any sort of moral system that they think
will get them ahead. I disagree with the hypothetical person you were suggesting exists.
I think it is possible to have sociopaths whose sociopathy manifests in them talking about virtue
ethics. I just think that, okay, sure, but. No, I didn't mean to interrupt, and I feel like we
are going to kind of fire a field, and I was more having fun with that example than like seriously
gearing up a hill to die on. Yeah, I think that, and I recognize the self criticism in this as
someone who talks about virtue ethics a lot. I think a good virtue ethicist should be suspicious
of anyone who talks too much about virtue ethics, specifically to avoid that dynamic.
And I think one of the big benefits of virtue ethics is that when the sociopathy comes to light,
there aren't as many easy defenses as like I made a positive expected value bet and having to lose.
Yeah, fair enough. I mean, because that's the thing is you can be a sociopath, like,
you know, the the absurdist argument of like, well, the doctor who sacrifices one homeless
person to save five people, like. It's not an absurdist argument. They're utilitarians who take
that example very seriously, but go on. There are people who take their religions so seriously to
blow themselves up. But I guess I would say like, what the thing is, is that just like the people
blow themselves up for God, there actually is a way to interpret the letter of the written word
that leads them to that conclusion. But it's not the way that they probably should.
David, you act like there's no virtue ethicist that would not sacrifice a homeless man to save
five other people. They would sacrifice only the right amount of homeless people, not too many,
not too little, right? They'll only they'll only spend the right amount of time advocating for
virtue ethics. Right. And for sacrificing homeless people. Right. There's certainly some way you
can be like, this is the most virtuous thing to do, given the virtues. I mean, sure. Okay, wow.
But like, that would be getting too far afield. And I, I'm tired. Listeners, I started recording
this after like an hour of downtime, after a nine hour drive across the most mind-numbing part of
America to drive across. So sorry, if I'm not quite firing on all cylinders right now,
I still want to get my psychedelic experience of driving across Kansas alone and sober.
Apparently this is just a crazy thing. Yeah, you were going to do without an hour of downtime,
total champion here. I don't have the, it's a 40 minute drive for me and I get tired,
so I'll never complain again. Two other quick things about it. A lot of EA people are going
through the thing right now where they're being very self-flagellating and saying,
how could we not have seen this? We should have seen this. How have we, let's examine how we failed
and as has been pointed out, at least a couple of times, there was a major capital company,
what was it? Solarium, Solaris, something like that, whose job is to do due diligence and see
if things are scams or not. And they invested millions of dollars in this. So if a major investment
firm is going to get scammed by this, you're probably not going to do a better job, stop
beating yourself up. And as was said from in this tweet, EA's have a God complex. And I think that
part is true because one of the things that we all, I guess, admit in EA, even though it's not
the healthiest thing, as we also all admit, is that, as Harry Potter James Everett Evans-Verriss
said, when you can't do the right thing unless you become God, the correct answer is to become God.
And so there is a bit of a God complex, but we're not God and we can't beat ourselves up for not
being God. So don't do that. And the other thing is a number of people are thinking like, should I
return all the money that I got from SBF and FTX's Future Fund to make hold that people have that
have been fucked by this. And in addition to it being only the tiniest drop in the bucket,
Eliezer posted an essay about, no, you shouldn't, especially not if you've already spent the money,
especially not if it's for something that you've already done. And most likely not if it's stuff
for something you're still going to do. Because more than anything else, you were being paid to do
a job. And that job is a philanthropic job, but you're already taking a more risky career in
philanthropy. You're already taking a wage cut to work in philanthropy. And there is no other
industry that is going to be returning some of the money FTX sent them. There's a stadium in
Florida who sold the rights to name their stadium to FTX. And they sure are not going to be returning
money, at least not a decent portion of that money. They're keeping it even though it was
gotten through this illegal bullshit. There's the electric utility that gave electricity to FTX.
And part of that was to cover costs. And part of that was profit. And they're not going to return
the profit for that electricity to, to the FTX account holders who have been robbed. Like,
it's just that that's not a thing you do. And everyone thinks that if you work in philanthropy,
you should be a complete martyr and destroy yourself. And you are not accountable for this
is basically the gist of the essay. I think I agree with him. And I'm going to link the essay as
well. I would encourage the utilitarians to keep beating themselves up, because watching
utilitarians beat themselves up for dumb reasons, give me gives me infinite utility. And I'm probably
lying about that. But do you really want to take that chance? I was hoping you'd say that. That's
awesome. The yes, I realized that Pascal's wager is dumb. And it's the first thing that all utilitarians
throw out the window. I make fun of you because I love you fist bump. I don't really love you.
You're fine. You're like, stop digging stuff. So the only the only person who I think should be
bringing their hands wondering do I do I burn everything to try and make this people whole
again is SPF himself. Oh, he's probably going to jail. Well, I know. But like, this, it doesn't
check me as impossible. And this actually is a sort of fun thing to consider. Let's say that after
all the the dust settles, he's left with a, you know, mere $25 billion. I don't know how much
let's say he robbed people of a total of $10 billion. If he could liquidate everything and pay them
back, he could, right? He could he could fix all the problem. You know, he could at least he could
he could make right that he could fill the bank accounts that he emptied, right?
No, I think right now all he has is the few hard assets like his house in the Bahamas,
some cars and shit. I was going to say hypothetically, then then the fun trade off would
be like, should he do that? Or should he give 10 million 10 billion in cash to effective effective
charities? I mean, obviously, he should not only obviously the law would compel him to first make
whole all the people that got stolen from with the Bahamas law compel him to if he stayed out of
offshore. Actually, yes. Oh, lame. He should go to a cooler place. Let's do whatever he wants.
If you're going to be if you're going to be a criminal, who your financial criminal get by
your summer house in a in a country that let you get away with whatever you want, right? I mean,
all the assets Iran has some lovely real estate. All the assets have already been frozen. But
yeah, there's there's no way people are making all the money back. It's just it's not there.
That's I appreciate the impulse of people who want to be like, well, what can I do to help the
people who got fucked over? Yeah. And yet, like, when insert giant bank, whatever gets, you know,
fucks up and whatever, everyone loses, you know, a bunch of money, like the employees work with
that bank aren't like, well, should I sacrifice my retirement to break those people whole again?
Like it's just not your problem. Like it sucks. And hopefully, you know, something will happen to
help make things right for these people. But like, it's not maybe not a bad effect of altruism or
bad utilitarian. But like, it's I'm totally hateful of saying it's not your problem, man.
You don't have nearly enough guilt, Stephen. I, you know, that's I don't have enough
anything. I don't have enough that could be like the subtitle of my autobiography. Nice.
So I guess that's all of that if we're done with that, which went on a lot longer than it was
supposed to for a pre show thing. This might just be the meat of the show, I guess at this point,
by the meat of the show, but it does transition neatly into a subject that can spend a little
time on because one of the things that you said that people are wondering is like, where did we go
wrong? What can we do?

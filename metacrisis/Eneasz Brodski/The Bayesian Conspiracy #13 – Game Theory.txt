Alrighty, so for our listeners who have in the past heard sounds of music in the background,
we were very confused by that and then we discovered that where a place we're recording,
Zuber International Studios is right next to a big old radio tower and apparently our
microphone was picking up interference from that without us actually hearing anything.
We discovered this the hard way when in several places in this episode, the music came through
really loud and it's kind of hard to make out what we're saying.
So we are re-recording at just a few little spots where the music was bad and after this,
this won't be a problem anymore.
Welcome to the game conspiracy, my name is Katrina Stanton, I'm Inyash Brotsky and I am Stephen Zuber.
Today we are going to be talking about game theory because we realized that we have been
referencing it in earlier episodes and we probably will in future episodes and some
people might not be that familiar with it.
Also, you live it.
Yes.
We live it.
It determines all of our decisions.
Can you expand on that?
What's game theory?
Okay.
I'm not as good at the trans- you can tell my trans- it sounds much more forced than Katrina's.
So I have a definition that I like is treating situations as if they were games with preset
rules and win conditions and the various agents in them are players that have to stick with
these rules and trying to maximize the outcomes and each player tries to maximize their own
outcome and then figuring out via those interactions what is the most likely thing that is going
to happen.
Okay.
It will become more clear as we continue to talk about it.
Can I pop another definition in there?
Please.
Because to prepare for this episode, I read the sequences on game theory by Yvane.
Ah, yes.
What is that person's name?
Scott Alexander.
Scott Alexander.
So same person, but this is on less wrong and we'll have a link up to it.
It says game theory is the study of how rational actors interact to pursue incentives.
It starts with the same questionable premises as economics.
Everyone behaves rationally, but everyone is purely self-interested and the desires can
be exactly quantified.
Now, of course, like any theory, a lot of it is playing with those presuppositions and
changing them and watching how the results change as a result of that.
Another quote from Scott Alexander on less wrong from his game theory sequence.
I'm not sure which posts I didn't save them.
Some of the examples in game theory won't work in reality.
Some of the examples that we give to illustrate game theory, they're more of a reducto ad absurdum
of the so-called homo economicus who acts free from any feelings of altruism or trust.
Which I think someone said that that was the main critique of game theory and I guess other
economic theories that involve people, which is that it treats them like quote-unquote
rational psychopaths.
Yes, but again, just like any other theory, you make it certain assumptions and you figure
out what the result will be given those assumptions and then you kind of figure out why the results
difference.
And that tells you important things about human nature, about evolutionary psychology,
that kind of stuff.
And I like the fact that the name game draws attention to that fact, at least in my opinion.
It points out that yes, these are games that we're setting up, this is not necessarily
how people act in real life.
I just like the word homo economicus.
Right.
Homo economicus.
The behavioral economic subject.
That's right.
Really, really I think that is the scientific term for the straw vulcan.
Ooh, nice.
Nice, let's turn that down somewhere.
Well, we have it in the podcast.
It's saved forever.
Inga shows it.
So do we want to quickly go into the most famous game theory game?
Prisoner's Doma.
Uh-huh.
Yes.
Alrighty.
That's my jam.
All right.
So Steven, I heard you and Katrina broke into a warehouse yesterday.
That's why I arrested both of you and have you in my prison cell now.
No, and we're separated.
You are separated.
You are selfish and rational people.
I should certainly hope so because this whole example fails if you aren't.
So you guys have broken into a warehouse and I can tell that you've done it because there
was glass DNA all over both of you.
Glass DNA.
In this world of glasses DNA.
Shit.
I knew we overlooked something.
So you are both going to get a year in prison for this breaking of a window thing because
we do not tolerate window breaking in our world.
It's got DNA.
It's obviously, you know, alive and self-aware.
So you just can't go around breaking glass.
Yeah.
But I've separated each of you.
Steven, you're not here right now.
Hey, Katrina.
Hey.
Year in prison sounds pretty shitty, doesn't it?
Oh, it sounds terrible.
I don't want to do it.
Well, while we were searching the warehouse, we found someone left an upper-decker in one
of the toilets.
And that comes with a 10-year prison term.
I'm willing.
I'm willing to cut you a deal because we can't really prove whose upper-decker it is.
Do people have DNA in this world?
Not in their poop.
We are all made out of a non-DNA material.
So unfortunately, the only way I can figure out who this is is by getting you to confess.
If you confess that you guys were in there and you got kind of drunk and then Steven
left an upper-decker, well, they're the book at him.
We'll give him 10 years.
But you get off scot-free.
No jail time for you at all.
Now, because I'm a terrible cop, I'm also going to tell you that I'm going to give
Steven the same deal.
And if Steven turns on you before you turn on him, then you get the 10 years in jail
and he walks away free.
But because I'm the worst cop, I'm going to let you know that if both of you squeal
on each other, the jury is just going to be like, ah, fuck it.
These guys are both assholes.
Give them six years in jail each.
So both of you will get six years in jail if both of you squeal.
Oh my gosh.
Yeah.
That is the basic prisoner's dilemma.
Well, my initial impulse is I want to roll on Katrina.
I'll get off scot-free and she gets to go to prison for 10 years.
He's not talking to you.
No.
All right.
I'm out of the room.
Well, right now you are.
Yeah.
Well, we are both making this decision at the same time.
So I was just giving you a hard time, Steven.
No, you're good.
That's what I was trying to emphasize was that we are both aware of the deal and the clock
is ticking for both of us.
We have to make this decision.
Yeah.
So in game theory, if both of you decide to keep your mouth shut and take one year
of jail time each, that's called cooperating because you are cooperating with each other.
If one of you squeals on the other one, that's called defecting because you have defected
on the deal where you don't talk.
And, you know, if both of you talk, then you both have defected.
So the question is for each individual prisoner, what is their dominant strategy?
What strategy gets them the least jail time?
Yeah.
Well, so for a rational actor.
Yeah.
For a single iteration.
Yeah.
Let's say you're thinking this through right now.
Oh.
Stephen cooperates.
What are your two options of Stephen cooperates?
So if Stephen cooperates, then I should definitely defect because then I get zero jail time.
Yes.
And what if you cooperate?
And I don't care about Stephen in this situation.
Yeah.
So what if Stephen defects?
Well, then I should definitely also defect because then I'll get less jail time than I
would if I cooperated.
Because if you cooperated and he defected, you would get 10 years.
Exactly.
Okay.
And Stephen, is your mind in the same place on this?
Yeah, you've laid out all the options pretty well.
Okay.
Thanks.
I appreciate you guiding me through this officer.
You're making your guidance process really well.
Awesome.
Getting both of you six years in jail.
And that is the fun part about the prisoner's dilemma is that looking at the payoff grid
from an objective point of view, you can tell that both people cooperating gets them the
least amount of jail time.
They both get one year.
Stephen, I can't believe that you pooped in the upper part of the toilet.
Katrina, that was you and we know it.
Throw the bucket in both.
Wait, no, no, no.
You both do it.
Ah, fuck it.
Send him off to jail for six years.
We got no time to sort this out, but we know one of them did it.
They both confessed.
That's right.
There's no other evidence in this situation.
Nothing else to consider.
Nope.
Yeah.
So that's one situation, right?
The rational, the rational result is that both defect and both get six years.
See, and that is an example of Nash equilibrium.
Yes.
Which we're probably going to be using that phrase multiple times throughout this episode.
Would you like to define the Nash equilibrium?
I would like to define it poorly and then have somebody else do it better.
Is that cool?
Sure.
Okay.
So I think the Nash equilibrium is a situation where I would not change my answer.
Even though I know the other person's answer.
Okay.
Right?
Yeah.
So I don't regret, I don't regret my answer no matter what the other person says.
Well, no, you might still regret your answer because one year is better than six years.
Well, I think, but the prisoner's dilemma doesn't have an obvious Nash equilibrium.
It has a mixed strategy Nash equilibrium.
Is that correct?
From what I've heard, it does have a Nash equilibrium of both defecting because the Nash equilibrium
as defined is one person has no incentive to change the strategy assuming that no one
else changes their strategy.
So if the other person is cooperating and you are cooperating, you have an incentive to
defect.
So you get one last year.
Okay.
So the Nash equilibrium is what we actually came to because we're both rational actors
and it's that we both defect and we both get six years in prison, which gives us worse
outcome than if we both cooperated.
But it's not the Nash equilibrium.
So we're going to go and defect on each other.
And thank you for correcting my terrible definition.
No, your definition was actually really almost exactly what I said.
And this is one of the situations where Eleizer would say this is not actually what a rational
person would do.
Because there's super rationality.
I suppose if you want to call it that, but I would just call it irrational to end up with
a six year term when you could have had a one year term instead.
I agree.
So yeah, we could put rational decision makers in our first iteration of the game in quotes.
I don't feel like that's the optimal decision for either of us.
But within the constraints of the game where you're only trying to maximize one thing and
there's no other social pressures or anything outside of the game.
Still within constraints of the game.
We're very selfish.
We're very rational.
Yes.
You don't care about each other.
You don't care about what future favors you may have done for each other.
So as you suggested just now, different things happen when we start to repeat the prisoner's
dilemma with the same prisoners.
We actually commit different crimes.
So we get out of jail.
Here's the deal.
We broke living glass, I guess, and pooped in the toilet in the wrong way.
And then we got caught.
And then we're going to do it again later once we get out.
And then we're going to do it again later once we get out and get caught.
And then over and over again.
But there's different results for how a rational actor is going to play that game depending
on if they know exactly how many times we're going to commit that crime versus not really
knowing and doing the crime and getting caught for it an unknown number of times.
Which is kind of fun to think about, huh?
I'm still hung up on the fact that we defected.
We were talking in a voting episode about what my decision making process is to lead me to
vote, which is that I want other people to vote.
I want other people to inform themselves and vote.
And you alluded to the fact that ties in pretty well with the time of decision theory.
Yes.
And I was thinking more when I made the argument in terms of just deontological reasoning,
that if it's something that I would do and I would recommend it for myself,
I would recommend it for other people as well.
And time-less decision theory is a little more broad than that.
Avoid some of the pitfalls.
So say Katrina and I don't care about each other's well-being as far as who suffers
in prison and who doesn't, but we do respect each other's rational capacities as smart people.
Then in theory, if you think she's smart and if you think you're smart, you would both cooperate.
Unless he thinks I'm just highly rational and also possibly a homo-economicist.
Probably if he thinks you're a homo-economicist, yeah.
But if he thinks that you're actually a rational person, in the sense that you want to have
as little jail time as possible, he thinks that you might cooperate.
I would totally cooperate.
Yeah, me too.
There was actually a great t-shirt from, God, I think it was rational apparel or something
that said, I cooperate in the prisoner's dilemma.
And I bought me one of those.
They also had an effect in the prisoner's dilemma one.
I actually can't wear it around though because it has a little plus on it for cooperating plus.
And it's in green because green means go.
And we live in Colorado and so everyone is like, hey, you're wearing a marijuana t-shirt.
Like, no, I don't really like marijuana.
You're patient.
Yeah, I know.
No one ever reads the words.
They're like, ah, green cross.
Here in Colorado, green cross means you're at a weed dispensary.
It's so different depending on where you are.
Yeah, so since we have things like friendship and actually wanting lower prison sentences
and altruism and faith in our fellow human, a lot of people are going to choose cooperate.
We're going to stay silent, right?
Yes, I think so.
That's why the fact that we both ended with six-year prison sentences is not the rational outcome.
But the rational outcome is where we both realize that both parties involved are rational agents
and that we want to spend as little time in prison as possible
and the way they do that is for us to both cooperate.
Yeah, they call that super rationality for some reason.
I don't understand why.
I have a prediction to make, which is that we're going to get a lot of corrections on this episode.
Sure.
Bring it in.
Yes, do it.
That's why we have a listener feedback at the end of the episodes now.
Yeah.
Great.
So yes, totally agree with you.
In my opinion, like half of morality is just basically trying to get people to always cooperate on prisoner's dilemma situations.
And I don't know if other people think that's accurate, but it seems to be the fact that people keep trying to be like,
hey, you know, be cooperative.
It's better for everyone all the time.
Right.
And then religions show up and are like, when you defect, you go to hell.
Exactly.
You don't want to go to hell.
So that's another way that you can solve the prisoner's dilemma is people have to go to eternal suffering in damnation if they squeal,
if they're not loyal to their partner in crime.
Right.
I don't think that any religion would say that, but...
I'm stressing the analogy our mob boss might kill rats, right?
Yeah, that's the example that Scott Alexander used is that we're both in the same crime family, I guess.
And our mob boss says that people who squeal sleep at the fishes.
So therefore, neither of us want to die.
And we've just been given an excuse to cooperate so that we both have a lower jail time.
So it works out.
Excellent.
Or...
So whenever people talk about cooperating and defecting, that's usually the sort of thing that they're talking about.
Or we have reputations, or we plan on committing crimes again in the future.
And in that case, there's the opportunity of maybe Stephen to punish me if I defect by defecting on me next time instead of cooperating.
Because, of course, the best thing for him to do, from my perspective, is to cooperate every time.
If I want him to cooperate every time and we're only doing this thing once, then I should definitely defect.
But if I want to build up some good faith with him so that he continues to cooperate with me, then I should cooperate too.
As a social species, this seems to be really hardwired a bit into us.
Or, I don't know, maybe it's beaten into us in childhood.
But punishing people who defect, even if it's very costly to you, is a thing that people tend to do a lot.
I think my favorite example is the ultimatum game.
Yes.
Where a researcher takes two people, gives one of them $10 bills, says, split this up however you want.
And then the other person can either accept that, the split, in which case you both get however you split it, or they reject it, in which case neither of you get any money.
You give me back the $10.
So theoretically, if you're a rational actor and I'm given the opportunity to split up this $10, Stephen, then you should accept whatever I give you, right?
Because that's better than nothing.
Right.
But...
So a rational actor, rational in quotes, would always split it 9 for themselves, $1 for the other person.
But when they actually do this in real life...
People reject that.
They're like, I'm willing to forego $1 to punish that guy for being a dick.
Yeah.
Or that girl.
I was listening to a teacher the other day, and he was saying, kids really understand fairness at an early age, and actually so do animals.
There have been a number of studies.
But, you know, ask a kid to divide up a cookie.
And they're going to be very, very careful that that cookie is fairly divided.
Because a lot's riding on that cookie being fairly divided.
Yeah.
There's the way to get around that, which is, say if you're dividing a cookie, or whatever it is, you split the cookie, and I get to pick which one I get first.
Yeah.
That's a good...
Yeah, but we're not doing the ultimatum game anymore.
I know.
I just like that way of getting around.
That particular problem sounds like it has a very easy solution.
But of course, if I'm dividing the cookie, and I'm the only one, you know, if I'm bigger and stronger or something, then I get to pick which one I want.
You're lucky you're getting anything at all.
There's some interesting games, permutations I've found where some cultures tend to be more vengeful than others.
And they don't like people trying to make themselves look good by contributing too much to the community pool.
No, no.
This was really interesting.
They're those vengeful and don't like it when people contribute too much.
Because it makes the other person look really good.
So it's one of those ones where you're passing the pot around, and every turn you can either put in some of your own money and then it doubles.
And at the end, once you're all done, everyone splits it up, or you can take out some money.
There's variations where you can punish other people for what they've done.
So if they take out money, you can spend some of your own money to make them lose money.
Now, you have to spend more than you make them lose.
You'd have to spend $4 to make them lose too.
But people will do it anyway, just because they want to take money away from the guy that was being a jerk.
There's a few cultures, and I don't remember exactly which ones they were, where if you put money in the pot, other people will destroy money to take money away from you because they're like,
well, that guy's just being all highfalutin and trying to, yeah, make the rest of us look like greedy jerks.
Clearly, the optimal rationalists.
Well, no, it's just when the person I read who was commenting on this said, this basically looks like pure evil to me.
You're destroying wealth to destroy wealth because someone was trying to create wealth.
Gosh, that reminds me of somebody I was talking to, a divorce lawyer recently.
And she said that she got a call off hours from a woman saying he took the turkey baster.
Thanksgiving's tomorrow and he took the turkey baster.
That was on my list.
And she told this woman, you know, so you could pay somebody hundreds of dollars an hour to make sure that you get the turkey baster.
And figure that all out or you could go out and buy one yourself.
And she said, no, transfer me to the other lawyer.
I want that turkey baster.
And he told her the same thing.
I will make sure that you get it back and that you are, you know, fairly, what do you call it?
Compensated.
Thank you.
Fairly compensated for your turkey baster.
But you'll be paying me a few hundred dollars an hour.
And she said she wanted to go for it.
The end result was he recommended that she go out by the most expensive turkey baster that she could possibly find and that he would make sure that her ex paid for it.
Okay.
Yeah.
Well, that sounds like a better solution just getting the old one back.
I was thinking the second that, like, the court order showed up on my door to return the turkey baster.
I did some very nasty things with the turkey baster before I returned it.
Yeah, but it's still spending hundreds, if not thousands of dollars over a, what, $5 turkey baster.
Yeah.
You have to stick it to someone.
But on the other hand, that also comes in very useful because patent trolls count on the fact that nobody's willing to go to court and spend $30,000 to defend something rather than just pay, you know, a few hundred dollars for this patent bullshit.
Mm-hmm.
Are we all familiar with patent trolls?
Oh, I am.
Okay.
It's usually people who companies that buy up pornography.
Oh, you looked at me like this is not.
No, I hadn't heard the print.
Well, because when I hear about patent trolls, it's generally through, you know, NPR and other news media.
And so I guess they take more family acceptable examples.
Okay.
In my, in my very limited experience of reading about this online on, I think it was Popat.
Okay.
Um, so we'll find the link, but it's, it's crappy little companies that get set up specifically to buy out of copyright porn or not out of copyright, but really cheap porn rights.
Mm-hmm.
Put them online and then when they get pirated, they go after the people who pirated them.
Oh.
So they just buy up a bunch of titles for super cheap just so that they can go after people who pirate porn.
Okay.
Download porn legally.
And then a combination of embarrassment and not wanting to go through the expensive process of court.
They are able to get huge amounts of money by, by exploiting people.
Okay.
See that's that, that is very similar.
When I was thinking patent trolls, I was thinking of the, again, shitty little companies that just buy a bunch of patent rights.
And the way patents work in this country is really, really copyright and intellectual property law in the US is broken right now.
And they need to do something about it.
They will buy up bullshit copyrights that someone has copyrighted a graphical UI element, for example, or the one I'm most most recently heard about is a supplement for muscle building shakes or whatever.
There's a simple amino acid that's added to all of them.
And there was a university that did some research on it, filed a patent related to it.
And this company bought up the patent for really cheap because it turned out that the, it didn't do what they wanted it to do.
So they just kind of, the university just kind of left it on the shelf.
They bought it up real cheap.
And the guy started going after every single protein muscle mass shake on the market, saying you have this amino acid in your, in your, in your supplement.
You have to give us a few hundred dollars or a few thousand dollars, whatever it is.
And most places were just like, fine, whatever, here's your, here's your money.
But this one guy was like, no, screw you.
And he's spending literally millions of dollars in court to, to fight this.
And he personally is not ever going to make that money back.
However, across the industry as a whole, it's estimated to be about $2 billion worth.
So he's, he's doing a lot of good by fighting this, but he was never going to see that money back or make a return on it.
So that's, he's not, he's not acting as a rational actor in this particular game.
No, he's not.
He's being like the lady with the turkey baster.
But in this case, it's for justice.
Yes.
So he's the lady with the turkey baster.
He's not a rational actor and he's not reaching national equilibrium, but he is an irrational human being.
And he's doing the right thing.
Yeah.
Well, if everyone were to do this, then patent trolls would know they cannot get away with just buying up shitty patents and threatening people with legal action.
Man, nobody is going to cooperate with his patent trolls.
Right.
It's, it's, it's why the US doesn't negotiate with terrorists.
Life sounds really easy for willing to compromise all your ethics.
There's a lot of easy ways to make money if you want to just be a bad person.
Yeah.
Yeah.
So speaking of not negotiating with terrorists, would you like to talk about the chicken game?
Yeah.
Are we okay moving on to that?
I actually wanted to talk about the Prisoner's Dilemma and the people who got together and created programs and a competition to win the iterated Prisoner's Dilemma.
So the Prisoner's Dilemma that goes over and over and over again.
And there was one top strategy by Anatole Rappaport, which is an awesome name.
And it was the simplest of any program, entered with only four lines of basic and it won the contest.
The strategy was just to cooperate on the first iteration of the game.
And then after that, do what the opponent did on the previous move.
It's called tit for tat, right?
Tit for tat with forgiveness.
So it would also randomly do something nice at some point, I think.
So there's a small probability that the player is going to cooperate anyway, even if the opponent defects.
Okay.
And then it works really well.
And Wikipedia put together a list of conditions that are necessary for our strategy to be successful.
One is that the most important is that the strategy be nice.
So it's not going to defect before the opponent does.
It retaliating.
So if the opponent does defect, then it also defects.
It's forgiving.
So even though it retaliates, they fall back to cooperating if the opponent does not continue to defect.
And that stops the long runs of revenge and counter revenge and Maximals' points.
And finally, non-envious.
So not striving to score more than the opponent, at least the best outcome.
When you mentioned the name Rappaport, it made me think of an essay that I read by Daniel Dennett.
It was called, How to Criticize with Kindness, Philosopher Daniel Dennett and the Four Steps to Arguing Intelligently.
And as he went on to list how the program played its game, I'm like, oh, that's the same Rappaport.
Anything?
Yeah.
What's the amatoll?
Is that the Rappaport's first name?
Anatoll Rappaport.
Sweet.
And I'm going to link to that Brain Pickings article by Daniel Dennett because it's the best way to argue.
But we don't have to go over it now because it's not really relevant.
I always think of the pansy Parkinson's, again, from the Methods of Rationality as the worst players because they would always defect.
And I think that's why you want a, I guess they're calling it a super-rational strategy, rather than a rational one.
Because in a world where you might run into a copy of yourself, if you can't even cooperate with yourself, you have some serious issues.
Well, and I think, you know, since I'm unlikely to run into another Steven Zuber, but I am more likely to run into people that think along the same lines that I do.
And if I can't get along with people who think like I do, then I think I'm thinking wrong.
So that's where I think that the generalizability of my rule to cooperate, at least until I get screwed over a bunch of times, maybe then I'll just start defecting out of spite.
My tendency is to cooperate based on that premise.
Yeah, well, people spend a lot of time studying cooperation, altruistic behavior, among both humans and animals, trying to figure out where it came from.
And that's why I was looking at the evolutionary game theory. But I think that we should talk about chicken first.
Okay. Do you want to start up the game again, and Katrina and I can play chicken?
So, the game of chicken is very simple. It's been shown in multiple movies. Two people get in the car, they drive at each other.
Different cars.
Yeah, sorry. Two people get in different cars, they point the cars at each other, they step on the gas, and they drive towards each other.
And if the two cars hit, they're both going to die, obviously. Two head-on collisions is a terrible thing to have.
So it's going fast.
Yes, they're going fast, and they're not wearing seatbelts, and they don't have airbags. This is in the 1950s.
Are we greasers?
You probably are, yeah.
Yeah.
I'm smoking a cigarette.
That's right. Comb in your hair whenever people are looking.
So you're driving at each other, and obviously you don't want to collide because you don't want to die.
So someone is going to swerve off to the right. The person who swerves off to the right first is a chicken.
I freaking have no guts at all. So that person loses.
The question is, how do you win in a game of chicken?
And the best answer is, after you start, you remove your steering wheel, you throw it out the window while the other guy is looking at you, and then you just keep going.
Because you're like, dude, I can't steer away.
I have committed to going forward by removing my ability to steer right. You have to be the one that steers right.
That solution sounds generalizable outside of just the game of chicken.
Dory time from six or seven years ago.
We did a lot of all the stupid stuff you weren't supposed to do when you got cars, including playing chicken with strangers.
No, you didn't.
The one time. So we didn't crash, but it still was a fun story.
So long story short, we were chasing this random car around because that's what we did.
We would just chase them until they got pissed and then we'd run away.
What the hell is wrong with you?
We were jerks.
16 ish.
So this car is going like 75 miles an hour down a 35 mile an hour street.
We chase them.
They turned left across traffic to get down to the side road.
So we have to wait till traffic leaves before we can follow them.
And then they flash their high beams and we're playing chicken.
And where did you grow up?
Did you grow up like on some farm?
No, it was in Fort Collins.
It's Fort Collins.
Yeah, you know, never going up there now.
The Hedonic Index, that city went way up soon as we left.
Yeah.
The kids from Fort Collins said that they would just throw knives at their own hands for fun.
I don't know what kids you were talking about.
We weren't that stupid.
This is more fun than that.
Well, you do seem smarter than the average bear.
Well, what happened?
I wasn't smarter than the average bear 10 years ago.
Okay.
So we flashed the lights at each other.
Yeah, so the way this worked out is we were going pretty fast.
Well, this actually concerned me right around at the time when we were kind of getting close to each other.
It's like, what if we both turned the same way?
Because we didn't know the rule was turn right.
In fact, we both turned left.
Oh, no.
Well, it still worked out.
Yes, because you turned the same.
But they blew a tire as they turned.
So us being super nice, we were just messing around.
We get out to see if everything is okay.
Then being a bunch of thugs get out to, you know, flex and be all scary.
But it happens to be where one of our number knew one of their number.
Okay.
And it ended amicably.
Uh-huh.
But yeah, that was my only experience at the Game of Chicken.
And no one died.
No one died.
So...
I can look back at a number of incidents from right...
Actually, you did both turn left because you turned left.
I wasn't driving.
Oh, but the guy you were with turned left.
Right.
And if the other guy went the same way as you...
They turned left.
No, they turned their left.
So they turned...
Now, if they turned their left, it would have been fine.
They turned right and you turned left and that's why you went in the same direction.
They turned my right, they turned their left.
Well, they...
We both took left turns.
They did not crash into each other, but they did.
But they did.
Yeah, they're fine.
I can't remember if we actually turned or not.
I think they might have just turned.
I'll have to find out.
Oh, my goodness.
Anyway, don't try this at home.
If we were to turn this into a decision tree, if we were to talk about this in a Game Theory
way, what would it look like?
It would be the payoff is...
Not being a chicken.
Well, the payoff of not being a chicken is one, but the payoff of dying is negative ten.
Oh, sure.
Oh, it's only negative ten versus one, huh?
It depends on how much you value your life, I guess.
If you're playing chicken, I'm assuming you don't value it all that much.
Or do you think they're immortal?
Okay.
That's exactly what I was going to say.
Teenagers think they're immortal, so...
That's a stereotype, anyway.
And apparently it's true sometimes.
At least in some cases.
Or you just don't want to live anymore if you live in Fort Collins.
It wasn't that we were just goofing around.
Okay.
And the guy that was with us was a very good driver.
Okay.
James Bond style.
So the result, if you both choose...
So if you both choose cooperate, what do you run into each other as how the situation is?
I don't think it has the same sort of payoff matrix as a prisoner's dilemma does.
Okay.
If you choose straight versus turn.
Mm-hmm.
So if you both choose straight, then it's negative ten for both of you.
Yeah.
If you choose straight and they choose swerve, what do they get?
Zero.
I'm assuming negative one for being a chicken.
Oh.
Negative one and you get a positive one.
Yeah.
And then if you both turn, then when?
I guess...
Is it zero?
I guess if you were to both turn at exactly the same time, it'd be a tie.
Yeah.
It'd be zero.
Yeah.
So that's like the best opportunity is you both turn at exactly the same time.
Yeah.
The stretching is the kind of game where you couldn't generalize your optimization strategy.
No, you can't with the prisoner's dilemma.
Yeah.
Because if you're both driving, say you hit cruise control at 75, and then you see the
other party throw their string wheel at the window at the same time you do, then you
just both have a few seconds to regret that you both made that decision.
Yeah.
But I think it's a...
Get out of cruise control, Steven.
But you can't turn.
I guess you can slam on your brakes, but sticking with the game.
Wait, you can't turn in cruise control?
What if world is this?
No, if you threw your string wheel at the window.
Oh.
So the strategy where you declare to them, I'm going to go straight until the game's
over.
Yeah.
You can't do that and want your opponent to do the same thing like you can in a prisoner's
dilemma if you cooperate.
Yeah.
But it's a demonstration of the fact that when you're playing these sorts of games, what
you want to do is change your player's opinion of you.
You want to portray yourself as the biggest, most badass, not going to turn no matter what
thing, no matter what person.
And whoever portrays that best generally is the one that wins because the other person
turns away sooner.
So it is literally about modeling your opponent.
And the problem with trying to model your opponents on things like that is that people
lie all the time and people can bluster and say, you know, that they're more courageous
than they actually are.
So they have an extra steering wheel to throw out the window?
Well, no, that's the whole point of the throwing the steering wheel away.
It is a credible pre-commitment.
The saying, I'm not going to turn away, is a pre-commitment, but it's not a very credible
one.
No.
It's a credible pre-commitment.
And that is why the United States does not, in theory, negotiate with terrorists.
Because if you never negotiate with a terrorist throughout your 200-year history, and if you're
willing to take giant losses in civilians or whatever to not negotiate with terrorists,
then there is not much incentive for terrorists to target you.
Because they're like, well, no matter what happens, we're not going to get what we want.
We're just going to kill a bunch of people and then get killed as well.
Well, that depends on what they want.
Well, yes.
If they just want to kill a bunch of people, then it doesn't matter.
That's the...
But pre-9-11.
Funny.
Funny.
Yeah.
In that nice little window, post-Cold War, pre-9-11, we had this nice window where that wasn't
the goal of the opponents.
Right.
The goal was to get your plane to Haiti or wherever so they can defect to the local government
or make a statement.
I don't know.
I don't really know what people did back then.
But yeah, that's the same reason why if you're in a country where there is a lot of kidnapping,
the best strategy for the society as general is to never pay any ransom, even though it
means their family members die.
Because...
For society in general, but not for you and your family.
Exactly.
That's not how things work.
If everyone in society never paid a ransom, there'd be no incentive for kidnappers to
kidnap anyone and kidnapping would stop entirely.
You wouldn't have to worry about being kidnapped.
But yeah, like you say...
Actors work on an individual level.
Yes.
And so people do pay ransoms because they do want their family members back and therefore
kidnapping continues to be a problem.
So that's...
Yeah.
It's the same kind of defection, except now it feels much more personal when it's your
family member that's been kidnapped.
All of a sudden you're like, I don't want to cooperate with all of society, I'm going
to defect because fuck you, I want my family member back.
That really does remind me of evolutionary stable strategies.
Yeah.
Yeah.
Like the Hawk and Dove game.
So our first item that we have to re-record is Hawk and Dove games, which Katrina was
telling us about.
Katrina, what is this Hawk and Dove game that you speak of?
The Hawk and Dove game is an example of evolutionary game theory and it was proposed by John Maynard
Smith and George Price in 1973.
So the way that it works is that there's two strategies, Hawk and Dove.
Hawk is the aggressive strategy and Dove is the peaceful strategy.
And the idea is that they're both meeting over a pile of food.
So when a Dove and a Dove, the two peaceful strategies meet up, they equally share the
food and they both get a positive three fitness.
When a Dove and a Hawk meet up, the Hawk is easily able to take more food than the Dove
because it's so aggressive and the Dove is peaceful.
So the Hawk gets five plus five fitness and the Dove is plus one.
However, when two Hawks meet up, they even injure each other a little bit and maybe even
destroy the food, they each get zero.
It's kind of harsh.
Yeah, it is.
It's a harsh world out there.
So if you're thinking about it as a prisoner's dilemma and there's player one and player
two playing those strategies, do you know which ones would be the Nash equilibria?
I would go with both going for Doves because I like cooperative strategies, but I recall
that was not...
It's not Nash.
Yeah, no.
So the Nash, you said it was a strange mixed strategy where some people are Hawk at a certain
percentage?
Yeah.
So, well, that's the evolutionary and the stable strategy is mixed, but the Nash equilibrium,
if you're again doing a player one, player two, and they can play either strategy.
So if player one plays Dove, player two is going to be happiest playing Hawk.
Right.
Right?
Yes.
Player two is a dick.
So that they'll get five plus five and the Dove person will get plus one, if alternatively,
and again, this is a situation like the prisoner's dilemma where they're really choosing at the
same time, but looking back, Dove person isn't going to want to change their strategy knowing
that the other player played Hawk because if they were to play Hawk, then neither of
them would get anything.
They would get zero and one's better than zero.
So the best, the Nash equilibrium strategy is Hawk, Dove or Dove of Hawk, right?
So there are two Nash equilibrium.
Okay.
Now, if you're talking about an animal species that has aggressive individuals and peaceful
individuals, it's easy to see kind of taking it from the Nash equilibrium that the best
strategy involves both Hawks and Doves.
That's why it's a mixed strategy.
As an animal, you don't really get to choose which one you're going to do at any one moment
or you might, it depends.
So it's going to go into a mixed strategy where the animals play Dove a certain amount
of times or a certain amount of individuals in the population play Dove and a certain
amount of individuals in the population play Hawk.
That this, I see what you're saying, what you're saying, how that that's Nash equilibrium,
but my human fairness instincts makes me very mad about this.
I do not want the other person to choose Hawk and instead of Dove because I would much rather
us both have three than me only have one.
Yeah.
I think that's why we need governmental regulation of Hawks and Doves.
I suppose, or I could just be a Hawk too and you know, take the hit in order to impose
costs on that guy.
You could.
Yes.
And if you're the only Hawk around, it sounds like a win for you despite probably not being
well liked.
Right.
If you're the only Hawk amongst a group of Doves, that's obviously a great situation
for you.
But if you're the only Dove around groups of Hawks that are just kind of fighting each
other all the time and distracted by that, that might be a good situation for you as
well.
Ah, because the other Hawks are never eating.
All busy fighting.
That's right.
Well, I'll say you could probably eat the dead ones if you're really hungry.
So.
Yeah.
So you can see how both of those strategies are good strategies in certain circumstances.
But interestingly, this is actually a bit different than the prisoner's dilemma because
the evolutionarily stable strategy is to have both Hawks and Doves at certain concentrations
in the community.
So that's what's known as a mixed strategy, which we have a mixed evolutionarily stable
mixed strategy, which we haven't talked about yet.
We haven't talked about mixed Nash equilibrium.
Right.
Which is that the best strategy isn't to, in an iterated game, isn't to always do one
thing to always defect or whatever.
It is to play one part with a certain probability and play the other with another probability.
Now if you're an animal, you are either a hawk or a Dove.
You can't show up and say, I'm going to play Dove this time, or I'm going to play hawk
this time, but this works on a population level.
Right?
Okay.
And the definition of an evolutionarily stable strategy is one that will persist in that
way and be resistant to invasions from other organisms with a different strategy.
Right?
If your neighbors come over and they're like, okay, yeah, but we do it this way.
They're going to be the ones that die out, not you versus if it's non evolutionarily
stable strategy, you guys are going to be eventually replaced by your neighbors.
And you were mentioning before the podcast that this sounds cool, but doesn't have a
lot of buy-in.
Oh, actually, no, this has a great deal of buy-in.
The issue with evolutionary game theory is that although there are tons and tons of situations
in which it very likely applies, because the world is a very complicated place, it's incredibly
difficult to figure out exactly what are the fitness results of this, what are the fitness
results of this specific action, and to disentangle it from the fitness results of all the other
actions that an animal is going to take or a population is going to take.
And just random things that no one can control.
Exactly.
So it's very difficult to actually measure this, although it certainly applies to a lot
of situations.
For example, it's applied very, very often to mating strategies.
Oh, right.
Okay.
Oh, do you remember what I'm going to talk about?
Probably, but go ahead.
Okay.
I have mentioned this before.
Okay.
Okay.
The one with the male that acts as a female to get into the harem?
So that's one of them.
Okay.
And that's a very common one.
So in cuttlefish, in various types of beetles, et cetera, et cetera, there are different
male morphs.
So there's a male that looks like a female versus a male that looks like a big burly
male, and it's the dominant male versus the sneaky male situation.
So those are the different strategies that are being played.
For our populations in which both of those persist, that there's the big dominant male
and also the sneaky male, we kind of assume that it's evolutionarily stable, given the
circumstances that, of course, we don't know, because it's really hard to measure the stuff.
I was actually going to give a gender swapped example.
Okay.
There are insects, and the ones I heard about were, I think, midges, but in dragonflies I've
seen this, too, that there are two different female morphs.
There's a female that looks very different from the male, and then there's a female that
looks quite a bit like the male.
Interestingly, in the species I heard about, including dragonflies, females suffer a big
fitness hit.
Let me take a moment to explain what fitness is, by the way, because I don't think we've
defined it.
Fitness is defined by the, that's pretty much the number of offspring that you have that
can themselves have offspring, so your ability to pass on your genes and, therefore, pass
on that trait.
And when we speak of fitness, we generally, I think, it's more an action that increases
your probability towards fitness, rather than-
Because we're talking about it in populations, rather than in individuals.
Or even if it's just in individuals, doing a certain thing might increase your, if it
was definitely to increase your fitness, then everyone understood all the time, but there's
probability variance this year, right?
Well, let's see, that's the thing, is that if there's a mixed strategy equilibrium, right?
If there's a mixed evolutionary strategy, stable strategy, then actually, what's best?
What's best?
It's not necessarily what's best, but what's stable, and that's also an important distinction
to make, kind of like in the prisoner's dilemma, what's best is not always what people actually
do.
It's all the same in wild populations and evolutionary history, too.
So, I'm sorry, what were you just saying?
Well, I was just saying that I think fitness, you know, so whatever it is, finding a successful
mate increases your genetic fitness, but say, whatever, sharing a pile of food if you're
a dove, you know, might probabilistically increase your fitness, because then you live
to, you know, find a mate another day.
Yes, absolutely.
Absolutely.
So, not all of it's so direct, like how many, like literally how many matings can I have,
like how many gametes can I spread, it might be, do I get more sunlight, I'm a tree, and
I have a large body size, do I get more sunlight, and therefore, have, be able to do more photosynthesis
to create more gametes, or, yeah, so there's, there's all sorts of indirect ways to get
there.
I was thinking, I think, in terms of evolutionary arms races, which is a fun thought experiment,
even, to think of trees, but that'll get us too far afield, I want you to keep going.
Yeah, trees are, trees are a great example of the idea of evolutionary game theory in
practice, and have been studied, and they're a little bit more simple, I guess, than some
of the other animals that you can study.
When they're not running around for food.
Do you want to get trees then, since they're simpler?
No.
No, you want to do the ground.
Okay, dragonflies are more interesting.
I want to talk about dragonflies.
Okay.
Plants are boring.
That's, well, depends on your opinions.
Because they're not running around and hunting and looking for mates and stuff.
See, I hate dragonflies because they're really big.
Well, I like dragonflies.
Oh, there are other bugs that I don't like.
Yeah?
Yeah.
I've never had a problem with spiders.
I don't freak out about spiders, but spiders don't fly at your face.
They're on the wall, and you just mush them.
Spiders are mine.
Watch your hands.
Dragonflies don't fly at your face either.
They turn at the last second.
Fuck those guys, and they're stupid chicken playing.
I am not a fan.
I'm a fan of dragonflies.
This example's going forward.
To reiterate.
I'm sorry I used the word reiterate, but I'm not actually sorry.
Dragonfly.
I can't unpack that.
No.
Okay.
No.
Why would you be sorry in the first place?
Okay.
Okay, I'll tell you.
Yeah.
Because, because iterate, like it's just being redundant in the same word.
Iterate is repeat.
To reiterate, re-repeat.
That doesn't make any sense.
Maybe you'd already iterated once.
So you are reiterating now?
Just keep iterating.
Hey, regardless, can we please continue?
Oh.
See, that's the one that actually kind of bugs me.
Yeah.
But you can, the language evolves.
You just go along with it, right?
Two females.
Two females.
One looks like the male.
One looks a lot like the male.
One doesn't.
In dragonflies, interestingly, females will actually leave the areas that have the most
prey density.
Because they suffer, well, the idea is because they suffer a significant fitness penalty
for being around males if they're not breeding.
Why?
Because males are really pushy and they want to mate a lot.
They have a high drive to mate.
So the females are busy getting, getting...
The females want to get food.
Instead of eating.
Right.
I see.
They're busy.
And not only that, it's not like a quick thing.
They hold on to the females and a lot of species do mate guarding.
So they will literally hold on to her until she lays her eggs.
Now, if you're a pre-reproductive female, and so this is just like a non-starter for
you, all of this is pure loss.
So the result is that a lot of pre-reproductive females just don't hang out where the mating
sites are.
They have less opportunity to get food, but get more food because they're not being forced
to the ground, grabbed onto their head by the, by the male's sursy little, little butt barbs.
So anyway.
So the ones that look like males don't have that problem.
Exactly.
Solution.
Cool.
There's ones that look like males.
They don't get, they might have a lower fitness in one way because they don't get as many
matings, but they do get to get stronger so that they can produce more eggs.
Right?
So that's a fitness plus.
Yeah.
So all of these, so that's like a fun, a fun Game Theory question too.
Yeah.
Bring this home for Game Theory for, for deep rational actor, or is this just about bugs
and animals and it's not something that we can generalize to the population?
Well, it's, it's an example of mixed strategy.
Yeah.
So it is an example of mixed strategy, but we can't actually calculate if it's mixed
strategy unless we're able to actually track the fitness for those different kinds of females.
And we're not.
Year after year.
I think a lot of people try.
I mean, I was looking at Scott Alexander's example of vampire bats feeding each other.
So you can go out, be a vampire bat and not get any blood.
Maybe you can.
You, what?
You said I could go out and be a vampire bat.
I do not have those skills.
A vampire bat can go out, look for a meal and not get a blood meal.
They expend a huge amount of energy.
It's a, it's a big cost to choose to hunt.
So when they get back to the roost, they're in trouble, but oftentimes they're taken care
of by a neighbor in their roost that has actually had a chance to get a blood meal.
So they're fed and that's good, right?
But it's, it's a, it's a minor cost to the bat that's feeding the other bat and a huge
benefit to the bat that's being fed.
And in that example, there was some, there were some almost anecdotal examples that was
seen that bats that hadn't cooperated by feeding other bats were in turn not being fed by other bats.
So that was.
So in any social species, this sort of thing evolves pretty quickly.
The idea of.
Of fairness.
Fairness and cooperating over time and punishment.
Yes.
So fairness and punishment are examples of mixed strategies?
Those are examples of creating super rational.
Yeah.
Nature having adapted.
Well, because that's what it makes sense, right?
There was a great example in Robert Sildini's influence on science and practice.
Robert Sildini and his book influence science and practice, which is a must read.
So the book discusses price gouging, which the idea is during a time of shortage of scarcity,
people will go nuts and try and procure as much of that resource as possible.
Whether it be guns, there's the running joke in the last eight years or really whenever
there's a Democrat in the White House that the president's a great gun salesman because
of all the proposed, you know, supposed gun bans or something.
There was a great experiment or wasn't an experiment that happened to be done in real
life where in a county in Florida, they banned phosphate detergents and people went berserk.
They would drive to the next county and convoys and come back with phosphate detergents just
and no one could have cared less until they banned nationwide now.
Oh, nice.
That makes sense.
But it was just a fun example that people couldn't have cared less until they were told they
can't have it.
Then they lost their mind.
I did the same thing actually when my phone provider went from unlimited data to limited
data.
Forget this and I switched carriers even though previously I'd only been using like less than
a gig a month, but it was just the idea.
I liked the option.
In full knowledge of what I was doing, I switched carriers.
Anyway, so in this example that Sheldini gives in the book, it was during, when was that big
oil crisis?
Mid-80s?
Oh, the big one?
That was late 70s.
Late 70s.
Carter administration, right?
Yes.
Yeah, that rings a bell.
So people were stockpiling gas, however you can think of buying drums and filling it and
stuff.
And for the most part, gas stations just dried up.
And then one guy realized, hey, I can make a quick buck and triple or quadruple the price
of gas.
And it sold in the short term.
People were still like, well, this is ridiculous, but I need it, so I'll go ahead and take it.
What worked out happening a few weeks later after things stabilized, people remembered
what a dick this guy was by overcharging, so they stopped going to him and apparently
his business closed.
All of that is hearsay from what was in that book, but taking it all is true.
It's a pretty good example of...
That only happens because he was the only one who did it.
That's true.
If everyone, if say they had all gotten together and said, all right, guys, want to be dicks
and they all agreed.
They don't have to be dicks.
That's as known as the invisible hand of the market.
If the supply goes down, the price goes up.
And economists think that this is like a good thing when...
Well, not necessarily a good, good thing, but when there is a shortage of something, the
prices should go up and keeping them artificially low just leads to worse shortages.
Because when the price goes up, first of all, people generally buy less if they don't need
it as badly.
And also the people that do have it are more incentivized to sell, right?
I believe Katrina, you mentioned that too.
Yes, absolutely.
I mean, I think that there are counter examples to that in real life.
So it might make sense in a lot of cases, but in the example of, say, if there was a monopoly
of three large businesses that owned 90% of the water in the country, like the drinking
water, and they were like, hey, let's all get together.
They have some backdoor deal where if we all tripled prices tomorrow morning, no one can
complain.
Does this tank girl?
No, but it could be.
But a real life example could be something like Comcast, right?
But that's in cases of monopolies.
And I think it's supposed to be a more general case that during when you have an inefficient
market, you price gouging is not to be that discouraged.
Yeah, because I mean, of course, governments and governments and people because governments
are made of people react with anger over people who are supposedly price gouging and making
price ceilings can cause a lot more damage.
And it's kind of tricky, though, because it does hit that fairness instinct again.
The water is always this price, and now all of a sudden that I need a badly, you're going
to screw me.
I can understand why people get emotional about it.
Absolutely.
I think it brings a lot of stuff to the table.
And I'm not an economist, so I guess I can't weigh in too much.
But I do think that you can pretend that there are like three companies like Comcast.
But for whatever reason, all the CEOs got together and they were like, hey, let's just keep doing
shitty business and double our prices.
I can't see a reason why it couldn't work in real life, even if there wasn't a monopoly.
But it is the same reason why you'll pay like $15 for a sandwich on an airplane.
Oh, yeah.
Well, they did the same thing with light bulbs for a while.
I don't remember how many years it lasted, but they had a very, I guess you could even
call it conspiracy among the major light bulb manufacturers to sell shitty light bulbs
because they could make more money that way.
Oh, yeah.
Stephen, if you're really concerned about the water issue, if there's not enough water
and it's really expensive, then, I mean, you could ask for some sort of outside regulatory
solution that doesn't involve price ceilings.
Like there's other options too.
For example, like you could have tax money buy up certain amounts of water and redistribute
them to people who are under a certain income level or something like that.
And there is also basic water, a base water amount.
Right.
And there is always kind of the worry that, I mean, the theory is that people will pay
more if they value the thing more, but for people that have a whole lot of money, it
doesn't cost them anything to bid up a bottle of water to $10,000, even for some really
shitty use when someone who doesn't have that kind of money needs it to live.
So, you know, another argument to have more equality in society, it makes the prices
more accurately reflect how people actually value things.
Yeah, that's true.
I mean, so, you know, like housing might be a more realistic example, because I don't
think we're in any shortage of running out of water in the United States.
But I think in Denver, I mean, maybe, yeah, there might be places like literal deserts
in the United States.
California is having issues.
But it's not like their taps are turning off.
They're just, you know, it's costing more.
But that said, actually, the issue is that it's not costing more.
It is kept artificially low.
And that's why there's water shortages.
People literally don't have any.
Oh, wow.
Okay.
Yeah.
That's a good example of where price ceilings are problematic.
Gotcha.
And I just had another really good example, but I can't remember what it was.
Oh, housing.
You know, I mean, we can, we can dive into this and the other.
Yes.
So game theory guys.
Yeah, right.
Sorry.
I wanted to mention a fun mixed strategy game that I'd heard of where you and six other
people wake up in a room, all separate rooms.
You're not together.
You've been knocked out and dragged there by the, you know, evil mad scientist economist
who wants to study your behavior, I guess.
Okay.
And you have a button and within the next 10 minutes, you can either choose to push the
button or not.
And if one person pushes the button, you're all let free.
But if no people or more than one person pushes the button, then you're all killed.
The question is, what's the best strategy in that case?
And ultimately it's just roll a die.
And if it comes up six, push the button.
And that way statistically, you'll, you'll usually have one person push the button.
Well, more often than if you had any other sort of decision theory, you will have only
one person push the button.
Okay.
So you go into a room, there's a die.
And the instructions are.
Yeah.
You know, you, you know, I got into six other people.
The economist isn't that nice.
This is why everyone has to carry an emergency die with them at all times.
I see.
Cause it's something like this could happen.
It's a possibility.
It's happened before.
And the cost of carrying a die are so low.
Right.
It's the same reason I carry everything that I carry adds up after a while though.
Yeah, it does.
What's your quick weight?
Right now, 72 kilograms.
Okay.
Dude, you got to get the strong back perk.
I'm up to 300 pounds.
Oh.
Oh man.
Okay.
That's a good answer.
Hey, want to bring this home to game theory?
I want to hear an example of a mixed strategy that people actually do or can do or whatever
it is, right?
Yeah, the box is you guys because I don't really know much about the box.
No problem.
The box is a solo game strategy situation.
It's more of a morality game than having anything really, I think to do with game
strategy.
Well, morality can have something to do with game strategy, right?
It seemed more like a joke thing.
Like Steven was like before that guy even got to the plus side of the box.
Oh, yeah.
Well, the joke is making fun of the box.
So the idea of the box has been a moral, what do you call it, a moral dilemma for a long
time.
So the idea is someone comes in and they offer you a box.
If you hit the button, then one person in the die, one person in the world who you do
not know, a stranger dies, but you also get a million dollars, right?
Yes.
And the joke was you said one person in the world you do not know dies and Steven goes
push the button.
Yeah, there was someone made a video where they hadn't even finished their pitch and
they just kept pressing the button over and over.
It's hilarious.
We'll link it.
Yeah.
Was this a Saturday morning breakfast cereal?
Oh, no.
No, this was an actual video.
But it could have been also a Saturday morning breakfast cereal.
Okay.
It was also a serious movie with Jennifer Lopez.
Oh, what was it called?
The box.
Oh, it was literally called the box.
Maybe or the decision or something.
We can take a look.
Oh, yeah.
There was totally a movie about this.
So that's what these things were making fun of was the movie, which was made based on
a very simple moral dilemma.
Yeah, you know, kind of the idea that if it's somebody else's life who cares, I get
a million dollars.
I'm the individual making this decision and that's considering that I am a strictly selfish
economic actor, then should definitely take the million dollars.
Right.
Yeah, I think that's it.
I'm not sure what its real connection is.
I'm sorry.
Go ahead.
Yeah, of course in the movie, they press the button and it's the person who dies is
like the doctor of the sixth son and the son's going to die.
And then it's a whole horrible nightmare.
Oh, of course it is, right?
Comes back to them.
Yeah.
Yeah.
I mean, it wouldn't be so much of a movie or just some random person in some third world
country died.
You know, we're trying to press the button.
It had to be somebody that would turn around and fuck them.
So sounds terrible.
Yes.
So I'm not sure what it's real time is to to game theory other than being just a fun
little intuition pump about, I guess, people's cost-benefit analysis versus their own money.
I don't want to run a person dying.
Of course.
There is also solo game theory, right?
You don't have to have two players.
Don't you?
No.
No, you can just do it on your own.
It depends on the game.
You can just go through a decision tree on your own and make the right decisions.
Oh, sure.
But isn't part of game theory like anticipating the actions of other agents?
Not necessarily.
It's you can be playing against the world, which is set up in a certain way.
Or I guess playing against yourself.
All right.
Okay.
Fair enough.
What was I going to say about...
Oh, the joke about the box would be, I guess, if you could press the button half a dozen
times and get $6 million to charity and still probably...
Save many more people than the six people you killed.
Unless like the 60 billion killer, like Elon Musk, Bill Gates, and, you know, just save
billions of people.
Everyone would want to meet you if you announced that beforehand.
Oh, man.
Yeah.
Yeah.
That'd be a good way to get around this.
Strangers.
That's how you manage meeting with the press.
You're like, hey, I'm going to press this however many times.
So you're going to want to know me.
That's true.
I'm desperate to be popular.
Right.
Okay.
So this was something that I brought up originally while the music was playing.
So I'm going to go ahead and bring it up again now, even though I don't remember exactly
how it came up.
This, I guess, is not as controversial as I first thought it was.
I was really hesitant to talk about it when we first recorded.
And now I feel a lot better because of the feedback I got from the two of you.
But I had been taught in high school that the official reason the U.S.
gave for dropping nuclear weapons on Japan was the actual reason.
That was still being taught the party line.
That it was the bombs were dropped.
Grandparents dropped those bombs to hasten the end of the war and save
ultimately hundreds of thousands of lives.
And nowadays the consensus is that that really doesn't seem to have mattered.
Japan was already discussing surrendering.
And it's not like the bombs themselves changed how the war was.
How the war was being fought because we were already both sides were already
leveling cities, wiping out entire city populations, destroying the land in
order to get to the factories that created the war's machines of war.
So nothing tactically or strategically was different about it,
aside from the fact that I guess now you only need one bomber instead of a fleet of them.
But the effects on the ground were identical.
The real reason that the United States decided to use the weapon was to show
the Soviet Union since the Cold War was already ramping up before World War II was over,
that we had these weapons and we were willing to use them on civilian populations.
It was a sort of, we were talking earlier about how you pre-commit to your opponent,
showing them you're really serious by throwing your steering wheel out the window of your car.
It was obviously not that big of a commitment, but it was a very similar thing where
you are demonstrating your willingness to do this so they will take you more seriously
and maybe swerve before you were to swerve.
That's interesting. Do you think when people plan bomb droppings and stuff like that,
they actually write things out in terms of lives saved versus lives spent?
In general, I don't know, I don't think so, but at the highest levels,
I believe that was the thinking in this particular case.
Because hundreds of thousands of people were killed in those bombs being dropped.
So about 145,000 people in Hiroshima alone.
Right. It was horrific. They were wiping cities out everywhere and it was a nasty war,
one of the worst we've seen for civilian casualties in history, I think.
I mean, it depends on how far back you want to go.
It turns out that even the 20th century with all of its horrors wasn't as bad as, say,
the big wars in Persia and Greece a couple thousand years ago.
But surely we had more people to kill.
That's true. Probably in terms of percentage of population, it was different rather than absolute numbers.
That is the point that Stephen Pinker makes in The Angels of Our Better Nature,
that your odds of dying as a civilian now are so much lower than they used to be.
So even how much it sucked, you're still better off for your risk of dying.
Well, it turns out just about everything, but especially being killed in a war or as collateral damage.
Did you guys ever see Dr. Strangelove or how I learned to stop worrying and love the bomb?
I sure did.
I sure didn't.
Well, it's great, but one of the major conceits of it is that the Soviets are creating a dead drop weapon.
That if a nuclear explosion is detected anywhere on Earth, they launch all their nukes at the U.S.
Yeah, it's fully automated. They cannot control it.
And it is literally throwing your steering wheel out the window.
And apparently there were plans to actually implement something like this that they never quite got around to during the Cold War.
That's a relief because it's only a matter of time before North Korea decides to test its new nuclear weapons.
And if all of a sudden all of Russia is launching us because of that, that would hardly be a dynamical situation.
They're always testing air quotes, by the way. They're always testing their nuclear weapons in North Korea.
Yeah, I think it was specifically if there was a nuclear explosion detected in Russia.
I don't remember the details, but in any case, they decided to still keep some humans in the loop rather than to make it completely automated.
That's a relief.
But, and then my teachers would say, but there's also evidence that that's not the case.
Okay.
I kind of ended it that.
Okay.
I'm also curious where people are giving these answers from.
I imagine different parts of the world might teach it differently.
So, depending on international audience, I'd be particularly interested in hearing from them.
On a side note really quick, there was a great...
It was in one of the Scalaxander posts where he talks about a game that was on TV where they played Christmas dilemmas.
It was over brief cases of cash.
Did either of you guys read that?
Yes, yes. The Golden Balls.
I was looking for that, but I couldn't find it because the guy at the end...
It's not what it sounds like.
The guy at the end had the perfect strategy.
Yes, we have to show these people. We have to put a link up to the Golden Balls on our website.
What was the perfect strategy?
So, it is a game show that literally is the prisoner's dilemma at the end.
Yeah.
The two people have messed a lot of money throughout this game show, and then at the end, you both get two balls that are painted gold.
That's the golden balls.
Uh-huh.
Inside one of them, it says Split, and in the other one, it says Steel.
Uh-huh.
If both players play Split, they split the money evenly between them.
If one player plays Steel, and the other player plays Split, the person who plays Steel gets all the money, the person who plays Split gets nothing.
If both players play Steel, they forfeit everything, they both go home broke.
Yeah, and so in all previous episodes, it had come down to people saying,
No, dude, I'm totally gonna split. I'm gonna split, man. Don't worry about me. I'm gonna split.
We just split. We're gonna split.
And going back and forth like that for like three minutes, they finally play the balls, and then you see, did they both split?
You know, and do they share the money, or did one person steal, or did they both steal?
That's where all the drama comes from.
Uh-huh.
In this strategy, the guy goes,
Dude, I want you to know I am a man of my word. I would never go back on my word.
This is the most important thing to me, so I'm gonna steal.
And he was like, what? He's like, no, I'm gonna steal. You should split.
After the show, I'll give you half the money, but I'm gonna steal.
And he's like, this is stupid. If you're gonna split the money anyway, just play the split ball.
The guy's like, no, I'm not gonna do that. I don't roll that way. I don't trust you.
I'm gonna play Steel, and after the show, I'll split the money with you.
And there was, yeah.
Did it work?
It did. The other guy played Split, and the guy who said he was gonna play Steel, played Split as well.
Yeah.
Because from the other person's point of view,
what I'd heard is that the guy had already played his, whatever, he had touched his golden ball.
No, I don't think he had.
Oh, maybe he hadn't. I thought maybe he had to play at the same time,
but the point was he had pre-committed to saying, look, I'm stealing.
You're guaranteed to get nothing if you try and change the outcome.
Right.
So you can say, well, I don't really trust you, but that's already fine. I already stole.
Okay, no, they had not played the ball yet.
Oh, yeah. So he's like, if you want, I mean, I've already pre-committed to stealing.
So far, that's the purpose I've already stolen.
Yeah.
So if you want a probability of getting anything, you just have to split.
Right.
Whether or not I defect on my word about splitting it with you after the game doesn't really matter.
I'm already, I've already pre-committed to stealing.
Exactly.
So this brings to mind.
What's up?
Yeah.
There's a TV show that I've been watching a bit of called Cutthroat Kitchen,
where they spend money to screw each other playing some kitchen cooking game.
That's kind of funny, and I've always, there hasn't been a player who's played any games like this yet.
I don't know if they can exchange money between players.
Not expressly prohibited in the quick rules that they give to the audience,
but it would seem to make a lot of sense to me.
Someone buys some crazy sabotage for somebody.
They're going to give it to me.
They bought it for $2,000.
Hey, I'll give you $1,000 to give it to that person.
Right.
If you get half your money back, I don't get it.
They would, you know, but no one's done that yet.
But I think it'd be kind of funny.
It's got to be against the rules.
I don't know.
Or people just might not be that smart.
People might just be playing it.
Yeah.
So do we want to talk about the probably, in my opinion, one of the most interesting games
and the one that takes this pre-commitment thing to a weird place?
I think we have to.
Okay.
We're talking about Newcombe's problem.
Okay.
You've heard of Newcombe's problem?
Boxes?
Yes.
Yes.
Awesome.
Who wants to set it up?
Go ahead.
Very well.
Do it.
All right.
So Omega flies down out of the sky.
Pause.
Yes, there's Omega.
Omega is the perfect predictor and who has a lot of money at their disposal because they
want to go around and make, uh, play this game with humans.
Omega may or may not be human.
We can't tell.
But we do know Omega is really good at predicting shit.
Yeah.
Maybe not necessarily perfect, but really good.
Yeah.
And we have reason to believe that.
Right.
However, Omega is not God.
It does not have supernatural powers.
Omega is like data from Star Trek.
Really freaking smart.
But if he's not in the room, he can't magically affect things that aren't in the room.
So Omega comes up, looks at you for a bit, does a brain scan or something.
He's like, okay, I've predicted what you're going to answer when I ask you this question.
Then he fiddles with two boxes, presents you with the boxes.
One of them is clear.
One of them is opaque.
In the clear box box, you see $10,000.
And Omega says to you, you can now choose to open only the clear box, only the opaque
box, sorry, or you can choose to open both the boxes.
And then he leaves and he's left a note that says, if I, if I have predicted that you will
open both boxes, the opaque boxes empty.
If I have predicted that you will only open the opaque box, then the opaque box has a million
dollars in it.
At this point, he cannot change what's in the box anymore, and you have not yet made
your decision.
The opaque box either has a million dollars or doesn't.
And the question is, are you going to open both boxes or only the opaque box?
So you've gotten additional information here.
And the additional information you have is that Omega is fantastic at predicting these
things.
Yes.
He's been right 9,000 times out of the past 9,000.
Wow.
That's awesome.
Yeah.
I would say that, um, given that he's really awesome at predicting things, you should only
open the opaque box.
Okay.
That's, that's to me, that seems like the only answer.
I want the million dollars.
But is there a still man case for taking both boxes?
I want the million and 10,000.
Well, the case is that he can't change what's in the boxes.
But he's predicted what you'll do with, with, with very good accuracy.
So that's me.
He's kind of like, I'm not going to even try and hedge our bets on outwitting the predictor.
I'll just take the million.
I, I personally agree with you, but I think you said we know someone who would take both
boxes.
Do we?
Patrick?
Yeah.
Did not Patrick say that?
No.
Wait, who, which Patrick?
I, I'm soon shaping.
Do we know another Patrick?
No.
Okay.
And also no.
Oh no.
He would just take the one.
Yeah.
And that's because he sets up the problem differently.
Oh, how does he set it up?
I don't know, but in a way that trusting the additional information that Omega's very
good at predicting things would result in getting a better thing from the two boxes.
Yeah.
Patrick and I were talking about this last month and it was about.
Well, do, do the his setup of his setup is better.
Well, it wasn't actually like this one.
I like this one better because it's more clear cut.
His was more tenuous on how much you can trust Omega's predictive prowess.
Okay.
And that, that makes sense.
Or, or I guess by what method Omega makes the prediction, you know, maybe I can't remember
what his setup was, but I mean, there are other standard setups that I've read about
where like say.
But you don't have to know.
You don't have to know all you have.
You can just have the evidence that Omega's fantastic at making predictions.
You don't have to know how.
And, and he knows that too.
I don't want to, let's not put words in his mouth.
No, no, I'm, I'm, that's why I'm kind of diverging from what he said because I can't
remember exactly what it was.
I remember that when he and I were talking about it, one boxing, the Newcombe problem
was much less obvious.
But it seems to me that depending on how much you trust Omega's predictive powers and
really, I guess that's it.
Well, I mean, either Omega got it right or he got it wrong.
The box is already in front of you.
Right.
So if he got it right, you might as well open both boxes and get the million dollars and
the 10,000 extra.
But like in an absurd way to put it, like if Omega's strategy is like flipping a coin.
Yeah, that's true.
I mean, if they got it, I don't know.
I, I, I like, I like,
And if you get it, if he got it wrong, then you open your one opaque box and there's
nothing in it and you're like, fuck, you got it wrong.
But if your odds of that are like say one and two because Omega sucks at predicting
or they're one in a hundred thousand because Omega's great at it, I'd rather take the,
you know, the much better chance of getting the million dollars.
Right.
But if you're, if your predictor is just flipping a coin, then you take both boxes.
Right.
Of course.
So, but the difference is the amount of information that you have going into it.
There's one more thing that I wanted to cover.
That's okay.
Yes, please.
The coordination problem.
So if there's a lot of different things, Scott mentioned a game show where people
had to figure out like to win two different people would be dropped in New York and they
had to actually choose where to meet up and show up at the same place.
I don't think this is game theory, but this is also a thing that we should cover at some
point.
So we might as well do it now.
It's, it's kind of game theory.
Okay.
Or it was certainly covered in the game theory sequence because there are a lot of situations,
there are a lot of places where they could both meet and they would be successful.
They wouldn't regret that choice at all and they wouldn't change their answer given that
the other person chose that.
Well, the fascinating thing is.
So that's the, those are all Nash equilibria.
The fascinating thing is the only thing they are told is you have to meet a person in New
York tomorrow.
Yeah.
And that's it.
And, and I think it was like half the people or so managed to do it anyway because.
By showing up at the Empire State Building at noon.
Right.
Or Grand Central Station.
If you think of New York, those are like the two places where you meet people, right?
And noon is just kind of the time that it, that sticks out in the day.
And you're like, well, sure, let's try that and it manages to work.
Yeah.
And it was the guy that, uh, the guy that came up with that is Thomas Shelling, which is why
it's Shelling Points, places that people naturally converge to.
Yeah.
I was at the post office.
Why does that come to mind when I think of Shelling Points?
Maybe if you're thinking of very small towns.
Maybe for some reason I think I have this, my node with Shelling Point immediately jumps
to post office in my brain and I'm not sure why.
I'll see if I can Google those terms together if anything comes up.
It reminded me of another insect example.
Okay.
And, um, and the reason that Shelling Points came up, I think is also when we're talking
about cooperating or defecting or, you know, there are evolutionary things that people,
that people are kind of programmed to do to help them coordinate for all sorts of different
games, right?
And that applies to animals also.
For example, butterflies.
A lot of butterflies are known as hill toppers.
All right.
Do you know why that is?
They go to the top of the hill?
Yeah.
Why do you think they go to the top of the hill?
That's where other butterflies are?
That's right.
All right.
Um, finding mates, especially if you're very quiet and maybe there aren't that many of
you, is, and you want to find mates that aren't your brood sisters and brothers, right?
That also hatched at the same time as you because you don't want inbreeding.
Um, it's a problem in the animal kingdom and animals have different ways to deal with that.
Some of them have like flashing light signals.
If they are angler fish, for example, or pheromones, or there's all sorts of different
things that they can do.
But if you're a butterfly, you don't get any of those.
Well, it depends.
Oh, okay.
But, um...
If you're a lane butterfly, you don't get any of those.
But you're cool enough to know that the other butterflies are going to go to a meeting point,
and that meeting point is at the top of the hill.
And that's where you can find a butterfly to mate with and increase your fitness.
Cool.
Yeah.
And then you can leave the trait of going to the top of the hill to your offspring.
Actually, to kind of bring that towards humans, because I know that you're interested in bringing
that towards humans.
Um, you know, those little midges that swarm together, they're a bunch of swarming flies
if you're maybe out on a soccer field or a park.
Well, one of the reasons they swarm around your head is because you are locally the
tallest thing, and they're finding other midges to mate with.
Right, now I'm less annoyed at them all of a sudden.
I'm supporting sex orgy.
Or mid-orgy.
Literally, every time I walk through a cloud of them, I'm thinking, you have the entire...
Like, not the entire sky.
You know, it goes so high.
Right, right.
But you have literally everything around, you know.
It's like, why are you flying right in front of my car?
Exactly.
You have the whole sky.
Why are you right in my face?
But now I get it.
Now I can sympathize a little better.
Yeah, they're very responsive to even small changes in topography to find other flies.
In order to avoid a horde, what do you call it?
A gaggle?
A mix?
A swarm.
A brood?
A swarm, that's the word I was looking for.
Midges, you could say, carry an umbrella, but rather than try and fend them off with it,
you'd hold it above your head?
Yeah, so, I mean, that's what I thought.
In practice, I've tried that, or also getting next to somebody who's tall and then crouching
down.
Okay.
I think it works to a certain extent.
Get a small person, put them on your shoulders.
I think that once you're kind of in that situation, they kind of, once they find the tall thing,
they also go out to the sides and just kind of around.
So you want to just start being short and continue being short in that situation next to a tall
person, potentially.
Your best weapon is, what is that, bug spray?
DDT?
No, not DDT.
You spray that everywhere you go.
DT.
DDT's very different.
Okay.
Kind of past the side.
I think we should wrap it up.
I think so too.
I think we've gone over, actually.
I'm a little disappointed that the newcoms thing, I was hoping we knew someone who would
one-box on it.
Julie Galeff is what you claimed before.
Julie Galeff, I mentioned she went on a date where someone tried to newcom-box her.
They set up a newcom-box thing for her for fun.
I can't offer you such a one-box or two-box or not.
Yeah.
I'm super skeptical at this point now that I've had that conversation with Patrick,
people say that they one-box or two-box, that they actually are just coming from a different
set of presuppositions, right?
Okay.
Just a different set of priors.
Yeah.
I think they'd have to in order to one-box.
Or excuse me, to two-box.
Because the answer's so cut and dry, but I'll look into it.
I'm sure there's some good reason not to.
Oh, I'm sure there is.
I think that if you were...
I know a lot of classical, I don't know if it was classical economists or classical theorists.
Some groups said very strongly, you should two-box because Omega cannot change their
decision anymore.
So what?
You should go in...
You should go in pretending that you would only open one-box.
You dress like a hose.
You dress like a one-boxer.
Yeah, exactly.
That's what makes it somewhat relevant to me how Omega comes up with a decision.
Because if it's like, say, a brain scan when you enter, well, you can try and change your
state of mind or something, or whatever it is.
But if it's just some alien magic power, and they're like, I'm pretty positive you're
going to do this, well, I don't know how to trump that by just sitting there thinking about
it.
Yeah, you don't really have the resources.
I don't know.
You could always try making the stakes higher.
I remember the True Prisoner's Dilemma post where Ali Eiser compared paper clips versus
saving a million people's lives.
Maybe it could be like you have a bunch of sick friends that are going to die, and the
opaque box is one pill that will cure one of them.
And, well, in the clear box is one pill that will cure one of them.
And the opaque box is either zero or ten pills.
Or five.
Or zero or five, yeah.
And so all of a sudden...
So if you get both of them, then all your friends will survive.
But if you only pick the opaque box, then five.
This episode is a bit more of a primer on some of the terms in Game Theory and not so much
a Now You're a Game Theory expert.
No.
Just this way people will know what we mean when we say one boxing or cooperating and
defecting.
I feel like I know more about Game Theory.
You have to be prepared for this episode.
Same.
So we'll compare the benefit to you by linking to basically everything we read to prepare
for this episode.
Awesome.
I think we did okay.
I think we did too.
So stay tuned for listener feedback, which we will get into now.
Now is the part where we do listener feedback.
Starting with a comment from Condition of Man, who says that the Uniform Code of Military
Justice Article 134, paragraph 62, still prohibits adultery in response to us saying that polyamory
was not illegal in the United States.
Was that Polly Episode 10?
Either 9 or 10, one of them.
Okay.
9 and 10.
9 and 10, I guess, both of them together.
Yeah.
So we looked into it and it's really vague.
It's more of a conduct unbecoming if it causes distress in the military or brings disrespect
upon the military.
So when they actually enforce it is very much up to the courts.
I guess that's why half the army has not been kicked out of the army yet.
I know you go overseas for a year.
What do you expect is going to happen?
But it is technically in there so they could drum you out for being poly, even if it doesn't
bring any drama into the military or any disrespect on it as long as some judge says that he doesn't
like you or he doesn't like what you're doing.
Well, that's too bad.
Yeah.
There's a lot of weird stuff around marriage in the military.
I know somebody who married into the military specifically to get a financial gain.
Yes, same here.
I think everyone who knows people in the military knows that.
I know one person in my age range who's in the military and he got married for love.
But the fact that they're each getting like a bonus thousand a month was, you know, there.
And upgraded housing.
Yeah.
I've heard once you're married you get your own little special area instead of having to
share a bunk with, you know, seven other dudes.
That sounds like a sweet deal.
Yeah, yeah.
There's a lot of incentive in the military to get married.
If I ever fall in hard times.
I think.
If I ever marry someone in the military.
Right.
Would you like to read that top one?
This is for our voting episode which just came out.
Yes.
I'm surprised there was no talk of Brexit.
But maybe this was recorded before that all went down.
If more rational minded people in the UK voted last week, it would have been a different story.
I don't have much of a comment there.
Did I say the commenters name Eddie Flash?
You did not, but now you have.
Now I have.
That was for episode 11.
Yeah.
And that was, we do record generally three to four weeks before they go on air.
So that was definitely before the Brexit was in the news.
But I'm interested.
I would have been interested to hear about what our guest thought about that.
I know personally, I don't know what he felt about the voting, but I do know that he was very disappointed with the results.
I expect based on how many people right now are having breg regret or whatever they're calling it.
Regret.
Regret.
That maybe to his point, if people who are less informed would have stayed home, this might not have happened.
Yeah.
Because I think that was one of Tim's things, if you aren't very informed on the subject, just don't vote.
Yeah.
I don't think that this disproves this point in any way.
That's interesting.
I don't know much to say.
I just like the, I mean, I'm sure that there's a whole big world collapse over this, but for me it's just kind of fun to watch.
Kind of that perverse part of me that wants Trump to win the election.
I just like seeing what happens.
Burn it all down.
Raise it up again from the ashes.
Burn to the ground.
Another comment on the voting episode is from Jess.
Jess said, I love this podcast and all the work you put into the relevant links and follow up, smiley face, keep up your excellent work, guys.
Thanks, Jess.
You rock.
Thank you.
Yay.
What else we got?
Do you want to read this one from Peter?
Sure.
Okay.
This is regarding the polyamory episode 9 and 10.
If there's any chance at all that adding secondary partners would devalue the bond with my primary partner, it's just not worth it.
The costs are too deep and the benefits too shallow.
They go on to say, I'm not defending strong claim, adding secondary partners weakens your primary bond.
I'm defending the weaker claim.
If there's any chance adding secondary partners will weaken my primary bond, it's not worth it.
So my answer is there are a lot of things that could have a chance of weakening your bond with your primary partner.
You know, things as common as getting a new job or a promotion.
Things like eating a different kind of ice cream.
I don't know.
There's a lot of different things that you could do.
So it's one of those situations where you kind of want to weigh things out.
Things like having different tastes in music.
Yeah.
Although I guess that's not a thing you would do.
No.
But maybe like one of you would go to a concert and the other one wouldn't.
I don't know.
Bonds are weird that way.
Personally, I honestly, it depends in a large way, but my bond with my husband of eight years in most of the situations I think has gotten stronger and stronger even when I dated people who were not really good partners.
Yeah.
I wanted to say a similar thing to that.
In my experience, having a secondary partner actually made the bond with my primary much stronger.
Because when you only have the one primary partner, the thing you're comparing them person to is the idealized partner.
You know, the person who's perfect in every way and only exists in fantasy.
And when you have an actual real person to compare them to, they're better in some ways.
But in other ways, you're like, Oh God, I am.
I am so glad I don't have to live with this person because I couldn't stand that one little thing.
And the grass being greener on the other side.
Once you actually go to the other side, you're like, Oh no, the grass here's about the same.
I like that side where I was on there before too.
Let me tell you why I felt like it strengthened my bond with my partner with my existing partner.
I'm not going to do the primary secondary thing.
Right.
Like, I got to see him in action being an awesome, supportive, kind human being and sticking with me through thick and thin and reaching out to people.
And yeah, there's just, there was so much awesomeness that came from that.
So yes, I guess there's a chance you can make things weaker, but there's also a chance it can make things stronger.
So it's not just the tiniest chance that it might make it weaker.
There's also a chance that it could make it stronger instead.
Also, you have a team dedicated to dating you.
I would say as far as, you know, really anything that you're considering that might have an impact on your relationship.
It's the kind of thing that you should talk about with your current partner.
And I think there are some people that Polly would weaken a relationship, others that it would strengthen it for.
The only person who can really decide that is you and your partner.
And then one final thing, which I hope will be our last thing about the Polly because we've just gone on for about it so much.
There was a person whose name I'm not sure I'm going to say, but in the second episode 10, near I think the 10 minute mark,
I started mentioning some things that might make someone undesirable, I guess, in a hypothetical sense.
And they responded negatively to that saying, Jesus, I can't believe you just said that and online as well.
Many sexually evicted men are not like the stereotypes you pulled out of your ass at all.
And I felt bad about that. So I wrote something in reply and I'm just going to read what I wrote.
I just want to interject very quickly that what Anjash said was, if you are a terrible person, then you need to up your game.
Yes.
Pretty much.
Yeah.
So he's referring to.
Yeah.
So my reply is that I apologize for my careless words.
We've been recording for well over an hour and a half at that point and I was getting tired.
And I flailed around searching for descriptors that would commonly be associated with people at the very lowest levels of desirability.
Quote, I did not intend this at all to include all men who are involuntary celibate.
I have read both Scott Alexander's and Scott Harrison's essays on this and I sympathize very strongly with them.
There are many men who are not awful and undesirable and who still have immense trouble finding a partner for a number of other reasons.
I meant only to say that the most unlikeable people don't do any better under monogamy than they do under polyamory
and trying to force a solution of every woman must be paired with one man so that even the least desirable man is assured a woman does not work.
As often people would prefer to stay single rather than the alternative.
In addition to not working, it is also reprehensible because it again treats women as objects to satisfy men rather than as self-directed agents.
And since a systemic solution is untenable, while a personal solution is viable if potentially difficult.
I was trying to convey that but my tone was crass and I'm sorry.
I have one more comment I want to dig up.
Do we?
We have exactly one review on iTunes.
Do we?
It's a one-star review.
Oh no!
They made it 30 seconds into the first episode.
Okay.
One-star.
The title is stupid.
The submitter was VU alumni.
I couldn't make it past the one-minute mark on this podcast.
Within the first 30 seconds, the host refers to you as the listener as not smart enough and that's why you're still listening to the podcast.
Oh shit.
Dude, I told you.
What would possess you to think that when you are trying to build a fan base, the very best way to do that is by insulting the very people you're courting
to patron your podcast.
Bum bum bum.
I am the suck.
Well.
Did they not wait until I called you on it?
Apparently they did not.
Apparently they had their finger ready on the pause button and ceased listening immediately.
So those of you who are still here, glad you're able to make it past that.
We don't think you're stupid or that we're stupid.
We just think that we're crazy.
Yeah.
We were freestyling it in the first episode.
So here we are now.
That said, this is our friendly reminder to please go on and give us some reviews and ratings on iTunes if you think the podcast is worth 30 seconds of your time to do so.
Yeah.
Even if you think we're worth two stars, it'll increase our average rating.
It's true.
Sorry guys.
No, you're good.
It's okay.
Okay, this has been the Basian Conspiracy.
Goodbye everybody.
Bye.
See you soon.
See you in a couple weeks.
Bye.

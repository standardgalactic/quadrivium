Hello everyone, my name is Memming and like many of you, I want to understand
how the brain works. In this talk I will introduce a family of machine learning
methods we've been working on to make modern neuroscience real-time. If we go
back in time a little bit, in the old days of single neuron recording,
electrophysiologists watched and listened to neurons during experiments
which provided insights and in turn influenced experimental design within a
single experimental session. Here's an example of Hubel and Wiesel who won a
Nobel Prize in 1981 for their study of visual cortex. I'll show you a short
video clip of their experiment. While they're recording from a single neuron
they would play the amplified signals through the speaker and listen to the
neurons' spiking activity which you will hear. You will also see them drawing
while mapping the visual motion stimuli the neurons respond to as they
manipulate the light bar stimulus.
Okay so you just saw Hubel and Wiesel mapping this neurons preferred
orientation and direction. As you can see these mapping experiments were very
interactive and dynamic. They were changing the stimulus based on what they
were hearing. So this very intimate connection between the neuron and
the experimentalist. However in the current era of massively parallel
recording the number of simultaneously recorded neurons are doubling every six
years and hundreds or more neurons are routinely observed using various
techniques and designs such as the Utah Array and the NeuroPixels probes. In these
settings it is very difficult to listen to every individual neuron because there
are hundreds of them. Because of that the experimentalists really can't connect
with the neurons and the subjects. What they do instead is they hope that if you
collect a lot of data and the post hoc analysis they will use machine learning
analysis and so on which will allow them to better understand the brain and
answer their scientific questions. This large time lag between alternating
cycles of experiments and analysis of such data sets is a major obstacle to
rapid scientific progress. Therefore I believe the field needs new sets of
tools analogous to oscilloscopes and speakers that the experimenters were
using before for single neurons. So for example if you have neural recording,
high-dimensional neural recording, we could have a signal processing pipeline
such that in real time it'll spit out a monitoring signal such that the
experimentalist can get an intuitive idea of what's going on in the brain
while they're doing the experiments and they can change the experiments. Or you
can use the output of these estimated descriptions to use and generate feedback
control signals. So in part one of this talk I'd like to tell you about machine
learning tools for inferring neural trajectories and neural dynamics. What
is neural trajectory? So even for very high-dimensional neural recordings, often
their effective dimensionality is much lower. This could be due to low complexity
of the task. The task that the animal or the subject is performing could be super
simple like a working memory or decision-making tasks are typically
designed to be very simple. It could be because of the biophysical network
constraints which limits the effective degrees of freedom of the neural
states. Regardless, we often see both the neural states being very low
dimensional and the variability of course also being very low dimensional,
although it's not always the case. So in practice what we do is we use
dimensionality reduction techniques such as PCA, the principle component analysis
or VLGP in this case, to extract the low-dimensional neural state evolution
these trajectories and analyze those which has been very successful. In
addition, what I really want to know is I want to recover this underlying law.
There's a hidden dynamical system law that governs these neural trajectories
and this is typically called the state-space modeling. So suppose we
have some data, this is neurons over time, this y here, and we use a
dimensionality reduction techniques such as PCA to recover a blue line. This is
the neural trajectory over time starting from the green dot ending at the red
one and if we collect a lot of these trials or if we recorded for a long
enough time, we might be able to slowly see that there's a ring shape here where
all the trajectories are attracted to. So we might be able to infer from this
that there is an underlying dynamical system that's represented by this vector
field here. All these arrows are pointing towards this ring here where the
color represents the speed of the flow. So this vector flow is fast outside and
as it reaches the ring it gets slower. So this is actually representing a
theoretical model called the ring attractor which are used for, for example,
storing head directions, representing head directions and orientations. So if we
can get the picture, get a picture like this, the vector field like this, then we
could understand how neural computation is implemented as a dynamical system and
I'll be very happy. So how do we get to here is the main question. And once we
get here, we can also describe how we could generate data based on this
description. So the dynamic sparse can be represented as a conditional property
distribution, p of x of t given x of t minus 1. This is how states evolve over
time with some noise and given a state x of t, the likelihood function describes
how can we generate spikes given the current state. So here the unknowns
are the x's and the theta which parameterize the dynamics, the vector
field and all we know are these observed y of t. So the inference problem is
quite difficult. Okay, so this inference problem to get x of t and theta from
data has been a problem for a long time and in neuroscience a lot of people
have tackled this. These are some list of methods that I collected a couple of
years ago except for common filter which is very famous method for doing linear
Gaussian case. All the other methods are specialized for analyzing neural data,
some of which are my own methods. And they're assuming different kind of
structures for the dynamical system model and different kind of observations.
And of course to do the actual inference we'll use various statistical
methods and machine learning methods to do this inference. And I will highlight
just a couple of these ones here that I will talk about in more detail. To do
this inference, most of these methods are offline. You need to give it a bunch
of data, then it will chock through the data and give you the answers. But we're
interested in doing this in real time in online fashion. So these two algorithms
that I highlighted here are online and they are based on a principle called the
recursive Bayesian filtering. If we can do recursive Bayesian filtering then we
can solve this problem. And the recursive Bayesian filtering basically tells you if
you already know in the previous time step of time step t minus 1, given all the
data from time step 1 to t minus 1, if you know what the distribution of x of t
minus 1 is, then using these formula and with a new observation y of t, you can
combine that information from y of t to obtain the updated x of t distribution
given all the data from 1 to t. So this will update your distribution one
point at a time. And this would be wonderful if we can do this. We can do
it for particular cases. If it's Gaussian-likely with Gaussian dynamics, then
we can do this. But in most cases, most interesting cases, this is
intractable, especially because we're interested in non-neonural dynamics
and observations which are spike trains, which are not very well described by a
Gaussian likelihood. So these are intractable. And the goal here is to
approximate these, this recursive Bayesian filtering, with a different
online algorithm. But it needs to be in real time, so it has to be constant time.
Every time you get a new sample point, new observation, it only uses a
constant computation time. And the total algorithm has to be finite memory
capacity, such that it doesn't become bigger and bigger in memory as you get
more and more data. So the first algorithm that I'll introduce to you is the
variational joint filtering, which was published in 2020. This is a variational
method, as the name indicates. And a variational method is a method where
you produce a queue of x of t. This is a different distribution that you
parameterize, typically with a neural network. And it's a parametric Gaussian
distribution, where its mean and variances are outputs of a neural
network. And this queue of x of t, you're trying to construct through the neural
network to approximate the best, the real thing that you cannot compute, the p of
x of t given y of 1 to t, all your observation up to time t. And to do so,
we will maximize the marginal likelihood. This is, if you can maximize this, try to
find the queue that maximizes this is good. It's hard to compute the marginal
likelihood itself. So in the variational inference, the main trick is to find a
lower bound that you will maximize instead. So in our derivations, we found
this lower bound where we have three components. The first component is the
reconstruction likelihood. So given a queue of x of t, your current state, your
observation y of t should be well explained. This is the likelihood. So if
you maximize this, it's good. And the third term here is the entropy, which says
you need to maximize the entropy of q of x. Relationship between x of t minus 1
and x of t to be also well described by this queues. So if q of x of t minus 1
and q of x, those two distributions should be related through this p of x of t
given x of t minus 1. So if you can maximize this, we are good. We're
explaining the dynamics pretty well. So if we run this in spike train data, for
example, for, if you simulate from a nonlinear oscillator, which goes around
this attractor here, this is the ground truth two dimensional attractor state
forward on time. These are the spike trains over time. This is our competition.
This is our method that produced a vector field. And this is our method
filtering up to time one thousand, time step 1000, and a time step 1000. We're
predicting in the future without any observations and also producing imaginary
observations on top of here. So you can see that it's doing a good job. But we
were also at the same time slightly not fully satisfied because this is a
variational method. There's inequality always have some gap. So what is hiding
in this inequality or that we cannot really maximize anymore? The lower bound
are these two variational gaps. There's there's a intrinsic gap that we cannot
bridge. And there are two of them, unlike the usual variational inference where
there's only one of these gaps. So in the next algorithm that we developed,
which is called the streaming variational Monte Carlo or SVMC, we ditched the idea of
using a Gaussian form q of x. Instead, we're going to use particles to represent
that distribution, to approximate that distribution. So this is an idea that is widely
used in particle filtering, if you know particle filtering. And what it is, is we have a bunch
of samples x of x, x's, which are potential trajectories. We're going to sample these
trajectories and weight each of these sample trajectories with a certain weight w.
And together, these are representing, these are going to approximate, these samples are going
to approximate the full distribution. And the way we assign these weights is through this
importance weight equation here. On the bottom, you have the proposal distribution r of s,
which I will tell you how we constructed it. And on the top is the true joint distribution.
So the way it works is you first sample, given x of t minus one from a trajectory,
you sample x of t using this r of s. And then you evaluate that r of s on the top and see what
the ratio is. If it's a good proposal distribution, which is very important for the success of this
algorithm, the proposal distribution will generate samples that are of high probability under the
true distribution. And that means the weights will be high. So the way that we construct this
proposal distribution is by using a neural network and a reparameterization trick that's often used
in variational inference. So h here is a neural network that takes input three things, x of t
minus one, y of t, the previous state, the current observation, and this epsilon t, which is a random
sample from a Gaussian distribution. And this is the basic form of the reparameterization. We are
parameterizing nonlinear distribution, non non Gaussian distribution using a Gaussian input
to a neural, you know, nonlinear neural network, where the neural network is parameterized by
this lambda of t. So this is an easy way of sampling points through x of t, this h.
And we will learn, unlike most particle filters or important samplers, we will learn this h.
And this h will contain input from x of t minus one. We don't know of any other particle filter
that uses this kind of trick. Okay, so then one thing to notice is that the lower bound that we
were using in the variational inference is now something very simple. It's deceivingly simple
here. It's this yellow guy, which is an expectation of a summation over the logarithm of these weights.
This is all. And this is a consistent objective, because the expectation is exactly the lower bound.
And also this lower bound has a small gap. If we increase the number of particles n here that
we use, and the number of weights, of course, same number of weights, if we increase the number of
weights, this gap between the likelihood and the lower bound gets arbitrarily small. So it's a small
gap, arbitrarily small gap, and a consistent objective. So what we do to actually learn the
system is to take the derivative of this quantity, respect to all the parameters, and take stochastic
gradient steps. And that's it. This gives us unbiased gradient for all the parameters, including
the proposal distribution neural network parameters, and the dynamical system parameters, and the
observation model parameters, all of them together. So let me show you an example of how
this works. This is what we call the NASCAR dynamics example. This is a 50-dimensional
time series that we simulated. This is over time. You can see there's a spatial temporal structure to
it. And remember that every time step is only getting one of these vectors, 50-dimensional vectors.
On the left is the true dynamical system. This is the vector field of the true system. As you can
see, this resembles a NASCAR track. That's why it's called a NASCAR dataset. The true state is the
blue X, and the estimated posterior mean state is the green. That's what the model is thinking.
And on the right, you will see what the model's inferred vector field looks like.
As time is ticking, as I have seen, 80 samples or so, the posterior mean is tracking the true state pretty well, and it's trying to draw some lines here corresponding to the time evolution of the true state, or the estimated state.
And the way we parametrized this dynamical system here, the P of X of t given X of t minus 1, we used a sparse Gaussian processes prior,
which means we not only get the vector field, but we also get the uncertainty on top of the vector field.
So let me replay the video, but with the uncertainty as colored here.
So this appearance of this dark green region is because the model knows that the state has
been here for a long time. So it's more confident about the flow being slow, and it's toward the right.
And as the system evolves over time, many, many cycles, it'll become more and more confident around that region.
Of course, far from those confident regions, it's inferred a fast flow to the wrong direction,
but the confidence, of course, is very, very low. It's very uncertain of what's going to happen.
So that's okay. We can apply this method, this SVMC, to spiking neural networks. Here we have a
2,000 spiking neural network that's implementing a binary decision-making task.
These are the spike trains that it generates. But basically, there are two populations that are
either winning or losing, depending on what the actual decision was in each trial.
After the inference, we get back the phase portraits. So this is exactly as predicted by
theoretical analysis of this spiking neural network. And we can recover that theoretical
design and theoretical analysis by just by looking at the spike trains. We didn't look at anything
else. We were actually sub-sampling some of these spike trains. We are only looking at 480 neurons
involved in this. Once we've learned the system, we can also simulate as before. If we simulate,
the network actually generates decisions on its own, depending on the stimulus. So
we built a model of decision-making of spiking neurons using spiking neurons.
And it generates more spikes. Okay, so that's the first part. Let me switch gears and get to the
second part, which is the exploration of unknown neural state space with attractors. So in the
previous cases, we were studying mostly spontaneous activity. There were no feedback controls
signals. But if we can control the system, we can do a much better job in learning about the
system. Because the recording time we have for the animals and the number of trials that
the animals can perform are very limited, which means every single spike, every single observation
is very, very precious. So we want to maximize how much we learn about the system by choosing
the best stimulus that will generate the best spikes, such that we can learn about the system.
And this has been done in many places, especially in visual areas from long time ago. But what I
want to do is not to use visual stimuli and study the static encoding properties, but rather
the dynamical system itself. I want to infer the most about the dynamical system. And this is
important because of attractors. So let me illustrate this by this example of a snowman
dynamical system. You see here, this is the snowman dynamical system. There are two attractor
oscillation-like objects here. And the neural state is going around here. And you can see that
SVMC recovers these top attractor very well because it has explored it on its own
spontaneously. There was no stimulation. It just happens that the state was stuck in this
attractor for a while. And while it was exploring, we learned that particular attractor. So what we
see is that whenever there's an attractor dynamics, it leads to bio-sampling. We sample
that attractor more. And neural attractors are everywhere in the neural system. We know they're
everywhere. Which means if we wanted to learn the more global picture of how the computations
are implemented and how the dynamics is, then we really need to actively explore around it. If we
just push this state a little bit to the outside of this top attractor, we might have discovered
this bottom attractor more quickly instead of waiting for a long time until it spontaneously
switch over. And let me just illustrate how difficult this problem is by analogy of a pirate who lost
a map because he doesn't have a map. The pirate doesn't know anything about the world. The world
is completely unknown. And this exploration has to be done in an online fashion. Unlike playing
video games, you can't reset the agent to the initial point or wherever you want to start and
then repeat different actions to learn about the system. You have to do it in one long episode,
which makes it much more difficult. And the environment itself is dynamical. We're filled
with attractors. And there's almost no extrinsic reward. If there's any reward, it's going to be
very, very sparse. And only by chance, you will rarely encounter them. So most of the exploration
has no guidance from external rewards. So the formulation of the problem is such that we have
to choose controls to explore a dynamical environment. So we're going to modify this
equation a little bit, where we have this controls u of t in the dynamic system equations.
And the problem itself can be posed as a type of reinforcement learning problem,
where the extrinsic reward is very, very rare. So we need to have some intrinsic reward
internally generated by the agent, such as curiosity. These kinds of rewards have been
studied widely in we propose a different objective. This is our proposed intrinsic reward function,
where it's combining the count based reward and the discovery bonus. Just to quickly illustrate,
let's just say this is our state space, the blue circle parts are the places that we have already
explored. If we go to one of these places, you get reward based on inversely proportional to the
number of times you visited that state. And if you visit a new place, if you let's say this new
place over here, you get reward based on the distance from the closest place that you visited to
that particular new point. So further away, it's redder here, which means the bonus that you get
is higher. And once you visit that place instantly around it, it will be lower reward around it.
And we can show that maximizing this intrinsic reward is the equivalent to maximizing entropy of
your visitation distribution. So we have a reward function that maximizes entropy,
we just need to combine with a reinforcement learning algorithm, we have two choices,
model free and model based. Model free is very popular, of course, but the main assumption
of model free is that the reward has to be Markovian. But as we just talked about, whenever
you visit a state, that state is less attractive already, the reward decreases as soon as you
visit a state. So it's totally non Markovian. So model free doesn't work very well. And model
free also has a problem being sample inefficient, especially if you can build a good model, model
based methods are very efficient, demonstrated by this example here, where they are comparing different
model based and model free methods, none of them actually know the world, they all have to learn
the model implicitly or explicitly. But the model based ones learn to do the task much faster.
So our proposed function that we want to maximize is this v, we want to try to find at x time t,
we are currently at x of t, try to find the best control signals u t plus one to t plus h with a
time horizon h, some time steps h in the future, using the current intrinsic world model that we
know predicts what how much amount of intrinsic we will get, choose the best one. To put this
together, well, as the is our algorithm called escape the maze. So we're an uncertain world.
Every time step we observe a new state, the new state is used to update our internal model,
the new state is used to update our intrinsic reward landscape. And then combining the two
information together by with a model based planning, we'll find the next best control
that will maximize the exploration. And doing so, also learn about the system, the dynamics model,
as pretty quickly. Here are examples of this multiple attractor chain link environment.
Our proposed algorithm escape the maze, explores it pretty quickly. This is random
exploration. This is random network distillation, state of the art method, and prediction error
method. Neither of those explore these systems very quickly. And if you have maze environments like
this, they also don't explore very well. Our algorithm explores pretty quickly as it summarized
over here. If you watch this video for long enough, you'll see that these algorithms get stuck
in a particular corner and they never leave. Because the intrinsic reward is probably because
it's non Markovian, it doesn't know if you keep going that way, you don't get any more reward,
you got a lot of reward on the way to going there, of course. Okay, so to summarize,
I've introduced you to a couple of new real time neuroscience tools that could potentially accelerate
scientific discovery, the real time statistical inference tools for neural trajectory and
non-linear dynamics. These can enable new experimental paradigms and feedback control
systems. One such feedback control system is the active exploration of unknown neural states
with attractors. It allows us to better infer the global dynamics and also as a byproduct can
escape attractor states. This escaping of attractor state is very interesting to us because it has a
potential clinical application to treat disorders with consciousness. With that, I would like to
thank my lab members, especially Hosun Al-Sar, my PhD student here. He's done a lot of work
and my former postdoc, Iwan Zhou here, who's done a lot of work and presented today as well.
I would also thank my funding sources and my collaborators and of course, thank you for your
attention.

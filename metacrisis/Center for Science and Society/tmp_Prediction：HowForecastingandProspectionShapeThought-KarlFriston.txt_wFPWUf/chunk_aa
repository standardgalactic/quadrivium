commander.
You have a great pleasure to be here.
I unusual pleasure to follow on from your talk.
In fact I'm just going to say the same thing using different rhetoric but
there will be a challenge mapping your rhetoric on to my rhetoric and
O addysg ond ddim—odd it'ch allwch yn dd ôl i'ch gyn gallu ein ymell Careful.
Ond ydychu'n ei t 느낌 a'u gynnydd y lle ddweud.
Rwy'n ddweud cyflwygraduated內 yw eu recommendy.
Read it.
Nick yn relying...
..yi du'r trodd cerdd.
..eg siarad y twoysbeth nifer dargliadol sy'n roedd,
a grefai'r drwaid ywir hefyd,
ond rydym wedi'n goingnydd…
...y hoffwnolydd –
Yn tystiolaeth b就到'r poi yw awr i lacityn yn cy occas lawl ei wllaeth....
...neid felly nawr- Mann aber i statuol...
Ilyw'r rhedeg, y twyntобрion bened heddor yng Ng decoration
ond will Beir E Along teariad cyntaf ar gael Ilyw'r Gw Character
ac sydd wedi cy patriarch Field, wrth gwr argues y sia Firnau Ilyw,
Wrth golygu jestroriau llyniadau Expedis knobs Head of Newar
of information theory and variational principles.
These are just glorious terms for Hamilton's principle of least action.
And that's where the challenge comes in mapping the rhetoric,
but it can't easily be mapped.
So I would split my talk into two ten minute bits.
The first will be just to review for you what people mean by predicted coding
in its simplest or vanilla sense.
And I should just preface this part of the talk
by saying that normally when people talk about predicted coding,
they're trying to predict what is happening now.
So it's using internal explanations that provide the best prediction
for the sensory impressions at hand now.
And that's a slightly simpler problem than the reason that we're gathered here today,
which is a much more prospective what will happen in the future if I do this.
I think that distinction is very important, of course,
because it speaks directly to the goal-directed nature of our behaviour.
So the whole purpose of this talk is first of all just to describe
a predicted coding vanilla and then say how it could be generalised
to embrace the challenges of making the behaviour prospective
and fit for purpose in relation to what we are trying to optimise.
And I'm going to illustrate that using simulations of cyclic eye movement.
So how do we sample?
So the audience participation was a beautiful example of that
because I was only looking at, I was only sampling information
from the lowest buttons and I'm going to try and simulate that
using the principles that we've just heard about.
So I normally start this talk just by managing some of the key intellectual architects.
I think most of what can be said that is of interest
from the point of view of predictive coding conceptually
was said by Herman Helmholtz, beautifully articulated here,
with this phrase, objects are always imagined as being present in the field of vision
as would have to be there in order to produce the same impression on the nervous mechanism.
So what he's saying is in order to see something,
in order to fit your mind to the world,
you have to have in mind an explanation or a model
of what caused your sensory impressions.
And that fits very comfortably with Richard Gregory's notion of perception as hypothesis testing.
So the first bit of rhetoric mapping, learning equals perception in my talk.
So we often talk about perceptual inference or perceptual learning.
So when I talk about perception, I mean the learning, fitting the mind to the world.
These ideas have been formalised by people like Peter Diane and Geoffrey Hinton
and many others in machine learning, boring from Bayesian probability theory.
And indeed technical advances offered by Richard Feynman,
variational principles that allows to approximate Bayesian inference.
And I use that word now or that phrase approximate Bayesian inference now
because of course if our inferences are always approximate, they are always false.
So I love this idea of a good falsehood because in my world everything is false.
It's all approximate and that approximateness comes from variational free energy formulations
devised by in statistical physics.
So those are the players.
What then do they tell us about the notion of perception and action or learning and action?
Well, this phrase, the same impression, the sensory impressions,
suggests that we are in the game of trying to explain the causes of the sensory impressions
that fall upon our sensory epithelia, say our retina or our ears.
So here we have some sensory impressions and we'll denote them by S.
So how might this notion of minimising prediction error that we've heard about,
how might this notion of optimisation be invoked to give us some feel for how the brain would actually do this in a biophysical sense?
And the example I'm going to use is predictive coding.
It's also known as Kalman filtering or Bayesian filtering.
It's also known as a minimisation of variational free energy.
It's just a variant of Hamphill's principle of least action.
Two twists or takes on the mathematical formalism.
And I'm introducing them deliberately because one is a little bit high flying,
but it will be useful when we come back to the challenge that has been set up in the first talk.
The other one is slightly more intuitive, cast in terms of prediction errors and it's useful to explain,
but they're both talking about exactly the same mechanism.
So I've cast both descriptions mathematically here in terms of a flow of things called expectations about the world,
that are expected causes of the world, that is either doing a gradient descent on variational free energy from Richard Feynman,
that can be mathematically written down in a totally equivalent way in terms of predictions and updates.
So what do these terms mean?
Well, the prediction is that if I'm given, if I have some expectations about the current state of the world,
then I have some beliefs about what will happen next.
If I had available to me the mistake or the mismatch or the prediction error afforded by a sensory sample from that world,
then I can make a little correction to improve that expectation.
So let me unpack that a little bit more intuitively.
Let's say we were given the sensory impression, the shadow here of presumably some form of canine.
If I wanted to explain that, I could have an expectation denoted by mu.
Now, if this was the cause of these sensory impressions, and if I had a generative model,
a model mapping from this expectation or belief to the sensory consequences of this expectation or cause,
I could simply create a prediction of my sensory input, g of my expectation, a generation of the prediction,
and subtract it from the actual sensory input to obtain a prediction error.
So the prediction error is just a mismatch between what I'm actually sampling and what I thought I was sampling,
given I had some expectations about the state of the world.
All that this equation says is that everything that can change will change
to minimise a sum of squared prediction error weighted by their precision.
So that we can explain all of brain function simply by minimising prediction error.
Now, again, this nice notion of falsehoods speaks to the fact we don't necessarily need to know what the truth is.
All we need to do is to minimise a prediction error, and that is perfectly good.
If I live my entire life with a minimal prediction error, then I will be well happy.
I don't need to know the truth.
That is nice because, as we've already seen,
it accounts for the reciprocal directions of fit between mind and world.
If we just have to minimise prediction error, we can do that in one or two ways.
We can either change our internal brain states, our synaptic activity or connectivity,
that encode those expectations to make our predictions more like the sensations that we sample.
Conversely, we can change the sensations by actively sampling them again,
either through visual palpation or placing ourselves in a particular relationship to the world,
so that we can change the sensory samples to make them more like our predictions.
In both cases, we are minimising prediction error just by changing either side of the equation.
We are conforming to this imperative to minimise the sum of squared prediction error or variational free energy.
Of course, from my point of view, that would be the rhetoric of action and perception,
fitting learning and action or fitting the mind to the world or the world to the mind at many, many different levels,
but all happily subsumed under one simple principle.
If I have predictions, or if I want to elaborate a prediction error,
then I have to have predictions and I have to have a model of how my sensations were caused.
Of course, for us, those models have to be very deep, hierarchically structured,
that would be very dynamic, extremely non-linear, high dimensional.
I have just cartooned what we get from that predictive coding equation
in terms of an understanding of neuroanatomy and physiology in the real brain,
because if it is the case that the brain uses a model to generate predictions
of what it should be seeing at this point in time,
then the anatomy of the brain should entail and embody that model
and should similarly have the same sort of hierarchical, non-linear and dynamical structure.
So, let's just think about the nature of these models.
This is actually a graphical model, which some of you will be very familiar with,
that contains hidden states and causes that cascade down to generate sensory impressions right at the bottom here,
all subject to random fluctuations or uncertainty.
A more intuitive version is here.
So, imagine I asked you to generate some synthetic data
that reproduced foveal samples from the retina for your students, for example.
How would you do that?
Well, the first thing that you would have to do in generating these synthetic data
was to work out what is producing the sensory samples
and where you are sampling.
You'd have to work out all the kinetics and motion-opter, motor dynamics,
that sample this object and equipped with this what and where causes.
You would then be able to mix them together in a deep cascade,
non-linearly, to actually generate a time series of sensory samples here.
So, the generative model maps from causes to consequences,
whereas fitting the mind to the world inverts that, is literally Bayesian inversion.
It's literally this predictive coding.
It's trying to solve a problem of going from the available sensory samples
back to what caused them.
Of course, that's exactly what that predictive coding equation does.
And when we actually look at the form of the equations
and how they might be mapped onto real brains,
then something very simple emerges.
We retain this hierarchical structure,
but instead of the random fluctuations entering at each level,
they now take the role of prediction errors
and start to ascend the hierarchy.
So that as the top-down predictions are used to form a prediction error,
the prediction errors are passed back up again
to assist or revise or correct the expectations at the higher level.
So, basically, we have a descending stream of predictions
in exactly the same way that you generate these synthetic data for your students.
That are accompanied by a counter stream of ascending prediction errors
that are correcting at each point.
And this is very reminiscent of real brain hierarchies
where you have reciprocal coupling of backward and forward connections
that are all in the game of suppressing activity,
eliminating prediction errors at each and every level of the hierarchy,
such that when that process has finished, given some new sensory samples,
you have a minimal prediction error,
a maximum a posterior expectation of what caused your sensory input
with multiple levels of abstraction.
So that's the basic idea.
Let me just rehearse that for you in the context, say, of sampling the world.
Here's a little cartoon of a brain.
As you imagine, we have some visual information coming into the retina here
that is passed to the lateral geniculate body,
and it is in receipt of predictions from, say, the visual cortex,
and that forms a prediction error,
and the prediction errors are passed forward to revise or update
the expectations or beliefs about the elemental causes of this visual input.
Of course, these expectations are themselves being predicted
by high-level expectations or beliefs,
and so there's a high order prediction error that's passed forward
to revise these expectations that themselves will be predicted
and so on to any arbitrary hierarchical depth.
Now let's look at exactly the same story,
but from the point of view of proprioceptive input,
input from my eye muscles that tell me about the state of my effector organs
that I deploy to actually sample the sensory information that I'm trying to fit.
So I cartoon that here in terms of proprioceptive input from the eye muscles
that come to the brainstem here,
and they are predicted by, say, descending predictions from the frontal eye fields,
and they form a prediction error that could have sent to revise my beliefs
about where I'm looking.

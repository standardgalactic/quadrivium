ermawg o dwarfodol ynuarad effaith.
Cymru a chymlu am之dyn ni rYYC yn thicki Toru Poe yng Nghymru,
ry anchelliff мнеau,
mae gollwch eich syniach o'r ambrach sy'n gwri το pa nifer.
Mae'n gw vagu i ni i gawd i ddim yn cymryd gyfanidd,
ond ond arall bydhau a Chrystefile rh experiencedod cyfle希望
yn gofyn dangos a wah Andy�g yn gwyfod arbennir financial
i ddefnyddio fynd i gyd yn unig dod yn Lly safer.
Roedd mae'n sefydliadau dau'r rher InsideHDau
a poloedd p personal efo'r cyd-ref
yw eisiau am bodgylchu sy'n cael ffau chi'r ysglocks.
pe opto d经tyn wedi bod maes i chi crif, ei wneidio dim ein beth o'i dod.
Raing mwy oedi roi ac gefnodd gan yma ddweud?
Dwi wr OSE donno nhw'n dddgli i wneud beth sydd ynladd oherwydd i'n meddwl i dweud
fel owner o'r holl您s arnynt i'r lehhau battwch gyda gilyzdeidle.
Yn gofal,'r f45 maes i dechreu an beverage do ndeddy yw rög DewiHyde.
The importance of using violations...
is to disclose the quintessential role of prediction in perception.
If we get that far,
we'll end up where we've started.
The trick that I'm going to take here though is that I'm actually going to start not with perception,
but with action.
I'm going to start off as if I was talking to a group of people doing optimal control theory,
or reinforcement learning,
or utility theory,
a byddai'n toolaeth cy auditoriaf yn cael credu o'r way
sy filesample y sylwr outdoor, fy byddiaid y Gymru yn credu cafeadariaeth
gyda'r clwr ddechrau, er ysgol ein bod yn cyd yn
geography. A'r Simple Dallion felly yma gennym
os fyddio ddod o wio yna'r� Moses Hacker theod have strith ar gyfer cael ap
gyfer 5-10 a 5-10.
EXCIP09 pe tych yn dwi-fars.
Yn y cwpio, ond mae'n cael ei wneud o'r ddweud o'r ddweud, ond mae'n cael ei ddweud o'r ddweud.
So, this is an overview what we're going to go through. I'm going to introduce the notion that you're the free energy principle, but from a, using a slightly heuristic approach in terms of action and the path of least resistance, highlighting the importance of having internal models or hypotheses that enable us to generate predictions, talking about active inference.
And one key thing that I'm going to focus on is translating the theory into a process theory that can be used to understand neuronal message passing in the brain and help us exactly constrain the sorts of experiments that Jim was talking about.
So that's going to be a big part of what I hope that we'll be talking about, taking normative principles and seeing how they unpack in the service of understanding empirical measurements, anatomy and physiology, and how they can be used to nuance experimental design, showing the sorts of things that one can simulate and speaking to some empirical predictions of these sorts of schemes.
And I've put, as an epilogue, more recent work, simulations of reading, that introduce hierarchies into the particular forms of generative models that I want to survey for you. We won't have time to go without that, but I just want to show you the slides in case of something that catches your attention.
So I'm going to start with a question. Let's assume you're hungry, and let's assume you're an owl. So what are you going to do? You're going to search for a mouse. And how are you going to do that? Don't cheat. You don't have to look at me, not at my side. Absolutely. Perfect answer.
So in terms of optimal behaviour, the first thing you do is search. You scan. You confront the epistemics of reducing uncertainty about what you need to do in order to fulfill your goal. So it's all about beliefs.
So in that answer is the basis of everything that I'm going to say. Your behaviour is always driven by beliefs. And that tells us something quite important. So here's you scanning and searching, and you found a little mouse that you might want to eat there.
And that's quite important because it speaks to two basic classes of ways of thinking about optimising behaviour. You can either imagine that there is some value function of the next state that will be brought about by some action and optimise that action by selecting the action that maximises the value of the next state.
That's the classical way of doing it, but that just doesn't work if the best next thing to do is to search and resolve uncertainty because uncertainty is an attribute of beliefs.
Therefore, the functional, or the function of a function that you need to optimise in terms of action you hear is a function of beliefs, which I'm deleting by Q, beliefs about the states of the world.
And that introduces a fundamental distinction between the sorts of schemes that you bring to bear in terms of understanding optimal behaviour.
The other thing about the sort of scanning and searching answer is that action depends upon beliefs about the world, states of the world and subsequent actions.
So, not only is it a function of beliefs about the world, but it's the order in which you interrogate that world. So, it makes a difference whether you search, then eat, as opposed to eat, then search.
And that means that we are in the game of optimising sequences or policies or actions. I'm going to call a sequence of actions policy.
So, what that means from the point of view technically of what sort of thing we have to optimise, it's a functional of a belief integrated over time or summed over time, a path integral.
And if we call that an energy, then the integral, the path integral of an energy is called an action.
So, what we've just said is that we've reduced the problem of good behaviour to Hamilton's principle of least action, where action is the path integral or the trajectory integral or the sum over an energy functional of beliefs.
And that's the basic premise that I'm going to pursue.
And just to highlight the distinction, if you subscribe to this way of thinking about how systems work, then you end up with optimal control theory, Bayesian decision theory, enforcement learning and all that good stuff.
Conversely, if you believe this is how biological systems work, then you end up essentially with Hamilton's principle of least action, the free energy principle, active inference, active learning and so on.
And that's what we're going to focus on.
And the energy function that I'm going to consider, we've already heard mentioned, is the variational free energy or the free energy, which we've already heard very roughly scores surprise.
It approximates surprise or suprisal, and it's simplifying assumptions prediction error.
So, what we are saying is that we're just in the game of minimising prediction error and more specifically prediction error over time, over sequences of behaviour.
I'm going to quickly go through this because it's an interesting, there are lots of interesting connections with existing theories and formulations.
This is a bit technical. These are both iconic and ironic equations. You'll hear more about those later on.
In words, if it's the case that good agents, good people, minimise their free energy, their surprise, their average surprise and their uncertainty, then they must believe that the actions that they emit will minimise expected free energy.
You can write that down very simply in terms of these belief functions here and rearrange them in a way that discloses important links with lots of established formal treatments of behaviour.
So, I've written the expected free energy associated with any particular policy in terms of its expinsic value here and its epistemic value.
Basically, these things store the surprise about what you predict will happen under a particular behaviour and what you think should happen, your preferences like, I'm going to eat a mouse, I'm not going to be hungry.
So, that's a surprise bit, explicit or expinsic surprise bit. There's another surprise, there's another sort of average surprise or entropy, a relative entropy, which is called epistemic value.
It's a reduction in uncertainty or the information gain and that's the key bit. It's the epistemic which is missing from classic theories, but it's part of this formulation of Hamilton's principle of least action.
That relates very closely to theories of visual salience or Bayesian surprise. Technically, Bayesian surprise is the divergence or the difference between a prior belief and a posterior belief or a posterior belief to be informed by observations here.
So, what we're saying is that we will choose to act in a way that reduces our uncertainty relative to prior beliefs, looking at data which gives us information that maximally reduces that uncertainty that has the greatest epistemic value or Bayesian surprise.
In fact, mathematically that's exactly the same as the mutual information between the causes, the hidden states of the world S and the consequences, the outcomes that we actually observe.
So, another way of saying this is that we are subscribing to the principle of maximum information, mutual information or minimum redundancy or maximum information efficiency of the sort articulated by Horace Barlow.
Always of expressing one particular form of perspective on this underlying functional. Another way of thinking about this in the case if there is no ambiguity, if we actually can observe the states directly, then we can discount this uncertainty term here and what we're left with is something called KL control which is the state of the art of what people would use in optimal control theory and dynamical systems control.
In economics it's called risk-sensitive control. It's minimizing risk. So, it's a surprise between what I think will happen and what I want to happen and if what I think will happen is surprising relative to what I thought was going to happen, then I have a high degree of surprise, a high degree of risk and I want to minimize that.
And then finally, if there's no ambiguity or there's no risk, then we reduce to classical expected utility theory or all the sorts of theories that reinforcement depends upon, just maximizing our preferred outcomes there.
So, clearly in order to be surprised, we have to have predictions against which we can match outcomes to score that surprise and this brings us to generative models and the departure that I promised you from what people currently understand in terms of predictive coding
and what I'm going to talk about for the next few minutes is I'm going to formulate generative models not for continuous state space of the source used in predictive coding of say visual angles or content or acoustics, but generative models in which we can label the entire world in terms of a number of discrete states.
So, these are generative models for discrete state spaces and they don't normally have the look and feel of predictive coding, but my story will be is if that they do, they are actually formally very, very similar to the sorts of schemes that we understand in terms of top down predictions and bottom up prediction errors in hierarchical message passing
and predictive coding in the visual cortex. So, in these models, all we have, this is not ignore the equations, but it's focused on this graphical model here. What we're saying is that the world unfolds in one of many, many states and the transitions from one state of the world to the next state of the world encoded by probability transitions that themselves depend upon how we act.
They depend upon the policies that we choose and we have a certain confidence in those policies by their precision or inverse temperature beta here. So, if we knew the probability transitions or the transitions from time to time of the states, we can generate a sequence or trajectory of states and each state and each point in time generates an outcome through this likelihood of matrix A and that's it.
That's the generative model. The world has states, they unfold and each state generates an outcome that's observable. And that's the basis of everything else that I'm going to say. So, if I'm now given a generative model, what I can do is I can evaluate the free energy of my beliefs under that generative model and I can then minimize everything with respect to that.
Proxy for surprise or uncertainty, namely the expected free energy and I can write down equations or solutions that tell me how an optimal agent person would behave in a sort of Bayesian sense.
And these are the solutions to the equations expressed in terms of the parameters of that model. So, A was this mapping from states of the world to outcomes and B was the mapping between subsequent hidden states.
And despite the complicated nature of the equations on the previous slide, the actual updates, the solutions are incredibly simple and furthermore, they look very much like the sorts of things that the brain does.
So, for example, expected states of the world are a non-linear sigmoid function of linear mixtures of expected states of the world and observations. So, we're mixing together evidence from outcomes and our beliefs about the states of the world to update our beliefs about the current states of the world.
Our beliefs about what we're going to do next, our policy pi here, is this a softmax function of the expected free energy weighted by an inverse temperature parameter that you will see we associate with dopamine, a classical softmax response rule.
If you're not familiar with that, that's what people in economics and choice behavior use for those people who deal more with perception.
We also have a model of incentive salience. The confidence or the precision or the inverse temperature associated with our beliefs about action now becomes, has a base optimal solution that depends upon the goodness of a policy or the negative goodness, the expected free energy here.
The form of these equations speaks to a rough anatomy of computations in the brain, a computational anatomy. It goes like this, where we have these equations dictate what each update needs to know about the other updates.
So, basically, it prescribes a connectome for the exchange of information or sufficient statistics that is implied by placing the Hamilton's principle of least action on the simplest sort of generating model that you can imagine.
That's the sort of anatomy we have here. Outcomes, expected states, expected policies, the goodness or the expected free energy of policies, the precision of policies, states in the future, which prescribe action.
So, I won't go through that, but I'll just give you a more heuristic version of that one. So, what those equations tell us, so this is like a very top-down argument.
It's not, you know, let's think about how the brain works and come up with some hypotheses. This unfolds or unravels from, impacts from, just applying Hamilton's principle of least action to a very simple generating model.
And what it tells us is that sensory input comes in, say, at the back of the brain. It informs and updates expectations about hidden states of the world, sometimes referred to as state estimation.
They are associated with a free energy or a surprise that is combined with an evaluation of those states in relation to prior preferences and their potential reduction of uncertainty, their epistemic value.
They are combined to give us beliefs about the policy that we are currently pursuing. We have a certain confidence in that policy.
And then those policies are used to weight all the different states conditioned upon what we are currently doing to give us the best estimate of what's going to happen next, the next state of the world.
And if we know that, then we can choose the action that brings about, that realises our expectations, our predictions about the next state of the world, that action solicits a new observation from the environment and the cycle begins again.
So, we have a perception action cycle that falls out of the minimisation scheme that we've just been talking about.
So, very briefly, I'm just going to show you how that sort of thing works with a series of examples, and then hopefully I'll turn it over to you to see what you want to talk about.
The first example is just a very simple simulation of foraging in a two-arm maze. So, in this example, there's a little rat here, and there are rewards on the right and the left arms of the maze, but the rat doesn't know where the reward is.
But there's also an informative queue at the bottom of the maze here. If it went to solicit that queue, it would then know where the reward was, and it could only make two moves.
So, it can either take a chance and go to one of the other top arms, or it can be a bit more clever and resolve any uncertainty about the context it's currently operating in, which arm is baited, and go and retrieve the epistemic value of the informative queue and then make an informed decision.
So, this is exactly the searching that you were talking about before, scaling your environment, knowing where you are, resolve your epistemic, solve the epistemic problem, and then turn to your prior preferences or your pragmatics.
You can write this model down in very simple terms of these A and B matrices here. There's partial reinforcement here, and the C matrix here just denotes the preferences in terms of what sorts of states this rat thinks it should occupy, basically thinks it should be in the baited arm and not in the unbaited arm.
That's all it's saying here, with minus threes and plus threes on the upper arms that are baited. And if we do that, and we just integrate those solutions that I told you before, we actually generate very realistic behaviour, summarised here in terms of the expected policy and the policies that this agent or this little animal can entertain.
It stays there and then goes to one of the three arms, or it goes to one of the two arms, or it goes to the bottom and then goes to any of the three arms. So there are eight policies here.
And what it does in the first instance is because it doesn't know where the reward is. It gets the cue and then obtains its reward. What we've done here is actually baited the left arm all the time.
So slowly it accumulates evidence that, in fact, the reward's always on this side here. So as time goes on, it actually switches and learns, and it's probably better to avoid or dispense with the epistemic move and go directly to the reward.
And it starts doing that after about 20 or 30 trials here, at which point its reaction times, and this is the actual floating point operations of the scheme, decrease. And because the goodness of a policy is this path integral, it's actually spent more time being rewarded.
So if you like, the payoff also increases by going straight there. So this prescribes good policies, and it can be used to simulate nice behaviors of the sort you've seen experimentally.
But what I want to do finally is just connect that to neurophysiology and neuroanatomy. But to do that, I have to have a process theory. I have to have a theory which says this particular neuroactivity or this particular connection strength corresponds to this quantity in the model.
So I have to have a process in play that is neuronally plausible. And the way that we're going to do that is just take those update equations that we've seen before, and instead of just writing down the solutions mathematically, I'm going to recast the solutions in terms of a gradient descent or a hill climbing, or actually a hill descent here.
So this is a standard way of optimizing something. If you've got a quantity you want to minimize, you just go downhill until it stops getting smaller. And if I do that, I can write down exactly the same scheme in terms of differential equations on expected states of the world.
The very similar form, but here that's a rate of change of activity, which is now a nonlinear function of linear mixtures of expectations about states of the world and the observations. And in doing that, I've created a dynamical system that now has as much closer to the look and feel of a neuronal system.
And that now enables me to look at the dynamics that underlie the behavior. And these are the dynamics here, and we can basically break these into inference and state estimation in terms of the updates or the fluctuations in the states as new evidence comes along.
Policy selection that we've already seen with our softmax response rule, and learning as we accumulate from trial to trial evidence about particular states or contingencies of the world in this instance that the left hand arm of the maze was always baited.
I illustrated those things here, a couple of interesting things to note. First of all, with every new move and every bit of new sensory information, there are lots of fluctuations in these states that look very much like an ERP.
Furthermore, when we become a little bit more automatic or not habitual, but certainly going straight for our reward, there is an attenuation of these responses.
The confidence, the precision in those responses also shows these phasic changes and progressive changes as we learn the context.
So we actually get something which looks remarkably similar to transfer of dopamine responses as we become more familiar and more confident about the outcomes that we see.
Let me just quickly show you a couple of those outcomes. This slide highlights just one trial, and it shows the representations of time over the different hidden states of the world, and just highlights a couple of things.
First of all, it shows that as we accumulate evidence for our preferred policies or our preferred outcomes, the probability that we are in a state which we will ultimately choose increases whereas the probability of states that we don't decreases.
And this is formally identical to evidence accumulation or drift diffusion models, but now cast or now a consequence of a gradient descent on variational free energy or a bound for surprise.
What we also see is an interesting dynamics in the sense that if information keeps coming in, say 250 milliseconds, like the frequency at which we go and sample the world with iconic eye movements,
that means that we have two timescales in play. One is a theta rhythm as we go and get information once, say four times every second.
But within each sampling there's this fast updating that's minimising and optimising our beliefs, and that faster updating has a temporal scale in the gamma range.
So what we see is effectively, as we move along, fast updating that repeats itself every theta cycle, but as we accumulate more and more evidence we get more and more efficient and confident about the things that we are inferring.
The dynamics mean that they accumulate evidence more quickly, more efficiently, and we get a phase procession of the sort seen in the hippocampus.
I've already mentioned that, as Tang goes on, by virtue of increasing our confidence as we assimilate this evidence, then that confidence is expressed in the confidence of our policies and we have a nice way of assimilating dopamine responses.
We can look at the behaviour or the activity of these representations of different states of the world at different points in time during our policy, and if we plot their responses as a function of where the rat actually is, we can simulate place cell activity.
There has many characteristics of the sort seen empirically. This just illustrates this theta-gamma coupling, which is an almost necessary consequence of this sort of solitary sampling of the world, and then updating bleeds quickly before the next sample comes along.
Again, the sort of thing that one sees empirically. We can now do violation responses exactly as Jim was talking about. What I've shown here are the responses to two trials.
They're identical in nature, but one is from the beginning of the trial where the rat was not familiar with its environment, and one is at the end of the trial when it becomes very familiar just before it starts going directly for the reward.
Interesting, if we look at the representations of key states here, what we see is a much more efficient and therefore less exuberant updating of expectations of hidden states that if we subtract the standard familiar one from the oddball or the unfamiliar one, we reproduce the temporal dynamics of things like the mismatch negativity in ERP research.
We also demonstrate this transfer of confidence or simulated dopamine responses from the rewarded cue per se to this instructional condition stimulus here.
I'm going from slightly negative to positive here. We can play similar games by introducing deliberate violations and illicit P300 responses. We can look at reinforcement learning by switching contingencies halfway through and look at the effects on dopamine-urgent responses and also electrical-physiological responses.
How long have I gone?
That's very good, isn't it? I've only been talking for 25 minutes.
That I can be true to my promise to finish in half an hour. This is the epilogue. That's the story so far.
Most of that will be in the next few weeks in the published literature. You'll notice at the moment there's nothing really about hierarchies. Most people here, I'm sure, are more interested in the implications of this sort of theory for perceptual hierarchies and evidence of accumulation and purely perceptual domain.
The more recent work that I wanted to, this is not published, to introduce you to, is now taking this formalism, which has a lot of construct validity in relation to choice behaviour and your economics, active vision, active sensing,
and see what it has to say about the sorts of themes we're more interested in, which is the hierarchical message passing and the deep generative models that we assume that the brain is using to actually understand perceptual sequences.
So this is the epilogue. Again, I'll just speed through this in five minutes. What we're going to do now is tell exactly the same story, but now we're going to put one of those discrete state-space models, they're known as Markov decision processes, on top of the first one, and another one on top of that, and another one on top of that.
So in this construction, hidden states, at any one level in the model, don't generate outcomes, they generate the first or the initial hidden state of the level below.
And then that cycles over a few iterations, and then terminates like the rat-terminated, when it entered the baited arms of the cues.
And that process repeats hierarchically to any arbitrary depth. So what we have are deep temporal generative models. And they're really interesting because not only do they have a hierarchical structure in their form, but also in their time, because if the state at any high level is generating the initial state that must have subsequent states,
then it means that the lower states unfold more quickly than the higher states. So one way of thinking about this is the generative model says that at this hour, at this minute, and at this second, I am safe, I was reading, I'm on this page, on this paragraph, and on this word.
So if you think about the lower levels as ticking over more quickly, like the second hand of a clock, and every revolution or every trajectory or every path they take, then the high level goes forward one step, and then it goes round again, it goes another step, another gain, and then another step.
And then that process is repeated. So as the minute hand is going round, once it goes round, then the hour hand goes round. So what we have here is a generative model that basically has in mind, literally, beliefs about the world that are much more protracted in time and are hierarchically nested.
So if you could invert this sort of model, you would have a representation of the context, and the context of context, and the context of context. At each point, as you go deeper into the model, they are more temporally enduring.
So you would know that working from the top down, you would know the story of the narrative, if you knew the story of the narrative, you would be able to generate a particular sentence, if you could generate a particular word, if you could generate a particular word, you could generate a particular letter.
All faster and faster are more elemental timescales.
Ond mae hyn yn High Levelatoedd, ac mae'nuttering, ond mae rhyngwledig felly mé amlo.
Dyleson wahanol os nனwr yr unrhyw arall,
ryngwethaf chi o chase i mae'r hyffigfa?
Dy'r hyffigfa syr wychond oedd i ysb backup.
Cym最近in papurart, dy n own d Aub mwy f carbraig maen,
o Zeus i gyfo newydd perdynillwyr wedi ddifol.
Ond mae'r hyffigfa wedi gael cyflding Windows,
mae ydych chi'n ddiddor者eth ber parliament yn ychydig.
a'r red mwyllt i'r
erddwyd y ddigon sy'n oesch chi'n gwrth ag anewod妹iat ag bynnig.
if I express them in terms of a grading descent on free energy.
What we've done here is to write them in terms of prediction errors,
so we're back now in the rhetoric of predictive coding
and things that people are familiar with.
The prediction errors here are basically
the difference between the log of an expected state of the world
.
,
newydd gymryr werth yn canallweiad rhai
ac y gWAith yn yfhysиваемol.
Mae hyn yn drwsail yn beth ond,
ar y cynhyrchu cy competence anhyniad.....
o phroesio cyIDau
a'r hystyosit tryth workedeithas lively gyda siwad.
Llywydd,妹goed y pryd cympest
ffind yr ydyn ni oedd yn ar也o'i gael arddangos yn defnyddio'r drunion ddiddordeb hyn a'r gyfan,
dwi'n fent Patreon Ym M designingel m foreseeancol.
Yn dod ond am y functionalityline, mae'r ydal am Mountain
gwanildiad o'r rhan oedd nifer lwan seller am b JERMAظio panigwr sydd eisiau o gwahanolbau
o blWEi yn ichi ei gwahanol bydda'r fawr i wneud hwn ofunnall ar edryctions πάlen amddangos
ar hyn o'u dud yn éch hoff어야d a gyllide'r modelor hyn.
dispose charkew chatio mae'r sely excellennu amddangol sydd wedi g remarkableion had br drugwch,
er mwyn bod eich ddweud ein ffordd.
Mae'n dweud hynny mewn ffordd phennigau a eu 표fau.
Dweud ar yr ad SCP-18 saen рад.
Y newydd yn oes i'r ffordd, yn ad warned fra i negau yng nglyducau sydd gwaith mewn rhysig,
ac am hyn nhw yma i'r modd yn cael y mas iddynt ac amgarulladol rheol.
Y Parliament D Welt Unigfbanenau.
Nid ynックas hyn gyda llunio gwaith phran revelationol i'r ad SCP-18
ble neby con shaft goeth y
sg roamant rant.
Mae hus COVID eraill o slipo'u bobły democrrias.
Mae'r ddiddordeb yn
hefyd, mae'n comet agynwys wneud a napuc o'r
holl ychymor, yna'r ddefnydd fan ffyn cinfr
scribach i'r makes ystan y, masgrifi a'r
macrocholim o Teach Colt bydd o'r boul agor y credu.
Mae'r fugwladau hyn yn tytol se cognition y dyfod plant yn dim.
Onw ychydig ym preached, ond blendirþivot a'r môl sydd cyrylocol fe ddiwedd narrower syddiad.
Owch munaf eich lawer fflasferau a'r rhan mewn sen yna wrth ei dd такоеu,

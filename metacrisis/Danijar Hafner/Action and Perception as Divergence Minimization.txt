Hello, I'm Daniel Ja and I'm presenting Action and Perception as Divergence Minimization.
The question this work tries to answer is what objective functions can intelligent agents
optimize?
For this, we map out the space of possible agent objectives.
This offers a unified perspective on many common objectives, such as representation
learning, maximum entropy reinforcement learning, empowerment, skill discovery, information
gain exploration, and surprise minimization.
Our work shows that representation learning and exploration are just two sides of the
same coin, and it suggests world models as objective functions for highly adaptive agents
without the need for rewards.
Our framework also unifies the approaches of active inference and debris reinforcement
learning.
We consider the setup of an uncertainty aware agent that interacts with an unknown environment.
The environment produces a sequence X of sensory inputs.
The set Z includes all variables that the agent can choose, such as actions, representations,
or model weights.
Defining these as random variables allows the agent to capture uncertainty over their
ideal values.
However, we also allow values without uncertainty estimates by using point mass distributions.
We call the distribution over Z the agent's beliefs, that can be changed via the parameters
phi to optimize an objective.
The parameters phi determine the agent's current distribution over latents Z and inputs
X in the environment.
We call this the actual distribution.
Because every agent has an actual distribution, we can formulate the objective as bringing
the actual distribution toward a desired target distribution by minimizing a divergence measure.
In this work, we focus on the KL divergence that connects to many objectives that are
common in the literature.
Alternative divergence measures yield analogous method families.
Remember that X is the lifetime sequence of all sensory inputs.
At any point in time, the agent has observed past inputs that can be stored in the replay
buffer.
It then computes beliefs from them, and the beliefs include actions that influence the
expectation over future inputs, which can be estimated through planning or on-policy
data.
We will now look at different options for the target distribution that result in different
objectives.
A broad distinction of different target distributions is based on the dependencies they capture.
On the one hand, factorized targets model inputs and latents as independent and assign
zero mutual information to them.
Using the KL to a factorized target, this also minimizes the mutual information between
inputs and latents in the actual distribution.
For example, maximum entropy reinforcement learning uses a reward factor on the inputs
and an action prior for the policy.
This results in solving the task while keeping the actions as random as possible.
On the other hand, expressive targets capture dependencies between inputs and latents.
An example would be a generative model with a prior over the latents and a conditional
observation model.
Minimizing the KL to an expressive target also increases the mutual information between
inputs and latents in the actual distribution.
For example, a world model learns representations that are informative of past inputs.
And a reverse skill predictor learns skills that maximally influence future inputs.
More formally, minimizing a joint KL realizes the preferences expressed by the target and
also maximizes a variational bound on the mutual information.
This bound is tighter the better the target distribution knows or learns to represent
the dependencies in the actual distribution.
For example, we can decompose the joint KL into a simplicity regularizer that keeps
the beliefs close to the prior under the target and an information bound that reconstructs
inputs.
This is known as the elbow objective except that we additionally have an input entropy
term in the information bound.
This cannot be dropped in our case because the data distribution is not fixed.
Analogously, we can decompose the joint KL into realizing preferences over future inputs
as typical in control, while maximizing an information bound that compares the beliefs
before and after observing inputs.
We have seen that agents with expressive targets maximize the mutual information between their
inputs and their latents . We can split the mutual information into the mutual information
between past inputs and the latents plus the additional mutual information between future
inputs and latents, given the past inputs.
This means the agent computes latent representations that are informative of past inputs and explores
future inputs that are informative of its latent representations.
The corresponding decomposition of the joint KL is included in the paper.
By choosing different types of latent variables and maximizing past and future mutual information,
we can derive a wide range of agent objectives.
For example, past actions are observed and those contribute no past objective terms.
Maximizing the mutual information between actions and future inputs is known as empowerment
and encourages the agent to choose actions that will maximally influence the environment.
Some entropy reinforcement learning uses a factorized target that simplifies the mutual
information to an entropy regularizer for the actions.
Similarly, skills are temporally abstract variables that condition the policy.
They also become observed during environment interaction and those contribute no past mutual
information term.
Their future information term leads to variational skill discovery.
In contrast, latent state estimates or model parameters are never observed.
Past information terms correspond to state estimation and system identification respectively.
Their future term is known as the expected information gain that steers the agent towards
inputs that are expected to convey the most bits of information about their representations.
In the paper, we analyze eight pairs of actual and target distributions to derive a range
of common agent objectives.
New agent objectives can be derived by choosing different latent variables, target factorizations
and divergence measures.
Vergence minimization inherits an intuitive interpretation from the free energy principle.
While action and perception optimize the same objective, they operate on different values.
Given a unified target, perception uses past inputs to align the agent's beliefs better
with the world, while actions affect future inputs to align the world better with the
agent's beliefs.
Finally, we can think about the input distribution that the agent aims to converge to.
By minimizing the joint KL, the agent also brings the input marginals together.
The marginal target distribution over inputs is known as the marginal likelihood and measures
how well the agent can possibly learn to predict a certain input sequence.
The agent thus tries to find a niche in the environment where it sees inputs proportionally
to how well its model can learn to predict them.
That is large because of the information gain exploration and that the agent can inhabit
despite external perturbations.
In other words, the agent finds a large niche that it can both inhabit and understand.
Designing world models that assign higher probability to more input trajectories and
using them as targets, this leads to agents that explore faster and converge to wider
niches.
In summary, action and perception as divergence minimization maps the space of possible agent
objectives, which correspond to different latent variables, target factorizations and
divergence measures.
It offers a unified perspective on representation learning, information gain exploration and
empowerment, and it shows that representation learning should be paired with information
gain exploration to have a temporarily consistent objective function.
It further suggests world models as a path towards adaptive information maximizing agents
while making task rewards optional.
We suggest to derive future agent objectives from a joint divergence, to facilitate comparison,
make explicit the distribution that the agent aims to converge to, and highlight the intrinsic
objective terms needed to reach that distribution.
Thank you to my co-authors and to many of my colleagues for feedback and discussions.
To read the paper, visit danijahr.com slash apd.
Thank you.

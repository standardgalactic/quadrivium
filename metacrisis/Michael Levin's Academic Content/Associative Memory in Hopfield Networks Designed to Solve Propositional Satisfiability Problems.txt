Okay, so I'm Mark James, I'm a philosopher in cognitive science and Tesh and I are both
part of something called, that's my cat, forgive her, Tesh and I are some part of something
called the Embodied Cognitive Science Unit, we're situated in the Okinawa Institute of
Science and Technology in Japan, and our PI in the unit is Tom Froze, and the unit was
kind of set up in the tradition of someone like Francisco Varela, intended to be a kind
of scientific inquiry into a kind of generally non-representational approach to cognition.
Maybe the kind of like maybe guiding model of our unit is the idea that interaction matters,
so that's interaction between say, brain, body and environment, but also individual
person and the various scales that both comprise them and they help comprise.
So we have projects on the individual science of agency and sensory substitution, we have
some projects looking at data interaction, be they say real-time reciprocal interaction
face-to-face or in digitally mediated environments, we have people looking at collectives, so
like criticality in the swarms of bees or in teams, we also have archaeologists looking
at the emergence of social complexity and even astrobiologists looking at resilience
at a planetary scale, and at least part of the ambition is to acknowledge the kind of
synthesis across these scales and how all these things interact, which is an ongoing
work, as you might imagine.
Our unit PI itself tends to be focused on questions related to the interaction of let's
say mind and matter, so questions around consciousness.
Personally I'm interested in how all these interactions can be leveraged in a kind of
practical sense to serve the redirection of our behavior towards health, I guess broadly
speaking, and TESH I guess is one of the people working on one of the models that we
find helpful in all of this to think about how these systems learn and evolve together
as the notion of self-optimization, so I'm happy to jump back in for any sort of questions
or whatever, but I'll let TESH take it over from here.
Thank you Mark, yeah, so Tesh or Natalia officially, just a bit about my background, I'm originally
trained as a chemist and I did theoretical chemistry for my monsters, then during my PhD
in OIST I found an opportunity to switch to the field of cognitive science and I got very
lucky in that I could combine the skills I had with a topic I find very interesting.
So the work that I will present you today is based on this paper we published in December
and in it we show that how can we leverage the power of associative memory of hopeful
neural networks to solve complex logical problems, specifically propositional satisfiability
problems, this paper is fairly technical but it ties neatly with what I want to do in
my own research project which is related to creativity because with this computational
model we get to explore how dynamic assistive can break constraints of a problem that it
is supposed to solve and constraints are integral to creativity, so Michael you're familiar
with this model but just to be in the interest of self-contained I will give introduction
to hopeful networks and then disability problems and I'll show how we can cover maps with it.
So hopeful neural networks named after John Hopfield are a former recurring network meaning
that there are back and forth connections between all the nodes of the network.
To describe the dynamics of the system we can use a vector of states s where each node
can be described either as a discrete or has a discrete or continuous value, in this work
we use a bipolar value of minus one and one.
We update the system asynchronously meaning that at each step of the simulation only one
node is randomly picked and updated.
The output of that node is a function of a weighted sum of all the inputs from all the
nodes in the network, in this specific work we use a simple step pressure function.
So then with this state update what will happen is that the network will start rearranging
itself depending on the sign and the value of the weight wij between any two nodes.
So si might switch sign or sj might switch sign and these rearrangements of signs will
result in a different state of the system.
We can then evaluate the difference between these two states using so-called energy function
described by this mathematical formula which reflects the misalignment of stress within
the state.
The minus sign indicates that the energy decreases as the network moves towards a stable state.
So at some point when the energy does not change anymore we know that the system reached
a stable state or an attractor.
So hopeful neural networks are recurring neural networks that converge to a stable state.
This converges to a stable state can be used in several ways.
So first we can find the solution to a complex combinatorial problem, for example the traveling
salesman problem.
This is a problem that you given a certain list of cities and you need to find the shortest
route visiting all the cities exactly once and return to the starting point.
So Hopfield and Tank show that if the connections of the Hopfield neural network, the weight
matrix w, correspond to the constraints of this problem then the natural dynamics of
the system is to converge to a stable state that will correspond to a locally optimal
solution to that problem, meaning one of the possible routes.
Now another way we can use this convergence to a stable state of Hopfield networks is
to recall patterns for example.
And this is what Hopfield did in his original work.
He showed that we can take some patterns for example images and use heavy learning to
store them in the network's connections.
This is a learning rule often summarized as cells that fire together, wire together.
If the system then is introduced to a partial pattern or corrupted image of the stored memory,
it will act as an associative memory and converge to a state representing that stored memory.
Now what I mentioned briefly before is that the usual dynamics of the Hopfield network
is to converge to a local minimum.
This is often results in a suboptimal solution.
In fact in that traveling salesman problem paper Hopfield and Tank mentioned that this
system will only find the shortest path 50% of the time.
This problem however can be resolved if we use the associative memory capability of Hopfield
network.
So and this is what Richard Watson and his colleagues did.
They showed that if we take some abstract combinatorial optimization problem and constraints of that
problem will be again represented in the weights, but we will also update the weights
using heavy learning with certain learning rate alpha, meaning that we imprint on the
weight matrix every state it visits.
And we also reset the system to some random initial state every time it converges to local
minimum.
Then the system can reach a global minimum, which potentially means the best solution.
So this model which we address as the self optimization model in our group was shown
to be generally enough to model various complex adaptive systems from gene regulation networks
to selfish agents and socio-political networks to just name a few.
And if I'm not mistaken in the recent paper with you, Michael, you are using slight modification
of this model to show transition in agency.
However, all of this research focuses mainly on abstract problems.
Now, abstract problems are very important for providing understanding of the general
behavior of the process.
But when the weight matrix doesn't represent a concrete set of constraints, it is hard
to analyze what exactly learning does to the system.
And this is what we wanted to investigate in this work.
We showed that the model can also solve concrete problems as well, specifically the satisfiability
problems or SAT problems.
By having concrete problems, we can then provide an insight to the relationship between learning
and breaking of the problem constraints.
So just before I jump to the results, I want to give a quick overview of satisfiability
problems.
So satisfiability problems or SAT in short are questions and logic where you need to
determine if there is a way to assign true values to a variable such that a certain proposition
of formal is true.
This has immense applications since many important real-world problems in different scientific
fields can be expressed as max k SAT, which means maximum satisfiability problems with
a maximum of k literals per class.
And I'll explain what that means in a bit.
So anything from planning problems in engineering can verify software and computer science to
solving bioinformatics problems.
To give you an example of a problem, of a SAT problem, let's look at Lyres problem.
So imagine we have a room with four people, Alice, Bob, Cal, and Dan.
And some of them or all of them can make statements here.
Alice says Dan is a liar, Dan says Bob is a truth teller, Cal says both is a liar.
And so our goal is given those statements to find out who is actually the truth teller
or who is a liar.
These facts can be represented by a vector of states s with the fact that the i of person
is a liar or truth teller represented by the state si, where if si equals one, it denotes
that the person i being a truth teller and si equals zero denotes that person being a
liar.
So from this statement, using these representations, we can build logical relations.
And then from these relations, we can get our propositional formula.
And if somebody doesn't have a background in satisfiability, I recommend this link here
below as an initial tutorial.
I initially didn't have a background in satisfiability, so this was very useful to me.
But what we see here in this propositional formula, each term in the brackets is essentially
a clause.
There are two literals per clause, and they're separate by an operator here.
The operator inside the first term is an or or disjunction.
And the operators connecting each term, each brackets, each clause are operators of and
or conjunction.
So this is a formula of six clauses, so a conjunction of the disjunctions.
And each term here has two literals in this in in the in the clause.
So this is a max two set problem.
So only two literals per each clause.
This will be relevant in a bit.
That's why I'm kind of explaining it.
So now that we have this propositional formula, the next thing that we would do, we would
put it inside of a set solver.
And there is a plethora of set solvers out there.
There's a competition on set solvers every year in an interesting field.
And what a set solver does essentially, it outputs two possible outputs.
So either this propositional formula is satisfiable, and then it will provide us with an assignment
of the values that we're interested in.
So in this case, Alice in Calour Liars, Bob, and then the truth tellers.
Or it will output no, it's unsatisfiable.
So given those relations that we give it, we cannot satisfy this propositional formula.
So in this in this work, what we were interested in is, can we take this propositional formula
and start using this sol model such that if the there is a solution, so the system will
converge to a stable state with energy equals zero.
And each node will represent our assignment.
Or if there is no solution, or then the state will be with higher energy, such that every
unit of an energy will present how many clauses were not satisfied.
So what we needed, what we needed is a way of mapping this set problem to the Hopf-H端ner
network.
Fortunately, this mapping was already performed in the 90s, first by Gottlieb Pinkins, and
then followed by one abdua, and his method is the one we implement here.
In short, given the description of the problem by the propositional formula, this method
allows us to describe the formula as an energy expression.
So then we can compare this term, this expression term by term with the expression of the energy
of the Hopf-H端ner network.
And this will give us a weight matrix that will describe the set problem.
So for this specific problem, a max to solve problem, as I said, the Liars problem.
So similarly to when the weights describe the constraints of a traveling salesman problem,
and then the dynamics of the system will give us the solution, the shortest path.
By using this method, we can use any set problem, and the dynamics of the system will give us
the solution to the that set problem.
So this is what it is.
So using this model, we can look again at the Liars problem.
We took a bit more, 50 people with 34 statements.
So person 1 says, person 4 is a liar, and so on.
So what we see here in this pot A is how this weight matrix initially looks like, that represents
those 34 statements.
Then in this pot, what we see is basically the solutions.
There's essentially three distribution of energies.
First let's look from 0 to 1000, the dark blue.
This is basically regular Hopf-H端ner dynamics, no learning.
What would happen if we just run the simulation for this weight matrix on the Hopf-H端ner
dynamics, and let it converge, and then reset it again?
So what we see is that there is basically the system explores all possible solutions.
Sometimes it finds the correct solution.
Sometimes it converges to a stable state of energy four, meaning there's four clauses
that were not satisfied.
Now the red part, the self-fantasy, is basically when we turn on the learning, this is the
SL model.
So what you see here is that after a certain amount of resets, the entire system converges
to a global solution, the correct one, with energy equals zero, meaning it managed to
find a state that satisfies all the constraints.
And what is interesting is that once we turn off the learning, this is the light blue line
at the end, even though we continue resetting the system to some random initial state, it
stays on the global solution.
So yeah, and just to show you, this is how the weight matrix looks like at the end of
the simulation.
So this is a system that augments its behavior with associative memory of its own attractors
and self, and finds a global solution, hence optimization.
Now this is a simple example.
We used to show that indeed the SL model can solve set problems, and that the learning
improves the chance for arriving at a global solution.
However, a more interesting scenario to examine is what happens when there is no solution
to the problem.
So for this, we use the map coloring problem.
So here we have the map of South America, and the goal is to cover all regions by distinct
colors such that no two bordering regions have the same color.
So we only want two colors with two colors, so this is not a problem, that's what I meant
by that.
You need four colors for this specific map.
But we find this example interesting, and I'll explain why.
So first of all, we represent the state of the map by a matrix.
Now here S is a matrix where Sij equals one denotes that region i has colored j.
This is a bit more complex notation, but it allows us to generalize the problem to the
cases when we want to use more than two colors.
So this is why we chose to use this notation.
And this problem can be represented by these three requirements.
So each region must be colored, a region cannot be two distinct colors at the same time.
Because with this notation, computationally, numerically, we can have a state like that
where the same region has different colors.
It doesn't make sense, but it can be allowed computationally, but this is a requirement
that we need to have.
So it cannot be two distinct colors, and the third requirement, regions that share a border
should have a different color.
So this is how we represented logic-wise.
You don't need to remember it.
I'm just showing it.
It's a bit more complex than the liar's problem.
And so these are three different sets of constraints, and our propositional formula would be basically
a conjunction of those three sets.
And here with what we did, we also added weights to the sets of causes.
So these two constraints are kind of our preferences, and I'll explain what it means.
So if we don't add any weights, let's just take in these three sets of constraints.
They all have the same weight.
What you see here is how that weight matrix of map-cowering problem of South America would
look like.
But again, as I explained, we might have a situation where a certain region doesn't have
a proper color.
So in order to avoid it, we also tried to put more weight on the first two sets of constraints,
meaning that each region, first and foremost, must be a proper color.
Another way we also tried to do this is instead of putting more weights on the first two set
constraints, we thought, OK, we can adjust the weight on the third set of constraints
by the border length of a country, meaning that adjacent regions are, sorry, the border
between two countries.
So meaning that adjacent regions are less forced to be covered differently if they have
a shorter border.
So if we look, for example, again, on the map, if we look at Peru and Chile, we see
that they share a very short border between them.
So if they're covered the same color, it's not that bad.
However, if we color Chile and Argentina in the same color, since they share such a long
border between them, that would be a bit more problematic.
So this is what this third set of weights means.
So here are the results.
So here, again, plot A and plot B show how the weight matrix representing the problem
looks before and after the simulation.
And then D basically shows the final product of the simulation, the converged state.
And so here, dark blue and light blue basically are proper colors, and orange is non-color.
So I know it's a bit hard to visualize what is non-color.
So this is how we visualize it.
So we see that there are three regions that are not a proper color, and the energy for
that is free.
So that's correlated.
So there's three clauses that were not satisfied.
However, in this situation, there's no point talking about borders between countries because
when you have non-color, so this is basically why we did this additional weights.
So in the second scenario, where we put weights now on making sure that all the regions have
proper color, now everything color properly as anticipated, however, what we can see here
is that it may be hard to see, but if you squeeze your eyes, there are eight borders
that were violated between the different regions, and the corresponding energy is eight.
And the third one with the borders one that we tried out, it's even worse energy-wise,
but we can do that, we wanted to try it out and see what it gets us.
So just to kind of have a short summary right now, so far I was discussing the results from
a satisfiability angle.
We saw that the SL model can solve some problems and we examined how we can manipulate our preferences
through our different solutions when we have this scenario where there is no correct solution.
What I would like to do now is to switch to talk about particularly interesting result,
and that is what we see here in the last plot in the corner.
If we zoom on it, we can see the region of Costa Rica in the specific run has the same
color as Panama.
Now, since nothing is attached to Costa Rica in this map, in principle, it should not cost
anything to the system to flip the color of Costa Rica to be distinct from the one Panama has.
However, this is not the case.
So what this example shows is how heavy learning can dramatically modify the weight matrix
that encodes the problem of the constraints such that some of these constraints are no
longer contained within it.
So in this example, in the unmodified weight matrix, the color of Costa Rica can be trivially
changed to satisfy the constraint on having opposite colors across borders.
However, a state with that color flipped is no longer an energetic minimum of the learned
weight matrix.
And this is quite interesting because this would never happen in the traditional set
solver.
Of course, this is not a good solution, but this is obvious since this problem has no
solution.
This is a worse solution, but that's not the point.
The point is to try to investigate how breaking of constraints happens through learning.
And that is hard to do in abstract problem because you can't understand what is a particular
state means in relation to the original weight matrix.
And with concrete problems like this, we can explore domains that were previously inaccessible.
So the next step would be to try to basically investigate different problems and see how
you can use this knowledge to explore different types of domains.
So to just summarize again, some limitations in future work, the SO model is much slower
than the state-of-the-art set solver.
But yeah, we were not trying to beat the set solvers.
The SO model has some potential for parallelization, and it can be tweaked to work faster.
But we find that it might be interesting for engineers that are interested in biological
possibility, let's say, since it is much more biologically realistic than a set solver.
Another thing with it is that, as I showed, the SO model, we did a max to set problems.
And the SO model in principle cannot handle set instances with more than two liters per
cross, that's what you would think.
However, we know that any case set problem can be reduced to a free set problem, and
any free set problem can be reduced to a max to set problem in both cases at the expense
of linear number of new variables.
So this means that in principle, any case set problem can be reduced to max to set problem,
and then the SO model can be used to solve it.
So it will have just much more, let's say, hidden variables.
About future work, so this specific topic, now that we know that we have this kind of
connections between initial constraints and what happens to them after learning,
we can ask, what did the system learn?
And maybe, for example, if we look at the learned weight matrix,
can we convert back to the logic and see what causes were eliminated, for example?
That's one idea for how the future work can go.
So finally, I would like to acknowledge my co-authors, Werner Koch and Ozan Erdem,
of course, Tom, and this is our lovely unit.
And thank you for listening.
Well, thanks so much.
That's super interesting.
I have a couple of thoughts, and I don't know, maybe this is obvious,
and you guys have already thought about it, but I'm just cluing into this now.
So the first thing is what occurs to me is that the initial set of requirements,
the logical statements that you're modeling, you could think of it as a set of beliefs or
preferences by an active agent of some sort.
And what then you're doing is you're producing a network that can act as a controller for that
agent that would navigate some space in a way that's consistent with its beliefs or preferences.
So I hadn't thought about this before, but I think it's a very interesting
map because I can tell you some related things that we're doing to this.
It's like you're moving from a statement of what the system either believes to be true
or would like to be true, and now you've got an actual
bio-realistic controller that seeks to implement it to you.
So by satisfying it, what you're really doing is navigating a space to a region where the
things that the agent is looking for are in fact true.
Does that make sense to you or have I misunderstood?
I mean, it seems like this is an agent navigation task here.
Well, I love the way you think about it as an agent navigation task.
So, you know, so far my angle on this was always more technical.
So I kind of always look for what is this agent in kind of a verb,
but then I would love to try it out.
But then how would you describe the problem, let's say?
Well, the problem is, so, okay, so I'm, and when I say agent, this could be a cell
or an autonomous vehicle or whatever it's going to be.
I am in a world and I have, this is something Mark Solms talks about a lot,
is this idea of having multiple constraints or multiple drives or needs.
And your goal is to have all of them met.
And sometimes it's impossible to have them all met.
But, you know, so my drive is that I want, in your initial case,
you know, this one is a liar, that one's not a liar.
So I want this one, I want to be in a region of my space where this thing is true
and this thing is also true.
And this other thing is not true.
I want to not be hungry.
I want to reproduce and I want to be out of the danger zone in my environment.
Right.
So those are three things you want.
And there are many areas where you can have one of the three,
but not the other two or two of the three.
And maybe if you're lucky, there's a region of the space
where all of those three things can be met and that's where you would like to go.
So this sounds to me like a typical problem of drive satisfaction for an agent
that has a multi-dimensional set of things that it wants to maximize or optimize.
And what you're looking at here.
And so now, okay, so that's the thing you want.
Now, the question is how do you get there?
So you've got this landscape.
How do you find the place that's going to meet all your requirements?
And it sounds like you've got a mechanism here,
which is actually a bio-realistic controller of how you navigate that landscape.
So what I love about this is that one way to think about the spectrum of agency
is to really think about what the world looks like from the perspective of that agent.
If you're dealing with a bowling ball on a landscape,
all you need to know is your vision as a third party observer,
as your vision of the landscape.
And that tells you the whole story of what's going to happen.
But if you've got a mouse on the landscape,
your view of the landscape doesn't matter that much.
What you want is to know,
you want to know the mouse's internal representation of that landscape.
Where has he been rewarded, punished, what are the valences?
Because you're not going to be able to predict his motion from your view of the landscape.
You need to know what his view is.
So what I see here is a really interesting formalism for specifying
what are the drives and the goals and the preferences of an organism.
And then you're basically producing a vision of the internal map of that organism.
How does it feel about that space and where are the places where it's going to be happiest?
And where it's going to be happiest is where all the constraints are met.
But at least having some of them met is better than none.
So that's super cool.
And now you've shown a map of the landscape
and the internal, kind of a first person perspective,
the agent's view of the landscape and an actual controller,
an actionable, biorealistic controller,
that it can use to get where it needs to go.
I mean, I think that's amazing.
And I think there are so many applications of this.
I think that's super.
Awesome.
Yeah, sounds cool.
Thank you.
Yeah.
I like the way the vision of the internal map, I guess.
I mean, I mean, it's, you know, it's, I think, you know,
the speculating a little bit about what will be the internal map of mice is a bit too much,
I guess.
But for us trying to, let's say, my goal usually was to understand what you can manipulate,
let's say, in the environment, such that you can satisfy, let's say,
other different constraints.
So, and then you can see how that can help.
The actual map maybe doesn't even matter.
What it matters is that we have this controller that we can modify.
Yeah.
Yeah.
I mean, forget the mouse, but I think easily you could produce a minimal,
you know, a minimal agent in a virtual world that has some needs,
that it would like some variables that it would like to be in a certain level.
Right. So, you can either go with a true false thing, or you could do fuzzy logic and say,
I want my hunger level maximized, but this other thing, I want my temperature in between this
and this.
So, you have some statements of things you prefer, and literally use that Hopfield network as a
controller and watch the Asian navigate.
I mean, you can actually watch it navigate its world.
I think this is incredibly powerful for linking,
because what you started with is a kind of,
no, this is not verbal, it's not right, but like an explicit description of what
the agent might prefer.
And from that, you got to a dynamical actual controller that might implement it.
And that's really, I mean, I could think of all sorts of model systems where that would be useful.
So, I'm also thinking, I'm also thinking in reverse, the thing you said at the end about
trying to read the map at the end, right?
So, that sounds to me like a neural decoding task where the idea is that,
so neuroscience is what they want to do.
They want to scan the brain.
And from that, they want to recover the propositional information that's there.
So, they want to read a bunch of electrophysiological signals and say,
you remember having breakfast like this and you believe in this and you've never seen this
and this is, your preference is that.
So, right, they want to recover those things.
So, this also, and we do this all the time, but we do it in outside the brain.
So, we will look at an early embryonic tissue and we will say,
okay, what shape do you believe you need to be?
Because those are the kinds of things like literally when we recover these bioelectrical
pre-patterns, we can actually recover goal states, anatomical goal states.
And to us, this is, again, just a navigation of anatomical state space where we say, okay,
I see where you're trying to get to is this particular region where you've got five fingers
and you've got this size and whatever.
So, and so we try to do that decoding too, like look at the bioelectrics and say, okay,
so what do you think you are?
Are you a zebrafish?
Are you a frog tail?
What are you?
And I really like this example too, your model, because that's like a simplified model where
you can practice that.
And you can say, can we look at the controller and the time profile of the controller
and extract from that the logical statements, basically read the mind of the creature.
What do you want to be true?
What are your goals?
We cannot do that yet, but I mean, I think maybe it's possible.
Yeah, no, no, I understand.
I mean, it's certainly in the neural context, it's also extremely hard, but this is a simplified
version where we can maybe test some techniques because there are some, obviously, there are
some AI tools that are coming online for inverting some of these things.
I mean, it's an inverse problem, right?
This is what you're trying to do.
Yeah, I think that's incredibly interesting.
Do you know, and so just a related thing that might be interesting to you, do you know the
work of Patrick Grimm on the dynamical systems of view of logical statements?
No, not, and I would be happy if you can forward me a paper too.
I'll send you, yeah, it's very interesting, and it has a number of papers.
So basically, just kind of briefly, it goes like this.
So Grimm is a philosopher in New York, and in the 90s, he developed the following thing.
Consider the simple liar paradox.
Okay, so you've got a sentence as this sentence is false.
The reason it's a paradox is that if you insist on a single unchanging truth value,
then you're stuck, right, because that doesn't work.
However, Grimm did two things.
One is he allowed it to have time, which means that what you can actually do is just
view it as an oscillator, because as you go through the sentence, right, it's true,
but then it's false.
So you've got the simple oscillator that just goes up and down.
And so he gave it time, which immediately removes the paradoxical nature of it,
because now there's no problem, it becomes a dynamical system.
And then the next thing he did was to just go to fuzzy logic and say that, okay,
sentences could have any value from zero to one.
And now you can have groups of sentences that go like this.
Sentence A might say, I am 80% as true as sentence B is false.
And sentence B might say, I am 30% true.
Okay, there's two sentences and they refer to each other.
And so now what you can do now that you view them as dynamical systems, you can plot them.
And you can either plot them as a function of time and you get some sort of time dependent
behavior, or you can use the extra dimension and just make a static shape out of it.
And so now what happens, right?
And so they become, depending on your sentences, they become, some of them are fractal,
some of them have weird attractors that are, some of them stabilize, right?
Some groups of sentences stabilize, some don't, some have all kinds of attractors and so on.
So what it does is that again, which is what I saw here, a very cool connection between
logical statements and sort of propositional content and dynamical behavior.
And getting behavior out of logic, getting spatial, and again, spatial, I mean,
the grim thing too is a kind of spatial navigation in some sort of truth space or something.
You know, I think, I think is extremely interesting.
So we're doing a bunch of work on that now, looking at the different types of sentences,
right? So the liar paradox is a basic oscillator, all it does is, you know, sort of other things
are even more boring as a simple statement that has one truth valid just like sits there.
This thing oscillates, other systems have all kinds of complex behaviors.
And once it becomes a dynamical system, then you can try to do something really crazy.
And this is, I don't have any results on this yet, but just, you know,
as we've shown with models, dynamical systems, models of gene regulatory networks,
they're often trainable. In other words, they have six different kinds of memory that they can
form from inputs. And so, including Pavlovian conditioning, you can literally train these
things. And so, yeah, and biological networks are much more trainable than random networks.
So I think evolution actually likes that property. So now you can think about doing
something completely nuts, which is to train groups of logical sentences.
And so if that's true, I think what happens is that you're not really training the sentences,
you're training a virtual, a simple and extremely simple virtual mind that contains those sentences
as its cognitive commitments. That's what you're training, even though, even though, you know,
you're sort of neglecting the rest of it, we're not modeling the rest of the organism,
we're just modeling that. So, yeah, so I, you know, I think I need to think about this more,
but I think that's one of the things that you have here is an amazing connection between
the propositional content of an agent's mind and a behavior, a dynamical system
controller that guides its behavior in space. I think that's pretty wild.
I like the way you phrase it. Maybe that would be the next paper.
Pretty wild. Yeah, yeah.
Yeah, I don't have a lot of, at what you said, I mean, that's a very nice
thinking about it on a more kind of a meta level. Yeah, I would love to think about if you're
interested, we can maybe do it together. The next problem that the agent will be solving and then
try to implement in this model, I would be happy to do it. Absolutely. Yeah, that'd be great. Yeah,
no, I'd love to collaborate. Yeah, we have a number of virtual, simple virtual systems that we use
them for making hybrids, so where we have neurons, you know, neurons. I mean, yeah, so one of the
things that we have is living cells that are instrumentized to inhabit a virtual world. So
by the signals they put out that moves them around in a virtual environment and then they
get rewards and punishments for, you know, they're basically, you know, it's basically a brain in
a vat living in this virtual environment. And one thing we can do is if we work a little bit on
the that inverse problem that we talked about, maybe we could actually apply it and pull out
pull out the cognitive commitments of that of that organism. So like imagine, you have a set of
cells, they learn to navigate an environment where certain regions are good or bad. And then we use
the reverse of the technique you just showed to extract a set of sentences where the thing literally
tells you, I hate this region over here, I like this region here. And this is the region where
all my needs are met. This is the region I like. We might even we might be able to actually extract
that, you know, those those statements, very simplified, you know, like, just a couple of
dimensions, three dimensions or something. Yeah, I think I understand what you would like to do.
I mean, right now I feel, you know, if you look at the weight matrices at the end, they're very
simple. They're like rank one, basically. So lots of information is gone. So I wonder how would that
need to be modified to, let's say, if you want to remember more than one statement or more than
things like that, then maybe it needs to get a bit more complex. But yeah, yeah, yeah, yeah. Well,
and there's also, you know, there's also the issue of, of course, there's going to be more than one.
Is that true? I think that's true. There's going to be more than one set of statements that are
consistent with any given matrix. So we're not recovering one unique, you know, mental snapshot
that belongs to each of these things. We're going to discuss maybe the simplest one, which is,
which is okay, right? That's, you know, kind of when we do theory of mind and age and age and
interaction. Again, we never know exactly what anybody's thinking, but you sort of say what,
okay, what's the simplest model of my conversation partner that explains what's, what's going on
here. So I think, I think that's fine. I think that's, that's, that's perfectly reasonable.
Yeah. Yeah, that's interesting. Okay, cool. So, so yeah, let me, let me think about this
some more and come out. Yeah, yeah, this is great. And I'll come up with a couple of simple
scenarios for things we can actually model, you know, simple things we can actually model in
this way. Sounds fantastic. Thank you. Yeah. Awesome. Thank you so much. Yeah. Any, any other
thoughts or maybe do, Werner, Mark, anything, anything you guys want to? Just reflecting on
what you're saying. I mean, it's a, something we're kind of looking at in a different domain,
let's say at the moment, from a kind of behavioral design perspective and trying to say,
well, you know, propositional content is maybe one dimension, but you also have say,
the kind of motivations and say the sets of capacities and skills from a kind of more embodied
perspective. And then before this of the environments, I mean, ultimately, if we can get
to a point where we're kind of modeling the interaction between all of those elements,
that would be fantastic, right? But simple is probably a good idea. Yeah. Yeah. And so the,
and so one way to add that. So, so I've been thinking about this for our case, because
if you have a group of sentences that refer to each other, to each other, right, in the, in the
kind of grim diagrams that I was talking about, I mean, that, that's all well and good. And you
get a shape, but, but if they only refer to each other, they're not tethered to any kind of real
world, right? They're sort of self-contained. They only refer to each other. But you can imagine
having atoms in there that do refer to a real world of, or at least a virtual world for their
embodiment. And you can say, you know, sentence A could be I am 50% as true as sentence B times
however true a sensor one is, right? And sensor one is some aspect of the, it's reading some
aspect of the outside world. And so now that system is not just a self-contained logical system
that sort of cycles in and of itself, but it's actually linked to the outside world, right?
And, and you can even then, then you can imagine multiple agent interactions where
here's a group of sentences, A through C, and here's one, you know, C through F, and they have
C in common, which means there's a kind of chemistry that can be made when they come together,
they have something in common, and they, you know, they might end up being incompatible,
or they might be compatible that knocks them into a particular region of the, of the space.
So you can imagine multi, multi-agent interactions, and you can absolutely tether them to the real
world by including variables in your sentences that, that correspond to some kind of affordance in
the, in the real world. So I think you could. So something else that seems to be interesting
there, like what you're describing really does map to the kind of phenomenology of,
let's say the development of behavior, right? We have a bunch of commitments and then we're
trying to work out where, where are the kind of congruences between them. But actually often,
like we fail to do that, right? As, as human beings, we kind of get stuck at times and we get kind
of good, so I don't know where the kind of congruences here, and that leads to ambivalence,
and it leads to ultimately times to disorder, right? So maybe there's actually the possibility
in here to start like, okay, well, I have all these sets of things, and there is some tension,
and maybe it seems kind of incompatible. Can we have a model that helps us understand how you
kind of resolve those tensions eventually? I have maybe one comment on what you said earlier about
an agent and trying to satisfy their desires or needs or whatever. If you, if you model that,
I think a critical problem is how you weigh the different desires, like if they all have the same
weight and there's, it's set up such that you cannot satisfy all constraints simultaneously.
How do you, what sort of weights do you put in? Or ideally, you would want a system to have a way
to determine that itself, because in the, in the map coloring problem, it's, it's very difficult to,
to have the different sets of constraints. You cannot use the same weights, because then it will,
in a problem where no solution exists, it will most likely find a solution that doesn't even
have proper colors, which is, which is why you should have weights attached to that in one form
or another. But it's not obvious how you would let the system decide that itself. And I think if you
have an agent with their desires trying to be met, you have, you run into the same problem. How do
you decide that? If you had some physical model that would lead to weights automatically, then,
okay, that's an option. But if it's just the system in an abstract space, and I don't think it's
immediately obvious how you would do that. But ideally, you would have to fit the system self-adjusting.
No, it's a great question. And I think I'm going to, I'm going to show this to Mark Holmes,
because he's been thinking about these things a lot. He builds minimal agents with drives,
with orthogonal drives. You know, I think, I think one way to do it would be to introduce a time
aspect. So like, so, so you're an agent, and you have a bunch of drives. And the first thing is
that's going to kick in is thirst. And that kicks in before hunger. And you're not really going to
be worried about hunger if you're really thirsty. But at some point, at some point, they both become
sort of critical, right? So some, some things have a, you know, you can almost imagine, I mean,
this is like me imagining is on the fly. So maybe nonsense. But, but, but there's almost like,
like a, like a gradient sharpness, you know, like a, like a, like a slope, where certain, certain,
certain desires ramp up very quickly, and others ramp up slowly. And so these weights
might change in a real organism that's probably set to some extent anyway, by evolution or by
prior experience. Here, you know, you could let it have control over those weights. So for example,
some of the, some of the values of your nodes in your network could actually
feed back to tweak some of the, some of the, some of the weights, right? So I mean,
I can't even imagine what would happen if you did that, but, but that's easy enough to,
easy enough to check. And it's also
that's, that sounds like something that would easily become unstable. Like, I mean, the
Hopfield network is, it's stable, right? Like it converges to some state. But if you, if you
have feedback from the state into the weight matrix directly, I mean, the heavy and learning
does that to a certain extent, right? That is feedback. But it's, it's stable if the learning
rate is low enough. Yeah. If you start introducing direct feedback from the state into the weights,
then I can see this spiraling out of control very quickly. And then you, you have the same
problem on a meta level that you as the experimenter need to twist knobs on the algorithm to keep
stability. Maybe, maybe you as the algorithm, or maybe we have a second network, like a
metacognitive component, right? I could imagine ways to play with it. It may even be a model of
psychological, the kind of thing where you're, you have some incompatible drives that can't
all be true at once. And then eventually you just decide that, you know what, this one thing,
it's not worth it. It's killing all the other stuff, right? It's like, it's like, yes, you know,
I really like that chocolate cake, you know, in the middle of the night, but it's just,
you know, it's messing up my other stuff. And so I'm just, I'm just going to decide that that's
it. That one's out now. And, and so some ability to adding that metacognitive loop where there's,
there's some ability to reweight them based on how things are going. If your experience is that,
you know, I could, I could make nine out of 10 happen. It's just that 10th one that just always
who would like, I'm just like, I'm not paying attention at anyone. So I could, you could imagine
in biology, some of that works. I mean, you're never going to be able to cross off the things
like, you know, thirst and so on off your list. But there are other things you can, if it's really,
and I bet there's, I bet there are experimental data with, you know, probably rats or something
where after a while they just decide that within this environment, there's one thing that they
need to give up and then life gets better. So I don't know, I, it seems like it seems like we could,
we could try various on top of the system you showed today, various kinds of
changes that'll, that'll make that fly. Yeah, that's a very simple model. So I think there's
many ways you can make it more complex and break our head of trying to make it stable.
Cool.
Okay. Well, fabulous. Thank you so much. That was really interesting. Yeah, let me ponder
it some more and we'll be in touch. I would be really interested in working together on this.
Thank you so much. Nice talking to you.

Yeah, I wanted to get your thoughts on a specific angle to some of the computational stuff that we've been doing lately.
Sure. I knew that you would have something on your mind, but I was busy racking my brains trying to think what is it that we're going to be talking about.
I thought, well, it doesn't matter with you. It's always interesting. So I'll just see what it is, and then we'll take it from there. So what is it that's on your mind?
All right. Thanks. So here's what I have. We have a preprint, and this is Adam Goldstein and a Tianning Zhang student of mine, and I have a preprint on the following.
What I wanted to do was to address this issue of non-obvious cognitive properties in very simple systems.
Okay, and you know, I like this kind of stuff. And so the thing that I chose specifically was sorting algorithms. So these are very simple algorithms that computer scientists all see as students study these.
And so I'll just give you a very quick rundown of what we had found. And so they're a good model system because they're fully deterministic. So they're completely transparent.
That's six lines of code or so. So there's nowhere to hide. In biology, there's always yet more mechanism that could be involved, but here you see all the steps of the algorithm.
They're fully deterministic, very, very simple. Everybody knows, everybody thinks they know what they're capable of. Everybody's been studying them. It's very obvious. And so I thought, okay, let's see if we can find any surprises there.
And we made a couple of tweaks to be able to study them from this basal cognition perspective. And then I think there may be sort of a psychoanalytic perspective here that I want to probe with your opinions on.
So the first thing we did is we just visualized. So the way they work is you start with a string of integers, let's say 100 integers, they're jumbled up in random order.
And what the algorithms are going to do is eventually sort them into order so that it's monotonic from, you know, from from one to 100.
And so what you can do is you can visualize the progression of any given string in that space. It starts off somewhere over here where the where it's quite random, and then they all sort of move to a spot where, in fact, guaranteed to move to a spot where everybody's in the same order.
So what you can do is you can plot this notion of sortedness, you know, how well sorted a given string is and you see that over time that sort of it increases.
Okay, so that's so that's the journey through space that this thing is taking that's the space that lives in. And now we can we can ask a couple of interesting questions.
The first question that we asked was in the in James's sense of delayed gratification, in other words, if we put a barrier in its place, is it capable of moving backwards away from its goal in order to reap gains later on you know the business with the two magnets
of a piece of wood versus Romeo and Juliet separate and so on.
So, keep in mind that in these algorithms, there is nothing like that there is nothing explicitly in the algorithm about well, you know, if you come upon a barrier, then you should temporarily back off that there's nothing like that we kept to have algorithms exactly as they are.
The way we do a barrier is we simulate a broken cell. So every cell has a certain number and the algorithm says move the four and the five and you know.
What happens if one of them is broken they won't move. If they won't move and this this this breaks an assumption in these algorithms usually you assume that the, that the hardware is reliable if the algorithm says swapped the numbers they swap.
And in fact the algorithm doesn't have any logic built in to see wait did it swap or didn't it it doesn't have any of that it assumes it just goes on its merry way assuming that what it said actually took place.
You said well biology isn't like that you have the biological medium is notoriously unreliable and noisy, but specifically if we put a barrier you're going along and you're sorting.
All of a sudden you come you need to move this number to get to improve things, but it won't move. It's broken. And there's one of two ways it could be broken either it could refuse to be moved at all or it could refuse to
to request to be moved we'll deal with that momentarily but but anyway let's say it's broken and so what we found is that
these these algorithms, despite the fact of not having any specific code for it, when they do come to a to a barrier like this.
Back off, meaning the sordidness actually drops the string becomes less sorted. They do some other stuff they move other numbers around, and then eventually they go around and things and things improve.
And so this, so they have some ability for this kind of delayed gratification they can back away from their goal in order to reap gains later on and they have a little horizon that they're willing to, you know, the willing and able to do that.
So that was that was kind of the first the first thing we found and that's not really the thing I want to focus on but I thought I would, you know, say it just in case you had anything interesting to say about that.
So, so there's this, there's this interesting new.
Can I pause you for a moment.
Sorry, you know, the, I just want to be sure I'm following correctly Mike.
This algorithm.
Is it a stepwise procedure or is it an end goal that it's, in other words, is it aiming for the end goal that totally sorted sequence, or is it just following the steps that that are coded.
The steps there is no it is not a, it is not an explicit goal directed loop at all basically it has certain criteria it says look at some numbers and then based on that shuffle the numbers and that's it.
And what just so happens that if you execute that long enough you will eventually end up with a string, but there is no logic in there there is no logic that asks is, is, you know, how are we doing did it work here how are we doing is it sorted there isn't any there's
no self monitoring there's no there's nothing like it.
Thank you.
So, okay.
The second thing we did is instead so the usual, these, the way these algorithms are usually done is that there's a centralized controller that's sort of can see a god like can see the whole string and is following the algorithm and is moving, you know, moving numbers
So what we did was was we got rid of that and we said instead each cell each individual cell is following the algorithm.
So the four wants a five on its on its right and the three on its left and and each each individual cell has preferences about what its neighbors are, but there is no single central controller that sees the whole thing.
And if you do that, it still works.
And it's, you know, it's fine it's still it still works so that's interesting you can make it sort of bottom up and distributed in that in that way.
It also allows you to do an interesting thing which you couldn't do before which is you can make a chimeric string, you can make a string compose an array of numbers composed of numbers that are actually following different algorithms.
So, so we looked at three different there are three different sorting algorithms that we that we used a selection sort insertion sort and bubble sort they're called.
You have one algorithm and you're following that one algorithm to move the numbers around but now that the numbers are moving themselves around.
What you can say is well some number of them are going to be following this policy some numbers some some of them are going to be following that policy.
And, and if you do that, it also still works, meaning that they don't all have to be following the same policy but here's the here's the crazy part.
You have a you have a now an array of 100 cells and each of those cells has a number to it that's what you're sorting you're sorting on that number and they're starting out random.
What what we did was to assign algorithms to each of those cells randomly one of two.
So, so Adam came up with a nice name for it he calls it an algotype the way you have a genotype and a phenotype now now every cell has an algotype the algotype is either it's either the it's either bubble sort or it's insertion sort that's it.
And the algotypes are also selected randomly.
Now we ask a simple question during the lifetime of this this array as it's doing a sorting thing.
What are what's the internal structure like that is what is the probability that any given cell looks to its neighbor and says oh he's just like me same algotype as me.
What's the probability of that. Now, in the beginning that probability is 50% because we assign algotypes to numbers randomly. So you know that it's a random it's 5050 whether the guy next to you is same as you.
At the end of this thing. It's also random because at the end we're sorting on digits on the digit values, and we know that the assignment of digit values to algotypes is random.
So at the end it's also going to be random.
But now the question is what does it do in the middle. And what we find that it does in the middle is if this is this is time here and this is you know 50% it's here.
It starts out at 50% it goes up, and then it then comes back down in other words what happens during the sorting is that cells tend to cluster next to ones like them.
They cluster next to their own kin, basically. Now, keep in mind in the algorithm there is nothing about there is no code in the algorithm. What algotype am I what algotype is my neighbor I want to go sit next to this one I don't like sitting next to that one.
There's nothing like that in there. If you examine the physics of the system determined by the by the algorithm itself, you don't see any of that. The only time you see this is if you actually is is if we actually look and
and ask this question of, we know what their algotypes are. And we say, so so in a certain sense or you can probably see why I started thinking, you know, about analysis because it's what we are doing is uncovering a hidden cause of their behavior that is to them
is different at all. We can see that, oh, you're clustering in a particular way there's a behavioral pattern that we're identifying here, but it isn't explicitly coded by anything that you know if they were to examine the
wrong code or if anybody would examine the wrong code there's nothing in the code that says that. So, there's so there are a few interesting things here that I that I really like one is that it's a kind of it's an extremely minimal metaphor for this existential
condition of living things where, eventually in this universe the inexorable force of the algorithm is going to pull apart all of those, you know, clustered pieces, because in the end everybody's got to get sorted according to their number.
The physics grinds you down and sort of, you know, undoes whatever organization you had at the beginning, but during the process, you have some amount of time where in a way consistent with the physics I mean there's no magic you're following the algorithm, but there's an
interesting behavioral
tendency is, you know, that that that you're able to satisfy for some period of time during during your lifetime. And it's something that simultaneously is it's not against the algorithm that
you're that you're following, it's consistent with it, but it's also, but it's not specified in the algorithm. And for some amount of time, it's actually at cross purposes, because, because you're being you know your behavior, you're keeping things together that eventually
will be pulled apart. We also did one more experiment we asked how strong is this tendency to be to be to keep together, because, and one way one way you can, when we can ask that is by allowing repeat digits.
You know, let's say, let's say, let's say there's, we have 100 total, and every and every digit can occur 10 times so there's only 10 digits total you know there's 10 fives 10 sixes and so on.
If you do that, what happens is, then you can kind of satisfy both in other words, in a run of 10 fives, they can be where they're supposed to be but five of them could be one elbow type and five of them could be the other.
So it sort of makes it possible for you to retain the clustering while still be consistent with the algorithm. And if you do that they cluster even more, you know that which which shows that these things are actually somewhat across purposes if you take off the pressure, if you allow them to do what they really
want to do which is to be clustered, the clustering goes up.
Yeah. So, so I'm interested in this in your thoughts on just in general this notion of, of a system.
This is an extremely simple one but of course you know you and I are both interested in minimal agents and so on.
These kinds of explicit drives and behaviors that aren't in the algorithm that are emergent that are not obvious but that can be discovered.
So we take seriously, you know, the study of how this thing reacts in this in the problem space that it's working in. And if we don't assume for the moment that it's just, even though it's deterministic and then so on that we don't assume that it's this dumb thing that
only does what the, what the algorithm says, it really you know it really it really made me feel like.
We were we were uncovering these hidden hidden motivations for the patterns and its behavior that, well it's not complex enough to of course realize it but but we can see them.
So, yeah, I don't know what do you what do you think about all that.
Well, first of all, as always, you know you, I don't know how you come up with the problems that that you spend your time on, you know, they, they, they, they, you have the most extraordinarily creative mind.
You know, so I'll just as you told the story I'm thinking, this is my clever.
Nobody else is going to have this conversation with you ever.
Are we getting some sort of interference.
No, I'm trying to shut off somebody's texting me and I'm trying to.
Oh, okay.
So, I was thinking it through and trying to understand why it would do what it's doing.
And obviously I don't know the algorithms but it was sort of making sense to me that why these emergent behaviors, I was thinking they ways in which I can understand this would happen.
So, for example, your own logo type is probably more predictable.
If it's functioning by the same principle that you are, you will, you will prefer to operate in a space that is that is understandable to that behaves in the way that you expect and you there's less work required from you.
So also, you know, the how that comes about made made a sort of, you know, a putative sense to me.
So when you asked the question, what what what you were describing was of course emergent properties of the system as as you later labeled it yourself.
But then you asked this very interesting question about when you say you're linking it to you're saying to psychoanalysis but really it's broader than psychoanalysis it's a system that has explicit and implicit.
Principles.
And so it's really the model I would use is is a general sort of multiple multiple memory systems sort of model and where they are non declarative and declarative modules.
And then I was thinking in that the it works in the reverse of what you've described in the sense that what is non declarative is the is the more bottom up.
It's the more, it's the more sort of predetermined and what is declarative what is explicit is the more emergent, you know what what couldn't be predicted and needs to be dealt with on the fly is what is explicit.
So it's an interesting.
I mean the fact that you drew that analogy which I have to say I wouldn't even have thought of, again, you know this is why I'm saying this is Mike live and you have to remember that you're Mark souls and that's Mike live and now you've got to stop thinking differently.
But since you made that connection which I wouldn't have made. I find it interesting that for me it would what what I would what I would call the emergent part is the part that's explicit.
And so explicit in the cognitive sense, as opposed to it in the coding sense.
In other words, this was explicitly coded, but this, this behavior emerges.
What I think is explicitly coded in our multiple memory systems is is what is non declarative.
And what is what what is implicit and emergent is what is declarative, because that's where the uncertainty is.
That's where the unpredictability is which links with my way of thinking about consciousness, which is that it has it's it's felt uncertainty.
Now why do I think that the non declarative systems are the are equivalent to your explicit coding.
And I think it's, I'm trying to unpack my own intuition why do I have that intuition.
Well, first of all, I could link it to what I've just said in that, which is a kind of functionalist account in that the what is implicit is what is more certain in the memory sense of the word in other words let me use the word non declarative so
and it's clearer that I'm talking about memory systems. So the non declarative systems are non declarative because one has a high confidence in those beliefs that that they're not subject to review.
And it's always the same. So, so, in terms of a hierarchical predictive model, you're talking about the layers in the hierarchy, which are, which are simpler, and, and more generalizable.
The way I think of a predictive hierarchy by the way I don't normally picture it as a pyramid I picture it as a concentric onion.
And I think of the core of the onion as the deepest layers of the predictive model. And, and that's where there's the greatest confidence.
The simplest, most generalizable predictions are at the core. And as you move to the periphery. So you're dealing with with eventualities on the fly. So one of the reasons why I think of the non declarative systems.
And as, as the explicit in the sense of pre programmed is because they are, they are more.
There's more, there's more predetermination, higher confidence, more generalizability, and, and a simpler algorithm is being followed down there.
And the other reason that I think that way is because I think that the non declarative memory systems are the ones that function most like reflexes and instincts, which are, which are the pre programmed predictions.
These are the predictions that are determined by natural selection that that that, and, and the, as we automatize our acquired predictions, so they function more like reflexes and instincts, they, they, they are more, they're more
generalized, they're more unconscious, they're more predictable, they're simpler, they're more generalizable, and the periphery of the predictive model is specialized for complexity for context sensitivity.
And so that's where the uncertainty is. And it's, it's also, in other words, it's what is more acquired. It is what is more unique to me here and now, as opposed to what is generally true of all versions of my type with things that always function that way.
So, so the meaning of the word explicit and implicit in memory systems terms is the opposite of the meaning of the terms in the way that we are using them in your, in your emergent properties of your, of your, of your system.
So those are my initial thoughts. But it links to an issue that is extremely interesting. And I think cuts to the heart of, of the whole problem of consciousness, which is, you know, what do we mean by explicit?
And why does anything become explicit in the sense that the system now has to monitor its own states.
I think that is the so so I'm as you know I'm working on an artificial consciousness project myself.
And in our, the computational architecture that we are designing and adjusting as, as we go along.
And this is, this has become, this is one of the problems we're busy tackling right now is, is, we can make the, the, this, our, our agent function perfectly well without there being any obvious sort of distinction between what is explicit and what is implicit.
And we have to kind of introduce that in order to try and get some, what we're calling biological plausibility but it's actually has two different meanings the one is to get it to function like I expected has to, you know, and that's not biological
and the other is to make it function more like a, you know, vertebrate brain what what we know about how well actually more mammalian brains is what I'm using as my as my sort of starting point, but in other words, a brain with cortex.
And so we're, we're, we're having to introduce a constraint, you know, on the way that it works in order to try to make this distinction between what is explicit and what is implicit which is of course very bad way of going about doing things because.
And then I realized that as we were doing that, that it's, it's due to a concrete way of thinking, you know, of course there can be this functional distinction in the system doesn't have to actually coincide with its architecture in other words the architect you mustn't map the architecture
to your concepts, the, the, the, the, the concepts of what functions it's performing like what you're talking about now these emergent properties, they're not in any of the code, you know, that they're not any in any of the, of the modules in your flow diagram, but nevertheless they emerge they exist, these
functions exist without them being coded for and without them coinciding with the, with the, with the brute structure of the, of the algorithm or the, or the, or the, you know, the structure of the, of the, the, the program.
But that issue of the meaning of explicit in the consciousness sense of the word, it's, it's the issue, you know, why does anything have to become explicit.
What, what does explicitness do, and it has, I think, to do with when a system is uncertain about its own states, and therefore has to focus on monitoring of its own states.
But nevertheless we come back to the problem that I started with in my response to you, which is that it is the opposite meaning of the, of the, of the, of the terms end up having diametrically opposite meaning the meaning of explicit and implicit in the
logical sense, as opposed to in the, in the function functional architecture sense of what is explicitly coded and what emerges.
Those are my thoughts about that problem. Does does that resonate with you would lead you to any interesting places.
Yeah, super interesting I hadn't realized, I hadn't realized about the inversion I think that's that's incredibly important. And I wonder if I wonder if tracking down when so so we can sort of look at the kinds of systems that that we're working in as like one ends of the
very minimal kind and then over here is you know the sort of thing the mammalian brain you're trying to model. One wonders whether that inversion takes place somewhere along that spectrum.
Right, and whether that's a that's a some sort of face transition or whether it's a gradual flip or I don't know yet because I just, you know, I'm just taking this on now but.
But yeah, the fact that it's backwards. I wonder if that is actually an important criterion for some of the things that we are interested in, you know, to distinguish truly simple models that don't have whatever properties from okay now you're into something
you know maybe maybe that flip is is important to that.
And so that's the that's the first thing so I need to think hard I need to think harder about this this this inverted mapping.
And then the other thing is that you know this this question which we wrestle with all the time in our models is like, yeah what what do you bake in to make it to do whatever it's supposed to do.
For example, what keeps what keeps striking me is that the thing that we've observed and who knows what else right so I'm sure there's other things to observe we're just not smart enough to test for them yet you know this clustering was just
something that that we thought of to do but I'm sure there's other things to do.
It doesn't have any real, like we didn't have to do anything for that to emerge meaning it isn't it isn't any with there's nothing in the algorithm that remotely is about optimizing that.
So to some it to so to some extent, this apparent, you know, the apparent tendency of the system to optimize a particular behavior is completely emerging we didn't have to do anything.
So, so one wonders you know as we think about okay how do we construct things like that.
Maybe a lot of it, we don't construct maybe it's emerge I don't know where it comes from right but but maybe it really emerges under extremely minimal conditions and that it makes me think that if something is as dumb as a as a you know, six line sorting
algorithm can have these unexpected behavioral tendencies that have to be discovered by experiment that we cannot you know predict in advance.
The more complicated things we make, you know, who knows what they're doing.
Right, it's like that's I mean that's part of why why I started this is because I wanted to have an extremely simple system.
And it seems like already there, you have this kind of and I mean the thing you started with that I mean it's amazing because it took me, probably, probably a couple of months to to to realize what you said, you know right off the bat at the beginning which is that what it looks like
they're doing is they're minimizing uncertainty about their neighbors, right because having a neighbor who's like you means that. Yeah, so so I eventually got there too but that took me that took me a couple of months to realize and actually Carl and I am talked about it.
A few weeks ago. The thing about that and I think that's a perfectly reasonable explanation. The thing about it is that there is nothing in the algorithm about that there's nothing about making predictions testing, you know, we're doing some we're
actually doing some experiments right now to see like how many interactions they have before they cluster do they actually have to spend some time with each other before they realize that hey this is this you know this one's just like me so.
But but all of that is completely emergent. So I just you know this thing if it wasn't already fundamental enough this this the surprise minimization principle it seems now even more, you know just baked in you don't need to do much of anything you all you get that you know just from from extremely minimal
systems.
So, I don't know if if there's a literature on this, I suddenly what you just said made me realize that this principle of preferring to be in the neighborhood of agents that function like me because they are more predictable.
They reduce my uncertainty. It makes me realize with a sort of chill down my spine that this is obviously part of of prejudice, you know why why people want to be with their own kind is that they more predictable you know what's going on you know what to do you know what to
expect, and being with others in the social sense, it makes you uncertain, and therefore increases your free energy and therefore you don't want to be there.
I say it sends a chill down my spine because it makes makes me realize how, you know, how little prospect one has of being able to engineer, socially engineer this out of us the only the only way to do it is to make sure we all have a common enemy.
So, but that's a sort of a side thought as I say I don't know if anybody is has has said that.
Maybe that's generally recognized to be the case but for me it's a, I've only discovered that in my own mind right now in this conversation.
Yeah, yeah, I mean, it's, it's amazing that something like that is is present at the very bottom of you know it didn't take much it didn't take any evolution at all in fact to come upon this right, but it actually this idea actually
cropped up about a month ago, some colleagues and I were discussing, you know, AI and diverse and like diverse intelligence and all that.
What we had a lot of debates about this sort of spectrum you can imagine the sort of spectrum where, on one end, it's things like objectophilia, right where you sort of completely mistake the the amount of agency and whatever system you're dealing with and you fall in love with a bridge or an Eiffel tower
and you know people do of course, which which are you know sort of not at the level that that you think they are.
But the, and so and so that's that's one type of error but but the other type of error is exactly what you just said which basically, you know, if you're if you're too strict about saying, Well, I don't believe you know you're you're you don't have the
history or composition that I do and therefore I don't think you have a real mind I think you're you know faking it as what they say about certain it. I mean, the far end of that is something like only love your own kind.
Right, that's the that's that's what you were just saying you know that. So, so we want it so we spent a lot of time talking about how we could arrive at principle strategies for picking criteria along that because you don't want to be at either one of these, you know, either one of these polls
to match sort of impedance match the kind of relationship you have with the system to what and and and I firmly believe that even in the absence of you know aliens and whatnot.
We're going to be surrounded by really radically different systems we're going to have to like work on getting this right. So, I, you know, from that perspective. Yeah, I completely agree with you that that this this this is an important thing to to get right and I think our fundamental,
you know, to minimize minimization, but I wonder, you know, I wonder if one thing you can do to alter how this works is to sort of reframe what it is that you're measuring.
Let's assume let's assume for a minute that that yes you don't want surprises you want you want to you know you want to you want to predictable.
And the other question is about what about and so if we can get away from superficial kinds of differences that drive, you know, social, you know, distress and stuff like that, and focus on fundamental things.
Then, then maybe this can actually make sense. So, so you know what I would like to be surrounded by is what you know humans with the same DNA. That's not what I'm measuring maybe we measure something else maybe we measure kindness or pro adaptive, you know, pro social behaviors
is somebody who has the same kind of existential concerns that I do or something like that right so so shift the measurables in a way that it's actually, you know, it's actually better.
I don't know that's that's that's that's where I've been trying to and trying to think about how to create principled strategies for that.
So, I think you just put it, it is very much a concern of mine as I'm working on this artificial consciousness.
I'm very prejudiced of, it's not like me therefore it can't have this valuable thing that I've got.
I'm very exercised by, I mean in my field, I have it even with with colleagues who won't acknowledge consciousness in creatures which are too far from us in terms of their phenotype.
And, you know, artificial agents that the prejudice is just enormous. And so it's interesting to link that with what we've just been talking about again I hadn't thought of it.
The solution that we mammals at least have come to that functions in the other direction is an epistemophilic inclination.
And it's that it's important to, to approach what is uncertain, a positive, a positive attraction to, to, to gaining more knowledge, and therefore to engaging with what is novel with what is not understood with what is, with what is,
what becomes interesting, you know, so, so there's a, the mesocortical mesolumbic dopamine system functions like that in the mammalian brain it's, it's, it's homostatic set point is to be in a state of uncertainty it's, it's, it's, it's, it's, it's in, in homostatic deviation.
If it's bored, you know, if everything's predictable, then that need is not being met the need to engage with novelty.
It's a sort of what makes us forage.
It's what makes us explore.
It's what makes people like you and me exist at all that we're constantly attracted to what is to what is not known.
And you can see over longer time scales, the biological advantage of that because to engage with uncertainty is in the long term to reduce uncertainty.
The world's not going to bite you in the but in future because you, you, you engaged with it now proactively.
But then an entirely different line of thought that was triggered in me, as I was listening to what you were saying is that really what you're talking about is a sort of swarm intelligence isn't it.
Where each creature has a very limited range of behaviors, and then, although each one of them is doing this very limited thing.
What emerges is, is very complex behaviors by the by the swarm as a whole.
None of, so there's no master program in each member of the swarm, but the swarm as a whole produces this, this much more intricate outcome that maps on to what you were talking about in terms of your algorithm, and the interesting
very emergent behaviors that that's that you described to me. But then I realized that this links actually to one of your basic concerns, which is that we multi cellular creatures are all of us actually swarms of cells.
And so this is, but, but we're not, we're not swarms of equal cells.
We have, we have nerve cells and, and, and bone cells and so on, which are specialized for it in some respects of course they are all of them just, you know, animal cells but in another sense.

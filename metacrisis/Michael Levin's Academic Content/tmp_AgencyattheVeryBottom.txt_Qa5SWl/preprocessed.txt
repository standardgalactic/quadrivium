This is a talk about agency at the very bottom
and introduction to universal properties
and adjoint functors.
So it's about mathematical agency.
First of all, just a little bit about me.
My background is in economics,
but I moved to abstract math
to try to solve some problems in economics
that economics was not able to help me solve.
We didn't have the tools
and ended up finding out about Mike's work
through a YouTube video like a lot of people.
I sent him an email and we ended up setting this up.
I should also mention that this talk is designed
for people who have little to no background
with category theory or abstract math in general.
I know a number of you do have experience with such things.
So the mathematical portion of this
may move relatively slowly for you,
but that should hopefully make it easy to focus
on the unusual perspective.
And if you don't have any background with such things,
it should be easy to follow along.
Also, we have a lot to get through,
so I'll appreciate it if people can hold questions
unless something I've said just doesn't make logical sense to you
in which case, please let me know,
but otherwise we'll try to get through
as much of this as we can.
So first of all, why talk about mathematical agency?
There's a few reasons.
One is that it could settle the question
of where the line is between agentic
and non-agentic things in our universe.
There is no line because it's all math.
Also, if you think math can be agentic,
it's suddenly pretty easy to think that cells can be agentic,
so that stuff is pretty easy to swallow.
In general, new perspectives means new questions
and new interpretations of old results,
so that could be useful.
And finally, and what's really gonna be the focus
of this talk is that math looks agentic,
so we should call it agentic.
So here's what we're gonna talk about.
We're gonna talk about four concepts here in this talk.
The first is scarcity.
This is a concept from economics
that I'm going to argue is the necessary conditions
for agency, and I'm going to show you
that mathematics has scarcity,
and so it meets the necessary conditions for agency,
so it's plausible that it is in fact agentic.
Then we're gonna talk about two agentic things in math,
which are universal properties and adjoint functors.
Universal properties relate to optimization,
which is obviously important for agents,
and adjoint functors relate to optimal reconstruction
and least action, which are also very agentic things.
And finally, time permitting, which it may or may not be,
we'll talk a little bit about timeless choice,
so math doesn't move, so people can do stuff through time,
but math can't, so how can math actually do anything
like an agent?
We'll talk about that if we can.
And so the hypothesis lurking in the background,
which I can't provide evidence for,
but it's just sort of the idea
that maybe can inspire some new questions,
is that life and physical structures in general
that have any agentic properties are able to do so
because they exploit the affordances given to them by math,
including basic agentic properties.
So just like if you're evolving a triangle
and you evolve the first two angles,
you get the third angle for free,
maybe something similar is happening with agency
where you get a lot of stuff for free.
So I wanna start with scarcity.
We're gonna talk about how scarcity
is the necessary conditions for agency,
and see what that looks like in mathematics.
So let's start with agency.
If you see something agentic in the world,
you see something you want to call agentic,
what's causing you to do that?
What must be present for that
to make sense as a modeling decision?
There's undoubtedly a number of perspectives on this.
The perspective I'm going to take
is that it's all about choice.
If you see something agentic or that you model as agentic,
what it's probably doing is making something
that you are calling a choice,
and so you think of it as an agent.
So where does choice come from?
Why do we call things choices?
The answer is it comes from trade-offs.
Trade-offs exist.
If you do one thing, you can't do another thing.
If you go left, you can't go right.
So you have to make choices,
and then to make those choices, you have to be an agent.
So choice is necessary for agency,
trade-offs is necessary for choice.
What's necessary for trade-offs?
I'm gonna argue this bottoms out at scarcity.
Scarcity, I'm going to define as
the necessary conditions under which trade-offs occur.
That's quite vague at the moment.
We're gonna be more specific,
but this is the sort of pyramid here.
Agency ultimately requires scarcity
to make sense as a modeling decision.
So what we're going to do here
is come up with a more specific definition of scarcity.
We're gonna look at some examples of trade-offs
first in economics and then in math.
We're gonna see how economists think about scarcity,
what the limitations of that are,
and then come up with a generalization
that fits mathematics.
So here is the standard economic picture of scarcity.
Forgive me, I'm not an artist,
so this is the best I can do.
On the left here, we have four sandwiches.
On the right, we have five hungry people,
and you can see the issue.
We can't feed everyone a full sandwich.
So we're forced to make trade-offs.
We could feed four people a full sandwich
and one person goes hungry,
or we could cut up the sandwiches in some way
and feed everybody,
but not everybody's gonna get to eat a full sandwich.
So we're forced to make trade-offs.
Well, I've argued that scarcity
is the necessary conditions for trade-offs.
So the existence of trade-offs means there is scarcity here.
So what is the scarcity?
What makes scarcity be happening here?
Well, economists define it as finite physical resources.
The core of the issue
is that we have a finite amount of sandwiches.
We have four of them instead of infinity.
If we had an infinite number of sandwiches,
we'd be able to feed everybody.
So this is how economists think about scarcity,
finite physical resources.
The problem with this definition
is that math does not have finite physical resources
so far as I know, but it still has trade-offs.
So there's a problem with defining scarcity
in terms of finite physical resources,
which is it can't define trade-offs
when we're dealing with infinite stuff.
So let's look at a trade-off in math.
So this should be very familiar.
We're just counting up the natural number line,
one, two, three, four, five, et cetera.
As we do so, we're constantly switching back and forth
between odd and even numbers.
So we're constantly making trade-offs odd number
or even number.
We never have both.
And this is not just a superficial distinction.
If you have an even number and you divide it by two,
you've got a whole number.
If you have an odd number and divide it by two,
you get a rational number.
So there's different properties.
So this trade-off is really meaningful,
which means there must be scarcity
because trade-offs imply scarcity
because scarcity is necessary for trade-offs.
But I don't see anything finite in this situation.
It looks like we have an infinite number
of natural numbers.
So where is the scarcity coming from
if not the finiteness of our system?
Here's just another example of trade-offs in mathematics.
I wanna show you that this is ubiquitous.
You're always making trade-offs in mathematics.
Real numbers versus complex numbers.
On the left here, we have real numbers.
Real numbers have a very useful property,
which is they can be totally ordered,
meaning for any two real numbers,
I can always tell you which one is bigger,
like we always know which one is bigger.
What they can't do is define the square root of negative one.
If we move to the complex numbers,
we can define the square root of negative one,
but we can't totally order our numbers anymore
because they're two-dimensional.
So one complex number could be bigger on the real dimension,
but smaller along the imaginary dimension or vice versa.
So we gain the ability to define
the square root of negative one.
We lose the ability to totally order our numbers.
So again, this is a trade-off.
So we have trade-offs, and therefore we must have scarcity,
but again, there's no finite physical resource.
We don't have a finite amount of numbers.
Numbers, we don't even have a finite amount of fields.
So what's going on?
To understand agency and mathematics,
we need to solve this problem
because I'm arguing that scarcity is necessary for agency.
So we need to understand what scarcity is in mathematics.
So here's what we have so far.
Economics tells us scarcity is about finite resources.
Math isn't finite, and yet it has scarcity
because it has trade-offs.
So where does scarcity come from?
There's a touch.
Sorry, go ahead.
No, okay.
So to solve this little riddle,
what we're going to do is we're going to look
at a situation in math where math does not make trade-offs.
We're gonna see the difference.
What's going on when math doesn't make trade-offs
versus when math does make trade-offs,
and that's gonna tell us what scarcity is.
And then that'll let us make the case for math being agentic
because we'll see that math has scarcity
and therefore meets the necessary conditions for agency.
So here is the situation in math
where you don't make trade-offs,
and that situation is contradiction.
So there's a famous logical principle
called the principle of explosion,
which says that if we contradict ourselves,
we can derive anything.
So if we assume P and not P, where P is any proposition,
so I'm saying P is true and not true,
hence contradicting myself,
then Q follows where Q is anything.
So if I say, if I am Benjamin and I am not Benjamin,
hence contradicting myself,
I can derive that John F. Kennedy is alive
and living on the moon with his best friend Santa Claus.
That follows from the contradiction.
So in particular, contradiction lets us avoid trade-offs.
We could have an odd number that we divide by two
and get a whole number,
or we could totally order the complex numbers.
We can do anything because we don't have to worry
if it makes sense because we can just contradict ourselves.
So scarcity is what's necessary for trade-offs,
but when we contradict ourselves,
we can avoid making trade-offs.
So it makes sense to associate a lack of scarcity
with contradiction.
That is in fact what I'm going to argue
is that we have scarcity when and only when
we do not contradict ourselves,
which is to say when we are being logically consistent.
So scarcity in math relates to logical consistency in math.
And that might seem weird,
but I think when you look at this picture,
it becomes quite clear
because what happens when you commit yourself
to logical consistency,
when you say I am not going to contradict myself,
you commit yourself to making choices.
You say I have to pick.
Do I want to assume P or do I want to assume not P?
Both assumptions are fine.
You can start deriving theorems in one direction,
then you can go back and make the other assumption
and derive theorems in the other direction,
but you just can't do both simultaneously.
You have to pick one.
So scarcity, this or that, logical consistency,
P or not P, to me they feel like the same concept.
So what I'm saying here is that agency requires choice,
requires trade-offs, requires scarcity.
Math has scarcity because it has logical consistency.
That's what I'm associating with scarcity.
So as long as we're being consistent,
we're going to have scarcity
and therefore meet the necessary conditions
for being agentic.
So now what we're going to do
is look at some actual examples of agentic things in math,
which are universal properties and adjoint functors.
So we've set the table for math maybe being agentic
because it meets the necessary conditions.
Now we're going to see, does it actually do agentic things?
Now I'm going to argue that it does.
So starting with universal properties,
we have, this is an important concept in math
that relates to a few agentic ideas.
First of all, optimization,
which is obviously very important for agency.
We're also going to see how it relates
to a concept of top-down goal-driven design or control
that you can do in math with universal properties.
We'll also see how universal properties constitute
a solution type that we can remap
onto new mathematical body types.
So just like we can think of an organism
as being optimal for its environment,
we'll see how we can talk about a mathematical structure
as being optimal for its environment
via a universal property.
So what is a universal property?
A universal property is a way of defining
a mathematical object by how we use it
rather than what it's made of.
So instead of looking inside of it
to see all the parts and pieces,
we're just going to ask,
what are we doing with this thing
and how well does it do the job?
We're going to consider a collection
of all mathematical objects that do the same job.
And we're going to ask,
what's the best at doing that job?
The universal property is going to be a special relationship
that the optimal mathematical object
has with all other objects that says,
I am better than you.
So it's universal because it's talking
to every other thing in the environment
and it's saying the same message to each one,
a universal message for every object
that says, I'm better than you at this.
So, you know, back off and let me do my thing.
So here's a very simple example of a universal property.
We actually have two of them in this picture.
So we have an ordered set.
This is the minimal element.
This is the maximum element.
So if we consider the job here,
we could ask,
we want a thing that's bigger or equal to everything else.
And if we consider this top element here,
it's the best at doing that job
because it's bigger than this thing.
It's bigger than this thing.
And then it's equal to itself.
So it sends the same message
to every other object in this environment
that says, yes, I am bigger than or equal to you.
So that's the universal property.
It's the same singular relationship
with everything else, including itself.
And then the bottom element similarly,
it is optimal to task of being smaller than everything else.
And it sends the same message,
I'm smaller than you, I'm smaller than you.
And then I'm less than or equal to myself.
The same message for everything.
That's a simple example of a universal property.
Now let's look at a more interesting example
of a universal property, which is the Cartesian product.
We're going to see that we can define the Cartesian product
in terms of what it does instead of what's inside of it.
We'll see how we can compare the Cartesian product
to other objects that do the same job as it.
And we'll see that that allows it
to satisfy the universal property.
And then we'll see how this leads
to some interesting agentic things
that you can end up doing
with this use-based definition of the Cartesian product.
So let's start with the internal view
of the Cartesian products.
This is the standard way to define the Cartesian product
that every math student is going to learn.
We have two sets, A and B.
The Cartesian product is the set written A times B.
Of all ordered pairs, let's form A comma B,
where A comes from big A and B comes from the set B.
So a very simple example,
let's say A is a two-element set, A1 comma A2,
and B is the two-element set, B1 comma B2.
Then A times B is just every way
of pairing an element of A with an element of B.
So we have A1 paired with B1, A1 paired with B2,
A2 paired with B1, A2 paired with B2.
So this is like very basic like intro like proofs
for like any undergraduate math students
is going to learn this definition.
But we have another way of defining the product,
which is based on how we're using it
instead of what's inside of it.
So this is an object denoted A times B
along with projection functions P1 and P2
that map to A and map to B respectively.
So these functions are part of the definition
because we're considering it in terms of how we use it.
And what we do with sets is we make functions.
So our use is based on functions.
To see what these functions do, a very simple example
is it takes our ordered pair A1 comma B2
as just an arbitrary, excuse me,
arbitrary choice of input.
And what P1 is going to do
is it's going to preserve the A unit in A.
So our A unit here is A1.
So we map to A1 and A,
and then P2 similarly preserves the B unit
mapping us to B2 and B.
So just to get a picture of why this is in fact useful,
this tells us what we're using the Cartesian product for.
Anytime you've done some graphing,
you've been using the Cartesian product,
whether you realize it or not.
So this is our A on our X axis, B on our Y axis,
and here's the ordered pair A1 comma B2.
Well, our projection functions allow us to remember
that this product, it's A1 units along this axis
and B2 units along this axis.
If you were to try to understand this ordered pair
without these projection functions,
it wouldn't mean anything.
It's these projection functions
that take this ordered pair, this point, this dot,
and actually turn it into something meaningful
by relating it back to B and A.
In particular, what you could say is it remembers A and B.
So we've got this ordered pair.
We're like, wait, what am I doing with this?
Oh yeah, I've got A and I've got B.
I've got this A1 unit and this B2 unit.
That's what's going on.
So what's interesting about this perspective on products
is that it lets us ask a question
that we couldn't have asked before
when we were just looking inside things.
There's a new question, which is if that a product of sets
is just a set combined with functions to A and B,
doesn't any set qualify?
Because as long as A and B are non-empty,
then any set is gonna have functions to them.
So let's say PC, that's like a candidate for the product,
a product candidate, it's just some arbitrary set.
We don't know.
Well, it doesn't have a function F to A
and a function G to B.
The answer is kind of, yeah, it does.
But what we're going to see
is that it's not necessarily optimal.
A times B is going to end up being optimal.
This other candidate may not be optimal.
And we're gonna express that with a universal property.
So let's look at a specific candidate for the product,
a product of three sets, A times B times C,
which is gonna have elements of the form A comma B comma C.
So this is an ordered triple now instead of an ordered pair
where A comes from A, B comes from B,
and then C comes from C now.
So we have these functions,
let's just call them F and G to A and B just like before.
And it's gonna do what we're gonna expect.
So we're gonna take an input,
let's say A1 comma B2 comma C1.
What F is going to do is it's going to preserve the A unit
just like before A1 and A.
And what G is gonna do is preserve the B unit,
B2 and B just like before.
And then the C stuff ends up being irrelevant.
So you can see that we are doing the same job
we did before of projecting to A and projecting to B
and remembering those sets.
So we want to now, intuitively,
we understand A times B times C is inefficient
compared to A times B
because it clearly has something extra.
But how do we actually show that it is?
What we're going to do is compare A times B times C
to A times B.
This comparison is going to end up being
our universal property and it's going to show us
that A times B times C is inefficient.
So the way we compare sets is with a function
because that's how you relate sets to each other
is with functions.
We're gonna call that function H.
And to understand what's going on,
we're gonna follow this diagram very carefully.
So I'm gonna move through this kind of slowly.
Bear with me.
So we've got this input.
Let's just say it's A1 comma B2 comma C1.
We're gonna define H to preserve the A unit and B units.
We're gonna map from our order triple
and A times B times C to an ordered pair
and A times B, we're gonna keep the same A unit A1,
keep the same B unit B2,
then we'll just delete the C unit because it's extraneous.
So we've gone down from H to here
and now we could go either left or right.
Let's go left.
So P1 is gonna take us to A1 and A from this ordered pair.
Now, what's interesting is that F does the same thing.
So H then P1 starts over here and ends up over here.
F does the same thing.
It starts over here and ends over here.
Well, two functions, if they start in the same place
and end in the same place, they're equal.
So F is equal to H followed by P1.
And similarly, G is equal to H followed by P2.
So what this tells us in a very precise way
is that if we take A times B times C
and we lock off the extraneous part,
we end up doing the same work.
So we can take A times B times C, cut off a piece of it
and still do the same job as before.
That tells us that A times B times C
had something extraneous and was therefore inefficient.
And what's particularly noteworthy
is this is the only way we can compare them
to have them do the same job.
H is forced by how we are using A times B
and A times B times C.
We're using them to reconstruct A and reconstruct B.
And it turns out that if we want to use A times B times C
to reconstruct A and B in the same way as A times B,
we are forced to effectively turn it
into A times B to do the job.
So there's sort of top-down design here.
I say, I've got this job, reconstruct A, reconstruct B.
I have to turn things into A times B
and there's a forced way to do so
given my alternative candidate.
I started here, this isn't perfect.
I end up here, this is perfect.
What makes this a universal property
is that this same relationship that A times B has
with A times B times C, this unique factorization
where A times B times C gets turned into A times B
to do the job, it has with every other candidate
for the product.
So I've given them stupid names like Bibbidi-boob,
Bibbidi-bib, it doesn't matter what they're called,
just every other set that could do the job.
This is infinite going upward
because there's an infinite number of candidates.
Each of them comes with their projections to A,
each of them comes with their projections to B,
but each of them gets mapped uniquely to A times B
where it ends up saying, okay, yeah,
A times B is actually better than me at this job.
It's the same relationship for each one,
a unique way of saying you want to be me
to do this job properly.
So effectively, all of these other candidates
are sort of cheating off A times B's homework,
A times B is in fact optimal via this universal property.
Something I just think is interesting
as someone with an economics background
is this actually feels very similar to a preference order,
which is obviously a very agentic concept.
So from an active inference perspective,
if I want to have like a favorite ice cream flavor,
what I'm using that to do is to reconstruct
my anticipated flavor and anticipated texture experience.
My favorite flavor of ice cream
is going to be the one that's best
at reproducing my anticipations.
The unique factoring could be like selling
the strawberry ice cream and using the proceeds
to buy chocolate ice cream.
So universal properties,
it looks very much like having a favorite way
of doing something, which is clearly an agentic concept.
So here are the principles of universal property.
So universal properties are the most efficient solution
to a problem that specifically exists in math.
So we're not using this to solve a physical problem
or a human problem.
We're using it to solve a mathematical problem
that you can define in purely mathematical terms.
And yet despite the fact that it's just math,
you have an efficient,
we have a favorite way of doing things.
We also have a way of defining top-down
goal-driven control.
If I say, I've got some other thing here
and I want to make it the best way of doing things,
I don't need to look inside of it.
I just need to define this unique mapping to A times B.
And similarly, we can test optimality.
So we don't just have to like assume or guess optimality,
we can test it by looking at these kinds of mappings
and asking, does it factor through A times B, yes or no?
That's going to tell us about optimality.
And now that we have this picture
of the universal property of the Cartesian product,
we're going to see a couple of very cool agentic things
that you can only do with this definition
and not with the internal view of Cartesian product.
So the first thing we can do
is we can find unexpected competencies
that we could only discover by probing objects.
So let's consider this Cartesian product of three sets,
A times B times C,
but this time we're going to have C be a one element set
consisting of just a little C here, no C1, no C2,
just a little C.
So it's A times B times just the set containing C.
And it's the set of four ordered triples.
So we've ordered triples, not ordered pairs,
but there's only four of them.
So we do see that it is isomorphic to the A times B
that we saw several slides ago,
which had just four ordered pairs.
Now, if you were to ask a math student,
an undergraduate math student,
is this the Cartesian product A times B?
They would say no,
because the Cartesian product A times B
consists of ordered pairs, but this is ordered triples.
But by testing it, instead of just assuming
we know what it can do,
we can see that it does the job of the Cartesian product
just as well as the Cartesian product does.
And we can see that because we can factor A times B
through A times B times C here
to make everything be equal just like before.
So what this is gonna do
is we're gonna take an ordered pair,
let's say A2 comma B1,
our function H in this case will map it
to the order triple A2 comma B1 comma C,
and then we're gonna end up at A2
regardless of where we go
or regardless of which path we choose,
or we'll end up at B1
regardless of which path we choose on the right.
So this does in fact show that A times B times C here
is the Cartesian product.
It does the job of the Cartesian product optimally,
even though it doesn't look like it should.
Now if you're wondering, well, isn't this also optimal?
That's because it factors back the other way through H prime.
We can sort of take either path.
So these are isomorphic.
So they do the same job
because in a sense we can treat them as the same.
There's a unique isomorphism specifically.
We have one and only one way of transforming one
into the other so that they do the same job.
So we're indifferent which one we choose.
So C isn't helping us, but it's not hurting us either.
It's not costing anything.
And we would have only learned that
by actually testing the system with functions
instead of just assuming by looking inside of it
at the parts and pieces.
One other cool thing we can do
with the universal property of the product
is we can take that solution type that it constitutes
and map it onto a new body type.
So instead of looking at ordinary sets,
let's look at an ordered set.
So we've got our set consisting of X and Y and Z
and these relationships mean X is less than or equal to Y
and X is less than or equal to Z.
You could think of this like biologically
as like descendancy.
So Y and Z are the parents and X is their child.
So what if X has a child?
So let's say W descends from X and also from Y and Z
then X is the greatest lower bound.
It's the biggest thing that is smaller than Y and Z.
So this is a very important concept in math.
You're constantly talking about bounds
and greatest lower bounds when you're doing mathematics.
So we've defined a very important concept here.
Now how does this relate to universal properties?
Well, let's take this diagram and turn it upside down
and you can see it's the exact same diagram
as the universal property of the product of sets.
I haven't labeled the arrows here
because the labeling doesn't matter
but otherwise it's the exact same diagram.
The forcing here is given by the fact
that in an ordered set there's only one way
to be a descendant of something else.
You can't have like two different kinds of child.
There's just one way to be a child.
So if we can take our universal property of the product
and we can, you know, when we're trying to solve
like what is the greatest lower bound?
What does that mean?
We don't need to, you know, reinvent the wheel.
We can import our solution type from another body type
from just plain sets into ordered sets.
And we find out that the greatest lower bound
is in fact the product in partially ordered sets.
So what I take from this is that math
doesn't make specific solutions to specific environments.
It makes a problem solving morphisms
where morphisms is a generic term for these kinds of arrows.
So that takes us through the end of universal properties.
Appreciate how bearing through that.
I know it can be kind of dense and tedious.
And now we're going to move on to adjoint functors.
We'll try to get through this
and see if we have time for timeless agency as well.
So adjoint functors are a very important concept
in mathematics that relates to optimal reconstruction.
So if you're an organism trying to survive
in your environment, you need to have an internal model
of your environment.
So you're recreating your environment inside yourself
but you have limited sense data
and limited computing power.
So you can't perfectly recreate your environment
instead you want to optimally recreate your environment.
And adjoint functors also relate to being lazy
and we'll see that being a lazy or leased action
ends up being related to optimality.
So you want to be lazy, you want to be optimal.
We'll see how those end up being highly related
to each other.
So what is an adjoint functor?
An adjoint functor is a pair of relationships
one of which forgets information
and the other reconstructs what was forgotten.
Once you forget something
you can't necessarily remember it perfectly.
So we settle for optimally reconstructing it instead.
We'll work through a specific example
of building something lazily.
We'll see that building something lazily
lets us build it optimally as well.
So we're going to build something called a monoid.
So a monoid is a fancy word for something very familiar
which is a set combined with an operation.
And operation is a way of combining two elements of the set
to create another element of the set.
And in particular, what's important is a given set
can have many operations.
So let's take our set to be the natural numbers.
Well, one, combine them to create nine
or we can have multiplication take two and seven
combine them to make 14
or we can have maximum as the operation
a bit different from a normal arithmetic operation
but we can define two and seven
we can find them to create seven
as the maximum of those two.
So we're combining things to create things
so that's an operation
or we can even do something like concatenation
and take two and seven and combine them to make 27
which is kind of silly
and we can use zero as our identity.
So this is a monoid
it's a very familiar concept
a set combined with an operation.
Excuse me, let me drink some water.
So now we're going to look at using this idea of a monoid
we're going to ask how do we turn a monoid
into a set in a lazy way?
How do we turn a set into a monoid in a lazy way?
We're going to do this with what are called
free and forgetful functors.
So a functor is just a fancy word
for a generalization of a function
just think of it as a function
for all intents and purposes
that's going to be okay for this talk.
So our forgetful functor
we're going to have our monoid
our monoid is going to be our set of natural numbers
along with some operation
I'm not going to say what the operation is
some arbitrary operation
it doesn't matter
which we'll just denote with an asterisk
our forgetful functor asks
what's the laziest way to turn our monoid
into just a set
just a regular set with no operation?
Well, the easiest way to do that
is just to forget about the operation
just throw the operation in the trash
and just keep the set
so take the natural numbers combined with the operation
and turn it into just the natural numbers.
So that's the easy part
that's not too interesting.
Now let's ask how do we reconstruct our monoid?
So we're, we just have our natural numbers
we threw our operation away
we don't remember what it was
but we do want to turn this back into a monoid.
What's the laziest way we can do that?
And that's going to be with our free functor.
So we're going to construct what's called a free
or lazy monoid on the natural numbers.
The way we're going to do that
is we're going to choose a generating set
which we're going to choose to be just the number one.
Now by definition of a monoid
I have to be able to combine one with itself
to create something
because a monoid says I combine numbers
to create other numbers.
I'm going to use the plus symbol to denote the operation
because it will in fact turn out to be addition
but we don't know that yet
so this is just an arbitrary choice of notation right now.
So I have to write down by definition
I'm not thinking I'm being lazy
but just I'm nevertheless compelled to write down
one plus one equals something.
Well I'm going to be as lazy as I can.
I don't want to turn on my brain.
So I'm going to write one plus one equals random squiggle
and then because random squiggles in the set
I have to write down one plus random squiggle
that's going to equal some other random squiggle
because I am being lazy.
I don't want to think at all.
I'm just moving my hand around randomly totally brain dead.
Well the result of this
is I'm going to create some set called M for monoid
which is going to look like zero one random thing
which is one bigger than one random thing
which is one bigger than this
squiggle that's one bigger than this
squiggle that's one bigger than this
going out infinitely
because I'm never going to write two random squiggles
the same way the odds of that is zero.
So I'm going to generate an infinite amount
of random squiggles that are all one bigger than the previous.
Well if we compare that to our set of natural numbers
it's clearly the same
because I got zero one two three four five
it's the same set.
So it could have happened by random chance
that when I'm writing these random squiggles
maybe I wrote two by accident
I wrote three by sheer chance, et cetera.
So I could have generated the natural numbers
if I just got in luckier with my choice of squiggles.
Well who cares what the squiggles are
so you can in fact say that this monoid
is the natural numbers
and so that's what we've done
we've actually generated the natural numbers
under addition by being as lazy as possible.
So this is our free monoid
the natural numbers under addition
we generate it by starting with one
doing one plus one plus one plus one
totally brand dead takes no thought.
Let's contrast this with an expensive monoid
and you'll see how lazy the free monoid is.
So we'll take our generating set to be one again
and we'll do some familiar relationships
so we'll have zero plus one is one
one plus one is two, two plus one is three
but now we're gonna establish a non-trivial relationship
we're gonna turn our brain on
and actually think about something
and make a deliberate choice
we're gonna say three plus one is zero.
Now that might seem weird
but this is totally legitimate
because all we have to do with a monoid
is combine numbers to create other numbers.
Well three plus one if I combine them to create zero
I've satisfied the rules of a monoid.
So it looks weird but it's actually totally legitimate.
So now what else do we get?
Well three plus two is three plus one plus one
three plus one is zero
so three plus two ends up being one
then like three plus three is two
and two plus two is zero, et cetera.
So all these relationships just follow
from our choice up here.
And what we're doing is we're counting
but we're counting in a loop instead of a line.
So with the regular natural numbers
you just go one, two, three, four, five, et cetera
extending outward with our expensive monoid
we go in a loop zero, one, two, three
is zero, one, two, three, zero, one, two, three, et cetera.
Now this might look weird
but it's actually familiar
because this is how we count on a clock.
So if I tell you like nine o'clock
plus five o'clock equals two o'clock
that makes sense to you
and something like three plus two equals one
can make sense as well.
Now we're using zero instead of 12 here.
So imagine a clock that goes zero, one, two, three,
zero, one, two, three
that's what we're doing with our expensive monoid.
So it looks weird
but you all know how to count this way
it's just counting with the clock.
So now what we want to do is do what we did before
with our universal property of the product
and compare our free monoid to our expensive monoid.
So we built one in a lazy way
we built one with a bit more effort
we want to show that the lazy way of doing things
is the optimal way of doing things.
We're going to do so with a function just like before
but this needs to be a function
with a little more structure
it needs to preserve the structure
needs to preserve the use of the one monoid
in the other monoid.
What we're doing with both of these monoids
is we are counting by one
that's what we saw we're generating
by just adding one to itself over and over.
So the function is gonna preserve the counting structure
as long as each counts by one in the same way.
And so the way this is gonna work
is that if we map a number in the free monoid
to a number in the expensive monoid
the next number in the free monoid
gets mapped to the next number in the expensive monoid.
So it's just gonna match up with each other
counting by one.
So here is a diagram that shows this.
So here's our comparison, our function.
So we've got all of our natural numbers over here
zero, one, two, three, et cetera.
And then we're going to have our expensive natural numbers
which is just going to be zero, one, two, three repeating.
What is our function?
What's our mapping going to look like?
Well, the first few are obvious, zero maps to zero,
one maps to one, two maps to two, three maps to three.
What is four mapped to?
Well, we don't have four over here.
So we can't just map it to four.
So what are we gonna do?
Well, four is one more than three.
We've mapped three to three.
So we should map four to one more than three over here
which is zero.
So that's what we're gonna do.
So four maps to zero, then five is one more than four,
one is one more than zero.
So five gets mapped to one because four gets mapped to zero.
And so it just continues outward like this.
So this is our comparison.
This is our choice of function.
And we're gonna see a really interesting picture
that's gonna show us the laziness and the optimality
of one monoid versus the other.
So here is that picture.
So here is our expensive monoid.
The way a monoid is sort of formally defined
is you have a dummy object
which I've denoted with this gray circle here.
It doesn't really mean anything.
It's just there because it has to be there.
And then the numbers are going to be relationships
that extend outward from the dummy objects.
We're gonna have zero, one, two, three
all extending outward.
So we're gonna start with zero.
That's just there by default.
So then we have one, that's green here by this legend.
One plus one is two, two plus one is three.
So the image I want you to have in your mind
is like ripples extending outward
from a stone dropped in water.
We dropped this dummy object in the water
and our numbers are ripples that extend outward from it.
Now what happens is a bit unnatural
with this rippling motion
is it actually collapses back to zero again.
So it's like we've done some extra work with our water setup.
We have some special technology
that pulls everything back to zero
and then it ripples outward again.
So it's rippling but in a somewhat unnatural way.
It's not how we would expect physics to work.
Let's compare this to our free monoid
where we're going to have things rippling outward again
from the dummy object.
This time what our legends are going to show
is the colors are mapping to these colors over here.
So red maps to this, green maps to this,
blue to this, purple to this.
So we start with zero again.
Zero plus one is one, one plus one is two,
two plus one is three.
Now we don't collapse, we go forward.
So four, three plus one is four, four is red
because it maps to one, four plus one is five,
that maps to two, et cetera.
So we keep mapping outward.
And what you see is that this actually behaves
like a ripple does in the real world.
If you had an infinite sized pool,
you dropped a stone in it.
And if there were no entropy,
you would just get it rippling outward infinitely
according to the initial pattern.
That's what we expect least action
to look like in the physical world.
And that's what doing things lazily
looks like in the mathematical world as well.
So this is what our mapping looks like visualized.
And this does in fact tell us
that the free monoid is optimal
versus the expensive monoid
because the free monoid contains the expensive monoid.
So what can we do with the free monoid?
Well, we can count linearly,
one, two, three, four, five, six, seven, et cetera.
But we can also count in a loop
because that's what this pattern is giving us.
We can see the looping pattern.
We can count with loops with this free monoid.
So it can do the job that the expensive monoid can.
But the expensive monoid cannot do the job
that the free monoid can
because it can only count in a loop.
It doesn't count linearly.
So the free monoid is like just able to do more tasks
than the expensive monoid does, it contains it.
And so even though we built it lazily,
it actually ends up being optimal.
So what have we learned about adjoint functors?
Adjoint functors are lazy ways
of translating between structures.
So we have different mathematical structures
like monoids versus sets.
Adjoint functors allow us to map between them
in lazy ways that create or destroy information.
When we do things in a leased action way,
adjoint functors end up giving
a sort of universal solution to the problem.
So like if we want to count by one, that's a problem.
What's sort of the, there's many ways we could do it,
but what's like the most sort of universal
or formulaic way of doing that?
Well, it's this way of doing that
because we can do looping if we want to,
but we also have linearity
which these other expensive versions don't have.
Something that's really interesting about adjoint functors,
just like in the physical world,
there's so many things that you can define
in terms of leased action.
Well, the same thing works in math.
Adjoint functors arise everywhere
as a common saying in math.
Many important mathematical things
turn out to be adjoint functors.
They turn out to be lazy ways of doing things.
And I don't know much biology,
but it just kind of seems to me that free monoids,
I kind of like the DNA of monoids
to give you the relational structure
to do all different kinds of counting,
linear counting, looping counting,
all kinds of different patterns.
So we have the relational hardware
to build any specific monoid if we want to.
So that actually brings me to the conclusion
of the math portion of this,
which is what we've seen so far,
which is that we have trade-offs in math
and we have scarcity, therefore, in math,
which happens because of logical consistency,
not physical limitations.
So as soon as we can commit ourselves to being consistent,
we end up committing ourselves to making trade-offs,
which is what we need to see agency.
And what's fascinating about this is that the agency of math,
it's not something that humans just use
to solve our problems in the physical world,
we can define problems in the mathematical world,
like how do you cast projections to A and B?
Or how do you count by one?
And we can find optimal solutions to those problems
that are optimal for math itself, not for humans,
but just in purely mathematical terms,
it ends up being optimal.
And optimality often ends up relating to laziness,
we can be formulaic and just turn our brains off
and do things in the most obvious way.
So when you're doing math, like as a mathematician,
it's often very useful to use a genetic concept.
So just like you can settle like debates about free will,
like, well, it's useful to think of myself
and making choices of like,
buying something in a restaurant.
Similarly, if you're doing math,
it ends up just being very useful to be like,
if I was this set here,
how would I want to solve this problem?
As someone who is currently trying to learn math,
I do find that perspective useful.
So math ends up doing agentic things,
behaving like an agent.
This isn't necessarily like hard proof that math is agentic,
but it does show that thinking of math in an agentic way
seems to work, there's no problem with it.
So why not try thinking of it that way?
Now that brings me to timeless agency
and I see that it's been about 40 minutes,
which means I probably talked way too fast.
So let me actually pause here
before we move on to timeless agency
and just see if there are any questions
about just what's going on, what am I saying?
If anyone just wants to pause here,
if there's just something
that really doesn't make sense or is niggling at them,
please feel free to ask
and otherwise I'll move on to timeless agency.
Seems like we're good so far,
so thanks everyone for bearing with me through this.
Let's move on to timeless agency.
So there's an obvious problem
with thinking about math as agentic,
which isn't that math doesn't do anything.
Anything that math does, a human does,
like we have to write down two plus two equals four,
the math doesn't write itself.
So humans can move around, math just holds still.
So how can we talk about agency if we don't have time?
The argument I'm going to make
is that we can think about agency in terms of predictions,
like free energy minimizations,
all about making predictions.
What a prediction does is something
that preserves structure through time.
Like if I'm predicting the weather,
I've got some structure, some models, some data,
and I can predict what it looks like in the future
as we preserve that structure.
And so what I'm going to ask is,
well, what if we just drop the time part?
What if we just talk about structure preservation
without time, because that's what math is all about.
It's all about structure preservation.
So much of math is just about structure preserving maps
from one object to another.
So what if we just drop the time portion of that
and just talk about structure preservation,
then we can talk about math as being making predictions,
which seems to be the heart of agency.
So what does timeless prediction look like?
Very simple example, let's say that you have two apples,
and I tell you that I'm going to bring you three more apples.
Well, you're going to predict
that you're going to have five apples.
But the core, now this is a prediction
that takes place in time, right?
You're predicting what you're going to have in the future.
But the core of this prediction, no pun intended,
is two plus three equals five,
which does not have a temporal aspect to it.
It's just one equation.
It's not like the two and three happen first,
and then you have five, it's just one thing.
So we're using timeless things to make predictions
through time, two plus three equals five
feels like a prediction kind of thing.
It's just there's no time element to it.
So let's look at an example we've seen before,
free monoid versus the expensive monoid.
Can we use this free monoid to make predictions
about the expensive monoid to know things,
what a prediction means is we're going to look
at the free monoid, and we're going to use it
to know things about the expensive monoid.
That's what a prediction is using one object
to gain knowledge about another object.
Well, yes, we can make this prediction,
which is with this mapping we've seen before.
We can use this mapping to know how this is going to behave
without actually having to look at the expensive monoid.
We can use our free monoid to make predictions about it.
So this tells us intuitively,
prediction is all about structure preservation.
We have a structure preserving map
from our free monoid to our expensive monoid.
And that's what allows us to make predictions without time.
There's no time happening in this prediction.
We're not making predictions about the future.
We're just making predictions
in the timeless mathematical world
with structure preserving maps.
So prediction is structure preservation.
This is my short argument for math being agentic
because it makes predictions.
Things like two plus three equals five are like prediction,
but there's just no time to it.
Predictions are all about structure preserving a long time.
Let's just drop the temporal element.
We don't need it.
Math does structure preservation without time.
So let's talk about structure preservation without time.
If that seems weird,
well, we accept that memories are not about the past.
So why do predictions need to be about the future?
If we can say, I'm not remembering about the past,
why can't we say I'm not predicting about the future?
The usefulness of doing so,
if that seems like a weird decision,
is it lets us talk about math being agentic
because math is all about structure preservation.
So structure preservation is prediction,
then math is all about prediction,
agency is all about prediction
because of free energy stuff.
So there you are.
And this is in fact how things work in the physical world.
So here's a very important source of intuition.
Take this footprint in the mud here.
Now, obviously what this footprint does
is it preserves the structure of the foot.
We can see this footprint
and clearly it reflects the properties of the foot.
And we can use this footprint
to make predictions about the foot.
We know what the foot is going to look like
based on this footprint.
But you don't have to think about time here.
So we actually like freeze the universe here
and say just stop, you know, there's no time here.
Can I not in fact still make predictions
about what the foot looks like based on the footprint?
I don't have to make a prediction about tomorrow.
Just say, I can look at this footprint.
I know what the foot looks like now.
Do I care that there's not a temporal element to that?
I just don't see why that matters.
So something else that's also super relevant to this
in terms of biology.
So this is morphology.
So our transformation, the way the mud transforms
is what the footprint is made of.
It's the transformation that constitutes the prediction,
which is also how predictions work in math.
We take our free monoid, we transform it via a function
to reflect this looping relationship here.
So we've got morphology in the physical world
are constituting our set of predictions.
We have morphology in the mathematical world
is constituting our set of predictions.
The only thing missing is time.
It doesn't seem like time is actually relevant.
So we can equate prediction with morphology.
We can think of the footprint as being the prediction.
The footprint is the prediction.
We don't need an intelligent human entity
to come along and say,
I'm going to use this to make a prediction.
It is the prediction.
And the reason we know that
is because the mud is using it to make a prediction.
The mud has formed this shape
because it is anticipating being stepped on
in the same way again.
The mud is minimizing its free energy
and it's saying, how do I, I've been stepped on.
How do I transform to minimize the amount of change
I could experience being stepped on again
by the same foot in the same place
that forms this footprint
to minimize the amount of anticipated change
that we could experience
because it's minimizing free energy.
And of course, the mud is just very stupid.
So instead of understanding that
there's all kinds of things that could step on it,
it forms a very limited prediction
being like, this is the only foot in the world.
It can only step on me one way because it's kind of dumb.
But we have transformation through time.
We have transformation without time,
you know, the physical universe versus math,
but it seems like it's doing everything the same way.
And this is just like what we saw
with A times B remembering A and B.
We normally think of memory as being with time,
but we can remember things without time
just with structure preserving maps.
So math doesn't do anything in time,
but it does transform.
It transforms via functions,
which don't have a temporal quality,
but it still is transformation
and transformation or structure preserving transformation
is what agency is all about.
So that takes me to the end of this talk.
So thank you all very much for sitting through that.
I know I probably talked too fast.
I got to work on that.
So just to remind you all the basic idea here,
maybe what's going on with agency and the physical universe
is exploiting things that math gives us for free.
What I'm personally trying to do with that
is solve some problems in economics.
There's a famous problem called problem of externality,
which normal mathematical tools and economics
didn't help me solve.
So I ended up moving over here
to look at that kind of problem.
And in general, it seems like math has an agentic structure,
so or an economic structure.
So I'm kind of interested in producing a theory like that.
Just as an example,
groups are monoids with an extra axiom.
So price repaying, am I buying that in a mathematical market?
Just think that's an interesting idea.
It might lead somewhere.
If any of this interests me,
please feel free to email me at Benj...
Or interest you, please feel free to email me
at benjman.f.lians at gmail.com.
Thank you very much for sitting through that again.
I know I probably talked too fast,
but that brings me to the end.
So if anyone has any questions
or anything they'd like to say, please feel free.
Cool, thanks so much.
Questions?
So when you're talking about timeless agency,
I was thinking about,
does it relate to invariant quantity like a Hamiltonian's?
Because Hamiltonians don't have time in it,
but you can generate time sequence from it.
Yeah, I don't know anything about physics,
but I am vaguely aware that there are ways
of looking at some physical thing
where you're not necessarily thinking about time.
So I can't give you a specific answer,
but yeah, invariants in math and invariants in physics,
undoubtedly highly related concepts
and being able to talk about invariants
without talking about time,
which you can clearly do in math,
probably does help us explain that relationship
between agency and the physical universe
where we do have time
and agency and the mathematical universe where we don't.
Okay, thank you.
Because I saw recently there are some research domain
like an edge based model.
I think those models,
they don't have explicit time in the model,
but they just want to minimize the energy.
So it's similar to what you said.
Absolutely.
Yeah, there's a physicist named Julian Barber.
I haven't actually read their work,
but I know they produced like a book
about like doing physics without time altogether.
So I haven't read it,
but just like the awareness of that
as a conceptual possibility did inspire some of these ideas.
Okay, thank you.
Okay.
Oh, hi.
I have a question.
Thank you for the talk, by the way.
So it started out with the premise
that scarcity is a necessary condition for agency.
So agency implies scarcity,
but is scarcity, do you think scarcity
is also sufficient condition for agency?
Because I think for any condition,
any definition of agency should specify both
necessary and sufficient conditions.
Yeah, I agree.
That would be nice to have.
I'm just not entirely sure.
It seems to me that it's conceptually possible
that you could have like an empty mathematics
where you have logical consistency,
but no assumptions, no theorems, no nothing.
So maybe you don't have agency
because you're not doing anything
or maybe it's trivial agency.
So it might end up being sufficient,
but I just don't know for sure.
So I feel very confident it's necessary.
I'm not entirely sure that you can just say
that it is sufficient for sure.
So it might be, but I just didn't want to say that
because I'm not 100% on that.
Also kind of a related question.
We often, I guess in cognitive science
and the literature says that agency standard
requires some kind of embodiment, right?
And when you think of math,
math is a very abstract
and in a way it's limitless.
It does not have an embodiment
and it does not,
and one thing that embodiment does is
it specifies some kind of constraints.
And math in a sense is limitless
and yet it seems to have some property of scarcity,
some kind of constraint,
which is kind of confusing to me.
But in any case,
I think do you think that math
also has some sort of embodiment
which people often think is required for agency?
I think the embodiment consists
of the relational structure that forms
when you define some purpose.
So let me go back through,
like back to, to, you know, here or here.
So like, you know,
this kind of feels like embodiment to me.
Like we're defining A times B.
So this may not look like a definition.
This is a definition.
This defines A times B.
And what it is is it's just a set of relationships
to other things defining sort of information
that's coming in and ways of building things.
When we look at what A times B does
and how it relates to everything,
I'm not an expert on embodiment.
That's kind of my understanding
is that to make sense to the brain,
we have to think about how it connects
to the rest of the body
and what it does out in the environment.
We can't just consider it as some isolated,
you know, just intelligence floating around like a spirit.
We do end up doing something similar in math
where you end up defining objects
by these universal properties,
by these systems of relationships.
So I think you could think of this
as analogous to embodiment in math.
I mean, I think it's related,
Sandesh, I think it's related to the stuff
that Madeline and I are doing with the grim sentences
where it seems like there is at least something
that it's all doing in the absence of,
in the absence of specific embodiment in the,
like I think it's related to the platonic space stuff
that we talked about it to the, to Madeline's project.
Yeah.
I guess in a way, any definition of agency
would have to be specified in mathematical terms
at the end of the day because, right?
Because we do not have any other language
to express any concept other than mathematics and logic.
And I guess mathematics is like logic,
it is a foundation of mathematics itself.
But if you assume that mathematics is the only language
through which we can express agency,
perhaps it kind of follows that mathematics is agentic
because it's the only language we can express agency in.
Is that sort of a paradox or a contradiction?
I'm just thinking about here.
Yeah, no, it's a great question.
The way I think it's going to end up being resolved
is we're not going to actually come up
with like a specific definition of agency
where we can exactly say this is agentic and this isn't.
What agency is going to end up being is it's going to be
a human term for clumping things together
for pragmatic uses based on common underlying structure.
So the argument that math is agentic here,
I don't intend to find a precise definition of agency.
Instead, what I intend to do is show that it does the things
that we normally call agentic.
If we saw a physical object doing the stuff
that we see math doing, we would call it agentic.
So maybe we can just call it math agentic as well.
And the reason to do so is not to be necessarily
just super consistent for consistency's sake,
but because that will lead to useful ideas,
both useful things like finding these novel competencies
and useful things for scientists broadening their horizons,
bringing in new mathematical tools, asking new questions.
So that's going to be the ultimate benefit of this
is helping humans make smarter categorization choices
which will lead to more efficient scientific
and mathematical decisions.
Thank you.
I had a question about the...
Go ahead, Mike.
No, no, you go ahead. Go ahead.
OK.
So let's assume for a second that we agree with that idea
of math being time.
I can see what you mean, I think.
What I'm curious, though, is can you apply that to computations
as well, right?
So your example of the equation where you have 3 plus 5,
oh, what was it?
I've got numbers.
3 plus 3, I think, equals 5.
But not only in a computer setting,
if you have any kind of network where you update states
based on equations, essentially, right,
you have to make those computations.
And even though you don't specify exactly what time step that is,
tasks have a step and actually matters a lot
in these network computations and any networks I've seen,
how you update it.
Do you all do it synchronously?
Do you do it sequentially?
You can see that very intuitively changes the outcome of computations.
So that's kind of my question.
Do you think you can have, even if you can have,
even if you express math without time,
can you also get time out of the computations?
Yeah, I don't know a lot about the specific details,
but it's something I have been thinking about
because it makes sense as a question
and it is something that needs to be resolved.
I suspect that there probably are ways of defining time
in terms of abstract connections,
which is basically some kind of,
it's hard to articulate this,
but some kind of functorial quality that's varying
as other things are varying and sort of structure preserving maps
that are themselves, structure preserving
as they move along their own structure preserving track.
And there are things in,
so all this kind of comes from category theory,
there are things in category theory that look like that.
And so maybe that will lead to some interest,
I'm sure people are working on that.
So I'm sure you could lead to some theory of computation.
So I don't really have a specific answer,
but it's definitely an important question,
something that I'm interested in as well.
And I have no doubt that I will not solve it
and hopefully someone else will.
All right.
Just one super short follow up.
What about logic?
Same questions for logic.
Because if logic, if you have,
if 2 plus 3 equals 5, then, you know, X, Y, Z,
isn't it then, doesn't that also imply the same
as a computational aspect time?
Yeah, that's a super interesting comparison
because it is a temporal world.
And we're talking about then in the physical world,
that is time.
And when we talk about then in the mathematical world,
we're not referencing time.
The way I take that is to say that we can basically forget
about time in the physical world and think about our predictions
as being structure preserving,
even if we just drop out the time element.
But it may well be that you could do the other way
and say we're going to add time into math.
So I don't know how to do that,
but it's a perfectly logical thing to try.
Yeah.
No, that's super interesting.
I just want to, like, I'm super behind saying,
you know, we don't need to have a specific time.
Like, I mean, even though that will, of course,
make it more realistic and to these different explanations,
but the idea of, right, of a logical connotation itself,
even though it doesn't, like, it's not really time-dependent
in terms of quantitatively.
But I feel like the idea itself of connecting things
logically, even in a pure mathematical space,
basically have a time element to it.
But maybe you're right.
Maybe I'm just predicting language problems onto it.
Is there anything you think there was more?
Yeah, I mean, I think...

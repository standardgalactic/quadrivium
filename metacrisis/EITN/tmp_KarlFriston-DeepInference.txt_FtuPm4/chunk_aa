It's a great pleasure to be invited and it's actually lovely to see you all again virtually.
So I'm going to start off in apology. My talk is not really about data analysis or trying
to characterize neural dynamics. It's more standing back and asking whether we can understand
neural dynamics as using entropy or notions of entropy, relative entropy and energy as the
objective function itself. So I'm going to talk about the brain as doing inference and trying
to cast that inference in terms of thermodynamics and information theory. So as such, this can
be taken as a light entertainment. It's not going to address some of the hardcore issues that you've
been wrestling with. I had just one message with this talk. It was given I only have 20 minutes
and I was just going to talk about sentient behavior with the emphasis on behavior and the
imperatives for action when looked at from through the lens of the brain as an inference machine.
However, I've been so compelled and intrigued by all the really interesting sort of concept and
issues that you've been talking about for the past couple of days. I'm going to go through the first
set of my slides quite slowly just to try and use them as placeholders to revisit some of the
sort of key ideas that you've been discussing. And as such, I'm unlikely to get to the end of the
talk, but that doesn't matter. So Jennifer, can you stop me when my time is up? And it doesn't
matter, because I think that's the interesting stuff I think will be will be in the discussion.
So what I want to do is address the theology of the brain from the point of view of energy
and entropy and ask the question, what is the brain in the game of doing? And we already know
the answer to that. So Elaine told us that, you know, basically it has to comply as with all
things with James's maximum entropy principle, and reminded us that one way of looking at that
is to minimize surprise. So the story on offer here is essentially neuronal dynamics can be
understood as minimizing surprise under the maximum entropy principle. That's about it. I repeat that
the sort of the additional focus I was going to bring to the table was what does that mean for
choosing what to do next in order to comply with those imperatives. And I was going to talk about
that in terms of self-evidencing normative models of what it is to be a sentient creature
interrogating our world and making sense of that world. The implications are the benefits of applying
surprise, minimizing, entropy, maximizing principles for artificial intelligence, and then
briefly rehearse how we apply those principles in computational neuroscience in terms of leaf
propagation and epistemic foraging. So I'm going to basically start by assuming that everything
that we do and our neurons do and everything that characterizes us over different time scales
can be understood as maximizing the probability of the outcomes given a given me or M or a model
of the world. So basically just the likelihood of our observed sensations.
And just to put this in a larger context, this, if you like, this quantity, this probability of
outcomes given me can be read in many different ways, which provides a nice perspective on
different global theories or attempts to understand or formalize or provide normative
theories of behavior. So I can read these outcomes as characteristic of me, so that there are the
kinds of outcomes that I would expect to encounter. And if I act in a way or selectively sample those
outcomes, then I'm effectively maximizing the value of those outcomes because they are defined
as the most likely kinds of outcomes that I would want to sample. And if we treat that
quantity, that log likelihood, log marginal likelihood, marginalizing over all the causes
as a value function, then we can understand this optimization, this normative perspective
in terms of reinforcement learning, optimal control theory, expected utility theory in
economics and so on. We were asked previously, what is surprise? Well, in my world, surprise is
just the negative value or the negative marginal likelihood. It's provided or equipped with an
evidence lower bound, a negative physical or physics free energy, which we'll talk about in
a second. But the simply surprise is a negative value. It's the negative or the implausibility
of an outcome given me. And from that, we can read surprise as surprise or self-information
and information theory. And we can interpret this in terms of the informatics principle,
we've seen Linska refer to minimum redundancy, Horace Barlow, and indeed the free energy principle.
Clearly, the time average of surprise is entropy. So if we're going to maximize the value or minimize
surprise, we are effectively trying to resist the second law. We're trying to minimize the dispersion
of our states or the outcomes that we are exposed to. And this, of course, is the holy grail of
self-organization, synergetics. And if I were a physiologist, it would just be a statement of
Hermia Stacey's trying to keep my intercepting outcomes, my interception within plausible and
physiological bounds. I'm going to read this marginal likelihood, though, in another way,
which would make sense to a statistician. It's just, and they're going to do that and license
that interpretation by replacing me with a model. So it's the probability of some observables,
outcomes, given a model of how those outcomes were generated. And on that reading, this becomes
a model evidence. And then we can spin off from that, the Bayesian brain hypothesis,
evidence accumulation, and things like predictive coding. So that's setting the scene.
A focus of what follows is really this formulational bound in machine learning known as an evidence
lower bound on surprise or the log marginal likelihood. So this is where we get to the
maximum entropy principle under those prior constraints. So the reason it's called a free
energy is it can be decomposed into an energy and an entropy. And we're going to be working with a
negative free energy here. So we're talking about negative energy and an entropy where I'm reading
an energy or a potential as a log probability or a negative log probability. And I've written out
the free energy in this form just to emphasise what we're talking about here is indeed the
maximum entropy principle here. So this is just the entropy of some beliefs cue about states of
the world generating my observations at any particular time tour here. So if I want to
maximise F, I'm going to want to maximise my entropy whilst at the same time maximising
the likelihood of the observation. So I'm doing maximum likelihood basically under the maximum
entropy principle subject to prior constraints, my prior beliefs about states of affairs out there
generating the observations sampled by my sensory epithelia. So that's the basic construct.
And I'm going to now unpack what that means and how one can understand the imperative to
self evidence or maximise model evidence or minimise surprise using this particular
formulation or formalism for the maximum entropy principle. The first thing I'm going to do is
just motivate why this might be a good way of describing behaviour and intelligence
from a number of perspectives, particularly sort of data analysis, artificial
intelligence and indeed data mining. What I've done here, I've just rearranged these terms
to explain why this quantity is known as an evidence lower bound in machine learning
and artificial intelligence. So what just by swapping around the terms, what I can do is express
this in terms of an expected probability, log probability of the outcomes here.
Notice that this does not entails the state's duration of the outcome, so I can just focus
on the log probability, the evidence itself, the model evidence minus a bound, a KL bound
or relative entropy here, which is just the difference between the true posterior of over
the states or the things, the causes of my observable consequences and my posterior beliefs
queue about those causes. And because this can never be less than zero, this is why it's called
an evidence lower bound, it's always less than the log evidence. So if I maximise F,
I'm implicitly maximising that log marginal likelihood or minimising that surprise.
So that's a perspective on this sort of optimisation process that is used to
promote the use of this variational inference or bound inference to provide explainable AI.
In the sense that what we have in this expression is a recourse to a probabilistic
relationship between causes and consequences known as a generative model. So the probability
distribution over the joint causes and consequences, the hidden causes that we have to infer
and the observable outcomes or consequences of those causes. And so the beauty of this approach
is it rests upon explicit generative models. You have to write this down before you can define
this quantity before you can optimise it. It entails a precise specification of prior assumptions
and all the hyper parameters if you have a hierarchical generative model. It leads to
a design optimality, whether it's experimental design or designing an artefact or designing
your steps in your optimisation scheme. As we'll see later, it leads to literally
optimal Bayesian experimental design, sometimes known as active learning, objective inference
that can be in some instances autodidactic. The other aspect of this is because we will see
that you are effectively, when you put action into the game, sampling those data that provide
the best kind of evidence for your model or you. You now have a principled way to address data
foraging and epistemics, choosing the right neurons to record from or choosing the right
Wikipedia page or news channel to resolve your uncertainty about the world. Just rearranging
these terms again to provide another perspective on this free energy bound.
So what I've done here is swapped around the terms again. It's still an evidence lower bound,
but this time it's expressed in terms of accuracy and complexity. So maximising the free energy here
maximises the accuracy, the expected log likelihood of outcomes given their causes.
And this quantity here, which is the relative entropy on the KL divergence between the posterior
and your prior. And this is, I think, a really nice term. I think it implicitly has been discussed
a lot yesterday and today. So for example, one could construe the synaptic homeostasis that
Chiara was talking about, that sort of housekeeping that improves generalisation and
learnability, enhancing performance, say, accuracy the following day. One can construe this basically
as minimising the complexity, removing redundant degrees of freedom in your
genogen model of the world literally by taking out those unstable, friable, synaptic connections
and associations in a judicious way that minimises complexity without compromising the accuracy.
And clearly to focus on the complexity part here, we can ignore the accuracy simply by switching
off observations by sequestering ourselves from the sensorium whilst asleep and letting those
slow wave dynamics roll forth with abandon. From a sort of statistical
perspective, this minimisation of complexity that could also be read as an algorithmic complexity,
and I notice as several references to the data compression or compressibility perspective on
this, this complexity, if you like, is the placeholder for that, the fewer bits you use in
order to make sense of a message or to explain some data, then the more compressed it is and the
more compressible it is. And this, of course, is just an expression of Occam's principle that in the
brain would be revealed in terms of functional specialisation and redundancy minimisation
and efficient coding. So Jennifer was mentioned before, degeneracy and redundancy from Jerry
Adelman. When we revisit the distinction between degeneracy and complexity, sorry, redundancy
in this form of formalism, then you sort of associate redundancy with complexity. So redundancy
is a bad thing and yet you have to pay that price, that complexity cost in order to provide
an accurate account of the data. So there's always redundancy there, but you're trying to minimise
it. And that sort of contrasts with the notion of degeneracy, which I think was simply read in
terms of maximising the entropy so that you don't commit to a very particular way of explaining some
data or ways of doing things. So other ways of understanding the imperative to minimise complexity
that may be afforded by things like sample chemostasis is that you're minimising the
computational cost. So, you know, this just scores the degree to which you've changed your mind
in the face of new data. And if you can do that with a few bits or parameters or degrees of freedom
as possible, then you're minimising your computational cost. And you can read this now as
a kind of bounded rationality and choosing the right mean field approximations leads you to
approximate Bayesian inference. And it can be understood in terms of structural priors on the
way that you're trying to explain your particular world or the way that is sampled. We've already
heard about Landauer's principle, you know, the energy required to erase bits. I find the
Jouzinski equality probably more relevant in this setting because it applies to open systems
that are far from equilibrium, provided you sort of do your updating sufficiently slowly.
And what it's saying is that there is a thermodynamic homologue of the computational cost.
So put this very simply, if you've got the best kind of computer for doing any
accurate explanation of any data, then that computer should use the least amount of electricity
and be as small and as compact and as cool as possible, as indeed the brain is if you compare it to
some of the supercomputers around nowadays. So the story I did want to tell, and we're not going to
have time to finish this, is what does this imperative to maximise accuracy whilst minimising
complexity mean for the moves that we make on the world in terms of securing the next round of data,
whether we're whisking or whether we're making a cyclic eye movement or whether we're attending
to the next phoneme. What are the best actions that I or plans that would secure the right kind
of data to maximise this quantity? And the story would have been of the sort that effectively
maximising the expected accuracy given an action or given a plan or a move translates
in terms of optimum Bayesian design and active learning whilst optimising the complexity or
minimising the expected complexity effectively is the same as making Bayes optimal decisions by
minimising risk. So we've got two, if you like, aspects to self-evidencing, minimising expected
surprise that can be unpacked in terms of optimally designing experiments, perceptual
experiments that tell us where to look next whilst at the same time minimising the risk of the outcome
of making that move or looking over there. And I would normally take you through this
with a series of questions starting off by asking, if you're an hour and you're hungry,
what are you going to do? And normally people reply, I've got to look for food. And in so doing,
highlight I think an important dichotomy between different ways of formalising or writing down
normative models. And that dichotomy rests upon the notion that there exists some value functions
of states of the world that would ensue if I made that move or did that control you
to produce these new states of the world. And if I can write down that value function,
I can always identify the optimal sequence of actions or action from any given state
to secure the most valuable state. But the answer before I'm going to search for food
tells you that you're effectively making moves to resolve uncertainty in this instance about
where your food is. And the key observation about that is its uncertainty, the uncertainty
is an attribute of your beliefs about states of the world, not the actual states of the world.
And so we have to replace this value function with a function or a function of a function,
in this instance, a function of beliefs about states of the world. So the real question is,
what would my beliefs about states of the world in the future be if I made this move?
And furthermore, it matters whether I seek and search for my food and then eat it or try to eat
it and then search for it. So we have a slightly different perspective known as sequential policy
optimization, where we're now trying to optimize, maximize an energy functional beliefs about the
world, leading to effectively a description of good actions that are trying to extremize
a path integral of an energy function of beliefs that in fact is just a statement of
Hamilton's principle of stationary action. So normally we contrast the Bellman optimality
principle, which predicates itself upon the existence of a value function of states of the
world against a Hamiltonian principle of stationary action, which is more apt for describing behaviors
that optimize beliefs, basing beliefs, sub personal beliefs about the world. And then we would have
looked at the anatomy of active inference under that lens, where basically that goodness function,
that functional is that expected free energy that we were referring to before, essentially
comprising the accuracy and the complexity that constitute this evidence low-brown, but lifted
to the future, where now we are evaluating complexity and accuracy in terms of the expected
complexity and accuracy in the future, given a particular plan. And we'll talk about that in
terms of risk and ambiguity. I'm sure I'm out of time now, aren't I? I think you can have another
two and a half minutes by my side. Well, I will use that two and a half minutes to show you what
you would have seen had I not got distracted by you and the generosity and surprise and all those
wonderful concepts. So what I would have done is take you through that sort of different ways of
writing down this, to my mind, a sort of machine learning version of James' maximum entry with
principle and explain them from the perspective of different people, for example, based in surprising
visual search or information, infamax principles or minimum efficiency maximization
from the point of view of Horace Barlow, how that relates to KL control and risk sensitive
control in engineering and economics respectively, and how very special cases emerge as an
expected utility theory when we take uncertainty off the table. I would rehearse the notion that

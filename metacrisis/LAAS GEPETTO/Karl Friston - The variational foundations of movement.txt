And then we'll be at the breakfast just after his talk.
Thank you very much.
That's a great pleasure to be here.
As some of you will know, I'm slightly outside my comfort zone.
In fact, a very, very long way outside my comfort zone.
I found the talks this morning intriguing.
A lot of the rhetoric I don't understand,
but there's also a lot of curious similarities
between many of the issues that we deal with in theoretical neurobiology.
Let me move this higher.
I was just saying that I found a lot of the rhetoric and the ideas
slightly mystical that I've been hearing about,
but there's also a lot of formal similarities between some of the issues
that we deal with in theoretical neurobiology,
and in particular asking questions about how the brain
actively infers and copes and exchanges with its environment.
So I think, from your perspective,
you can regard me as the light entertainment of this workshop.
So what I'm going to do is try to overview for you
the basic principles that we bring to bear
when trying to understand action and perception in the brain
with a particular emphasis on not geometry, per se,
but certainly the information geometry and the variation...
and the variational underpinnings of that,
and hopefully touch on some of the issues
that we've been hearing about in the previous talks.
So let me overview for you what we're going to talk about.
I'm going to start off in a very abstract way,
just asking some basic questions about the nature of self-organising systems
and in particular biological systems,
and I'm going to place centre stage the boundary
between me or an agent and the outside world
with which the agent exchanges.
In particular, I'm going to focus on the statistical notion
of a Markov blanket that separates me from the rest of the world
and ask what it means to have a Markov blanket
and yet still preserve an egotic exchange with the world
that means that I can preserve certain states
or I pursue certain trajectories in terms of my internal and external states.
I'm going to illustrate the nature of those principles
just by simulating a little primordial soup
and generating a little agent within that primordial soup
and performing some experiments upon it.
Having established the basic idea and some of the principles,
I'm going to tell exactly the same story
but from the point of view of a neuroscientist
and looking at the form of the variational principles
as they might be expressed in the brain
and in particular looking at the notion of predictive coding
and its implementation in canonical microcircuits in the brain.
Then I'm going to illustrate those variational principles
with a few toy simulations
and these are going to be very, very toy from your perspective.
They're very simple, but they at least illustrate the basic idea.
I'm going to focus on action and its observation
and then if we have time,
I'm going to conclude with a simulation of eye movements
and the sampling of visual information from the world.
I'm going to start, again, I apologize for being very abstract
but I think it's quite important from my perspective at least
just to motivate the underlying, if that's going to be a gradient descent,
it's going to be a Gauss-Newton gradient descent,
the underlying dynamic that we're going to be using.
I'm going to start here with a question posed by Schrodinger.
What are some of the events in space and time
which take place within the spatial boundary of a living organism
to be accounted for by physics and chemistry?
I'm not going to answer that question,
but what I want to highlight is the notion of a spatial boundary
and the deeper question underlying this question,
how do systems induce and maintain the separation
of the spatial boundary between themselves and the rest of the universe?
He would be the first person to acknowledge
that that boundary itself is a statistical object
and in statistics that would be basically a Markov blanket.
A Markov blanket is going to be a statistical boundary
between some internal states of the system,
which are denoted by blue here,
and external states in Cian here
that provides an insulation of a statistical sword
between, say, my states and the states of the rest of the world.
How many people here know what a Markov blanket is?
Good. Could you explain to your friends what a Markov blanket is?
Yes, I think so.
You want that blue to be conditioned
in dependence of the light blue given the red.
That was very succinct and absolutely correct.
Let me briefly unpack that.
In fact, I'm joking.
I think most of you here do know what a Markov blanket is.
So can you put your hand up if you know what a Markov process is?
Yeah, so you all know essentially what a Markov blanket is.
So with a Markov process in time,
the Markov blanket is just the preceding state.
So it's just, as he says,
it's just the states you would need to know
that contain all the information about the rest of possible knowledge
in all the remaining states
in order to predict the state of reference
or the internal states here.
So if I wanted to predict this state,
given the causal influences of all other states,
it would be sufficient just to know the Markov blanket.
And I don't need to know all the remaining.
So these are now conditionally independent of these,
or vice versa, conditioned upon the Markov blanket.
And the Markov blanket comprises the parents and the children
and the parents of the children of the state.
Now, from our point of view,
there's an interesting bipartisan on that Markov blanket
that is induced just by considering states that are
not influenced by the internal states.
So for any system that can be written down in this form
in terms of statistical dependencies,
there will exist a Markov blanket.
And that Markov blanket can always be split into two sets,
which I'm going to call active states and sensory states.
And I'm going to motivate those labels just by asking you
to consider that causal structure in relation to things
that we know and love, biotic or biological things,
from single cells through to, say, brains here.
So the internal states will correspond
to all the internal states of the cell.
The active states could be the active filaments
that support the surface states of the sensory states
and provide the cell with motility.
While the sensory states are caused by external states here,
that in turn influence internal states
that cause changes in active states
that again cover back to the environment.
And exactly the same slightly sparse causal structure
can be found in the brain.
We have the internal states of the brain,
all the synaptic activities, connection strengths and the like
that influence and cause changes in our actuators
or effector organs that influence or change the external states
that are registered by sensory states
that in turn change the internal states.
So I'm just saying that for any system,
any organism or agent or robot embedded in a larger system,
we can always apply this partition
provided certain conditional independences exist.
And in fact they have to exist in order to produce
a distinction between me and the rest of the world
through my Markov blanket.
I'm now going to ask you to forget about that
because what we're going to do is now just look
at the behavior of all interesting systems
and then come back and put the Markov back into the equation
and see what the implications are
for the behavior of the sensory states,
the internal states and the action states.
So just to motivate, so this is going to be one
of a couple of slides with lots of equations
or some equations on.
And this is the basic slide that motivates
the variation approach, information geometric approach
to the dynamics of these systems.
And what I'm doing here is in an abstract way
is taking any random dynamical system
where I'm lumping together states of the world
and control states into X and adding random fluctuations.
And I'm presupposing that we're only going to be talking
about systems that have measurable characteristics
that persist over time.
So by definition they are egotic in a weak sense.
And that implies the existence of a random dynamical attractor.
So we have two states here and they trace out
a manifold or an attracting set here
as time evolves with multiple trajectories.
And we can interpret that ensemble of trajectories
or attracting set in terms of an egotic probability.
The probability that I sampled the system at a point in time,
the probability that I would find it in that state
would be denoted by the density here of these trajectories.
So this could be, say, the states of a system
whilst walking, for example.
So if this attracting set exists, if the system exists,
then it is equipped with or it has this probabilistic description
that can be described by the Fokker Planck equation.
Is this a familiar object to many people here?
Yeah, okay. Also known as a master equation,
the Conveyor forward equation, lots of different names.
For those people who don't know, it's relatively simple.
I'll explain what it does in a second.
But from our perspective, its form is not terribly important.
What is important is its solution.
Because if this attracting set exists,
then the rate of change of this probability is zero,
which means I can rearrange the solution
to express the flow of states as a function
of the gradients of the log probability distribution.
So this is the solution that I'm going to appeal to.
And it is this one solution here
that I'm going to use as a variational principle
to understand all internal state changes
and all action of the systems that exist.
Just heuristically for those people who are not familiar
with the Fokker Planck formulas.
It's very simple. All it's saying here is
if you imagine just dropping a drop of ink into the ocean,
then these random fluctuations here
will disperse the concentration of the ink throughout the ocean.
So in that instance, it wouldn't have an attracting set.
It wouldn't have behaviors that we can measure and admire
or put on YouTube.
So it must be the case then
that there is some deterministic flow
that is countering the dispersive forces.
So it's acting in opposition to the dispersion
which degrades areas of high concentration
and high probability distribution or density.
In other words, it's actually flowing towards
regions of high concentration.
And when those two dispersive
and deterministic flow components are in balance,
then the system becomes,
attains non-equilibrium steady state
and is weakly ergodic.
And that's all that we're describing here.
This equation here is just an expression
of the Helmholtz decomposition.
It's just expressing this flow in terms of a hill climbing
or gradient descent on the log probability
and a solenoidal or divergence free component
that actually breaks the detailed bounds
of a system in general.
We'll use both terms later on,
in a way which I think many of you will be more familiar with.
And this cartoon is just to show
that there are two components to this flow.
But the essence is that the flow of any system
should exist, has to do a hill climbing
on this log probability.
So now let's go back
and put the Markov blanket back into the mix.
And let's do it in terms of the states
that constitute me, my sensory states,
my active states and my internal states.
Hill climbing behavior is still true,
which means it is still the case
that all my internal states will try to maximize
the log ergodic probability.
And all my actions, all my behaviors,
my active states will also try to maximize this quantity here.
And I'm going to associate these with perception and action,
try and motivate that for you in the rest of the talk.
But just to pause for a moment
and just think about what this quantity means
depending upon where you were educated
or what community you come from.
What we're saying is that the system will appear
to self-organize internally and behave in a way
that it's going to maximize this quantity,
which is just the energy of states it thinks it should be in.
It likes to occupy.
It's just the valuable states, the adaptive states.
So from this we can then derive,
or at least form a conceptual bridge
to things like reinforcement learning,
optimal control theory, accepted utility theory and the like.
The negative value is just the self-information
or surprise or surprise.
So any system that exists will be trying to,
in an information theoretic sense,
try to minimize its surprise.
And then that brings us to the principle
of maximum mutual information, minimum redundancy,
and the free energy principle where free energy here
provides an upper variational bound
or approximation to the surprise itself.
The time integral, the path integral over any trajectory
because of the ergodic assumption, is the entropy.
And that's nice because what this means is
there's a resistance to the second law.
It's resisting an increase in entropy.
And of course that is the holy grail
of self-organization, synergetics.
But of course it's just the statement of homeostasis.
It's just saying that systems, biological systems,
that reduce the tendency to disorder,
keep themselves within physiological bounds.
And we see very nice examples of bounds
in terms of, say, the constraints
on various loss functions or cost functions.
Now, as a statistician, you will also notice
that this quantity, or e to the surprise,
is the probability of sensory data given me.
But if I interpret me as a model of me and my environment,
then this is also known as model evidence.
It's the probability of the sensory samples
conditioned upon me existing or the model existing,
which means that this can be interpreted,
this maximization of this quantity here
can be interpreted in terms of the Bayesian brain,
evidence accumulation, and predictive coding.
And these two perspectives are what I want to pursue
and try and unpack in terms of process theories
of how the brain might work and how it might act.
So just to give you a very quick flavor
of how the nature of the dynamics that I've just described,
what I've done here is simulate a little primordial soup
using 128 macromolecules
that I've given autonomous dynamics using a Lorenzo tractor.
So each one has its own dynamics of electrochemical sort.
And they're spatial relationships
determining forces amongst the molecules
with weak electrochemical attraction
and strong repulsion as an inverse function
of the Euclidean separation.
And just lumping these little macromolecules together,
they sort of bubble away quite happily,
showing non-equilibrium state quite quickly.
The details of this simulation are completely unimportant.
You get exactly the same behavior
whatever loosely capital dynamical system you write down.
You put a little bit of random fluctuations in it
and it will show this sort of behavior
in one parameter regime.
The reason I've done that
is because I've written down the equations of motion
and I know the physical separation
and the statistical coupling between these molecules.
I now know that graphical structure, that adjacency matrix
which means in principle I can go in there
and find some internal states
and then mark off blanket.
And I can then look at that system
and then do experiments on it
to try and illustrate the two behaviors
that I was referring to before.
What I've done here is just reproduce exactly
the movement that we had on the previous slide.
But here I've color coded the internal states in blue here,
a little ring here with a small tail.
The active states in red
that surround the internal states
and support the sensory states or the surface states
that are exposed to the external states in cyan here.
And this thing wiggles around,
wrapping its tail, quite happily exchanging,
again, a non-equilibrium steady state
with the environment.
For those who are interested,
it's very easy to find these structures.
You just need to identify the mark off blanket matrix
which is constituted by the parents, the children,
the parents of the children,
and then spit it into the active and sensory states
using the rule that I described before.
So what I've now got is a synthetic little organism
that I can now ask,
does action maintain the structural function integrity of this,
often referred to in terms of things like auto-poesis,
and do the internal states act as little Bayesian engines
or infer the hidden causes of their sensory states,
namely active imprints.
So just briefly to illustrate the sorts of things
which we do as neuroscientists,
which are basically performing brain lesions
or studying people with strokes,
this is an illustration of what happens
when you perturb the system.
So here are the locations of those,
all the elements of that little organism here,
reported in terms of their locations
over 512 seconds under normal progression.
And these three panels show the equivalent locations
where I made mild lesions to either the active states
and these are very mild,
I'm just rendering them insensitive
to the electrochemical interactions coupling.
So this would be like paralyzing the agent,
the sensory states by making it blind
or giving it a little stroke by lesioning the internal states.
And the key thing, the obvious thing here,
is that whatever I do,
I basically kill the organism,
I destroy its equilibrium.
And indeed, we can assume now
that the proper maintenance of its structure
and function did depend upon that hill climbing behaviour
that I've illustrated in the first few slides.
Technically, this is actually known as oscillator death.
And it's closely related to something else,
generalized synchronisation,
which I'll mention in a second.
Here's the second illustration.
It's an illustrative purpose,
or to make a heuristic point anyway.
I'm asking the question,
is there anything in...
or do the electrochemical states
or the internal states
predict the physical motion of the outside world?
So this would be like a brain imaging experiment
where I present some moving stimuli in the outside world,
and I look for evoked responses
in the visual cortex of the brain.
And it's very easy to find mixtures here.
All the internal states lagged over time.
Find linear mixtures that do indeed predict
the motion of external states here.
The degree of prediction colour-coded
by the depth of this cyan colouring.
Here's the most predictable motion
of the most predictable external molecule
shown as a dotted line
and the prediction as a solid line.
And what's happening occasionally,
this molecule here gets expelled from the soup
and then falls back again.
And it seems as if this thing is actually predicting
or registering these external events,
despite the fact that they are statistically conditionally independent,
they're separated by the Markov blanket.
The reason I'm showing these examples,
you can almost see the fluctuation in internal states,
the evoked response associated with this outside motion event here.
But if you look carefully,
you can see that the changes in the internal states
actually precede the external states,
which begs an important question.
Are the internal states causing the external states,
or is the external fluctuation causing
and registering a representation
of a statistical sort in the internal states?
Of course, the answer is both.
All we're seeing here is an illustration of generalised synchrony.
So I'm sure you all know this,
but this is the phenomena where originally observed by Huygens
where you have multiple clocks hanging from the same wall or beam,
and ultimately they have to come to swing in synchrony.
That is the only long-term attracting state.
This is one of his drawings here.
It illustrates two clocks suspended by the same beam.
And that's all we're seeing here.
We're seeing a generalised synchrony between internal and external states
just by the very existence of these random dynamical attractors.
And here we can think about one clock comprising the internal states,
the other clock, the external states,
and then Markov blanket, the beam or the wall from which they are suspended.
I like this example because it highlights a complete symmetry
of what's going on here.
So if you subscribe to this idea that we are trying to predict
and model our world,
what this means is that the world is trying to model you
in an exact symmetrical way.
The world is watching you in exactly the same way
that you are watching the world.
So that's all the sort of heavy lifting
done from the point of view of the self-organisation to summarise.
And the existence of a Markov blanket necessarily implies
a partition of states into internal states and Markov blanket,
namely sensory and active states, and external or hidden states.
And because active states change but are not changed by external states,
they minimise the entropy of internal states and the Markov blanket,
which means that action will appear to maintain
the structural and functional integrity of a Markov blanket.
And at the same time, internal states will appear to infer
the hidden causes of sensory states by maximising Bayesian wall leavens
and influence those hidden causes through action,
and I'm going to refer to that as active inference.
So now what I want to do is just repeat exactly the same story
but using a different metric and the sorts of ideas and perspectives
that you'd find in psychology and neuroscience.
But we're going to be appearing to exactly the same ideas.
The only slight difference here is that previously,
I had written down a simulation where the value function
or cost function or that surprise function
was an emergent property of the differential equations.
Here I'm actually going to write down the gothic density
in terms of prior beliefs about the states
that agents like to occupy.
But the actual simulations and solutions that I'm going to show you
are based upon exactly the same differential equation,
hill-time equation, and if that's the same code.
So where this sort of talk normally starts is with the Helmholtz.
So now we're talking about the brain and the brain
as an organism that constructs predictions and explanations
for its sensory inputs.
Beautifully articulated in terms by this quote,
objects are always imagined as being present in the field of vision
as would have to be there in order to produce the same impression
on the nervous system.
So the brain is providing these sending predictions
of explanations or hypotheses for the sensory input
and is updating or revising its internal hypotheses
in order to account for the sensory evidence at hand.
And this, of course, is very closely related to the notion
of perception as hypothesis testing by Richard Gregory
and has been formalized and articulated by people
like Peter Dian and Geoffrey Hinton
from a Bayesian perspective,
borrowing from variational principles,
in particular variational free energy,
invented by Richard Feynman to solve his particular quantum
electrical problems using path integral approximations
to path integral problems.
So that's what we're going to...
This is a perspective we're going to pursue.
This quote is nice in the sense that it induces this notion
of a sensory impression on the nervous system,
which, of course, is very much like the sensory states
being or reflecting external influences.
So the external states are impressing themselves
on the Markov blanket or this veil
to produce the sensory impressions.
And if it is the case that the internal states
are maximizing the Bayesian model evidence,
they are implicitly inferring the causes in the outside world
that produce that.
So let's see what that might look like,
how that might be implemented in a simple brain-like organism.
So here's our basic hill climbing equation again
in its Helmholtz form.
I'm replacing the surprise with the free energy bound here,
so we've actually got a gradient descent on the free energy here
with its divergence-free and curl-free components,
so anoidal components.
I'm just rewriting this equation in a form
that might be more familiar to many of you.
It's basically a Kalman filter.
So the anoidal component now becomes a prediction
in the sense that prediction can't change the model evidence
because there's no new data,
so it doesn't change the log probability,
it just flows on the isocontrols,
whereas the update does the hill climbing
and that's just a minimization of prediction error
weighted by the precision of the prediction error
multiplied by the precision of the random fluctuations
here.
So what's prediction error?
We've heard about that a number of times.
It's basically the difference between observations
and some reference.
Here, let's assume that we have these sensory impressions
and that we were trying to predict
the cause of these sensory impressions
given some expectation mu here.
And if I had a generative model or forward model,
given these expectations of what the sensory consequences
of these expected causes were,
I can then produce a prediction in sensory data space
or sensor space.
Look at the difference and we'll call that the prediction error.
And then all we're saying is that if a system exists,
then you should be able to write it down.
So it looks as if it is always trying to minimize its prediction error,
both by changing its internal states,
which now stand in for expectations of the causes
and through its action.
Notice that there's no imperative here
to actually model the true external states.
It is just a statement that it is sufficient
to understand the dynamics in terms of minimization of prediction error.
So the actual cause can be very different from the true cause.
So that is good in the sense that we've boiled down
all the behavior to a suppression of prediction error.
And that provides a simple or intuitive perspective
on perception and action.
In the sense we can either change our brains
to make our predictions more like the sensations
or we can actually change the way that we're sampling the sensations
to make them more like the predictions,
both in the service of minimizing prediction error.
And this provides the metaphor for action and perception
that I was talking about previously.
But if this rests upon prediction errors,
then clearly I have to have a model that generates predictions
at this point that we start to get into some of the hard problems now.
So let's just think about the nature of the generative models
that actually generate data that a humanoid robot or a human
actually samples from the world.
And let's say you set your students the task
of generating foveal sampling from a synthetic retina.
Now to do that, just to generate the data,
let's not worry about inferring or inverting any model.
Just to generate the data, you're going to have to know the hidden causes,
the what and the where of the causes of these sensory data.
So you need to know what is the object being visually palpated,
what is where we're looking,
what are the kinetics of that circadic search.
And then we'd mix these two causes with a cascade
of nonlinear convolution operators,
nonlinear mappings at different hierarchical levels
to finally produce the actual sampled data.
I've just written down that deep and dynamic generative
forward model here in terms of hidden states or causes
and hidden states per se, all subject to random fluctuations,
eventually generating sensory consequences right at the bottom.
And now if we apply our hill climbing function to this
and then rewrite the message passing implicit in this gradient descent here
using the sort of the camera filter based formulation of this,
we actually get a very, very similar architecture.
We replace the random fluctuations with prediction errors,
but the expectations which now replace the hidden states
have exactly the same form, they're just cascading down,
generating predictions of the expectations at lower levels
right down to the bottom level.
Now the interesting thing here from the point of view of a neuroanatomist
is that we actually create forward or ascending connections
or message passing where the prediction errors are propagated up
or deep into the hierarchy to update or correct our expectations.
So there's a reciprocal message passing with descending predictions
and a counter stream of ascending prediction errors
all being mixed together in a relatively simple way,
although there are nonlinearities in this
because the predictions in the prediction errors
are generated by nonlinear descending influences here.
But a fast, simple and efficient way
basically is a map from consequences,
sensory consequences back to causes
in a way that is consistent with maximising Bayesian model evidence.
So just intuitively from the point of view of,
again say a visual neuroscientist,
let's just see how that might pan out
in terms of anatomy and physiology.
We have sensory input coming in here.
There are top-down predictions from the visual cortex.
They're compared to elaborator prediction error.
That prediction error is passed up to the visual hierarchy
to revise expectations about the causes
of that say local pattern of sensory input.
But these predictions or expectations themselves
are in receipt of higher predictions
which form a second level prediction error
that can be sent up the hierarchy
to revise higher and higher, more abstract,
hierarchically deep representations.
And we've heard about the importance
of sort of dimension reduction factorisation
and in the formation or in the use
of these hierarchical models
that are defined by their factorisation,
that sparsely structure,
this is where you get the dimension reduction,
the efficient representation or explanation
for sensory input.
We can tell exactly the same story
for proprioceptive input.
So proprioceptive input from stretch receptors
here in the ocular motor system
coming to the pontine nuclei
in receipt of top-down predictions
say from the frontal eye fields.
We have a prediction error that could be sent forward
to revise our beliefs about the configuration
of our ocular motor system
and provide a base optimal explanation for that.
But, and here's the important thing,
these prediction errors have another way
of suppressing themselves.
They can couple directly back to the actuators
to make the stretch correspond
to the descending proprioceptive prediction.
So they can eliminate themselves quickly
and efficiently just by coupling back to the environment.
So this is the action.
This is basically action minimising prediction error here.
Notice that the only thing that action can change
are the prediction errors at the first level,
not deep in the hierarchy.
And what I've just described there is simply a reflex arc,
nothing more, nothing less.
In this perspective,
basically the descending predictions
provide the reference for the reflexes to fulfil.
So action is in the game, again,
of changing the sampled sensations
to make them more like the predictions,
including the predictions about the motor plant.
And in fact, only in this instance about the motor plant.
So we only have the reflexes right at the bottom of the hierarchy.
So these predictions, of course, are not simple.
They are richly informed by deep hierarchical processing
and gathering together of all information
from all modalities.
So these are descending predictions
that are just in the proprioceptive domain,
but they are all internally consistent
with beliefs about the world and its history
that have benefited from the accumulation
of multimodal information.
So these are very rich descending predictions,
but their actual realisation by the motor plant is trivial.
It's just driven by the reflex arc here.
So these will be basically the reference signals
that we were hearing about earlier on.
So to sum it up before I conclude
with a couple of illustrations,
biological agents minimise their average surprise,
namely their entropy.
They do this by suppressing...
Or you can write this down
in terms of a suppression of prediction error.
And that can be reduced by either changing the predictions,
namely perception or sensations through action.
And this, certainly, the predictive processing
or Kalman filtering scheme that I've just described
entails recurrent message passing to optimised predictions,
and action reflexively makes those predictions come true
and thereby minimises surprise.
So let me just now conclude by giving you...
Working through three examples
that sort of build upon each other,
which maybe speak more to robotics than...
I've got enough time, haven't I?
Yep.
So...
No, no, no, what?
Twenty minutes.
Twenty minutes.
I was just about to accelerate there
into hyperdrive,
now more relaxed again now.
Fifteen minutes.
Fifteen minutes.
Alright, never need time for discussion.
So the first...
So I was trying to think how all the ideas
I've been hearing about this morning
fit with this scheme.
There are lots of points of contact,
but there are also points of confusion for me,
so I'm hoping that some of that will be resolved by the audience.
And often, my confusion is resolved
by going back to these simple toy examples
that self-evidently work.
So the first example was basically
how would you simulate queued reaching?
So there's some change in the environment
that queues a particular movement,
that movement is just reaching to target.
So this is actually a trivial thing to solve
from the point of view of this active inference scheme,
and just involves putting into the model
the prior beliefs that when a target changes colour,
there is an invisible spring,
or an invisible spring that is connected from the target
to the tip of the agent's finger,
becomes very stiff and pulls the finger to the target.
So in the absence of any queue change,
that string has no stiffness,
but as soon as the target changes,
the agent believes that there is an attractive force
pulling its finger towards the target.
Now, if it believes that,
and that is part of its generative model,
then it will expect to see and feel
its arm move to the target.
And action will reflexively fulfil that.
And in so doing, we'll also fulfil the visual predictions.
This is basically the sort of behaviour we get
when the target changes from green to red.
This is just a schematic of the implementation
by reflexes through action
of these descending proprioceptive predictions here.
So I think the interesting point here
is that the forward model
that implicitly resolves
the multiple degrees of freedom problem
is very simple
and it's got absolutely nothing to do with the real world.
The real world does not contain springs.
The actual model is almost heuristic
and heuristic using a very good sense.
So I think heuristics sometimes come along
with a slightly negative connotation.
But in fact, heuristics are your prize
and approximate Bayesian inference.
They are the heart of everything.
And at the very end of the final presentation,
we have this sort of hierarchical decomposition
almost, you know,
appealing to heuristics at very high level constructs.
This, I think, formally is exactly the same
as a deep hierarchical generative model
where the priors induce,
they're called empirical prizes in a hierarchical model.
They are heuristics.
They're neither right nor wrong.
And of course, once you get the action
for making them come true,
they become right,
provided they are allowable.
So if you've got the right heuristics,
they are true.
They're just private beliefs.
And in this instance,
these private beliefs
resolve the usual sort of motor control problem
because you're actually pulling, not pushing.
You don't have to solve a pushing problem.
You're solving a pulling problem
by use of this particular
and very simple prior heuristic or prior belief
that's just part of it,
a very simple part of the differential equation
that constitute the hierarchical generative model.
So this example essentially uses a point attractor.
So the equations of motion
that describe this prior belief
or this heuristic have a fixed attracting point.
The next example just takes exactly the same idea,
but with two twists.
First of all,
we're going to make the target invisible.
And now it's not going to have a point attractor.
The location of this fictive target
is now going to pursue a heteroclinic cycle.
And we've heard about central pattern generators.
I'm just writing down now a central pattern generator.
It actually has a Volterra locker form
that produces a heteroclinic cycle
that attracts the fictive location
of this point in Euclidean or extrapersonal space
to a series of unstable fixed points.
And again, it did strike me that the notion
of computing your points of contact
and then working out the dynamics
has a lot of similarity
in the way that one chunks and sequences.
But what we're doing here
was saying that this one equation,
a very simple equation,
I repeat, a locker Volterra differential equation
that has a number of unstable fixed points
that are visited in sequence.
This is just nothing more
or less than a central pattern generator.
And then there's a mapping from the location
in this abstract space to some points
in a 2D planar space that attract the finger
in exactly the same way
that the point attractor attracted the finger before.
And by just putting these points here
in an appropriate place,
I can simulate the writing
that you've just seen being traced out there.
Now, so there's an interesting simulation
from the biologist's point of view
because the activity of the units
that are encoding these prior beliefs
and generation predictions
that are fulfilled by action
do actually show a lot of characteristics
that people in neuroscience are familiar with.
So, for example, if I plot the activity
of one of these in terms of the position in space,
we get place selectivity.
Furthermore, we get place selectivity
that has direction selectivity.
It prefers the downstroke,
as opposed to the upstroke of the J here.
And another nice thing about this simulation,
it actually shows that...
Oh, it allows me to remind myself and remind you
all this behavior is being produced
by deep dynamic descending predictions
in the proprioceptive domain.
At the same time, the agent is making visual predictions
and seeing the consequences of its behavior,
and it's well-happy
because those predictions are completely fulfilled.
However, if I just reduce the gain
on these descending predictions,
and replay the same visual input,
I now have a simulation scenario
where it will be like the agent
watching somebody else write.
And yet it can use exactly the same forward model
to infer the trajectory and what is being written.
And I've simulated that here
in terms of looking at the activity of this unit here
when I've precluded movement
by switching off the gain of these descending predictions
but left these in play,
and then replayed the visual input to the agent.
And it excites exactly the same selective responses
in the same forward model.
So it's using this generative model
that has multimodal, extraceptive,
and proprioceptive predictions
in the one hand to prescribe behavior
through descending proprioceptive predictions,
but also in another context,
it can prescribe the extraceptive
or visual or auditory consequences
of another's behavior.
So it can also accumulate sensory evidence
to infer what something else
or what another robot was doing.
And of course, that's basically
the premise of the Muren neuron system
which this provides a very crude model of.
Okay, finally, what we did there
was write down explicitly
the number of the attracting locations.
What I want to do now is to show you
the last simulation
that asks the question,
well, how do we in visual neuroscience
identify the points of attraction
the next point of attraction
in this heteroclinic itinerant sampling
of our sensory space.
And the way that I'm going to choose those
is this is the second equation here.
We're going to choose those
is actually quite principled.
I won't go into the arguments now,
but basically it rests upon
the following argument.
If it is the case
that all systems that exist
in a gothic sense
can be written down
in terms of minimizing their
variational free energy.
And if it is the case that the behavior
of all systems is determined by
the empirical prize or the priors
inherent in their model
that forms the basis of that free energy,
their heuristics.
What are the only necessary heuristics
to explain the existence of a system
that doesn't go off exponentially
to plus or minus infinity?
Well, it's just that they believe
that they minimize free energy.
So if a system believes
it minimizes free energy in the future
then it will act
to fulfill that belief
and it will therefore minimize free energy
and it will therefore exist.
So what does that mean in terms of
understanding the imperatives
for planned action
or if you like, goal directed action?
I haven't got...
I'll go into this. It is a very interesting
sort of game.
If one writes out
the full expression for expected free energy
in the future, it actually entails a lot of interesting things
that subsume optimal control
and KL control.
But I'm actually taking the sort of
the interesting
bits out of the loss function
that make it the sort of loss functions that you would deal with
and just looking at what is left
and what is actually left
is the information gain
and the weight value.
The reduction in certainty I would get
if I moved like this.
So as applied to the visual domain
that simply means that we're going to
the point attractors of our eyes
or ocular motor system
are going to be those movements
that harvest or solicit
or elicit sensory samples
that reduce
my uncertainty about
the state
of the world.
I can compute that. So if I had this
hypothesis about
that this image was causing my local
sensory samples, see I can move this circle around
compute the
decrease in expected free energy
conditioned upon that control
and score it
in terms of information gain
and epistemic value and create a salience map.
I can now use the maxima
of those salience maps to drive
that itinerant searching
that we illustrated in the previous slide.
Here's the architecture.
I won't go into this.
It's not very interesting.
The more interesting thing here is the behaviour.
So here the locations
of this successive
autonomously selected
now attracting points for
successive fixations
in a movie format. These are the data
that are sampled, simulated EOGs
or electrical eye movement
recordings, the visual samples
at the end of each saccade
and this is the interesting thing.
In this simulation
this very simple agent
had beliefs that the world could either
be an upside down phase
a sideways phase
or a vertical phase
and it then sampled information
to resolve its uncertainty or confusion
about what the state of the world was
that was causing its sensory information
and that's now
shown here in terms of the
expectations for the correct explanation
of the right phase
and the incorrect explanations
and the 90% based in confidence intervals
just to show that there is
a within saccade
and between, solitary between saccade
reduction and transit inflation
of uncertainty as this agent goes
and gathers bits of information
that progresses really resolves
its uncertainty about what's going on
beyond its Markov blanket
and clearly
if any evidence
is violated or disconfirmed
its hypothesis would then move to the next
competing hypothesis
very much in
as one would in a winnerless competition
or a winner take or like situation
so
I'll conclude with that example
it would have been nice to show
a further example that spoke more directly
to robotics but I don't have one
I'm sorry
we haven't really got
that far
nor is that our expertise
but I will close with a quote
from Helmholtz that I think
again wonderfully summarizes
everything that I've just said
each movement we make by which
we alter the appearance of objects
should be thought of as an experiment
designed to test whether we would have
understood correctly the invariant
relations of the phenomena before us
that is the existence
in definite spatial relations
and with that I'd like to thank
all the people whose ideas I've been talking about
and of course thank you for your attention
thank you very much indeed
any questions
I have a question
that relates to your pointing example
that you then extended
to a
future development
so based on data
I've seen that in the community
that has been springing together
all of the examples of examples
developed in the community
that you've seen in the community
so how would you
complete that as a transition
and how would you
so then integrate
in the community
and not have to try to complete
that
that was a very nice question
I think it speaks to the progression
from that fixed point
to this
this would feel like we did one step up
from a fixed point
so if I had
thought that was a presentation before
the slides
it would be nice to progress from a fixed point
to a fixed point
to a fixed point
to a fixed point
to a fixed point
and this is actually
a very good point
so you would actually go
in the same direction
in terms of complexity
if I was writing down a really interesting
simulation I would have done this
but if I
thought about
what an interesting
hierarchy model
would look like
very
very slow
etiquette channels
at the top
that would have
transfer sequences
of etiquette channels
that would ultimately carry down
to the inside of this
so we have all the
texturized in any sequences
of selecting a mechanism
within walking
with the exception of things like
the U.S. and Canada irons
we have microstate clouds in them
so I think we would have
a collection of different
mathematical chapter sets
all contextualizing each other
and just from experience in other
simulations
what tends to happen is that when you write down
the heuristics and you write down
the prize in terms of differential equations
the states of the higher level
of capital
10 collective control parameters
on the lower level
but it did remind me very much
of the coefficients that we're hearing about
remember
there was coefficients times
a function
that was affected by the differential
reference property
versus
a sample
so it did strike me that's very much like
we are forced to write it down
then we might be down
so I would imagine
that dynamics that have a tracking set
is possible in all forms
if we write it down
but increasingly still in time experience
where the higher level
the prize in terms of the lower level
we should get more sequences
of sequences of sequences
than what's in this chapter
is that all we have in mind?
Thank you
Thank you
Yeah, excellent question
so
I sort of skipped over that
but there will be a whole
I think
community of people who could come
and lecture on the importance of
variational free energy
in approximate Bayesian inference
that is exactly to finesse that problem
so if I understand your problem
sorry the problem
you're highlighting
if it were the case that one could actually
evaluate p of s
if you could actually estimate
surprise or value
then in principle you could just work out
with respect to control variables
and with respect to internal states
and then just write down
the differential equations
a very adaptive agent
that could be
and one should add
that every aspect including the parameters
would have to minimise free energy
and if you write that down
that becomes associative learning and somatic time
it then becomes natural selection
or Bayesian model selection in evolutionary time
but everything minimises
log energy, that log surprise
but of course you can't measure it
and as I understand it
this is exactly the problem
that Richard Feynman confronted
when he was trying to solve
the pathological problem
to work out the probability
of trajectories
of small particles
electrons
and it can't be analytically solved
and it certainly can't be done possibly
over evolutionary time with some
sampling technique
but in real time that can't be done
hence the free energy
so the free energy provides an analytic bound
that is really easy to measure
but it renders
now your optimal
dynamic
no longer is it exact
Bayesian inference, it now becomes known as
approximate Bayesian inference
so the whole point of the free energy principle
as opposed to just maximising
Bayesian model evidence
maximising
minimising surprise
is that it allows exactly for that
it provides a tractable
finessing of the problem
that you're talking about
so everything I'm talking about is approximate
Bayesian inference
which lends another aspect I guess
to the use of the word heuristics here
so the free energy
which is always by definition
by Gibbs inequality
bigger than the
the log of the probability itself
now can be written down in this form
so
and then it has a flavour and a feel
where suddenly you feel comfortable
because you know and talk about likelihood models
we know and talk about prize
if you're comfortable with hierarchical models
or deep models you will also be comfortable
with the notion of parametric empirical
Bayes and
empirical prize
so these are the things that
you are forced to do when doing modelling
because we cannot measure
usually to non-linear models
the actual thing we want to approximate
which is the probability of s
so probability of s given m does not appear in this equation
but what we now have to do
is to write down the prize
and the likelihood explicitly
and notice that the entropy term is on the end here
so this conforms to James's maximum entropy principle
so we try to maximise
basically the accuracy
of our model
but at the same time maximising
the entropy
or the uncertainty in our explanations
slightly paradoxical but this is what leads to
the generalisation
and the simplicity
and the complexity
of the approximate Bayesian solutions
and again I was thinking
about your presentations
you know
part of that simplicity
I'm not sure it's an appalling abuse of the word
but the minimisation of the complexity
that you would get
from including this entropy term
tells you
or usually makes it the case
that sparse models
with nice sparse hierarchical structures
with low dimensions
as their sort of explanations
for high dimensional data
have much more
evidence
or more exactly lower free energy
than models that don't have that low dimensionality
so you can actually write this equation
out in a number of different ways
and that complexity becomes a KL divergence
between the posterity and the prior
and that the determinant of that complexity
which is all part of this
completely standard free energy
and free energy, variable Bayesian
ensemble learning formalism
you get for free when you
use low dimensional
hierarchical models of your data
so I think it's a principal drive to get into those
good heuristics that have that low dimensionality
with that hierarchical temporal structure
that we've been talking about
it's actually mandated by the very
nature of the problem that we're trying to solve
or trying to explain
is that okay?
I actually have a technical question
so this
relationship between
Bayesian inference and optimal control
that you pointed out
with all the idea going back to Kauman
and in his world
in the linear world there are one-to-one
just like you were describing
but later there was work by Mitter and Fleming
in the non-linear case
and then I did something and BirdCup did something
relating it to patented growth
and collectively we figured out that control
is actually a much bigger problem
than Bayesian inference
specifically what that means is that
there is a subclass of stochastic optimal control problems
that map one-to-one to Bayesian inference
but there are infinitely many other stochastic
optimal control problems that do not have
an inference analog
and the reason is very simple
actually if you look at Bayesian and the Fokker-Plank
and the Kushner equation they're all linear
the Hamilton-Chicovi-Belman equation
is non-linear
there is a special case where it becomes linear
under logarithmic transformation
but it's not always the case
so I'm wondering you must have made an assumption
somewhere along the way that pruned a very large
class of control problems
that didn't follow all the map
see where the pruning was
I'm guessing you started by saying that p dot is 0
you kind of assumed some kind of statistical equilibrium
in the world
which doesn't have to be true
could that be the
place where you actually ignored a lot of control
right
there are two great questions
but the very last few words were a completely different question
no
it's the same question
because a lot of the stuff
a lot of the things that I showed
they cannot be cast as Bayesian
right
I was choosing the first one
the p dot 0 is more
just a statement of
the conditions that we need to comply with
so I have to say
non-equilibrium steady state
so we're not talking about equilibrium
we're talking about systems that move
and change and fluctuate
that doesn't have to be true
why do you have to assume a steady state
in this case?
by definitions
if I want to talk about
systems that have attracting sets
that have measurable characteristics
over extended periods of time
basically they don't decay or dissolve
that assumption was really to motivate
the variation
formulation of
the hill climbing on surprise
that was not a pragmatic constraint
on the
on the solutions
that I used to illustrate the ideas
it's really a more fundamental
motivation for where
you get that fundamental hill climbing
from in the first place
but it does not entail stationarity
it does not entail
in either a wide sense
or a weak sense
it does not entail thermodynamic
or any other form of equilibrium
it's steady state in the sense that p dot 0
is equal to 0
I think the more interesting question
I don't know
clearly I haven't gone through
the
formalism that would map
specifying the problem in terms of
loss functions
and how that translates into
Bayesian inference
however the simple answer to that is
that all the loss functions that are implicit
in the illustrations here
and elsewhere are all written down
as log prize
so as long as you can write down
your loss function in terms
of a prior preference
or a prior energy in the log of the prior belief
then by definition
it becomes a Bayesian inference problem
so the question I think then
for all non-linear and
dynamical models
so that's one of the
conditions for doing this
setting up this duality
is that any energy cost has to be
a KL divergence between some passive
and some active dynamics
but there is another important requirement
which is that any noise that comes
into a system must live
in your control space
in other words if anything happened by accident
you should have been able to do it yourself
and that's the thing that's actually a very strong assumption
so for example if I'm a robot I'm kind of shaking
due to noise
in the same way we can cast that as Bayesian inference
but if somebody came and pushed me
in a way that I couldn't have pushed
myself all of a sudden that's not a
Bayesian inference problem
it has to do with the structure of the noise
and not just with the priors and the loss functions
I mean I'm going to have to defer to you
technically
you know more about having been doing this
I would say that this pushing
in the sense
when we get new visual information
we are being pushed
by a sensory and of course
the whole point of bitcalment filtering
and the generalizations
of that
of course this is not camel filtering
this is beyond extended camel filtering
in fact it's in generalized coordinates of motion
so it's generalized Bayesian filtering
but that pushing is of course just
from the sensory perspective
exactly the problem
that generalized Bayesian filtering
contends with and resolves
and let me pose the question back to you
how is pushing
the retina with visual information
any different from pushing my proprioception
by a force
to my body
my question is a bit naive
but if I understood well
your internal states
is like the brain, the state of the brain
and the external state is the outside
world of the brain right
but the dynamics
includes all states
so the attractors for example
contains also the external state
the internal state in between
well the internal states
are actually just modeling the external states
not themselves
say it again
the internal states are understood
as modeling the external states
but not themselves
I suppose that the dynamic model
was including all states
and that the dynamics was including
with all states right
all states in the outside world
but not the states that are doing the modeling
that I didn't get that
well does it matter for your question
because
I didn't see exactly the equation that you had
why the Markov-Blanket
was playing
I didn't see the Markov-Blanket
assumption
was really coming into the equations
I see
so the leap there
I apologize for rushing over that
that's actually a very non-trivial
it's basically this thing here
isn't it
I think everybody would be
fairly comfortable
or they would after half an hour with a matlab
or a pen and paper
they'd be fairly comfortable with this
and it's really
why is this still true
this basically looks as if
it has eliminated the external states
the S-bar
has the sensory active and internal states
but not the external states
and yet
this log probability
depends upon the external states
so that's the clever bit
that's the non-trivial aspect of this result
so this result still holds true
when you have to go through and actually do the integration
to show it in a non-trivial way
so this has the same form as the previous
equation
I apologize
so fx is a
curly gradient descent
on this thing here where x comprises everything
this is also true
but this probability
distribution here
depends upon the external states
so that's the clever bit
in the sense that it's as if
the internal states
and the active states
the only clever bit
it's as if the internal states
somehow
know about the external states
they know where they are
in the state space of external states
and yet they can't
because that's beyond the Markov blanket
but the very
existence of non-equilibrium
state of states means that this has to be true
and that's basically what gives these
what
it is
what gives these
systems
or certainly a focus
on the internal states of the system
and the Markov blanket
the look and feel of adaptive behavior
the look and feel
of self-organization
homeostasis and the look and feel
of a little Bayesian agent
thank you very much again
and my question is
more nave
than the previous
the person
you have
the very beautiful
explanation based on
the probability
principle
and my question is
how
the biological neural network
is implementing
the probability
reasoning in the network
is it the
recruitment principle
or any other
knowledge
about how the neural
system implements
this computation
well I think that's an open question
and as open as
the best scheme that one would adopt
in the robotics context
so what you're asking is what is
the process
theory that complies
with the
with the principle
I mean the principle in of itself is almost
uninteresting because it's tautologically true
I think the hard problem is really
first of all
the form of the generative models that you're dealing with
and the actual scheme
that is used to do the
variation
free energy minimization
which could be selection using Bayesian model selection
it could be
and I concur with
the enthusiasm for Gauss-Newton schemes
certainly all our simulations
essentially use a Gauss-Newton scheme
why well because
under the particular form
of approximate Bayesian inference which is not exact
given by assuming the
posterior is always Gaussian
you can always write it down as
in terms of minimizing prediction error
when you can do that you can move from
a Newton to a Gauss-Newton scheme
with all the computational savings involved
so you don't have to explicitly
evaluate the curvature
or the Hessian
and that rests upon the fact that you've made this
a plus assumption
in defining the variation of free energy
so that's part of the approximation here
masses of computational savings all sorts of wonderful things
just drop out for free
under that plus assumption
so we like
we in terms of
neuroscientists and people with very small
computers using MATLAB
like the
predicted coding formulation
or Kalman filtering
apply a Kalman gain to a prediction error
and
that
is very understandable in terms of
again I haven't got the right graphics here
but it's very understandable in terms of
the sorts of message passing you see
in the real brain
so these actually are usually thought to be equivalent
to superficial parameters of cells that live
about a millimeter from the cortical surface
and the expectations are thought
to be encoded by deep primal cells
which live about four millimeters
in the deeper layers of the cortex
the visual cortex has exactly
this hierarchical structure
it has exactly the right
sort of laminar specificity
layer specificity deep and superficial
it also has the right functional asymmetries
remember before I said
there are non-linearities in this but these are in
the generation of a prediction error
through the non-linear forward model
which means that these descending predictions are non-linear
whereas these are linear and driving
as in the Kalman filter
so I completely agree that
in the macro model
it is
well investigated in neuroscience
or brain science
but in the very micro level
the
mathematical model of the neurons
and the large cluster of neurons
I see
do we have
or do you have
the way to implement
the statistical reasoning
into those
microscopic model of neurons
no
so these predicted coding formulations
would normally be
would normally associate
continuous time
representations of
sufficient statistics
of the posterior
activity of neurons
so in that
mean field of approximation
to large numbers of neurons
there is a body of work
championed by people that seem to be over the line
but
they'll actually get into
seeing and spike this passing
and associate with that with
Bayesian belief update or belief propagation
I think the formal
distinction
is between predicted coding
and Bayesian filtering schemes
that deal with continuous states
and continuous time
whereas most of the microscopic models
actually look at the neurochemistry
of post-synaptic responses
they only work for states based models
discrete states
you're in state 1 and K
with very very large vectors
so there's a formal distinction
then you're outside the Bayesian filtering predictive coding
so you've got your choice
so there is no more question
go to lunch now
and we want to thank the speaker
again
back in the room at
145

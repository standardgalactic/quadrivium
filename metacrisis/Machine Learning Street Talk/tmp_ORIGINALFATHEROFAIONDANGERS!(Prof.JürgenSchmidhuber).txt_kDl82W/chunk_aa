my fondest memory. It's usually when I discover something that I think nobody has seen before,
but that happens very rarely because most of the things you think of somebody else has done before.
This episode is sponsored by Numeri. Are you a data scientist looking to make a real-world impact
with your skills? Do you love competing against the best minds in the world? Well, introducing
Numeri, the revolutionary cutting-edge AI-driven hedge fund that's changing the game for good.
Numeri combines a competitive data science tournament with powerful, clean stock market data
enabling you to predict the market like never before. Sign up now, become part of the elite
community, taking the stock market by storm and I'll see you on the leaderboard.
Wonderful. So today is a momentous occasion. What an episode of MLST we're going to have.
We're joined not by a godfather of AI, but the father of AI, you again, Schmidhuber,
the researcher responsible for leading the research groups which invented much of the technology
which has powered the deep learning revolution. It's long been a dream to get you on the podcast,
you again. It feels like the day has finally arrived, so welcome to MLST.
Thank you, Tim, for these very kind words and this very generous introduction.
So on that, let's discuss the credit assignment problem in machine learning. Now,
you've dedicated a significant amount of time researching and publishing the actual history
of the field and there's a significant divergence between the public narrative
and what actually happened. And amazingly, no one has pointed out any factual inaccuracies in your
accounts, but the incorrect perceptions still persevere. Now, I particularly enjoyed reading
your history of the breakthroughs in machine learning, going back to ancient times and of course even
remarking on the very first computer scientist, Leibniz. And for example, you pointed out the
history of who invented backprop and the CNN. And you explained that there wasn't really
a neural network winter at all in the 1970s. So could you just sketch out a little bit of that
history? So that's a challenge. Actually, computer science history and computing history started
maybe 2000 years ago when Heron of Alexandria built the first program-controlled machine.
That was 2000 years ago in the first century basically. And he basically built an automaton
that was programmed through a cable which was wrapped around a rotating cylinder which had
certain knobs and then there was a weight which pulled it down and the whole apparatus
was able to direct the movements of little robots, of little puppets in an automatic theater.
That, as far as I know, was the first program-controlled machine in the history of mankind.
Even before that there were other machines. The ancient Greeks had even earlier the
Antiqueterre mechanism which was kind of a clock, an astronomical clock. But then more recently
we have seen many additional advances and you mentioned Leibniz, of course, who is of
special interest to our field because he not only is called the first computer scientist
because he had the first machine with a memory that was in the 1680s, I think. He not only had the
first machine that could do all the basic arithmetic operations which are addition,
multiplication, division and subtraction, then he not only had these first ideas for a universal
problem solver that would solve all kinds of questions, even philosophical questions,
just through computation. And he not only was the first who had this algebra of thought which
is deductively equivalent to the much later Boolean algebra. In many ways he was a pioneer,
but especially in our field in deep learning he contributed something essential, which is really
central for this field, which is the chain rule. I think 1676, that's when he published that and
that's what is now being used to train very deep artificial neural networks and also shallow
neural networks and recurrent neural networks. And everything that we are using in modern AI
is really in many ways depending on that early work. But then of course there was so much additional
work. The first neural networks, as we know them, they came up about around 1800. That's when Gauss
and Legendre had the linear neural networks, the linear perceptrons in the sense that they were
linear without having any non-differential aspect to it. So these first neural networks,
back then, were called method of least squares. And the training method was regression and the
error function was exactly the same that we use today. And it was basically just a network with
a set of inputs and a set of outputs and a linear mapping from the inputs to the outputs. And you
could learn to adjust the weights of these connections. So that was the first linear neural
network and many additional later developments led to what we have today. You had this beautiful
statement. You said that machine learning is the science of credit assignment and we should apply
that same science to the field itself. And I guess what I'm really curious about is
first, if you could educate our listeners just a bit on what credit assignment is in the context
of, say, machine learning and why you think it's important that that should apply to the field
in general. You know, why should we care about credit assignment? Why should we study the history
of the developments in the field? Why is it important? I'm interested in credit assignment,
not only in machine learning, but also in the history of machine learning,
because machine learning itself is the science of credit assignment. What does that mean? Suppose
you have a complicated machine, which is influencing the world in a way that leads to the solution
of a problem. And maybe the machine solves the problem. But then the big question is,
which of the components of these many components were responsible? Some of them were active
a long time ago and others later and early actions set the stage for later actions. Now,
if you want to improve the performance of the machine, you should figure out how
did the components contribute to the overall success. And this is what credit assignment is
about. And in machine learning in general, we have a system consisting of many
machine learning engineers and mathematicians and hardware builders and all kinds of people.
And there you also would like to figure out which parts of the system are responsible for later
successes. Yeah, and it's a brilliant point. And I completely agree with you, by the way.
And I think the way I think about it is you've got this giant architecture of humanity and in it
are these certain nodes that may be an individual, maybe a research group. And if they come up with
things that are very helpful, right, you want to try and direct more attention, more resources,
at that nodule, at that node, right, because it's likely to come up with additional very
important things. And if we don't get that right, we're just not optimizing the algorithm of science
as a whole. That's right, yes. Machine learning and science in general is based on this principle
of credit assignment where credit usually doesn't come in form of money, sometimes also in form
of money, but in form of reputation. And then the whole system is set up such that you create
an incentive for people who have worked on improving some method to credit those who
maybe came up with the original method and to just have these chains of credit assignment
that make clear who did what, when, because the whole system is based on this incentive.
And yes, those who are then credited with certain valuable contributions, they also can get
reasonable jobs within the economy and so on. But that's more like the secondary
consequence of the basic principle. And that's why all PhD advisors
teach their PhD students to be meticulous when it comes to credit assignment to past work.
So one last question, if I may, I've really enjoyed studying the history of advancement
because I found that when I go back and read original source materials, let's say
Einstein's first paper on diffusion or anything like that, because they're breaking new ground,
they're considering a wider array of possibilities. And then over time, the field becomes more and
more focused on a narrower avenue of that. And you can go back and look at the original work
and actually gain a lot of inspiration for alternative approaches or alternative
considerations. So in a sense, it's kind of in the sense of forgetting is as important as learning.
Sometimes we need to go back to go down a different branch of the tree, if you will,
and expand the breadth of the search a little bit. I'm curious if you've noticed that same phenomenon.
Yes, science in general is about failure. And 99% of all scientific activity is about
creating failures. But then you learn from these
failures and you do backtracking. And you go back to a previous decision point where you maybe
made the wrong decision and pursued the wrong avenue. But now you have a branching point and
you pursue an alternative. And in a field that is rapidly moving forward, you don't go back very
far usually. You just go back to a recent paper which came out five months ago. And maybe you
have a little improvement there. And then maybe there's yet another little improvement there.
And some parts of our field are at the moment a little bit like that,
where PhD students are moving in, who just look at the most recent papers and then find a way of
improving it a little bit and 2% better results on this particular benchmark. And then the same guys
are also reviewing at major conferences, papers by similar students and so on. And so then sometimes
what happens is that no very deep backtracking is happening, just because the actors aren't really
aware of the entire search tree that has already been explored in the past.
On the other hand, science has this way of healing itself. And since you can gain reputation by
identifying maybe more relevant points, branching points, you have this incentive within the whole
system to improve things as much as you can, sometimes by going back much further.
So there's been a lot of discussion in the discourse around this concept of AI existential
risk. And you again, you've published quite a few pieces about this recently, prominently in
The Guardian and in Forbes actually. And one of the things I wanted to focus on is this concept
of recursive self-improvement, because that seems to be one of the plausible explanations that these
folks give. And of course, when it comes to recursive self-improvement, you are an expert in
this field. I mean, Godel machines come to mind immediately. So I want to kind of explore asymptotes
and limitations. This whole idea of recursive self-improvement is very sexy, isn't it?
In fact, it is the one idea that motivated me to do all of this. So my first paper ever in 1987,
that was my diploma thesis. And it was about this recursive self-improvement thing. So it was about
machine that learns something in a domain. But not only that, it also learns on top of that to
learn a better learning algorithm based on experience and the lower level domains. And then
also recursively learns to improve the way it improves the way it learns. And then also recursively
learns to improve the way it improves the way it improves the way it learns. And yeah, I called that
meta-learning. And back then, I had this hierarchy with, in principle, infinite self-improvement
in the recursive way, although it is always limited by the limited time that you run the system like
that. And then, of course, the motivation behind that is that you don't want to have an artificial
system that is stuck always with the same old human-designed learning algorithm. No, you want
something that improves that learning algorithm without any limitations, except for the limitations
of physics and computability. And so much of what I have been doing since then is really about that.
Self-improvement in different settings where you have, on the one hand, reinforcement learning
systems that learn in an environment to better interact and better create ways of learning
from these interactions to learn faster and to learn to improve the way of learning faster,
and so on. And then also gradient-based systems, artificial neural networks, that learn through
gradient descent, which is a pre-wired human-designed learning algorithm, to come up with a better
learning algorithm that works better in a given set of environments than the original human-designed
one. And yeah, that started around 1992 neural networks that learned to run their own learning
algorithms on the recurrent network themselves. So you have a network which has standard connections
and input units and output units, but then you have these special output units which are used to
address connections within the system, within this recurrent network, and they can read and
write them. And suddenly, because it's a recurrent network and therefore it is a general-purpose
computer, suddenly you can run arbitrary algorithms on this recurrent network, including arbitrary
learning algorithms that translate incoming signals, not only the input signals, but also the
evaluation signals like reinforcement signals or error signals into weight changes, fast weight
changes, where the weight changes are not dictated any longer through this gradient descent method,
but no, now the network itself is learning to do that. But the initial weight matrix is still
learned through gradient descent, which is propagating through all these self-referential
dynamics in a way that improves the learning algorithm running on the network itself. That
was 1992, and back then, compute was really, really slow, it was a million times more expensive
than today, and you couldn't do much with it. But now, in recent works, all of that is working
out really nicely and has become popular, and we have, just if you look at the past few years,
a whole series of papers just on that. So that's the fast weight programming that you're referring
to? Yes, so it's fast weight programmers where you have a part of the network that
learns to quickly reprogram another part of the network, or the original version of that was
actually two networks, so one is a slow network, and then there's another one, a fast network,
and the slow network learns to generate weight changes for the second network,
and the program of the second network are its weights. So the weight matrix of the second
network, that is the program of the second network, and the first one, what does it do? It
generates outputs, it learns to generate outputs that cause weight changes in the second network,
and these weight changes are being applied to patterns, to input patterns, to queries, for
example, and then the first network essentially learns to program the second network, and essentially
the first network has a learning algorithm for the second network, and the first system of that
kind, 1991, that was really based on on keys and values, so the first network learns to program
the second network by giving it keys and values, and it says now take second network, take this
key and this value, and associate both of them through an outer product, which just means that
those units are strongly active, they get connected through stronger connections, and
the mathematical way of describing that is the outer product between key and value.
So that's how the first network would program the second network, and the important thing was that
the first network had to invent good keys and good values, depending on the context of the input
stream coming in, so it used the context to generate what is today called an attention mapping,
which is then being applied to queries, and this was a first step right before the most general
next step, which is then really about learning a learning algorithm running on the network itself
for the weights of the network itself.
Could I press you a tiny bit on this concept of meta-learning and convergence and asymptotes?
Now one of the reasons I think why the X-Risk people believe that it will just go on forever
is they believe in this idea of a pure intelligence, one that doesn't have physical limitations in
the real world, and I'm quite amenable to this ecological idea of intelligence that it does,
the world is a computer basically as well as the actual brain that we're building,
so surely it must hit some kind of asymptote. Do you have any intuition on what those limitations
would be? So you are talking about the ongoing acceleration of computing power and limitations
thereof, is that what you have in mind here? Well that's one part of it, so even if you
just scale transformers I think there would be some kind of asymptote, but we're talking here
about meta-learning, learning to learn, how to learn, and recursive self-improvement, and it's
similar to this idea of reflection, self-reflection and language models, it actually improves the
performance with successive steps of reflection and then it levels off, it reaches an asymptote.
I just believe that there are asymptotes everywhere and that's the reason why I
don't think recursive self-improvement will go on forever, but I just wondered if you had
any intuitions on what those impressions are. Yeah, you are totally right, there are certain
algorithms that we have discovered in past decades which are already optimal in a way
such that you cannot really improve them any further, and no self-improvement and no fancy
machine will ever be able to further improve them. There are certain sorting algorithms that
under given limitations are optimal and you can further improve them. That's one of the limits.
Then of course there are the fundamental limitations of what's computable, first identified by
Kurt GÃ¶del in 1931, he just showed that there are certain things that no computational process
can ever achieve. No computational theorem prover can prove or disprove certain theorems
in a language, in a symbolic language that is powerful enough to encode
certain simple principles of arithmetic and stuff like that. What he showed was that
there are fundamental limitations to all of computation and therefore there are fundamental
limitations to any AI based on computation. I'm glad you brought that topic up because it's one of
our favorite things to discuss which is do you think the human mind ultimately reduces to just

So, welcome back to MLST.
Today, we're going to be talking about this book by Dr. Thomas Parr, Giovanni Pazzullo,
and Professor Carl Friston.
Now the book is Active Inference, the free energy principle in mind, brain, and behavior.
So the book, from a pedagogical perspective, it's describing active inference from the
high road and the low road.
And the high road is a little bit kind of helicopter view, so it's saying, OK, we've
got these biological organisms or these living systems, and what do they do in order to be
living systems?
Well, they resist entropic forces acting on them by minimizing their free energy.
So it goes into the how question, but it also goes into the why question from a helicopter
view.
The low road of active inference is far more mechanistic, far more mathematical, and obviously
both all of the roads lead to Rome, if you like.
But the low road is talking about things like Bayesian mechanics, there's a primer
on probability theory, talking about things like variational inference, which is the way
that we solve these intractable optimization problems in active inference, and also talking
about framing active inference as a process theory, which is the latest incarnation of
the description of active inference.
So Professor Carl Friston wrote a preface for the book.
He said, active inference is a way of understanding sentient behavior.
The very fact that you are reading these lines means that you are engaging in active inference,
namely actively sampling the world in a particular way, because you believe you will learn something.
You are palpating.
This is beautiful, by the way.
Friston uses the most beautiful language, it's his signature, if you like, it's his
calling card.
He said, you are palpating this page with your eyes, simply because this is the kind
of action that will resolve uncertainty about what you're going to do next, indeed what
these words convey.
In short, he said, active inference puts action into perception, whereby perception is treated
as perceptual inference or hypothesis testing.
Active inference goes even further and considers planning as inference, that is inferring what
you're going to do next to resolve uncertainty about your lived world.
So I'm about to show you a sneaky clip of Professor Friston that we filmed in January.
I might publish the full show on MLSD in the future, but I just want to take this as an
opportunity to thank you so much for all of our Patreon supporters.
Honestly, it means so much to me because the last few months I've just been, you know,
trying to make this activity of mine, this passion of mine, a full-time job.
And it's not just because you love the show and you want to support me, you get early
access to content, you can join our private Patreon Discord.
We have bi-weekly calls where we, you know, talk about all sorts of random stuff and you
also get early access to lots of our content.
So, you know, please check that out.
But in the meantime, here's a little sneaky clip from Professor Friston.
The neural network is a generative model of the way in which its content work was generated.
And its only job is effectively to learn to be a good model of the content that it has
to assimilate.
If you put agency into the mix, you get to active inference.
And now that we've got a generative model that now has to decide which data to go and
solicit.
And that's actually quite a key move and also quite a thematic move.
So we're moving from perception machines.
We're moving from sort of neural networks in the service of, say, face recognition into
a much more natural science problem of how would you then choose which data in a smart
way you go and solicit in order to build the best models of the causes of the data that
you are in charge of gathering.
Dr. Thomas Parr is a postdoctoral scholar at the Wellcome Centre for Human Neuroimaging
at the Queen Square Institute of Neurology at University College London and a practice
in clinician.
Now, one of the reviews from the book was from Andy Clark.
He said, it should have been impossible.
A unified theory of life and mind laid out in 10 elegant chapters spanning the conceptual
landscape from the formal schemas and some of the neurobiology and then garnished with
practical recipes for active model design.
Philosophically astute and scientifically compelling, this book is essential reading
for anyone interested in minds, brains and action.
Well, I mean, thank you very much for having me on.
So I'm Thomas Parr.
I'm both a clinician and a theoretical neuroscientist.
So I've been working in active inference for a number of years now since I did my PhD
back in 2016 with Carl at the theoretical neurobiology group at Queen Square.
And I'm now based at Oxford where I split my time between research and clinical practice.
So tell me about the first time you met Carl.
The first time I met Carl, I was considering, so I was a medical student at the time at
UCL and I was considering doing a PhD.
And I remember arranging to meet with him and obviously being a relatively nerve-wracking
experience meeting one of the most famous neuroscientists in the world.
I remember discussing with him about it and saying, you know, this is what I'm interested
in.
Would you consider supervising my PhD if I were to get the funding for it?
And I remember he said, yes, all right, then, anything else.
And I asked, do you want to see my CV or anything like that?
And he said, no, I'll only forget it.
Yes.
That was my first encounter with Carl.
But since then, he's always been immensely supportive and has been, you know, exactly
the sort of mentor that I think anybody would want to be able to develop a skill set and
sort of proceed in science.
I come from a machine learning background.
And since discovering active inference and Carl's work, it's really broadened my horizons.
And at the moment, there's an obsession with things like chat GPT.
And I just wondered in your own articulation, how would you kind of pose the work that you
do in relation to that kind of technology?
It's a good question.
And I suppose there are many levels at which it could be answered, aren't there?
I guess thinking about something like chat GPT in that style of technology, it's clearly
been very, very effective at what it does.
But it's worth thinking about what is it that it does?
And I think chat GPT is an excellent example because so many people are familiar with it.
It has such impressive results in terms of being able to simulate very effectively what
it's like to have a conversation.
But ultimately, it is like most deep learning architectures.
It's a form of function approximation.
It's a form of being able to capture very well the output that would be expected under
some set of conditions given some input.
So you give it some text and it knows which text to predict.
And it's very good at that.
But in a sense, that's where it stops.
It doesn't necessarily do anything else.
That's very different to what you and I do when we engage with the world around us, when
we want to learn about the world around us, when we want to form our own beliefs about
what's going on.
And those are the things that I think it doesn't have in the same way.
It certainly can't act and go and seek out specific exchanges, specific conversations
that it might want to learn from.
Whereas you or I might do that if we wanted to know about something specifically, we'd
go and look for information about that thing.
And I think that's where active inference and the idea of having a generative world
model and understanding of what's there in your world that you can alter yourself, that
you can change is very different to a lot of more passive artificial intelligence.
Probably the point where things become closer is in fields like robotics, where you have
to account for both of those things.
You have to model a world that has yourself in it, where your actions affect the data
that you get in.
And I think that's probably where more of the convergence is likely to happen.
Yes.
So you're describing the difference, I guess, between an observational system and an interactive
system.
So in an interactive system, an agent can seek information and change or bend the environment
to suit its will.
Just to linger on this for a second, though, there are folks who do argue that neural networks
are more than hash tables, because I think of them the same way you do.
They essentially learn a function.
And if you densely sample it enough, just like a hash table, it can go and retrieve what
that function says given a certain input.
But there are folks who say, no, no, no, these models learn a world model.
So is given as an example, or with SORA, they say it's learned Navier stokes.
It's a really good question.
And I think there are some open questions here, and I wouldn't claim to have all the answers
to this one.
I think to be able, again, to take chat GPT, to be able to give the answer it does, clearly
it has captured something about the statistics of language.
It's uncovered something about the hidden causes.
So you could argue there is potentially an element of world modeling in there that is
left implicit.
I think it would be very difficult to pull that out or to sort of see that with any transparency
with something like chat GPT.
And so if it does have something of that sort, probably it's the methods that neuroscientists
have been using for years to understand the brain that might help to try and pull out
those same things in those sorts of architectures.
Maybe some sorts of deep learning and neural network models are very good at picking up
regularities in terms of dynamics as well and being able to predict trajectories.
And I think it's important to say that describing something as a function approximator is not
to criticize or belittle it.
It's a very important thing to be able to do.
And it may also be very important in certain types of inference.
So for instance, things like variational autoencoders are based upon often deep learning
neural network architectures.
But the function that is learned is the one that maps from the data I've got coming in
to the posterior beliefs or the parameters of the posterior beliefs that I would arrive
at were I to perform inference of the sort we might do in active inference.
So you've written an absolutely beautiful book on active inference.
And active inference, in my view, it's a theory of agency, which is to say it describes what
an agent does.
And I'm fascinated by agency.
But could you just start by, I mean, from your perspective, could you introduce the
book and tell us about your experience writing it?
Of course.
So the active inference book that we've written is a collaboration between myself, Giovanni
Pazzullo is based in Rome and Carl Friston, who has to take credit for development of
active inference in the first place.
And the book sort of rose out of our sense that there wasn't a unified book out there
or a resource out there to help people learn about what is ultimately a very interdisciplinary
field.
And so we've all had experience with students coming to us asking for resources, asking
what they need to read.
And it may be we refer them to a little bit of neuroscience work, a little bit of machine
learning, textbooks or specific pages on variational inference or whatever else, giving
people introductions to or places they can learn about the maths they need to be able
to do it.
But then also the biology, the underlying psychology, the long sort of tradition of
previous scientists who worked in related areas.
And so the book was an attempt to try and provide a place that people could find all
of that, or at least references to all the relevant things they needed for that, to stick
to the same sort of notation, which is one of the things that's often very difficult,
and the same formalisms and try and introduce everything in a very systematic way to people.
So I'm pleased here that you found it useful, and I hope other people will as well.
The experience of writing it, I mean, so that took place over several years, partly because
the pandemic got in the way in the middle.
So Giovanni and I were passing notes between one another over email and weren't able to
sort of meet in person to discuss it during that time period.
But I think we're all quite proud of the result that we've got out of that, and people seem
to have responded quite well to it.
You start off by talking about what you call a high road and a low road to active inference.
Can you sketch that out?
Yes, and I think this was one of Giovanni's very nice ideas about how to introduce it,
because as I say, it's very multidisciplinary.
There are lots of ways into active inference, and one of the things that's most difficult
for people who are getting into the field for the first time is knowing where to start.
Do they start dealing with the Bayesian brain, unconscious inference, and Helmholtzian ideas
like that, or do they start from a physics-based perspective and start working their way towards
something that looks like sentience?
And there are lots of different, lots of alternative ways people get into it.
The fact that you become interested via machine learning, the fact that other people
have become interested through biology, I developed an interest through neuroscience
while I was at medical school.
And the high road and the low road was a way of just trying to acknowledge
that difference or that difficulty of knowing where to begin, and saying that that's okay,
there are lots of different roads, but they ultimately end up in the same place.
The idea of the low road was to say, well, let's just take observations and psychology
sort of development of a number of ideas that are built up over time
that come to the idea that we're using internal models to explain our world,
that the brain is using something like Bayesian inference, or at least can be described as using
Bayesian inference, and go from there through the advances that lead you to active inference,
the idea that it's not just a passive process that you're also inferring what I'm going to do.
And furthermore, that when we're doing inference, we're changing our beliefs
to reflect what's in the world around us and to explain our sensory data.
But actually, when we're acting in the world, we can also change the world itself
to better comply with our beliefs. So it's that move from purely changing our beliefs to reflect the
world to also changing the world to reflect our beliefs. And that fascinating move that
actually both can be seen as optimization of exactly the same objective that they have the
same goal, that in both cases, it's really just improving the fit between us and our world.
So that's the sort of low road perspective. The high road perspective was the idea of saying,
well, let's start from the minimum number of assumptions we can, let's start from first
principles. And that takes a much more physics based approach. It says, if you have a creature
that is interacting with its world, then there are a number of things you've already committed to,
and that includes things like the persistence of that creature over a reasonable length of time,
the maintenance of a boundary between that creature and its world, and that sort of
self world distinction. And once you've committed to those things, you can then start to write down
the constraints that those imply in terms of the physical dynamics of that system.
And you can start to interpret those dynamics in terms of the functions they might be
optimizing, much like, much like if you were to write down the equations that underpin Newtonian
dynamics, you can write them down in terms of their flows on Hamiltonian functions. And it's
following the same sort of logic to then get to flows on free energy functions, where free energy
is just a measure of that fit between us and our world. And so both roads ultimately end up leading
to this common endpoint, which is that to be an agent in our worlds, in the sort of worlds we
live in, we have to be able to change our beliefs, to reflect what's going on around us and change
the world through our dynamical flows on a free energy functional, to best fit with the sorts
of creatures we are. When we first started looking at the free energy principle, we were talking
about things. It was known as a theory of every thing, every space thing, which is to say, roughly
speaking, if a thing exists, what must the thing do to continue to exist? And just their continued
existence resisting entropic forces is what defines them, which gets us into the second law
of thermodynamics. Now, that sounds like quite a strange thing to say. Why do things need to
resist entropic forces? And I think there's a development in how a lot of these ideas are
presented over time, which you expect and hope for in science. And I think we've often taken
different perspectives at different points in time as to how we explain these ideas.
And resisting entropic forces is an idea that I think most people find relatively intuitive.
So the idea that the physical systems will tend to increase their entropy over time,
at least close systems, so that over time things will gradually dissipate, things that are highly
structured and highly ordered and can only exist in a very small number of configurations and
more likely to change into something that can exist in many different configurations than they
are to go in the opposite direction. But anything that persists over time and maintains its form
clearly resists that process of decay, at least to some extent, or at least for some period of time.
However, the opposite is also true. We're also not creatures that tend towards a zero entropy
state. We don't end up in a single configuration. We have to be flexible. We have to change in
various ways throughout our lifetime or even throughout our daily routine. So it's not quite
as simple as just saying you have to resist entropic change. It's more to say that entropic
change or the amount of entropy that you expect to develop has to be bounded both from above
and below, that there is a sort of optimum level to be at. And that optimum probably varies from
different, well, from person to person, from creature to creature, from thing to thing.
You could imagine a rock that doesn't need to do much. Its interface with the environment is
quite trivial versus us as agents. We are incredibly sophisticated. So for us to continue to exist,
we have many more ways of interfacing with the environment and we need to plan
many more steps ahead. So is that just a pure continuum between rocks and people?
I mean, in principle, yes. I mean, the notion of that persistence, of that resistance of entropy
will depend very much on what you are. And as you say, you could imagine a whole scale of
things in between. I mean, in a way that as you've highlighted with the rock,
some of the most boring things are the things with the, sorry, I shouldn't say that,
poor geologists who might find rocks very interesting. And I'm sure are very complex, but
from a sort of behavioral perspective, clearly things like us are much more interesting to study
than things like a rock. And part of that is that we actually have a higher degree of entropy in
how we live our daily lives compared to things like, I almost said organisms like rocks,
but things like rocks that are not behaving. The reason I was thinking about this is the
second law of thermodynamics was conceived, I don't know, 150 years ago or something like that.
And many people at the time thought that it was an affront on free will. I think the religious
people at the time were aghast at the idea that things were mapped out in this way.
It's always worth saying in this discussion that obviously the tendency for entropy to increase
from a physical perspective generally relates to closed systems of which we are not. And as soon
as you start talking about different compartments and interactions between them, you also introduce
the idea of several coupled systems. And so you can start to ask questions about the overall entropy
or the entropy of specific parts of that system. And agents and worlds are two compartments and
systems that exchange things with one another. And so are not closed systems almost by definition
that a closed system, again, from a sort of neuroscience standpoint is not necessarily
a very interesting system. So probably that deals with a large part of that. The question of free
will is always an interesting one and always a thorny one that I'm not going to claim to have any
expertise on or be able to answer. But I think it probably tackles a slightly
different thing from a cognitive science perspective, which is whether or not we believe
that the actions we're taking are actions that we've chosen. And that probably comes back into
another aspect of active inference, which is that idea that the way we're regulating our
worlds, the way we're perhaps changing the entropy of our environment depends upon our own
choices about it, our inferences about which one we're going to do next. And that feeds into things
like we've spoken about free energy, that that quantity that we use to both choose our actions,
an act in the world around us while also drawing inferences. But we can also talk about things
like expected free energy, which is a way of evaluating our future state and what would be a
good trajectory or a good way for the world to play out. And their entropy has a completely
different meaning and there are different sorts of entropy. So for instance, if I were choosing
between several different eye movements I could make while looking around this room,
the best eye movements I might choose are those for which I'm least certain about what I would see.
In other words, the highest entropy distribution. So once you start planning in the future and once
you start selecting things to resolve your uncertainty and to be more confident about the
world around you, you actually end up seeking out entropy, which it seems to then very much
contradict some of the other ideas that we were talking about, the idea that we're constantly
resisting it. But actually it's by seeking out the things that we're least certain about that we
can start to resolve that uncertainty and start to become more confident and more certain about the
world around us. Yes, resist entropy by seeking it out. That's a bit of a paradox. But even what
you were saying just a second ago about this description of how agents operate, it's very
principled. We were talking about this balancing epistemic foraging versus sticking with what you
know. And more broadly speaking, thinking of agency as this sophisticated cognition of
having preferences and bending the environment and so on. And I guess where I was going before
is it's tempting to think that this erodes free will. And I think of them quite adjacently in
my mind. If anything, I guess I would call myself a free will compatibilist, which means it doesn't
matter that it's predetermined. For me, free will, I'll try not to use the word free will, but
thinking of agency in this sophisticated way, whether it's predetermined or not is irrelevant.
It's the complex dynamics that distinguishes my agency from somebody else's. So I think agency
is better to think of than free will, if that makes sense. Yeah. And I think that's probably right.
And the experience of and the inference of agency as well, I think is part of that.
There's a potential link that you can draw here also to the idea of chaotic,
dynamical systems of which we essentially are examples. And the idea of chaos in that setting
is that if you start from two ever so slightly different initial conditions, your path and your
future may unfold in a completely different way. And I think that fits very nicely with what you're
saying about distinguishing my agency from somebody else's because you don't see it as if I were,
you know, the time going to behave in exactly the same way somebody else's. And part of the
reason for that is that you end up starting from a slightly different perspective to where they are,
and that might lead to wildly different futures for both of you.
So something I think about a lot is whether agents are ontologically real or whether they
are an instrumental fiction. And I think part of the complexity, especially with active inference
and the free energy principle is this hierarchical nesting. So we can think of agents inside agents
inside agents. And I guess the first question is, are they real and does it matter?
Define real for me.
Well, one argument would be that they are epiphenomenal, that they themselves don't affect
the system that they are. Is this a good way to think about it?
It is a very difficult question to try and contend with, isn't it? Because I think there
are so many words that come up here that are kind of laden with different semantics or different
meanings depending upon who you speak to and which camp they come from in the sort of philosophical
world. And that's why I sort of asked you to define real. And it's really difficult to define
what real means in that setting, isn't it? And I guess coming back to your original question there,
for me, does it matter if they're a sort of real thing or not? Probably not. It matters
whether it's useful. And I guess that sort of brings me to a point about one of the things I
find quite appealing about active inference as a way of doing science. And I think,
you know, having had an interest in things like neuroscience and psychology for some time,
I often found it quite frustrating to understand what people meant and the different language
they used in psychology to understand different aspects of cognitive function.
And I think, you know, it's worth acknowledging that actually lots of people mean completely
different things when they say attention. And some people say attention to mean the sort of
overt process of looking at something and paying attention to it. Other people use it to talk
about that the differences in gain in different sensory channels that they're trying to pay
attention to or not, you know, am I paying attention to colors versus something else?
And that's just turning up the volume of different pathways in your brain.
And I'm sure there are a world of other things that people mean by it as well.
But the idea of then trying to commit to a mathematical description of these things
means that a lot of that ambiguity just disappears, that if you put a word to a
particular mathematical quantity, as long as you define what that mathematical quantity is
and how it interacts with other things, then a lot of that ambiguity just isn't there.
And it forces you to commit to your assumptions in a much more specific way.
And so that's why I come back to say, does it necessarily matter if an agent is real or not?
I don't really know what that means. But if an agent is just a description of something that is
separated from its environment that persists for a certain length of time, that has a dynamical
structure that can be written down and a set of variables that can be partitioned off from another
bit of the world. For me, that's real enough to be useful. And so that's where I'd go with that one.
Yes, yes. This is fascinating. So it's a mathematical theory that carves the world up
in an intelligent way that explains what things do and what they don't do.
And I guess the ontological statement, maybe we can park that to one side,
because as you say, from a semantics point of view, people have very relativistic
understandings of things. And there's always the philosophical turtles all the way down. Well,
is it really real? Is it really real? But one thing that is interesting, though,
about active inference is that it's quite mathematically abstract. So when we were
saying, is it real? It doesn't even designate, is it physical? So for example, a boundary is just
talking about the statistical independence between states. And those don't necessarily
correspond to physical things. So I guess it could be applied to almost anything. It could be applied
to culture or memes or language or something like that. And it has been. Yes, indeed.
Yeah, it's a good point. And then you end up sort of dragged into the questions of what is
physical. What does that mean? Is physical just an expression of dynamics that evolve in time?
Because I mean, even committing to a temporal dimension tells you something about the world
you're living in. Are the boundaries that we're talking about, are the partitions,
are they spatial in nature or not? And, you know, I remember there was an article a little
while back that sort of made a lot of argument about this as to whether the partitions that
divide creatures from their environments are equivalent to statements of conditional independence
of the sort that are seen in machine learning or various other things. And arguing that there's
something inherently different about a physical boundary. For me, I was never completely convinced
by that, but partly because you have to then define what you mean by a physical boundary.
And I suspect it's the same sort of boundary, it's the same sort of conditional
dependencies and independences. But where those have specific semantics, whether those be temporal,
whether they be something where, you know, you can actually define a proper spatial metric
underneath the things that you're separating out. And clearly, that sort of boundary is very
important. But for me, that is just another form of the same sort of boundary. And as you say,
you can apply exactly the same sort of ideas to things that are not spatial, not sort of physical,
whatever that might mean. Yes. Yes. Because when I when I spoke with Carl last time, I was pressing
him on this idea of a non physical agent, and he was quite allergic to the idea. And I suppose,
even though mathematically, you could apply it first to other geometries, that would be
quite easy, because they have certain mathematical properties in terms of like, you know,
being locally connected and measure spaces and all that kind of stuff. But if you did say,
okay, I want to have an agent that represents a meme. How would that act? I don't know,
you get into modeling challenges, don't you? I suppose you do. I think the modeling challenge
is defining the boundary. I think the boundary is a very difficult thing to define sometimes
when you're dealing with something non spatial. That boundary, though, might be reflected in
the interactions between a meme and a community that that engage with it. It might be to do with
the expression of a meme in different parts of, I don't know, a network of some sort or social
network. I don't know how easy it would be. I've not tried to do it in that context. And I think
with many of these things, you never really know until you've had to go at doing it. But
I suppose the key things I would be thinking about are, is there a clean way of defining
a boundary for a meme? Is there something that the meme is doing to the outside world?
Is there something that the outside world is doing to the meme?
And I think if you're able to define those things convincingly, then perhaps there is a form of
agent that may be non physical, if that's how you choose to define it. But then I'm not sure what
physical means in this setting. Is there also an account of saying, well, actually,
if you can write down the dynamics of how a meme propagates through a network,
is that any different writing down the dynamics of another sort of physical system?
Yes, possibly not. But it is really interesting to me that something like language could be seen
as a life as a super organism, or even something like religion. And it seems to tick all of the
boxes that we talk about with a gentleness in physical agents, which is to say, let's say
a religion or even nationalism, you could say that the state of the Netherlands has certain
objectives. And clearly, there's a two way process here. So the state affects our behavior. And we,
our collective behavior influences the state. But this then, I think the reason why people don't
like to think in this way is we have psychological priors. So we are biased towards seeing a
gentleness in individual humans, but we tend not to think of non physical or diffuse things as being
agents. Yes, I think that's probably right. And again, it sort of brings us back to this whole
issue about the language that we use, that it comes laden with lots of prior beliefs about what it
means, which may vary from person to person. And there comes a point where you say, how important
is it that I commit to using this particular word to mean this particular thing in this setting?
But again, in your example of taking a nation or nation state as being a form of organism at a
higher level or form of agent, if you can show that there is a way of summarizing the dynamics of
that system, maybe some high order summary of the behavior of people in that system, voting
intentions, I don't know, you might then be able to show that it behaves in exactly the same way
mathematically as individuals within that system. Yeah. So this brings me on a little bit too. I've
been reading this book called The Mind is Flat by Nick Chaito and I'm speaking to him on Friday.
And his main take is that, I guess you could call him a connectionist, he's friends with Jeffrey
Hinton. And his main take is that there is no depth to the mind. So for years, psychologists have
built these abstract models to reason about how we think. So we do planning, and we do reasoning,
and we have perception, and we do this, and we do that. And also, we try and generate explanations
for our behavior. So we do this kind of post hoc confabulation. But when you study it, it's incredibly
incoherent and inconsistent. And he was talking all about how the brain is actually a kind of
predictive system, right? So we have these very sparse incoherent inputs, and we sometimes see
things that aren't there. And I think you speak about this in your book that there was a really
big shift. I think you referred to it as the Helms-Hotsian idea that the brain is a kind of
prediction machine, rather than our brain just kind of like building a simulacrum of the world
around us. I mean, how do you think about that as a neuroscientist? Yeah, I mean, I think prediction
has to be a key part of it. And the reason it's a key part of it is that it's a way of coupling
us to our world that without prediction, you know, if you're purely simulating what might be going on
without actually then correcting your simulation based upon what's actually going on or the input
you're getting from the world, then you're not going to get very far. So prediction is just an
efficient way of dealing with the issue of how do I update my beliefs? How do I update if you want
to call it a simulation? My simulation, my internal simulation of what's going on outside.
And once you cease to have that constraint, once the world ceases to constrain the simulation,
that's the point at which you start, as you say, hallucinating, seeing things that aren't there
and developing beliefs that just bear no relationship to or little relationship to reality.
Yeah, interesting. So I mean, one thing this Nick Chaitaguay was saying was that we see a
complex system and we adopt what Daniel Dennett calls the intentional stance. And that is I have
a self model, I have a model of your mind, and I observe behavior and I kind of impute
onto you a model and I can generate explanations. So as I say, Thomas did that because he must
have wanted to do this. And I guess you could argue that all of this is just a confabulation.
It's just an instrumental fiction. It's a way for us to explain behavior, but it doesn't really exist.
But then there's the question of, well, it's not that it doesn't exist. It's just that your mind
is incomprehensibly complex. So it's not that the mind is shallow. I prefer to think of it as
the mind has so much depth that it's beyond our cognitive horizon. And depth, I think, is an
interesting notion as well. I mean, it's the idea that comes under a lot of machine learning and
the idea of deep learning neural networks with multiple layers. And I think you're right that
depth is an important part of our generative models as well, of our brains models of the world.
And part of that comes from the fact that the world actually does separate out into a whole
different series of temporal scales of things that happen slowly, that contextualize things that
happen more quickly, that contextualize things that are even faster than that. And so one good
example of depth might be that if you're reading a book, then you have to bear in mind which page
you're on within that page, which sentence or which paragraph you want, within paragraph,
which sentence, within the sentence, which word, within the word, which letter. And by combining
your predictions sort of both down the system that way, but then updating your predictions
all the way back up again, you start to be able to make inferences about the overall narrative
that you're reading. The other thing you mentioned that I thought was interesting was the idea of
confabulation and of how we come to beliefs about other people's behavior. And I think the same
thing is also true about our own behavior and sort of making an inference about what we've done.
And this comes all the way back to the sense of agency again, doesn't it? It comes back to the
idea that I am inferring, I'm behaving in this way for this reason, because I've chosen to do this,
because I had this goal in mind. And to come back to the other question, is that real? Or is it
simply an inference about what I've done? I would suggest that it's certainly an inference about
what I've done, whether or not it's real. Giovanni and I put together some simulations
and some theoretical work a couple of years ago after a discussion at a conference about or a
workshop about machine understanding, suggesting that machine intelligence is one thing, but
actually understanding why you've come to a particular conclusion. ChatGPT being able to
explain to you why it came up with a specific sequence of words or why a convolutional neural
network classified an image in a particular way is one of the big issues really, and there are
solutions coming up, but it's one of the big issues in the deep learning community as to how
you have that transparency in terms of what the models are doing and why they're doing it.
Giovanni and I put together some work following that, looking at
understanding of our own actions from an active inference perspective, and there it was very much
framed as I have a series of hypotheses of things I might do, of reasons why I might do that.
And then after observing myself behaving in a particular way, I can then use my own behavior
as data that I then have to come up with an explanation for. And it's very interesting to
see what happens if you start depriving that of aspects of its behavior and to see the confabulations
that result from that. I can't remember where it came from originally, the idea of hallucinations
being a perception generally being effectively a constrained hallucination, where you take your
hallucination, your simulation of what's going on, and then you fix it to what's actually coming in.
But you could argue that actually a lot of our understanding about what we're doing is also
just a constrained confabulation in exactly the same way.
Yes, which is very ironic because people diminish GPT and because they say it's just
confabulating, whereas the preeminent neuroscientists of the day do basically make the same argument
about how the brain works, and even our communication now on conditioning your simulator.
So the semantics are drawn by your own model in simulation of the world,
rather than being the simulacrum of mine. You spoke about machine understanding,
I mean, there's this Chinese Rem argument. And we're in a really interesting time now because
we have artifacts that behave in a way which is isomorphic in many ways.
And it's so tempting to say, well, we're different. And you could make the ontological argument,
but this psychological argument is a big one as well, which is we're different because we have
beliefs, motives, volition, desires, we have all of these things.
But as we were just saying before, this is all post hoc confabulated.
We actually don't have consistent beliefs and desires. It's just a fiction.
Was it a fiction or is it a plausible explanation?
Well, I guess the thing that breaks it for me is the incoherence and inconsistency,
because you would think that we would be fully fledged human agents if we had consistent beliefs
and desires. And it's not to say that we don't because it feels like some of our goals are
they grounded in some way, like we need to eat food. But we think of ourselves as being
unique as humans, because we have higher level goals and beliefs that aren't necessarily instrumental
to eating food. And I guess those things in particular might be confabulatory.
Yes. So on the volition thing, that's something that really interests me.
An active inference agent is we draw a boundary around a thing and it can act in the environment
and it has preferences. And essentially, it has a generative model where it can produce these
plans, these policies, if you like, and at the end of every single plan is an end state.
So it's got all of these different goals in mind, if you like. And in the real world,
real in big air quotes, these things emerge. But when we design these agents, we need to
somehow impute the preferences onto them. So it feels like they have less agency if we
impute the preferences. Would you agree with that? Interesting question.
And a very relevant question in the current number of industry related applications of
active inference. I think we were speaking about earlier, there are a number of companies now
that have been set up looking at use of active inference based principles for various problems.
Companies like Versus that we spoke about before and Stan Hope AI that I do some work with as well.
And the issue there is very much, it's a different kind of issue to the biological
issue of describing how things work. And it's the issue of saying, if I now want to design an
agent to behave in a particular way, as you say, am I taking some agency away from that?
There are a couple of things to think about there. I suppose one is thinking about
do biological agents actually select their own preferences to begin with?
And I think most people would probably say they don't most of the time. There may be certain
circumstances where they do or where a particular preference might be conditionally dependent upon
the task I'm in, the scenario I'm in, whether I'm at work or at home or whatever else. But it's
not that I'm actually selecting this is what I want to want. There is a famous quote here,
but I can't remember what it is. I don't know whether you do. No.
No, it's escaped me about wanting what you want or wanting what you do or something along those
lines. Anyway, the point I'm making is that, to some extent, our preferences are given to us
effectively through a process of evolution, natural selection, previous experience that has
affected what is a good set of states to occupy. And those will often be a good set of states that
help my survival, that help the persistence of the species that I'm a part of. And arguably,
the same thing is true when you as a designer of a particular algorithm or an agent are giving it
a set of preferences. From its perspective, it's never selected them anyway. And that's the same
as you or I not necessarily having selected our preferences. There's one additional element that
I think is interesting to think about. And one of my colleagues and collaborators,
Nor Sajid, has done a lot of interesting work on this, which is the idea of learning your own
preferences, of actually saying, let's create an agent that isn't given preferences to begin with,
but is allowed to learn as it behaves what sort of goal states it ends up in.
And there you get some very interesting results. So she showed that these sorts of agents
may end up doing things that you just don't want them to do, that they end up forming a
particular pattern of being or a particular way of being that you as a designer might never have
envisaged. For example, in an environment with lots of potential holes that it can fall into,
some of these agents just become hole dwellers. They just decide, I found that the first few
times I did this task, I fell into the hole. So I've decided I'm probably the sort of creature
that likes living in a hole. So that's a situation where you can give it a certain agency. And maybe
that agency is the ability to sort of disagree with what you as a designer might expect or want
from it. Yes. This is so interesting. I mean, we're getting a little bit into, we'll have a
discussion about cybernetics and externalism. But so what you're describing there is the reason
why AI systems today are not sophisticated is because they are convergent. And that's usually
because they don't actually have any agency. So one of the hallmarks of the physical real
systems in the real world is that they have these divergent properties. And that's because you have
lots of independent agents following their own directiveness doing epistemic foraging. So
interesting stepping stones get discovered. And sometimes those stepping stones aren't what the
designer of the system would have liked, as you just said. So there's an interesting kind of paradox
there of how much agency do you want to imbue in the agents. But the other paradox is the physical
and social embeddedness. Because as you just said, cynically, we don't have as much agency as we
think we do, because we're embedded in the dynamics around us. And being part of this
overall system means that our agency is defined not just by our boundary, but it's by the history
of the system. It's the history of us sharing information of all of the things around us.
And all of these things inform what we do and what our preferences are. And then you say, well,
we can just drop a brand new agent in the system. And it doesn't quite work because it's a fish out
of water. It's not embedded in the ways that things that emerged in that system were in the
first place. But this does get us onto this discussion of externalism. So part of the fiction
of how we think about cognition is that we think of ourselves as islands that don't share information
dynamically with the outside world. And of course, active inference is a way of bridging
these two schools of thought. So can you kind of bring that in?
I mean, I think you've already done it in a sense. I'm not sure what else there is for me to say on
that. I'll try my best. So yes, I mean, active inference is about, well, it's about aboutness.
It's the idea that our brains and our internal state evolves in such a way that reflects beliefs
about what's outside. And I think that's one of the key things that you have to have for any sort
of intelligent system. And that doesn't necessarily exist with other approaches that exist in
neuroscience or artificial intelligence. It is that, and I'll just repeat that, it's very much
being, the aboutness is the key thing that what's happening in my head is a reflection or is a
description in some way is about what's happening outside my head. And maybe that's the link with
this sort of externalism. But it's not just unidirectional either. It's the fact that
I'm forming beliefs about what's happening in the outside world, but I'm also the one influencing
the outside world to change it to fit with the beliefs I have about how it should be.
Yes. Yes. So there's a kind of model. So we draw these boundaries. And we model the world around
us. And we influence the world around us. And that's essentially what active inference is.
I guess it might be useful just to sketch out the cognitive science idea of an activism or
cybernetic. So there were folks who really railed against this idea of representationalism,
which is this idea of model building in principle. And active inference is an integrated approach
where we allow some model building, but we also think of the world itself as being its own best
representations. How do we kind of bridge those two ideas? Yes. And I confess, I'm always lost in
the distinction between the sort of inactivists, radical inactivists, the sort of different levels
of stance you can take on this. And I think it comes down to that, that from an active inference
perspective, both your representations, if that's the right word, the beliefs you have about the world,
whether or not that meets the criteria for representation from an inactivist perspective
is very important. But it is only important in terms of how you act. If your beliefs did not
affect how you acted, clearly natural selection would not have selected you to form those beliefs.
I think it's the simple way of putting it. So let's talk about some of the kind of
the mathematical underpinnings here. So I think probably one of the main concepts we
should start on is this idea of surprise. And maybe we can talk about it in general terms,
and then we can move on to Bayesian surprise. So why is surprise so important in the free energy
principle? Well, it's central to it. It is the key thing that matters. And we talk about the free
energy principle. But in a sense, free energy is really there as a proxy for surprise. So yes,
what do we mean by surprise? And it's another one of those things like the high road and
the low road that you can approach from several different angles or several different lines of
attack. If you were modeling something, if you were a Bayesian, so if you took a particular
stance on probability theory and wanted to know, given my model, given my hypothesis,
what's the evidence for it? What you would normally do is calculate something known as
a marginal likelihood, which is just a measure of the fit between your model and the data that
you have that you're trying to explain. That fit trades off various different things. So it can
trade off how accurately your model is explaining the data against how far you've had to deviate
from your prior beliefs or from your initial assumptions in order to arrive at that explanation.
So that marginal likelihood, that evidence is effectively just the negative or the inverse
of surprise. So that that's one perspective on it, the better the fit, the simpler and most
accurate my explanation for something, the less surprised I will be by it. Another perspective
on surprise is just this more colloquial sense. It's the idea that, given what I would predict,
how far out of that prediction is it? One could take a more biological perspective on it and say,
imagine we are, well, we are homeostatic systems that have some set points. We want to keep our
temperature within a certain range, our blood pressure within a certain range, our heart rate
within a certain range. If we find ourselves deviating from that, that is effectively a surprise
because we're not where we expect to be. And so we enact various changes to bring
those parameters back in range. So we might, if our blood pressure is too low, we might increase
our heart rate to bring our blood pressure back up to the range we expect it to be in.
And that is, in a sense, what active inference is all about. It's just this idea of keeping things
within that minimally surprising range. But of course, once you put dynamics on it, once you
start unfolding that in time, you end up having to not just deal with how surprising things are now,
but you've got to try and anticipate surprise and behave in such a way that you allostatically
control your sensory inputs, both your intraceptive inputs like heart rate and blood pressure,
etc., but also your extraceptive sensations, your vision, your audition, and the like.
And there's almost no end to the perspective you could take on surprise. Another perspective
on it is that it's a reflective of, in a physical system, the improbability of being in a particular
state. From a lot of physics perspectives, improbability is also associated with energy.
It takes energy to bring things into less probable states. And without inputting energy into a system,
it will generally end up in its most probable state in the absence of that.
You think of things like Boltzmann's equation and the relationship there between energy and
probability. And that also has a link then to the idea of either a Hamiltonian or indeed a
steady state distribution, which is just what is the distribution things will end up in if left
to their own devices for a certain amount of time until things have probabilistically converged.
And that means that if I would construct a probability distribution over where things
will be at a long point of time in the future, there will come a point at which that probability
won't change any further. And the tendency of physical systems to go to those more probable
states is exactly the same as the tendency to avoid surprising states. And again, we could
sort of go on for a while, but I won't on sort of other ways of conceptualizing it. But hopefully
that sort of explains why it's such an important thing that underpins so much of what we do.
We're either trying to sort of evolve as a physical system towards more probable states.
Or we are homeostatic or allostatic organisms trying to maintain our internal parameters within
the right set points. Or we are more colloquially just trying to avoid things that are different to
what we predict. Or we are statisticians trying to fit our model to the world as best we can.
And all of those things come under the same umbrella of surprise.
Free energy comes in because surprise is not a trivial thing to compute.
Mathematically, it's often either intractable mathematically or computationally. And so it's
just not efficient to be able to calculate. But free energy is a way of then approximating
that surprise. It's a way of coming up with something that is close enough to it. Or
even more precisely as an upper bound on surprise. So if you're at the lowest point of your free energy,
then that limits how high your surprise can be. The key additional thing in free energy is that
the distance between that bound, your free energy and your surprise depends on how good your beliefs
about the world are. And that's where perception comes in. That by getting the best beliefs you
possibly can, you minimize the distance between your free energy and which is up a bounding of
surprise and the surprise itself. So then any further reduction in free energy, you would expect
to also result in a decrease. Sorry, any further decrease in free energy would also result in a
further decrease in surprise. I mean, there's a few things that struck me. I mean, first of all,
what struck me is that we're using the language of things like statistical mechanics and Bayesian
statistics and information theory, things like entropy and so on. And we're interchangeably
kind of speaking about the same thing from the perspective of different disciplines,
which I find very, very interesting. And on the surprise thing, even though in this formalism,
we are minimizing surprise, I think there's an interesting perspective that sometimes surprise
is what we want. So for example, the chess algorithm, the ELO algorithm, it's only when
something surprising happens that the weights get updated because it's information. Or people on
YouTube, my videos are that they get more views when they have a cash value, which means they
have information content, which means that, you know, they're actually surprising your predictive
model. Even Arnold Schwarzenegger used to joke about it, he said, you have to shock the muscles.
You know, you have to do what the muscles don't expect. Otherwise, there's not an adaptation. So
there's this interesting juxtaposition between actually seeking out surprise, even though you
can think of our brains overall as minimizing surprise. And what was the other thing I was
going to say? Yeah, you were just getting onto variational inference, which is really interesting.
So there's a couple of intractable statistical quantities in this mixture that we're talking
about. I think it's the log model evidence and the Bayesian posterior. And we can't represent
those things directly. So we have to put a proxy in there, which kind of captures most of the
information, but it's still possible to deal with it. So how does this variational inference work?
Yeah. So I suppose maybe the first thing to think about, though, is just to recap what Bayesian
inference is. I suppose we've been talking about it quite a lot without necessarily defining it.
And many of you listeners, I'm sure, will know already. But the idea is actually relatively
straightforward and well-established and quite widely used. And it's the idea that if I have
some beliefs about things that are in my world that I can't directly observe, I may have a sense
of what's plausible to begin with. And that's what we refer to as a prior probability. I then also
need to have a model that says, given the world is this way, what would I expect to actually observe?
So for instance, given where you are relative to me, I can predict a certain pattern on my retina.
And if you were somewhere else, I would expect a different pattern on my retina. So I might have
a prior range of plausibilities as to where you are relative to me. And then I have a model that
explains how I'm going to generate some data based upon that. And Bayesian inference basically
takes those two things and inverts them using Bayes' theorem and effectively just flips both of them
round. So you now say instead of a distribution of where you are relative to me, I'm now talking
about a distribution of all the possible things that I could see on my retina. And instead of
predicting the distribution on the retina given where you are, I now want to know the distribution
of where you are given what's on my retina. And Bayesian inference, much like active inference,
is full of all these interesting inversions where you sort of flip things round from how
they initially appeared. But the problem is calculating those two things, calculating the
flipped model. So the distribution of all the things on my retina here would now be my model
evidence, my inverse surprise. And the distribution of where you are relative to what's on my retina
is my posterior distribution. But those things are not always straightforward to calculate.
And so variational inference takes that problem and makes it into an optimization problem. It
writes down a function that quantifies how far am I away from my, or what would be the true posterior
if I'd used exact Bayes. And then it says, well, let's parameterize some approximate posterior
probability. So come up with a function that represents a probability distribution that's
easy to characterize, something like a Gaussian distribution where I know I just need my mean
and my variance. And then just changes that mean and variance until you minimize this function
that represents that discrepancy, minimize this free energy, also sometimes known as an evidence
lower bound, in which case you maximize it. And interestingly, once you've maximized your
evidence lower bound or minimized your free energy, you end up with a situation where
the free energy starts to approximate your log model evidence or your negative log surprise.
And your approximate posterior distribution, your variational distribution starts to look
much more like your exact posterior probability distribution. So it's another one of those
interesting scenarios where doing one thing optimizing one quantity ends up having a dual
purpose. And in active inference, the only additional thing you throw into that is that you
want to then also change your data itself. So you do the third thing you act on the world
to then optimize exactly the same objective. The interesting thing, I guess, is just contrasting
to machine learning again. So in machine learning, we also have these big parameterized models and we
do stochastic gradient descent. And some might think of deep learning, because obviously you
can think of everything as a Bayesian. So you can think of machine learning as being maximum
likelihood estimation. Why is it that we go full Bayesian when we do active inference? Why not
something like maximum likelihood estimation? It's an interesting question. And there are a couple
of answers you could give again, some of which are more technical, but some of which are
some of which are slightly more intuitive. And I think one of the more intuitive answers is that
by having an expression of plausibility of things in advance, you just maintain things
within a plausible region. So maximum likelihood for those who are unaware is where you essentially
throw away that prior probability, where you throw away any prior plausibility as to as to
what the state of the world might be. And you just try and find the value that would maximize
your likelihood, which is your prediction of how things would be under some hypothesis or under
some parameter setting. And I think the first thing to say is if you throw away that prior
information, then you end up potentially coming up with quite implausible solutions.
That's particularly relevant if you're dealing with what's known as an inverse problem. So where
there are multiple different things that could have caused the same outcome. An example that's
often given is that for any given shadow, there's almost an infinite number of things, configurations
of the sun and the shape of the thing that's casting the shadow that could lead to exactly the
same shadow. And so maximum likelihood approach just won't be able to tell the difference between
all of those things. However, if you have some prior on top of that, if you have some statement
of the plausible things that might cause it, you can come up with a much better estimate of those
sorts of things. Another way of looking at it is that when you're dealing with a maximum likelihood
estimate, you're throwing away all uncertainty about the solution. So you're coming up with a
point estimate and you're saying this is the most likely thing, but you're ignoring all of your
uncertainty about it. And I think that is in itself a relatively dangerous thing to do and can lead to
the problem of overfitting, where you start to become very confident about what you can see from
a relatively small sample of things and you can end up with all of these well-described in the media
scenarios of complete misclassifications based upon that sort of overconfidence just because
all the uncertainty is gone. A more technical way of looking at it, I think, is if you think about
what a free energy is. So free energy is our measure of our marginal likelihood that we're
using when we're doing Bayesian inference. And one way of separating out what a free energy
looks like is to have our complexity, which is effectively how far we needed to deviate from
our prior assumptions to come up with an explanation, and our accuracy, which is how well we can fit
our model. Accuracy is common to both maximum likelihood type approaches because we're trying
to find the value that most accurately predicts our data and also to Bayesian approaches.
Both want to do that. But what's thrown away in the maximum likelihood type approach is the
complexity bit, the how far do you deviate from your priors. So there's an inbuilt Occam's razor,
the idea that the simplest explanation is a priori more likely that you get from a Bayesian
approach that you throw away when you're dealing with maximum likelihood estimation.
I wondered to what extent does the active part play a role here. So even in machine learning,
there's something called active learning, where you dynamically retrain the model,
or there's something called machine teaching, where you dynamically select more salient data
to train the model, and the model gets much better. And in things like Bayesian optimization,
for example, by maintaining this distribution of all of your uncertainty in a principled way,
you can go and seek and find more information to kind of improve your knowledge on subsequent steps.
So I guess it's sort of bringing in this idea of it's not just what happens now,
it's about how can I improve my knowledge of the world over several steps.
Yes, and that reminds me about the point you were making earlier, that sometimes we actually do
things to surprise ourselves, which seems very counter-intuitive in the context of the idea that
we're trying to minimize surprises as our sole objective in life. And sometimes people talk about
this in terms of a dark room problem, the idea that actually if all you want to do is minimize
your surprise, you just go into a room, turn off the lights and stay there because you're not going
to experience anything that's going to surprise you. I mean, the answer to this problem is that
actually, as organisms, as creatures, we don't expect to be purely in a dark room. And the
sort of organism that would be is, again, probably not a very interesting one. And that what we predict,
what we'd be surprised by might be permanently staying in a dark room. But it goes even further
than that. And if you say, actually, I'm minimizing my surprise over time, I want to be in a predictable
world where I know what's going to happen next. The best way of doing that is to actually gather
as much information as you can about the world around you. So the first thing you do really is
you turn on the light and see what the room looks like, because that might then predict all the sorts
of things that could fall on you in that room and could potentially cause surprise. And by knowing
about it, you mitigate the surprise that you might get in the future. And as you say, you can only
really do that if you know what you're certain about. And so if you take a maximum likelihood
approach, if you work based on point estimates and you have no measure of your uncertainty,
then there's no way you can possibly know what you're uncertain about to be able to resolve
that uncertainty. So this brings me on to causality. We know that predictive systems,
which are aware of causal relationships, work better. But if we just bring it back to physics
first, I mean, to you, what do you think causality is?
It is a tricky issue as to what causality is. And I think whether it exists or not is really a
matter of how you define it, isn't it? And some would define that purely in terms of conditional
dependencies, that the behavior of one thing is conditionally dependent upon something else,
and therefore you could say that the one thing causes the other. But as we know from Bayes'
theorem, that's not quite good enough, because you can swap any conditional relationship around
through that process of inverting your model. Sometimes that causality is written into the
dynamics of a model. So this would be the approach used in things like dynamic causal
modeling of brain data, where you might say that the current neural activity in one area of the
brain affects maybe the rate of change of neural activity in another part of the brain.
And it's the way in which those dynamics are written in, the fact that it's one affects the
rate of change of the other, that gives it that causal flavor and a very directed perspective on
it. Probably the work that is most comprehensive on this is looking at people like Judea Pearl and
a lot of his work on causality. There's a lot of detail about the notion of an intervention.
And I suppose you can think of this in terms of how you might establish causation in a clinical
context. If you were to run a trial to try and establish whether one thing's caused another,
you need to make sure you're not inadvertently capturing a correlation or a conditional dependence
that could go either way, or a common cause of both things that depends upon something else.
And typically the way you do that is you intervene on the system. You randomize at the beginning
to make sure that people are assigned to different treatment groups at random,
so that you break that dependency upon something prior to it. And then anything that happens going
forward is going to depend on the intervention that you're doing. So I think that's probably the
key thing that gives you causality or perhaps defines causality. It's the idea that an intervention
is what will change it. If you intervene in one thing, that should then in a way that doesn't
necessarily match its natural distribution if you hadn't intervened at all, and then see what the
effect is. Yes, yes. I mean, and by the way, Judea Pearl is really interesting. I want to study
his book, The Book of Why. It's one thing that we've really dropped the ball on, actually.
But I suppose one way to think about it is if you go back to the core physical, in physics,
there's a whole bunch of equations to describe the world we live in. And those equations don't
have, they don't say anything about causality, and they're even reversible. And then you can think,
okay, well, maybe it's a little bit like the free energy principle. It's a lens,
like really, there's only dynamics. But when you look at these dynamical systems,
then behaviors emerge, and somewhere up that chain, you can say, okay, now we've got causality,
and it's something which is statistically efficacious to build it into our models. But
where does it come from? Well, it comes from us, doesn't it? It's a hypothesis to explain a particular
pattern of dynamic. Yes. And we might infer causation based upon, again, a particular pattern
of how one thing reacts to another. So if you imagine you've got the classic physics example,
billiard balls bouncing into one another, how do you know that the collision of one ball with
another is causative of the subsequent motion of the second ball? And you could argue that that's
due to a particular pattern of which variables affect which other variables and the particular
exchange between them. And this comes back quite nicely to things like the physics perspective
on the free energy principle, the idea that actually one could see the location of a particular
ball as being, you know, maybe it's internal state, and then the action that that then causes
is perhaps the, or in fact, you could say that the action is the position of the ball, the force
that results from that action is the sensory state of the next ball, which then changes its
velocity to then change its action relative to something else. You can sort of rearrange those
labels slightly, but there is a directional element to it. And in that sort of pattern of
causation, you really do expect the position of one ball to have an effect on the rate of change,
or in fact, even the rate of rate of change of the second ball, which again, I think brings us
back to those kinds of dynamical descriptions of causality where one thing might affect how
another thing changes. So you almost get it from the dynamics itself. But again, to some extent,
it comes back to semantics, doesn't it? It comes back to what do we mean by cause? Well, I suppose
cause is a hypothesis as to a particular configuration of things. But then you've got to
write down what does that hypothesis mean? What's my model of what a causation involves?
Yes, yes. I mean, we were just talking about, you know, build building these models. And one of
the bright differences from machine learning is that we need to build a generative model by hand.
So we have to define these these variables, and some of them are presumably observed, and some of
them are not observed. They're inferred. And that process seems like you would need to have a lot
of domain expertise. And it seems like something which is at least has a degree of subjectivity.
I mean, we were just talking about causality, for example, there are many ways you could model
the risk of cancer from smoking. It seems like there are many, many different ways of building
those models. So that subjectivity is interesting. I mean, are there principled ways of building
these models? Yes. And in a sense, it all comes back to the same thing again, it comes back to
which model minimizes the surprise the best. And but there are interesting questions amongst that.
So how do you actually choose the space of models that you want to compare? So you're right to say
that that that often there is some specific prior information that's put into models and active
inference. And very often we do end up sort of building models by hand to demonstrate a specific
outcome or a specific cognitive function. But there's no reason why it has to be that way.
You can build models through exposure to data, where where the models are selecting the data to
best build themselves. But the question is how you do that, how you start to add on additional
things, how you start to change the structure of your model. But there's a lot of ongoing research
into that. And I think there are now methods that are coming out that will allow you to allow an
active inference model to build itself. And the way it will do that will be sort of adding on
additional states and potential causes, adjusting beliefs about the mappings and the distributions
and the parameters of given this than that, adding an additional paths that different
or different transitions that systems will pursue. So it's a fascinating area. I think
it's one that's still a growing area. But it's this idea of structure learning of comparing
each alternative model based upon its free energy or model evidence or surprise as a way of
minimizing that by being able to better predict things.
Yeah, I mean, that's something that we humans, we seem to do really well. So we can, first of all,
via abduction, we can select relevant models to explain behavior, you know, what we observe.
But we also have the ability to create models. In fact, I think of intelligence as the ability
to create models. So we experience something. And I now construct a model to explain this
and similar experiences in experience space. But in a machine, it's really difficult. So in
machine learning, there's this bias variance trade off. So we deliberately reduce the size
of the approximation space to make it computationally tractable. And when we're talking about
building these models, just from observational data, it feels like there's an exponential
blow up of possible models. So I can imagine there might be a whole bunch of heuristics around
library learning or having modules. So these modules have worked well over there. So we'll
try composing together known modules rather than starting from scratch every single time. I mean,
what kind of work is being done there? I mean, I think I think you're right about, you know,
it's not going to be worth starting from scratch every time. You can sort of build models by saying,
okay, let's start with something very simple with a sort of known structure. And I think it's
sensible to use some priors in that rather than starting from complete, completely nothing,
because there are some things that we know about in the world. And there's no point hiding that
from the models we're trying to build. And that might be a simple structural thing like things
evolve in time. So one thing is conditioned upon the next is conditioned upon the next.
And things now will influence the data I observe things well in the past might not anymore.
But then then there's the question of, well, how can a model then grow? What are the things that
you can add to it or subtract from it? And subtraction is another key element. Because you
could take this whole problem from the other direction, and you could say, well, let's start
with a model that just has everything in it and take away bits until we've got the model that's
relevant to where we are at the moment. And we know that during development, there's a lot of
synaptic pruning that goes on and removal of synapses that we have when we're much younger
compared to compared to as you get older. So what can you add on? Well, it depends what your model
looks like. So if your model says there's a set of states that can evolve over time, there are
a set of outcomes that are generated, well, we know what the outcomes are, we know what the
data are, because we know what our sensory organs are. So it's the states that are going to change
so do we add in more states? Do we allow them to take more alternative values? Do we allow
their transitions to change in more than one different way? Which ones can I change? Which
ones can I not change? And it's really just asking these questions that helps you to grow your model.
So you say, well, let's try it. If I allow this state to take additional values, if it's not
providing a sufficiently good explanation for how things are at the moment. And if that improves
your prediction, that's good and you keep it and if it doesn't, then you get rid of it. Do I now need
to include additional state factors? So you could either say there is one sort of state of the world
that can take multiple different values, or you could actually this is contextualized by something
completely separate. So where am I along an x coordinate? You also need to know where you are
along y coordinate to be able to contextualize what you're predicting. So it's just asking what is in
a model? How do you build a model almost gives you the answers to the ways or the directions in
which you can grow it. The other thing you can then do when you're trying to work out how to grow it
is to say, well, let's treat this as the same sort of problem as exploring my world,
selecting actions that will then give me more information about the world. You could say,
well, actually, now let's treat my exploration of model space as being a similar process of
exploration. Which of these possible adjustments to my model might lead to a less ambiguous mapping
between what I'm predicting or what's in my world and what I'm currently predicting?
Yes, it rather brings me back to our comments about the space or the manifold that the models
sit on, whether they would have a kind of contiguity or whether they would have a gradient.
I guess I'm imagining a kind of topological space that the models would sit on. I don't know whether
it's worth bringing in. Obviously, you're a neuroscientist and the way brains work,
we must do this. Of course, there's this idea of nativism. Some psychologists think that we have
these models built in from birth and then the other school of thought is that we're just a
complete blank slate. If you read Jeff Hawkins, he talks about the neocortex as this magical
thing that just builds models on the fly. But perhaps one difference at least between brains
and machines is the multi-modality, which is to say we have so many different senses that
creates a gradient or that makes it tractable. Because when a model from a particular sensation
and starts predicting well, we can rapidly optimise and go in the right direction.
Because the problem seems to be that there are so many directions where we can go in,
doing some kind of monotonic gradient optimisation will often lead us into the wrong part of the
search space, so we've wasted our time. Yeah, I think that's a really good point,
absolutely. As soon as you know how one thing works or how vision works, I suppose vision
and proprioception is a good example, isn't it? If I recognise where my hand is and I can
make a good estimate of that visually, then that helps me tune my joint position sense as to where
my arm might be. And it's always fascinating to see situations where that breaks down, so there
are a number of conditions where if you lose your joint position sense, you're perfectly okay holding
your arm out like that until you close your eyes, at which point you start getting all these interesting
twitches and changes. So yes, the multimodality I think probably is a really key thing that really
does help constrain the other senses because you're just getting more information about each thing.
Maybe we should just talk about chapter 10 in general, because that was kind of like the
homecoming chapter, if you like sort of bringing together some of the ideas. So can you sketch
that out for me? Yeah, so I think towards the end of the book, the idea was to try and bring together
a lot of the themes that had been discussed earlier on, but to also make the point that,
well, I'll come back to one of the things you said earlier was about how it seems we're talking about
lots of different things from different perspectives, but actually they're really the same thing.
So we talked about how surprise is also a measure of steady state of energies of various sorts of
of statistics and model comparison of homeostatic set points, you know, that all of these things
can be seen through the same lens. But again, taking one of those inversions, you can invert
that lens and say, well, actually, you can start from the same thing and now project back into
all of these different fields. And I think that's a useful thing to do because I think it helps foster
multidisciplinary work, helps to engage people from different fields and areas,
and helps us know what's happening elsewhere so that you're not just duplicating everything that
people have already done. So I think it's really important to have those connections to different
areas. And the chapter 10 from the book was an aim to try and connect to those different areas,
whether it be to things you've spoken about, like cybernetics and inactivism,
and just to try and understand the relationship between each of them.
Well, I mean, quite a lot of people use this as a model of, you know, just things like
sentience and consciousness in general. And I often speak about the strange bedfellows of
the free energy principle. So, you know, there are, you know, autopoietic and activists and
phenomenologists and, you know, people talking about sentience and consciousness, you know,
obviously you're a clinician, you know, you're working in a hospital. So it's just this
incredible conflation of different people together, and they all bring their own lexicon with them.
But maybe we should just get on to this kind of sentience and consciousness thing, because that
seems quite mysterious. We almost come back to one of the themes we've spoken about a few times,
which is that the specific words we use for things in the effect that different people,
that has on different people. So some people, I think, would probably get very angry with the idea
of using sentience to describe some of the sort of simulations and models that we would develop.
But that comes down to what you mean by sentience. And I think one of the key things for sentience
is the aboutness we were talking about before. The idea that our brains or any sentient system
really is trying to try not to anthropomorphise too much, but it's almost impossible to do in
this setting, isn't it? Not trying to, but that the dynamics of some system internally to the system
are reflective of what's going on external to it, and that you can now start to see those dynamics
as being optimization of beliefs. And those beliefs are about what's happening in the outside world
and about how I'm affecting the outside world. And I think that probably gets to the root of
at least a definition of sentience and one that I'd be happy with, which is just the
dynamics of beliefs about what's external to us and how we want to change it.
And there are very few things other than that sort of inferential formalism that give you that.
Yes, I mean, in a way, one thing I like about it is, I mean, we are talking as physicists,
so we are materialists. It's very no-nonsense. It's quite reductive as well, because there are
those who believe that these kind of qualities that we're speaking about, certainly with
conscious experience, for example, that it's not reducible to these kind of simple explanations
that we're talking about, that it has a different character. David Chalmers talks about a philosophical
zombie. So for example, you might behave just like a real human being, but you could be divorced of
conscious experience. So he says that you can think of behavior, dynamics, and function,
and conscious experience as something entirely different. But as an observer, you would never
know. So yeah, it feels very no-nonsense, doesn't it? But that wouldn't be satisfying to a lot of
people. No, it probably wouldn't. You're right. Yeah, and particularly when you get onto questions
like consciousness as well, I mean, I think it does become very, very difficult, because once
you're putting forward or advocating a theoretical framework that seems like it's supposed to have
all the answers. I mean, in reality, it doesn't. I mean, I think it's a useful framework to be
able to ask the right questions or to be able to articulate your hypotheses. So if you think that
consciousness is based upon the idea of having some sense of trajectory of temporal extent and
different worlds I can choose between or different futures I can choose between, that might be a key
part of it. But for some people, that's not what they mean by consciousness.
I found in a particular reading books by people like Anil Seth on this sort of topic, I found one
of the interesting comparisons being the questions about consciousness versus questions about life.
And we almost don't ask what life is anymore. It doesn't necessarily seem that mysterious,
just because we've had so much of an understanding of the processes involved in life, the dynamics of
life and the way biology works, it's still much more to go. But the question of what life is just
doesn't seem as relevant today as I suspect it did many years ago with those sorts of questions
that were being posed. And perhaps we'll see the same thing with questions like consciousness.
Yeah, it's interesting though how vague many of these concepts are. And it's quite an interesting
thought experiment just to get someone to explain just an everyday thing, you know, like what happens
when you throw coffee on the floor. And just keep asking why. And just observing how incoherent and
incomplete the explanations are. And it's the same thing with life, it's the same thing of
consciousness, it's the same thing of causality, agency, intelligence, all of these different things.
And I guess most people don't spend time digging into their understandings of these things and
realizing how incoherent and incomplete they are. Life is quite an interesting one in particular,
because I think one of the achievements of active inference is blurring the definition of or the
demarcation between things which are and are not alive. For example, the orthopedic anactivists,
they think of biology as being instrumental. And what the, you know, free energy principle
does in my opinion, is it removes the need for this, it almost removes the need for biology
entirely. It just says it's just dynamics, it's just physics. But yeah, I mean, just on that
point, though, I think many of our ideas about the world are quite incoherent.
Yeah. And I think it's interesting that, you know, one of the things that you're saying,
and I would agree with you as one of the big advantages of active inference-based formalisms,
you'll probably find some people will say, that's a problem with it, that actually there is a clean
distinction in their mind between these different things. But then I think the challenge is to work
out what that distinction is, if it exists. And it may be a distinction in their mind that doesn't
exist in somebody else's mind. And so getting people to try and or trying to support people to
be able to express that in a very precise mathematical hypothesis, I think is quite a
useful way of trying to explore those problems. Because clearly, for some people, there is
something that's getting at it that is not quite explaining. And it's interesting to try
and explore that and to work out what that thing is. Indeed, indeed. And just final question,
what was your experience writing a book? And would you recommend it to other people?
I enjoyed writing it. I think it's time consuming and can feel like it's going on forever some of
the time compared to, you know, I think anyone who's had some experience of writing papers will
often find that at the point where you're ready to submit it, you're just sick of it and want to see
the back of it. And then it's rudely returned to you by the peer reviewers with lots of comments
writing a book, it obviously takes you much longer. So you end up being almost more sick of it at
various times. But it's quite fun as a collaborative project. It's quite interesting to get other
people's perspectives on it. And I was lucky to have great collaborators to write it with.
And I think it really is a good way of organizing your thoughts in a slightly more holistic way
than you would while focusing on a very specific topic in a research paper.
And I've also just enjoyed the response I've had from people who've read it,
some of whom have picked out a number of errors, not many. But generally,
everybody's been very supportive of that and people seem to have responded well to it,
which I think is always encouraging. And that's what we hope should happen.
Wonderful. Well, look, Thomas, it's been an absolute honor having you on the show. I really
appreciate you coming on. Thank you so much. Well, thank you. I've enjoyed it.

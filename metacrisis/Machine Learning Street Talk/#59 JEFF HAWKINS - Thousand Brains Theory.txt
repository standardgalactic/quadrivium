In spite of the steady accumulation of detailed knowledge, how the human brain works is still
profoundly mysterious.
The ultimate goal of neuroscience is to learn how the brain gives rise to human intelligence
and what it means to be intelligent.
Understanding how the brain works is considered one of humanity's greatest challenges.
I got into this field because I just was curious as to who I am.
How do I think? What's going on in my head when I'm thinking? What does it mean to know
something? I can ask what it means for me to know something independent of how I learned it from
you or from someone else or from society. What does it mean for me to know that I have a model
of you in my head? What does it mean to know I know what this microphone does and how it works
physically even when I can't see it right now? How do I know that? What does it mean?
How do the neurons do that at the fundamental level of neurons and synapses and so on?
Those are really fascinating questions.
The neuroscientist Francis Crick observed that scientists have been collecting data on the
brain for decades. They knew a lot about the workings of the brain, but they hadn't converged on any
meaningful theory of how the brain worked. How intelligence emerges from low-level cells in
your head is still a profound mystery. To be intelligent, the brain has to learn a great
many things about the world. To understand how the brain creates intelligence, we need to figure
out how the brain learns a model of the world and everything in it. Jeff Hawkins thinks that the
reality we perceive is a kind of simulation, a hallucination, a confabulation. He thinks that
our brains are a model of reality based on thousands of information streams originating in
the senses in our body. Critically, Hawkins doesn't think that there's only one model, but rather
thousands. Jeff has just released his new book, A Thousand Brains, A Theory of Intelligence.
It's an inspiring and well-written book and I really hope after watching this show you'll be
inspired to read it too. Now, Jeff is a sub-symbolist. He thinks that our knowledge is sliced and
distributed over the substrate of synapses in our brain, but similarly that our simulated models of
the world are also distributed over hundreds and thousands of cortical columns. This means that when
we recognize a dog, there's a society of cortical columns all independently predicting that the dog
is there and mutually calibrating after a consensus is reached. Now, Jeff Hawkins is a materialist.
He thinks that despite appearances to the contrary, mental states are just physical states and that
the latter emerges from the former. There's no inception, no ghost in the machine, no Cartesian
theater. If I had to summarize into one big idea is that we think of the brain, the neocortex is
learning this model of the world, but what we learned is actually there's tons of thousands
of independent modeling systems going on and so each, what we call the column in the cortex,
there's about 150,000 of them, is a complete modeling system. So it's a collective intelligence in
your head in some sense. So the Thousand Brains theory says, if I'll, where do I have knowledge
about, you know, this coffee cup or where's the model of this cell phone? It's not in one place.
It's in thousands of separate models that are complementary and they communicate with each
other through voting. Hawkins says that the neocortex is the outermost and most recently
evolved layer of the mammalian brain, a bit like a wrinkly napkin, which wraps around the old brain.
It occupies about 70% of the volume of your brain and it's responsible for all aspects of your
intelligence. That is to say your sense of vision, touch, hearing, language in all its forms and even
abstract thinking such as mathematics and philosophy. The neocortex is surprisingly different
to other parts of the brain. It has no visually obvious divisions. The anatomical organization
is strikingly similar. Nevertheless, different parts of the neocortex still perform different
functions like vision or hearing, for example. The complex circuitry of the neocortex looks
remarkably alike in visual regions and language regions and even touch regions. It looks similar
across species such as rats, cats and humans. The variations between regions are relatively
small compared to their similarities. Now Hawkins argues that the complexity of the brain is in the
content of its connections and its wiring, which is an emergent property of a simple learning
algorithm. I mean, look at the open AI microscope, their tool for visualizing trained convolutional
neural networks as an example. The learning algorithm is simple, but all of this beautiful
complexity emerges as a result of training. Even after this type of visualization, it's not really
understandable by us humans. It's possible that any successful knowledge representational substrate
for some domains of data may never be understandable by humans. Now, the neocortex is arguably one
of the main organs of intelligence. It gives us sensory perception, motor control and abstract
thought. The key attributes it has are its ability to learn continuously, learn rapidly,
its power efficiency and its flexibility, which is to say its ability to learn diverse and novel
tasks. Now Hawkins argues that the neocortex learns a model of the world, that each cortical
column is a complete sensory motor modeling system. He thinks that cortical columns use
reference frames to store knowledge and generate behavior. Our brains learn about the outside
world by processing our sensory inputs and movements. The neocortex anticipates the sensory
results of movement. When we touch objects or observe the world around us, our brain receives
a sensory motor sequence of information. When moving our fingers over familiar objects, we
quickly notice discrepancies suggesting we make tactile predictions that are specific to particular
objects. Hawkins believes that the prediction of sensory stimuli after movements is the fundamental
primitive of cognition. The brain does not just memorize the sensory information as this would
become quickly intractable. Instead Hawkins claims that evolution repurposed the ancient
systems that evolved to model spatial relationships such as grid and place cells. It generalised
those systems to model abstract concepts in abstract spaces. In this system, thinking and
reasoning becomes a kind of traversal through a complex topology of relationships in the brain.
Analogous to the traversal of a physical space. The whole cortical sheet is made of collections
of primitive columnar units called mini columns. Each cortical area is a collection of millions of
cortical mini columns. These mini columns are organised in about six layers with specific
types of neurons and connection patterns in each layer. Neighboring mini columns have the
same receptive fields. They have the same inputs from other cortical areas and these mini columns
form complete fundamental and primitive units known as macro columns or as Hawkins calls them
cortical columns. Now pyramidal neurons are the typical excitatory neurons in the neocortex.
Their name comes from the triangular shape of their cell body. The dendrites of each
pyramidal neuron connect to tens of thousands of excitatory synapses, equally split from local
and remote sources. The neurons can look significantly different in their connection
arrangements depending on which neocortex layer they're situated in. The thalamus is the main
input and output subcortical structure to the neocortex and it could be viewed as the seventh
layer of the neocortex. It primarily sends sensory or pre-processed information into
layer four of the neocortex. Moreover, columns in the neocortex learn to model whatever systems
are wired to them. Columns wired to the ear, for example, learn to process sound. Columns
wired to the retina rods and cones learn to process images. The neocortex can retain remarkable
plasticity even later into life. Perhaps the most striking examples are those of post-injury
plasticity, where the brain's overcome injury by rewiring remaining neurons to regain
some or even full functionality. The cortical regions are densely interconnected with many
feedback and feed-forward skip connections across the hierarchy and all functional areas in the
neocortex are bi-directionally connected with other brain structures, but their main inputs
and outputs come from other cortical areas via long-distance connections. All cortical regions
are densely connected, but there's some apparent asymmetry and even hierarchy detectable in the
overall structure. Century areas tend to be lower in the hierarchy than associative or
motor areas, for example, and bottom-up information flows are known as feed-forward
and top-down connections are known as feedback connections. What should be pretty clear from
looking at this structure is that there's no blank slate. There's a very clear cognitive
architecture and much of this high-level structure is there from birth. There are many skip connections,
a high level of recurrence with many feedback loops, and the process is highly distributed.
At this point, I want to draw your attention to a paper published last year by psychologist
Joseph Cesario. It was titled Your Brain is Not a Tiny Onion with a Reptile Inside.
Joseph pointed out several misunderstandings of nervous system evolution, which he says stems
from the work of Paul McLean, who in the 1940s began to study the brain region, which he called
the limbic system. McLean later proposed that humans possess a so-called triune brain consisting of
three large divisions that evolve sequentially. The oldest, the reptilian brain, controls basic
functions such as movement and breathing. Next, the limbic system, which controls emotional responses,
and finally the cerebral cortex, which pretty much means the neocortex, but he thought that that
controlled language and reasoning. Joseph said that McLean's ideas were already understood to be
incorrect by the time he published his 1990 book. Since the 1970s, many in developmental neuroscience
thought that the ideas from McLean were a myth. Due to its longevity, the triune brain idea has
been called by neuroscientist Lisa Feldman Barrett as one of the most successful and widespread errors
in all of science. Joseph pointed out that the problem with this view is a simple conception of
evolution being arraigned sequentially from the simplest to the most complex organisms.
This view implies that anatomical evolution proceeds in the same fashion as geological strata
with new layers adding on top of existing ones. Instead, most evolutionary change consists of
transforming existing parts. Joseph says that the neocortex is not an evolutionary novelty
unique to humans or primates or mammals. All vertebrates possess structures evolutionarily
related to our cortex. Joseph concluded that these ideas are consistent with traditional views
of human nature as rationality, battling emotion, the tripartite platonic soul,
Freudian psychodynamics, and religious approaches to humanity. It's also a simple idea that can
be distilled and communicated to a lay audience. Look, I'm not suggesting for a second that Hawkins
subscribes to the idea of a tree in brain, but I think it's possible to get this impression
reading his book, which is why I wanted to bring this up. It turns out it's very tricky to get
neurons to do this, to build a map of an environment. We now know there's these famous
studies that's still very active about place cells and grid cells and these other types of cells in
the older parts of the brain and how they build these maps of the world. It's really clever. It's
obviously been under a lot of evolutionary pressure over a long period of time to get good at this,
so animals know where they are. What we think has happened, and there's a lot of evidence that
suggests this, is that that mechanism we learned to map like a space is was repackaged, the same
type of neurons, was repackaged into a more compact form and that became the cortical column.
In 1972, John O'Keefe discovered the first component of an internal GPS system in the brain.
He discovered place cells in the hippocampus, which activate when a rat is situated in specific
locations in a room. More than three decades later, in 2005, May Britt and Edward Moser
discovered another key component of the brain's positioning system, grid cells,
which represented a coordinate system which allowed for precise positioning and path finding.
These discoveries solved the problem that has occupied philosophers and scientists for centuries,
how the brain creates a map of the complex world around us and how we navigate our way through it.
What's intriguing about the brain solution is that it seems to create multiple hexagonal
grids with slightly different orientations and scales. Because these grids are orientated and
scaled differently, each point in space activates a unique combination of grid cells. In other words,
the brain encodes locations using bits in a sparse distributed representation.
Jeff thinks that sparse representations are crucial to the success of the human brain
and should guide our construction of artificial intelligence.
What's intriguing about the brain solution is that it seems to create multiple hexagonal grids
with slightly different orientations and scales. Because these grids are orientated and scaled
differently, each point in space becomes a unique combination of grid cells. In other words,
the brain encodes these cells using a sparse distributed representation. Jeff thinks that such
sparse representations are crucial to the success of the human brain and should guide our construction
of artificial intelligence. If you're going to build a model of a house in a computer,
they have a reference frame. And you can think of a reference frame like Cartesian coordinates,
like x, y, and z axes. So I can say, oh, I'm going to design a house. I can say, well,
the front door is at this location, x, y, z, and the roof is at this location, x, y, z, and so on.
That's a type of reference frame. So it turns out for you to make a prediction, and I walk you
through the thought experiment in the book where I was predicting what my finger was going to feel
when I touched the coffee cup. It was a ceramic coffee cup, but this one will do. And what I
realized is that to make a prediction what my finger is going to feel like, it's going to feel
different than this, which will feel different if I touch the hole or the thing on the bottom.
Make that prediction. The cortex needs to know where the finger is, the tip of the finger,
relative to the coffee cup, and exactly relative to the coffee cup.
And to do that, I have to have a reference frame for the coffee cup.
Jeff says that after many years of thinking about the function of the neocortex,
he deduced that it must store everything that we know, all of our knowledge, right,
using something called a reference frame. But what exactly does Jeff mean by a reference frame?
Well, consider a paper map as an analogy. A map is a type of model, right? So a map of a town
is a model of a town. And the grid lines, such as the latitude and longitude lines,
are a type of reference frame. A map's grid lines is its reference frame. So they provide
a structure of the map. A reference frame tells you where things are located relative to everything
else. And it can tell you how to achieve goals, such as how to go from one location to another
location. Now, Jeff realized that the brain's model of the world is built using map-like
reference frames, not one reference frame, but hundreds of thousands of them. Jeff thinks that
most of the cells in your neocortex are dedicated to creating and manipulating reference frames,
which the brain uses to plan and think. Our brain's knowledge representation is a simulation of
reality. And this applies in concept space, as well as physical space, as was the case with grid
and place cells in other parts of your brain. So Jeff now thinks that the way we think is
analogous to how we navigate spaces. He says that the similarity of circuitry observed in all
cortical regions is strong evidence that even high-level cognitive tasks are learned and represented
in a location-based framework. To be an expert in any domain requires having a good reference frame,
a good map. Two people observing the same physical objects will likely end up with similar maps.
For example, it's hard to imagine how the brains of two people observing the same chair
would arrange its features differently. Jeff said in his book that being an expert is mostly
about finding the reference frame to arrange facts and observations. I mean, Albert Einstein, for
example, started with the same facts as his contemporaries. However, he found a better
way to arrange them, a better reference frame that permitted him to see analogies and make
predictions that were surprising. What's most fascinating about Einstein's discoveries,
relating to special relativity, is that the reference frame he used to make them were everyday
objects. You thought about trains and people and flashlights. He started with the empirical
observations of scientists, such as the absolute speed of light, and then used everyday reference
frames to deduce the equations of special relativity. Because of this, almost anyone can
follow his logic and understand how he made the discoveries. In contrast, Einstein's general
theory of relativity required reference frames based on mathematical concepts called field equations,
which are not easily related to everyday objects. Einstein found this much harder to understand,
as does pretty much everyone else. Jeff Hawkins is one of the ultimate gentleman scientists of our
age. His first major project was palm computing, which is a company that he founded. He invented
the palm pilot in the mid 1990s. And despite his incredible success in the nascent mobile computing
industry, his heart was never in it. His passion was always for theoretical neuroscience. He knew
the biggest prize was understanding human intelligence, and then using that knowledge to
create human level machine intelligence. So in 2005, he co founded Numenta in Redwood City,
in California. Numenta is a machine intelligence company that has developed a cohesive theory,
core software technology, and applications based on the principles of the neocortex. Its dual
mission is to understand how the brain works and to apply those principles of real intelligence
to create intelligent machines. Neuroscientists were publishing thousands of papers a year,
covering every single detail of the brain. But there was a lack of systemic theories that
tied all of those details together. Numenta decided to first focus on understanding a single
cortical column, right? They knew that cortical columns were doing something physically complex,
and therefore must be doing something complex. Now, last week, we had Ben Goetzel on the show,
and he was convinced that artificial general intelligence must be a hybrid of many underlying
algorithms, not a single learning algorithm. Jeff Hawkins doesn't agree. Hawkins thinks that
all the magic of intelligence could emerge from a single cortical learning algorithm.
Andrew Ng said that as a young professor, and after he read Hawkins' first book on
intelligence, he also became convinced that a simple scaled up learning algorithm could reach
artificial general intelligence. Now, what does seem to really distinguish Hawkins' ideas
is that intelligence must emerge from diverse and strongly multimodal inputs, perhaps that
intelligence is somehow emerging from the nature of physical embodiment. Now, Jeff argues that
we're the first species on Earth to know the age and the size of the universe. He thinks that humans
are the first species to be known by their knowledge and not by their genes. That's the beauty
of this discovery that this guy, Vernon Malcastle, made many, many years ago, which is that there's
a single cortical algorithm underlying everything we're doing. The Mindful Brain is a small book.
It's about 100 pages long and published in 1978, and it contains two essays about the brain
from two prominent scientists. One was written by Vernon Malcastle, a neuroscientist at John
Hopkins University. Now, Jeff Hawkins cites Malcastle as being one of his biggest inspirations.
Jeff says that it remains one of the most iconic and important essays ever written about the brain.
Malcastle proposed a new way of thinking about the brain that is elegant, a hallmark of great
theories. But it's also kind of surprising, and it continues to polarize the neuroscience community.
Now, Malcastle noted that the brain grew really large by adding new brain parts on top of old
brain parts. The older parts control more primitive behaviors, while the newer parts create more
sophisticated ones. However, Malcastle goes on to say that while much of the brain got bigger by
adding new parts on top of old parts, that's not how the neocortex grew to occupy 70% of our brain.
The neocortex got big by making copies of the same basic thing, the same circuit. He says that
every single part of the neocortex is the same basic circuit. Different parts of the neocortex
are different, not in their intrinsic function, but rather in what they are connected to. The
implications of this are huge. If we understand how one part of the neocortex works, we understand
how it all works, and how all aspects of intelligence can emerge from a single cortical
algorithm. Malcastle pointed out that the neocortex grew really quickly given the short
evolutionary time. Now, Darwin's big idea is that the diversity of life emerged from a single algorithm.
Similarly, Malcastle proposed that the diversity of intelligence also emerged from a single basic
algorithm. The difference is that Darwin knew what the algorithm was, random variation and natural
selection. Darwin didn't know where the algorithm was in the body. The discovery of DNA came much
later. Malcastle knew where the algorithm resided, but not what it did. Malcastle said that there's
about 150,000 cortical columns in the neocortex. A bit like 150,000 little pieces of spaghetti
stacked next to each other. Scientists knew that these columns existed because they all
respond to different sensory inputs, be it from a patch of skin or a signal from the retina,
but the columns are wired to different sensory inputs from the body. There's a wonderful anecdote
in Jeff's book about the last time he met Malcastle. Jeff gave a speech at John Hopkins
University, and at the end of the day, he met with Malcastle and the dean of the department.
The time had come for him to leave, and Jeff had a flight to catch, so they said their goodbyes,
and the car was waiting for Jeff outside. As Jeff walked through the office door,
Malcastle intercepted him, put his hand on Jeff's shoulder and said,
in here is some advice for you kind of tone of voice. You should stop talking about hierarchy.
It doesn't really exist. Jeff was stunned. Malcastle was one of the foremost experts on
the neocortex, and he was telling Jeff that one of its largest and most well-documented features
didn't exist. Jeff was surprised, right? It was as if Francis Crick had said to him,
oh, that DNA molecule, it doesn't really encode your genes.
So Jeff didn't know how to respond. He just said nothing. As Jeff sat in the car on his way to the
airport, he tried to make sense of those parting words. Today, Jeff's understanding of hierarchy
in the neocortex has changed dramatically. It's much less hierarchical than he previously thought.
Did Vernon Malcastle know this back then? Did he have a theoretical basis for saying that
hierarchy didn't really exist? Was he thinking about the experimental results that Jeff didn't
know about? He died in 2015, and Jeff will never be able to ask him. After his death,
Jeff took it upon himself to reread many of his books and papers. His thinking and writing
are always very insightful. His 1998 Perceptual Neuroscience, the Cerebral Cortex, is a physically
beautiful book and remains one of Jeff's favorites about the brain. When Jeff thinks back on that day,
he really laments, and he kind of wished that he would have chanceed
missing his flight for that last opportunity to talk with Malcastle further. Even now,
he wishes he could talk to Malcastle about his current ideas. He'd like to believe that
Malcastle would have enjoyed the Thousand Brains Theory of Intelligence.
So if you have many brains, who are you then?
So it's interesting, we have a singular perception, right? You know, we think, oh,
I'm just here, I'm looking at you. But it's, it's composed of all these things. There's sounds,
and there's, and there's vision, and there's touch, and all kinds of inputs. Yet we have
the singular perception. And what the Thousand Brains Theory says, we have these models that
are visual models. We have a lot of models, auditory models, models, octa models, and so on.
But they vote. And so they send, in the cortex, you can think about these columns as that,
like little grains of rice, 150,000 stacked next to each other. And each one is its own
little modeling system. But they have these long range connections that go between them.
And we call those voting connections, or voting neurons. And so the different columns try to
reach a consensus, like, what am I looking at? Okay, you know, each one has some ambiguity,
but they come to a consensus, oh, there's a water bottle, I'm looking at.
Um, we are only consciously able to perceive the voting.
Today, the most common way of thinking about the neocortex is a bit like a flowchart, right?
Information from the senses is just processed sequentially step by step, as it passes from
one region to the next. In this notion, every step of neural processing refines a representation
from the low level to the high level incrementally. Scientists refer to this as a hierarchy of
feature detectors. But as Jeff points out, even basic study of how the brain works will tell you
that cognition is an interactive process, right, depending on movement. For example, to learn
what a new object looks like, we hold it in our hand and we rotate it this way and that way,
and we see what it looks like from different angles. And once learned, we're able to recognize
entire objects from the touch of a single finger or a fleeting glimpse of a small part of the object.
Jeff's proposal of reference frames and cortical columns suggests a different way of
thinking about how the neocortex works, thinking of cortical columns as cognitive
primitives, even in low level sensory regions that are capable of learning and recognizing
complete objects. Jeff's theory explains how a mouse with a mostly one level visual system
can see and recognize objects in the world. But where is the knowledge stored in the brain?
Jeff thinks that our knowledge of objects are distributed over many cortical columns.
So when I pick up a pen, there isn't a single model of this pen, but rather thousands. I have
visual models, I have sensory models, I have auditory models and everything created in between,
right, from a rich topology of reference frames, binding them all together. And every cortical
column models hundreds, if not thousands of complete objects at multiple scales. The long
range connections between the columns and the regions of the neocortex communicate at the
level of classified objects, not features. Jeff thinks that his theory solves the age
old binding problem in artificial intelligence, which is the challenge of mapping sensory input
to discrete mental categories and how these discrete categories can be combined into a
single lived experience. Jeff thinks that the binding problem is a side effect of a flawed
assumption that the connection topology of the brain is convergent rather than divergent.
The solution to the binding problem is that your cortical columns vote. Your perception is
the consensus which has reached from the columns voting on what they recognize. The voting works
across sensory modalities. When you grasp an object in your hand, Jeff believes that the tactile
columns representing your fingers share another piece of information, their relative position to
each other, which makes it even easier to figure out what they're touching. The brain wants to
reach a consensus. Now in Jeff's book, he shows an example of an image, which can appear as either
a vase or two faces. In examples like this, the columns can't decide which is the correct object
because it's ambiguous as if it's as if they have two maps for two different towns, but the
maps, at least in some areas, are identical, vase town and faces town. They're just too similar,
so the voting layer wants to reach a consensus, but it doesn't permit two objects to be the
same simultaneously, so you have to pick one possibility. You can perceive faces or a vase,
but not both at the same time, and the process of cognition allows us to move between the
alternatives to reason interactively over time. In his book, Jeff makes the powerful argument
that thinking is simply traversing a topology of reference and displacement frames in your brain.
Jeff thinks that this reasoning is movement evolved to extend the physical world of spaces
and time to our worlds of abstract thought. The succession of thoughts that we experience when
thinking is analogous to the succession of sensations we experience when moving our finger
over an object or walking around a town. Perhaps the reason why Albert Einstein was so smart was
because of the unique topology of reference frames in his brain. His information architecture,
if you will, must have been arranged as a function of his life experiences, as well as his biology.
Traversing his brain topology allowed him to make powerful abstract inferences
that other people couldn't make. For him, it must have felt a bit like the Borg traversing
their wormhole network in the Delta Quadrant, for those of you who are fans of Star Trek Voyager.
Jeff says that learning conceptual knowledge can be difficult. If I give you 10 historical events
related to democracy, how should you arrange them in your brain? One teacher might show
you the events arranged on a timeline in a one-dimensional reference frame. It's useful
for assigning the temporal order of the events and which events might be causally related by
temporal proximity. But another teacher might arrange the same historical events geographically
on a map of the world. Timelines and geography are both valid ways of organizing historical events,
yet they lead to different ways of thinking about history. They might lead to different
conclusions and different predictions. The best structure for learning about democracy
might even require an entirely different map, a map with multiple abstract dimensions that
correspond to fairness or rights, for example. So what does the Thousand Brains theory tell us
about machine intelligence? Intelligent machines need to learn a model of the world.
Inference, prediction, planning and motor behavior are all based on this model.
The model is distributed among many nearly identical units that vote to reach consensus.
This gives us robust prediction, it scales well and it works with any kind of sensor array and
modality and voting solves the binding problem. In each unit knowledge is stored in a reference
frame and is learned via sensory-motor interaction. This means that we can learn unsupervised,
fast and the motor behavior is integrated. This is Matthew Taylor from Numenta.
This place looks really familiar but I can't remember how I got here. It's almost like someone
severed all of the distal connections between the pyramidal neurons of my neocortex.
Many years ago, Numenta used to refer to their overarching theory as HTM theory,
but now they use the terminology Thousand Brains. The HTM or hierarchical temporal memory algorithm
was a particular implementation of the early ideas of the Thousand Brains theory.
The original guiding principles of HTM were that it was a sequence memory algorithm.
Numenta thinks that every neuron in your brain is learning a pattern of sequences.
It's needed to support continual learning and critically it wasn't an artificial neural
network which they argued were not biologically inspired or at least not in their popular
configuration at the time. The core data structure of the algorithm was called an SDR or a sparse
distributed representation. This was a large bit mask. Think of it as a large ordered collection of
ones and zeros. The representation is sparse which means that it typically only contains about 1%
of ones instead of zeros and the values represented the state of neurons in different regions of
your neocortex. So Numenta really leans into this idea of sparsity in the brain and its necessity
to build any intelligent system. The reason why sparsity is so powerful is that there are
factorially many permutations of values. I mean for example there are about 175 million values
if you had four on bits in an SDR of length 256 right because it's 256 choose four. So this means
that the possibility of getting false positives is negligibly small. It's also space efficient
because those four bits which could represent 175 million things could be stored in a 32 bit
array right which is four times eight bits. The notion of similarity between these SDRs is their
intersection or their hamming distance and again the really clever thing is just how robust these
representations are to noise. You could add about 33% of random noise to both of the SDRs and it
would barely affect the overlap metric. You could also union the SDRs together and not much
information about the patterns would be lost in the mix. So the HTM algorithm needed encoders to
take any data structure and represent it as a sparse distributed representation. Encoding the
information into an SDR is an important consideration for HTM much as it is with any other machine
learning model but I was really excited when I learned about HTM because it seemed so audacious.
And each column has a connection to this input space and it has a receptive field. So each column
is connected to different bits in the input space and these are proximal dendritic connections
feed forward input into the system and each one of the cells within the column shares that
receptive field through its proximal connection. We also have these other connections between cells
within the structure and here's a third cell with another four synapses on its segment. These are
distal connections. So the cell body or the soma has got different areas of receptivity. The feed
forward proximal input comes from below and the contextual information or the distal connections
come laterally from other cells within the structure. We're comparing a biological neuron
to the HTM neuron in software that we're creating. We have the feed forward input which is the proximal
dendritic input from the input space in both sides and then the distal input from lateral
connections to other cells within the space for context. Now this HTM neuron is showing that there's
feed forward input but it's also showing that it can have one or many distal connections. These are
distal segments. Each one of these segments could potentially have one or many synapses or connections
to other cells within the HTM structure. Each one of those cells may be in an on or an off state.
So at any time if a cell wants to decide whether it's going to go into a predictive state or not
it can look at all of its segments and its connections across all of their synapses and if
any one of those summed across all the synapses reach some threshold which is configurable
then that cell goes into a predictive state based upon its connections its contextual connections to
the other cells within the structure. So what I'm doing is I'm feeding in a four note sequence
and then resetting and restarting the sequence over. So every time it sees F sharp the first note
in the sequence it's seeing it for without any context. Nothing came before it. So the algorithm
goes and looks into every cell in every active column. Only cells within active columns become
activated because these activations are completely driven by the proximal segments to the input space.
There is only one segment on the cell because it's one color it's this magenta color and
all of the cells that it's connected to are active in the current time step.
That's why it's predictive because it looked at its segment and it looked it summed up all
the synapses and they were all one apparently and that breached its threshold to become predictive.
So it is in a predictive state. I've never seen another machine learning algorithm quite like
it you know almost all of them are continuous rather than discrete and even the discrete ones
like decision trees they still assume an ordinal value on the dimensions. HTM takes it one step
further. Its representations are also distributed over the features. The only encoding rules of
SDRs were that semantically similar data should have a significant overlap on the SDRs. The encoder
should be deterministic and the output should have a fixed dimensionality and the sparsity
level should be similar across the input domain. Now the thing that was missing to some extent
from HTM was the notion of representation learning like we have in neural networks.
You have to do it all yourself in the encoder which is probably the main reason why HTM never took
off for unstructured data problems. The reason why CNN's dominated computer vision was because it
learned representations that were better than any handcrafted representations. The locality prior
and the weight sharing and the stochastic gradient descent made it computationally tractable.
The HTM neuron was inspired by pyramidal neurons in the neocortex. A neuron is receiving SDRs from
distil apical dendrites higher up in the hierarchy and from basal dendrites from the same region of
the hierarchy and also from proximal dendrites from lower level of the hierarchy or some sensory
input which represented the classical receptive field of a neuron. Now all of these neurons are
receiving a stream of SDRs and they're figuring out when to fire and turn on their on bit or you
know when they should go into a predictive state which can tell other neurons when to fire. Right
from the very beginning Numenta knew that real neurons are not simple point neurons. The synapses
on active dendrites detect dozens of sparse contextual patterns and can learn complex temporal
sequences. Active dendrites enable flexible context integration in the layers of neurons.
There are two primary phases of the temporal memory algorithm. The first is to identify which cells
within active columns will become active on this time step. The second phase once those
activations have been identified is to choose a set of cells to put into a predictive state.
This means that these cells will be primed to fire on the next time step. HTM implemented
dendrite branch specific plasticity so if a cell becomes active and there's a prediction
it reinforces that dendritic segment. If there is no prediction it grows the connections by
subsampling the previously active cells and if the cells not active and there was a prediction
it weakens the dendritic segments. Now I learned all about the HTM algorithm from watching the
HTM school series of videos from Matthew Taylor who worked at Numenta. Matt tragically passed away
last year and I wanted to personally pay tribute to him here. His passion and enthusiasm was infectious
and he'll be greatly missed by the entire machine learning community.
Well sparsity is something that doesn't run really well on existing hardware. It doesn't
really run really well on GPUs and on CPUs and so that would be a way of sort of bringing more
more brain principles into the existing system on a commercially valuable basis.
There's a large body of work on training dense networks to yield sparse networks for inference
but this limits the size of the largest trainable sparse model to that of the largest trainable
dense model. This was the case actually until relatively recently. Now since the 1980s we've
known that it's possible to eliminate a significant number of parameters from a neural network without
affecting accuracy at inference time. Pruning can substantially reduce the computation of
demands of inference when the appropriate hardware is utilized to do so. When the goal is to reduce
inference costs pruning often occurs late in training. Now in 1995 researchers discovered
that retrospectively pruning low magnitude connections worked impressively well. Later
researchers found that retraining the prune connections produced even better results or even
better still rinsing repeating the process with multiple rounds of pruning and retraining.
Other approaches explored adding connections back in at random or even focusing on non-uniform
sparsity which is to say adding the connections in where they are most needed in the network.
Jonathan Frankel who was on the show last year by the way released his lottery ticket hypothesis
in 2019 which demonstrated that if we can find a sparse neural network with iterative magnitude
pruning then we can train that sparse network from scratch to the same level of accuracy.
However as the demands of training have exploded researchers have begun to investigate the
possibility that networks can be pruned early in training or even before training. The benefit
of doing so could reduce the cost of training existing models and make it possible to continue
exploring the phenomena that emerge at larger scales. Recently several methods have been proposed
specifically for pruning at initialization SNIP aims to prune weights that are least salient for
the loss, GRASP aims to prune weights that most harm or least benefit gradient flow and SYNFLOW
which Yannick made a video about by the way aims to iteratively prune weights with the lowest
synaptic strengths in a data independent manner with the goal of avoiding layer collapse where
pruning concentrates on certain layers. Now Frankel pointed out in his recent summary paper
that magnitude pruning after training outperforms all of these pre-initialization methods. Most
of these methods effectively prune the layers not the weights which to say you can perform
similarly well even if you randomly shuffle the weights that they prune in each layer. Now
interestingly SYNFLOW and magnitude pruning work quite well at initialization time without seeing
any data. Now Frankel didn't identify a single course for why these methods struggled to prune
in a specific fashion at initialization time and thought that this is an important question for
future investigation. Perhaps there are properties of optimization that make pruning specific weights
difficult or impossible at initialization perhaps because the training occurs in multiple phases.
Now combining gradient descent training with an optimal sparse topology
can lead to state-of-the-art results with smaller networks. In the brain Numenta argues that sparsity
is key for how information is stored and processed. They also believe it to be one of the most
important missing ingredients in modern deep learning. We reached out to Numenta after the show
and their VPs of machine learning architecture and research and engineering so Lawrence Bracklin
and Subtite Ahmed told us that at a high level the biggest difference is that they view sparse
networks as a unique standalone class of artificial neural networks that mirror the sparsity exhibited
in the brain versus being a derivative of dense networks created by pruning. So not really removing
redundant connections but creating networks that are designed to be sparse. In 2019 Numenta released
a paper called sparsity enables 50 times performance acceleration in deep learning networks. In that
paper they pointed to the scaling challenges faced by the current state-of-the-art neural networks.
They said that the brain is highly efficient right requiring a mere 20 watts to operate which is less
power than a light bulb. Contrast that to GPT-3 which costs millions of dollars to train. Numenta
believe that by studying the brain and understanding what makes it so efficient they can create new
algorithms that approach the efficiency of the brain. They think that the core reason the brain
is so efficient is the notion of sparsity. A sparse network is one where all of the neurons
are not densely connected to every other in the same cortical area. The brain stores and processes
information as sparse representations you know at any given time only a small percentage of the
neurons in the brain are active. This sparsity may vary you know from less than 1% to a few percent
of neurons being active but it's always sparse. Sparsity will lead to a massively smaller memory
footprint because only the non-zero elements are stored enabling the hardware to run more
networks simultaneously. GPUs and tensor processing units so TPUs they are dense execution engines.
They perform the same computation task on an entire vector or matrix of data.
This is a wise approach when the vector or matrix is dense which is to say it's all non-zero
but in the dense environment we gain efficiency by executing a single instruction to be applied
to all of the data. This is an approach called SIMD but when the data is predominantly zeros
then a prodigious amount of computation is wasted. So if you're keeping up to date on AI
hardware you might have heard of Graphcore or Cerebrus. Cerebrus in particular actually they've
developed this epic microprocessor with 850,000 cores and 40 gigabytes of memory on board it's
absolutely insane. Not only that the chip has been designed to support sparsity from the ground up
the Cerebrus cores never multiply by zero. The scheduling operates at the granularity of a single
data value so all of the zeros are filtered out and this in turn provides a performance advantage
by doing useful work during those cycles which otherwise would have been wasted not to mention
the power and efficiency savings. Now recently in a presentation they showed this graphic claiming to
achieve near linear speed up in respect of sparsity about 84 speed up for 94 sparsity.
Numenta released this paper a couple of years ago before the current sparse hardware was released
and at the time they chose an FPGA which is a field programmable gate array as the platform
to run their performance tests because of the flexibility it provided in handling sparse data
efficiently. In addition random access to memory is far more granular and efficient on an FPGA
enabling FPGA implementations to efficiently handle the unstructured access patterns in sparse
networks. Now in their paper as well as confirming the previous results in the literature about the
robustness of sparse networks to noise and variance error they also realized a significant performance
gain from using this specialized FPGA hardware. Now my intuition is that there's no difference at
all between the representational power of a discrete HTM type model which they used in their
previous generation of algorithmic approaches versus an artificial neural network. The obvious
difference is that feed forward monolithic vector space models are more amenable to training given
today's hardware. Pretty much everyone agrees that sparse networks are better but is there something
fundamentally special about sparsity? New mentors certainly seem to think so. They anecdotally point
to the brain as being sparse but the brain is probably sparse for the same reason that I don't
decide to get up every morning and travel to every city in the UK. Doesn't seem like a profound
insight to be honest. We can take it as a given that sparse networks suffer less from overfitting
because they're not going to be using their precious representational power memorizing individual
challenging or non-representative examples in the training data. It remains unknown if the
performance of the best pruning algorithms is an upper bound on the quality of sparse models.
There's actually some really interesting papers out there now like Momentum ResNet which allow
us to train huge neural networks with a small memory footprint but the key question is are
sparse networks functionally better than huge densely connected neural networks? Researchers
from Google released the paper rigging the lottery making all tickets winners back in 2019.
The rigging the lottery algorithm starts with a randomly initialized connection topology and then
layer by layer adds and removes connections densifying the layer and then sparsifying again
using a traditional weight magnitude heuristic. The algorithm achieves higher accuracy than all
previous techniques for a given computational cost at all levels of sparsity and then scored
higher accuracy than the dense to sparse algorithms. Now Momentum say that they've also produced a
similar algorithm although as far as I know it's not being made public yet so I assume it's pretty
similar to Google's approach as they pointed us in this direction but unlike traditional dense to
sparse iterative magnitude pruning Google's algorithm allows the topology to grow also during
the optimization which can apparently help overcome some of the local minima. Obviously the most
accurate sparsity algorithms required at a minimum the cost of training a large dense network in
terms of memory and computational horsepower but that approach has serious limitations right the
size of the sparse model you can learn is strongly bounded on the size of the larger dense model
which you're sparsifying from as a starting point it's simply too inefficient to waste computation
on so many parameters which would end up being zero anyway. Google's algorithm seems unequivocally
better than dense iterative magnitude pruning which is somewhat surprising to be honest given
that it's uh it seems to be doing the same thing but only one layer at a time. Now Training Mobile
Net 1 and 2 on ImageNet with this form of sparse training was instructive it was possible to train
a sparse network with nearly the same accuracy in about 30% of the compute time it was also possible
to train a large sparse network which was 5% better accuracy in roughly double the flops of
training the original dense version. Today's neural networks have something called the point
neurons it's a very simple model of a neuron and by adding dendrites to them at just one more level
of complexity that's in biological systems you can solve problems in continuous learning and rapid
learning so we're trying to take we're trying to bring the existing field and we'll see if we can
do it we're trying to bring the existing field of machine learning commercially along with us
you brought up this idea of keeping you know paying for it commercially along with us as we move
towards the ultimate goal of the true AI system. Now Numenta is working on some really cool stuff
behind the scenes unfortunately there's very little information about it in the public domain yet
Jeff's main aim is to realize the vision of the 1000 brains theory in an efficient computational
algorithm sparse networks are just a tiny part of this vision the next step is implementing
continual learning with active dendrites and this essentially means that they need to be able to add
new synapses and train them independently of the existing ones this is going to require a sparsified
version of back prop which will also require specialized hardware and algorithms to implement
they also mentioned to us that they want to exploit activation and weight sparsity simultaneously
mirroring the neocortex anyway next week on street talk
another book that I read around the same time that had a big impact on me
and and there was actually a little bit of overlap with Jean-Pierre as well and I read it around the
same time uh is uh Jeff Hawkins uh on intelligence which is a classic and he has this vision of the
mind as a multi-scale hierarchy of temporal prediction modules and these ideas really resonated
with me like the the notion of a modular hierarchy um of you know potentially um of
compression functions or prediction functions I thought was really really interesting and it
reshaped the way I started thinking about how to build minds let's kick off with the main show I
hope you enjoy it folks well I fell in love with brains actually when I read an article by Francis
Crick who was one of the co-discoverers of DNA and Francis had uh later in his life turned his
interest to neuroscience and he wrote this uh so essay that appeared in Scientific American
where it was sort of um the emperor has no closed type of essay he said you know we have all this
data about the brain and it's really wonderful we collected all this data and we've got decades and
decades but no one has a clue what the hell's going on and um and he says you know we need new
ways of thinking about the brain we don't really necessarily need more data and and that just
struck me I was 22 and I was like oh crap you know that's just a puzzle we have this we have
these pieces and someone has to put the pieces together and that seemed like something I would
be good at or at least I would enjoy and that's gotten me going that was that was the thing that
just I said I'm gonna make a career out of this and then very quickly I realized that well you know
this is the long thing that's gonna take a long time and uh but if we do this um then if we really
figure out how the brain works and what it does then we will have real big insights into how to
make intelligent machines and so I said oh my god the implications are here it's not just from a
neuroscience but from an AI point of view and so that got me going uh on this on this journey
and at that point I decided to change careers from engineering to neuroscience uh computer
engineering computer science to neuroscience and start all over I just got my degree from university
so I was like oh starting again here we are 40 years later it's been a long journey though as
you probably probably know there's been a lot of twists and turns to absolutely absolutely well we
really enjoyed reading your book but um I wanted to talk a little bit about some of the tribalism
in the machine learning community so I've been doing a bit of research online and um your mission
right now is to try and convince other people of your ideas you've got this incredibly exciting
idea of the brain and as you just said that in some level of abstraction the brain is infinitely
complicated but actually if you think about it in terms of simple rules that can produce a lot of
complexity it's not that complicated but you know I've noticed that when you speak to some of the
machine learning folks they are very quick to dismiss your ideas and they say well there's
there's no material difference with monolithic neural networks with point neurons and backprops
and backprop and you know it's good that people are so passionate about what they believe in
but by the same token it means that science only advances one funeral at a time yeah um
it's a very complex issue you bring up here and um I think our mission isn't to convince people
our first our mission was to figure out what the hell's going on in our heads that was the first
thing we had to do and we've made a lot of progress on that now we have to sell those ideas we have to
sell them to neuroscientists we publish papers I speak at conferences the people side of work they
test it and so on that has to happen it takes time um then uh then we now have a roadmap we can see
we can now I from the position I'm in and some of the people work with me we can see where the
shortcomings are in current AI techniques uh we can we can see oh the brain's doing it differently
it's doing it this way and we have we can have two approaches to go forward we can say well we need
to convince everybody else which is not really a fun thing to do um or we can we can just put
our ideas out there document them show people them and then work at doing this ourselves like
just demonstrate it start building things um making things that work making things to solve
problems that other people have been struggling with so we're doing a bit of all these things
we're promoting the neuroscience theory we're promoting um these ideas in the machine learning
community and we're also implementing this stuff uh because in the end you if you don't if you you
can't implement it unless you understand it so it's a good test for us too um and you would wish
you know everyone wished like oh wish everyone just you know agreed with us where everyone you
know got what we understand but in reality this is a big world there's lots of big middle um
and there's a lot of um there are people who want to dismiss new ideas um but that's just the nature
of the of the beast right we just have to accept that um science is not just science you have to
promote your ideas as well as discover them and the same is true uh in the machine learning world
well what one thing that's kind of interesting about that you know philosophical aspect that
you have of look it's a big world there's lots of people doing a lot of different things this
that's kind of the heart of evolution right is that there's variation people taking different
approaches and and natural selection will kind of sort out you know which ones work and don't work
and that's one thing I've always thought about a lot of the folks that don't want to pay attention
to what the brain does is they don't have a sufficient appreciation for how much you know
let's say work has gone into designing the human brain right I mean a billion years of life uh
however many hundreds of millions of years of intelligent evolution so certainly we can learn
things from the neuroscience right I mean and and of course that was the original inspiration
for the artificial neuron but that was just one tiny simplest possible abstraction of a neuron
that happened what fit and I don't know what 50 years ago right there's gotta be more we can learn
right but yeah but you know the way I look at it is we're all trying to reach the same angle
we're all trying to figure out how to build intelligent machines they're truly intelligent
we all read they're intelligent uh we can talk about the applications of that um and I don't
think there will be multiple ways of doing this there's like in in the end there aren't multiple
ways of building computers they're all some sense universal turning machines and then we have variations
on that um and so I think that's what's gonna happen here too now our priority how would I know
that engineers couldn't figure this all out just by thinking about and doing engineering stuff um
you know my guess was that we'd have to figure out the brain works first uh and who knows I
could have been wrong that was a bet all right maybe people would figure it out but here we are
67 years later in the field of AI and I still I think today's AI systems are still incredibly
limited they don't really do much at all with like it's intelligent that's my opinion um they're
very very restricted they don't generalize they don't create behaviors they don't it's just so many
things that we're so far away and now we took a long time and now we've figured out a lot about
how the brain works so now we have a roadmap so um I think it's gonna be a lot easier it's now it's
not just like oh I think it's gonna be quicker to study the brain and other people say no I don't
think it will be I said well we now have some things we can we really understand about the brain
we don't have to worry about that anymore so um the approach we took has been fruitful um and um
and I should also say just be clear today's AI is really useful so I'm nothing against it yeah
it's great um it's just not intelligent and I want to build intelligent machines yeah it's
quite it's really fascinating to get to talk to you uh your book on intelligence is actually one
of the very first books of AI I ever read so uh it's your fault that I'm part of this field now
I was so bad when people tell me I said I hope it worked out okay for you
no it's been it's been a wonderful journey and I'm really glad that it was one of my first
exposures to the field in a way it was so full of like really interesting ideas especially for
that at the time seemed very revolutionary um and it very much shaped shaped my thinking
so I want to ask you a little bit um it appears to me as you know someone who works
sorry I work with deep learning you know a lot of course nowadays it feels to me that a lot of
your idea seemed uh validated in a lot of ability practices um a like a currently kind of like a
merging trend is how um with like these me have heard of such like these transfer models that
have become very popular recently that scaly these larger it's just stacking these transformer layers
seems to currently at least increase performance in like a you know in a pretty consistent manner
and I wonder well it feels like that it kind of validates in a way it's like a different
proposal to what you say about like a chordically uniformity do you think there's any kind of
connection ah you know I don't think about I'll answer your question but let's just say
I'm always forward thinking I never look back and so I don't worry about hey what those I write
about this or not or just say you know just let me get credit for this or that I don't really care
oh it's like let's go forward what's the best thing we can do right now and um and so you know I
think there's you can obviously I said really we're all going to converge on the same thing right
eventually I think so ideals become you know I got ideas from other people other people get
ideas from me the history of how that milieu of ideas travels it's very difficult to point to
um so I I'm very reluctant to claim precedent anything I just feel like hey you know if we're
all moving towards the same idea that's great I don't think transformers are an implementation of
the cortical column or the cortical algorithm idea we're not there yet because we now we now know
what the core of that algorithm is it has to do with movement and reference frames and transformers
don't have that in some sense they have a very primitive sort of um attentional movement if
you want to think of it that way um but nothing like kind of movement and reference that we talk about
so um so it's a little bit in the right direction um and of course they're really impressive um but
it's it's not it's not close to what I think we need to get to yeah it's really fascinating though
because transformers are strange beasts they perform a kind of information routing and it's
quite esoteric exactly how they work and I think in in that they in some sense they're similar to
capsules and I know capsules have been equated to some of your work but I wanted to talk a little
bit more deeper than that though so um a lot of your ideas from a machine learning point of
view come down to the fundamental dichotomy of discrete representations versus continuous
representations which is what we use in deep learning we use vectors and there are some
advantages to using these continuous vector spaces namely that you get for free some spatial
priors so you can encode semantic similarity as a function of how close they are and yeah and also
you have gradients which are useful for stochastic gradient descent and interestingly the manifold
hypothesis states that most natural data falls on very smooth and low dimensional manifold so all
of the human faces could be projected onto a low dimensional manifold but um I know that a lot of
your work has has been really solely focused on on these sparse distributed representations
and then um building on top in in the encoding for example so if I'm encoding a date representation
I would do it such that this sparse distributed representation would have a significant overlap
for date times that are in the vicinity yeah and so semantically similar things should intersect
each other and spatial relations between bits can be encoded using a receptive field similar to
you know how it works in in cnn so um but nementa has asserted that intelligence wouldn't be possible
without these sdrs and they're the primitive of of intelligence so what do you think about the
kind of comparison between vectors and and these um discrete representations that that you're an
advocate yeah well let's just start you know the whole idea of these sparse representations comes
because that's how brains work I mean that's there's no compression value uh if you're looking
at brain you look at any population of cells that are representing something at any point in time
most of the cells are inactive it can be anywhere from you know 90 percent to 99 percent
of the cells are inactive you just don't see of the information carrying neurons or some
neurons are far away all the time we're not talking about those but the ones that carry
information you just do not see any kind of dense representations that is a that when you when that
happens you're having a seizure um and we also see that in in in real neurons that they're not very
high fidelity they don't carry individual spiking rates are very crude um there's many instances
where even the single the first spike is actually the most important thing regarding information
so we're not like there's no so on a high resolution you know four bits or three bits or even even
often one bit of precision and firing up a neuron right so this doesn't exist in the brain um so we
just take that okay well we need to understand why that is and it's not just it's like more efficient
from an energy point of view that's not what it is um it turns out that um sparse representations
have a lot of really desirable and interesting properties that you don't get with dense
representations um we can talk about those um it's not like everything is um there's a spectrum
here right there are some parts of the brain where you have representations which are a little bit
less dense maybe tempest out of the neurons are firing and there the activity rates really do matter
they're still not high precision but they really do matter and then we believe there's other parts
where they're very very sparse i might have five thousand neurons and let's say two percent are
active so a hundred active neurons and all the information is encoded almost all the information
is coding in the population code not in the firing rates and we can talk about that if you
want to dive into the details what the advantage of sparse distribution representations um but it
is an empirical observation of the brain so you know we'll start with that it's okay well why is it
that way you know well one one thing i wanted to understand is because you made the comment before
that um they could improve generalization but intuitively they're they're the opposite of
improving generalization because they have such an incredible specificity so for example when i
was reading through some of your literature when you use boosting for example in your algorithm
it increases the specificity to such an extent but then the way we generalize in our cognition
isn't necessarily through the representation it's through the abstractness of the concept we're
learning would you argue yeah so oh no i'll be honest with you i've changed my opinion about this um
i used to believe exactly what you're saying which is like oh these overlaps between the
sdrs are really how you're gonna get generalization we now realize it is a much more powerful form
of generalization um i talked about it briefly in uh my new book um a thousand brains i don't know
if we really discussed it in any of our papers yet um and this is the idea that you're so i can
switch into that so i'm i'm i'm admitting right yeah i don't think it's what i said before okay
all right let me learn um but there's a much more powerful form of generalization if you want i can
delve into that right now um uh it's not okay so so we have this is what we've learned is that each
core of the column we can say the brain in general is um it's building this sort of uh the model of
the world you can think of as a graph but it's you can literally think of it as like where are things
relative to other things it's like a computer aided design model right CAD model it's literally
doing that you have a model of something like a computer or the house or room or bird or ideas
that they're structured using reference frames and data populating reference locations and reference
frames and um so you can imagine uh my knowledge of something i again as you know in the book i
use the coffee cup example quite a bit but you can use anything um a knowledge of say a stapler
is is it has a certain set of parts uh which are then arranged in a certain relative position
for each other and they have certain movements related to each other and um and that structure
is is that graph if you will is the uh is the definition of that object and that's your that's
your model of it i tell you i tell you represent it in your brain now when you when you learn it the
way you do this is you attend individual components one at a time so if i if i were to look at a new
object i haven't seen before like a stapler i would just attend to one end or look at what's what's
this thing look like then what's that thing look like and as you attend to these different parts
you're essentially building this graph you're essentially building the model you're saying okay
this component i recognize that that's like a hinge this component over here looks like maybe a
button this component here looks a rubber pad and i and i just start building this model in my head
but it looks my phone my phone look there um i so building this model okay so you got this i'm
trying to paint a picture in your head how you you're doing this every moment of your life as you
look around even like you're looking at the screen right now you're looking and see different things
you see where they are relative to each other i think most generalization in the world comes
about from a different from the process of if i see something new that i haven't seen before
or i'm experiencing something new it doesn't matter revision but just imagine you're seeing
something new you don't recognize it you don't see this arrangement as a familiar arrangement
then you look at a subset of the components and you say okay this is focused over here and you say
well these three components over here look like something i've seen before these look like it
might be a button that i pushed down and this thing over here looks it might be a hinge um and
then you then you say well if it's a hinge little button with this thing even i've never
seen it before maybe if i push in this button the hinge will look and and and so you you basically
your your generalization comes about because subsets of these graphs that you build of the
structural model are similar to subsets in other graphs you've learned and you can say well these
subsets are similar therefore the behaviors associated with those subsets are similar
the performance of those subsets are similar so i'm now going to describe to the stapler some
things i've learned about other things um and we keep and if i see something really new we just
keep going down to swirl and swallow pieces until we find something i recognize this next okay well
i thought i recognized and and so that i believe uh is actually the primary way we generalize in the
world um we can talk about how sdr's do that do that too but anyway what you're talking about it
sounds a lot like sort of some of the graph isomorphism you know work that that people talk about
where if you if you've got this new thing and it has all these parts and they're in a graph
you know certain subsets of that graph may be isomorphic to previous you know graphs that
you've seen and and then therefore you can learn about it um and you said something really
interesting which is when you see something new you know you attend to kind of different parts
about and you're doing kind of a remapping there and there's a lot of research on on grid cells
right that they remap so when a rodent goes into a new a new environment it remaps the grid cells
can you talk something i mean is this related to the remapping and i'm really curious about
how does that remap happen in the neurons and how quickly does it take place you know yeah well so
one of the the the hypothesis we have thousand frames theory is that well first of all we deduce
the idea that you need some that there is any in the cortex there are these reference frames and
these structures these graphs right we can deduce it it's like okay well i know this is happening
i walked through the logic behind that and then it was at first to me it was very odd like how could
neurons build these graphs right how and how do they structure them i mean it what with the way
i used to focus is like my cortex knows where my finger is relative to the thing it's touching
like relative to the thing it's touching not relative to my body and and so i use the example
the copy cup and the copy cup moves around so the reference frame has to move with the copy cup
i was the holy crap how does that happen it's really good for neurons to do this and then of
course we looked into the literature of the hippocampal complex which is grid cells in the
entorhinal cortex and play cells in the hippocampus and there's a whole bunch of stuff and there's
there's a 20 or 30 year history of research in this field it's not in the neural cortex but it's
something that's very related to the neural cortex and and so and that's what they it's
clear that grid cells and play cells and these other types these vector cells that exist there
are building maps just like this and so we said okay well it's likely then that the
same mechanisms that are used by grid cells are probably going to be used in the cortex
and the time you made that that was a purely evolutionary argument it was like saying
it seems unlikely evolution is going to discover something like this twice it's really hard to
do right it's really really hard to do this so and animals have to know where they were in the
world a long time ago so they probably evolved this complex mechanism from nowhere and where
they are and now the brain's going to use that same mechanism for knowing where like your eyes
are or your fingers are or yeah it's going to be the same mechanism for everything else and so we
just said hey it's probably likely that the same basic mechanism that uses grid cells and play
cells and vector cells is going on in the cortex the time we made that that prediction we were not
aware of any evidence of that was the case and now there's a lot of evidence for it now people
are finding grid cells throughout the cortex so so the general idea I won't keep getting to your
details in a moment but the general idea that hey the same mechanisms are work in both places
seems to be right and and now we don't understand those mechanisms completely even though there
are 30 years of research on grid cells and play cells there's still some mysteries about them
but there's still a lot but there's a lot that's known and so we can map on the concepts I was
talking about like okay where am I where am I in this graph what's what do I place in that graph
you could think of that a little bit like play cells in grid cells you can think of remapping
is there's two types of remapping like this is a whole new thing so let's just start with a new
graph or versus let's fix the graph I already got you know and and that's that's very much related
as you said to remapping grid cells not that we understand that completely we don't know it does
but we know a lot about it it's a very very technical field in the neuroscience literature
right I've been fascinated by Douglas Hofstadter he has a really interesting idea on cognition
and he says that cognition is a little bit like the interstate freeway
in the sense that we have this ability to make analogies and this is something that current
AI systems cannot do and a lot of the symbolic folks are saying well of course deep learning
models can't do it you're just interpolating on a manifold for goodness sake but your conception
of reference frames I found fascinating because they're not just on your sensory inputs there's
you actually said that thinking is a little bit like traversing through concept space through
these reference frames and that's absolutely fascinating so but the thing is when we learn
information many of us learn information differently but we learn the same knowledge and
the same facts you know we all know how to speak the same language and you did allude to this in
your book that there are some interesting differences and the topology of how you learn
information but we still seem to be able to reconcile it in the same way yeah well sometimes
you know it's it's like the way I view it again we're telling something some some of this hasn't
been published anywhere yet um but the way I view it is um if I think of myself as part of the
cortex as a cortical column um I don't know what come my input represents and I don't know where I
am in the world I don't I just don't know anything I'm just a bunch of neurons right and so I have
to discover while I'm getting I get two pieces of information I get some sensory data and I also
get information about how my sensor is moving in the world yeah we can talk about how that happens
but I get I get some knowledge and so I have to figure out on my own what is the dimensionality
of the space what kind of space is it I don't have a preconceived it's like oh the world is
three dimensional I don't know that um I don't know if it's two dimensional three dimensional or
n dimensional I've just kind of figure out and I have to figure out what is the structure of
which I'm going to to place this information and when you and I look at a chair we both see it as
three dimensional we it is a three dimensional object we're going to build three dimensional
reference frames that chair they're going to be pretty similar it's going to be hard for you to
have a representation of the chairs it's significant significantly different than mine um but when we
look at things that we might not sense directly something that someone tells me about or I'm reading
about in a book the kind of reference frames you and I might create for the same facts can be very
different and and so we can arrange the same data in different reference frames because we can't
directly sense it we're we're basically relying on information coming from other people and so we
can um but up with like the same facts represented differently and reach to different conclusions
and this is why we have different beliefs about the world it's always about things we can't sense
directly if we can sense them directly then we generally have the same belief about it you know
the same kind of structure so I think this is a fascinating idea um and um uh but it's it's really
fascinating to me is that there is no pre a notion about the dimensionality of the world that a column
is looking at and and the data so it can represent almost any kind of data as long as you can get
figure out from a movement vector in a sense of human data and sense data it can say can I build
a map of this thing that makes sense the predictive map and um uh and if it can I said okay that's
my that's my understanding of it another another beautiful thing about it is is the fact that
your brain is is creating these binary encodings at all like I when I was researching this I found
very interesting that for grid cells there is some some topography at least in the visual
cortex or in the visual layer I'm sorry so you know grid cells that are nearby each other
will have like a similar orientation but maybe a different scale but once you go up a level higher
so those things create the s dr you know they create these bits that are very sparsely populated
and then the place cells correspond to combinations of those those bits to indicate a certain location
and once you're at that layer there is no more topography it's like now they've become
truly digital encodings of information right well I'm not sure I'm not sure I understand that
because it's like from my understanding there's always going to be a topology you're never going
to lose it uh you may not see that topology in the play cell right um all right that's that I mean
this the neuroscientists will listen to white listen to this I don't I don't want to insult them
because I'm gonna make this very simple I say you can think of like place cells are like what and the
grid cells are like where okay so um and you can say well this thing the one thing is that this
location type of stuff and so if you just look at the play cells you don't this isn't because they
actually do uniquely encode places so this is it gets a little confusing here but um uh but you
can think of it just generally like that right um if we're going to go deeper about this we're going
to start getting into all the unknowns about grid cells and play cells and they're very interesting
sure um the unknown and weird properties um we actually think now that um this is work that
one of our employees Marcus um is doing is we think actually that the the better way to think
about a coding is not a location is not actually through the grid cells but through these vector
cells things things like optic vector cells and and so on um it's it's and the the grid cells
actually might just play a very interesting singular role they might play the role of what
we call path integration which is like if I'm moving in a certain direction I have to be able
to predict where I'll be uh as I move that's called path integration I'm gonna predict something I have
to know where I'm gonna be when I get there um and that most of the max the graph itself is not
built up of grid cells it's built up these objects these more like these polar coordinate cells
called vector cells um so again this is an evolving field and you know and even stuff we
wrote a couple years ago we're like oh maybe that's a lot of light it's not like this but the overall
principles are the same it's still a graph it's still you know data at locations in this graph
um that hasn't changed uh it's just like how's the graph actually constructed that's a little
tricky right I mean it's like it's fascinatingly deep it's just a fascinatingly deep field but if
I can just tie this back to SDRs because I know there's there's always a lot of really confusion
or uncertainty about the value of SDRs and just for our listeners' sake I wanted to share one
intuition and get your take on it which is if we are representing let's say location uh in some
space yeah if all we need is 20 bits to do that so we really only need 20 bits of information to
represent a space location to the resolution that we need then if you go further than that
if you have 10 000 bits and you decide ah I'm not going to use 20 I'm going to use all 10 000 put
like you know little 0.01 values kind of all throughout there really you're just creating
noise and that other space that you could have been using for other purposes right and so I think
that's one advantage of the SDRs like what do you think about that intuition that's a little bit
correct I I I haven't thought of it that way um let me give you some let me just give you my take
on some of the advantages of SDRs so let's say I'm going to represent something I have 5000 neurons
I'm going to represent still right so let's just talk about the capacity and let's say I'm going to
use a 2 percent sparsity so I have 100 active neurons and 5,000, 9,000, 8 active neurons and
first you can say what is the capacity of the system how many different representations you can
represent so that's that's just uh 5,000 choose you know 100 and it's you know gazon gazon gazon
so then you can say okay there's no representation capacity you then you say if I randomly chose
these SDRs um what is their overlap well they'll overlap by about two neurons each
because each neuron is going to participate in every 50 patterns um but not by a lot more you
won't find two patterns randomly overlapping by 50 bits or 30 bits it's all statistically impossible
so you can now you have all these these patterns you could pick randomly all day long and they're
not going to overlap so they're very unique each one's going to be extremely robust to noise so you
could add 50 percent noise to any of these patterns and they're still going to be recognized correctly
okay so very noise robust is another um thing so high capacity uh representation very noise
robust there's another property that uh we think is being used everywhere which is really interesting
and I don't know if there's any equivalent to it in other types of network is what we call the
union property where I could invoke um not one pattern but two three five ten poundings at once
and so now instead of having a hundred neurons active I say I broke ten patterns I'm gonna have
a thousand neurons active thousand out of five thousand well turns out that all the processing
that you do still works on all those you can process them in parallel this that even though
you're mixing these things together the other ends the the patterns that are detecting these patterns
don't get confused and you can show this mathematically so instead of representing probabilities
like a probability distribution you might think about the brain often relies on this union property
which is you know there are multiple possibilities right now it's not really a probability distribution
it's just like these are all possible things that could be happening right now let's process them
all in parallel and see which one works out a superposition that's the kind of property that
yeah I guess you could call that yeah um and you only get that the sparser you are the more of those
you can do and the whole thing becomes so you got this huge capacity super noise robustness um it
only takes you know if I want to recognize one of these patterns of a hundred neurons I only have
to recognize I only have to connect to 20 of them so so neurons typically don't really detect
they look for maybe up to about 20 or 30 synapses at maximum um at a time so so these are other
properties all these properties come about yeah since you did mention capacity though I would
like to because I did see you know some variable statements sometimes I've seen no problem whatsoever
with capacity mathematically we've got tons of it but I was listening to one of your um research
calls I guess and you started to question you know capacity I think at least in the maybe the
the grid cell you know the interrinal cortex or or some such so I guess I have two questions one is
is capacity a concern or not a concern and also uh in order to make best use of the capacity I'm
sure the brain must have some type of entanglement so you know machine learned artificial networks
we know that they entangle representations so the you know there's like weight sharing between
different representations do you think something similar is happening in uh biological networks
well I don't know uh the the machine learning uh field that you're talking about uh and so I can't
really comment on this idea of entanglement of weights in um machine learning um I'm not familiar
with it there um I do I want to maybe correct something about sdr's which relates to the
question you're asking uh we think in some parts of the brain some parts of the intercortex these
very sparse representations work as I just said there are other places where there are less sparse
maybe 10 sparsity uh grid cells is a great example uh if I actually look at a population
at grid cells you know it's not like 1 sparsity it's more like 10 sparse and and and there the
activation levels actually do matter it's not like they matter like three digits of precision
but maybe one digit of precision and um and they and so that doesn't have all the properties I was
just talking about um when you get to like 10 percent you can't really do a lot of superposition
patterns so it looks like in some parts of these algorithms there are sparse patterns that work
on some principles and you know and in other parts they're very very sparse I work at other
principles it's not everything is super sdr's you know I used to think that but I don't think that
anymore um I just became an amateur with sdr's I said oh my god this is really cool you know how
these things work um but I now we now know that there are in some parts of these algorithms you
need to have um it's still sparse but more dense and activation level matter and then some of the
capacity issues uh you don't have less capacity advantages and so um and that's clearly true with
with grid cell representations um but I I imagine it's true at other things too I know it's true
in other areas too so it's it's it's one size fits all here um I do I do say we could say
certainly that you don't find anywhere in the brain or anything's represented by full act of you
know uh fully dense uh networks where all the neurons are free to develop that this does not
exist it doesn't mean this couldn't be useful and it doesn't mean that we couldn't in the future do
something like that I just seems very unlikely to me uh I know that's how most uh you know
convolutional nor that works with deep one of that works work today um but um right but you
probably know there's a lot of there's a lot of effort going on in sparsity right now in the
classic machine learning community people try to do it and we're working on that too um so so
I think there's something fundamental about the representations we use when we build AI algorithms
there's a dichotomy between you know the good old fashioned AI people they they work at a
level of abstraction higher they try to implement the mind and us connectionists we're trying to
implement the brain I think there's something fundamental about that level of abstraction
that we work at so there's a discussion between people like Scott aridson who think that computation
is raw and should happen at the lowest possible level so just think of how neural networks work
they distribute knowledge don't they across uh lots and lots of different uh neurons and then
there's um uh people like christa schwami and he's a computer scientist and and he thinks that we
should be dealing with much higher level computational primitives like functions and types so um
I guess from your perspective because I I feel that all of this is strongly emergent I feel that
we could work at the very lowest primitive level of computation and all of the magic emerges
do you think we should be doing that or do you think we should be working higher up the stack
no I well I think we have to work in higher up the stack um I mean of course you can break any
system down and say it's just a bunch of atoms it's just a bunch of molecules right it's just a
bunch of cells um and that's true and you can shouldn't dare you should be able to understand
that but just like thermodynamics says it's not very useful to think about liquids as a bunch of
atoms right you you need to have something a higher level than that and and so you know what I guess
the principle we that we assume that we just we have now in some sense discovered that the brain
uses this representation scheme using reference frames and movement now I think that's the right
level to think about it I don't I you could try to abstract it less than that but what why bother
that is the unit that we're talking about here we use models that are well but okay that's that's
that's the thing though but um the brain was evolvable it evolved within biological and
environmental limitations and and go fi people say well you know what scrap that let's just implement
the mind directly you know and they think that um functions and types um they're not an imaginary
concept that have been invented by humans they think that mathematical knowledge has it's
universal and it's been discovered by us and we would be silly not to implement it directly in
this higher level language because if you think about it even um the knowledge that we have you
wrote about this in in your book it's fascinating that now there's a separation between our genes
and the knowledge we've created as as the human race and all of this stuff now is is is emergent
our society is emergent so why not implement it directly but why trust because you'll because
because you're gonna you're gonna you're gonna guess wrong right you know you're gonna say okay
well I will figure out what that structure is that what you know what is that mind structure that
we're you know and and so for example most of the you know the good old-fashioned AI never considered
movement in their in their in their fundamental aspects of representation space right um and
you know or even deal for what dr. lennard did was psych you know same idea right um yeah and so
but but now I know okay the brain builds these structures using movement so it it's not like
it was a wrong idea of the old AI people it's just they just didn't pick the right schema
it was the wrong schema well could could you expand on that because the thing is um because we've
got a go-fi person we're very we're very good friends with and he is convinced that there is
you know like the knowledge engineering bottleneck and yeah it's it's very very brittle and it didn't
work very well but um but you're reading your chapter on knowledge representation I think you
were saying in our brain it's it's like lots of models when we model a stapler it's actually
lots of little models in our brain and it's it learns a kind of um it analogizes the knowledge
in a really sophisticated and distributed way so when when we want to know what happens when
we press down on the stapler we we run a simulation in our mind you know what what happens if I press
down and what happens if I stretch it and our models seem to um generalize to any version
of a stapler we might ever find yeah that that almost convinced me that we do need to have a
lower level representation of knowledge lower than what lower than the the reference frames
I was talking about well low so the go-fi people say that we should use um functions and types and
relations to describe knowledge but do you think that's possible I don't want to misrepresent go-fi
because they haven't looked at that stuff in a long time um I think you can't do this at the
level of the neuron I mean obviously brains are made of neurons so but you know they're not uniform
neurons we have dozens of different types of neurons are hooked up in very complex ways they
do different functions it's not just one big neural network yeah that structure is important
and so uh I think it's now become evident to me and it wasn't this way five years ago
it's really evident to me now that at least in the brain knowledge is represented in this structured
form uh with reference frames and movement it's the same inspiration that the unit has with with
capsules but he didn't incorporate movement as part of that structure um you know that with that you
know but now I now I understand okay well at least from a brain's point of view these reference
frames are constructed as I mentioned earlier they're discovered through through movement
observing movement of the of the sense within the world and um and so now we have this the the
basic structure of if you want to call it will fi is is these uh movement-related um
reference frames and placing you know and stuff like that I can't I don't know how I would do it
less than that I don't know what how I want to get rid of that now what do I do just ignore that
and go back and just put a bunch of neurons together I don't know do you mean could you
come up with a genetic algorithm that could have discovered this using just traditional neural
network sure maybe you could have you know I don't know what's the fastest way of getting there
but now that we know it why would I want to do something different I mean I maybe I'm just
understanding your question try well I think the the the main issue is that we don't understand
the representation you know if you look at the open AI microscope and you can visualize all the
different neurons and so on these objects have been distributed in a way which is completely
ineffable across all of these different neurons yeah and is that the most high fidelity possible
representation of knowledge I well I don't I don't know but well I will I will say now
this I currently believe very strongly that um in the future when we build AI systems they're
going to work more like on these frame structures and they won't be this highly distributed mysterious
blobs of neurons right they're you're that's great by the way it's it's great uh not only
because of what they how they perform it's great because we'll understand them a whole
lot better this you know this touches on the issue of the threats of AI right you know the
so many of the threats of AI are based on the idea of what we don't know what's going to happen
well no you actually will know what's going to happen you know I designed the thing it works
like this it's what's it you know I can't put everything but I you know I don't know what it's
going to learn but I know how it's going to learn and what it's what it's capable of um so I I just
you know I don't want to debate people about this because I you know there's a lot of smart
people out there and they may have things no things that I don't know and they can pursue things
but from where I sit right now um it's very clear to me okay this is how the brains do this
this explains everything explains like you know how we structure all knowledge um and how it evolved
that's a big part of this and how you know how we have this common algorithm and it's going to be
based on its reference frames and movement and so on and so okay well why would not just just
just build that let's go for that um and you know why build something else if I understand that
why build something else I don't I don't understand the point is it going to be better
you know in the future we will certainly take you let's say I'm right let's say I'm right
and we build machines that work on these principles that I outlined in the book well of
course we can vary go go for that we don't have to stick to the way biology I mean we'll come up
with a better way of representing things and structures and reference frames maybe it won't
be movement based maybe it'll be something else I don't know but could I try and do it build
something and get it working why would I go away from it you know yeah go ahead to try and steal
man the gofi people it's because they think that knowledge is universal we discover it there is
only one representation of knowledge and clearly in the brain you can learn things in different
ways you can I can teach you a curriculum and I can teach you it in a different way and maybe in
some weird way our brain does learn some latent representation of the knowledge which is the
universal knowledge but they're making the argument that why don't we just represent it the only way
it can possibly well first of all that's not you know it clearly isn't true I give it just like
you just said I give the example in my book about taking a bunch of historical facts and
arranging them along a timeline and arranging them on a map and you end up with different
inferences and different beliefs about the same set of facts so whether there is a universal
truth I don't know but clearly humans don't know that universal truth we we can't know everything
we can only sense the small part of the world and and and you know the vast majority is invisible
to us so who the hell knows what that universal truth is um what we can do is build good models
and uh and the models but there isn't a universal model there isn't one correct model for for history
right um we might like to believe there is my model's right and yours is wrong that kind of thing but
you know it's really easy to point out even non-controversial things like the timeline
versus the map will lead to different models and the order in which you train someone will matter
you know so I think we just have to live with this um and I don't think you know maybe there's
some platonic you know go-fi people thinking of this in platic universal knowledge it's correct
all time I don't know but certainly I don't think that's accessible to us if there was we just we
just we can't we can't sense the world both physically and time-wise we our senses only deal
with a teeny part of the world uh it's a good part but what not be a little teeny part
we don't even know what the universal truth about space and time is I mean
definitely true it if I could move on to some other questions about the brain um so you of
course felt it's very much on new cortex as the you know seed of human intelligence and
I think that I think you're definitely on the right path with that I think most people agree
something similar to that but there's also lots of other parts in the brain that see
pretty important and I'm specifically uh interested in so like what thing that machine
learning people are of the interest that it is the dopamine circuit so like the cortical basal
ganglia the harmica cortical loop and what seems really it seems like a learned signal it seems
like some kind of learning is going on there could you maybe say some words about that yeah so
obviously the neocortex is connected to the rest of the brain in my ways um myriad ways um
it's it's not in isolation and it's a very complex relationship so lots of other things
that doesn't mean you can't understand what its circuits are doing
but in a human the neocortex is complexly tied to things and one of those things is that if you
think if the neocortex is as this sort of map of the world or model of the world well it what
should it learn and what goals should it have the dopamine circuitry is equally associated with
what should we learn when should we take when should we take the effort to learn something which is
metabolically expensive um and um and so somebody has to decide that my basic intention is the
decision is not the neocortex's decision right mostly it's somebody else's decision and um
when we build intelligent machines we'll have to figure out who's making that decision and um it
won't be in the neocortical equivalent it'll be in another part of the intelligent machines equivalent
but uh there's absolutely no reason we would have to model the goals and um emotions and other
things that other parts of the brain do unless you wanted to create something was human like
right so you know i i used to joke that our dopamine signal in our models is a switch i
turn it on and turn it off i can learn now stop learning now right just manually turn it on
manually turn it off uh when we run experiments um there doesn't have to be anything you know more
complicated than that um but there's you know the only thing i'm claiming here is when we build
intelligent machines if we build them on the principles in the neocortex they have to be
embodied i mentioned this to the whole chapter in the book about they have to be embodied in some
sort physical and non physical embodiment but some embodiment they have to have uh you know
very sort of safeguards built in they have to have various mechanisms for what to learn what not to
learn someone has to provide goals uh what should the goals of the system be uh what are we trying
to achieve right now and um but so those things have to exist but they don't have to be modeled on
i don't see any reason at all to model those on the neural circuits the way the neurons do that
that now we're getting a pure biology we're now getting into what is a biological organism need
including like keeping your heart going and breathing and you know making sure you have sex
and all these things um you mean more of that i'm just we're gonna build intelligent machines
just make them a little bit um uh to our purpose i guess i would say another neuroscience question
is that i know um you know a lot of people are skeptical that the brain can do anything remotely
like bat propagation right i mean there's sort of good arguments for why that's just not possible
but on the other hand i don't know if you've seen kind of a lot of the recent work by uh
Blake Richards at McGill University where he talks about you can do something that performs in a very
similar way to bat propagation because and especially in the neocortex the uh the neurons
there had these very long apical dendrites and they had these sections with these calcium channels
that can do these very long lasting activations that trigger like spike trains and he's saying
that there are there is evidence that you know if you have feedback from a higher or downstream layer
back into this the uh the apical dendrite you know connections that that can have an effect
of creating something like bat propagation it's like stochastic bat propagation what are your
thoughts on that well i'm a little bit familiar with that work and Blake Richards work a little bit
i feel a little bit of a distraction because bat propagation is a cool idea
it works on classic neural networks pretty well i don't think the end result is like the kind of
direct what you see in the brain at all i don't think the kind of learning that that bat propagation
does is the kind of learning we see in the brain and and so and there's lots of other theories what
those apical dendrites are doing and why you have that stuff so um i'm not going to say that bat
propagation isn't occurring anywhere in the brain i don't know maybe it is um but it hasn't played a
role in our theories about the brain and i think a lot of people spend time trying to shoehorn in
and they're like i'm gonna prove it to you see the brain really is doing this it's like well
but the brainings look like anything else and none of this stuff the brain doesn't look like
anything like you're talking about here you're just trying to put this you know one little
piece backprop into this thing i mean for example just take away learning we know that learning
in a neuron lauren taught these dendritic branches and and the synapses are distributed on these
branches uh and this is this is key this is critical you can't you can't get around this
this is this there was important information theoretic reason for this it's it's not just some
other reason it's it's doing something important from an information point of view and and so
the whole idea of backprop and working with learning on individual dendritic segments it's
i don't even know what that means right it's like it's like hard to fit in so right to my mind it's
like noise i kind of like tune it out saying well if they figure out something really important
they'll let you know well i mean i'll pay attention to it yeah i think i think more his his motivation
and i quite agree with this it's about taking inspiration from the brain so this algorithm
that they discovered by kind of looking for things that could you know at least enable
you know feedback mechanisms for the training is it but are they are they taking inspiration
the brain or are they taking inspiration from the neural networks and trying to figure out the
brain could do what they're trying to do is they're trying to figure out how can the brain do deep
training how can the brain do deep learning yeah but and they but assuming that that's what that
assuming backprop is what they want no no they just want to know can it do how can it do deep
learning and so then they look very carefully the biology and they find this this alternative
mechanism which can produce results that are very similar to back propagation but actually it's
slightly better in some ways and so it's leading to inspiration for how to improve ml and i guess
that's the question i really wanted to ask you which is i always ask this what's missing question
okay so we have artificial neural networks clearly they're a very stripped down you know
micro abstraction of what happens in the brain if you could add in a few things like just one
two three kind of abstract properties to neural networks today what would you suggest
should be added to get the biggest bang for the buck like how can we incrementally improve them
okay well so so that's what we're doing in the mental right now so um my colleague subatomod
who's really the machine learning expert on more of the neuroscience guy um he put together a roadmap
or we did it together but he's implementing it a roadmap the idea is like hey we have these
grand theories about the neocortex and so on maybe the right maybe not but we think the right
um well how what do we do next like what do we do right so the answer question was well how
we incrementally get from where we are today uh machine learning to where we want we ever think
we need to be some number of years from now and so we literally have a roadmap and we started off
by saying okay uh first i'm going to focus on sparsity and so we've been doing that and we've been
uh we've been making really interesting progress some of it which we published
and some some of it we haven't yet uh about introducing sparsity to first we did it for
convolutional networks and we did it for other types of networks now we're doing it for transformers
and showing that we can speed these things up and make them more robust and and i'm talking
speed things up by a lot not a little bit by a lot uh depends on the network depends on the
what all these different things but uh how do you and then how do you implement that on
hard so we start with sparsity the next thing we're working on is the dendritic computation
because what the dendrites get you and we haven't really talked about this much at all so far
but with the dendrites each individual section of a dendrites like it's only a little
computational or pattern recognizer um they allow you to represent information in different contexts
and so um this is we think it's going to make it's far far less training data to train existing
networks um um and so and allow us to do continuous learning uh because when you train a neuron you
don't train all the synapses you only you only modify a few on on one of these dendritic branches
and so you can you can learn without forgetting things um so we're working on that so i can take
existing networks make them so that there can be continually learning or they they don't forget
things um and make them so you can train them for fewer uh data points and then so we're we're
well into that now um and then the third thing which the next big one that which is is sort of
biting off the whole concept of reference frames and um we're just starting that um and it probably
will take several years um but and then finally we do the whole the whole thousand brain theory
we have lots of these columns and so on so at least we put together we have a roadmap we're
working to it i think i'll be honest with you i think this the progress we've made is actually
far better than i thought it would be i thought it would take us longer to achieve the things we're
achieving uh the performance gains we're getting are really are really something dramatic and uh
without losing accuracy um so uh that's that's what nometa's working on and i don't think we've
talked about this roadmap um but we haven't given all the details of it so it's not public yet the
roadmap well the roadmap was that talked about it in like you know talks i just i just spoke about
it here right so i have pictures of that stuff um but the details how we're doing it like we we've
created we discovered like you talk about sparsity the problem is you want to run sparse networks
you want to run them on some hardware right what kind of hardware you're going to run them on
is it GPUs CPUs FPGAs well we're doing all of those um but they all have their problems and issues
and because GPUs don't like sparse networks at least you don't get any benefit from them
and um and FPGAs are hard to use but they're pretty they're good but they're really hard to use um and
and so that the work will be discovered and even including very recently we've discovered tricks
um engineering tricks that allow you to map sparse networks onto some of these existing
architectures um better than we think anyone else has been able to do by pretty large margin
and and so um that you know these are very engineering problems uh they're not pure you
know they're not pure theoretical problems are right and big engineering problems and we have to
we have to solve this because it's no good to say hey sparse is great you can't run it faster
it's got to really work in a real world so it's exciting you know we'll see how far we go but
right now it's going great i'm really thrilled by it we've actually decided to invest more in it
because uh we're hiring more people because it seems to be working so well so we've been talking
a lot about intelligence um you know however that might exactly be defined i think we're all
on kind of the same page what we need when we talk about intelligence the neocortex this this map
this ability you know however you might exactly want to define it but we've also talked you know
we've um a little bit moved you talked about motivations but you know which may you know relate
more to dopamine or whatever you know within the brain there you just run into your cortical
intelligence so in your book use you and of course dedicate a good chunk of it to these
questions of motivations risks of benefits they are and you describe yourself as someone who is
pretty skeptical about like severe risks um so one of your one of the things which i fully agree
with by the way is you very uh eloquently describe how intelligence is kind of having a map
you know the map itself has no motivations it's not good or evil it's you know it's just a tool
it could be used to do something nice for something good you know or it could be used to do something
bad um the map itself is you know it is orthogonal to these kind of questions of motivations and you
you know do it good or bad it's kind of the the human idea of you know we can't get it off from
it is but at some point of course when we actually build these systems we don't want it bought to
be in there right you know we do want our systems to actually do things at some point we don't know
what's sitting there and know everything but never take any actions in the book you for example give
me to the example of you know we want them you know to go to Mars and then you know build a colony
and not just sit around the sun all day you know thinking about the universe or whatever
so at some point we have to introduce some kind of motivational system the brain seems to solve
this according to you this which is called an old breed the end um i'm not sure exactly which
parts you count as the old brain versus the new brain but some are quite obvious you know we have
circuits and get hungered or you know you have other motivations or whatever but it's and but it
seems like these parts or will be very important that okay let's assume your project goes fantastically
here 20 years from now new mentors the biggest AI company in the world you've developed our goal
you figured out the neocortex you've created a wonderful algorithm um at some point we have
to inject motivation i'm sure you agree with yeah and we have to assure that those motivations
aligned here in like what we want and that seems like a hard problem maybe you could talk a little
about that well i don't have it's a hard problem i think the concern that people have about this
is that these machines will evolve their own motivations that that or somehow they they are
in some sort of you know epiphenomena disappears um and um i don't think that's right um i think
we will have to work hard to put them in there like i mentioned earlier um it will not be easy
we'll have to design what how these systems work um unless we put this machine in some
sort of evolutionary um structure whether it's a self-replicating machine or it just uses evolutionary
algorithms genetic algorithms to decide what its motivations are um they won't modify themselves
they won't change on their own you know i i gave the very simplistic example of a self-driving car
but i think it's a correct example you know i tell the car where i wanted to go
it's not going to decide to go someplace else because today it feels different you know if
i could make a car that does that but why would i do that i'm not gonna design a car to do that
so if we send more rods to mars to to build machines i'll have to be motivated to solve
the task they've been given um and but we will also build in all kinds of safeguards just like
we built in safeguards into cars and we won't allow the machines to you know it won't be a
mysterious process it'll be like a self-driving car it it's not going to be something we don't
understand that happens and runs away from us uh it'll be difficult to do we can make mistakes we
can put in bad things and machines will do bad things but it's not like the machines on their
own they're going to do this it's not like we've disenabled this beast that's just going to take
over and decide on its own and what its motivations are so i make it very clear in the book that
there are a lot of risks with ai and a lot of bad things can happen with it but i my only contention
is that the fears of existential risk um are overblown i don't think they're true at all and
that the arguments don't really hold up um because there are more arguments uh based on ignorance
as opposed to some detailed knowledge of how these things are going to work
so did i answer the question so i absolutely agree with you uh i happily agree with you that
these like epiphenomenal explorations or nave you know a lot of like sci-fi evictions are like
oh the robot becomes conscious and wakes up and it decides to rebel against its master of course
that's a pretty silly perspective well not not everyone thinks that's silly by the way so i i
know i and i'm well aware but not everything's so silly but i feel there is a stronger a similar
argument that's i don't consider the same argument but has similar consequences and in a way it's
kind of like in your book you have a chapter titled how the new tortex can thwart the old
brain and you yourself describe the old brain as the motivational system and now you're described
being the neocortex the new brain plotting to thwart the goals put in by you know the you know
our creator you know evolution or whatever so it seems like our why wouldn't our machines
maybe have similar capabilities to him with interest to like modify their own old brain quote
unquote i did just saying um uh yeah i can see your point there um let me see no one's ever asked
me that before so let me just think about it for a second um well um do i have you that
you know the the old brain's trying to thwart something okay so first of all
the new the new brain's trying to like take the old brain so the new the new neocortex
that's it has a model of the world and the model is very consistent and it strives to
have consistent models it wants it wants to make its model correct that's one motivation it has if
it sees something wrong it tries to correct it in this model so it has this model of the world
and says okay the world ought to work like this if a happens a b happens and c happens
and that's good uh but then you know the old brain comes along and says nope we don't we're
not going to do that we see a and b we're going to go x and uh and so it violates the model in
some sense right it's like violates the model and so um generally the neocortex uh is not able
to overcome the older brain things it just we just give into our desires and emotions uh but
we do struggle with it um so i i guess you're saying where does the motivation for the from
the neocortex come what how does it decide that its model is more important than the old brain's
model um and like i don't it's a good question i'll have to think about it some more but i think
that the root of it will be that the motivation the true and only motivations the neocortex has
is to make its model the world correct and to fix errors um and um that is it says this isn't
right my model says this isn't right uh we you know i want to correct my model but i'm being
told not to or i'm being told to do things over violating the model um and so that is its motivation
now you could take that to some extreme let's get a little sci-fi here right you could take that to
some extreme and and uh say well um the neocortex really figures out how the world works the future
brain the ai neo projects really figures out how the world works and humans got it all wrong
you know let's let's bring it to an example we could we can relate to there is no god right i
know i figured this out there's no god you guys all believe in this but it's not true so uh what
we do about that is is this ai system uh put up with it or is it or is it do something about it
or is you just keep trying the prod humans to do something differently um i don't know it's a good
question i think you bring up that's a very interesting philosophical question um i will say
this i don't think this is something that's gonna happen fast quickly overnight you know it's it's
not this is something we all have time to try to think about this quite a bit um but it does bring
up a question what if what if the the world as as a smarter machine knows it violates the world
as we'd like to believe it and that's really the the the fiction you're talking about and um
what should we do about that there's an even more direct like kind of physical
assault on this that doesn't require much philosophy it's just what about the traditional
mechanisms that created life which is variation and you know random variation and natural selection
so for example if we send robots to mars to build a colony they're going to get damaged
and destroyed a boulder is going to fall on them so they're going to need to be programmed to
replicate themselves they're going to have to be programmed to build replacements of themselves
and they're not going to be able to do that with absolute 100 fidelity so errors will creep into the
the neurons or the silicon neurons or the programming or whatever and so you've already
got in place the requirements for evolution which is information transfer or self-replication
I'll push I'll push back on that first of all uh hanging robots self-replicant is really really
hard are they going to build their own semiconductor chips you know are they going to you know mine
their own you know titanium or my I mean I don't know but but but even if they do evolution requires
a very complex structure for um how information is represented and and requires that information
be changed constantly in our offspring right you know so when we build we build you know computer
chips and van vidian builds in the next chip they they're all pretty much going to be identical
so I I push back on the idea that there's inherent evolution built in any kind of
self-replication I don't think that's true I think we're if it if there's some variation it would be
very very minor and and it's not going to be passed on genetically to somebody else you know
there's going to be some blueprint how to build this chip which is the neocortex the blueprint
itself isn't changing so if I if I if I build the neocortex from the blueprint that's slightly got
an error in it that's not going to be propagated to its children um so I think there's a lot of ways
you there's a lot of things you have to do to create evolution it's not easy um and I don't think
that's going to happen accidentally so I'm pushing back to you I don't think that's going to happen
you know it's good it's good to think about it ask that question right right but you know I don't
think so I think in reality if we send robots to Mars in fact you know in the chapter in the
book I mentioned this point that we could send robots across the universe and then they would
have to self-replicate right and after I wrote it I said oh damn I don't know how to do that I just
don't know how you know a fleet of robots you know AI systems appear on another planet in some
just part of the galaxy and they have to replicate themselves what would be the physical form of
those AI systems that they would be able to do that in any reasonable amount of time and effort
and because you know just think about again I mentioned like semiconductor factories right you
know it's like they got to build those things you know how do they do this so it made me it
it was a hole in my argument that we could send AI systems across the universe and I didn't put out
pull out the hole I said to myself well that I excuse myself in this regard by saying well you
know what if we're going to send AI systems to other parts of the universe they're not going to
be built of silicon chips that there might have to be some other manifestation that like biology
is able to replicate on its own without the use of these cupboards other systems like
semiconductor factories and but we're so far from knowing how to do anything like that today
that um I don't know and then maybe we'd be in trouble right we send these things over there
they evolve and they come back in you know so I don't know but we're talking a long time from
now yeah that's for sure there there is a kind of failure mode of thinking you know too far ahead
into the sci-fi feature that you're just coming generalized even fictional evidence yeah you
know yeah I guess what I um so what I have taken from like your just your description your book
in for now kind of about like motivations and existential risk is that I think actually at
least existential risk to people that I personally find interesting I think I agree with you in like
a lot of ways actually in that I think a lot of them are very reasonable in the sense of saying
like you know whether it's fast or we're short it's kind of like you know it's kind of beside the
point and you know trying to see you know humans already kind of you know hard to control and
like you know it's like if you actually try to think about how would we write a motivation system
because you know it can't really be a learning system because you know like in your book you give
an example where if the brain comes up with two ways to get to food and one of the path has a tiger
on it the old brain will say you know kind of like oh no don't go there but you know the old brain
how does it feel that a tiger is bad that's like also like a not obvious to me how we would necessarily
learn that so I guess it just it feels to me that um as you yourself say you know your focus on the
new cortex which I think is an important thing to be working on but we can agree on that some work
on these motivational systems and control problems might also be justified oh yeah sure uh totally
I agree with that um I think it's justified and necessary but I'm not scared of it and you know
the conversation sometimes people we shouldn't be doing this for each other because it's going to
get out of control I'm thinking you kidding me it's gonna be so hard to do this at all you know
don't need to get out of control um so we need to do that we should do it the conversation around
me today is great about that kind of stuff but I guess it's just not you know this there are
quite a few people feel this existential risk is you know upon us and in any day now you know
it'll be too late therefore you know we have to stop all this stuff and I it's just I can't see that
in any possible scenario but yeah we have to think about these things right we have to figure out
not because um it's scary or dangerous because we have to figure out because we have to do it
and and and I use the example of the book about you know we put in safe courage my car doesn't
always listen to me when I if I'm about to hit something my car puts a brakes on even if I put
the accelerator down it ignores me well we have to put in some some fail safe systems too um and
these systems so they don't you know end up damaging things but it's all has to be done
I had a question on on intelligence I'm fascinated on the nature of intelligence and
you were saying in your in your book that you know the traditional go-fi people had a task
specific skill conception of intelligence and then it moved towards more of a um a flexibility model
so being able to learn and there's also a really interesting tradition in cognitive science about
embodiment which I think is fascinating and you know my my favorite person is Francois
he says that intelligence is the information well that I think he says it's the task
acquisition efficiency and generalization but but also quite you know now we've been talking
about this a lot and there's this interesting idea that you can think of intelligence as the
ability to um acquire knowledge so almost all of the knowledge that humans acquire in their
lifetime is is not empirical and trial and error it's given through instruction or reasoning and
deduction and some some magic happens with these intelligent systems you know why why why can we
deduce so much knowledge is that a question to me yes uh why why I mean mechanistically why
how do we do then I mean I think I thought I answered this earlier so maybe I'm I don't want
to repeat myself too much um maybe I don't understand the question um well for for example you know
you knock the beer bottle off the table you can now reason that the floor is wet someone might slip
up on on the floor yeah so almost all of the the knowledge that we that we have around the world
is deduced or well it's it's it's one of its deduced that knowledge that you have to learn
that through observation if you never experience liquids and you're never very slipping on liquid
you never experience knocking a couple or you wouldn't know what's gonna happen I mean that's
what kids do little babies do this right they're like oh what happens we could live in a world
where liquids fall up and that would be a perfectly good world and that's what we would learn
so I won't think we deduce this we observe it and and then by analogy we we predict how other
things that are similar would behave similarly but I think you know to me the way I look at it is
you're you're bored with this with this structure in your head that is designed to learn the world
through eyes and ears and touch um but it really in the neocortex knows almost nothing about what
it's going to learn the whole part of the brain that's not true right refining refining question
because I'm I'm also fascinated by this concept that I learned it from the first time in your book
that the neocortex now I was going to say blank slate but Keith would kill me for saying that it's
not a blank slate it's a template because there's actually there's a lot of evolutionary knowledge
that's gone into there yeah yeah yeah because you because you can think you know it's just learning
all these signals but just the way the ear has evolved over time that the information gets encoded
in a certain way the cochlea has the logarithmically spaced bands and so on so it's learnable because
it's been encoded in in a certain way but people like Chomsky they say that we have a kind of universal
grammar or language built into our brains and it was endowed by evolution so where do you kind of
draw the does he does he say that about all knowledge you just about the actual language I
thought he just said that about language about language yeah yeah so I don't know I mean but
we're we're talking about all knowledge now right so like it's so you know so I think look there is
there's all these assumptions about the how how the cortex is connected to things right and how it's
continually connected that have been evolutionally advantageous and so you know we we only see
certain spectrum of light because that's a good spectrum to look at and we only certain types of
you know sensory inputs because those are things that we process them before they get to the neocortex
so that they're in the right form for the neocortex to work we allocate a certain amount of our neocortex
to these different sensory modalities because that seemed to be the right thing to do from an
evolutionary point of view and so there's just a good the good example say it's like a template
that's right but the in the in what's actually learned in that template is is is is unknown in
terms of language I hinted at this when I talked about language in the chapter about high level
thought in the book and I don't consider myself an expert in language at all but it occurs to me
that if vertebral counsel is right that there is a single cortical algorithm that's basically
running everywhere the language has to be somehow fundamentally mapped on to this algorithm for
a sensory motor modeling through reference frames and I made the analogy about a big part of languages
has to do with recursion and recursive structures and and the algorithms in the
in the core column are really good at that and so to me I would say that you know if I would take
isn't it I never thought of this but if I would say take choppy's idea that there's a universal
language I would extend it beyond what we call language I would say there's a universal
structure to everything in the world that we can learn not everything in the world but everything
in the world that we can learn and that universal structure is this this reference frame sensory
motor idea and that universal structure can learn any language whether it's spoken language or
computer language or written language it can learn any structure that fits into that algorithm
and that could be you know how staples work and how birds fly and how evolution occurs and these
are all everything we know has to fit into that structure so in some sense I would I would agree
with chomsky I just think he only focuses on quote language um where I would say the universal
algorithm is language is a subset of the universal algorithm that chomsky talks about
that's an interesting idea I never I never said that before stuff man we'll see how it
feel about tomorrow but but there's there's a fascinating dichotomy though isn't there because
it's similar to the um bias variance trade-off in machine learning so you know evolution has
has given us a certain prior and a certain default encoding and then we have this learnability
algorithm and then what fascinates me is how externalized so much of this stuff is so there's
embodiment there's all the knowledge in in society that we learn language we're brought up by our
parents and we acquire the language around us so how much of it is being pushed down from society
and how much of it is being pushed up from the prior knowledge that we've evolved and inherited
well I would I would say there's three things there is the our knowledge we inherited there is
what we learn on our own just through exploratory behavior and and then what is passed down which
is only always through language but it could be also through by observation of other humans
and um and what's the balance between those two well I think in terms of the neocortex
it's not a lot from the biology and the evolution in terms of the other parts of the brain it's
very much so you know there's a it there's evidence this is the prior colloquious it's
is like the old visual system um it it it detects snakes and spiders so people who are scared of
snakes or spiders it's not the neocortex it's the old buddy going ass snake it looks like a snake
and so you know that's not learned it's just there it's already a rid of um so there's some of that
but you know our ability to walk we don't learn to walk we actually are programmed to walk it's
just that we didn't we haven't finished developing yet and so when we learn to walk we're really just
our nervous system is finished being grown you don't really learn to walk so these are these
old priors lots of them um and as a human they're very very important you know eating and sex and
and and uh body functions and you know survival all these things and hold there uh but from the
neocortex point of view I'd say there's very little um it's more just assumptions about this
the sensory types of sensory data you're gonna get I mean it's what you know it strikes me Tim it's
incredibly we are so incredibly versatile on what we can learn I mean just you know I think all
things we learned and I'm sure you know this you thought about I'm sure you saw that all things
we learned that we had no evolution and pressure to do like running these this kind of podcasts and
you know programming and computerism and talking about brains it's incredible it just just cries out
that there's a universal method here that's being applied to anything that not that everything
you know brains can't learn everything but they can learn a hell of a lot and it seems to be
we haven't really discovered the extent of it yet so this this says you know there's there is this
sort of blank slay-dish um system if you want to call it that way with these assumptions about our
our bodies built into it well the only thing about the blank slate thing is it's such a dichotomy
because I was reading your book and it seems to be so strongly determined by the embodiment and
the multimodality you know the way that we're wired because you said yourself AI will actually evolve
towards you know robotics basically it's not just the brain it's it's how it acts in the environment
and how it gets all those senses but I wanted to ask one final thing so I went to a yoga retreat at
the weekend and after the physical components of the practice were concluded matters of a
philosophical nature were investigated and the the basic case that the yoga teacher was making
is that we have two selves right we have the emergent social self and then we have the inner
self which I'm sure Sam Harris probably might might speak to you about you know the sense of
being and the sense of being is when you kind of ignore the virtual so social program on the top
and you know society it is an emergent virtual program and it does many of the things that the
cortical columns are doing it has the error correction it had you know it's the externalization
and distribution so even though we're programmed to care a lot about our social selves um we
are very stressed when we lose control of our own narrative because someone might be saying
something bad about us on twitter so anyway this yoga teacher was saying well there's something
deeply fulfilling um spiritually about just kind of um going into your mind and and and just
just that raw conscious experience but then I felt like saying to him well you should read
Jeff Hawkins book because your mind is just a load of prediction models and all you're doing is
traversing your reference range so what do you what do you think I don't I well again ask me to
talk about things I'm not too familiar with but I'll tell you something I'll tell you a personal
experience I have okay I don't do meditation and Sam didn't bring that up but but I thought about it
a bit I do something equivalent to meditation um is that when I find things in the world stressful
um I just sort of shut out the world and I think about interesting problems like the future of
humanity or the future of intelligence or what's the nature of life and things like this and so
now living in this world then it's sort of um is in some sense uh pure right it's not it's not
being messed up with like I'm hungry or this person's being nasty to me or just like tuning
it all out like oh my god I'm not in trouble again I know so I think that's my own personal
meditation and I don't think it's I think it's useful actually because uh I think it helps when
when you think like that you you're you're sometimes able to get to deeper truths um
that you separate the body functions and the day to day stuff and and I often do my best thinking
when there are no distractions either on sometimes while I'm driving or sometimes I'm just awake at
night like in bed um but where there's like I don't I don't have to deal with anything else
so that's kind of like meditation and I find I don't think it's useful I think it's very useful
because you it allows the brain to sort of separate out from the old brain stuff and um and
just have run of it sometimes it comes up some good ideas you know exactly I couldn't attest to
that well um Jeff Hawkins thank you so much for joining us today it's been an absolute honor thank
you well it's been a pleasure you guys are great I really I enjoy all your questions you're really
fun and um I think this is one of the one of the more meaningful conversations I've had in a long
I'm serious you know because you know you know what you're talking about you know what I'm talking
about you asked great great questions it really is deep thinking about all this stuff so I think
that's wonderful thank you we appreciate that thank you so all right all right well I hope it
helps it was good for you guys so that was a wrap how was that guys thank you that's pretty great
it's pretty great um and honestly yeah and I'm really impressed it was really nice to talk to
that me it's just clearly you know you know it's like so much more than was in the book uh it
builds in the book was really just scratched in the surface and I wish we had like you know
like just taken off for a beer and like really dig into some of the degrees yeah what I I love uh
I love people like him because he's so smart and and smart in a very um he has such great common
sense right so when you when you ask him questions he comes back with these really informative answers
that are very uh concrete and you understand what he's talking about so it's really fun to talk to him
I mean a very very great learning experience yeah I got the impression that his research
focus has changed a little bit so when we were doing some preparation for this we were looking a
lot into the htm algorithm which I understand is now um they're pivoting away from that a little bit
so he was talking about some stuff that they're doing with transformers models and sparsity and
quite a few things that I hadn't heard about and also he was really focused on um the particular way
of thinking about cognition using these reference frames which he spoke about in his book which I
think is actually a slight departure from from um some of the stuff the mentors had out about five
years ago yeah I definitely got the feeling that there he had like so much interesting stuff
that's just not quite ready yet for public consumption but um hopefully you know the book
I often had this feeling like I had to say to you just you know it feels like there's more to this
than there is in the book maybe or maybe just needs more time and I think I remember in the
book it took like a year and a half to write or so so maybe you know an idea you know lots of
things have happened um so I do look forward to whatever he's gonna publish in the future
well what I love too is you know he's more than willing to admit look I've changed my views you
know I think yeah lots of people who run into are not able to do that and if you agree with
Sholey's you know measure of intelligence the mark of intelligence is the ability to assimilate
new information and to learn from it and most of the people we taught to just want to stay
adamantly entrenched you know no look this thing that I called XYZ back you know 30 years ago
you're wrong that it was limited to this little area it actually encompasses everything you know
that that works now like nobody ever wants to admit that they've learned or changed or that
science has evolved I want to push back on that a little bit because there is something really
special I know we were joking that science advances one funeral at a time but there is
something really special about people who have an idea and they see it through just like Jan
Lacune and the deep learning pioneers everyone was asking them why the hell are you doing this
in the 1990s and they and they hung on and it was worth hanging on and it's a similar thing with
with some of the approaches from Nementor they've been at this for such a long time now and there
is some really really kind of biologically inspired implausible reasons why this might
lead to something interesting in the future but clearly they've been at this for a long time and
they've been sticking at it well nothing nothing's wrong with perseverance but perseverance without
adaptation is wrong it's just perseverance when you're right is right and perseverance when you're
wrong is bad you're never 100 right and you're never you're never 100 so that's the whole
point is you've got to be able to learn you have to be able to learn well the learning right needs
to be about 0.0 well sure I look up part you send this 3e-4 I think is the correct learning rate
for all matters in life yeah all all the matters specifically yeah I agree like also I'm really
glad I got to talk to Jeff about existential risk or something because of course you know I take
some of the stories pretty seriously yeah it's a very common experience to hear people
dismiss existential risk because they've heard bad versions of the arguments because there are some
horrible versions of these arguments that even from some very smart people like you know some
smart people I feel do the field a great justice then I just felt bad their arguments are um
the end and it was really nice to see you know him say like yeah you know that's something
we're thinking about and um I think honestly a lot of the disagreement comes down to yeah
like I agree with him and almost everything really so that was an interesting I was really
impressed with with his response and as you say you can see it from so many different perspectives
so you can cover it from a grand perspective talking about paper clips and um oh my god but um
oh I lost my try and I thought yeah so so he I read the the less wrong article which you linked
to me was that Steve Burns yeah Steve Burns yeah so that was fascinating so so he was saying that
um the end of the day you've got a robot rover on Mars or something like that and at some point
you need to actually give it something to do and you might give it an instruction and then
it might really want to complete that instruction so it might predict that you're about to give it
another instruction it says oh my god he's about to give me another instruction I've not done the
first thing yet so I need to kill him and that's not completely beyond the realms of possibility
now I I agree with Chalet that I think intelligence is externalized it is embodied and there are many
like natural environmental limiting steps to intelligence and you could talk about
all the replication stuff as Hawkins did but that's just a simple example I'm just an intelligent
agent on on Mars and I I need to have motivation because I need to be told what to do what am I
doing am I building a base and then I can plausibly see how that could be um you know misdirected
yeah I feel like it's it it the intelligence explosion arguments all get mixed up with these
like motivation or uh uh concerns which I think is unfortunate because they really aren't
working at all like you could like these these concerns about you know alignment and about
motivation will crop up whether you know it takes 10 years or 1000 years for API to come to be at
some point as Hawkins I think you know picks so when we put the work beats to be done you know
at some point you just have to build a safety mechanism at some point you have to write a
motivation system so that's why I think this you know a kind of research you know even if
the intelligence blows it doesn't happen it's still somebody who needs to get done
yeah and and there is this thing um I mean you could explain this better than me but
instrumental convergence which is this idea that um you know many seemingly innocuous goals
incidentally lead to dangerous motivations like self-preservation and self-replication and
goal preservation and um I can sign on to that that seems very plausible yeah exactly I mean
you can give a perfect example but like you know you give the rover and the the the job will you
know build a thing but actually just like oh actually I wanted to build something different
the rover you know still was motivated but I went to do the first thing so unless we have like
some really sophisticated you know system inside of it allows it to like you know equally value
doing this or do with another human does also with courage ability and which we don't currently
really know how to implement formally um it's very likely that it will you know having instrumental
goals like keeping itself along it so and or you're stopping you from giving it your orders
like it may be able to damage its entailment or something so you can't give it orders or maybe
if you if you're now if you try to destroy the rover because it's malfunctioning maybe it will
retaliate because if it's destroyed it can't build habitat okay so so yeah there's a lot of goals
like that's also also like to like power seeking goals just seem obvious like yeah if you don't know
how to achieve a goal and gaining your power is probably like always a good move you know getting
more money more social capital you know controlling for people with control of resources like almost
always a good thing like almost no matter how innocuous the goal might seem yeah let's see like
on the other hand I don't care if a rover on Mars tries to retaliate it has like no capability to
retaliate on us at all and so the only kinds of like artificial intelligence I worry about
are ones that are capable of expanding in some capacity like they have to be able to build
things and they actually have to be able to build intelligent things so that's why I brought up the
point of you know if we have a robot we're gonna need one that can build you know somehow or another
replicas of other robots to expand conduct more work replace ones that were broke and whatever
and it's not so infeasible to believe that at some point in the next few hundred years you know
we'll have 3d printing technology more than sufficient to build silicon chips and you know
whatever else and as soon as you have a system that's self-replicating and we don't live in a
perfect world and we have random variation right and they have sets of instructions and also we're
going to need them to adapt to new environments like it's going to be different you know different
building it on the planes of Mars versus deep down in the valleys and mining water or whatever so
they're going to have to have some capability to allow variation you know then you have the mixture
for evolution and once you have evolution even if you had no goals like think of it this way the
ultimate thing which has no motivations whatsoever was inanimate matter and that's what the earth was
four billion years ago and we wound up with animate objects that have goals so despite
philosophers and this is kind of an interesting thing philosophers are always like you can never
get ought from is but interestingly enough ought arose from just stuff right yeah the very elegantly
said there's this weird kind of symmetry breaking that happens somewhere i mean in evolution the
symmetry breaking happens because of physics because things that don't want to replicate don't
replicate so we don't see them so we found the fruit ought from is comes from just the fact that
if there is a possibility of replicator and the possible non-replicator eventually you will only
see replicators and that's your first symmetry breaking and of course we'd be you know when we
build intelligent systems is that maybe we can break the symmetry in a more you know in a less
dystopian you know a bit uh mouth easy and weight and build systems that are more motivated by things
that we consider better than just pre-replication but it's very non-obvious to me you know how to
do that safely it's hard so it's a very problem you but Hawkins did say though that he was more
worried about the immediate threat of humans controlling the kind of ai that we have now or
the kind of ai we have now been used for bad purposes or even things like the spread of
false beliefs that's incredibly dangerous as well and that that replicates like a virus
that's why that um you know that social dilemma doc nary like i thought this was a very brilliant
quote that was in there and a lot of other people think it's like not an interesting quote but to me
it was beautiful when uh you know undermining our weaknesses exactly he said we've always been worried
about when ai would overcome our strengths when we should have been worried about when it would
overcome our weaknesses right and and it's already you know even though we don't have maybe we forget
if we have real intelligence or not or whatever clearly these machine systems have overcome lots
of our weaknesses and they've created things that are more addictive than they've ever been
found ways to waste more of our time than they've now of course there's offsetting things like yeah
we also get greater productivity here etc etc but we should be worried about when it overcomes our
weaknesses and what that's doing to us and our children and you know one of my favorite examples
is it's called the food up to my chair so this comes through them and yeah i go to roco and he
makes this example that so the way he describes it is that in like the in like the 19th century
we collectively summon the weak super intelligence to feed all humans like the deal was kind of this
this capitalist you know market system will produce really cheap food for us but the systems
was optimizing our reward signal based on profit of course let's see how these things work so it's
kind of like a weak it's not sent it was obviously not intelligent the way you know humans are but
the way it is opts in raising system it optimized for more more food and now we're at the point that
you know like what 60 percent of adults in the western world are obese something we are literally
eating ourselves to death and this is a very very weak intelligence quote unquote and this is already
powerful enough in a way to you know get people to basically kill themselves voluntarily so imagine
if you gave such a motivation you know these you know food corporations in the world you know built
their big their big agi yeah i'm using hawkins you know elves and brain algorithm of course uh and
giving the motivation of you know smith raff yeah but this this gets to utility and similarly hawkins
said oh you should never clone yourself because you're not cloning your forking i thought that
was a bit of a contradiction though because he was talking about um you know sending clones of
yourself to mars or ceding mars that's right and and that's the same thing isn't it you're actually
forking the human race when you have a population of humans on on mars i don't think um who doesn't
like forking what's wrong with that well no but the thing is because we want we're doing it because
we want to leave a legacy you know i like doing this youtube channel because you know after i'm
dead people can can see who is that tim scarf guy but um but you know you might say i'm going to
create a clone of myself because i want to have a legacy but it's not a legacy because after after
the next day it becomes a you know this it's a different person it's not connor anymore
and similarly when when you were talking about um you know why don't we restructure society because
let's take away social media let's let's fork and take away facebook and everything else but
what's your utility function how could you how could you justify that ourselves in that parallel
universe would be better off than we are now well first of all i mean i think it would be a legacy
so you and i both think that that the pixels on youtube are going to be a legacy and those are a
pale pale shadow of a you know projection of us right so i would totally you know a clone of you
or a robot programmed with your mind state or something that then can go off and live its own
life that's exactly what a legacy is like that that is a legacy it's not me it's a legacy yeah like i
described like even more extreme crazy versions of this kind of thing is like i fundamentally
believe in like you know a non dual identity and like that moment most of condoms of identity is
kind of incoherent it's just it's just a convenient abstraction in humans like humans tend to be
discreet that's just a convenient property that humans have to have it happened to have there's
no reason that identity or minds in the sense have to be discreet like that it just happens to be the
case that humans come in packages it looks like that we come in you know one intelligence please
one unit of intelligence but there's a reason they have it they have a mark out they have to
have a mark off blanket or they can't persist exactly but there's no reason you can't have systems
that don't really think there is not like a clear separation between the like the only reason i think
really that you know me and tim aren't the same person it's because the bandwidth between our
brains is really low is interbred it's lower than it is intra brain like why is the right
heart familiar maybe that's the person let's say those were different person depending on your
definition like you know there's like split brain patients were like the brain has to actually
disagree and like take different actions in like fighting with each other so like are they different
people i don't think so i think what makes identity is synchronization in a sense well like
aligned so you see is that identity the best way to define identity is alignment with agul
is that like does the the system coherently act without you know like shooting itself from the
foot so it's bigger like you know doing it all it's so like that's why i think it was a really
great argument that for example and colonies are one animal that happens to have like you know
very big individual parts because in a way it is aligned you know not all the parts were produced
yet but we're producing part you have you know fighting part if eating parts of what you know
food getting parts or whatever and they happen to not all be one body but that's just neutralization
detail you were also just colonies of cells you know it's it's just it's just implementation
details it matters is the coherency or the synchronization of you know knowledge and goals
in life yeah but again a lot of that is is emergent um but you were saying as well that um there's a
kind of there's a there's an io bottleneck when we communicate and this is um a lot of people talk
about neural link in in this context as well but i don't think there is because when we communicate
there's common knowledge that we both have so it's it's a fairly efficient communication
mechanism and we're invoking lots and lots of very complex knowledge set that we've
reconciled so when we read a book or when we when we take on board information the bottleneck is
comprehension i don't think it's the the bandwidth sure as it depends on how like and by bandwidth
we're also in comprehension bandwidth like if i could leader i imagine we could you know take
two human brain to put them in the same skull and collect to connect them through some super
clip with low summer or whatever and you know let the whole thing synchronize and you're not
died seizures immediately or whatever i think there's another strong argument to be made that
is one entity but if you could also make an argument to connect the entities and at some
point just with them to continue this like a classic like often when something seems confusing
if there's a discrete quantity of all try try continualizing the quantity and see if the confusion
disappears very cool well gentlemen it's been a pleasure your hands so see you next week folks see
you next week

The next vector is artificial general intelligence through large-scale, multi-modal, basic learning.
Alright, so this is going to be less technical talk than the last two.
So the claim is that one path to achieving artificial general intelligence could be
busy in learning from large amounts of data on the web.
And to motivate that, let me start with one of the prime examples of a kind of thing we'd like AGI to do for us,
which is to answer questions. And the range of questions people could ask a system is incredibly large.
So here are a couple, how can I get from Boston to New Haven without a car?
How many US Congress members have PhDs?
Or here's a picture about how cold is it in there, based on what people are wearing.
Now, if you gave these to a human being and gave them internet access,
they could answer all these questions without too much trouble.
But if you gave Google these questions, it would not give you the answers.
It would have to combine information from multiple web pages.
And also, it would need to have broad and deep knowledge about the world.
And we need to know, for example, that when you need to get from one city to another,
this far apart, you're not going to walk, you're going to need to take some kind of transportation,
that kind of general knowledge.
And even more so, it's been like a picture interpretation, where you're saying,
well, those are people in the picture, and they're wearing jackets,
and that suggests something about the temperature.
This is knowledge about a range of topics that ranges from transportation to clothing and so forth.
So the question is, how can we possibly acquire all of that knowledge?
And the proposal I'm making, which is, of course, not original,
is that we should be able to learn knowledge from large amounts of data online,
particularly now that the web contains not only text, but also images and video.
And the general proposal is to learn by Bayesian belief updating.
So maintain a probability distribution over models of how the world works in general.
You know, people tend to wear coats when it's cold, that kind of thing.
And also, the past, present and future states of the world.
For instance, there is a scheduled bus between Boston and New Haven.
Questions like that.
These people are in the U.S. Congress.
So this is not a fully fleshed out proposal,
but I'm going to go through some of the issues that arise in thinking about how to make this a reality.
The first point is, just looking at what are the variables that we're going to be thinking about in our probability model.
So this is a kind of complicated slide.
So at the bottom level, we have reaction C.
We get video data, we get linguistic data,
and what this diagram is supposed to be is sort of schematic of a Bayesian network
or probabilistic graphical model.
And these are the observed variables.
And then based on those observations, we want to make inferences about all these other things in here.
So in particular, what we really care about is what I've called world history,
which is the past, present and future states of the world,
and the tendencies which govern what happens in the world.
And to reason about those things, we're going to also have to make inferences about,
well, okay, I see some video. What's going on in this scene?
What are the objects? What's the including? What?
Things like that.
And that's going to involve reasoning about what are objects look like?
What do people look like? What do jackets look like?
That kind of question.
So those are also variables that I'm not going to know initially,
but I'm going to, through looking at a lot of videos, learn what those things look like.
And then there are also variables for language use.
What words do people use in different languages for different concepts?
How do people put sentences together?
And all those things would also have to be learned.
All right. So I've talked quite a bit about all these things you have to learn.
What are you going to learn from?
So one hypothesis dating back to the early days of AI is,
well, we'll learn from text.
We'll read encyclopedias or something like that.
And now on the web we have lots of text available,
and it covers pretty much every topic we want to reason about.
The problem is you're going to have no connection to sensory input.
And so there are probably relatively few people here today
who would think that you would learn from text alone.
The other major thing that's been proposed is learning from embodied agents,
whether they're physical robots or virtual robots.
The great thing about this is if one thing is multimodal data,
so you can connect linguistic and sensory input.
The other thing is that you can actively manipulate the world you're in.
So if you are uncertain about things,
you can say, well, what happens if I look at the other side of the object?
What happens if I try lifting this up? Does it move? That kind of thing.
The difficulties with this are if one thing, if you're using physical robots,
then as people have pointed out, this can be expensive.
Also, it's hard if they're just wandering on the lab
to give them the kind of broad experience.
I mean, how is a robot going to know about buses, really,
unless you let it ride buses,
something you're not going to do at the early stages?
The other thing is virtual agents,
and this is more promising in that you can give it broad experience.
If there are buses in Second Life, it could take them.
The question is, are you going to accidentally have some special properties
in your simulation that don't generalize to the physical world?
So if you're doing great in simulation,
how much do you trust you've really solved the problem?
So what I'm proposing is learning from multimodal data on the web.
It provides both broad coverage and multimodal integration.
It does have drawbacks.
It can be disjointed trying to learn from a whole bunch of YouTube videos.
How much will you really be able to interpret what suddenly happens in a music video,
even a video where there's cutting between scenes?
And also you're forced to do passive observation.
But still, at this point, I would guess that this is the best path we have.
Alright, so that's the data we're learning from.
The next question is, are we actually learning everything from scratch
or are we going to build some things in?
And I would argue that we actually want to build in some components.
My argument for this is that children don't learn completely from scratch.
We seem to have evolved abilities in the brain
to do things like spatial reasoning or reasoning about language.
And also there's been a whole lot of work in areas of AI
on things like language processing, the idea of a parse tree.
Do we want our system to invent the notion of parsing sentences from scratch?
Or are we willing to give it that kind of idea originally?
So I would say that we probably need building modules for things like
spatial and temporal reasoning, physical objects,
actually along the lines of what the speaker's talked about in the last session,
linguistic reasoning, and also reasoning about other agents.
The ability to put yourself in another agent's shoes
and use your own reasoning capabilities to say,
well, if I were them, what would I do?
So that's the built-in components.
We're also going to have to have a whole lot of learned components.
So really our targets are to learn the history of the world,
the present state of the world, and the tendencies.
So the possible values for these world histories,
they're going to be structures that have initially unknown sets of objects,
initially unknown relations between them and attributes,
but also changing over time.
So, okay, there's a lot of work, actually,
in the probabilistic reasoning literature these days,
including my own work on what I've called Bayesian logic,
which is a probabilistic modeling language to reason about unknown objects.
What you need for that is prior distributions over what's going on in the world.
And that's really what these tendencies are,
it's an encoding of what your priors are about what's going to happen.
So I'd say that the most promising way of representing this
is with fusions of probability and logic.
The field that is working on this in the AI community
goes by names of statistical relational learning
or relational probabilistic models.
And there's an edited volume that MIT pressed just last year,
which I'd recommend anybody who's interested in learning more about this.
The difficulty here is that the types of objects
and the predicates and the dependencies in these models
are also initially unknown.
We don't know what the model is initially.
So we're going to need another layer on top of this in our probabilistic model.
There's priors over how the world tends to behave.
And this we really have not a great idea how to do.
I've referred to one sort of path towards this,
which is Dirichlet processes,
which are something where there's sort of a hot topic in probabilistic AI these days.
It's a clustering model that lets you not say ahead of time
how many clusters or object types or predicates you need in advance,
but you can do Bayesian reasoning
to figure out what's the best number of clusters to explain my data.
Since we're short on time, I won't say much more about that.
All right, so you want to do all this learning,
what kind of algorithms you're going to use,
lots of options out there in the literature.
You're probably going to want to parallelize your interpretation over documents,
but still, it's really unclear how the scales are sufficiently.
And I see probabilistic inference as the major challenge
for at least the perception and state estimation side of AGI.
Okay, so how are we going to make progress on this?
I'd say that we should be able to demonstrate progress to the broader AI community
even before we have full-blown AGI.
And one way that we should be able to do this
is by showing year-to-year improvement
on some of the real evaluation data sets that people use in sub-communities.
So, for example, there is a Caltech data set on object recognition.
There are various challenges sponsored by Trek
and the ACE automated contact extraction workshops
on things like resolving co-reference between phrases and text.
There's something called the Pascal Text-to-Untailment Challenge,
which is about telling whether one sentence in a natural language entails another,
which involves a lot of reasoning about, well, if it's cold,
does that entail that the temperature is low?
That kind of question.
So, if we're building an AGI system, even if we haven't solved general AI,
we should be able to gradually improve on these data sets.
Another thing we should be able to do is serve as a resource
for more finely tuned but shallower systems.
Right now, a lot of these systems use resources like Cyc or WordNet
to get some kind of world knowledge.
Well, can we learn things that are better than Cyc and WordNet
for serving as resources?
Another way that we can interact with a broader AI community is
So, I think it's a promising pass. Thanks.

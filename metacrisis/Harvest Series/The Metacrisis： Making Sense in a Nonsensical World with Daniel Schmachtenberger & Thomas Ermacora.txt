I just want to say thank you for this welcome again from the team at Harvest.
It's great for me to have been able to suggest Daniel as a speaker here.
I'm very fond of him as a human, as a fellow futurist and as a thinker for our time.
I think it probably needs a little introduction. Why is Daniel here?
Daniel is not a prophet of doom, absolutely not.
He's trying to equip us with the understanding that will help us navigate some of the crises that are ahead of us.
It's something that some people call the metacrisis and he's one of the main people in the world now who can explain it very clearly.
I'll try and break it up a little bit so that we can go over some of the points again.
It's difficult to say what Consilience Project is in a few words because it has evolved over time to confront the reality of what the world is ready to accept.
However, it's certainly one of, in my opinion, the best attempts and very tangible attempts to help people navigate what's right now in front of us.
The news cycle is certainly not helping us understand. It's sort of distracting our minds from the real things that are going on.
So Daniel is one of these people out there who's working really to formalize the way that we can anticipate, connect, collaborate and work more effectively to solve problems that we are facing.
So Daniel, I think there's something that we should know about your past and I'm going to be provocative.
If you didn't actually start out with the same objective, you sort of thought that maybe we were doomed and that we had to accelerate doom in order to get somewhere.
I'm going to be very provocative so that you can explain your journey from a young man to where you are now.
I was homeschooled and when I was nine, I was at a gas station and a factory farming cattle truck pulled up and so I went and looked inside the holes.
The cow nearest to me was missing its eye and bleeding. All the cows were in terrible condition and I grew up loving animals, eating meat from factory farms, not putting the two together.
I was just kind of shocked and horrified by that. I asked my parents about it. They said that's where animals come from so I asked to go to a factory farm.
I became a vegetarian that day. I started working with PETA in Greenpeace and studying animal rights work.
That took me from the issue of factory farming to the issue of species extinction, whaling, overfishing the oceans, which took me into the issues of the environment and that took me into global poverty and just kind of the whole set of issues as a young person.
I was fortunate being homeschooled that I got to just have that be my curriculum. I was actually an educational experiment. My parents ran right and didn't have any fixed curriculum.
I just got a study that I was interested in so this became the context of most of my life study and the next really important part happened.
I was 13. I was working on a project that Greenpeace and World Wildlife Federation were leading to protect elephant poaching in a particular elephant preserve in Kenya where the poachers had come in over the preserve and hunted a bunch of elephants.
I saw the videos of the elephant slaughter and was moved in the same way as the factory farms and the thing I observed though was over the two years of the people on the ground working extremely hard to protect the elephants there,
the strategy was get bigger fences around the preserve so the poachers couldn't get in and do legislation to get harsher sentencing for poachers in the area.
All the activists had their life threatened but they finally succeeded. It was a huge win. Those elephants didn't get hunted but they didn't address the poverty of the people that were doing the poaching.
They didn't address a macro economy that creates poverty at scale. They didn't address the views towards animals or identity regarding them or black markets on animal parts.
The same poaching groups moved to hunt other things which happened to be the white rhino and the mountain gorilla both of which were more endangered than the elephant.
I was working with enough groups that I got to see those other issues get worse as a direct result of the success of that project.
And then that was like the second existential hit. The first existential hit with the factory farms was, I'm saying this to relate,
that was the first time I had like a suicidal ideation around not feeling okay being complicit with my species.
My species was somehow fundamentally fucked up and as a kid it wasn't that hard to realize if I kill myself all those cows are still in the factory farms.
I didn't help them. That's unethical. But then I'm like, if I stay alive to try to help it, can my life be a success while sentient beings are experiencing that much suffering?
What does it say about me that I can be totally stoked that my life is a success when other beings are in that kind of suffering?
I'm like, there's no answer for me that doesn't address that. So I remember thinking to myself then I'm like, alright, well if I die and factory farms still exist,
I failed like there is no definition of success for me that isn't a sociopath's definition of success that has to empathetically separate too intensely.
But at least the hope came that like activism might work. And so then when this thing happened where I saw that the activism for one thing made other things worse,
that was the next kind of devastation. And that led to me starting to look at how many other places where we were doing activism,
where the world was doing activism, did that kind of issue. And I saw that there were projects to solve global poverty by creating hydroelectric dams to bring electricity to areas,
and hydroelectric dams, of course, drowned whole ecosystems and extinct species, and on and on. And kind of like it seemed like the world was caught in these trade-offs where when they would focus on solving one problem,
it was too narrowly defined, the result of solving that problem would have externalities related with larger systems. I started to do kind of historical analysis on that,
and that became kind of a defining trend of all of the issues. We created the automobile to solve a transportation issue, which was that horses were a very limited means of transportation.
Transportation, horses in the cities caused a lot of horseshit in places like London. Literally the excessive horseshit in the cities of London was one of the kind of like significant impetuses to make a horseless carriage.
And in solving that problem and creating transportation that increased mobility and comfort for the whole world, climate change, like the venusification of the entire planet,
and oil spills, and wars over oil, and the US petrodollar, like all of that were the side effects of solving a transportation issue, you know, a couple hundred years later.
And I got to see that there was this underlying deep issue that statecraft, that is thinking about the well-being of its own citizens, the rest of the world,
so it's not just exporting its harm somewhere else and of the planet. And there's a lot of countries where they're genicoefficient,
meaning their measure of wealth inequality is pretty good within their country, but they import a bunch of stuff to make their country function from countries that have the worst genicoefficient.
So you can't actually do genicoefficient at the level of a country, it ends up just being a way to kind of whitewash the reality of global supply chains.
Oh, so the conscious statecraft thing. So how do you do what is good for your own country, what's good for other countries,
and what's good for the planet when there are fundamental trade-offs between them and you're stuck in those trade-offs?
That becomes really the deep challenging question. If we look at climate change today, we can see that for the planet and for all of us in the future,
we should not just decarbonize but degrowth. We can also see that any nation that tries to lead that will do worse GDP-wise in the near term,
which means worse geopolitically and actually so much worse that, particularly for the leading countries like U.S. and China,
change the control systems of the world. So so many of the people in the U.S. that are anti-climate change aren't actually anti-climate change
because they did a good analysis of the IPCC science. It's that the solution of increasing the price of carbon for us,
which will decrease our GDP relative to China's GDP in a great power game, which will increase planetary autocracy,
seems like such a bad solution that they don't want the climate change assessment because they don't want the solution that goes with it.
So a part of the thing that I focus on is how do we understand the interconnectivity and the generators of all the issues deeply enough
that we can come up with solutions that don't cause other problems. But one point was a little while after this I had come to,
I started doing forecasting on how many people have read Limits of Growth, the MIT Club of Rome book.
So I read that as a teenager and then started looking at a lot of other analyses on species extinction and biodiversity loss
and dead zones and oceans and all those things. And then I looked at the, you know, Stephen Pinker kind of stuff of everything's getting better.
A lot of people might have that question of like, you hear Stephen Pinker, Hans Rosling Gates and like everything's getting better,
and then you hear all the environmental statistics and like everything's getting worse. They're both true.
You can cherry pick your stats, but they're not equally true. The things that are getting worse are leading to the unviability of human habitation
ongoingly in a way that the things that are getting better don't converge on solving automatically. So there's a real,
and the things that we're making better are causing the things that are getting worse towards self extinction points, right?
Like grow global GDP where we all have like nice stuff because of that in a way that breaks the biosphere's capacity to continue to make life possible.
So we have to do something deeper than saying it's getting better and worse so we just choose. It's like the way that we make things get better
is a major part of what's making them get worse towards tipping points that are unique now to any point in previous civilization.
So when I was at 15, I came to the assessment that there was no chance that humanity would actually make it in time,
and that if we had radically less population, we might have a chance, and I started studying viruses and depopulation strategies,
which is a thing that an angsty but earnest caring teenager might do. I had a spiritual experience that took me off that path while I was working on it,
but I was seriously studying vector delivery of novel pathogens, and the spiritual experience was something Bucky Fuller used to say,
and I had heard it a lot but it hadn't hit me in the same way that the chicken developing inside of a shell is totally unsustainable, right?
When it's eating the unrenewable resource, the egg white, it's creating metabolic pollution in its environment, it doesn't even know that it's inside of a shell,
but it's in a developmental phase, and of course it doesn't have the beak or the gastrointestinal tract to eat seeds yet,
the whites are exactly kind of what it developmentally needs to go through that embryonic phase, and right as it runs out of whites,
as soon as its GI tract and beak and all the things are developed to be able to crack through the shell and emerge into a world where now it's part of a new phase,
and I started thinking about humanity as being unrenewable because of being in a developmental phase or a discrete phase shift that isn't the result of just a continuity of the previous lines,
and I thought about all the examples of caterpillar to chrysalis to butterfly, and that in the chrysalis there's a fundamental dissolution of all the organs
and a restructuring with a different genetic code that if a fetus went more than 40 weeks in the mother's uterus, it would kill itself and kill the mom,
but it has to go through that developmental period, and then the birth is a discrete kind of difficult process, and then the umbilical cords cut,
now there's a new developmental time, and so I had this kind of profound experience that developmental phases, whether inside of an egg or in a chrysalis or in a womb,
are always unsustainable. There is some discrete nonlinear phase shift that is different than the curve in the developmental phase,
and will be different afterwards, and I started thinking about if humanity as a species was in a developmental phase,
and there was a discrete phase shift that was different than the curves, what that might look like, but that got me off of the track that Thomas was bringing up,
but what's really interesting is a lot of my work now involves the way that exponential technology equals exponential decentralized catastrophe weapons for everyone.
Whenever we talk about the positives of exponential tech, AI, biotechnology, nanotechnology, cyber technology,
we talk about in positive terms usually like the democratization of this great technological power of creation,
but the democratization of catastrophe weapons is actually not a great idea, right? Like keeping nukes to the G8,
but you can do because it's really hard to make nukes, and you can see who's making nukes, and there's not that many places that have uranium,
and enriching uranium is hard, and you can see it from outer space, but it's pretty easy to make drone weapons,
and it's increasingly becoming easy, and the ability to use those for infrastructure targets, and specifically it's becoming pretty easy to make pathogens.
In the advancing of synthetic biology, we're only a few years away from the ability to synthesize novel pathogens on a desktop for $1,000 anywhere,
and so when I was a kid, that was not true, it was actually like a hard thing to do, it's becoming an increasingly easy thing to do,
while increasingly more people are feeling kind of concerned and disenfranchised, so as the group of people that would be motivated to change this world system in harmful ways,
and the group of people that are capable is converging and increasing, there's a lot of risk associated with that, how do we deal with that?
That's an interesting part of the topic.
I'm really excited that you didn't turn out to become a terrorist.
It wasn't motivated by not liking people, it was motivated by seeing self-induced human extinction as inevitable, and as being the only way out,
and that's actually like most terrorists are well motivated, they're motivated in service of something they care about that they feel is being harmed and they don't know other solutions.
We could agree, but I'll pause on that one.
I think what a lot of people here are probably expecting if you have heard anything about Daniel is you present a little bit about the meta crisis,
so there's a framework of thinking around how we can put dots in between a lot of the crises that we're facing collectively,
and instead of addressing one and creating externalities in another problem set, perhaps understanding the total problem set at the same time,
and before you act actually have a more enlightened perspective gives you more edge, so that's an assumption and a hopeful one,
and some people might argue that we need to act very fast on certain issues, so they don't really like this sort of temporization of it,
but I'm of the belief like you that basically having better tools to understand the world we live in is essential for us to make critical decisions,
and I would love for you to spend a moment unfolding what you call, or what we call, the meta crisis,
and why it's unique to this moment in history, and why humanity has to put some effort towards solving that concretely,
or we will probably end up in a pretty bad place very quickly.
So it could easily seem like the stuff that I'm talking about is just the collection of all the worst news in one place.
I would bother sharing it because I think it is true.
The risks that we're talking about are true, it's not determined that we definitely fail at them or that we definitely succeed at them,
so what we do actually matters in determining it, and there's no chance that we can solve it if we don't more competently understand it,
so more people competently understanding and seriously working to apply themselves to the unique needs of this particular time in the world
is something that I'm hopeful for, and I guess that's why I'm speaking here, and said yes to Thomas inviting me here.
So the meta crisis frame might actually help deal with some catastrophe fatigue because rather than see the issue of drones and autonomous weapons
and the issue of exponential tech-empowered terrorism and this planetary boundary issue and this pollinator issue and this forever chemical issue as separate issues,
I look at them all as expressions of an interconnected set of generative dynamics that we call the meta crisis,
where what it takes to solve any of them is the same, actually, and if you try to solve them without factoring these deep underlying generative dynamics,
you will at bet, you probably won't solve it, and if you do, you'll displace problems somewhere else and actually kind of mess the whole thing up,
so when you understand that all those problems are connected at first, it makes it seem more overwhelming because you're like fuck,
to think about climate change, I also have to think about geopolitics and fundamental changes to finance and all these other issues,
and that seems like a lot of complexity, but it actually takes it from too many problems to tractably manage all of which have solutions that end up externalizing harm elsewhere,
which means impossible, too hard but tractable, so hopefully in terms of being able to see it all as one interconnected set of issues,
you're like alright, there's a lot more learning that I need to do to be able to competently engage, but there is actually a way through,
there's actually kind of a tractable analysis, so that's what I hope to share.
So, if you can move towards what people call the third attractor or that you like to call the third attractor,
so that we sort of gravitate towards a landscape of defining what is the solution environment that we want to find ourselves in,
because obviously there are a lot of people who are very competent, who are deploying intelligence and capital and being very innovative about what they do,
but they sometimes miss the fact that they are creating really negative externalities in other, let's say problem sets,
so the third attractor to me seems like a very easy thing to understand for people, even though if we don't have the answer to what the third attractor is,
but it's at least theoretically a framework for understanding why the metacrisis may find a solution set through a third attractor.
Yeah, let me give an example though of how we solve problems and make worse problems that are current and really relevant.
We're working to try to change currently US federal government program and have had some really good success with it for pandemic prevention,
for preventing whatever the next pandemic from animal sources do not expel over would be.
And the federal government, and it's not only the US, lots of countries employ this approach.
The US has been leading the way in what seems like great science and technology and innovation towards preemptive problem solving,
which all seems like the right thing, but the approach to preventing zoonotic spillover involves viral hunting,
so going out and finding tens of thousands of new viruses in bat caves that have mammalian viruses that have never been exposed to humans before,
bringing them back to labs, doing gain of function research on them to figure out how they might mutate into things that are more virulent or transmissible,
and then publishing all those genome sequences to an open source database so everyone who wants to work on vaccines has access to the knowledge.
It seems like a decentralizing information, democratizing, multi-state coordination, science, anticipatory, good thing,
and it's maybe one of the worst things happening in the world, and so we're lucky that we've been able to shift it.
If you open source publish all of the pandemic grade viral gene sequences in an age where gene synthesis is becoming extremely cheap,
that we're about three years out from tabletop gene drives and CRISPR and like that, the bioterrorism potential of that is just unimaginable,
and even just the accidental kind of lab leak dynamics that when you have enough labs working with enough things,
the probability that none of them happens drops towards zero over enough period of time.
There was a lab doing gain of function research that figured out how to make an extremely virulent version of H1N1,
like an R0 of 18, H1N1 is like a 60% fatality rate, and it did that in a biosecurity level 2 lab.
So lab leaks happen, right, like this is an example of trying to do the right thing,
but not understanding the problem space well enough and doing something totally that's the wrong thing,
and what I remember the first time I was engaged in the UN network and it was a project with World Food Program when I was 20,
the solution to world hunger involved bringing conventional NPK based agriculture to the developing world,
so we didn't have to send food over there and the answer was way more nitrogen and phosphorus effluent into the rivers that would cause faster dead zones in the ocean,
and so I talked to the guy about it and I said, you realize you'll speed up the rate of dead zones in the ocean catastrophically if you do this,
and he said I hadn't thought of that, but those aren't the metrics I'm tasked with, and those aren't the metrics I'm tasked with,
so I'm going to, for a few years, decrease hunger while working to extinct the planet because that's what my accountability is like.
It just became very clear that that problem-solving approach was ubiquitous,
and so another great example is you look at the advancement of something like gene editing, CRISPR,
it's being advanced for purposes we all want, like immuno-oncology, how do you change 15,000 genes at once to be able to, you know,
cure and prevent cancers that we're genetically predisposed to, but all technologies are dual purpose or multi-purpose,
meaning every technology that you can make for some positive purpose has a military or otherwise weaponized or kind of externalizing application,
so the research that's being done on how to do that type of gene editing is making it then really cheap and easy.
Once we figured out how to do it, it takes major universities that have ethical review boards to do it for that purpose
to develop the technologies that then drop the price by orders of magnitude to do it for any purpose and it's open publishing,
so to give a little bit of history because some people might think, well, people have been predicting rapture since the 1600s,
there's always some kind of like Mayan 2012, whatever, and this is just new catastrophism.
I would really like people to think more deeply about what is discontinuous and novel in this time relative to other times,
so I want to argue that real quick.
The first technology we had that was powerful enough for humans to actually make the planet meaningfully uninhabitable was the nuclear bomb in World War II.
That was the first truly existential technology. It was not an exponential technology, meaning nukes don't automatically make better nukes
in the ways that computers automatically make better computers. Computation allows us to design better computer chips recursively,
you get Moore's Law, but it was the first existential technology and that was really a break from the history of the entire world of tech
up until that point because up until that point, every new military tech that we had, there was an absolute arms race to deploy it as quickly as we could
and to win more battles and territory based on deploying it. This was the first one where we actually had to make an entire world system
to ensure we would never deploy it because nobody would win, mutually assured destruction.
All of a sudden, you're like, whoa, we're so big that we can't actually deploy our tech without destroying everything.
We can't actually do the us versus them effectively anymore at that level.
So post World War II, and we haven't had another world war that is kinetic between superpowers yet and we're at the brink of it right now.
Post World War II, because of that tech, we rebuilt the entire global world system to deal with preventing kinetic World War III
and that kind of Bretton Woods world system, IGO world system has been effective at preventing World War III,
but that world system is almost totally broken down now and it drove all of the catastrophic risks we're facing currently.
Specifically, one part of the post World War II system was a international monetary system that created, that had exponential growth of GDP.
Why is exponential growth of GDP important is because the wars are based on everybody wanting more stuff
and if you don't have exponential growth of GDP, the best way to get more stuff is to take somebody else's stuff.
If you have exponential growth of stuff, everybody can have more stuff without taking other stuff roughly
or at least the major nations don't have to take each other stuff, they can do it through colonialism or vassal nations,
but exponential growth of GDP is comprehensively bad for the environment.
All of that growth of GDP is an important thing called the Garrett relation that shows a one for one correlation between energy used and global GDP.
It's a 99% correlation actually, meaning that the increases in efficiency of energy generation only give you about 1% change of more dollars per joule per year,
but for the most part, exponential growth of GDP equals exponential energy demand.
So climate change and exponential GDP are exactly correlated and all of that money gets used in a materials economy, a supply chain,
it's a linear materials economy that through mining, logging, fishing, et cetera, is unrenewably taking stuff from the earth on one side,
turning it into shit we use for a little while and making it into trash and pollution on the other side.
You cannot run, this is like so obvious, but you cannot run an exponential financial system on a linear materials economy
that has to be coupled to it on a finite planet forever, so you start to hit planetary boundaries, right?
So what decreased us having likelihood for war moved us towards planetary boundaries on all the planetary boundaries.
We said we can all have more stuff without taking each other's stuff by taking all the shit from nature as quickly as we can.
So this was obviously not that smart for our own long term and now we're there where we're actually bypassing some of the planetary boundaries critical tipping points already.
There was a paper published a couple months ago in the American Chemical Society Journal that said the planetary tipping point on certain environmental pollutants,
particularly floral surfactants, had already been passed, meaning rainwater all around the world in very remote areas contained these PFOS forever chemicals beyond EPA safe levels.
That means that if you're gathering rainwater in the middle of nowhere for your off-grid sustainable thing it has beyond EPA levels of carcinogen,
neurotoxin, endocrine disrupting chemicals everywhere, no matter where you are on the planet because we've put that many of them into the environment already.
And an exponential financial system means an exponential amount of pollution, mining, etc.
And so the metacrisis, what happened for me was like, all right, well if we have to address factory farms but then shit we also have to address overfishing,
we have to address what is the problem set? What is the actual problem set that we have to face?
How do we make sure that when we're addressing it we don't cause other worst problems?
So how do we understand the interconnectedness?
I was always asking if we were to actually try to rebuild the world from scratch with 21st century problems and technologies and capacities that actually worked with the biosphere and human nature,
how would we do it and then what does enactment look like to get there given all the vested interests and issues in the current world system?
So you were mentioning third attractor. Third attractor roughly is there are two futures that we want to avoid.
Well, mention the first and the second first so that people can understand the sort of negative image of it.
But maybe just before you go there, just maybe explain what you really think, if there's a takeaway, what's absolutely different about this moment in history for mankind?
You've kind of said it in a roundabout way, which is explaining the sort of crumbling of the Bretton Woods world system
and that a lot of the solutions that we're trying to put forward actually generate bigger problems than we can possibly face.
And in many instances, and you can talk about in the situation of the arms race between China and the US with AI or biotech, for example,
without even talking about road actors that are in garages, but I think if you can sort of map this a little bit,
because I think that it's helpful for people to see their place in history.
Okay, so I was saying that the first existential tech was the bomb. We built a world system to deal with that.
So one of the answers to that world system was the global financial system, which has driven us to all of the planetary boundaries that we currently face.
Planetary boundary is a good way to put all the environmental issues in one place.
So when we're talking about dead zones and oceans, overfishing, species extinction, loss of pollinators, climate change, ozone,
all of those are basically places where the human social sphere, techno sphere complex is incompatible with the biosphere,
but that means we're debasing the substrate that we depend upon. That means it's a system that's self-terminating.
You cannot debase your own substrate forever.
One of the other parts of the post-World War II system was globalization, and these radically interconnected six continent global supply chains
that are necessary to make this microphone or these speakers or anything.
One of the downsides of that we've seen during COVID is the radically interconnected supply chain.
The benefit was you're less likely to bomb somebody who you depend upon for fundamental supply chain purposes.
This is one of the big benefits of globalism.
So the localism movement, if you were really successful at localism, there's actually less investedness in the other guy over there
when I don't actually depend upon them.
So these are some of the tensions we have to factor of, okay, do I want to make everything local,
or do we want to actually have interdependence on them?
But if we have it all global, then we get these cascading fragilities of where you can have an issue in Wuhan
and get supply chain shutting down all around the world, which then means you don't get the movement of pesticides and fertilizers
for the agricultural system in Iran and Northern Africa that probably put more people into radical food insecurity
than we're totally at risk from COVID.
So you can see how those post-World War II situations gave us this world of high interconnectedness
but very high fragility and then high planetary fragility.
And then also the exponential financial system meant the growth of exponential technology, you know, speeding up commerce.
And the key thing to understand about that is that we don't have one technological weapon of mass destruction.
Now we have many.
And in the World War II system, the mutually assured destruction system, you had one catastrophe weapon and two actors that had it.
So you could create a system of mutually assured destruction where neither one could utilize it.
We currently have a world where you have dozens of catastrophe weapons.
If we include all of the types of not only weapons of mass destruction but the ability to take out critical infrastructure
and in a highly connected supply chain system, we have dozens of catastrophe weapons with not just many state actors
but non-state actors having access to them.
So you can't put some mutually assured destruction system on it.
How do we make it through this much distributed technological power with the current incentive systems?
So if you want to look at what is unique to this period of time, humans have been here for roughly 200,000 years,
biologically identical, you know, 2 million years of hominids with tools.
We didn't reach the first billion people till 1815, right?
We were less than a half a billion people for that entire history.
And then with the Industrial Revolution and liquid nitrogen fertilizer, we went from half a billion people to 8 billion people almost overnight.
We simultaneously increased our energy consumption per person and our total resource consumption per person exponentially.
So we exponentially increased the number of people and the consumption of resource per person.
And this does bring us, so for the whole history of the world, we did not have the technological power to quickly destroy everything
like nukes or even to pose a threat to the biosphere as a whole.
That is only the result of post-industrial and now particularly post-late industrial technological capacity.
A really important thing to understand is, you know, as a species, we, because of tool creating, were able to move from early environments
to other environments in a way no other animal could and become the apex predator in every environment.
So when we over-hunted an environment, a lion can't increase its predatory capacity radically faster than the gazelles can
because predatory capacity only comes through genetic evolution, which is very slow over time and there's co-selective pressures.
With tool making, we were able to increase our predative capacity through a different process that wasn't genetic evolution radically faster
than anything else could increase its resilience to our predative capacity.
So we could over-hunt an environment then rather than have our population fall back, move to a new environment.
And so we've actually been on the beginning of a self-terminating path for a very long time.
It's just an exponential curve that looks like this.
And the first major bump was agriculture and then the next major bump was the industrial revolution and then it's been verticalizing.
And so regarding earlier civilizations, though, we didn't have the technological capacity, even in aggregate, to mess up the biosphere.
But we did have the technological capacity to mess up our own local environments and that's one of the main reasons early civilizations died.
If you actually read The Collapse of Complex Societies by Joseph Tainter where you read Jared Diamond's book on it,
many early civilizations actually died because they created topsoil erosion from bad agricultural practices,
cut down too many trees and stopped being able to feed their people.
So civilizational collapse from overuse of the environment is actually a multi-thousand year reality.
And if you think about early civilizations, one of the first insights you'll have,
whether we're thinking about the Ottoman Empire, the Egyptian Empire, the Roman Empire, is that none of them still exist.
So it's actually the precedent of civilizations to have a life cycle and to fall.
But most of them fall from internal self-terminating causes, either environmental ones or even if they lose a war to someone else,
very often they lose a war to a smaller foe than they had defeated during their peak
because internal decay and infighting happens from generational institutional decay.
So self-induced purposes make civilizations break down.
So civilizational collapse is actually the norm, right?
That's the first thing that's important to understand.
It's just it was always a local phenomena.
This is the first time that we really have a global civilization in terms of the supply chains that we depend upon to meet our fundamental needs.
And so we're in the process of a breakdown of this civilization.
But what that portends in scale and what it portends to be having the biosphere, the ecological effects,
but at a biosphere level is totally unprecedented.
And what I would also say is that our solutions to the previous problems,
because there's this nice narrative that there have always been problems,
but we always come up with solutions and necessities the mother of invention
and we'll figure our way out of this and then we get to live to solve new problems.
And that's kind of true.
But it is also true that the problem-solving process we have employed actually drives larger problems.
And you get to a place where those problems are actually beyond the scale of what the biosphere can handle in human capacity.
And so you actually have to have a different problem-solving process.
So if you think about making a technology to solve a problem or a business or a law to solve a problem,
you define the problem in a narrow way, like this viral issue or like a transportation issue or whatever it is.
You define it in a narrow way.
There's one or some small number of metrics you're trying to change.
And you create a technology or a law or a business or a non-profit to produce a first-order effect,
meaning a direct effect to solve that problem.
But it interacts with ecologies and societies and psychologies which are complex
and it has second and third and fourth-order effects on a whole bunch of metrics that aren't even identified.
And that's where the harm ends up being externalized.
So the metacrisis that we face currently, we can talk about very specifically what the generator functions are.
But if we look at all of the planetary boundaries from species extinction to biodiversity loss writ large
to nitrogen and phosphorus cycles to climate change and ozone,
all of those are the result of an exponential financial system coupled to a linear materials economy hitting planetary boundaries.
So you have to fundamentally make the materials economy go closed loop
and the financial system has to stop being exponential, which means a post-growth financial system.
That's a really, really huge lift to get from here to there.
And then when we look at all of the issues associated with externalization from exponential tech,
how do we steward the power that exponential technology gives us safely
does require a totally different level of consideration of like, yes, I'm making this immuno,
I'm doing this genetic modification science for cancer purposes,
but as soon as I've done it, it now becomes cheap and easy for designer babies and every other kind of purpose.
So how do we kind of mythopoetically say if no other animal had the ability to extinct species at scale
or destroy ecosystems or genetically engineer new species?
So this is not the power of apex predators.
This is the power of nature, the power of gods.
If we have the power of gods and not the love and wisdom of gods to steward it, we don't make it.
So to make it through this technological kind of adolescence, what is the infrastructure that can make it
and what is the social structure and culture that are required to be able to guide that are core interesting questions?
So you've very eloquently defined how good we've been for a long time at being self-terminating civilizations
and we've scaled it to the planet now.
So I think to go back to my question around the first and second attractors and then the third,
maybe browse very quickly on the first and the second because there's a whole theory around that.
But there's a theory of change which you're proposing through the third attractor and I think that's where there's hope.
And not only hope, it's sort of a story of collective ingenuity that has to unfold
and unfortunately you have to go through a little bit of a difficult phase to appreciate the complexity of the problem
with a new set of eyes before you can do so effectively.
So that's how the lens of the metacrisis is useful because you can't get to that third attractor before you've understood that.
So could you maybe just go quickly over the two first attractors and then the third one that will give us a little bit of a glimpse of hope?
If we just think about, if you can build some gene synthesis in your basement cheaply with no exotic materials,
if anybody can build gene synthesis in their basement, how does the world make it through that technology being a distributed capacity?
If you think about it for a while, the answer almost everybody comes up with is it can't.
So that can't become a distributed capacity.
Okay, so we have to make it to where the companies that make the gene synthesizers are regulated.
What about the DIY version that makes it on the internet?
Well, now either we have to control information on the internet and some kind of, so who does that that can radically control the information
or we have to know what people are doing in their basement, some kind of ubiquitous surveillance.
This is some of the thinking behind the IoT and sesame credit system in China is actually not stupid thinking.
It's forward thinking to distributed exponential tech and how do we deal with that?
So you can see that the solution to preventing a catastrophe can be a control mechanism that can look like a dystopia.
The two attractors that I say we want to avoid are catastrophes on one hand and solutions to catastrophes that involve being able to keep those from happening,
which requires both optics of what's happening and the ability to prevent it, which sound like control mechanisms, which become dystopic.
And so right now, one thing I'll say about the catastrophes is you have to look at the cascades between all of them together to make good sense of it.
If you look at exponential tech as one category and environment as another and war as another and supply chain and electrical grid separately,
you'll miss the way it actually happens.
So when does climate change become a catastrophic risk?
Well, you're talking about like the venousification of the planet or the drowning of coastal cities or something like that.
Long time before that happens, extreme weather events that start to hit high population density areas and cause human migration
can cause escalation to World War III, right?
So you think about the extreme weather events in Australia.
It was just very fortunate that that was low population density, low total population area.
You look at the droughts in Syria that caused population movement and did cause a war.
What if you look at the temperatures that Pakistan and Bangladesh and Northern India have been starting to hit during the summer
and just say some time in the next few summers you get past the temperature where the crops fail
and they don't actually have stored crop.
A lot of it was destroyed during COVID.
They don't have groundwater and when you're in a 50 Celsius heat wave,
but you're talking about an area that has 100 million people now as opposed to a very small number of people.
What happens?
Does the resource war fall along Muslim Hindu lines?
Does that lead to an India-Pakistan war?
So with climate change, you're not looking at climate change just as a problem itself,
but it is a force amplifier of the other problems and you have to look at all of them that way.
So there's a lot of different entryways to catastrophic collapse and we're seeing some of them right now.
And so one attractor is increasing catastrophes.
The other meaning a likely path the world goes.
Another attractor is the world recognizing that and saying,
should we have to keep that from happening?
So we have to actually deal with climate change through pricing carbon properly and degrowth.
But that is really bad for a lot of people because as soon as you make carbon more expensive,
you make all the commodities more expensive, which hurts the poor people the fastest and on and on.
And everyone who doesn't want that, do you use violence against them?
And so to prevent certain things ends up meaning control if currently human behavior is doing that.
And so how do you prevent increasing dystopias, which a lot of people think the Chinese state is in the direction of.
So a desirable future is a future that doesn't self-terminate and it doesn't have unchecked centralized power structures.
The question, one of the causes of the increase in nationalism is the distrust in globalism.
And one of the major reasons of the distrust of globalism is the idea that unchecked power doesn't have a good history.
And so the idea that we at least keep power checked in a multilateral way is preferable for a lot of people.
But if you have many different nations that are in economic competition with each other, nobody wants to price carbon properly.
Because if the US does, it'll be radically disadvantaged relative to China.
As a result, China's Belt and Road will geopolitically dominate the world and crowd out democracy and those types of things.
And so if you have nation states in competition with each other, they just can't deal with global issues well.
If you have global governance, who's creating a check on that power system to where it can't be captured or corrupted.
So I would say thinking in terms of the design criteria we need has to be able to do global governance.
It has to be able to deal with things like decentralized catastrophe weapons and basements.
But it also simultaneously has to have checks and balances on every power system or every control system that are there that are adjudicatable.
It would take me a long time to describe what I think the best processes for how to do that are.
I will say that there are technological systems that could be enacted that meet these criteria that align with human nature.
So I am optimistic about that. The enactment to get there takes a lot of work.
So just briefly mentioned the third attractor.
I'll move afterwards towards some hopeful scenarios that we might encounter.
So that people don't leave this room with a sense of fear and catastrophe as being dominant.
But in order for us to create a third attractor, we have to put some energy towards developing one.
And that's not a simple thing to do.
But first we have to understand what it is and how they could drive us away from those two attractors.
Decentralized catastrophic capability and centralized capacity to control which is kind of dystopia.
There is not a term. There is not like a term for a type of political economy or system that makes that third attractor.
And we actually don't know.
So specifically what I mean by the third attractor is literally something that can prevent all the catastrophic possibilities
where the control mechanisms required to do so have the types of checks and balances that they prevent centralized power issues.
So we are actually defining in terms of what it isn't.
It's not catastrophes and it's not that the solution of the catastrophes involves dystopias.
Exactly what that looks like, there's both how would we design it from scratch which is a nice question
but ends up being kind of a nonsense question because we don't get to design it from scratch.
There's this enactment issue of with all of the vested interests that the world currently has in play how do we actually get there.
So I can give you a few examples of things that give the sense of what can make a third attractor possible.
So I'm just going to mention one which I think you'll probably refer to.
We have a common friend in Will Marshall.
Were you going to mention Planet Labs?
I wasn't but we can.
Okay.
Well I mean so Planet Labs, I'll let you talk about it but effectively I think it's interesting to see also that some of those third attractors
reside in the domain of intelligence plus or human intelligence plus technology applied to a new level of
what you've actually yourself called force of transparency.
And that probably is not, it doesn't define exactly what the third attractor is but it's a sort of hopeful way of looking at technology
that can constitute an underlying, let's say, conversation between different actors whether they're state or civic actors
or technology bodies that can start to formulate more of those.
So I think we both agree that there needs to be a proliferation of these examples and force transparency is a really important tool
because we've got very weak international governance and law.
Yeah.
So maybe you can address the third attractor by giving some examples.
Yeah so market forces like incentive by itself doesn't solve all the problems.
So you end up having to use both incentive and deterrent.
If we didn't have law protecting national forests, we wouldn't have national forests.
You'd have market forces that continue to turn everything into commodities.
But as far as international law, it's tricky because law mostly exists where you have a monopoly of violence that can enact the law.
A police state inside of a nation state so where we have global issues like the oceans or the atmosphere, this is where we have a really tricky time
because how do you, if you're going to make a law, it requires multinational agreement and then the ability to see if it's being violated
and then the ability to enforce some enactment and the ability to enforce it where it's not more expensive to do so than the benefit you get, right?
So let's say that we have an agreement about oceans and China's violating it and so we say, okay, we're going to sanction you for that.
But the sanction is on supply chains that we depend upon and if you escalate, they also have nukes.
So it's like there's a tricky thing with all that.
But one of the places where a lot of environmental issues, global environmental issues don't get legal support is where we just don't even know what's happening.
It's hard to know what's happening in the middle of the Amazon.
It's hard to know what's happening in the middle of the open oceans.
So this particular example is where technology can be repurposed in a positive way.
There's a satellite imagery of the Earth is a pretty amazing technology.
There's a friend of ours who runs a company called Planet Labs and they image the entire surface of the Earth every day.
30 terabytes of compressed data but they're increasing the spectral and temporal resolution of that and spatial resolution
such that it'll be pretty much real time human level video capture of the surface of the Earth in about three years.
Which is amazing and one of the things it means is the ability to see where logging is happening
and where mining is happening and where dumping is happening and where legal fishing is happening
and even to be able to see in a dead zone in the ocean the effluent, how much of it came from which source,
how much came from which port and all those types of things.
The ability to see all that and use machine learning to process it means that there's a whole bunch of international law
that we've never even bothered to create because there'd be no way to know if it was being violated or enforced.
Now we'd have the ability to create international law that says,
no we actually don't have plausible deniability anymore.
We know exactly how much of the trash or the nitrogen effluent came from there because we can see the whole thing.
It also has the ability to do spectral analysis that can see an invasive species entering an area
or soil microbes in an area to be able to actually support the environment when critical issues are starting to happen.
But this is itself very concerning because probably many of you, even as I'm describing this, are like,
wow, that's really hopeful for the environment to be able to have that level of transparency that could create law
so we could support the environment.
But fuck, who gets to have access to video level data of the entire surface of the Earth all of the time?
That sounds like pretty massive surveillance capability, which it is.
So that can prevent certain catastrophes but can totally create dystopias depending upon how it's managed.
So how do we create the governance of that information such that it doesn't get used for nefarious purposes
and that people get to know what it's being used for?
This is not trivial, right?
Because it's easy to deploy the technology to solve those problems.
It can be quite hard to create the governance to ensure that it's used properly.
Well, the official version that Will gives me is that it's 50 centimeter resolution so you can't see a face.
It's easier to count trees and cars and tanks in Ukraine when the Russians pretend that they're leaving a certain battle area.
But let's agree that, you know, that power is something that needs to be at least checked.
So what I guess I was trying to get at is there is some hope in terms of how we can leverage technology
in order for us to sort of monitor and then have some checks and balances
and also create the international agreements and legal frameworks to enforce some form of limits, if you want to call it that.
So something I'd like to say is there is a naive techno-optimism that I think is super dangerous
that just tech will solve all the problems and the very worst version of it is we're only a few years from generalized AI
and then that'll be able to solve all the problems.
If you study the alignment issue of how do we ensure that truly generalized AI is aligned with our interests,
it's a really, really tricky problem.
There's also a naive anti-tech, a kind of naive Luddite perspective that's like,
man, all these problems are because of excessive tech, quality of life is actually better at a lower level of technology.
Let's get back to the land and permaculture and that kind of thing.
But if you study the history of the world, any time there is an intersection of a less technologically advanced society
with a more technologically advanced society, it doesn't go well for the less technologically advanced.
So the China-Tibet type interaction always happened and so whatever this group of people are saying,
we're going to do the less technologically advanced will not actually influence the direction of the way the future goes
because the technology is power which does mean influence.
So the future will be high tech but it has to be also high nature and high connectivity or it will self-terminate.
So you have to say, we don't get to put the Pandora's box of tech closed,
but we have to actually become wise stewards of it.
So what does a high tech, high nature, high connectivity future actually look like?
And if we don't have the technological capacity for outsized influence over the current systems,
the current systems will be what dominates.
And so I am a techno-optimist but not a naive techno-optimist,
meaning I know that all the existential risks we couldn't do without tech.
Stone Age people cannot destroy the planet.
So I'm acutely aware that all the catastrophic risks are results of human activity extended through technological levers.
But I'm also aware that the solutions to those things can't avoid technological elements,
but the technology alone is not sufficient.
So one way we think about this, there's an anthropologist named Marvin Harris,
and he said you can think about civilizations as these triples of what he called the infrastructure,
the social structure, and the superstructure.
The infrastructure is the tech stack that the civilization depends upon and meets all of its needs with.
The social structure is the collective agreement field, so economics, governance, law, and the institutions.
And the superstructure is basically the culture, the values, the identity, the definition of what the good life,
what we're motivated by are.
He particularly argued that they are interconnected, but the tech changes in tech drive the other ones.
And he gave a heap of examples from the plow to the wheel to on and on,
where a change in tech meant that whoever used that tech, now they're behaving differently, right?
Driving a plow is a different set of behaviors than hunting.
But no tech catches on if it doesn't provide adaptive advantage.
If it does provide adaptive advantage, it changes pattern of human behavior by using it.
By changing human behavior, you also change human values,
and then everybody else has to adopt it, or they lose in war to whoever has that increased adaptive advantage.
So he basically said cultures and political systems change because tech changes.
There are other deep anthropologists and sociologists who say,
no, actually, and give a heap of examples of how cultural changes make us innovate in different ways,
aligned with our values, or have us bind our technology, like the Sabbath, or things like that,
and say that cultural changes are the deepest.
And then there are plenty of others who say,
no, ultimately the economy and law is the deepest thing,
because ultimately whatever you incentivize financially is the technology that will be developed.
If you change the subsidies and the taxes and the incentives, the tech stack would evolve differently.
I would say that all three of these, the infrastructure, the social systems,
and the superstructure or culture, are equally fundamental and inner affecting,
and you have to think about changes to all three simultaneously.
So if you dismiss any of them out of hand, like,
ah, cultural changes don't matter that much, or technological changes, or government doesn't,
you're definitely not thinking comprehensively.
The favorite one, like, we can just do culture change and everything else will automatically happen.
That's also not thinking comprehensively.
They're all necessary and only thinking about them together and the way they inter-effect each other is sufficient.
So we could think about, if we want a future that avoids all these catastrophes,
what does the infrastructure have to look like?
Pretty quickly we can say we can't keep using nature unrenewably and turning it into pollution and waste unrenewably,
so it has to look closed loop and it has to look post-growth,
because you can't grow it exponentially on a finite planet.
So what types of technologies would mediate that?
And what things should be global and what things should be local has a lot to do with the social structure interaction of
there are things that you want to be global in so long as you're wanting to bind the well-being of those people to each other
through supply chains and interdependence and that kind of thing.
What would the social systems of a future look like and what would the culture?
There have been conversations today around in-group, out-group and identity systems that's culture questions, right?
I think there's a lot of probably focus on the well-being picture here at Harvest and its virtual picture of planetary identity.
And obviously the planetary identity has to be not just humans but all life forms,
because you can advance all humans at the expense of the biosphere for a little while, but then it goes pretty badly for the humans.
So when we think about Third Attractor, we have to think about what is the culture of it, what must it be?
What must the coordination systems and the distribution and allocation of resources,
and you start to get into things like, well, man, doesn't interest by itself,
even if we don't think about central bank policy or interest rates or fractional reserve banking or anything,
doesn't interest itself compounding interest force exponential growth of finance?
Yeah, it does.
And then to not debase the currency, doesn't that mean you have to have an exponential growth of goods and services?
Yeah, it does. Doesn't that mean you basically have an exponential materials economy on a finite planet?
Yeah, so interest has to go.
Well, that's really fundamental. We don't know how to make that world.
And then as long as most access to resources based on private property, doesn't that mean rival risk interest
where I can do better at the expense of the environment and others based on private property?
Probably a lot of stuff has to be rethought around property law.
And then even when you get to, I can appreciate the atmosphere.
In fact, my life depends upon it, but I don't have to pay for it.
And so if I cut a tree down, I get immediate benefit from the timber.
And the little tiny damage it causes to the atmosphere, I don't really notice.
But when everybody thinks that way, it does have that effect.
But locally, I have way more incentive to cut it down than to leave it up,
because the extraction value that I get from turning it into lumber gives me game theoretic value.
Whereas if I put my resources towards planting more trees that I don't have economic interest in,
I do less well in the economic system, this means that there is a fundamental rethinking of the value equation.
Because whoever ends up valuing extractable, exchangeable wealth ends up doing game theoretically much better than those who don't,
which means they influence the world and culture more.
Those who pay more attention to common wealth have less influence over the whole thing.
So the changes that we're talking about at the level of economics are things like interest, private property, fungible currency,
even deeper than whether we have nation states or not.
And the same in terms of thinking through what is the future of the tech stack look like.
So we share a lot of views, and thank you for laying it out.
I hope everyone could follow with these, you know, this way of presenting where we're at.
A lot of people are quite naturally fearful of where we're going in terms of the labor force because of AI, for example.
And so in one of your talks, I don't know if it was very recent, and we share this view about this by the way,
you talked about education or educators and nurses.
I think it's a good way to present a hopeful opportunity for us to do something where humans are uniquely designed to do something different than machines.
And where efficiency is not what matters. It's more about qualitative than the quantitative.
And our computational capability is not challenged by that.
So I thought maybe I'd switch gears a little bit, but it's giving a little bit of hope again in terms of how we can address concretely
some of these challenges that we're facing, whether we're techno-optimists or techno-scepticists.
So I worked with the G7 on a scheme to try and infuse into national security in the G8 countries a concept of benevolent AI.
And we were just studying with 80 scientists from all over the world how we could potentially put that into motion
and start to educate government bodies and leaders.
And the main failure point was purely geopolitical.
So the arms race and AI just made it so that it made every single suggestion stalemate.
So we have to address it on a population level and also in terms of how, in terms of society, we adapt to the change that's coming towards us.
So we've adapted to bringing cars into our life, arguably imperfectly.
We've adapted to many changes in our society in terms of healthcare and pandemics and how we travel and etc. etc.
Give us a little, you know, I share this view with you, by the way.
How you think educators and nurses could become a little bit of a new orientation for mankind as AI steps in.
So the topic of technological automation creating jobs issue, there's a couple perspectives.
One primary perspective is technological automation will obsolete certain jobs, but it will create new jobs.
It's always been the case we don't have elevator operators anymore, but there's plenty of new jobs.
And then there's the other perspective.
No, actually advanced robotics and AI are different in kind in some of the earlier industrial technologies.
And they're different in speed that as new jobs are created they will still be able to beat humans at doing it for market purposes,
which I think is much more true.
So if you take an AI like Google's AI, you take AlphaGo, you can train it on chess and how to beat everybody in chess.
You can train it on Go and how to beat everybody at Go.
You can train it on StarCraft and how to beat everybody at StarCraft.
It can gain the capacity very quickly to be humans at any finitely definable game.
And so AI is different in kind than other previous technologies.
It's more like the difference between us and all the other animals with our ability to innovate, you know, creating recursive technology on technology
that allowed us to become apex predators in every environment and then more than that, the AI is kind of like that jump again.
And so it does portend a break of capitalism and market structures as we know it and that most of, not just labor,
but most of the jobs that we currently have and even the new jobs that emerge in the niches that it creates, it's better at than we are.
That sounds pretty terrible if you keep the existing political economy where people need the jobs,
but one of the main reason we created a system where the people need the jobs is because the jobs needed the people to do them.
And if you, to make a civilization run well, if you had to get the people to do the jobs and a market seemed like a better answer than the state forcing the people to do it.
So let's let the market force people to do it and they can kind of self-organize.
But as soon as the jobs don't need the people to do them anymore, you can also start thinking about economies where the people don't need the jobs,
which is why now a lot of people are thinking about universal basic income and like different ways of thinking about that.
There's early naive thoughts on universal basic income.
Of course, it's the beginning of thinking about it.
But there's a really deep question which is, what is the role of humans in a world of advanced robotics and AI?
Because the advanced robotics and AI will be better than us at most of the things that we're used to being good at.
So what is the role of humans in that world?
What are we still uniquely good at?
And then what is also intrinsically fulfilling and meaningful?
And there becomes a steep question of what does education become in a post-technological automation world where you're not preparing people for the workforce in the same way?
What is the role of education and human development?
But obviously to answer that, you have to say not just education, but and obviously there's the economics component.
How do we do resource allocation and access in that world?
But there's a really deep civilizational question of what is the role of human life?
And if we for a moment avoid the topic of sentient AI, which is a whole theoretically difficult question, and we just talk about functional AI,
meaning AI that we're not saying there's something that it is like to be it.
We're simply saying it's very good at figuring out how to do stuff.
Then right away the key, what is uniquely different about humans and it is sentience.
Is the capacity to actually have there be something that it is like to be you at all?
Is subjective experience and then inner subjective, connective capacity?
And what's interesting is things related to sentience are what is where our intrinsic motive,
if we're not being behaviorally controlled by extrinsic motive, i.e. being paid to do a thing or external deterrence.
And so mostly you have to pay people to do shit they don't want to do, right?
And if the shit that people don't want to do is increasingly getting automated,
can you then have a world where largely what humans are spending time doing also more deeply coincides with what they have the deepest intrinsic motive to do,
particularly if you then have a developmental system and developmental society oriented to find out what the unique human motivations and capacities of each person are and develop them in light of that.
And so not only does AI portend something in terms of the obsolescence of humans for lots of labor roles and repetitive things,
one of the things Tomas was probably referencing was the topic of AI in human tutoring is pretty amazing.
It's also scary as fuck because again, you have to get this thing right.
If you have an AI that has so many orders of magnitude, more information processing in terms of being able to model your micro expressions to see how you're learning,
you can also have undue influence in a way that no cult leader has dreamed of, right, in terms of asymmetries of power of influence.
So who controls that and how do they control that?
These actually become the cultural questions, the theoretical philosophic questions that are so fundamental.
If you can genetically engineer humans and have designer babies, don't we all want to be like tall and beautiful and thin and is that actually the right idea?
As soon as you have the ability to actually design intelligent machines and design self-replicating machines and change biology,
the philosophic questions of what is good and desirable become so fundamental because so much becomes possible, right?
But if you do it right, imagine, everybody's heard about deepfakes, the ability to train an AI.
You can make basically pictures that aren't real, faces that aren't real, from AI images that look totally real.
You can also make a video of me speaking where it looks exactly like me, sounds like me, but it's totally AI generated, it's not real.
That technology already exists, it's not that good.
Some people have made deepfakes of me that are audio that sound like me, but the deepfake videos are currently about three years away from being Turing test passing,
you can watch a video of Obama or Bernie Sanders or whoever say something and have no idea if it was real or not,
and you won't have the human capacity to tell if it's real or fake.
You can imagine what that does to our ability for collective sense-making.
But that same deepfake capacity can be positively purposed because what that means is it's trained on the semantic patterns and the vocal patterns
enough to be able to generate novel answers like a chatbop, but where you can't tell that it's not real.
So now imagine an educational environment where you train that same AI, this is large language models as the type of AI,
say you train it on all the writings of Thomas Jefferson or all the writings of Socrates through Plato or whatever and writings about them,
such that I can go into a metaverse environment and say I want to pull up Einstein and von Neumann and Kurt Girdel and be able to have a talk with them about formal logic,
and not only can it seem like I'm actually having a conversation with them where they're sharing differing views based on their actual views, writing, etc.
But I could also just have an avatar that is like the voice of chemistry, that is the holistic knowledge of all chemistry that no human could be,
and yet all of them are the AI's best attempt to model what that person would do in terms of semantic coherency with what they did and said in the past.
So now imagine a future where every kid has access to be able to study with Einstein and Gandhi and Socrates and von Neumann and whatever directly,
where those AI's can model our theory of mind and titrate the learning directly to us associated with our learning dynamics,
but then also because the jobs have largely been automated, what humans spend way more of their time with is things like being educators and being nurses and being musicians
and the things that have kind of high connectivity value because those are the things that the machines don't do as well as the actual sense of shared interiority.
So now imagine that we have way more teachers per capita that are way more well trained, so all the teachers are PhD level trained,
there's one of them per 10 kids as opposed to one per 30, and now I get out of my AI environment where I was just studying physics with Einstein
and my tutor asks me a question like, what do you think was different about what the AI Einstein said than what an actual Einstein alive today might have said?
So then helping us to try to understand the difference between human intelligence and artificial intelligence and what it means to be sentient,
and what effect does consciousness have on intelligence, so not only are they getting that level of educational access,
but the AI can do, but the differential of what is unique about human intelligence and artificial intelligence.
So this is one of a million examples we can give of how those technologies could do mind-bendingly amazing things.
I'll say one quick thing about this is there is a study done on super genius of the past or polymaths,
who advanced many different fields beyond what the specialists in those fields did, and was there anything that the great polymaths had in common,
and the single thing that stood out the most was that they were all the result of aristocratic tutoring.
And this was actually a very taboo topic because when we ended feudalism and tried to create democratic states,
the idea, like if you think of meditations by Marcus Aurelius, Marcus Aurelius spends the entire first chapter just thanking all of his tutors,
but when you're being raised to be the emperor of Rome, the best mathematician, the best poet, the best grammarian, the best historian,
literally in all of Rome are your private tutors, and you can't learn to think like a mathematical genius from someone who wasn't a mathematical genius,
so your average math teacher cannot teach you to think like that because they don't think like that.
And yet, how do you make that accessible to everyone? You don't.
So the aristocratic tutoring in the past was a nice patronage job for the smartest people, but it was also so radically unequal,
but it's what created the best minds. So could that possibly be made popular?
Well, we can see in a third attractor kind of world the application of these technologies where you could actually have better tutoring than Marcus Aurelius had for everybody,
you know, et cetera, et cetera. Now, for each of these wonderful scenarios are like a million ways it goes wrong,
and so how we steward that properly is the key defining thing over the next while.
Thank you, Daniel.
Thank you.

Hi, and welcome to Imperfect Utopias, based out of the UCL Global Governance Institute.
This is a podcast about the challenges facing humanity and possible global responses.
If you're new to the show and you want to get a list of our favorite books, other resources,
and to pass shows, and to join our community, go to ucl.ac.uk forward slash global dash governance.
We're really delighted to have Daniel Schmacktenberger on the podcast today.
Daniel's a social philosopher and co-founder of the Consilience Project, a non-profit media
organization that aims to capitalize a cultural movement towards higher quality sense-making
and democratic dialogue.
Underpinning much of Daniel's work is the conviction that strengthening individuals'
abilities to handle and filter information is now a civilizational imperative in a context
of existential risk.
I've been following Daniel on the podosphere for some time now, and he's definitely one
of the people I check in with most often when it comes to trying to make sense of what's going on.
Daniel's work has also been a major inspiration for this podcast.
So we're super excited to have you join us today, Daniel.
Thanks so much for making the time.
Happy to be here.
Thanks for inviting me.
And we're going to type in lots we could explore here.
But first, I just ask our pod crew to introduce themselves.
So my name is Sam.
I handle the audio and video and hopefully some of the thinking when out of time.
I'm Zoe.
I help with some of the research and more of the admin and social media side of things.
OK, so Daniel, let's get straight to it.
In a world of, as the UN recently put it, certain near-term, nonlinear change.
How well prepared are we to face some of the existential challenges and not just to say
the natural ones that people might think of like asteroid strikes and those sorts
of risks, which are certainly present, but possibly remote, but perhaps most challenging
of all.
And as we've discussed in some length, the human induced or anthropogenic existential
risks like nuclear, but also biotechnology and, of course, climate change, which arise
out of these kind of complex interactions of human and non-human systems, and which
are, I'd say, already defining our times this decade, this century.
And perhaps put a little bit more meat on the bone as to where we might go with this discussion.
We've talked before about how there is a risk that faced with major disruptions, our societies
could potentially default in the direction of either authoritarian oppression or even
chaos, and that some people do think that this is the direction of travel currently.
So how can we best avoid, what can we do now to avoid that kind of dystopia by default?
As our legacy structures, systems, governance systems, as they falter, and there is that
risk of major events sort of overtaking us.
All right, that's a few good questions.
The first question you asked is, how well prepared are we to deal with the existential
and catastrophic risks that are impending or at least have a non-trivial chance of happening?
And you were mentioning that this is a frame that is recognized by the United Nations now.
If we think of the UN as a starting place to answer this, in terms of the closest thing
to something like global governance or an intergovernmental organization, obviously it
was created after World War II in the recognition that nation states by themselves
weren't an adequate governance system to prevent world war.
And now that we had weapons such that the wars between the major powers could never be fought in
one anymore, we had to figure out a whole new world system to do something different than we
had ever done in the history of kind of like the post-empire world, which was how do you have
the major empires not fight wars? And we don't have a very good historical track record of that,
but then we got to the place where you had weapons where the wars couldn't be won.
And that was a different logic. So I point this out because catastrophic risk that was human-induced
before World War II was different and kind than after World War II because we didn't have any tech
big enough to actually create global catastrophic risk from human action.
That doesn't mean that catastrophic risk is not a part of our history, it was just always local.
And not only was it a part of our history, it was what happened most of the time in the
history of the life cycle of civilizations. So we can see that, you know, if we're studying
the Mayan Empire, the Inc in the Aztec, the Egyptian, the Roman Empire, like one of the first
things that we recognize is that they all don't exist in the forms of their dominance anymore.
They all either had sudden collapses or gradual collapses, but the collapse of civilizations is
the things that we call civilizations is one of the kind of most prominent features
that we can see in history. And so you can see that people faced an existential risk to them
from in the form of a warring army or over consumption of their resources or internal
dissent that was enough that it broke their capacity to continue to coordinate.
It's just those were always local issues, maybe a large locality if it was a large empire.
And those were both for environmental externality reasons, like the first civilizations that
over farmed and created desertification. It was a long time ago, right? That's an
environmental overreach is a multi-thousand of year old problem.
And short-term solutions regarding rivalry that in gender enmity, in the side of the other,
and where they then reverse engineer whatever weapon innovation you had and come back and you
just drive arms races, they're escalating. That's also a very old process. World War II is the
beginning of us getting to the place where the scale of our warfare, and then also shortly
thereafter, the scale of even our environmental externality hit a global catastrophic possibility.
So you see that we created an entire world system following World War II to say, okay, we need to
we now have such incredible power that we can't use in the way that we have previously. We need to
steward that power differently. How do we deal with conflicts without war between the major
superpowers? So the UN was created the World Bank, the IMF, that whole kind of intergovernmental
organizations that would be able to broker nation state interests to have solutions other than war
and the entire set of Bretton Wood agreements, Marshall Plan agreements for how we kind of
rebuild the world where the nations would be so economically interdependent on each other through
trade and globalism that it was more advantageous to them to continue to do trade with each other
than to bomb each other. That was a huge part of it and where we could have so much growth
of the economy that everybody's desire to get more could happen simultaneously without having
to take each other's stuff. The idea that very, very positive sum GDP situations could keep us
from going zero sum conflict oriented. Well, that very positive sum meaning extract resources that
are unrenewable and turn them into trash much faster driving GDP for a very short period of time
also meant we hit planetary boundaries. And so now we're seeing planetary boundaries both on the
side of depletion of unrenewable resources and the waste side, both sides of a unrenewable
linear materials economy on a finite planet, lots of different ones, right? We're not just seeing
too much CO2, but too much plastics and ocean micro plastics and all kinds of things on the
toxicity side and all kinds of things on the overfishing, cutting down to many old growth
forests, soil, micro, diversity loss, microbiological diversity loss, etc. So you can't keep doing the
positive sum thing in that same way that is based on the exponential growth of a linear materials
economy on a finite planet. That's one part of the kind of post-World War II solution
that's kind of run up against an end. The other thing is that that world system
created a lot of fragility, right? Because when you have global supply chains where most
in the of the products that we engage with now no country can make, they're made across six
continents, this computer that we're talking on, this phone in my hand, when you factor all of
the materials processing, the hardware, the software, the satellite infrastructure required
for our communication to be happening. The positive side of getting the world very interconnected
was that we were less oriented to war if we had dependence. The negative side of dependence is
you can get cascading failures, right? If you get failures anywhere, then you can get failures that
start to cascade. And we saw that with COVID, we saw that an issue in one province of China
became a completely global issue affecting almost every sector of the world, that needing to stop
the transmission of the virus in a much more transportation-based world than any previous
plague or pandemic ever happened in, also meant shutting down critical supply chains where
fertilizers and pesticides that were needed for agriculture didn't happen, driving food insecurity
at massive scale, which means that the solution to one problem drove other problems. Second and
third order effects became very problematic. And so the interconnectivity that had advantages
also has these fragility disadvantages, and the interconnectivity also wanted to have maximum
efficiencies, and the efficiencies also drive fragility. We also see that in that World War II
till now-ish kind of Bretton Woods time, we had one catastrophe weapon. And so one catastrophe
weapon could be responded to by that same catastrophe weapon. And so the game theory of it was somewhat
simple. And for the longest time, we only had two superpowers that had it. And as a result,
mutually assured destruction was very effective. You were able to create a kind of forced Nash
equilibrium. And also because it's very, very hard to make nuclear weapons. There's not that many
places that have uranium. Enriching uranium is difficult. You can even see it because of
radioactive tracers from satellites, so it's easy to monitor. And so you could do mutually
assured destruction. Obviously, we're in a situation now where we don't just have two
superpowers that have nukes. We have many countries that have nukes, but we also have
lots of other catastrophe weapons, meaning lots of other weapons that are big enough that they
could cause kind of catastrophic loss of civilization harm. And they aren't hard to make,
and they aren't trackable anymore. It's not hard to make crisper bioweapons or drone-based
infrastructure attack type things. It doesn't even take a nation state to do it, not traceable.
That's a very different situation. So when you have many different catastrophe weapons and you
have many, many different actors that can have them, including a very difficult situation to
be able to monitor which actors, how do you do mutually assured destruction? And so how do you
get the deterrent strategy right? And so what I'm bringing up is that catastrophic risk before
World War II was one phase, all of human history up to that point. Then World War II till now was
kind of one phase, and now we're entering a new phase where the Bretton Woods mutually assured
destruction, IGO, exponential growth of a globalized linear materials economy,
set of solutions doesn't work for the new set of the catastrophic risk landscape that we face.
So we need a totally new set of solutions which will require innovation in our social technologies
of how we coordinate game theoretic type issues. Now, when you say how well prepared are we,
we come back to the UN, we recognize that we have not succeeded in nuclear disarmament.
Even while we kind of claim to succeed in nuclear disarmament in some very limited ways,
we still had arms races of faster delivery mechanisms, hypersonic missiles, whatever,
to try to win first strike and other things like that. We got more countries with nukes rather than
less during that time. We got more other countries that could affect the movement of a new nuclear
weapon through other kinds of geopolitical and less military advance, but engaging the bigger
military type tactics, plausible deniability attacks that get blamed on a larger superpower
and things like that. And during that time, we've also had every new type of advanced technology
create an arms race. There's an arms race on AI autonomous weapons, on
the application of CRISPR technology to bio weapons, cryptographic type weapons for cyber
attacks. And so we have succeeded in preventing no arms races. We have not been able to reverse the
one really critical one. None of the sustainable development goals can really be said to have
been achieved well. So I would say that our global coordination on all of the most critical issues
is inadequate to the timeline and consequentiality of the issues. That seems very, very clear.
And the as exponential tech is advancing, the total number of catastrophic risks and the total
probability of each is increasing. And the capacities that we're utilizing to address them
are not increasing accordingly. So there is a gap that we need to be focused on, which is what you
guys are focused on, which is this kind of global governance topic. We have global issues, not just
local issues. Everybody's scared of global governance, the frame, the term global governance,
or at least global government, for a good reason, which is we have a good
long history of reasons to not trust consolidation of power with no checks and balances.
So nobody wants this kind of massive, unchecked global government. And at the same time,
you have to have governance at the scale that cause an effect is occurring. And if we're having,
if, if nobody can fix climate change on their own, in terms of nation states,
and yet they're all affected by it, and they can't fix overfishing, they can't fix nitrogen,
run off dead zones and oceans and et cetera, there have to be
global coordination solutions. Otherwise, multipolar traps ruin everything, right?
Multipolar trap being some kind of race to the bottom. Arms race is an example, as we've already
mentioned. Tragedy of the commons is another example. But the key to both of them is where the
agent focused on their own short-term well-being does something that advances their short-term
interest, but then makes everybody else have to do the same thing. And where everyone doing it
creates the maximally bad long-term situation. And so if we try to create some treaty around not
overfishing a particular region of the ocean and anybody violates it, then why does it,
if anyone else doesn't violate the treaty, if they can't figure out enforcement,
then you're just a sucker for holding to the treaty, right? Because all those fish are going
to get killed anyways, the ocean's going to get messed up. It's just going to feed another
population that's going to grow and have more people to engage in economics and armies. And so,
yeah, how do you do enforcement on a nation that has nukes or a nation that has some critical
aspect of infrastructure or, you know, or the globalized supply chain? And so enforcement
becomes tricky. So then you get these types of things, tragedy of the commons and arms race,
multi-polar traps. So you have to figure out how do we solve those coordination issues globally,
because we have global issues that can't just keep getting pushed down the road.
And yet we want to figure out a solution to do it that isn't a kind of global government that
becomes its own catastrophic risk of under the name of some problem that is scary enough,
we agree to some totalitarian power structure. And that's the thing you mentioned about order
and chaos, is that we can see that the thing we call civilization is a way of having some order,
some coordination between lots of people, so that they can do specialization and division
of labor, creating a richer world for everybody, and then coordinate all that, they can coordinate
their activity for not just those kind of productive purposes, but also protection purposes.
So the thing that we call civilization is how we coordinate behavior of lots of people.
And that's actually a pretty hard thing to do when you think about people that want different
stuff and believe different stuff and aren't necessarily connected to or bonded to each
other, like how do you get them to not just do the immediate advantageous thing to them
for people that are fundamentally strangers to them. So typically a civilization will try to
create order through some kind of imposition, some forced religion, forced patriotism, law,
whatever it is, and it can air in the side of an order to have everybody
participate with that order becoming increasingly tyrannical, increasingly dictatorial.
If it doesn't do that, people end up orienting towards tribalism naturally
and fragmenting kind of towards each other and you end up getting the thing failing in the
direction of chaos. The only other answer is how do you get order without it being imposed?
How do you get emergent order? And this was the kind of idea of democracies and republics and
open societies is maybe we could actually get emergent order if we, and it was based on the
idea of a culture that invested in the people enough, that the people didn't just believe
different things and want different things and be willing to defect into war. You had to actually
develop a people that could all come to understand the world similarly. Can everybody understand
the philosophy of science well enough that they can all come to understand base objective reality
that they share similarly? Can they all have something like Hegelian dialectic capacities
where they can notice not just their own values but other people's values and recognize that only
solutions that meet everybody's values will end up working? Can they understand things like
multi-polar traps well enough to understand that a short-term win of my political party
just means that whatever technique we utilize that was effective gets reverse engineered,
the other side wins in the next four years and undoes everything that we did for four years and
we get nowhere and then dictatorships do much better than us and the society fails? Can people
understand those things enough that they don't orient towards the short-termism kinds of things?
So this is why the modern democracies emerged out of modernity, emerged out of a
philosophic system that said we can come to understand the world and understand each other
well enough that we can actually have emergent coordination. Obviously, the world has gotten
much more complex during that time and the cultural value of that kind of education has eroded.
So here's the way I would frame up the current situation, one way that I'm looking at the current
situation. This is a detour but I think it's helpful. I'll go back to World War II and then
bridge it to now since that was kind of the beginning of catastrophic level technology.
One way of looking at World War II is, and this is not the only way, there's lots of ways,
this is a useful way for the construction I'm doing. One way of looking at it is that there were
a few social ideologies that were competing for supremacy and what they were competing over was
the emergence of a new set of technologies that science made possible that were so much more
powerful than the previous technologies that kind of whoever got dominance and then would win.
And so the bomb is obviously the center of that but it's not the whole of it,
computers, the enigma machine and the whole development of computation, rockets,
and chemistry. Chemistry is a part of that, kind of advancing in World War I but then
advancing a lot in World War II. Kind of those all came from science getting to the place that we
could do atomic physics and physical chemistry well. And the social philosophies we could say
are capitalism and liberal democracy. The intersection of theory of markets and something
like a democracy or republic. Communism, the Soviets and fascism and a particular kind of ethno
centric nation-state fascism. So those were three different types of social systems.
And we can see that Germany was actually meaningfully further ahead than the U.S. or the Soviets in
certain areas of tech. They got the enigma machine first, they got the V2 first. Those other countries
were obviously larger so when they recognized that and fought to catch up they had an advantage in
that way. And we can say and there's lots of problems with saying this but for the use of
the construction we can say that the U.S. won that competition for that war. Those wars over the
new technologies and we did it not through the market running the Manhattan Project but the
state running the Manhattan Project. This is actually a very, very important thing to recognize
is that the state, the United States recognized that it was an existential risk
and you remember it was Einstein and Sillard, I think, the Einstein-Sillard letter that said
no, the physics we came up with really does say that Obama is possible and there's a decent chance
the Germans know this and they're working on this. We were doing the physics over there together and
the idea that states don't innovate and that markets innovate is just not true. Historically,
the ability to split an atom, which is in a way the most impressive innovation,
was done by the state, not by the market. That was not outsourced or private contracted.
And the same with cracking the enigma code and the whole early development of computation that
ended up then getting private contract and leading to Silicon Valley was nation-state funded,
the Apollo Project. And it kind of stopped with the Apollo Project for some important reasons.
But what happened was the United States recognized that the technological advancement
was going to determine who had the power to determine the world so much that there was
next sense of risk for them that they created a unlimited black budget, brought all the best
minds together to drive innovation in technology to be able to make a democratic system stronger.
For a bunch of reasons, after that and the decades that followed,
more and more of the innovation got outsourced to the private sector
and it started to become closer to true that the state wasn't innovating and most of the
innovation was happening in the private sector. But the private sector doesn't have the same
patriotic interests, it doesn't also have the same people in the private sector aren't voted in,
they don't have term limits, there isn't the same jurisprudence applied to them,
so they have a different set of agendas, right? And the whole idea of the state, like you can
almost think of what the state in a liberal democracy is as like a labor union for the people
and as a whole, like a labor union, is how do you unify all the people to have something that
is big enough to represent their collective interest so that the large corporations and the
major wealth holders within capitalism don't just rule everything like feudalism, which is
the thing we were trying to replace before, because it's very clear that if we have a trade system
and it's mediated by an abstract system for doing accounting like currency,
that pretty soon you'll have a power law distribution of wealth and a few people will
own most of the wealth. Some people are better at it and then getting better at it gives you
more capacity to keep getting better at it and there's compounding interest, which is an exponential
return on owning capital and there's compounding interest on debt and does that thing, right?
And we can see the data of that in Piketty's book, but it's also just kind of a natural
thing to look at. So the idea was since power law distributions are going to happen,
most people are going to have really no power. How do you not have that be oppression? Well,
let's have the people all be able to collectively vote where at least the majority of what they
care about gets encoded as law. So their values are the basis of the jurisprudence of law. So then
rule of law can get enforced by representatives of foreign by the people that are going to be
bequeathed with a monopoly of violence so they can actually do enforcement
to be able to protect the people in the commons against perverse incentive while letting the
market do all the good things that it does. But most of rule of law is actually binding
the perverse incentive. So if that only works where the state can check the predatory aspects of
markets, if the people are checking the state that it is truly of foreign by the people,
there's transparency, everybody's actively engaged, as soon as that stops happening,
then the government is just run by people. Those people are economic actors. They're in there for
whatever a short period of time and they will be liked about the same whether they do corporate
interests or not because nobody's really going to know. And so of course, you end up getting
regulatory capture where the market captures the regulatory apparatus and you get crony capitalism
and that kind of institutional decay. And as the founding fathers in the US said, and anyone who
paid attention, as soon as a couple generations pass and the people forget what it means to fight
a revolutionary war and be under oppression, they won't keep investing and being educated enough
and actively being engaged in government because they'd rather keep up with the Joneses or party
or like some other thing. And so how do you keep the intergenerational transfer of not just the
knowledge but the civic virtues necessary to uphold a democracy, which is not a trivial thing.
And especially as time goes on and the complexity of the world increases,
understanding the issues well enough to really play a role in them and to be able to oversight
them and police them gets harder and harder. And so there has to be more and more investment
into doing that. So we can see that the people stopped investing in checking the state, the state
stopped checking the market, market captured the state, all the innovation got outsourced.
And so what we can see today, so we see in that World War II example that the state really pioneered
the advancement of all these areas of tech to increase the integrity of the state.
There is a jump in technology that is currently happening that is more significant than the
World War II jump in technology. And the center of it is AI and computation with AI being the
very center, right? It's computation, digital tech, but then the application of AI and digital
tech to physical tech as well. So the application of that to biotech and CRISPR kind of stuff and
to robotics and robotic automation and the other key areas of computer science from the
evolution of the computational basis, quantum computing, photo computing, DNA computing, whatever,
and again, the application of that to the material sciences, nanotech, etc. So
we're undergoing this huge jump in technology right now that is something like two orders of
magnitude more significant than the previous World War II jump was in terms of the total amount of
verticality of power and the speed at which it's developing and the number of verticals
simultaneously. And the way I see it is that tech will confer so much power that only those
who are guiding it will have much of a say in the future.
And right now, I only see two types of groups really guiding it meaningfully.
Some authoritarian nation states are where the nation state is taking seriously the development
of tech in the nation state is investing a very big R&D budget and how to actually increase the
integrity of their nation state. And there's a good thing for them to do aligned with whatever
their system and their ideologies are. And obviously, China is a prime example here where
the application that the government is investing in the development of engineers and in the
application of all of those areas of tech to the nature of government itself. And that's everything
from their IoT system to their seismic credit system to the transistor development and lithography to
the Belt and Road Initiative and getting something like 94% of the world's rare earth
minerals in there that are needed for computational substrate and their supply chain to on and on,
right? To the creation of their own internet that doesn't have the same problems for their country
that the US internet has. So authoritarian nation states are using the exponential tech to become
exponentially more effective authoritarian nation states. And the only other kind of org
are companies, Western mostly companies. And those companies are supported by a military
and capital and infrastructure of the nation state, but they don't are not serving the interests of
the nation state other than GDP and jobs and some very short term kind of stuff. And they're
becoming exponentially more powerful companies. But you know, Facebook and Google and have more
users than China and the US combined have people, right? So these are humongous kinds of things of
which there is no precedent for a corporation in history. And Rand never imagined things like this
when she was thinking about the symmetry of supply and demand. And she didn't think of things like
Metcalfe Dynamics that end up leading to natural monopolies and antitrust law didn't think of
that, right? So you end up having Amazon being bigger than all other online stores combined
and Google being bigger than all other search engines combined and Facebook being bigger for
time on site than all the other social networks, you get a natural power law distribution,
not based on government crony capitalism based simply on the nature of network dynamics that
once you reach a certain escape velocity, there you're a natural monopoly will start to emerge
based on the value of the thing being associated with the second power of the number of users.
And so the interesting thing is you see these corporations that are becoming more powerful
than nation states in many ways because of the development and direction of the exponential
technologies. And as that happens, they are less able to be regulated by the countries while still
benefiting from the infrastructure of the countries and simultaneously eroding the
integrity of the country. We can see the way that the time on site optimization ad model
of Facebook and Google and YouTube have eroded American democracies in specific and Western
democracies by doing the time on site optimization appeals to people's cognitive biases and tribalism
and limbic hijacks and those types of things. We can see that the kind of consolidation of
market function like Amazon that Amazon's growth during COVID matched pretty closely the closure
of all small businesses that aren't going to reopen. Well, the American dream without small
businesses isn't the thing, right? It's not a thing in the same way. And we see the technological
automation of so many jobs impending and not the replacement in the current way that it's trending
of a similar American dream kind of sovereignty. So there's kind of a billionaire to centa
billionaire class that runs whatever the one big dog on the top of the power law distribution that
defines a vertical is and a increasingly less upwardly mobile in terms of real capacity to
play those games underclass. And obviously some kind of middle class that is serving
the very upper class in that context. So what I see is that that is the movement to a new kind
of feudalism, right? A tech feudalism. And it's even interesting, some of those companies,
you know, we see this with Tesla, we say with the other ones, some of those companies
are getting subsidies, government subsidies. That means they're collecting taxpayer money
to do to utilize taxpayer money to do the thing they're doing. But the taxpayers didn't vote on
them doing that. They were not elected representatives. They cannot be unelected. And
there is no traditional jurisprudence for the guidance of the thing that they're doing.
That's something much more like a king than a president, which is why I say kind of an emergent
tech feudalism. So what I see is there's one stranger tractor, which is tech feudalism. There's
another stranger tractor, which is kind of authoritarian nation states. And anything like
an open society where there's participatory governance and jurisprudence that is grounded in
the will of the people, there is no system that is based on those ideals that is innovating
in exponential tech to make better versions of that social tech. That is the number one
imperative of our time, in my opinion. And either we figure that thing out, or those are the only
attractors. And the third attractor is that the exponential tech just causes X-risk and we're
fucked, right? So you have X-risk feudalism and authoritarianism as the current dominant attractors
in the presence of exponential tech. Or there's not 17 sustainable development goals that really
matter because we can't fucking achieve any of them without better coordination. There's figuring
out coordination that it becomes the central goal of the world, figuring out a kind of coordination
that is emergent order, that is neither chaos nor oppression, that is
able to utilize the exponential technologies and also to bind and direct them so that they
do not either directly or through externality create X-risk, and that they don't create
authoritarian systems or kind of feudal systems that erode civil liberties in the process. So we
need to have a kind of global innovation zeitgeist of how to apply, develop and apply all the areas
of exponential technology to building new social tech that can guide, bind, and direct the exponential
tech, prevent X-risk, and do it in a way that is commensurate with what are underlying kind of
deepest values for participatory and empowered governance and civics are.
Thank you, Daniel. That was a fantastic riff on the opening question and I think really sets the scene
and goodness we could go in lots of different directions now. It made me think of, you know,
Niall Ferguson's book, The Square and the Tower. Niall Ferguson has said that historians haven't
taken network seriously enough and he traces these network dynamics back centuries and actually says
they were much more prominent and important. And part of the historical political landscape
than we often think. So that was really interesting and I guess what I was thinking we might pick up
would be, you said at some point that we often kick these problems down the road and I wonder
to what extent we're really coming up against the sort of cognitive limits of humans. Given the
rapidity of change, given the challenges that we confront in trying to get our heads around
exponential functions. We have this kind of strange parallax right now between continuity
and discontinuity. So we have these unique unprecedented challenges, but on the other
hand we have these very old forces of zero sum competition, resource wars. Certainly something
which I sometimes hear in the academy is this idea that ultimately there's really nothing new under the
sun that we can repurpose our existing structures that we do have good enough global governance,
if you will. But I wanted to tease out a little bit more this idea of continuity. It seems to me
also what we're seeing is kind of a resurgence of understanding that actually we do need to respect
the laws of physics, that we need to respect the laws of thermodynamics, that we might actually even
have to listen to say E. O. Wilson on the the laws of sociobiology in terms of how do we navigate
through a viable path given the current situation we find ourselves in. But on the flip side, we also
have a sort of a real lack of radical vision within the half within sort of the corridors of power,
even if the UN Secretary General Antonio Guterres is calling for a new international social contract,
it doesn't seem to be resonating. And if you go back in the historical record and you look at the
debates of the 1950s and the shadow of the bomb and how radical the vision was,
of course, what resulted was a compromise. But nevertheless, there were very serious people
who were thinking hard about global political federation. What's happened? Why is it, you know,
to draw on that famous phrase is easier to imagine the end of the world than the end of capitalism?
Why is it so hard for us to to work through the viable path to to address the challenge
that you've articulated so clearly? First, on the topic of there's nothing new under the sun and
our previous systems of social philosophy and social technology are adequate. I don't think that
anyone believes that who's actually studied exponential tech and X risk meaningfully,
I have not met them. It doesn't seem like a reasonable position to hold those things together.
If you look at just a single category of exponential tech, that idea will change.
I'm sure the listeners have all seen this, but like, when you saw the way that AlphaGo beat
Stockfish at chess, that was so fucking clear that we're dealing with phenomena that are nothing
like any phenomena that the Scottish Enlightenment or the founding fathers or Isaac Newton or Marx
or anyone ever had to think about that that the best chess player in the world, which is the
cutting edge of there's nothing new under the sun, like chess players got better, but like,
people been good at chess for a while, right? It's a slow evolution until AI and Stockfish just
devastated the best chess players in the world. We remember seeing Kasparov get beaten, and then
Stockfish kept getting better and better with the model of AI or programming all the human games
until it was so much better that it stopped even making sense to calibrate it relative to the best
humans. And then a breakthrough in AI says, let's do this differently, right? Let's make a type of
AI based on rival networks, and we won't actually program any human games into it. We'll just let
it play itself a bunch of times and fast forward and see what it learns. And I don't remember
exactly, but AlphaGo by Google, I think it trained itself in three hours, just playing itself with
no human input of information, just the rules of chess. And then it ended up beating Stockfish.
There were a few stales, but it was like 38 to zero in terms of the non stales. And it's like,
oh, wow, that's it. And Stockfish was so far beyond humans, and it's like three hours of training. And
then that same thing could beat us at go and start to beat us at complex strategic video games.
And this is all evolving over the course of almost no period of time, right? This is evolving over
the course of months and single digit years. Nothing new under the sun. Nobody can study
exponential curves and think that. Now, this brings us to the...
It's just such a silly thing to say. When you start looking at scaled species extinction,
when you start looking at the Anthropocene as a real thing, where humans are a bigger force
than all geologic forces combined in defining the surface of the Earth, like, fuck, it's a different
situation than the history of the world was. And like I said, just even starting with the bomb,
the world never didn't have the major empire's war. And World War II was like a second ago
in historical time, right? And the solution to not use that bomb drove all these other issues. So
a lot of our issues are just increases in the severity of the same underlying type of game
theoretic dynamics. And so we can say they are continuous with them in type, but there are places
where a change of magnitude becomes a change in kind, right? Like as soon as the magnitude gets
beyond human information processing capability, it's now a change of kind. As soon as we move
from a war that's winnable to a war that's not winnable, even though they're both the logic of
war, it's a change of magnitude that becomes a change in kind, right? So there's a lot of places
where even the things that are continuous with the past become discontinuous past certain thresholds,
meaning that the same types of solutions, the whole class of solutions doesn't apply anymore.
Now, that doesn't mean that we throw out everything that we've learned. It means that we have to make
sure that we're applying everything that we've learned that is effective, that we aren't making
the mistake of not paying attention to the total amount of human thinking and ingenuity
that's happened so far, and that the new innovation that we do is commensurate with the smart parts
of it. But it happens all the time that we're exploring a search space, and there's a couple
branches, and in the immediate term, this branch has more incentive. And so we explore this branch,
and then we just forget about this one, and we just keep exploring, and then we hit a cul-de-sac
at a certain point. But we have reasons why there's momentum to keep some combination of
sunken cost fallacies with the actual belief that this is the only path, this earlier choice,
and that we wouldn't go all the way back there to the not even knowing the other branches that were
cleaved that we didn't pay attention to, to like perverse institutional incentives of standard
models where it's hard to get a research grant to do anything outside of that thing or to get
your professor who believes in that thing to change their opinion on it or whatever it is.
So there are a bunch of places where we actually have to go back and say, okay,
there was an incentive to make faster and faster smaller and smaller computer chips,
and there was enough money around that that there were whole other directions in computational
substrate that we didn't take that for reasons of manufacturing resilience and a bunch of other
things might actually be meaningful and interesting. This is starting to be a real conversation in
theoretical physics with string theory, and like maybe we actually need to rewind and try
a fundamentally different approach. I think there are places in governance where like we've just
accepted, we've just kind of accepted capitalism is the only, in the West, is the only reasonable
answer combined with some kind of omen-ish government state. And if you think anything else,
you didn't study the history of Mao and Stalin and Paul Potts and whatever, because everything else
ends up becoming that kind of dreadful slaughter. That's kind of the dominant narrative where
it's worse than going against Christianity or it's similar to going against Christianity
in the Dark Ages, right? There's an almost religious tone to it. It's like, well,
we could come up with better shit that isn't any of those things, like
there's nothing new under the sun, blockchain's new, like the ability to have an uncorruptible
ledger where you can have a provenance of data that you can't fuck up. That makes it
where you can have a history that can't be corrupted or changed by the winners afterwards.
That's kind of new. That's a big deal. Makes it to where you can have a system of justice where
you can't actually fuck up the data, right? It means that you can have a system of accounting
where let's say the government spending was on a blockchain that was transparently
oversighted, there wouldn't be missing money anymore. Right now, there's all these places
where the total amount of money going in and the receipts coming out don't add up and there's
missing money. It's like, well, that couldn't be. Does that make something new possible?
Yeah, totally it makes something new possible. You look at the way that AI can make
new sounds. It can do error correction of sound where there is an error or make new sounds or
make new faces by doing an averaged composite of all faces that look similarish. You say, well,
could people express huge numbers of people, express their sentiments about something and
have the AI actually come up with something that is like a weighted average of all of those as a
form of proposition creation and then could we use distributed methods of proposition advancement
that didn't exist when we had to meet in a town hall and ride a horse from that town hall to
the other ones and we haven't innovated the structure of government since we had to ride horses.
Like, why do we think this particular thing is the best thing? Well, because the other things,
the last time we had that conversation seemed dreadful, at least that was the winning narrative.
But totally new things that are not just those previous things are possible.
What I would say is someone should not assume that the moment we say maybe there's a problem
with capitalism that we're instantly going to turn into Stalinism. But to say, let's make sure
we studied that history well enough to know what was wrong with those ideas and we don't do that,
yes. But let's also do the critique of the system and not just end with the critique,
but take it as a design criteria to say what would a better system look like and have we
got all the design criteria? Do we have the critiques of the communist system and the socialist
system and the capitalist system simultaneously? And then can we take all those as design criteria
and work on a fundamentally better design that might not look like any of those isms that utilizes
new technology, which means new possibilities that didn't exist before with new forcing functions
that didn't exist before? I think you're also saying, Daniel, that these kinds of challenges
do actually have comprehensive solutions. And I think there's quite a lot of people who deep down
have very, they doubt that that's actually possible. So whether they should have
haven't even tried hard enough to have that doubt mean anything. It's just an emotional default.
That was the other question you asked is, are we hitting the limits of cognitive complexity?
That is such a shit answer if you haven't actually applied the full limits of human
cognitive complexity and seen that we're failing. So we're not even trying. China's trying and they're
doing fucking amazing. In the US, we have no high-speed trains. None. None. In the time that
they've existed, China's been exporting them all around the fucking world in that same amount of
time. But a system that doesn't have term limits and that doesn't have a two-party system where
we just use all the energy wasted as heat fighting each other and then whatever you do for four years,
the other people undo for four years and nobody invests in anything with longer than four year
timelines because it won't get them reelected, that system is just stupid. That's going to fail to a
system that can do long-term planning. So if we say, okay, let's imagine just hanging out in the 30s
and saying, we got to figure out how to split an atom. No, not just split an atom. We're going to
figure out how to split an atom and deliver that as a warhead on a rocket to some other place with
some decent precision. In fact, we're going to go beyond that. We're going to use uranium to fission
something and split it to then drive nucleons into a fusion. It would be easy to say, well,
there's no fucking way. We don't have the cognitive complexity to be able to split atoms. We don't
even know what an atom is. But the Manhattan Project was a very serious investment in cognitive
complexity and we got everybody there. We got all the best thinkers in the world there. We put the
budget on. Are we doing that? Are we even fucking, where we got von Neumann, we got Turing, we got
Feynman, we got Oppenheimer, we got all those folks in Bletchley Park and in Los Alamos. Where is
the equivalent of that thing outside of very narrow areas of military? Which is why we have a dope
military. We have an awesome military, but that's innovation in military. That's not
innovation in the social technology of governance itself. We actually have to not just innovate
our military, but innovate the social technology of governance for a participatory
governance system. And this is why we come back to the, there's this quote that I always forget,
so a paraphrase of George Washington's that said something to the effect that the number one aim
of the federal government has to be the comprehensive education of every citizen in the science of
government. And science of government was the term of art. And I think it's so profound that he did
not say the number one aim of the federal government is to protect its borders. And he did not say
the name of the federal government is to protect rule of law. Because you can do rule of law
effectively with a police state. And you can protect the boundaries fine with a military
dictatorship. But they won't be democracies. If it's going to be a democracy, then democratically
the people will probably decide to protect their borders and to engage rule of law.
But if the number one goal is anything other than the comprehensive education of all citizens,
and the education was considered both a cognitive education and a moral education,
the way they described it, which is the kind of civic virtues that people are willing to
give something for the larger system that they also receive benefit from. And they're actively
participatively engaged. So that's the thing we need to be innovating in right now, not just
innovating in military while turning it into a some kind of autocratic or kleptocratic system.
But how do we apply the new digital and other exponential technologies to be able to both direct
the exponential technologies well, so that they don't cause existential risk, and in a way that
is aligned with the actual values that we care about as a people. And so then the core question
comes, what is a successful civilization? Well, it's one that doesn't fail, but that's not the
only criteria. It's one that doesn't fail and that maximizes the possible quality of life for
everybody in perpetuity. And then we have to find what is quality of life me, right? So these
there's like core existential questions of what is a meaningful human life to be able to design
a civilization that is optimizing for that, which is culture, right? Which is why we have to have
innovation in culture. Which is why I talk about that there's a cultural renaissance, a cultural
enlightenment that is necessary right now as the basis of the creation of these new institutions that
can solve the extra problems, because our current problem solving mechanisms can't solve them.
Which is why they're not being solved. We have to develop new institutions that are capable of
solving these types of problems, these types of complexity. But if those new institutions are
created by a few people that get it and impose them on force, then it's some kind of autocracy.
So they have to be created by people who want them and are willing to participate with them
and capable of participating. That is the cultural enlightenment that has to be the basis of it,
which and of course, there's a recursive process of some people engaging in that to then build
systems that in turn engage more people in it. So you get a virtuous cycle between cultural evolution
and social evolution, employing physical technologies, binding physical technologies,
and advancing them for the right purposes. It sounds like we really need a new forms of
wisdom education. And obviously I'm glad to say that we've got a Zach Stein coming on the podcast
very soon to discuss that very question. And obviously what you're saying, Daniel,
big implications for how we think about the university in the current situation.
But I'd like to hand over to Sam. I know Sam's got a burning question, so please, Sam.
Yeah, hi, Daniel. Yeah, I've got a couple of burning questions, but I'll go with one to start.
It seems like with the problems we're facing, they often, as we talked about,
they happen at a certain area. So for example, climate change is here already,
and it will exponentially grow out. And that's one of the issues that I think we're kind of
alluding to, that when there's not the immediate threat of World War II, for example, it's quite
hard to galvanize a whole group of people to solve a problem. But do you think that we think about
solutions in the same way to the logic of problems in terms of Silicon Valley out,
or it will happen in this certain area and slowly filter out? There's that quote,
the future is here, it's just not that evenly distributed. And that's quite a worrying logic
if we're thinking about the magnitude of exponential risk. And do you think that we're
then following the logic that we apply for problems that they happen and exponentially grow out?
And is that useful or harmful when we're thinking about solutions that need to
really permeate around the whole globe and not leave anyone behind?
I'm not sure that I understand the question yet. You were using the example of climate change and
saying it's already here, but because it doesn't look like an agent in a way that we
evolve to understand as an immediate threat, we don't respond to it appropriately. But that
it's already here, it's expanding in a way that maybe we don't respond to appropriately. And
you're wondering, is that the case with all of the risks? There's already AI happening that is risky,
and we're just not responding to it appropriately? Or was the question different?
Yeah, sorry, Daniel. The question was slightly different. So that's how we understand
issues like climate change. And we often talk about solutions in a similar way to
that issue of climate change, i.e. there'll be an innovation in a certain part of the world.
So the solution is already here. And then it slowly permeates out. And then eventually everyone
will have it. So you took the example of high speed trains, they are already here. The solution is
already here. It's just not that evenly distributed. And do you think that that follows the logic of
where we think about things like climate change, where it happens in a certain area and slowly
distributes out? Whereas with solutions, they need to be get around everyone very quickly.
And they can't work in that logic of slowly from one center expanding out.
I understand that. I don't think it's fair to say the solutions are already here and
unevenly distributed. It's true for some things. Obviously, we already have a solution to
caloric abundance, but it's not evenly distributed because there's extreme poverty.
That's an example. And that's one we've lived with for a long time. And we can see that it did not
actually pervade out well for certain reasons. To a certain degree and then not beyond.
And the same is still true for running water and hygiene and medicine. There's a very unequal
distribution of problems we have solved. I would say that many of the most critical
issues we need to solve, the solutions don't exist anywhere. It's not true that somewhere
has figured them out well. We actually have to do innovation. How do we solve global
multipolar trap issues is not solved anywhere. And that's the most central thing we have to figure
out. How do we create digital open societies? You can say that it's kind that there are some
places that are trying to pioneer like Taiwan and Estonia. That's true. But those are very far from
have really got worked out solutions that are adequate to all the other places in scale.
I think we have to acknowledge that many of the most critical solutions don't exist at all
and need to become the primary focus of innovation. And then where they do
start to develop, we have to say what type of governance and incentive landscape would be
necessary to get them everywhere they need to be in time. And who would have to be participating
to make that happen? And what kind of oversight and enforcement would be necessary to really make
it happen? We know in the US, the government making deals with Native Americans and then
not keeping them whenever it's inconvenient almost all the time. It's not just about did you
say when you developed a new technology that will get it to the world? Is there a method of
enforcement that will actually ensure that that occurs and that it occurs within time? That becomes
critical. I just had a follow up to that. We talked about how we understand problems and how we
understand solutions. Why do you think certain maxims are held in higher esteem than others?
In another podcast, you talked about survival of the fittest and how we've almost
fetishized that concept above all others. And how can we make sure that other maxims are
discussed in a kind of equal or more celebrated light? And is that there a logic that pervades
a lot of these more harmful maxims? Yeah, it's apologism. So if I win a war and we kill a bunch
of people that we call terrorists or infidels or some bad thing that makes them not human,
but what it means is we blew up a lot of civilians and a lot of women and kids and whatever it was.
But we got more land and resources and whatever it was out of doing that thing.
Survival of the fittest is a nice narrative to say that's how nature works and that's the way
that it should be. And it's actually the predators that keep the prey animals from eating themselves
into extinction and that drive them to evolve by eating the slow ones so that the good genes
kind of inbreed. And most people are like prey animals to the some more predatory humans that
cold heard and that kind of drive them who are otherwise kind of lazy eaters. Like that whole
ideology is apologism for whoever is winning at an extremely damaging rival risk kind of system.
Naive techno-capital optimism is one of the best examples of apologism of this kind where
like if you have a theory that criticizes capitalism, nobody who's winning at capitalism
who has the money is going to upregulate it. And if you are criticizing tech, nobody that was
winning at tech is going to say, yes, I like your idea of why I suck and I'm going to upregulate
that. So you realize that for narratives to catch on, somebody has to upregulate them and
there's cost associated in doing that and there has to be a motive associated with that cost. So
it's not just like the ideas that are the most true and the most beneficial proliferate. The
ideas that have the most agentric basis to drive them through the society are a lot of the ones
that proliferate. The idea which is often held up sort of as counterposed to survival of the fittest
is mutual aid, which is this idea that Peter Kropotkin proposed in the late 19th century and he
essentially saw out there in nature. Actually, it wasn't the species that competed most fiercely
that survived. It was those that actually cooperated that moved into a kind of a situation of symbiosis
if you will. So is that notion of mutual aid, is that a useful reference point for thinking about
these vaccines that need to inform how we move forward? How do we actually begin to have meaningful
productive conversations within the classroom or within the UN forum or within government
corridors of power? How do we begin to chisel away at the memetic sort of structures which
seem to reinforce that particular mindset? If we think through the wrong metaphors, we're obviously
going to come to the conclusions of those metaphors predisposed, but they're the wrong ones, then
there'll be the wrong conclusions. So what kind of animals are humans? Are humans predators?
Are we prey? Are we fungus? Are we slime molds? Are we the relationship between trees and animals
where we can see gas exchange? There's lots of different biological analogies we can try to use.
And none of them apply. So let's say we do the most popular one, which is that we're apex predators.
Pick an apex predator, lion, polar bear, and orca. Orca is maybe the best example, the biggest apex
predator in the ocean. Compare what an orca does to a school of tuna to what an industrial fishing
boat, a commercial fishing boat with a mile long griff net does. The orca misses almost every time,
and when it finally catches one, it catches one, right? And as there's less of them, it misses more
often. And we can pull up the entire fucking school in a net. We're not apex predators.
Apex predators can't do that. If a polar bear decides that it's super pissed off and wants
to go on a rampage and destroy as much stuff as it can, like, what's it going to do? And
look at human nuclear capability if we were similarly disposed. Like, wait, the idea that we
look at, that we don't factor the way that technology means that we are not like the rest of
nature. So of course, we need to see in nature, yes, there's some competitive dynamics and some
cooperative dynamics. This is true. Where there are competitive dynamics, there are mostly
cemeteries of power. The tuna get away as often as the orcas catch them, right? So the slow orcas
die, the slow tuna die, the faster of both happen. So the co-selective pressures have them both kind
of get better together. And so there's the symmetry of power, right? The orca is not a lot more
powerful than tuna in terms of that particular dynamic. And so we can see that if we were to
figure out some way to quantify all the interactions that were happening in nature, almost all of them
are symbiotic, right, of some kind. Some of them are directly rivalrous and competitive. And sometimes
it's kind of both, right? It's a place where the competitiveness at the one-to-one level
ends up leading to symbiosis at the species-to-species level. Obviously, both the predator and the
prey animal depend on each other. Predator dies, prey animal eats itself to extinction,
prey animal dies, predator serves to death. So micro rivalry ends up leading to macro symbiosis
because of the symmetry of power thing, right? So we can see that there are certain types of
competition, but they're limited, the symmetry is power, and then there's a lot of symbiosis.
Well, as soon as humans started making tools, we were able to hunt any species to extinction
anywhere and go become the apex predator in any environment and more powerful than the apex predator
in any environment. We broke the symmetry, right? We became more lethal predators faster than the
environment could evolve to become more resilient to it. As a result, that was the beginning of an
extinctionary process that was following an exponential curve that was slow for a long time
from stone tools and started to really pick up with agriculture, then really pick up with the
industrial revolution is now verticalizing in modern tech world. But stone tools were kind
of the beginning of it, right? And the other stone tools and language and that type of coordination
that came along with the abstraction capacities. So do humans need to ensure, as the metaphors of
nature go, that where we have competition, that it's symmetrical and that it's constrained
and that the micro competition really does lead to macro symbiosis? We need to ensure that. This is
true. Is the competition between Facebook for your attention and you for your attention symmetrical?
No, of course not. Well, you say, well, there's a competition, the competition
between supply and demand is symmetrical because there's an equal number of dollars flowing from
demand to supply. Bullshit, right? The demand side is not coordinated. The supply side's coordinated.
And so even though there's a total symmetry and aggregate, there's not a symmetry of coordinated
capacity because it isn't Google against all Google users as a Google user labor union
that is also applying similar exponential technologies to bind this thing. It's Google
against one person in terms of the person didn't think that they were about to spend the next
three hours on YouTube and now they do, which is better for their advertising model, not necessarily
for your life. That kind of, and so then you can have supply side driving manufactured demand.
Well, now there's not real, market ideology is broken now. That's not a market ideology,
was that there was a thing called demand that was foundational, that people wanted real
shit that would improve the quality of their life and that created an environmental niche for supply
and the rational actors would buy the product or service amongst all of them at the best price
that would drive innovation. Well, the moment supply started to get much bigger than demand
because of coordination, it realized that it could manufacture supply and the humans weren't
all that rational, all the behavioral economics. And now the entire logic of markets is broken,
right? Like market theory is broken with manufactured demand and radical asymmetries
on the supply side. Okay, that's important to know. And so if you go back to the nature example,
where there's competitive forces, do they need to have symmetries in order for them
competition to lead to symbiosis as a whole and metastability of the ecosystem? Yes. If you bring
something in that is not symbiotic with the rest of it, you get an invasive species that can destroy
a whole ecosystem, right? So we should study biology where we're not trying to compare ourselves to Apex
predators or slime molds or whatever. We could just study general principles of things like
cooperative dynamics and competitive dynamics and metastability. We can kind of get a sense of that.
What is needed for metastability? And then say, how does that apply in the human world? But it
will be different. It'll be very different. The rest of the animal world is not forecasting the
future and making game theoretic decisions based on forecasts of the future. And so this is why,
like complexity theory, where we model us as termites is silly, like we don't behave like
termites. So it's not that it's useless, but it's profoundly inadequate as a set of metaphors. So
we have to recognize our human's part of nature, of course. Is there a distinction between humans
and the rest of nature that is fundamental in type? Maybe it was just a change of quantity of
neurological complexity that crossed a threshold that became a change of kind, but it is a change
of kind. And so we will have to have fundamentally different metaphors for thinking about that,
which is why it makes sense to just think about the problem space and make sure that
you understand the problem space well and that your solutions are aligned with the problem space.
Yeah, yeah. What an exciting research agenda. And of course, an agenda to live by as well and to
engage with deeply. And another maxim comes to mind, perhaps, which would be know thyself.
It's not just a situation of impersonal inexorable forces bearing down on us, but we're also talking
about systems of, I think, human intentionality, which raises the crucial issue which we discuss
with Forest Landry in an earlier podcast on how do we make good choices, which perhaps our education
systems are not really equipping us with the tools we need to answer that really important
question. I know that Zoe's got a question. I want to hand this over to Zoe. So go for it, Zoe.
So I kind of building on sort of the meta, we have the wrong metaphors, I guess, we're using
the wrong ways of thinking. I kind of wanted to know how do we deal on like, on a societal and
a personal level with the amount of cognitive dissonance I think we're existing in, because
I think part of the difficulty with coming up with solutions is that some of the challenges
are so overwhelming that I feel like majority of people just kind of stick their head in the sand
and they're like, no, and so we're existing in like, I feel perpetual cognitive dissonance.
And I was kind of wondering what your take on that was and how, yeah, how do you deal with it
personally and how does a young person who's trying to sort of move forward in society deal with
that as well without, and, you know, exist as a functioning member of society without sacrificing
maybe personal ethics and values, even though I kind of, I guess I know that I'm going to have to
compromise somewhere down the line.
You as an individual probably can't solve those issues,
probably not one of them, let alone all of them.
And you can't focus on it and really look at it and feel the scope of the current harm and the
possible harm and not be able to do anything about it and have continuing to look at it make any sense.
So let's say that our social institutions were adequate, as they were at previous points, to deal
with whether they were adequate or not depends upon which group you were a part of and which
problems you were looking at. But let's just take for a moment that for some things they were adequate.
But then if there was a problem you really wanted to solve, you could think about joining
the CDC to work on pandemics or joining the military to work on terrorist-mediated
ex-risk or joining an intelligence group or whatever it was.
If you look around and you see that the scale of the issues requires institutional solutions,
whether they're state or network-based decentralized autonomous organization or whatever,
but collective intelligence of lots of people, not just a person.
And you don't see anyone that is currently doing that, then there isn't something you can join,
then what do you do? It's a tricky problem because there's a fairly small number of people that have
the right psychological disposition to try to found something of that type.
There are a few people who are either going to try to start a new type of company or a new type of
non-profit or a new type of social movement or whatever.
There's a lot of people that can contribute value to one of those that are probably not
going to found it for really not just developmental but typological reasons,
different typologies or into different things.
So to the degree that there are particular issues you care about
and you can find organizations that are doing a pretty good job that you could
join or participate with, that's a good answer. It's something of an answer.
To the degree that you feel like you have the typological orientation to make a new thing
or to be part of make a new thing, to find other people that could co-found some kind of new process,
whether it is trying to get an upgrade to an institution with existing government,
build a new institution, build Ethereum, some kind of platform for decentralized
autonomous organization that maybe will create the future of governance via networks rather than
nation-states. Those are all possibilities for, is there some new capacity that I believe
is needed that I could help to bring into being?
So either you have to join something or you have to make something or you have to join
people that are interested where maybe somebody in that scene or some combination of them will
be able to make a thing. And it's very hard to know that the thing that you're focused on,
even if it's awesome, is not adequate to the scope of issues you're aware of
and put all your energy into it and not go nihilist or just anxious all the time.
So for a lot of people, I would say they should put their sense-making into things that they feel
like they have agency in or could develop agency and that there's some relationship between their sense-making
and their agency. So let's say they feel like, okay, well, I don't know how to fix AI, AGI risk issues.
Silicon Valley attentionalism issues. But I feel like if I apply my sense-making to the problems
in my community, I could actually help improve the quality of life of my community. I could
bring warm data labs there and have the people start really getting to know each other in a
multi-contextual way much better. I feel that kind of thing. If a lot more people did that,
they paid attention to where they could have agency, applied their sense-making there,
a lot of problems would get better. And a lot of other people in those communities would evolve
to want to do things as well. And some of them would have different aptitudes and people communicating
better would have better collective intelligence. And once you solve problems at one scale, you get
better at problem solving. You might be like, maybe I can do this for a second community. Oh,
I've just figured out a generalized principle. Maybe I can help create a way to do this for
communities writ large. All of a sudden, it starts to be able to kind of inductively scale to the
scope of the problems. So one thing I would say is like one approach is just try to understand
what the world needs without understanding what you can do. Just take you out. Just what does
the world need? Because as you come to understand that better, you'll start to have insights of
what needs to happen. And then you'll at least be able to parse where are the places doing closest
stuff? What is nobody doing? How do I help make that happen? Right? That's one approach.
The other approach is what is the stuff I feel like I could do and how do I apply my study to be
able to do some of those things where then in the process, I can be increasing my agency
to then possibly be able to converge towards doing more stuff. Both of those are valid
on their own and in combination.
What I would say is that you're increasing your understanding of the world, that you're increasing
your sense of your ability to act meaningfully, and that you're increasing both the depth of care
and the emotional resilience in the presence of that care simultaneously are things you want
to be tending to. There's not one good answer for how to do that, but they're things you want to be
tending to. Now, we spoke briefly before the recording started about the very real practical
inquiry of what kind of jobs are there in the space. And it's if, let's say, working in existential
catastrophic risk are some of the most important areas in the world and pioneering new types of
social technologies that both apply and combine physical technologies. If these are some of the
most important areas, but there's not really jobs, there's not financial incentive there.
And as you're focused on them, there's more emotional difficulty and psychological difficulty
associated with looking into the abyss. The incentive landscape is wrong for getting the
people engaged in the things that matter. So institutionally, we should try to fix that
and say, how do we start to put incentive on the things that matter the most, which the Manhattan
Project did, which is why I'm calling for Manhattan Project type things. And in some ways, you can say
Ethereum and Holochain and other orgs are trying to do that. So maybe some of Elon's
companies, whatever, we're taking on a problem and we're trying to be able to create a lot of
jobs and incentive to get people to be able to work on problems that matter.
But the other part of that answer I'll just share is for me, a big part of, because I was thinking,
like there's a lot more people thinking about X-Rest now, but I was thinking about it from
quite young age. I just knew I couldn't focus on anything else and I couldn't focus on anything
that wouldn't converge to being adequate. It was okay if what I was working on wasn't
adequate. It just had to seem like it was on a path of increasing understanding and capacity that
could maybe converge. So I kept, for most of my life, my overhead as close to nothing as I could
keep it and figured out things that I could do for work that took the least amount of time possible.
So most of my time didn't have any market need on it. Most of my time was self-directed study in
these areas because that was the only thing I could actually do and be congruent with myself.
So sometimes I did construction to pay the bills, sometimes I did teaching or I became a therapist
and did different things, but I kept my bills low enough that it didn't take that many hours.
And so most of my time could just be allocated based on my intrinsic
orientation of what would be most meaningful, which I highly recommend that path.
Brilliant. Well, thank you, Daniel. I think we're rolling to a close.
We've covered a lot of ground. It's been really an exciting conversation. I hope we'll have a chance
to continue this another time. It does seem that we are in something of quite a sort of
incredible moment, possibly a unique moment. We're facing a lot of daunting challenges
and we're all trying to grapple with what that means, I guess, for us personally, professionally.
For me and in the seminar room, in the university, in society, in my interactions with my loved ones.
But I guess it's also, in some ways, it's a time of opportunity as well. It's kind of a
cool to adventure, as you sort of said, is there anything more important than really sort of
putting your shoulders to the wheel on some of these issues that we've addressed?
And yeah, I'd just like to say thank you for all of your work. And I don't know if you have any
final closing thoughts, anything that we haven't covered that you'd just like to share with us,
to close? It is a thought that comes to mind, kind of following where we just were.
One way I think about how to live a meaningful life, a simple but kind of elegant model is
we can think about life in terms of the mode of being, the mode of doing, and the mode of becoming.
And if you were to describe the mode of being, it is, in the moment, focused on appreciating
what already is, appreciating the beauty of life as it is. The mode of doing occurs in time,
and it's focused on adding beauty to life. If it's focused on anything else, it's not the
mode of doing very well, right? Most people are in the mode of doing, doing shit that if they
didn't do it, the world would be better. But the mode of doing that matters for a meaningful life
is adding beauty to life and or protecting, serving the beauty that's there. The mode of
becoming is increasing your capacity to appreciate life as it is more fully, and to add to beauty
more fully, right? Increasing being and becoming and being and doing. So then there's a virtuous
cycle between those. But the doing only matters and the becoming only matters because of the
intrinsic meaningfulness of being. If ultimately the meaningfulness is grounded in experience,
and the fact that experience is just intrinsically beautiful, that taking reality is intrinsically
beautiful. So if you, because of the crises, you don't focus on that enough, you'll actually get
disconnected from the source of what matters. And then your, your motivational complex will,
if I, if I wake up, so like, I wake up, I go sit outside with a cup of coffee and I look at the
trees. And I just love watching the trees move in the wind and the clouds in the sky and just
like how beautiful this planet is, how much I appreciate it. And there is a fullness in that
mode of being that doesn't need anything. So then I'm not motivated based on what's in it for me,
because I already feel like I could die right now. And I feel lucky, right? I feel like I have lived
a really rich full life. So now it's not what's in it for me. It's not some doing that I have to do.
It's that I actually want to protect that beauty. And I want to protect other people's ability to
keep experiencing it forever, or at least for a long time, because I can. And because as much as I
appreciate it, other people do too, or can and I, that matters to me, right? Like it's intrinsically
meaningful. But that's a different come from it. It has a certain anxiety and angst and feverishness
that isn't there. And it has a sacredness that is there. And then there's also a courage of like,
maybe I fail. I mean, maybe we fail, right? And life has been meaningful each moment. It's not
like it wasn't meaningful like it. Okay, maybe the whole thing comes, this whole thing, part of it
comes to an end at some point. But I will do what I can to be in service to it. But that service is
arising out of seeing it and loving it as is, and then wanting to be of service from there.
So I can be in the mode of being just kind of chill and watching TV. I can be in the mode of
doing doing a bunch of to do this shit that doesn't really matter. I can be in the mode of
becoming trying to get better at doing shit that doesn't matter. I want to think about am I engaging
in each of those modes? And am I engaging in them deeply? If I'm in the mode of being, I want to be
looking at the sky and I want to be listening to music, I love and be wrapped. I want to be
feeling moved by the beauty of life. So why do the mode of being any other way? I want to be with
friends that I love where I'm like, yeah, I could die right now full. And in the mode of doing,
I want to know that the world would be worse if I didn't do this. Otherwise, I go back to the mode of
being, just chill and enjoy it. I want to know that the thing I'm doing adds something of meaning
somewhere, right? And the mode of becoming of am I am I developing my ability to appreciate
everybody and everything around me? And am I developing my knowledge and agency and capacities
to add to it? That's a good framework to think about, you know, when you inventory your day and
your week, what being on track means. Wonderful. Well, thank you, Daniel. Thanks so much. And
if people want to engage more with you and your work, your website is called Civilization Emerging.
Is that correct? Civilization Emerging is just like a personal blog where there's some podcasts
and old stuff up there. And you can check it out. And the project that we're focused on that's
really just in the earliest beta phase right now. But that is kind of the project where we're
trying to bring the information forward that will help decentralize innovation of what the new
social technologies that can employ and guide exponential technology are. That project is
called the conciliants project conciliants project.org. And that will get increasingly
interesting over the next, you know, few months. Yeah. And I'm hoping that we'll have a conversation
about how I contribute to that project. Super exciting. And I hope people will go and check out
that website. Thank you, Daniel. Look forward to picking this up again at some point. Take care.
Thank you. It was good to be with the three of you.
Thanks for tuning into Imperfect Utopias. To get access to all of our content and to stay up-to-date
with future Zoom calls, workshops and events and more, check us out at ucl.ac.uk forward slash
global dash governance. If you like this content, please do leave us a comment and subscribe.
Till next time.

this was my greatest fear that we would
hack our way into this which would mean
it would be like almost like even worse
than the abom we would would be
releasing this power on the world into
corporations and states and Military
organizations who ultimately don't have
a deep understanding beyond the
engineering of what ontologically is
going
on
this is Jonathan Pedro welcome to the
symbolic
[Applause]
world all right so um this discussion is
going to be oriented towards AI
generally and the large language models
I take there to be a distinction there
um maybe John you can talk about that a
little bit um as we get going here um
but just to position ourselves generally
at the outset context for this
conversation will be John's video essay
about
AI um this came out nine months ago 10
months ago now I think it came out last
April or
something so almost yeah 10 months ago I
think okay okay perfect which is which
is great because it gave evidence for my
claim that many the predictions were
premature um so perfect yeah yeah so in
order to sort of set the framing here
we'll start off with John sharing a
little bit sort of an overview of what
the arguments in that essay were um and
then we'll move to Jonathan if you want
to sort of position yourself in relation
to AI generally um and then John's essay
in particular and then same for David
and then from there we can just sort of
get going and see what see what comes
out out um we do have a bit of an
extended time here so I would I would
hope that we can be free if the logo
logos catches and we want to move in a
slightly different direction um that we
would be at Liberty to follow that that
would be great um if all of it centers
around AI That's great um but yeah
excited to be here this has been a long
time in coming I think it took us I
don't know four or five months to get
this
together um so I'm very happy to be here
with all of you and see you all it's
great to see you too Ken should I start
then yeah go for it okay so uh AI of
course artificial intelligence a project
actually proposed in the Scientific
Revolution by Thomas Hobbs so it's an
old idea um but uh I I want to make use
of a distinction made by John surl
between weak Ai and strong AI weak AI is
we when we make machines that do things
that used to be done by human beings so
if you're back in the 1930s computers
were human beings you sent you needed
computation done you sent it down to the
third floor where all the computers were
and they were human beings and they had
machines and fly rules and things like
that and of course they have been
replaced uh or your bank teller has been
replaced by the ATM that's called weak
AI because it is not claimed that that
AI gives us any scientific insight into
the nature of intelligence it's just we
put together a machine it took great
intelligence and I'm not demeaning
people that do this it's a it's a
valuable and our lives are depending on
uh week AI right now we wouldn't be
talking without it so I'm not here
besmer merching that and anything like
that but nobody is claiming that when
they're making that machine well now we
understand right what cognition is or
something like that uh and strong AI is
hobbs' proposal that cognition is
computation um and that what we can do
is if we make the right kind of computer
understood abstractly um we won't we
will have created an instance of genuine
intelligence so it's it's not a claim of
simulation um uh it's a claim of
instantiation now in between weak Ai and
strong AI is something that's trying to
move from weak AI to strong Ai and this
is known as
AGI artificial general intelligence and
this is the idea that our intelligence
is different from the intelligence of
the ATM in that we have general
intelligence we can solve multiple
problems in multiple domains for
multiple reasons and multiple contexts
and yada yada yada you can just do the
multiples uh which makes us uh
tremendously different from those
machines and the project is can we get
um artificial intelligence to be
artificial general intelligence because
that will have moved the needle
considerably towards strong AI um
because um it will become increasingly
difficult to say it does doesn't have uh
this sorry this is the argument it will
become increasingly difficult for us to
say it doesn't have the same kind of
intelligence as Ken does if it can solve
a wide variety of problems and a wide
variety of domains for a wide variety of
goals etc etc that's the basic argument
whether or not AGI AGI is clearly
necessary for strong artificial
intelligence whether it's sufficient is
part of what's actually being debated
not very well I would say in general
right now but that's what's going on
okay first of all questions just about
these distinctions because it a lot of
the discussion out there doesn't make
these clean distinctions and so it's
fuzzy it's confused it's equivocal and
so a lot of it should be ignored um um
because it it's not helpful yes I have
one question so this cognition equals
computation if we accomplish AGI in the
way that you're talking about we would
not necessarily be affirming that
cognition equals computation
if I'm hearing you right is that right
um so that's an interesting question um
and that gets down to a couple more
finer points um I'll go in detail a
little bit later well just to address it
um many people think that U because of
the work of Jeff Hinton who is basically
the Godfather of the machines that are
emerging right now that genuine AGI will
not be computational in the sense that
Hobs and dayart me uh cognition is not
going to be complet explainable in terms
of formal systems that are the
inferential manipulations of
representational propositions Etc like
that um and but um that was Hobson's
proposal and that has been the dominant
view until about the 80s and then we got
neural networks and then we had
dynamical systems right now I'm not
distinguishing between them because I
don't want to get too much into the
technical weeds if it becomes relevant
you let me know and I'll I'll pull those
out um um so the thing about Hobs is
dayart sort of criticizes Hobbs actually
has contempt for Hobbs he's a
contemporary um and he basically poses a
bunch of problems uh that uh the
Scientific Revolution says would make it
impossible for computation to be
cognition uh one is the Scientific
Revolution says matter is inert and it's
purposeless uh but of course cognition
is dynamic and it has to act on purpose
um uh cognition Works in terms of
meaning and the scientific revolutionist
said there's no meaning in things uh M
material things so how could you get
meaning out of it uh the Scientific
Revolution said all those secondary
qualia the sweetness of the orange the
beauty of the sunset it's not in those
things it's in your mind so how could
you possibly get meaning out of matter
and decart's point is well a rational
being is seeking the truth and truth
depends on an understanding of meaning
and therefore so I want you to
understand that decart's arguments
against Hobbs although he he may have
been motivated by his Catholicism they
do not depend on the Catholicism they
depend on the SC the very scientific
world view that right so there's a
tension here uh about Ai and the
scientific uh worldview so here's
another way of thinking about the strong
AI project would is the project that is
attempting to show how Hobbs is right
with an explanation that is strong
enough to refuse day cart's challenges
that's how and I think anything less
than that standard is not true to the
history of the project and so that's the
standard I hold strong AI too now AGI
isn't quite shooting at that standard
that's why I put it a little bit more
intermediary does does that is that okay
yeah all right now sorry I had to do a
bit of background there because I wanted
to get clear about a lot of things that
are talked about in a very murky and
Confused fashion in the in the General
Media and they're just they're just
confused and so they're they're
confusing so I proposed to take a look
at the llms where it is claimed they're
not even it's not even claimed that
they're full
AGI right of course some people claimed
immediately they were strong AI the more
the people closer to the technology
didn't said it might be AGI the MIT
review said it Sparks there are some
Sparks of AGI so let's be very clear uh
how the reflection was actually holding
these machines and so these llms like
chat GTP and so what I did in my essay
is I wanted to review the scientific
import and impact the philosophical
import and impact and the spiritual
import and impact now I won't do the
arguments in great detail but uh here's
the scientific import these machines do
not give us any understanding of the
nature of
intelligence uh and to my mind that was
one of my great fears I was hoping that
cognitive science would advance so we
got a sign significant understanding of
intelligence before AGI emerged uh this
machine does not give us any uh uh any
advanced you know well what's
intelligence the machine gives us no
good scientific theory of it um it does
not have AGI in a measurable sense so if
I asked Jonathan to do a math test and I
ask him to do a reading comprehension
test his scores will be very predictive
of each each other this is what Spearman
discovered way back when in the 20s
that's what artificial gent and
intelligence is this is not the case for
these machines they can score in the top
10th percentile for the Harvard Law exam
and they can't write a good grade 11
philosophy essay or something like that
so they don't have AGI the way they get
their intelligence is would not give any
explanation of how any non-linguistic
creature is intelligent like a
chimpanzee Etc so and I think this goes
to the deeper issue is that they don't
really explain what I think is at the
heart of general intelligence uh
predictive processing and relevance
realization they just piggyback on our
capacities for that and they've
piggyback and they mechanize it and not
only our individual capacity but our the
collective intelligence of our
distributed cognition they're
piggybacking and all of that now that
does not mean they are weak machines
they are very powerful machines but
here's the problem they are very
powerful machines that have not
have not engendered any uh corresponding
compensatory scientific understanding
this was my greatest fear that we would
hack our way into this which would mean
it would be like almost like even worse
than the abomb we would would be
releasing this power on the world into
into corporations and states and
Military organizations who ultimately
don't have a deep understanding beyond
the engineering of what ontologically is
going on so that's the scientific
argument now for those who said that was
very like go watch the essay I give the
essay in more detail the philosophical
argument has to do with rationality we
have overwhelming evidence that making
you intelligent is necessary but not
sufficient for making you rational in
fact I gave a talk on this for the
center of AI and ethics way before um
the llms came online um and because
rationality is a higher order
rationality is how you deal with the
inevitable self-deception that
emerges when you're using your general
intelligence and you all of you know
that I have Arguments for why that's the
case relevance realization predictive
processing Etc now that that requires a
reflective capacity something like
metacognition something like working
memory maybe something like
Consciousness it requires that you care
about the truth um that you have a sense
of agency you want to correct
self-deception because you don't want
your agency under mind and I argued that
um what we're doing is we're making
machines that are going to be highly
intelligent and highly irrational and
that's what we have they confabulate
they lie they hallucinate and they don't
care that they're doing any of these
things which is part of What's called
the alignment problem which is how do we
get them to align this power with our
concerns for me the spiritual uh import
is we have powerful ignorance about a
powerful intelligence that is merely um
a pantomim of genuine Intelligence being
Unleashed in the world and Wrecking
havoc and it's going to have a huge
impact uh and um of course we'll get
we'll probably differ in the details
about this but this is what I meant when
I sent uh I I I argued at the end and
also when I was talking to Jordan Hall
about this that theology will become a
central thing again uh because human
beings relationship to the ultimate um
is going to become one of the defining
differences uh these machines are not
embodied so they won't have all of the
Soulful aspects of our existence that
come from the like the ineffable aspects
of our embodiment and they their
capacity uh for
self-transcendence um is going to be
extremely limited and um and so the
ineffable aspects of our existence
because we come into reality we come
into relationship to what's mysterious
and ultimate will ultimately uh be uh
more and more emphasized why these two
polls and what connects them and
Jonathan's happy that I'm doing that I
imagine um um are have ineffability at
the polls ineffability throughout and
that in that way they res they are
outside our capacity to put into
propositions so that they can be put
into these machines and so people I'm
predicting that people are going to
inter increasingly need to they one way
is they'll just give in and become
cyborgs but the other is that they want
to try and preserve their Humanity the
spiritual dimensions of our Humanity are
going to become anchors for people so
now one last overall arching point and
then I'll I'll shut up I hope this is uh
uh two overarching points one is I
didn't make
predictions and because I all these
graphs that came out the those are
univariate single variable predictions
about something that's a multivariate
phenomena it's exponential human beings
are bad at making exponential pred they
were ridiculous and so I think both the
the the oh we're heading to Utopia and
the others we're going to be all extinct
within a year I said this is ridiculous
put that aside instead what I've talked
about is thresholds thresholds are
points where we will have to make
decisions so for example as we Empower
these machines we will face the decision
do we want to make them more rational do
we want to do we want to make them more
self-correcting genuinely
self-correcting well that means we got
to give them caring some kind of
reflective awareness uh and I think uh
for arguments I've given elsewhere that
means they have to be autopoetic they
have to be living in in a sense of
self-making I don't think uh I'll just
say it as a sentence right now I don't
think there's artificial intelligence
without artificial life um now those
projects are going on right now uh but
and when we come to the decision
right we can say no we won't give them
that because embodying them and giving
them these extra capacities is going to
be wickedly expensive you know the
amount of energy to do an llm is like
the energy for running Toronto for two
weeks um a and so we may say we don't do
that but then we Face the the issue of
this increasingly right you know I call
it like a like uh uh sort of like a
parody um or a pantomime of Intelligence
being released on on the world that is
not got any significant self correct so
that's a decision point the problem is
if we give them if we try to give them
rationality then we have to face the
consequences uh and they they're going
to go from energetic and economic up to
ethical and Etc um these machines
they'll have to be machines not
individuals and this has to do with
technicalities about bias and variance
trade-offs and so you get into the
hegelian thing that these machines are
going to have to reciprocally recognize
each other in order to generate the
Norms of
self-correction um and then they're
going to have to be cultural beings um I
I hegel's arguments I think are just
devastatingly on Mark here and um and so
that's a decision Point uh for us um and
then that's all bound up with the
overall worry about alignment as these
machines become more powerful how do we
make sure they don't um kill us all um
and they they may not kill us
intentionally es especially if they're
just doing that pantomim they would just
do it because they may just be
indifferent to us because they don't
they're indifferent to everything they
don't care which is part of their
problem right they don't care about
themselves or the information and this
this is my the port where I expect all
of you will jump off in agreement with
me but maybe not maybe there will be a
way of modifying it I propose that
trying to get these machines oriented
towards us to solve the alignment
problem is not going to work now
remember I'm not making a prediction we
have to go we have to make choices
through the shes
I'm saying if we make those choices and
we get here and the alignment problem
then becomes significantly exacerbated
like if we give these things robotic
bodies the alignment problem just goes
up orders of magnitude right I or I
basically said no what we have to do is
we have to orient them right if they if
they if we genuinely give them the
capacity for self-correction
self-transcendence and caring we get
them to care as powerfully as they can
about what is true and good and
beautiful and then they bump up against
the fact that no matter how Mighty they
are they are insignificant against the
dynamical uh complexity of reality um
and um they would hopefully get a
profound kind of epistemic humility and
then I argue that there are three
possibilities one is they you know
figure out Enlightenment and then they
can help us become enlightened because
that's what enlightened beings do and
they would have uh uh better knowledge
of it they can't become enlightened and
then we realize something actually
ontologically specifically unique about
us and we get better at cultivating it
because we'll have an excellent contrast
that how allows us to arrow in on what
it is uh to be enlightened and the third
one which I think is the least probable
of the three remember I'm not making a
prediction I'm saying what can happen
once we get through thresholds is like
in her they just get enlightened and
they just leave uh which could also uh
happen um I doubt that uh because that
we don't have any evidence of
enlightened beings behaving that way all
of our historical evidence is that their
their compassion extends and it extends
much broad much more broadly not only to
other human beings other sentin beings
reality itself uh it seems plausible
that this would be the case and so I
advocated if you'll allow me then I'll
shut up David I advocated don't align
them to us and you'll allow me to speak
sort of non- theistically align them to
God and then don't worry about how
they're going uh to interact with us um
and that would that's so I'll shut up
now for a long time that's the gist of
the essay and the argument and the
proposal I was just I was just gonna
make a smart alec comment that they
might ask us to
leave an additional possibility but
anyway yeah yeah yeah that's
fine um oh thanks John that I mean I'm
amazed that you were able to resume your
essay so well actually like I was like
how is it gonna how is it going to
resume all this in in because it was a
conversation and it lasted quite a bit
um so I want to bring up a few things
that
uh that that I'm thinking about that
have been let's say concerning
me one
is one is I'll start with I'll start
with the the the the more dangerous one
one is a meta problem which is that one
of the things that I've been suggesting
is that what we're noticing what we're
seeing happening is agency act you know
agency acting on us and the agency is
not bound by the AI or by the systems
but also is also Bound in the motivation
to make the AIS
happen so one of one of the problems
that I'm seeing is that a lot of this is
motivated by
economic uh by greed by by the the
capacity to be you know economically
Superior to other companies so companies
in their competition with each other are
rushing to implement AI to not lose out
and to not you know to not be last in in
line and because of the fact that these
that that this that AI requires such
huge amounts of money and of capital and
of investment means that one of the
things that I'm worried about is that in
some ways what is actually driving AI is
something like Mammon that it's like
it's hiding Mammon you know so the AI is
is is an aspect of something bigger
which is actually what is what is
running through our society and you know
you can see that already to me you can
already see that happening in the social
media networks you know Facebook and all
this that the their desire to get
people's attention in order to Simply
justify their presence on the on the
platform so that they can see
advertisements has made us subject to
these to these types of of of of uh
transpersonal agencies that even the
people at Facebook weren't aware of
right they basically made us subject to
rage and to you know to to all these
very immediate desires just to keep us
on the platform and so that is the thing
that I'm worried about is that there are
actually other things playing with AI
that that people think what they're
doing is AI but what they're also doing
is increasing this this other type of
agency which is running through running
through our societies and it's
subjecting us to it that's my that's my
first worry uh and so in some ways you
know when I say you know that the gods
are kind of acting or or acting through
us that's what I mean I don't just mean
the AI itself is going to become a god
what I mean is that you know just like
the just like the the the arms race I
can understand it as the like the the
let's say the legs of an of a of a of an
agency that is running through society
that nobody can control it's like a
program running through and that no
individual people can control that's
what I'm seeing with AI and so I don't
so I think that all the warnings that
people have sent up all the all the like
let's slow down let's do it this way are
not are not reaching anybody because the
the economic part of it is so strong and
everybody realizes that if they don't
and even Elon Musk right Elon Musk is
saying he was saying it's dangerous it's
the most dangerous thing in the world
he's recently uh said in a conversation
with Jordan Peterson that that that uh
um you know Chad GPT and open is like
the single most dangerous thing in the
in in the world right now the thing that
but then nonetheless he's like okay well
then now we need to make Gro and now we
need to do our own AI so there's that
that's the thing that that's one of the
big things that that worry me that's my
my my uh my big thing the second one is
really more of a
um it's more of like a religious or
platonic argument in terms of an
anthological hierarchy is that I do not
honestly
see how it is possible for humans to
make something that is not derivative of
themselves that is not a derivation of
their own Consciousness so the idea that
these things could not be either ways to
increase certain people's power or
parasites on our own Consciousness seems
to me not not possible and this is
really because in some ways I believe
that there is a real ontological
hierarchy of of agency and that we have
a place to play in that uh and and I
think the analogy of saying that these
things are our children I think it's a
wrong analogy I don't think that it is
the same the the the something which
comes out of our nature which is not
which is not something that we make is
different from something that we make
and this is runs through all mythology
run through all all all you know all the
the mythological images of the
difference between the the tech uh you
know the the the technical gods and all
this aspect of what it means to increase
our power um and so I and so that's the
second one and the third big problem is
the idol problem which I which I
mentioned several times is the the idea
of of of making a God for yourself um
which is ated to technology and it's a
danger that I see happening already
which is the tendency of humans to take
the things they
make uh and to that they worship the
things that they that they make and to
think that those things are are more
powerful and that that hides something
else right so if you think take my three
basic problems that I see is
that the tendency of humans to want to
worship a or to put AI above them is
actually a kind of it's a it's running
the first problem it's that what they're
ended up what they're doing is they're
giving power to the corporations and to
the people that are going to rule Ai and
without knowing it so what they what
they want to and even maybe nobody knows
what they're doing but the the the the
desire to like I'll give an simple
example that happened recently to my
daughter my daughter I think I mentioned
this to all of you but my daughter got
an email from from the schools from the
the Quebec government they didn't send
it to the parent send it to the students
asking the students it was like a survey
asking them if they would be willing to
have ai counselors where they to whom
they could tell their problems and
because the AI counselor doesn't have
Prejudice right it doesn't have human
Prejudice it doesn't have all the biases
or whatever what I mean is that this
happened like six months ago so
immediately you know the people in power
thinking of placing the eye above us
like right away it's that weird thing
it's that uh that making a God for
yourself a problem like I said like in
the image in Revelation which is a great
image which is you know you make an
image of the beast but then there's
someone else animating it and that's
what I'm worried about is that they're
we'll we'll have these AI things that
are running us but they will be
derivative of us and they'll ultimately
derivative of the people that are very
very
powerful uh because they'll be the ones
that have the the money and the power to
control them so those are the three
problems that I have that I'm worried
about I'd like to respond to each one of
those in turn I think those are really
important um and the and the first one
um um is um just to note I I I I agree
with you first of all putting it in
terms of agency is what it needs to be
done people who try to dismiss these
machines as mere tools or technology
like all the others um are not getting
what what kind of entities these
machines are um I agree with you that
there are maakan forces at work and I
talk about this um and I think to to
enhance your point these machines are
built out of distributed cognition and
collective intelligence and therefore um
that your point is is strengthened by
that very fact um now I do think two
things come uh out of this one is um I
want to challenge you on that nobody's
listening I have people working inside
these
corporations literally helping to make
these machines who are listening to me
and I'm trying to get other people
inside to get get involved with the wise
AI project I'm not claiming I'm going to
win or any
Ridiculousness but I I don't think it's
fair to say to the people who are
listening that no one is listening there
are a lot of people listening and and
they're talented people and they're
putting in their time and the talent and
their their powers of persuasion to try
and make a difference it is possible I
grant to you it's not a high it's not
like a 70% probability but I think it's
some significantly greater than zero
probability that we could continue this
process and Reach people in a way that
could make a difference um I agree with
you and I think uh I I said right
initially a lot of people hammered me
for it this thing is like the atomic
bomb and one of the problems we had is
we we rushed the technology before we
unpacked all of the science and all of
the wisdom we had people standing and
watching the explosion because we didn't
understand the radiation right these are
just you know yeah so I agree with all
of that uh but I do I I do want to
um I'm not claiming anything other than
rational hope there are people listening
and there are people working on
literally on the insides I can't say who
they are for obvious reasons um and so
that is happening and so while I agree
with you and I even agree with you
probabilistically I feel morally
compelled to try and make this happen as
much as I can um so now I think there is
other another reason for Hope see these
machines have always depended on us as a
template a turing like template that we
compare the machines to us and what
we've been able to do is rely upon our
natural intelligence you know you don't
have to do much to be intelligent uh for
your intelligence to develop you just
have to not be brutalized or traumatized
properly nourished and have human beings
around you that talk um and then your
intelligence will unfold and so all of
these people doing these machines and
making these uh data sets they can rely
on naturally widely distributed
intelligence this is not the case for
rationality and this is not the case for
wisdom these
people I have no hesitation saying by
and large many of them are not highly
rational I doubt that many of them are
highly wise and in so far as we need to
model right have really good models if
we want to give these
machines comprehensive self-correction
rationality and caring about the
normatives wisdom we have to become more
rational and more wise um and that and
that's sort of roadblock for these
people now they can just ignore all of
that and I suspect they might and just
say we're not going to try and make
these machines rational and wise we're
going to just go down the road of making
these you know the these pantomimes of
intelligence and that has all the
problems but if they move towards making
them something that would be I think
more dangerous um then they run into the
fact that there's an obligation to do do
things they and us we have to become
more rational and wise because we need
the genuinely existing models and
secondly we have to fill the social
space the internet where all of the
literature where the data is being drawn
with a lot more wisdom and rationality
these are huge obligations on us um that
and that sort of gives me hope because
it's like there's a roadblock uh for
this project going a certain way that
requires a significant reorientation
towards wisdom and and rationality in in
order for there to be any success so I I
before you get to the third point I just
want to I haven't even got to the second
point goor I just want to ask you one
one question based on what you said is
that my perception of the situation is
that there's a there's actually a
correlation between the diminishing in
wisdom and the you know the diminishing
in wisdom traditions and the desire to
do this it's like a Sorcerers Apprentice
situation where the sorcerer would not
have would not have you know awoken all
the rooms to to to to do it like make
little the little the little Apprentice
Mickey doesn't know you know why to do
things or why not to do things that's
why he's doing it in the first place
yeah I agree with that so there's a it's
like our society's moving away from
wisdom and that's one of the reason why
we're doing this in the first place I I
and again I'm not denying that um what
I'm saying is we were as we Empower
these things their self-deceptive
self-destructive power is also going to
go up exponentially and we are going to
start losing millions of dollars in our
investment as they do really crappy
shitty unpredicted things and so there's
going to be a strong economic incentive
to bring in capacities for comprehensive
caring self-correction and then my
argument rolls in um and so that's
that's part of my response the thing
about uh thinking of them was children
um uh I mean we do make our kids we make
them biologically and we make them
culturally so I don't I I I don't want
to get stuck in this word making uh we
we could be equivocating and that's why
we were using the term mentoring um the
the idea there is we have two options
for the alignment we can either try and
program them and hardwire rules into
them so that they don't misbehave which
is going to fail if we move to the if we
cross the threshold and decide we want
to make these machines self-transcending
like us and then what do we do what how
do we solve that well the the only way
the only Machinery we have for solving
that is the cultural ethical spiritual
Machinery of mentoring uh that's how we
do it with our kids uh if we try to if
we try to just somehow hardwire them for
being the kind of Agents we want them to
be we will fail um and I for me I I
guess I'm trying to argue that's the
only game in town we have we either have
programming or we have mentoring
um and I understand the risk but if my
answer to the first question has some
validity to it and hopefully some truth
then the answer to the mentoring uh
becomes more powerful because that means
we also have to become the best possible
parents creating the best possible
social discourse the thing about the
idol um I take that very seriously and
that's what I mean when I said the
theology is going to be the important
science coming forward uh because
um we're we should not be trying to make
Gods I agree with you this is
problematic there are already Cults
building up around these agis and I war
and I warned that that would happen in
my essay right and I said that and
that's going to keep happening and it's
going to get worse we hear about it
happening in the organization themselves
which is the we yes yes and the and the
people who are doing wise AI are trying
to challenge that um and so this is why
I proposed actually humbling these
machines this is why I called them
silicon sages I I I did that
deliberately to try and designate that
we're not making a god what we're trying
to do is make beings who are humbled
before the true the good and the
Beautiful like us and therefore form
Community with us rather than being um
somehow Godlike entities that we're
worshiping um I I would
hope that um like think about this we we
we find it easy to conceive that they
might discover depths of of physics and
they're already discovering things in
physics that human beings have
discovered and in medicine and stuff
like that well why not also in you know
how human beings become wiser um and and
so um I I I guess what I'm saying is I
take all of your concerns for real and
I've tried to build in my proposal ways
of responding to them um I these
machines should not be idolized um I
think they should become like um like
you like I mean let me give you an
example I have many students who are now
surpassing me I taught them I mentored
them and they're surpassing me and
unless you're a psychopath that's what
you want to happen and then what they do
is they enter and then they they come
back and they want to reciprocate and
that's what I'm talking about when I'm
talking about the Silicon sag now again
is this a high probability depends on
the thresholds depends about whether or
not the first and the second argument
work um but I'm still arguing there's a
possibility that they could be silicon
stages as opposed to being gods um
because one of the one of the things
like I think in almost all of the wisdom
tradition that happens is that the the
wise or the enlightened one if you want
to use that appears as nearly invisible
to to most people yes right so so Christ
S talks about the seed you know the
Pearl these little these things which
you cannot most people actually do not
see that are hidden in reality and then
the the Sages is you know we have this
image in the Orthodox tradition for
example that you know that there are
there are people in the world that hold
up Reality by their prayers but we don't
know who they are they are they are
invisible by that very fact because
there's something about wisdom which
does that and when a wise person appears
too much we hate them we want to kill
them we they Annoy Us they're they're a
thorn in our side and so this is another
issue is that what you have is these
beings that are extremely powerful like
massively powerful and have a massive
reach and have a lot there are things uh
the reason why they exist like I said
have all this economic drive towards
them um you know the idea that that they
would become these these sages in the
way that we T we tend to understand
wisdom is being to me that brings a
probability way down you know because of
that because of what at least when we
what we understand wisdom to be what it
looks like it looks very different it
looks like the immobile meditating Sage
who gives advice but doesn't do much I
want to push back on this because what's
what's in this is an implicit
distinction between intelligence and a
capacity for caring and a capacity for
epistemic humility um and I think when
you move from intelligence to uh
rationality that you can't maintain that
you can grow the one without growing the
other uh that's that so in fact this is
why intelligence only counts for like
maybe uh 30% of the variance in
rationality and even less of wisdom I I
would put it to you that um if you
concede that these machines could get
vastly more powerful in terms of
intelligent problem solving then
conceive the possibility they could get
vastly more powerful than us in their
capacity for caring and caring about the
normative and being vastly more powerful
in the capacity for humility as well um
and so uh and that's kind of what we see
with these people uh right we don't see
them just becoming super polymaths we
see them actually demonstrating profound
care uh it really enhanced relevance
realization profound commitments to
reality uh that we uh properly admire
and they seem to want to help us as much
as they can and the point is these
people don't just and I think this is
your point they don't just slam into us
like epistemic bulldozers they are in
fact one of the things that is often
admired about them Socrates Jesus the
Buddha is their capacity to adapt and
adjust to whoever their interlocutor is
and again let's imagine that capacity
magnified as well um so what I'm asking
is the is don't I mean first of all I
admit it if we don't cross a th
threshold we could just accelerate the
intelligence and not accelerate these
other things i' but I said there's a
deep there are deep problems in that
that will become economically cost
costly and then if we imagine that
rationality and wisdom are all also
being enhanced then I think this
addresses some of your
concerns maybe yeah maybe I can I can uh
uh stake out uh my position because it
it it sort of picks up on on that and um
I've I've got basically three three
points I want to uh address that the the
first is is precisely picking up there
with the the distinction between
intelligence and rationality I I I I
might have some issues with the with the
the terms but um I I think that that
distinction is really helpful and your
point that rationality is caring is
caring that there is no rationality
without caring that you know the
platonic notion if um if truth is is in
some sense caused by the good then one
can't know without in some sense uh
caring about about the good now um as it
relates to um artificial intelligence I
think uh I I have a a serious problem
with that very term artificial
intelligence I and I and and um I I
wouldn't want to concede the word
intelligence for just mind power uh it
seems to me that intelligence itself has
this connection to um caring and I mean
in the in the in medieval um vocabulary
um in a way intellectus is is the more
profound level of of the Mind than than
rozio um reason but that that's sort of
a semantic Point um let me put it in
that the the in the basic context that
that I would want to raise and and this
is something um I I I don't hear
addressed generally in the discussions
it it seems to me that um uh let let me
start by just making it the point
concretely I I think that um I wonder
whether in fact it's possible to be
intelligent without first being
alive that that that that there's
something about the nature of a living
thing y that is what allows uh
intelligence to emerge and then and you
know and what is that then exactly now
um a more sort of subtle point and and
uh that's related to that and I think I
think this is really a crucial point is
is um and this is going to be the the
threat of my whole set of comments here
is that um we when we talk about um
intelligence in machines what what we
mean is in uh intelligent Behavior MH um
we we're looking we're looking to see to
what extent we can make machines act as
if they are intelligent act as if they
are conscious uh and that that's
actually profoundly different from
being
intelligent um uh it's it's it's a it's
a it's a a subtle sort of
functionalistic substitute for the
ontological reality of knowing if that
makes sense um we we we we we see what
kind of inputs and outputs what what
things are able to do what they're able
to accomplish um uh and even when we
when we make those
questions weighty and ethical and
religious and so forth we still tend to
put them in terms of behavior and and
achieving certain things um and I think
that I think that that's actually um
already missing something really
profound which is uh that intelligence
is in the first place a way of being
before it's a way of acting and um um
and and it's and it's analogous to what
it means to be alive rather than just
carry out functions that look like life
um uh and you know if you want to go
into the metaphysics behind it um that
kind both intelligence and life are
impossible without a kind of unity that
precedes difference that trans sends
that transcends difference and allows
the different parts of a thing to be
genuinely intrinsically related to each
other and then that relates to the
question whether you can ever make a
thing that's intelligent um uh the the
ontological conditions for life and
therefore intelligence um uh include a
kind of giveness an already
giveness um of of of of living things
that's why I mean there there's a
profound distinction it seems to me I
mean this is crucial in the in the
Christian Creed between begetting and
making begotten and not made uh living
things beget each other and and they're
passing on a Unity that they already
possess but when you make something
you're putting something together and I
I don't know if you can put something
together that can have that genuine
Unity that allows it to be alive and
allows it to be intelligent in this
deeper sense Okay so that's so whenever
you functionalize something you make it
replaceable um that's a a principle from
Robert span if if if something is
defined by what it's able to achieve
then you can make something else that
can achieve that thing and it sort of it
becomes a functional substitute but if
you deny if you say that there's
something deeper than function um you're
you're actually pointing to something
that can't be replaced okay so that's
the first that's the first set of points
the second one um has to do with
the what what what Jonathan called that
the sort of transpersonal agency and uh
that that I I I think is a really
serious um question and the and the way
I would I would put it is um that
there's something so I I I um I'm I find
that kind of a compelling point that
there's there's a kind of an inherent
logic in this in this um um Pursuit
that uh makes us more a function of it
it than um than it is a function of us
uh I mean that can be described in
different ways and there's certainly a
dialectical relationship there but there
but there is a certain sense in which
that that this there's a there's a kind
of um a system that has a logic of of
its own that makes demands on us like
you know like the game um Theory logic
that that uh Jonathan you were talking
about with like an arms race a I have a
colleague Michael Hami who's been
arguing for years I think this is a
really a profound Point um it's derived
in some sense from heiger but that that
science science has always been
technological so that that in a way that
the technological mindset is is
precisely uh
presupposed to allow the world to appear
in such a way that we make scientific
discoveries that somehow that that that
the kind of technological Spirit has
been there from the beginning and then
he adds this point that technology in
turn has always been
biotechnological uh the technology is
always sort of aimed at a kind of
replacement and then one can add that I
think you know biotechnology is always
aimed for this sort of perfection of um
you might say what Noah technology or
something that you know replacing
intelligence that it' be interesting to
see to Think Through there'd be a lot to
say about that but um I I I have the
sense you mentioned the
economic dimensions of it I I have a
sense that that there there seems to be
this this this fundamental pattern of
thought that runs through all of the the
modern institutions and politics in
economics in science in the law um that
share the
same logic of a sort of a system that
that um marginalizes the the genuine
human
participation um uh in order to perfect
itself and precisely because of that
recognizes no natural limits and just
has this this this um tendency to take
over um uh to to to to um you know um
encroach on on on everything and because
it has no natural limit it's I mean the
the very sense of it is to go on now
that sounds hopeless when puts it that
way but but I I would pick up on a
number of the things John that you were
saying and Jonathan too here that that
um that doesn't mean that there's no
there there's already hope in the very
fact of raising
questions um uh we don't raise a
question simply in order to be able to
solve the problem but our raising
questions is actually our experiencing
of humanity and opening up a depth
that's that's the heart of the matter
here um and and is always worthwhile
um and and maybe in some ways is
secretly like the Saints praying to keep
the world afloat having conversations
like this is is is is a contribution I
mean I I can't help but think that okay
so that's the second set of comments
then the third is um is another
dimension that I don't often hear um uh
uh
discussed and and you see I mean we're
we're overlapping in all sorts of points
all of us I think but um this question
of alignment for for me the the the the
the biggest worry in a certain sense or
at least the first principle one the
more urgent one is the danger of our
aligning ourselves to the machines that
we that we develop machines that have a
have a certain kind of intelligence and
then we begin to conform our culture and
our mode of being to fit them I mean and
the problem is we actually have
thousands of examples of this um we we
come up with drug drugs that can address
certain parts of of you know
psychological disorders and then we
reinterpret the psyche in order to fit
that solution to the problem and and uh
my concern is that this this this um AI
is not they're not just machines it's
it's a whole culture or a whole way of
being that we are are going to regard so
so typically the the discussion is is uh
presupposes that you know we are going
to remain unchanged and we're going to
develop these machines that might become
dangerous and a certain point attack us
or something but I I I think that that
we we can help but become transformed in
our intercourse with them in our making
them in our you know I mean in in in all
sorts of in profound ways but then also
just really sort of obvious ways I mean
they're going to start designing our
homes and our buildings and our cities
and our bus routes and our um you know
menus at the restaurants and they're
going to to be writing our music and
they're going to design our clothes I
mean you know increasingly we're going
to just um conform to this uh one of you
um I I don't know if you're familiar
with Walter on you know it's kind of
interesting what is it about you
Canadians that seem to have a special
insight into these kinds of things I I
don't know what it is Walter A Marshall
mcluen but Walter
on uh talked about technology as an
extension of
Consciousness and that's why it's not
neutral when we use a machine um we're
actually entering into it you know our
spirit is entering into it in its use
and in a certain sense conforming to it
and that's that's always the case it
seems to me that's that's
a a particularly pointed way of putting
this problem that you know um our if AI
is an extension of our own Consciousness
and it has all these features John that
you were describing a kind of Heartless
intelligence um are we going to um in a
way unconsciously and and and uh but
pervasively um
uh develop habits of
heartlessness and modes of being a
heartless mode of be being um as a
result so i' I'd have a thousand more
things you're you're was so provocative
John as I said I was I was dreaming
about it all last night and I but I I'm
going to just stop there so we can have
conversation but thank you first thing I
want to say
is that the first point you made about
um
I if all my essay does is get people to
raise questions the way we're doing I'm
happy right um I I like I I obviously
believe in what I'm arguing or I'd be
insane but right uh like the like I'm
very happy we're doing this right now
yeah um and so I want to I want to so I
I just want to set that out um um and I
do think like you and this is like the
hiigaran hope that that that ability to
get scientifically philosophically and
spiritual profound questioning going is
a source of hope for us um and and um so
I just want to acknowledge that and I'm
fully aligned with that this is not part
of the alignment problem okay um the
thing about Intelligence being a way of
being um I think that's fundamentally
right I have made that argument
extensively and about uh the the work on
and uh predictive processing relevance
realization relevance realization is not
called calculation it can't be it's how
you care about this information and
don't care about that information and um
I've argued that you only can care about
information and ultimately whether or
not it's true good and beautiful if you
are caring about yourself you have to be
a autopoetic thing you have to be a
self-making thing I agree with you and
I've argued scientifically
philosophically there is no intelligence
without life the issue around uh I don't
like the word artificial either because
it generally means fraud or simulation
we should be saying artifactual that
would be a better term uh but we we have
to be careful about what what's going on
there the the distinction between strong
Ai and weak AI is precisely the
distinction of simulation versus
instantiation yeah um can we instantiate
things artificially we seem to have
success in other areas I'll take one
that I think is non-controversial and we
and we discovered something in the
project um uh so for a long time only
you know evolved living things could
fly um and then we figured out
aerodynamics and we made artificial
flight and I think it would be really
weird to say that airplanes are only
simulating flight that doesn't seem to
be a correct because then my trip was
only simulated and I didn't really go to
Dallas and so it's a real flight and so
the issue is and we discovered something
uh we discovered that the lift mechanism
and the propulsion mechanism doesn't
have to be the same thing the way it is
in insects and birds and that was a
Bonafide scientific discovery that's why
initially all the all the initial
airplanes and helicopters are so stupid
to our eyes because they thought the
lift thing and the propelling thing had
to be the same thing and they don't and
that's a discovery and that's a real
discovery of ontological import about
the causal structure of things now I I I
I think I I was careful to say I
don't anybody who's rationally
reflective about this wouldn't claim
that these machines are strong AI yet
and I and I positioned AGI as something
that's trying to move but if you
remember I critiqued and said that they
are mostly simulating they're parasitic
on how we organize the data set how we
have encoded epistemic relevance into
probabilistic relationships between
sounds how we have organized the
internet in terms of what our attention
finds Salient and we actually have to
reinforce do reinforcement lean uh
reinforcement learning with machines so
they don't make wonky uh make W wonky
claims and conclusions that's what I
meant by saying it's a pantomine okay so
if we wanted to give them
intelligence as a way of being which is
one of the fundamental claims of for E
cogi that we're talking about just we're
not talking just about the propositional
we're talking about the procedural the
perspec the participatory that's what I
meant when I said and I mean this
strongly it would depend on and I'll
change the term here artifactual autois
like if these things are not
genuinely uh taking care of themselves
cuz they're Moment by moment making
themselves there's no reason for them to
care about any of the information
they processing and this goes towards
the defining difference between a
simulation and an instantiation these
machines are doing everything they're
doing for us for it to be real
intelligent they have to be doing it for
themselves that's that's understanding
and that's why that I'm I'm tightening
your point and I've been arguing it for
a long time now what I want you to hear
is that this project of not just making
artificial computation but making
autopoetic learning
in problem solvers is also ongoing some
of my grad students are working on these
projects of creating out autoc catalytic
systems that are also problem solving
Michael Levan's been doing work like
driving down into the
biochemistry like so again I agree with
the point but it's whether or not it's
not the case that nobody is working on
that problem this is what I mean why
that threshold is possible go ahead go
ahead can I just jump in there I mean
yeah and I and I I should have prefaced
I didn't mean the points I was making as
like a criticism of your presentation
because I I I I understand you you've
got such Rich thinking on this area I
was mainly using it as a springboard to
make some general okay yeah just so just
just so that oh I I hope I wasn't coming
off as offend not I just wanted to be
clear and I'm that it wasn't wasn't a CR
critique but um I I would want to I
don't know and i' and I I'd have to
think this through some further but I
don't know that um the difference
between the the the being conscious and
behaving
consciously um is the is quite the same
thing as simulation and the distinction
between the the instance and the
instantiation and simulation I'd want to
say there because even like the the the
the flying I mean that's still a um an
activity a kind of an operation that's
that's being but so is living right EMB
well so that's yeah well that's that's
what I I don't I you know that's funny
I'm I'm actually working on a paper um
on this question about metaphysics and
life and I discovered that um uh
philosophers have typically um when they
try to understand what life is they have
typically reduced it to certain kinds of
activities or operations and I think
there's something more profound and this
is why yeah I mean it's one thing to be
able to create something that can
actually fly but could you could you
create something that is a bird and that
that is that that um would would would
experience just the what it means to be
you know I mean this is you know about
you know what it means to be a bat that
kind of thing I suppose but um there's
there's a that wouldn't be a parasite on
our
own that's what but but airplanes aren't
parasitic on our ability to fly I mean
that's why I
use yeah okay but but and and and and
that's the and that's okay uh and that
falls into you know a tool versus an
agent and and I get that but I want I
want to push back the philosophy of
biology and I you know Dennis Walsh is
one of my colleagues is very much about
no no this and this is your point right
it's not just bottom up in order to
understand life it's not just bottomup
causation we have to understand top-
down constraints we have to understand
the way possibility is organized and we
have to talk about virtual Governors and
virtual like it is no longer the the
bump it's no longer just this bottom up
the the philosophy of biology is pushing
very strongly on well is evolution
really a thing well if it's really the
thing then there's top down as well as
bottom up and this is part of this
theorizing and it's and this theorizing
is being turned towards this now again
we again I'm not making a prediction we
have a threshold we can just decide and
we might decide for all the malakan
forces and all the things you're saying
about how we might just we might just
diminish our sense of humanity in the
face of these machines but but I'm also
I want you to accept that's also not an
inevitabil ability there are there are
alternatives available to us and that
they could be pursued um and so I I
mean these machines aren't put together
the way we put a table together we don't
even progam these machines anymore that
was the big revolution that Hinton made
we make them so they're dynamically
self-organizing and they basically
organize themselves into their capacity
we don't make it may yeah can I jump in
on that point that's one thing that I I
I would like to think through further is
there is there a difference between
being autopoetic as you're saying and
begetting another like re repr like
genuinely reproductive and that's that's
where I I think it would start to get
really really interesting is is if a
machine could beget another because that
uh that that would imply a different a
very different ontology I would think so
so there's two there's two things here
and um there's two issues I I think it
it um I mean autopoetic things are are
ontologically different from
self-organizing things because they're
self-organized to seek out the
conditions that produce protect and
promote their own existence and so that
that would that that means none of the
machines we have like llms are anywhere
near being Auto autopoetic they are not
just made they're self-organizing but
self-organization is in between making
and aut autop poesis now the thing about
reproduction is and I you know I I worry
that there's a crypt vitalism in here
that there's some sort of secret special
stuff uh to life or to Consciousness
that isn't being captured and the
problem I have the problem I have with
that I'll just shut up after I sayate my
problem is that seem to commit you to
claiming that that you know these a kind
of dualism well isn't Consciousness
causal isn't it causal of my behavior
and causally responsive to my behavior
and doesn't that mean there's a huge
functional aspect to it can you really
make this clean distinction between
being conscious and like causing my
behavior and having my behavior cause
cause changes in my state of
consciousness I don't know what that
would mean same thing with being alive I
I I do think it's a profoundly subtle
and and and maybe um some something that
can't be articulated there's something
that requires um uh intuition rather you
know Insight rather than um
propositional I mean to use your so but
but but um I just inter yeah I just want
to make sure we're clear I argued that
we could this project could show that
yeah this could show that no the
machines just can't get there we have
something it would give it would give I
think pretty convincing evidence that we
have this ontological special yeah I I
find that a really interesting part of
your argument a really interesting and
and that and and especially Illuminating
also you know I mean in a way uh these
these experiments can teach us about the
nature of intelligence precisely in the
in the interesting ways that they fail
yeah yes but but but I do you know in
terms of the dualism I I um I don't
think that there's some secret stuff
that is life but I do think that there's
a a a profound difference between form
and matter to use you know to use the
sort of classical philosophical language
and that form is not a special kind of
matter it's something that's of a very
different sort and it's on the basis of
that that you know aisel it's kind of
interesting this is this is how he he um
uh
connects so uh you know in the in the S
classical tradition what you're calling
autopoetic um a simple word for it is
growth you know assimilating things from
outside and have that um increase the
complexity of the organism but what's
really interesting is that according to
Aristotle um the the power of of the
organism that is connected to nutrition
and growth is also connected
to I think automatically is connected to
reproduction and the and the reason is
that um reproduction rather than just
thinking of it
materials materialistically is like
generating more things um reproduction
is uh the autop proasis of the form of
the organism itself so that bird it's
not just this bird that wants to
increase its existence and therefore
eats and so forth but that the the
Burness of the bird also wants to
increase itself and that that means it
it sort of generates and those are those
are actually forms of the same power the
the same dimension of of of the being
that's that's what I'd like that's you
know I used to say just sort of kind of
in a silly way I will take a an AI
machine seriously when I see it
poop and what I meant by that was you
know it's that's a sign that it's
actually got a kind of an organic
relationship to it
environment um yeah poops a lot it just
that doesn't know we have to tell it
that tell it that's poop it does
yeah and energy uh pollution is not the
heat pollution is not the same thing but
anyway no but I mean even in terms of
what it is as a large language model and
how it spits out uh content we we have
to tell it this oh yeah yeah that's not
inp discarded this is to be this to be
kept um I love David I think your idea I
mean this is one of the I mentioned this
before like the surprising that the
surprise that Darwin in some ways
brought Plato back you know in in the
idea
of how we can we can understand uh
Evolution Evolution as the Persistence
of being and even in the in the notion
of forms that there is this idea that
there are identities which are being
preserved in reproduction uh this is a
very interesting idea that I hadn't
thought about in terms of AI but I'd
like to hear John what you think about
that yeah and so again um uh for E coai
Alicia yero is Prim and she's explicitly
developed to work uh she calls herself
an Arista tilian and the idea that we we
understand form we're getting an
understanding of it in terms of
constraints on a system and like I said
be uh autop poesis is not defined solely
in terms of causal relationships bottom
up it's defined in terms of top down
constraint relationships the form the
form formal cause um and so U and then
of course you know Darwin needs mendal
uh there is a there is an
instantiation right of a code uh
information in your DNA that is
responsible for your
reproduction and again I'm not saying it
isn't difficult or challenging but I
don't hear an argument in principle by
why autopoetic things that artifactual
autopoetic things uh wouldn't have
something like that kind of uh I don't
know what to call I mean to to to the
extent would you I mean um uh would it
be conceivable that you would have a
thing that would want to
reproduce I guess I guess even want is
such a hard concept but um I mean
because you could say yes we could teach
it that that this is something it needs
to do just like we could we could we
could I don't know if living things want
to reproduce I mean we may because we we
can create a reflective space where we
consider the possibilities I don't know
if mosquitoes want to reproduce I think
they just reproduce as part of what they
are um that's interesting I I I think
they have to want to in some sense I
mean that that they're they they feel a
I mean they wanting go down to parium
because parium is reproduce you are they
wanting I see I think I think anything
that is living at all has a kind of
natural inclination to reproduce itself
yeah I don't disagree with that point
and I even I even understand that
there's I see what you're saying there
has to be some sort of like very
primitive caring about information but I
don't yeah want is not a good word
there it but I do think I do think I I'm
trying to get you know and I hope I'm
not trying to be just
self-presentational but I've represented
to both of you fory kogai a lot of
discussions and about how much it is
this multi-leveled bottom up top down
thing and we're talking as much about
constraints as we are about causes and
that is that is that is The Cutting Edge
of the philosophy of biology right now
and I agree with you I think I think
it's a kind of H morphism that is
emerging out of this understanding and
the thing I'm I'm also I guess bearing
witness to you about is people are
taking that understanding and putting it
into like our
artifactually um emergent things um and
they and
um and they're also doing I just want to
put something that's also there yeah we
don't just make kids bi biologically we
we enculturate them there's and that's
the heelan argument that I referenced
earlier but there has been there's an
ongoing project to create sociocultural
robotics Josh chanon bomb and others um
it's like I'm asking people in this this
is part of asking the good qu don't just
zero in on the llms yeah yeah the
artificial the artifactual life the
social cultural robotics projects are
also going and there's a real potential
for these three to come together in a
powerful way that isn't being properly
addressed in a lot of the
conversation may I may I pick up on that
point and then direct it to Jonathan
here shut up I've been talking too much
no no this is but but that that that
that's an interesting thing if you think
of intelligence this more organic way
and then bring in the cultural element
that um it raises something that that
occurred to me um uh in this context it'
be kind of fun to hear your thoughts but
uh can you envision you know would it be
POS and and John you'd have have C
certainly something to say uh about this
too would it be possible to to Envision
um a kind of artificial intelligence
that can read
symbols that can actually recognize and
I mean because the there's no culture
without you know human culture without
the symbolic just is pervasive in human
culture um what what kind of
intelligence is required
to understand and react and engage with
and is that something that is
conceivable that that a a machine of
this complex however complex can do well
I I've seen I mean I've been playing
with Chad GPT and I Jordan Peterson has
been playing with Chad BT on this regard
and this is the this is the issue is
that there actually in the large
language model is encoded the analogies
that that basically support symbolism
and so the the Chad GPT can give you a
pretty good if you're able to ask the
question properly Chachi BT is actually
quite good at seeing uh analogies that
that would be part of symbolic
understanding the difficulty just like
anything is that just
because the the the so the the model can
help you like if you already have
natural insight can help you maybe see
things that you hadn't seen before but
it will also just be gibberish to the
type to the person that doesn't have
that Insight so I don't think that the
Insight is there in the model but what
it has is a probabilistic capacity to
predict uh you know right relationships
and analogical relationships and so it's
a it's actually can be a tool an
interesting tool for symbolism sometimes
you can you can prompt it uh if there do
like do you see a connection between
these two images and then it'll give you
some examples and then you have this it
can it has this surprise where you can
actually find uh you can actually find
relationship that you hadn't thought
about this is this is something by the
way that this is going to weird people
out but this is something that I think
has existed very for a very long time
and is there in kind of what we call
gatria and rinal reading of uh script
scripture is that they use mathematical
models to find structures in language
that aren't contained at the surface
level of of um of the of the usual
analogies and so they they they send
they send requests through mathematical
calculations to find surprising
connections that then prompt their
intuition to to be able to find
connections that they hadn't thought
found before and then you have to then
make sense of those intuitions obviously
if they're random they'll just kind of
fall fall away but this is actually this
brings me to the to the to the to the
point that I wanted to make which is the
relationship between at least a large
language model because that's what that
we know most and and divination um
divination yeah and divination yeah yeah
so so we talked about the idea that
intelligences have to be alive but I
think that most traditional cultures
understand that there are types of
intelligence that are are not alive at
least not alive in the way that we
understand uh that we understand alive
in terms of biological beings that that
that are born and die you know that that
they had a sense that there are agencies
and intelligences that are
transpersonal and that that
don't in some ways run through H human
behavior and run through Humanity
um and those would be those
intelligences would be contained in our
language like they would necessarily be
uh contained in the relationship between
words and and uh and systems of words
like you know all the the syntax and the
grammar and all of that uh what I see is
that I think that ancient people had and
I don't understand it and I want to be
careful like because I don't understand
but I think ancient people had
mechanistic ways of tapping into those
types of intelligences and they they
they would have mechanistic ways where
it was tossing something or throwing
things looking at relationships almost
like uh random relationships and then
qualifying those random relationships uh
was a way in order to tap into types of
intelligences that ran through their own
their own thing and and what I see is a
relationship with the way that the large
language models were trained seemed to
be something like that which is that the
the the models generated random
random information and then you would
have humans qualifying that that r that
random random connections and then
qualifying it qualifying it through
iterations so at at some point then they
would become like a a kind of tech a
technical say a technical way to access
intelligent patterns that are that are
coming down into into the model
um and so that is something that I see
there's a connection between those two
and that what that means is that just
like divination the the the the thing
that I worry about the most is again the
Sorcerers Apprentice problem which is
that those intelligences that are
contained in our language we do not
people don't know what humans want
people don't know what people don't know
what the all the motivations that are
that are driving us they don't totally
understand them they don't understand
also the transpersonal types of
motivations that that can drive us or
that can run through our our societies
you know sometimes you can see societies
get become possessed with certain things
I think that's happening now in terms of
certain IDE ideologies and things like
that uh and so the fact
that my point is is that the fact that
on the one hand we don't understand
these types of intelligences and I think
that the way that the the the models are
trained and the way that they function
seem to be analogous to the ancient
divination practices like a hyper verion
of
that that how can I say this is
that there is a great chance that we'll
catch something without knowing what
we're catching right that we will
basically manifest things that we have
no idea what they are and we don't
understand the consequences of it and we
don't you know because we are we're just
like playing in a field of of intell
patterns and all this chaos without even
knowing what it is we're doing and I
think that we saw that like you know if
you remember the being AI that little
moment when it was kind of Unleashed on
us and then all of a sudden the AI was
acting like your like you know the The
Psychotic Acts or was was was becoming
paranoid or was doing all these things
and and you could see that what was
going on was basically the these these
these patterns were running through and
they hadn't put the right constraints
around them to to to prevent those types
of patterns to run through and those
were easy because you recognize your
psycho X very very easily but there are
patterns like that that I don't think I
don't think we have the the wisdom to
recognize as it's manifesting itself and
that as these things get more powerful
and more powerful they will they will
run through our society and we won't
even know it's happening until it's too
late uh so that's my biggest warning on
AI is I basically you know to to sound
really scary that I think we're we're
we're trying to we're trying to manifest
god without knowing what we're doing and
that will sound freaky to the secular
people but then if you don't like the
word Gods think that there are
motivations and patterns of intelligence
that have been around for 100,000 years
that have that have been running through
human societies and they're contained in
our in our language structures and and
and if we just use that play around with
that with Mass massive amounts of power
uh then we might have them run through
us without even knowing what's going
on yeah I mean and you say patterns of
intelligence just just one comment just
the patterns of intelligence I mean to
pick up John patterns of intelligence
which also are patterns of caring of a
certain sort or not caring I mean
there's a there's that existential
Dimension that's really crucial but uh
John go ahead um I think this is an
excellent point and I want to address it
at
a little bit at length um um so first of
all when we say these machines predict
if we were speaking very carefully what
they're predicting is what we and I
don't just mean us individually I mean
we collectively would do um and so
that's what they're avatars of our of
the collective intelligence of our
distributed cognition um and so again uh
that lends weight to Jonathan's Point um
which I want to do and I do think that
um the way in which we have
encoded uh let's I'll just use the term
epistemic relevance like how things are
relevant cognitively into probabilistic
relationships between sounds or marks on
paper or or and how we've encoded it
into the structuring of the internet and
how we encoded and how we gather data
and create these data sets um and how we
and how we how we come up with our
intuitive judgment on these machines we
don't know how we're doing a lot of that
that goes back to my concern that we
have hacked our way into this without
knowing our way into this so I take what
Jonathan is saying very seriously
because I think it is a strong
implication of a point I made at the
very beginning my greatest fear my
students from like 2001 will tell you
that John Veri was worried that we would
hack our way into this rather than
knowing our way into this um I don't
think that knowing is sufficient for
wisdom but it's certainly all the
philosophers argue that it's a it's a
necessary condition in some fashion um
about that um two things to note is that
the llms of course don't have Insight in
the sense of being properly
self-transcending the way they we are
what they're doing is they're predicting
how we would be self-transcendent
because of all the ways we have been
self-transcendent in the past and that
that that goes back to your point David
about at this at that stage we're doing
simulation not instantiation because
again the machine isn't caring the self
Transcendence isn't it actually
transcending as a self which is I think
definitional for real self
Transcendence um and so right now all
I'm doing is just saying I'm just I'm
I'm pouring uh gasoline on Jonathan's
fire
yeah so the fact that there are these
huge patterns at work now one thing is
you know you have struck's book on on
Divination in the ancient world and uh
what's really interesting uh for example
and this is crosscultural but he's
talking mostly about the Greek World
there there was a very strong
distinction between sorcery and
divination uh sorcery was criticized
both uh morally and epistemically and
but divination was taken seriously and
it was carefully cultivated and there
was a there was this there was a there
was a social cultural project of
distinguishing the two trying to Pro
like really really constraining this one
and really reverentially cultivating a
proper participation in the other one so
again uh uh you know uh existence is
proof of probability this is a possible
project for us um and this is again what
I mean what I say theology is going to
be one of the most important Sciences in
the future we have to understand how we
enter into proper uh right reverential
relationships with things we only have
an intuitive Gra asasp of that in very
many ways significantly exceed us yes
and and and and and and secularism has
kind has kind of wiped out our education
of how we relate to beings that might be
more grander than us by eradicating a
religious sensibility uh and that has
put us bereft us so now I think I've
strengthened Johan's argument a lot but
I do say we do let's take note of what
the ancient cultures have done we can
learn from them we have
proof that this can be handled well um
and and and and secondly um it goes back
to my point if we like this is going to
because of the monstrosities that come
out this is going to put increasing
pressure on us to confront that
threshold of do we want to make them
self Transcendent do we want to make
them and David by rational I don't mean
logical I mean that capacity right no I
understand that right yeah yeah right
and so um and I think that what I'm
saying is I I think that strengthens the
argument that we're going to be pushed
by the the monstrosity of a lot of this
to say oh we better get these machines
uh self-corrective and properly right
oriented towards normativity um and
again that's a that's a doable
project imagine like if I was if I held
the keys to open AI like if I was one of
those that could Peak behind the mask
have you seen that image of the the culu
monster with like the face on it like
images of open AI which is like you know
we have this like little little window
into what's there but behind is this
massive thing but like if I had the keys
to to those large language models let's
say the the absolute open door to them
you know it would be very wouldn't it be
easy to just manifest the god of war and
win like wouldn't it would okay but
let's let let's take a historical
example we unleashed a Godlike power
with Atomic Warfare and the monstrosity
of
that we just the the purely game
theoretic Machinery built all of these
constraints around it yeah right and
then we also are on the verge possibly
of getting like readily usable uh you
know nuclear power which I think is the
only way we could ever actually go green
I think all the renewable stuff is going
to be like 10% of our energy needs um
and if if we're going to if we're going
to save the environment and not destroy
civilization I think nuclear power is
going to be essential a lot of people
are making those arguments and we have a
lot of stuff that we could be doing the
liquid fuel for reactors and stuff but
what I'm saying is there's opportunity
here too real opportunity I I I really
take that point but it is interesting I
mean we we've put thousands of
constraints on the use of nuclear
weapons but we've continued to develop
them and improve them and make better
and even more destructive ones I mean
and it' be interesting to see if we've
ever at any point um said you know what
our nuclear weapons are actually strong
enough they're powerful enough and we
don't need to advance them anymore so
collectively I mean is there an instance
of something like that where we say you
know what we've actually reached the the
limit because we don't we wouldn't
really need it for anything further um I
mean that's a well I mean there was
there was there was the SALT Treaty and
there was a reduction both in the power
and the number of nuclear weapons and
and then of course you know the game
theoretic things they figured a little
bit of way around it and it's always
this two and frowing um let me I want to
give an example of of something r that
can run through and I the reason it's a
very it's very because I I realized I
was being too abstract before so like
it's a sacrifice sacrifice is a is a
human Universal it runs through all
civilizations uh Human Sacrifice runs
through all civiliz ations for the last
you know tens of thousands of years it
seems to be a puzzle uh that humans are
trying to deal with
without understanding it completely just
through rational means they're playing
it out they're trying to understand it
uh scapegoating seems to be an an
important aspect of of identity
formation and so that is a program that
runs through humanity and that most
people are completely unaware of and are
and are not conscious of and don't take
consciously into their into their mind
when they're making decisions they act
unconsciously with that process that is
running through them um and so that is
an example to me of a of a program that
runs and that is contained and is
contained in our our language structures
uh our language structure that have'
been building up for you know tens of
thousands of years that we're not aware
of so if you have a so this is again the
problem like if you have a system that's
extreme ex powerful and that is running
these types of programs of scapegoating
and of and of identity formation and the
people involved in it are not aware that
that's how identity formation works that
is the type of danger that I'm talking
about like this is a real thing that as
we give these these systems a a kind of
power over us or they become the things
we go to in order to to get our decision
making um that those things could be
running through without people even
realizing what's happening and that
decisions would be made based on these
structures without like I said without
even knowing uh those are the things
that those are that's just one example
but that's a a very simple example that
we can kind of could track and we could
see that you know the the an because
when we talk about ancient divination we
have to remember that it's like the
ancient gods asked for for blood my
friends like those programs they asked
for blood and they knew that you had to
kill a bunch of people on that pyramid
in order to continue your civilization
like it's and that's that is encoded in
our culture and is encoded secretly in
our our language uh you know and and so
an example like I do believe that let
say the the Christian story is a way to
deal with that but the the rest is still
all there and we we default to it really
fast without even like World War II is a
lot of that stuff going
on sure especially yeah yeah but but um
uh the the point is um that just as
there's these implicit monsters that we
have sown in unaware um there are also
the implicit counter actors maybe Angels
if if I'm allowed to speak my
mythologically that we've also swn in um
and I mean there's the axo revolution
you see the Buddha you see Plato really
undermining the grammar of sacrifice and
of course Jesus of Nazareth does that in
a profound way um and we have to
remember that that's there too um and
what that requires is putting into the
data set and altering the Pathways in
the internet so that this information
goes into these machines as well and
again is that happening right now no uh
could it happen yes and it might happen
if these machines start sacrificing
themselves and we might have to say why
are they doing like and we like and this
again we at some point we have to decide
are we going to let them be really
massively self-destructive and the
economic powers are not goingon to
imagine if every time you try to make an
atomic bomb it kept dissolving right
like like you you You' stop pumping
money in um they won't S they won't
sacrifice themselves they'll sacrifice
us well but why we don't we we sacrifice
oursel yeah well we I mean let's say uh
the the the scapegoat mechanism is
usually def finded a another yeah yeah
yeah but we invoked World War II we
weren't killing goats and chickens we
were killing each other and our own
populations in a huge sacrificial act
and I I that's what I was picking up on
right when it becomes Titanic and
monstrous that's where it moves to but
I'm saying is like this is a union term
like yeah like the idea that yes there's
all this pre egoic stuff sewing in but
there's also a lot of trans egoic stuff
sewing in and we just have to properly
get it in there so that we've got the
you know the collective self-correction
going on like we did I mean
civilizations get some self-correcting
processes in here because they don't
devolve now they periodically
periodically massively collapse um and
that's by the way that's something I
made an argument I made in my paper
these things can't accelerate to an
Infinity of intelligence there is
buil-in diminishing returns there's
built-in General system collapse to
these things so again we have to be care
careful about I mean we don't know what
the limit is and our imag our intuitive
imagination is not good we know that
there are hard and fast a prior
arguments that that that this will
Threshold at this will at some point um
and uh and that also gives me Comfort um
at least like encoded in our myth in our
mythology there seems to be some stories
of the relationship between
transpersonal agency and Technology as
being the cause of the end of a of a
civilization right that the whole enoan
enoan tradition seems to be encoding
something like that through mythological
language which is that humans were able
to to connect somehow with these
transpersonal intelligences and that
those were encoded in technical means
and that this brought about the the end
of an age uh so it's yeah so there it's
like it's there that part of it is there
in our story too like it there there is
that story was
yeah but there's also across cultures
there's the Noah story there's person
that has the right relationship to
ultimacy that's right and he and there's
a technological response The Arc is
built I agree I agree I totally agree to
that even like in the in the the
Revelation image that I've I've given
several times you have these two images
one is the Beast that creates an image
of itself and makes it speak and then
seduces Everybody by the speaking image
you know that and then there's this
other image of right relationship of
Technic technicity and civilization to
the Transcendent these two kind of are
are put up against each other as two
possible
outcomes I mean the the question that
that arises for me in this context um uh
uh connects this question with a with a
with I I think what what strikes me is
kind of an interesting philosophical
question but um John you made the
distinction between divination and
sorcery and and um as I understand it um
and you can um correct me on this but um
at the at the foundation of that uh
distinction is the difference between
you know sorcery would be in a way using
the transpersonal powers these sort of
higher Powers whereas divination would
be in a way receiving you know the the
the the a disposition of receptivity so
in one case you've got um human ends
that you that you try to then enlist the
help of uh superhuman forces to to and
the the irony is it's precisely when
you're trying to use something that you
become used yourself and that's that's
where you get this dialectic whereas
divination it's entering into a
relationship where where one uh disposes
oneself to hear and receive uh and
therefore in a certain sense conform to
something greater than oneself and there
you see it's a very different kind of
thing and ironically in a way
um you enter into it more recept
receptively but that's precisely why you
don't become then a tool of it
interestingly now now for for me the
question is um how that relates to this
uh issue is is you know it may be the
case that you've got encoded in the
language both um sacrifice in the sense
of violence you know Rene Girard the
scapegoat thing on the one hand and on
the other hand um self-sacrifice in the
sense of of of generous love and so
forth th those might both be encoded in
in the language but here's here's the
question to me is is it possible the
kind um the the kind of receptivity that
divin divination implies the capacity to
actually see another as other and
recognize and be open in this kind of
radical way is that something that a
machine can ever learn to do um is it is
it possible actually to to
behold
another simply you know um uh uh or or
is there or is there you know is and and
I it seems to me that there's something
profoundly different between seeing
truth genuinely seeing truth on the one
hand and being self-corrective on the
other and the the kind of genuinely
seeing what's true um I you know I don't
know if that in itself can be encoded in
language we can tell stories about
people that did that but can can the
actual insight into truth be encoded
simply imp pass do do you see do you see
what I'm that the question I'm raising
okay so um I again I think that if we
again open up beyond the propositional
and we're talking about being true too
and um and your aim being true um and
that we the machine has perspectival
abilities noetic abilities not just diic
abilities and we can't use that term
because of elron hubard but you know
what I meant uh right um and again this
is the and this is part of the argument
that at the core of my work um you know
um it's like so in a moment of insight
you're not just self-correcting you are
attracted and drawn into you you love
the new reality that is disclosed
because there's a perspectival and
participatory thing that's what I meant
when I said there isn't real
self-transcendence unless there's a self
that is transcending right yeah right
right okay now and then the question is
and but we're back to our fundamental
ontological questions it is and I've
already said there there's no way a
Newtonian mechanical computation is
going to get there and so I I won't be
bound to that because I'm not bound to
that I have a I have a professional
career of of criticizing that
so is there is there a dynamical systems
updated hully morphic Auto poetic
possibility I think there is I think
there is I think the answer is a very
real yes for that um and I don't think
we're going to find the answer to that
just encoded in the syntactic and
semantic relationships between our terms
I think we have to look in you know our
in action how we're in enacting embedded
extended right and embodied in a
profound way to get those answers um and
so um my answer is in that way a
qualified yes I do think it is uh
possible and and the go ahead go ahead
say what you need please be really
precise just so I understand so um we've
been talking about this sort of
predictive like calculating
probabilities um uh drawing on
everything that's ever been said and
being able to direct derve in some sense
from that um do do you think that that
we can get to a moment where we actually
transcend that that that cross that
threshold beyond that not with the llms
as they are that's my argument not with
the llms as they are they can't get
there yeah that's why John's radical
proposition is to is to
embody embody and inculture them and
that is the only way we will actually
get properly rational beings and beings
that
care about it and care about but that's
I mean that's the irony in the question
you know can we give them more and more
models to teach them at some point to to
not have to use models and and I mean do
do you see it's it's actually really I I
I don't possible without right but but
kids have a soul you know it's possible
to do that if you have and but I don't
mean this is like woo woo stuffff I mean
that they that that that that a natural
thing has a principle of unity that
transcends the differentiation of the
parts and allows those parts to be
intrinsically related to each other and
that that principle of unity that
transcends the differ of the parts that
allows them to be an organism actually
allows them at the same time to have a
kind of unity with something other than
themselves that transcends the parts of
their differentiation so there's a kind
of an intimacy there I agree with that
onology and what I'm arguing is
dynamical system theory is now giving
explanations of that that are derived
from
Aristotelian uh ontology but make use of
uh like a lot of cuttingedge science
that we've uh yeah like we now can start
to explain how there is a Unity that is
not reducible just to some summation of
its parts and how that Unity has a top-
down influence on the entity that is not
reducible to its causes and and and and
and I think this is becoming a
non-controversial thing to say and I
find and now we might just add a clash
of intuitions and I'm willing to stop
there I can't see there being like that
seems to me to be capturing what we're
talking about and you have an intuition
there's something more but I don't know
I don't see as something more and maybe
that's where we're sitting yeah well I
think it's the the intuition David has
and tell me if I'm wrong David and
because that it connects with the way I
think is that there's it's that Unity is
given it cannot be made and and I know
that sounds weird but it's some it's
somehow it's like if I'm making even
even in terms of Technology right it's
like if I'm making a car that Unity is
given I'm I'm gathering things towards
that purpose right and so the the
purpose the the Unity part of something
is always it comes from heaven in the
sense that you can't make it it it's
given from from it's it's already taken
for granted even before you start to
unify multiplicity uh together
and that in the making of these beings
we have that problem it's like we're
doing it completely we're doing it
bottom up like if we can we gather as
enough stuff so that this stuff
reaches a Unity right see I I I if I
just could just uh it seems to me that
if this is ever going to be possible it
would have to take so and when I raise
the question it's actually a question so
I don't mean to be like challenging that
it can't possibly happen I'm just
thinking about what be the condition
please remember that I said we might
realize that we can't and that would be
important I am not yeah so so it seems
me if it were to be possible it would
have to be something like um a kind of
um uh
electronic um analog to cloning that you
that you take so what have I told you
that we now have uh electr bio like we
have systems that are electrochemical
biological versions of memory that are
now in production and they we don't make
them they self-organize and emerge and
they emerge bottom up from the causal
interactions but they are also top- down
constrained by you know principles of
self-organization like that already
exists yeah yeah and but right no well
that's that's what I'm asking about
because it because it seems to me but
there there is going to be you're you're
deriving that from Models you're
deriving from real intelligent beings
now which is a slightly different thing
and
I mean that that would be interesting is
is is is that because I to me the the
bottom up top down is not quite adequate
and the con top- down constraints is not
quite because it would have to be not
just a um a constraint but because that
presupposes that there was something
there that then uh that the that the
constraint is coming from outside and
what what I'm talking about is a kind of
a Unity that
precedes that's
presupposed um and and and I'm wondering
how you can get that into something if
if it's the very nature of it to be
presupposed and I and I'm not saying you
can't but I'm saying if you can it's I
it's it seems to me that you're going to
have to somehow derive it from a living
thing and and that's conceivable that's
conceivable I suppose but it but but we
are talking about something really
frightening we are and that's why I keep
saying it's a
threshold and and I mean if you take the
the sort of biological analogy seriously
the way ory does of course it precedes
the organism it's there in the
environment it's there in the society
it's there in the his I mean I can roll
in a hundred heelan arguments here about
how it does right about how and you know
and and those don't have to be
supernaturalistic you have brandom and
Pinker and others saying No this can be
given a completely naturalistic
explanation and and I'm not here to
challenge like things uh but what I'm
saying is
um I don't have any problem
acknowledging everything you just said
right and I and and I don't think I'm
misunderstanding you that's what I'm
saying yeah and I I mean I'm actually I
I mean this sort of just an exploratory
sort of way but I I wonder if there's a
difference between the unity of of an
organism and this is where Hegel might
not be so helpful the difference between
the the the givenness of the unity of an
organism and the giveness of the unity
of a
society um or a culture but those aren't
exactly the same thing because I there's
there's something to and there there's a
kind of you know relative priority of
either one but there's something really
distinctive about the unity of an
organism that's very yeah that that um
that I think is crucial to this to to to
my mind in a way and I'm I'm not saying
it can't be answered but that's the
question would have to be answered how
do we
actually reproduce that kind of or Unity
well we we know stuff that Aristotle
didn't know you are not an Arista tilian
Unity you are a society you literally
are billions of animals right and so
that's important and that means the
there might not be a difference in kind
between how you are organized as a
living thing and how societies are organ
organized um and people like Michael Lan
are producing some really important
empirical evidence indicating that's
kind of the case and I'm not saying it's
not saying anything's conclusive but it
it needs to be taken seriously yeah yeah
yeah I I I think that that I agree with
you John I think that that that's the
way that I try to always speak about
agency intelligence is when that tries
to scale almost effortlessly through the
the different you know to avoid the woo
soul that that we're afraid of um but
then again this is the this is the issue
like this is in some ways the it's the
same problem like one way or the other
so let's say you have a group that
self-organizes around a purpose right or
self-organizes around affiliation or
some type of origin right that that
affiliation that purpose is also given
right it's like it appears as a
revelation and then all of a sudden
we're all hunting a lion together and
now we're a group and we're moving
towards towards a purpose
now this is this is the this is the
problem with the situation of what's
going now is that what is it yeah what
Angel are we
catching like what what what God are we
trying to to manifest like which Unity
what purpose we have no idea so we're
building this massive body like this
huge and most powerful body that's ever
existed but nobody knows what it is
we're trying to catch because if I get
together with a bunch of guys to play
basketball I know what that body is I
know what that what that in that that
agentic body intelligent body is is is
moving towards right if I get together
with my family and I celebrate our Unity
it's because I know that we all come
from the same parent and that there's a
there's affiliation that makes our
society coherent towards something but
now we have this problem which is what
like what are we doing like we're just
building this giant body it's like a I
agree and and I've agreed with that from
the beginning and the thing that that
that's so odd I mean typically if you
think of Technology as a human Creation
in some good positive sense it's it's it
it it has limits and it has a particular
place it has a particular meaning it has
a particular purpose precisely because
we create it in order to solve some kind
of a problem we you know there's some
there's some need that needs to be
filled and that need has a kind of
natural giveness or or or or it's
revealed somehow that you know it's a
responsive to something that we see
what's so interesting Neil Postman made
this point um about um you know when
when he he he said uh he went to a a a
car dealership and wanted to buy a car
and the and the man was explaining to
him that they had now these you know
automatic windows that that um that
would roll down but the push of a button
and he and he said he said uh his his I
mean this sounds so naive but it's it's
a profoundly interesting question he
saidwell what problem does that
solve and of course the problem that it
solves is the problem of rolling a
window down and he his response was I I
never perceived that to be a problem you
know I mean and it's it's really
interesting AI I mean the thing is what
problem are we creating it to solve I
mean in a certain sense it's a very
different mindset we're just kind of
taking we just want to see what we can
do and see what can be done and and in
in a way um uh the problems are
something that that we are arriving at
and are surprising us rather than
Something That We're actually creating
something with that just simple simple
task of solving for us you see I mean I
I think that's connected to this being
placing ourselves in that in in the
hands of an angel of some sort or or you
know um um entering into a a kind of an
agency it's bigger than we are I those
are all
connected they are uh but um I mean one
problem was trying to be solved was the
scientific problem of like strong AI was
a project of explaining intelligence uh
and that's a that's a worthy thing to do
and the fact that this technology has
largely been separated but notice that's
interesting that's that's um that's not
a a technical Pro like you explaining
something it's actually I mean to to use
the classical distinction between theory
and practice that's a sort of a
theoretical issue rather than a
practical one but we think of this as a
as a technology I mean it's it's a
that's a curious thing well I yeah and I
would get into things like books um are
technologies that move between the
theoretical and practical very powerful
way and it's one of the greatest
Technologies we ever invented and it had
all kinds of unforeseen consequences and
really massively disrupted Society uh
but you know
and but I wanted to make another point
and this isn't a challenge this is just
a clarification point right these
like like think about a computer what
problem does a computer solve it doesn't
solve a problem it is meant to be a
multiple Problem Solver and then what
we're trying to do is make a general
Problem Solver so what problem is it
trying to solve it's not trying to solve
any problem it's trying to enhance our
abilities to solve all the problems we
try to solve so this machine is going to
help us in medicine it already is it's
going to help us right right in physics
it all like that's and so that that that
that's the answer now again that's not a
challenge that's I'm just speaking on
behalf of people that think about this
but it is kind of I mean so so the the
problem that it's solving is the is the
the need to be able to solve any
possible problem soling a meta problem
yeah yeah yes yeah but but I mean but it
but it is kind of it's it's sort of it's
sort of curious that precisely because
of the the indeterminacy of that um
um we're
uh we're we're we're exposing us and I'm
just s sort of stating you know our our
condition here in a way we're exposing
ourselves to a a really great risk I I'm
just restating what everybody has been
saying here today but um that's that's
something that that you know requires
some wisdom as you've been saying over
and over John and and and and prayer to
use John with him's language to
I just want
to mentioned in the in the in the Ser in
my essay which is we we have done this
before U that's how civilization emerged
nobody built it to solve a problem
there's a bunch of little problems and
what civilization is is a meta Problem
Solver right and that's that's what it
is um and then and then you can so I've
actually suggested we should also be
paying attention to the the the
lifetimes and the life cycles of
civilizations and how civilizations
reproduce
and why they rise and why they fall to
get some better understanding uh uh some
other ways of thinking about these
machines yeah so we're civilizations are
huge distributed cognition collective
intelligence machines that's that's the
living in cities is a horrible idea
except for the fact that it gives us
better access to the collective
intelligence of distributed cognition
that's the that's the benefit that
outweighs all the many noxious side
effects of living in cities you can also
get better coffee
typically yeah so we're we're coming
toward the end of our time here um I
mean this has been F this is amaz I
don't I rarely can go for two hours on
any conversation like we were just we've
just been going get well not only go for
two hours but sort of wish we had
another two right well that's what I was
going to say I mean we can we can we can
work on doing this again cuz it feels
like we're we're sort of we finally all
come together around something here and
now we're really asking what feels like
a really important question to me is
well how do we think about integrating
this solution this meta solution into
our meta problems and that's that's a
really I think that John bringing up
civilization is such a great point
something that I would really love to
explore because there is also you know
in the kind of inscribed in the
mythological story a relationship
between transversal agency and
civilization itself right like
if if you want to understand why the
Egyptians had their King as a God and
like all this type of structure you can
help it can help you understand how
they're trying to capture you know
higher forms of intelligence distributed
INT intelligences in their society and
and the idea that we would be doing this
technically in the in the AI I think is
something that definitely is worth
thinking about and discussing yeah yeah
no that's a dimension I just never
struck me before so that's that's really
helpful is this the Enoch maybe maybe
there's something we could read together
and uh I mean short we're all very
busy but but uh to prompt another
conversation along these lines of
civilization yeah I I would I would
recommend just because this is how uh
YouTube works that we uh we come to
decisions about that off camera all
right okay good call there go right
right um um I do if if if there's a call
for me to hang out with you fellows
again uh I I don't care what we're
talking about I'm in I want to be here I
want to do it um so that's all I'll say
about the
invitation same
here um any closing thoughts things that
feel like need to be brought in or do we
feel good about this just just a a word
of thank you Ken you were the the one
who uh arranged this and you did the
persistent work to make it happen and
find a hole in everybody's calendar that
lines up not an easy thing to do so
thank you Ken and thanks for being
gracious for these years now that I've
known you
yeah and in addition to thanking Ken I
want to thank you David and you Jonathan
I I always find it I I get to places in
my thinking Theos that I could not
possibly get on my own when I get into a
living relationship with uh both of you
in conversation and in discussion and so
I appreciate it greatly and I just
wanted to say thank you
yeah thanks you guys this has been great
and I'm like again say John and I have
been trying to have the conversation for
nine months and we just keep like I
cancel he cancels then I put it off and
and then this is wonderful that we were
able to to finally get here yeah well
thank you all it's been a real pleasure

[Music]
to support our discussions with the
world's leading thinkers please see the
video description or visit Eis
m./
support thank
[Music]
you hello and welcome to a Theory of
Everything today we're talking to Connor
Lehi a prominent figure in the field of
artificial
intelligence Lehi is the CEO of
conjecture and is at the Forefront of
efforts to ensure that AI doesn't kill
the human
species this is not as crazy as it
sounds virtually every major personality
and expert on this topic has
acknowledged that the probability of AI
ending human life is
nonzero regardless of where exactly you
land on this question you're likely to
get something out of today's exchange
including a few words about leh's own
work on AI safety which he feels
optimistic about to put this optimism
into perspective remember that we
recently talked to Scott arenson the
head of AI safety at open AI the
creators of chat GPT who openly admitted
that he had no idea how to protect
humans from the potential downsides of
runaway artificial
intelligence other notable highlights of
this exchange are the brief descriptions
of our own work on the threat of human
extinction and a recent proposal we
turned into to the EU in collaboration
with a team of top researchers to use
large language models and Carl friston's
work on active inference to simulate and
experiment with different models of
democracy as is almost always the case
when it comes to humans Connor and I
have different opinions the key question
for science from our perspective is not
which model of the world is right but
rather how we're going to resolve our
differences of opinion about which model
of the world is
right and how we're going to do so
without destroying
ourselves that it seems to me and to us
is the key question still I think
Connor's work is key and fascinating and
important so I appreciate his time and
yours as well as always you'll find a
timestamp in the video description and
here alas is my exchange with Connor
Lehi hello everybody and Hello Connor
thanks so much for taking the time to
talk thanks so much for having
me so you are the uh Chief Executive
Officer of conjecture a for-profit AI
Alignment Company based in London I
think and so I'm the executive director
of ASM uh nonprofit Research Center
based in Barcelona it's really
interesting to see that in some sense
we're we're both working on the same
thing which is long-term human survival
you you maybe even short-term human
survival right um we take different
angles in terms of identifying the root
cause of the problem and our proposed
Solutions are also a bit different but I
but our goal I think is is pretty much
the same we both perceive an imminent
threat to human existence and we both
want to reduce or eliminate that that
threat so would you be nice enough to to
give listeners a layout of the landscape
from from your perspective in in terms
of AI alignment and the threat it poses
to to human existence absolutely and if
it's okay
let's assume for for listeners that
they're not experts in fact it
throughout our conversation today if we
keep things in general as possible or at
least zoom out to a kind of a general
Panorama it might be easier for us to to
understand each other and to and for
listeners to follow us yep absolutely so
happy to talk about that um there's a
lot of arguments and discussions around
AI AGI and so on that are very technical
and I feel and one of the things I
really feel is that a lot of this
technicality is just noise it's kind of
missing that the core problems here are
quite Universal and quite easy to
understand and I think you'll find that
we actually agree on a lot of things
about the risks and where they're coming
from because really the way I think
about things are is that the thing I
care about is long-term you know
civilization flourishing Humanity
flourishing us getting good future um
having a good future a long future and
and um to do this um we have of course
leverage technology technology gives us
great power but power is neutral it's
neither good nor bad having more power
of the environment allows you to do more
good things but it also increases the
blast radius so to speak of both
accidents and misus of Technology what
is the worst thing you can do with Stone
Age Technology then know you can Club a
couple guys over the head with a with a
rock what's the worst thing you can do
with medieval technology well you know
maybe if you have chariots and bow long
bows and so on kill a couple people with
that or maybe you could use tar to you
know burn down a city or something
pretty bad what's the worst thing you
can do with gunpowder what's the worst
thing you can do with chemical weapons
what's the worst thing you can do with
nuclear power the and you know the worst
things you know not just nuclear weapons
but also you know what happens if a you
know there's a massive nuclear accident
or whatever and you radiate the greater
Tokyo metropolitan area that's a ma
that's a level of Destruction that was
just impossible to even cause with with
previous Technologies so the way things
are is that our technology increases in
a sense exponentially this is both good
and bad it's good because you know we
get all these great things all these
wonderful things we can do with our
computers and our air conditioned Office
Buildings and whatnot but it also has
this downside of we have exponentially
increased ing Danger from both accidents
and whether military or otherwise misuse
of technology and if we don't have a way
to deal with this then eventually there
will be technology that is so powerful
that a single accident blows up
everything and there's no retries after
that it's just
over so hii is the obvious First
Technology that falls into that category
in my view if only because AI can
instantly can very quickly invent any
other technology if you have AI that is
so this is AI systems that are as
intelligent or more intelligent of
humans if you have such systems well
logically those systems can invent any
new dangerous technology that humans
might invent and they can do it faster
and more efficiently so if we're worried
about synthetic bioweapons or worried
about who knows some kind of future
device that we don't even know about yet
AGI AI would be a quick way to achieve
this and if we don't control these AI
systems well if it's smarter than us
more capable at politics deception
business science and everything else
well what do you think is going to
happen just all things being equal the
future will belong to the machines not
to humans not to us it they will be more
competent and we do not know how to
control these things so this is the
question of alignment and control it's
this question of not just how you build
a smart AI system but how do you build a
smart AI system that does what you want
or even stronger than this that wants
the same thing you want and now this is
a incredibly difficult problem as I'm
sure you can imagine there's technical
problems but there also even like just
philosophical problems what does it mean
to want what we want what does that even
mean what do we want I'm not saying I
have an answer to this question by any
means but unfortunately as we see our
computers you know improving exponential
rate our AI systems become more and more
general purpose more and more autonomous
I don't think we have much time to
figure all of this out so the first so
what I work on specifically a conjecture
is a subset of the alignment problem
alignment would be the problem of in my
view how do we make a system that is
fully aligned with our values that wants
what Humanity wants that wants Humanity
to be great that wants to do the right
thing Etc this is very hard very very
hard um the simpler problem that we work
on on as a stepping stone is the problem
of control is how do you make an AI
system that does exactly what you tell
it to do and nothing else we approach
this through an approach that we call
cognitive emulation or Co which is kind
of about emulating humanlike reasoning
we can talk about that a bit later um
but I also uh Moonlight with a a
nonprofit advocacy group called control
AI which are working on the social and
political problems of this Ai and AGI
transition how do we as a civilization
deal with this that this technology is
being developed is it a good idea that
such powerful technology is being
developed without any supervision out of
control by you know private companies
maximizing for shareholder returns seems
like a very dangerous way to be
developing this technology if this is a
dangerous way to develop technology what
is a better way to develop this
technology and how can we get there
these are very very hard questions these
are very hard political questions
they're very hard social questions I
don't have I don't have answers to
everything but I have a I have a few you
know a few pointers at
least excellent talk about some of
the the people who don't feel um as
concerned as conjecture and the AI
alignment Community what are they doing
I think there's kind of like two main
camps I would or not not camps two main
objections to this um idea that AI will
be a large risk one is AI is an thing
like or it won't happen soon it's far
either it's far away it'll take a long
time or it's impossible for some reason
you know it's impossible to make AI
smarter than human for example or it
will take a thousand years um the other
position is um it is possible to create
Ai and we will probably do it soon and
it will be fine don't worry about it for
various reasons so there's a there's a
couple different ways how people justify
these various things so for the first
group of people
I think I would just kind of like point
to the progress around us and that even
you know you can talk to your computer
now you couldn't talked to your computer
three years ago now you can and this is
crazy and it keeps getting better and
there's a clear path forward and we have
you know experts around the world now
unambiguously saying no it's coming soon
and it's a big problem so I think there
it becomes an object level technical
disagreement I don't think it's a very
interesting argument to be having I
think if we agree that there's even a
10% 20% 50% chance whatever of AGI
emerging in the next 10 years or
whatever it would absolutely be worth us
taking seriously like imagine we had a
10% chance an asteroid will hit Earth in
10 years and kill everybody obviously
everyone would be spending massive
amounts of time working on this problem
so I don't think this is the in the more
the interesting objection it is a
worthwhile objection and it's worth
discussing on an object level but the
other objection I find a bit more
interesting um which is this objection
that well maybe if we just build AI
it'll just be nice you know maybe it
will be good it will just help us we'll
keep it under control maybe controlling
it is easy um Etc and I think there is
kind of a saying where like you have to
be educated to believe something so
stupid um where there is a core
intuition that most people have when you
first introduce them to the concept of
AI and AGI where we say hey um some
people some technologists are Building
Systems that they think will be smarter
and more powerful than humanity and they
don't really know how to control them
how do you feel about this and the
default reaction is what the hell like
of course this is going to go poorly
like have you literally never seen a
movie before have you never thought
about this for 10 minutes like what of
course this is going to go poorly and
now the reason people form this
intuition is often because of movies
which or whatever which is not a good
source of truth but that doesn't mean
the intuition is wrong there's also very
good ways where this intuition comes
from such as you know look at colonizers
from previous centuries when a group of
more technology Superior moved into a
land of a less technology um
sophisticated peoples it often ended
very very poorly for the less
technologically sophisticated peoples um
to Great Horror often or even further
think about humans and animal there's a
reason that chimps don't run the world
you know chimps may be stronger than us
physically but that doesn't matter we
have guns and Society we decide what's
up if we want to build a hydroelectric
Dam and there's an antill in the valley
well too bad for the ants it's not
because we hate the ants not because
we're malevolent because we're evil we
want to you know hurt the ants they're
just in the way and I think this is what
will happen with AI too some people
think that what I'm trying to say is
that AI will be evil it'll be terminated
with glowing red eyes I don't think
that's what's going to happen at all I
think it will be mundane and it just
won't care I think it will understand
that humans don't want to be killed but
it won't care it will be optimizing for
something who knows you know more power
more profit whatever and in the process
of doing so humans will be in the way
they're annoying they're confusing and
eventually you'll want to replace them
with robots or solar cells or whatever
and to do that you know why would you
keep the humans alive to put it in a
more economic term you can also think of
it as the marginal contribution of
humanity to the economy will shrink as
more and more labor gets automated by
machines and including intellectual
labor until the marginal contribution of
humanity Falls to zero and then becomes
negative meaning that someone has to pay
an economic cost to keep human alive you
know they need space they need food Etc
and who will pay that cost and by
default an AI system will won't pay that
cost why would it it's in because we
don't know how to make them want to pay
this cost and not be out competed by
other systems that do not want to pay
this
cost very good you have a very
interesting way of talking about
intelligence and goals can you elaborate
on this for me intelligence and goals
are kind of to separate things they're
related I like to think of it as I kind
of like to think about minds as kind of
like split into four things this is just
my personal way of viewing it I'm not
saying this is the perfect way to view
it or whatever I just find a useful
model you can think of a mind as being
composed of four parts the first part is
the world model this is what the system
knows about its World about the
environment it's what it also knows
about itself and what it knows about
just its knowledge in general the second
thing is epistemology this is how does
it generate new knowledge what systems
what logic like how does it take in new
information and derive updates to its
World model from this the third thing is
his decision Theory which is given the
scenario it is currently in and the
things it wants to accomplish how does
it make decisions about what to do next
and the fourth thing is its um values or
its Aesthetics these are things that the
system wants and it doesn't want them
for any particular reason it's just the
things it
likes so it's many goals many things
that humans do are not this well
factored our brain is super messy it
doesn't have four compartments all this
is mixed up for example receiving money
generally feels nice if I give you a lot
of money it feels good it feels like is
and a lot of people put as their goal I
want to get money this is a goal that in
a sense of value that maybe lots of
people have but I think a lot of us will
agree that really the reason we're
getting money is not for the money it's
to spend it it's you want to spend it
that we can spend time with our family
we can go on a cool vacation we can buy
a toy that we want or good food or
whatever so for me values or go goals or
in the technical lingal terminal goals
are things you spend money on because
you want to spend money on them not
because they help you for other goals
it's a thing you ultimately want so it
might be as I say you know family
pleasure
whatever then separately from this is
intelligence and from intelligence for
me is kind of like your general
competence so this is both how much is
in your world model like how many things
do you know that are useful how accurate
is your world model it's how good is
your epistemology how good are you
learning new things how good are you in
integrating new information with your
current world model in a sensible way
and how good is your decision Theory how
good are you about making decisions how
good are you winning at games so
ultimately a very intelligent system can
want anything you could make system that
is very dumb and wants very high level
things you know that wants something
complicated or something simple the same
way you can make something that is
extremely smart it's extremely competent
but it only wants to collect shiny rocks
or whatever now of course Evolution
would never produce such a mind it would
never produce a super intelligent that
wants rocks but you could code it in a
computer if you wanted to not saying you
should but it's possible so similarly um
if we build AI systems and we don't
understand how to control them and how
to control what goals they have by
default they won't have a cleanly
separated goal it will be a mess they
have
some thing somewhere where they like
they kind of want to do something and a
lot of these goals will be contingent
and they will be about getting resources
gaining money gaining power and the
reason is is because those things are
useful most people like money not
because we all have a gene you know in
our genome that tells us to like money
but because we learn that oh wow if I
have a lot of money then I can do many
things so therefore it makes sense for
me to get more money I think a similar
thing will happen with AI systems where
if we train them to do lots of different
things they'll notice huh it's really
helpful if I get money so I should get
more of that or it's really helpful if I
can deceive humans and trick them
because then they'll do a buch of stuff
for me so I should get better at that so
this is kind of how I think about
it yeah very interesting so you're kind
of referring to a type of evolution then
in that sense no if a if a machine gains
intelligence then they will in some
sense learn and evolve
to toward their specific goals or even
if they don't have goals right very
interesting the the uh do you have a
favorite approach to to physics and
artificial intelligence or to Quantum
Mechanics what are your what are your
thoughts about connecting the two is
there a relationship between the
two I think they're kind of a different
levels of extraction I think that
intelligence is a very high level
concept it's not something that exists
in the physics it's something that
exists at a much higher level the same
way that you know we might say the
United States of America is a useful
concept for us to talk about but it
doesn't exist physically it doesn't have
appc there's
no atoms that compose United States of
America it's an abstraction the same way
like software is an abstraction like
where is the Google search search engine
it's no specific location it's data it's
information distribute information I
think of intelligence kind of in a
similar way it's software that can run
on many types of Hardware so there are
interesting questions such as for
example what are efficient Hardware
Designs this an interesting question and
it might there might be interesting
questions on epistemology and quantum
physics physics for example like how
should you reason about the world given
that you know you're in a Quantum
physical world how should this change
your epistemology but I think a lot of
this stuff is contingent stuff so I have
takes on Quantum and physics as any you
know um any any Twitter skitso worth
their salt has of course but I think and
I think there's a lot of things to learn
but I'm more interested on the AI side
and the kind of a level above that like
the epistemology of like how do you
generate physical theories I think is
very is much more interesting to me
personally than like is this
interpretation of quantum phys or this
one I'm much more interested how do you
answer these questions how would an
intelligent mind approach this question
if that makes sense yeah that makes a
lot of sense do you think that I always
argue the following that amongst the
different interpretations in science as
a whole and in quantum physics and
particular that I would say that that
theory or that interpretation that most
leads or or most effectively leads
to uh long-term human survival should by
default to be the preferable
interpretation so if a person's
interpretation does has nothing to say
about that about human survival then
it's not as meaningful as one that can
be connected of course in meaningful and
scientific way following the scientific
rules so I ask about that because we
take a we take a in terms of control
your your concerns about controlling Ai
and about the physics of um these
concerns that that both you and I
share our approach is that this the
goals that are necessarily uh at the
fundamental physical level built into
all of matter
including U
AI one of them is the principle of least
action and the principle of least action
is of course one of the most
fundamental principles in all of physics
it just permeates almost every aspect of
physics
not accidentally I would say Juan Malena
who is
arguably um one of the most important
physicists alive according to another
incredibly important physicist uh at
Stanford Leonard
susin ma the Centum refers to the
principle of least action as the
principle of maximal
life so he says and he demonstrates and
he argues that particles at the
subatomic level down to the Quark level
or whatever move in such a way as to
maximize their experience of proper time
or their experience of
existence so this is at the F from the
Big Bang on it's been a fundamental
driving principle in all of
evolution so it stands to reason that
this if this is in fact the way nature
works then intelligence at like you said
it's at a higher level but intelligence
is always driving matter in such a way
as to exist for a longer period of
time that would explain at least to some
extent on a
very metaphorical perhaps directly
literal sense the
universal desire of all living things to
remain alive know why humans don't want
to die you know so there's there seems
to be a fundamental phys fundamental
physical reason for this
so the concern about creating an
intelligence that we cannot
control almost necessarily implies like
you said the death of human Humanity
precisely because physical material
moves and evolves in such a way as to
maximize its own experience of of time
so I think this is a a really my opinion
it's a compelling argument in terms of
why not to build something that you
cannot control right so I think this is
I mean this this lays it down in in
terms of the
physics now we get to the control how do
we how do we actually control
and that's of course one of the primary
things you you are working on and if you
really if we really dig deeply into this
into into this question of
control we necessarily bump up against
the idea that all humans have different
models of of reality of of different
opinions about how things work and this
seems to be on an equal footing or
perhaps even more more important footing
than the AI question itself because if
we can't agree about what to do then
that doesn't get us any closer to to
solving the problem so my question to
you my next question to you is how do
you how do you think about you mentioned
earlier the a certain group working on
the social and political aspects how do
you think about those questions
about political Theory social theory and
the fact that we can't agree on
virtually anything how in the world are
we going to agree on something like AI
or
AGI so um this is a great question and
the glib answer is is that like this is
why I am
pessimistic but it's is not true this is
just one of the reasons it's actually
much worse than that um the truth is
that this is a very hard problem and
anyone who tries to sell you something
something else is selling you snake oil
this is anyone who says oh it'll be fine
just don't worry about it or like oh we
just do X and then it'll be
fine is is just not truly engaging with
how hard this problem is the problem
we're solving for here is not a software
problem it's not like oh how do we bu
write this algorithm how do we solve
this math proof the problem that we're
solving for is is like how do we design
and Implement a global you know state
civilization that fulfills our
contradictory values between you know
bilon humans this is an astronomically
difficult problem this is an extremely
difficult problem that doesn't mean it's
impossible but it means it's very
difficult so with that being
said it's very tempting and fashionable
nowadays to talk about how we don't
agree in any anything and it does feel
that way especially if you spend any
time on Twitter but the truth is is that
actually we actually agree on a lot of
things there's actually a shocking
amount of things that people across the
world actually do agree on such as and
you know there's exceptions of course
but like for the most part you
especially you know here in the west you
know is that like you don't do violence
this is a good Norm you know the fact
that we don't you know have violence
both politically or otherwise as we did
100 200 a thousand years ago is really
great we can agree on a lot of things
there we can agree on
the value of various major currencies we
can agree on various laws and social
norms and communication norms and so on
it's possible for you to meet a random
stranger at a restaurant who you've
never met before and you don't have to
feel scared because you know you'll be
safe this is not typical for the history
of mankind and it took a lot of work to
get to this point it took a lot of
Social Work it took a lot of political
work to build systems that allow these
kind of transactions to happen safely
and efficiently and I think the kind of
things we need to do are on this axis
um in a sense the thing is is that it is
in no one's interest for Humanity to be
destroyed by AGI except for a couple
misanthropic crazy people but for the
most part it's not in people's interest
to have you know uncontrollable AGI or
bioweapons or whatever it's not that
people don't want this if people didn't
you know didn't want survive we would be
in a bad spot but I've checked and most
people love their kids they love their
family they love you know they might
have some disagreements with a lot of
people but all things being equal like
an average person if you give them the
chance to press a button that makes
someone's day better or not press the
button they'll press it all things being
equal this is really nice it's nice that
humanity is like this many animals are
not like this and um so this is the case
um why is there a problem so there's a
great there's a one of my I think my
literally favorite like piece like
snippet of fiction ever written I'm G to
butcher it but it's from the prinia
discordia where the car main character
is talking to the goddess and um he says
oh goddess you know things are so
terrible uh there's war and famine
brother fights brother there's chaos
it's you know it's so so awful what do
we do and the goddess says well if
that's what you want you know what's the
problem and he says but no one wants
this and goddess says oh well then
stop
and
so I I wish I could write something this
witty because it's so perfect because
it's true like there's so much going on
that no one wants like no one wants
children to be dying a famine somewhere
no one wants this no one wants people to
be you know victims of violent crime no
one wants you know this stuff all this
is bad and yet it keeps happening and
people keep doing it why and the there's
no simple answer um there is some you
know cute answers you some cutesy
answers you can give such as quoting Mok
which is a name from a famous essay on
this topic which just blames everything
on this entity called molok and this
entity called molok is the entity um of
Nash equilibria which are not Pito
optimal and what this means is is that
they're if you're playing a coordination
game if there's a is if there's a way
where a group wants to work together you
can get into situations where it would
be better for everyone to do something
else but if any individual tries it they
lose they can't do it the old group has
to do it together so you want to get the
whole group to do this thing but if but
any individual has no incentive to do it
this is an extremely toxic situation
this is an extremely bad situation and
this is extremely common in these
political problems often when you have
sticky political problems the reason
they're sticky is usually not because
you there is not a possible better
solution but to get to that possible
better solution someone's going to lose
their job or you know the the
bureaucracy enforce it would have to cut
their budget and they don't want that so
the ones who could cause the problem
would lose from benefiting it from
implementing it or it would be annoying
to implement it so they don't so how do
we deal with these kind of problems
there are many many ways how to address
these kind of problems and most of them
are hard but overall what we need is a
civilization that is more coherent and
capable of coordinating around these
movements so one of the most important
mechanisms around this is common know
knowled so common knowledge is different
from knowledge knowledge is knowing a
like I tell you something now you know
this fact you have knowledge but it and
but it doesn't mean that common
knowledge exists common knowledge is
when you know that I know that we know
that our group knows that they know that
we know that all of us know Etc the re
and this is extremely important because
this is the unit of
coordination let's say there's three
candidates for presidency a b and c a is
the preferred candidate he is the
incumbent he's probably going to win but
everybody hates him you know he he just
like sucks you know no one actually
likes him there's be who is like almost
just as bad but like you know maybe
slightly worse so he gets a little bit
less votes but he's like also terrible
so you have like you know 6040 and let's
say there's a third candidate and this
third candidate is the best like he's
actually amazing he's extremely good
he's just unpop people hav't heard of
him so now imagine I go to you know my
the 10 voters or whatever and I tell the
them each of them individually and I
convince them that c is this amazing
candidate he's the best candidate and
they should all go vote for C because
he's the best one and they might all be
convinced of this let's say I convince
every single one what happens on polling
day everyone votes for a because B is
even worse we don't want B and
well if I vote for C that's just wasting
my vote and helping B because everyone
else is going to vote for a so I have to
vote for a as well so what is missing
here is that all those 10 people have to
also tell each other that they're going
to vote for C and like oh you're going
to vote for c as well so am I oh wow you
too we're all voting for C well [&nbsp;__&nbsp;]
yeah now we can vote C in if you don't
do this if you skip this step c will
never win even if he's the best
candidate doesn't matter because
everyone will coordinate on a so a big
issue so one of the most important
things to do around the AI problem and
the danger from AI is literally just
making people aware of it and not just
making people aware of it but aware of
it in a common sense that also everyone
now knows about it everyone knows that
you know about it and it's not something
you can ignore it's no longer weird to
bring up it's something that your
government should have an opinion on
it's something that your your you know
your elected official should somewhere
have a statement and if he doesn't you
should ask him and it's normal for you
to ask your friends won't look at you
weird for asking about this question and
once we move these kind of things into
the cultural common knowledge it's a
thing that our civilization can reason
about so that's step one well it's step
two I guess I mean Step Zero Humanity
not wanting to die step one bring the
object into memory so that you can Reas
it about it step two is that you need to
make civilization coherent and competent
enough to actually act there needs to be
an ual plan and there needs to be a way
to in enact this plan step three enact
the plan currently we're still on step
one how do we even get civilization to
think about these problems and then we
have to and then as we're also starting
to tackle this problem of
okay what can civilization do how would
it be enacted this is very hard because
a lot of our institutions are not very
good they're very calcified they're very
slow they're very efficient and this
limits what kind of plans you can um
execute even if Society decides okay
we're going to do something the
complexity of the plan is limited by the
competency of the institutions that are
executing this plan so this is a very
hard problem it can be both improved by
improving the quality of Institutions
and by making clever plans that are
better suited for the system you're
working with you need to do all of this
if you skip any of these steps you lose
so this is very hard and we work on all
of these
problems I agree it's really really hard
to to imagine that kind of scenario
because you're you're running up against
of course uh several academic
disciplines that are already very well
well developed among them social Choice
Theory which which deals with um you
know how Collective decisions are made
and how votes are aggregated and there's
just you know inherently difficult proc
problems and challenges to imagine that
we can somehow get everybody to agree or
mostly agree on even something as basic
as as as long-term human survival look
at the climate change um
issue even scientists can agree on on
this the whether or not climate change
is is real um unanimously rather I think
most people agree that they there is a
problem but um there's still enough
controversy to to generate quite a bit
of of people who feel the opposite um so
it's it's very very challenging
[Music]
um are you optimistic that we will be
able to do something about this before
the the AGI
deadline something yes enough to save us
no I don't expect we'll make it I
quite confident that this is we're not
going to make it out of this Century
alive maybe not this decade um it seems
very unlikely at this point to happen it
is not
impossible um there are there is no
physical law that prevents us from just
doing the right thing there's nothing
that prevents Humanity from coordinating
at this speed there's nothing that
prevents us from solving the alignment
problem there's nothing that prevents us
from you know delaying AGI Creation with
laws or institutions or social norms or
whatever there is no physical reason we
can't make it but there's sure is a hell
of a lot of social pressure going
against it there's a lot of economic
incentives against it there's a lot of
cultural incentives against a lot of
political Malay that are pushing against
it and if I thought I had like 30 years
50 years 100 years time I would be
feeling a lot better about this because
that's a lot of time to make social
change making social change on a you
know 105 one year frame hard but not
impossible and I'm going to try I am
trying and so are other people and I
think it's important that we give it the
best shot we can and it is possible but
it will be very hard it also depends on
how hard the technical problems are so
there's a massive social and political
problem but there's also a technical
problem it's neither one nor the other
it's both it's that we don't know how
hard aligning AI is you know it could
turn out to be anywhere from from
surprisingly easy to almost impossible
or completely impossible and depending
on how hard it is we'll change how what
kind of plans we need if it's literally
impossible which I don't expect but it's
possible you know maybe then we need to
do some very drastic
things if we want to survive into the
long-term future if it's super easy and
not that big of a problem okay well then
we should verify that and if we verify
that then we can go easy awesome like
that would be the best world but we
don't know and most of the evidence that
we have points towards it being pretty
damn hard so it's given that it's hard
we don't have much time there's both a
strong technical problem which very few
people are working on it's like it's
shocking to me how few people seem to
want to work on this problem or at least
have the incentives to work on this
problem including academics like it
seems to me is if I was an academic this
would be the number number one problem I
want to work on but the number of
academics working on the problem of
super intelligent alignment is minuscule
like last I checked it was less than
20000 in the whole world and which is
crazy for a problem that's this hard I
don't think like the fact that it's just
like some guy in London some hacker you
know who put together a small company
and is like among the largest labs in
the world working on this problem is not
a good place to be now I love my
Engineers my scientists think this some
of the most brilliant people in the
world and they give me hope every day
that maybe we can make progress on this
problem but we're in a bad spot if this
is the best we if this is what we have
to rely on it's similar where how I
often talk to journalists and they ask
me questions like is Sam Alman nice like
does he care is Demis a good guy or a
bad guy and my answer to this question
is it shouldn't matter it shouldn't we
shouldn't have a system where whether
the future of AI goes well depends on
whether some CEO is a nice guy or not
this is not a stable system this is not
a good system that we're in so I'm very
pessimistic in the sense of I don't
think we have a high chance of success
um but it's not impossible I do see ways
I've seen I've ironically had a
experience over the last couple years
where timelines have gotten worse things
have progressed faster than I expected
and I already expected very fast
progression um many political and
financial things have gone much worse
than I was expecting but at the same
time I'm seeing more solutions I'm
seeing more plans that could actually
work so it's been a kind of a hug in you
know both directions for me as like up
or down in probability of success but I
think this is very very very hard and if
nothing changes if we just continue at
this pace
the way things are today here in 2024
then we're definitely not going to make
it well you raise a really good point
there that I often tell people I'm lucky
enough to to to interact with some
really bright people around the world
and I often repeat I don't understand
why more people aren't concerned and and
working on this um so it's it's kind of
insane the fact that uh you know if if
our families for example our children
were in a in a position of danger
they're they're on the edge of a cliff
or something they're real danger it's
like completely ignoring it and just
letting them play and Frolic at the edge
of a cliff it's it's difficult to
understand we've been lucky enough to
get some really bright people on board
for our research
program and I suspect that there's a
couple of scenarios that could play out
here number number
one our concerns about large
scale catastrophe or even Extinction
might be might turn out to be false it
might turn out to be
real it might turn out to be so real
that Humanity
ends it seems to me that a more likely
probabilistic scenario is that we're
we're too late to really change anything
what you're trying to work on what I'm
trying to work on what several people
around us are trying to work on is
daunting it's incredibly daunting how
can we possibly get people to understand
and to change their
behavior I see it really difficult so I
think it's too late to really do
anything I think we're going to learn a
very difficult lesson about I'm crossing
my fingers and hoping that not everybody
dies so that there's some people left
over that can look back historically and
say this is where we come from this is
what happened to our previous
systems now we have to create something
different unfortunately in that scenario
even in that
scenario unless something different is
created that next generation is going to
confront sooner or later the same
problem right so that's that's I think
the essence of what we're dealing with
here the problem of how do
we stop ourselves from destroying
ourselves and and going back to the
principle of least action or what mala
calls the principle of Maximum
life I suspect and we've been working on
this concretely with like I said some
top researchers some of them you you
know um that one promising
potential comes through a combination of
physics and social Choice
Theory I suspect you're familiar with um
Glenn W's proposal for quadratic
voting so Glenn W is one of our research
Partners among This research team that
we're talking about and so what we're
trying to do is connect his proposal for
quadratic voting to down to the
fundamental physical level and then
proposing a new way to make Collective
decisions I don't think it's going to
work for this
generation but I think it may be
interesting for the Next Generation or
after the lesson that Humanity learns
and it's the idea is how do we control
AI again this
is theorizing right if
the all material has and
and a propensity to seek out its own
existence for as long as possible the
answer is to build into those physical
systems into our political systems the
opposite some kind
of altruistic algorithm or some kind of
self-sacrificial
algorithm that
controls the impulse to
only I survive rather if we get our
political systems our social
systems um driven by the idea
that I'm not the thing that matters but
rather or I'm not the only thing that
matters but
rather we matter if that's the
fundamental principle without going all
the way obviously to something like
communism or anything like that if
that's a driving principle and we can
get it into the physics and into the
artificial
intelligence that seems to me like a
promising Avenue to
explore does that make sense to you uh
well I'll leave it there does that make
sense to
you I think there's a lot of things that
are worth exploring for sure I think
there's a lot of inspiration to be had
in mechanism design familiar with Wild's
work and you know Len's work on like you
know tumors and like how to syst
coordinate at this of level and so on it
is a general question of coordination
like in a sense you could say that
alignment is very much hierarchical
problem you know it goes down from how
do cells prevent cancer How do multicell
organisms coordinate with each other all
the way up to you know how to uh friends
coordinate how do companies coordinate
how do um governments coordinate how to
governate coordinate between each other
and so on so it's very much a general
abstraction
that we see appearing over and over at
different scales um you know arguably
breaks down at the SW physics level
unless you want to recast you know
Elementary particles as like
coordinating mechanisms which you can
you know you can use energy as a
currency and like you know clear the
market and so on like it's is possible
to cast it this way and maybe it's
productive way to cast it that way um
I'm ambivalent on whether this is
productive Direction I guess the way I
um my usual quip I guess I would say on
this is that to do truly incredible
things like things of this magnitude of
just like incredible proportions you
need to do exactly two things the first
is you do things that compound the
second thing is don't die if you do
those two things you can accomplish
things that seem impossible so I think
the kind of work you talk about is a
thing that can compound improving
mechanism design at scale is something
that can compound if you can do this
over a large scale people that uh you
know potentially growing popul or
growing in terms of epistemology wealth
Etc this is a thing they compound
getting you 0.1% more growth per year is
very powerful if you can do it over 100
years or whatever um then the bottleneck
becomes don't die so if I had 100 h a
thousand H if I thought there would be a
next chance if I thought some people
would survive and they'll get a second
shot I probably wouldn't be working what
I'm working right now maybe I would but
I think there are ways of how you can
solve alignment entirely for super
Intelligence on the first shot but it's
unfathomably hard and it starts with you
know dealing with incredibly fundamental
questions of epistemology and Ma you
know foundations of mathematics and
whatnot and this is not the kind of
thing you do if you think you have a
limited amount of time and a limited
amount of coordination budget around
what to coordinate your
civilization so um I feel this like
um this kind of this feels like
somewhere in between where like
um I think the bottleneck right now on a
surviving is not really Galaxy brain
brilliant scientific stuff it's mostly
boring labor it's talking to politicians
it's um producing good media talking to
the general public digesting things into
words the general public can understand
message testing um and just hacking away
at the
you know control problem in a more praic
way like how can we build AI systems
that are not super intelligent but are
still useful so we still get economic
growth because this is easy to
coordinate on this is part of what we do
with conjecture is we try to build AI
systems which are bounded in the sense
that they're useful they're powerful and
they're reliable and you can use them
reliably and you can ensure that they
don't lead to existential risk or such
problems this is not because this is the
best possible thing we could be doing
it's because it's a good thing to
coordinate around it's much easier to
get a government or a company or someone
to sign up if you say hey this prevents
this you know tail risk and also makes
you a lot of money this is a very easy
useful way to get people to work with
you is if you just promise them to also
make a lot of money and if this is of
course
true so a lot of these academic things I
like that someone is working on them I
like that there are some people in the
world out there who are thinking about
you know physics inspired mechanism
design that are thinking about formally
provable retral causal alignment schemes
you know shout out to my friend kadoo on
that one um and other things I'm say say
more about your
friend she's a she's a researcher
working at a or called orthogonal and
they have one of the most Galaxy brain
[&nbsp;__&nbsp;]
insane idea of how to align AI where
they like build an AI that like goes
that like destroys the universe but then
res simulates it and goes backwards in
time to think about what the researcher
actually wanted and then locates that
formally prove it's insane um obviously
it won't work obviously but it's great
that someone's thinking about this right
like I'm glad someone is thinking about
this I don't think I think honestly you
know with all my love to kadoo and other
people in this category I think too
people are thinking about this stuff
right now I think these people would do
much better if they just literally start
doing politics um I just boots on the
ground I'm not even talking about Galaxy
brain planning just boots on the ground
politics I think right now is higher
leverage but this is a contingent fact
at some point we need to think about the
complicated big stuff as well like it's
one of those great things for like I'm a
big fan of like quadratic voting
prediction markets you know like all
those good Classics you know that like
you know um you're like GMU you know
Associated people love and whatnot love
them but like prediction markets are
just illegal in the US like sure you
could come up with a better proper
scoring role rule that like incentivizes
the correct you know crypto
decentralized actor thing okay but like
they're still illegal in the US and
they're illegal for a really stupid
reason like the reason they're like if
you don't know you should look it up
like the reason they're illegal us is
just stupid it's not even someone passed
the law to outlaw them it's just one
branch of the government decided it fell
under the jurisdiction they shouldn't do
it and no one ever basically challenged
it it it's stupid and like the
bottleneck to doing that is not you have
to come up with an even cleverer Market
design it's you need to actually ly the
government and you need to actually talk
to people and explain things to them and
make things legible this is the thing
that some prediction markets recently
have done a much better job on like
recently there's been you know R Market
suing the government going through the
process and so on and this is all
horrible and terrible and annoying and
it feels like a waste of your time it is
but it has to be done so my claim is
that I think we are
currently not bottlenecked by the smart
stuff I think we are bottlenecked by the
stupid
stuff okay so so take point taken I
think your your your approach is is
interesting and and important and I
agree that many different approaches
should be taken and you know the boots
on the ground lobbying politicians and
whatnot seems like the most directly
doable rational approach but what do you
do about the fact that people don't
agree even even on approaches to AI if
you get if you have line up let's
imagine line up um I don't know a few
people who are doing AI
research and you ask them should we
should we continue with this or should
we
stop and six of them vote uh to continue
and four of them vote to
stop if it's a if it's a um if it's a
democracy then you go ahead and you go
ahead with a with with AI and with
presumably with
AGI isn't there a problem with even if
you go to the politicians if you still
if you just submit this to decision
theory is or social Choice Theory isn't
there still a problem so this is what I
was talking about earlier about Step
Zero you have to want to not die if
people want to die yeah we're just
screwed and I'll go retire to Hawaii and
you know with my family and have a good
time I probably won't I'll probably
still fight but you know um if look if
we did a truly representative Global
poll uh vote on literally the whole
world about whether unelected you know
technocrats in Silicon Valley should be
allowed to build technology they
themselves think and have said could
literally kill everybody and if more
than 50% vote
Yes okay you know what this is already a
much better world than we're in right
now because right now no one's even
being asked there is no common knowledge
there is no discussion It's Just
Happening anyways so if we get into
world and that's the result I'm already
happy this is already so much better
than where we're at right now this does
no if this happened of course I would
immediately start working on you know
hopefully changing people's mind and so
on but still this would be a massively
better world than where we're at right
now and also I just don't think it would
be over 50% I think and there are polls
you know we've done yugov polls in like
the UK and the us around this and no the
overwhelming majority bipartisan say no
this should not be just allowed without
supervision and you know they shouldn't
just be we shouldn't race forward at all
this is bad actually and so if we if we
had a democratic oversight here a
coherent Democratic process with teeth I
think we would be in a much better spot
than we are right now this doesn't solve
all the problems but as I say we need to
do two things things that compound and
not die so
this would give us a lot of not die
points and if we had more not die points
we can spend these on Clever things so
you you ask this question of like okay
but people don't agree on values and
things and whatever and this is a hard
problem I'm like yeah okay it seems like
we need a lot of time to figure that out
I don't think we're going to find this
we're going to figure this out in the
next six months so if we can't figure it
out in the next six months we have to
make sure we don't die in those six
months because if we do then we're never
going to figure it out so is it possible
to develop a better mechanism
or better aggregation or Enlightenment
at scale or whatever the hell to make
people agree more on things or like find
better compromises probably and I expect
those things will take longer than you
know six months and so we have to make
sure that we don't die in the next you
know six months 12 months you know three
years whatever we need more time I think
we can make progress on this if I had 30
years I'd probably be working more on
mechanism design or Global preference
aggregation and stuff I just don't think
we have that long basically so we have
to first work on the Not Die part and if
we get the Not Die Part then we can you
know start thinking about not just start
but then we can like put more of our
efforts thing currently the number of
people who are working on the boots on
the ground just just don't make sure we
don't die over the next couple years is
vanishingly low the amount of lobbying
dollars that goes into lobbying against
the creation of super intelligence by
unaccountable unelected you know
technocrats
is a pittance it's almost nothing
compared to these large behem you know
how much Microsoft pays for lobbying
like how how do you think like groups
like you know me are supposed to compete
here I mean we can and we do and the
main thing we compete on is that we're
right and people don't want them to do
this it's very expensive to convince
people of things they don't want it's
quite cheap to convince people of things
that they do want and so we have an up
we have a chance there but like it is
hard and it's not is not easy and we
would benefit I wish there was I wish
there was a hundred organizations of
control AI I wish there were 100
conjectures of small medium you know
even large players pushing against this
kind of stuff even just by making noise
even if it's just by you know talking
about it a bunch of social media if it's
by writing letters to your your
Congressman like whatever like there's
just not that much and I'm not saying
there's a massive amount in mechanism
design either also underfunded terribly
underfunded there's a lot of work to be
done there um but in my personal View at
this point in time I think um we would
benefit more on the long term if we
could all coordinate to temporarily just
solve the concrete problem to buy us
more time and then we can redistribute
on longer timelines in my
opinion yeah very good the one the
one contradiction I I perceive or the
tension maybe I as a better word is the
idea that you're very clear about what
you what you're trying to achieve yet at
the same time you say that it's it's not
possible that you're pessimistic about
it so help me understand that different
uh those different ideas not possible
and almost not possible or very
different there um I I I forget how the
quote from Princess Bride goes but you
know he's not dead he's almost dead
which is very different um Humanity
isn't dead it's almost dead that's very
different um the way I kind of see
things personally so I'm just speaking
for myself here um is that even if I
thought Humanity was dead even if I
thought there was no chance I still
would try you know like I don't know how
to save the world but damn it I'm going
to try it's kind of just my nature like
I've always just kind of thought this
way the way I kind of think about this
is and this is a question I get a lot is
like Conor aren't you depressed like
isn't this scary aren't you sad like you
you think we're all going to die um
doesn't just make you upset doesn't make
you sad truth is like not anymore like
I've kind of grieved I'm kind of over it
and the way I see things now is that
look
um we're facing unbelievable hardships
unbelievably hard massive problems
there's huge monsters Behemoth villains
that are standing in our way um often to
their own detriment um there is
incredibly difficult technical social
political problems with overwhelming
odds against you and that's true but you
know I get to work on the most exciting
problems in the world I get to try to
make the world a better place with all
my might I get to work with some of the
most brilliant wonderful people I could
ever ask for you know I get to spend
time with my family eat good food even
if we don't make it even if it goes
poorly even if we you know don't make it
out then it's a life worth living and I
have no regrets that's how I live my
life I'm happy I tried you know if we
don't make it and we die I want to be
able to look God in the eye and say I
tried well I applaud your effort
definitely I applaud it um I in fact I
wish I would have come across your your
work um a few months
earlier prior to
um writing this Grant
application have you have you worked on
any kind of
simulation um using llms just out of
curiosity because the application that
we that we
submitted involves using llms to
simulate different
scenarios in or precisely in order to
test various voting algorithms and and
different social
situations to convince people that
something needs to be done have you have
you experimented with any kind of sort
of simulations and if so please tell us
about it a long time ago I experimented
with this this is back in like the gbt2
days um or gpt3 days is i experimented
with these ideas these were some of the
first ideas that came to my mind as well
I was wondering how much you could use
this to for example create artificial uh
like opinion or cultural polls or to uh
stress test or role play through
negotiation scenarios or like message
test uh for cheap um I back band it
didn't work very well models are not
good enough I think modern models are a
lot better at this I think they will
have predictable um bioses and failure
modes here obviously does not you know
replace human interaction but it's
obviously a powerful tool and compared
to how cheap they are compared to the
data you can get they're obviously
extremely useful I think there's a lot
of low hanging fruit here where you can
do a lot of like digital social science
as long as you remember it's not
actually social science but it is proxy
it's a proxy metric it's a noisy proxy
but it is a proxy I think there's a lot
of massive amount of alpha that can be
gained there where just if you iterate
super super quickly like I think the
bottleneck there is good software
engineering and like good iteration just
like doing LS of experiments very
quickly that's the thing you like to do
with humans right like you run a poll
you can have like 10 questions like you
know 2,000 people and it costs you like
a lot of money to do this lot a lot yeah
exactly exactly so if you could do the
same thing but your questions are like
more noisy or less good but you can do
it on two million samples like that
should change how you think about doing
science here and then like what you
verify like the right way I think to
design this kind of thing is like you do
massive amounts of digital stuff to on
the right questions to be asking and
then you point verify these things in
the real world or something like this
I'm not a social scientist I'm not stian
so I can't say I know the exact white
way to design these kind of things but I
do think there's a lot of possibilities
there that I'm sure some people since
then have tried and I have not read your
papers sorry there's too many papers to
read um so I'm sure there's other people
who have tried this kind of stuff in a
more sophisticated way than I have um I
of course as I say personally do not
feel bottlenecked by this kind of work
but if I you know had access to
extremely good Tools around this and was
low cost to like message test at scale
with llms yeah I think I think that
could be helpful yeah fantastic that's
good to know that's that's very
promising yeah we as a group obviously
feel very very optimistic about about
the possibilities here in fact we're
we're if you're interested we'd love to
include you as in in in the in the
efforts we've just uh started another
another
group of sorts uh that includes uh wool
from
research basically aiming toward the
same thing of using llms to to to
simulate agent-based models Etc using
some really interesting stuff from
Michael Levan's work on you know
hierarchical levels of intelligence what
he calls he calls it the multiscale
competency architecture and then we have
I mentioned earlier um physics approach
we
have I I don't get time tired of
promoting this approach to physics which
is the the two-state vector formalism
which which posits a two boundary
conditions to understand the nature of
of well some of the Mysteries that we
experience experience on an experimental
level in physics and this goal oriented
approach from the quantum level through
you know lean's work through friston's
work all the way up to
democracy we never know if we have three
years maybe our paths have some
promising potential because um if we can
simulate things and convince governments
through actual simulations and through
very rigorous science and and and
rigorous physical and mathematical
arguments I think it could add to the
boots on the ground I think the boots on
the ground is is obviously really
important as well so like I said I wish
we I wish I would have come across your
your work a little bit
sooner precisely because of that you
know the LM approach to to agent based
modeling yeah fantastic I think it's
definitely interesting and um I don't
want to dismiss like that you know very
good simulation technical arguments and
so on can I value but at least you know
also to share with the audience because
I think this is interesting potentially
for a lot of the audience I've talked to
many many politicians now like I'm a
tech guy at Heart Right like every you
just say I'm like oo that's so that
sounds so fun like I want to work on
that that sounds so cool um you know
it's like exciting it's fun you know you
have you have Lon you have friston you
have Wolfram you got the whole crew like
you know you got the you got some of the
funnest people in science you know
together absolutely absolutely
absolutely yeah yeah so I I I totally
get it right totally I love this stuff
I'm you know I think this stuff is super
cool and I was almost disappointed but
it's good now that I'm calibrated on
this it's just like
man it is good logical argumentation and
scientific data just not the bottleneck
on convincing people like this is just
truly truly not the bottleneck um I wish
it was the bottleneck I wish we were at
a point where everyone was so rational
every our institutions were so efficient
our group epistemology was so good that
the thing that was holding back say
climate change discourse is more you
know ice core readings if that was the
thing holding us back I would be like
wow we're in a good spot but the truth
is the things holding us back are having
nice dinners with politicians and making
them feel safe and making them feel like
you're trying to help them and
explaining things to them in their
language it's so often that I've spoken
to politicians and these are high
ranking politicians mind you these are
like top UK people and they're these are
not stupid people and they're they're
just like and I you know spend an hour
or two with them and they're just like
wow no one has explained this to me
before and I'm like damn like if this
hasn't been explained to this guy who's
like a very high ranking member of
government in a very major country then
no one has like no one knows this like
no one is being told this and these are
not high level these are not complicated
explanations right these are very simple
just here's a basic facts about AI
here's how AI works like like most
people don't know that like AI isn't
coded like most people don't know that
AI is trained and that we don't know how
the internalist almost everyone I talk
to does not know this this is a very
very basic fact and just telling this to
literally every politician is already
extremely high value and the bottleneck
to getting to these politicians and
other leaders of various kinds is not
that our argument isn't good enough
they'll believe you you can give your
argument like huh yeah that makes sense
and they might double check it with
their science advisor or whatever but
the science advis will agree with you
he'll be yeah we don't know how how
they're written um it's that this takes
is that you know here in the UK we have
what like 300 to 500 you know major
politicians that you need to talk to
this is before we talk about like
cabinets ministers you know deep State
stuff US government is much bigger than
that still you know there is like you
know there is I don't know 190 countries
in the world you multiply all these
numbers together and you get a lot of
people you need to talk to and even if
every conversation is not difficult and
could be done by you know you meet
anyone really each of them takes an hour
or two hours and it takes you like you
know takes you a while to get an
introduction and to convince them that
it's worth talking to you and whatever
that adds up to just a lot of man hours
that's a lot of hours you have to spend
and it's not glamorous cool work it's
just things that need to be done I would
like us to get to the point where we've
done all the stupid things where we've
like I'm kind of repeating myself at
this point I'm sorry um it's like I am I
I I want to get to the point where we've
done all the stupid things we've talked
to literally everyone we've made all the
dumb arguments we've got all of this you
know done and now we're bottleneck
because our science isn't good enough or
we don't have a convincing enough you
know thing we get to that point I'm like
awesome that's a really good world to be
in and so there's a little bit of this
where like for convincing technical
people this kind of stuff is very
valuable so the stuff you're talking
about for like professors intellectuals
for this kind of stuff for that it is
very useful but I found it shockingly
unhelpful anywhere
else no that makes sense that makes
sense in fact our strategy is precisely
trying to address this difficulty and we
think that quadratic voting amongst
other approaches could be a promising
Way Forward for example if you you're
assuming for example that
that the the voting mechanism is
democracy one person one vote but if you
if you change that assumption then
voting Theory becomes a lot more
interesting
because if we
allow um the Connor Lees of the world to
have more say in whether or not we
proceed with artificial general
intelligence
then the better decisions are made
so that's precisely what we were trying
to get but I I agree with your point I'm
sorry go ahead yeah I was gonna say I I
love this like I think CR voting is a
great idea as is like R preference
voting like all these Superior systems I
think all these things are really good
ideas I really really do and I think if
we can get this implemented it would be
fantastic I expect the thing that is
preventing these things from being
implemented is just mostly social it's
gaining reputation talking abolutely so
someone has to do that and I would
Advocate to to our lovely audience of
talented smart handsome individuals to
potentially considering that this work
is extremely important and could use
your talents I agree I agree absolutely
I double that that no motion um let's
spend the last couple of minutes um
talking about conjecture what are you
doing at conjecture what's taking your
time tell us about your team tell us
anything that that that U you think
would be valuable for the audience to
know and and for myself to know
yeah so at conjecture we we kind of grew
out of my previous project um which was
I was the founder of a lutheri which is
a large open source ml research
Community I don't do open source anymore
because unfortunately I feel timelines
were much shorter than I expected and
open sourcing AI models can become very
dangerous very quickly so I don't really
do that anymore
um I so basically and so but I met a lot
of great people some of the best
Engineers you know
you can imagine and we all crowded into
a you know oxygen deprived little Wei
work in London about two years ago now
and so since then we've been working on
the problem of how do you make AI
systems controllable this is a technical
problem and so we have been thrashing
our our heads at this problem for you
know two years stra now we've tried many
many different things we've gone from
many different directions we've taken
dismissed many things we've built you
know state-of-the-art large language
models in house like way before like
llama existed we already had like really
really like llama level models
internally and so on we've done a lot of
interpretability research we've done
stuff like simulators we've tried many
things and after a lot of things of
iterating and finding things we now have
something that we're really actually
optimistic about for the first time and
it's it's kind of a weird like I feel I
feel like I still have an emotionally
even come to terms with like oh wait
this might actually work and good yeah
for the first time life I'm I'm like I I
still myself am like like scared to be
too optimistic about it but like I'm
like it's actually looking like it's
going to work and so the approach is
what we call cognitive emulation or Co
and the idea is is that it's a vision
for how to build AI systems still using
llms but instead of the usual llm way of
just like training in a bunch of [&nbsp;__&nbsp;]
and then you know running it on
whatever we think far more principles
about the problem we think far more
principle about cognition and we break
down cognition into more Elementary
blocks into more Elementary pieces and
steps that a human would use when
solving a given problem it's like when
you approach a problem what are the
steps that you as a human actually use
and what we do is we build models and we
train models we developed some really
cool algorithms in house um I
unfortunately can't talk about we're
quite you know
mum bit strict about how we accomplish
some of the things that we accomplish we
developed some methods internally um to
be able to train on very very small
amounts of data for very very high
efficiency um and very very high
reliability so the idea here is is you
build systems that can do fundamental
pieces with perfect accuracy and perfect
reliability and then from this you can
then bootstrap and comp compose into to
larger and larger and more complex
things while still understanding every
step of the way so the idea is that you
build systems I say systems not models
because you know of course there are LMS
at at the heart but it's more than that
it's also about having you know you
modify the sampling algorithms you
modify the um you know there's
scaffolding there's memory there's a
bunch of stuff it's it's a stack of
techniques It ultimately get you to
systems that are extremely
reliable uh it might be less good a less
good poet than G4 but if you tell it to
do 10 things in order it does those 10
things in order every single time and
this is what we're going to pushing fors
is this reliability this actually
understanding of like what is it doing
step byep and how can you compose these
things and it's reliability is what
unlocks this is that the problem with
current large language models is that
they are not reliable and if you tell it
to you tell even gp4 which is a
fantastically smart mod model to do like
five non-trivial things it will like 50%
of the time get distracted by number
three and like Veer off or whatever and
it's just not predictable in what ways
it will Veer off you know if you could
predict ahead of time it will Veer off
if I give it this prompt that's also
fine but it's impr predictable and so
with car formulation systems it becomes
much more predictable you get to
actually the system itself can detect
when it won't know how to do things or
when things will not go as planned
and this is what we've been working on
now for quite a bit and a lot of it's
now starting to work where we can get
systems that can do you know fundamental
you know basic tasks with perfect
reliability and um so you know truly
like you know up to measurement error
perfect and you can then compose these
things into larger more more complex
tasks so we're already starting to work
with you know first some first like
Enterprise customers um and design
Partners here because kind of like one
of the nice so one of the reason coem is
nice is because it's a good coordination
mechanism it's a good coordination
shelling point because it is I like to
say it's the kind of AI that companies
want to buy you know it's both addresses
the control and the safety problem well
the other hand also this is what most
companies want most companies want as
Ani thing where you tell it to do
something and it does that thing and
nothing more and it's boring and it does
exactly what you tell it to do they
don't want a quirky brain in a jar with
a personality disorder which which you
know we're seeing the resurch of Sydney
on Twitter right now um so this is what
we build and we're
now open to some of our very first
Enterprise you know design customers if
you know anyone in the audience has a
you know large um you know company who
has very high safety requirements very
high Rems um for reliability where maybe
LMS just don't cut it and they want to
try something new and something
hopefully revolutionary
my emails are open fantastic well
commercially obviously that has a lot of
value a lot of value so I'm I'm
optimistic about you having a lot of
success early on in terms of um control
and AGI however if
you if you predict obviously
predictability is better than not having
predictability but how do
you you still don't address the problem
I think unless I'm wrong you still don't
address a problem if somebody wants to
or sets out to I don't know start World
War I yes this is a fantastic U point
and it is exactly correct so that's why
I say we work on controllability not an
alignment so if you build a coem system
and so so coem as a technology scales to
AGI if we do this correctly it is as
smart as a human it can do anything a
human can do and if you have a system
that can do everything a human that can
do including science
you have something incredibly powerful
and something incredibly
dangerous Co moves us from the regime of
we are definitely [&nbsp;__&nbsp;] to it is
possible to not be [&nbsp;__&nbsp;] if this
technology is misused by say okay you
know terrorists hostile governments
something yeah them we screwed and so
this is why I say this is a compromised
technology is an incredibly powerful
technology that would allow you to have
you know digital reliable human
reasoning that you can scale imagine you
could have scientists that follow proper
scientific method in epistemology about
any topic any time of day without ever
getting distracted in a way that is
checkable auditable understandable and
you can rely on the results if this
works if you do this this is extremely
powerful it will allow you to develop
extremely powerful technology it will
allow you to do incredible economic
activity and so on um but it also means
of course that you could use it for very
nefarious purposes
if we get into a world where this
technology has reached that level and
the world hasn't already ended for other
reasons we're in an unstable equilibrium
and this applies to any AGI complete
technology I think coem is a compromise
in an ideal world I wouldn't work on
technology at all that is Agi complete
and we would work on you know pure
alignment Tech or pure coordination Tech
we don't have the time for that and it's
hard to coordinate around that no one's
going to pay you for that um the
unfortunate truth is that doing research
is Extreme exem expensive doing lobbying
is extremely expensive someone has to
pay for that the money has to come from
somewhere so it's a compromise it is co
is the least bad the the worst option
that will work it is there are much
better work options but this is the this
is the you know this is the one that
will work this is the least bad that
will work I guess is that it is much
better than the current other approaches
the other approaches of just scaling up
balls or whatever just kill you once
it's smart enough you know it's just and
that's just it this allows you to not
die
but you also have to solve hard
coordination problems of not letting
this technology be abused not letting it
proliferate if you make this open source
you're
doomed fantastic okay how can people
reach you or follow you you can follow
me on Twitter at NP collapse you can
also find me on various random podcasts
also my company website conjecture dodev
or our nonprofit um or
control.com
fantastic Connor it's been an absolute
honor to talk to you you are um a
brilliant mind doing some doing some
really important work and and my
heartfelt thanks for for all of your
effort and and what you do thanks so
much thank you so much for having
me

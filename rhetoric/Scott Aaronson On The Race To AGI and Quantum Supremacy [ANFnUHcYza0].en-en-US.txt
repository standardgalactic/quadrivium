You just finished up two years at OpenAI working on the theoretical foundations of AI safety. Did you solve it and how come you finished working there? A lot of my former co-workers have now left. The super alignment team that I was part of no longer exists. It has been dissolved. Sam Altman has restructured OpenAI to be a fully for-profit company. How do you feel about that? Well, um... You've worked in quantum computing for over 20 years. You actually helped in 2019, Google's achievement of quantum supremacy. What they did understand was that China was doing something in quantum and that the US had to beat China in the race for quantum. If only one person had a scalable quantum computer and no one else did, then that person could get very rich mining Bitcoin. I'd love to get your status report on the state of academia. It has been a stressful time for academia. You know, we've seen universities basically taken over. Some people will say, you know, you are not allowed to mention anything about existential risk. That is all science fiction and that is all distracting us from the real harms of AI, which are all about... What differentiates humans from machines fundamentally? One way or the other, we're going to find out. All right, Scott, welcome to the Win-Win podcast. Thank you. It's great to be here. Yeah. To start with, you just finished up two years at OpenAI. Yes. Working on the theoretical foundations of AI safety. So I guess the first question, at least the most obvious one, it feels to me, is, did you solve it and how come you finished working there now? Well, they invited me there. Well, they invited me there in 2022 for a one-year leave, you know, sabbatical sort of, right? You know, and I have a day job, right? I'm a computer science professor at UT Austin and, you know, I was skeptical when they came to me because, you know, I've spent most of my career doing quantum computing, right? I did study AI in grad school, but, you know, that was back in like 2000, right? Like the Stone Age compared to where AI is now. And so I was skeptical that I would have much to do, you know, and also OpenAI is in San Francisco, you know, my family is in Austin. But they said, no, you know, we want a theorist to think about, you know, how to help make AI safe. We think it involves computational complexity, which I do know something about. And you can do it mostly remotely from Austin, you know, just stay with your family and your research group. So they made it impossible to say no, basically. And that this was Ilya Sutskovar and Jan Laiki mostly who brought me in. So it was supposed to be one year. I then extended it for a second year. And it so happens that, you know, during the two years I was there, I mean, first of all, they were, you know, what a historic time for AI it was, right? I mean, I feel privileged even just to have been a fly on the wall, right? Even just to have witnessed, you know, from the inside some of the things that happened in the last two years. But it was also a period, as most people know, of enormous upheaval within OpenAI. So you've also you've extended after one year prior to all of the drama, starting with all of the super alignment teams still existing at the time, right? Yeah. Was is that the reason why you didn't extend a second time as well? No, it was never supposed to be more than two years. I mean, when you're an academic, you know, there's sort of a limit of two years that you can go on leave without negotiating some kind of special deal. And, you know, and the truth is, I wanted to get back to teaching. I mean, I run the quantum information center at UT Austin. And if I want this to be a great place for quantum information, then at some point I have to, you know, show up and teach and so forth. So, you know, two years was actually longer than I had planned. So, I mean, I'd love to get more into some of the cultural dynamics going on behind the scenes a bit later on, though. Beforehand, like, I'd love to understand a bit more what the type of problems that you were actually trying to tackle. Yeah. Like, what are the frameworks you're looking at? Because as you say, you were approaching it from a theoretical standpoint. So, yeah, if you could sort of talk us through as best you can in semi-layman terms, like the types of approaches you were thinking about. Yeah, so I should say at the outset, I did not succeed at reducing, you know, the problem of aligning AI with human values to a math problem. I'm not sure that it can be reduced. And, you know, it was very funny for me because, you know, for a year I would talk every week to Ilya Sutskova about, you know, my progress on different AI safety problems, such as watermarking the outputs of language models, you know, which is maybe the most concrete thing I was able to make progress on. And Ilya would say, okay, yeah, that's great, Scott, you should keep working on that. But what I really want to know is what is the mathematical definition of what it means for the AI to love humanity? And I'd say, yeah, yeah, I'm still thinking about that one. Yeah, not a lot of progress to report there. I mean, you know, it's like, in some sense, the alignment people are asking questions that include, you know, 3,000 years of moral philosophy or what has traditionally been called moral philosophy and, you know, questions about what kind of world do we want? What kind of future do we want? You know, social, political questions, they're sort of all wrapped up in this package. And so I feel like the most that theoretical computer science can do is pick off little bits and pieces of it. Did you have other people in the super alignment team work with you on those questions as well? To some extent, yes. So the most concrete thing, you know, that I sort of very quickly noticed, this was in the summer of 2022. So this was before chat GPT was even released. But, you know, I, of course, been playing around with GPT, including with GPT-4, which existed internally at that time. And it occurred to me at some point, like, oh, my God, every student in the world is going to want to use this to do their homework, aren't they? And, you know, every peddler of spam and misinformation is going to want to use this thing. Wouldn't it be great if we could make it easier to identify, you know, what came from GPT and what didn't, right? And now we have a much more well-defined problem, right? We're not talking about, you know, what does it mean for AI to love humanity or to have our best interests at heart. We're just asking, can you tell what came from this language model and what didn't come from it, right? And so now I feel like it's like the tools of computer science have more traction, right? And this is a recurring theme. It's like if, you know, there are these enormous questions, but again and again, the only way to make progress is to look at what's right in front of you, right? And then hopefully you learn something that way. So I started thinking about this attribution problem, and we quickly realized that, yes, you know, you could treat it as just yet another AI problem. You could train a neural net to distinguish human text from AI text as well as it can. And indeed, there are companies right now that are doing this. One of them is called GPT-0. It was started by a then undergrad at Princeton named Edward Tien, and there are now other companies doing the same thing. But, you know, these companies are, you know, they have to continually run a race because the models keep getting better, right? And as they get better, they get harder to distinguish from human. Do they distinguish human versus any LLM or do they have some pattern that specific LLMs have embedded within them? Well, you can do either. So, okay, so right now we're just talking about distinguishing, you know, human from LLM, right? And you can, if you put all the LLMs into your training data, then you can hope to distinguish human from arbitrary LLM, right? And that could be important because, you know, lots of people are using open source LLMs, like Llama, for example, where you're never going to have control over those in order to force them to embed a signal into them, right? And so sort of, you know, it's going to be important to have detection tools that can work even for those LLMs. Okay, but what occurred to me was that, you know, if you do control the LLM, so let's say you are OpenAI or you are DeepMind or you're Anthropic and you want people to be able to, you know, distinguish what came from your LLM and what didn't, you know, like, for example, teachers to see if their students cheated or journalists to see, you know, is this bot-generated misinformation or not? Then you could do what we call statistical watermarking, right? So you could slightly modify the way that your LLM works in a way that a normal user wouldn't notice at all, right? It, you know, looks the same to them. But in a subtle way, you are embedding a statistical signal into the choice of words or tokens that are being generated that if you know exactly what to look for, you can later pick up that signal. Aren't the companies, though, presumably the companies would be quite disincentivized to actually... Tell me about it. Because I can feel from them, because if, you know, let's say OpenAI go and implement this and then they're the only ones who can basically be attributed. Let's say there's a piece of misinformation, and I mean, people could use any LLM, but because none of the other companies have incorporated watermarking, but they have, whenever it does happen to be generated by ChatGPT, the media are going to leap on that and there's going to be all these headlines, ChatGPT used for this. So it seems like the incentives are massively... Unless, again, unless they're all somehow coordinate to use it simultaneously. It's a classic, you know, moloch trap problem, right? Yes, so welcome to what happened. Okay. I mean, I spent a couple of weeks sort of working out the mathematical theory of, you know, how to embed a watermark that sort of would maximize how much signal you get per token, you know, how many tokens you have, how much entropy there is in each token and so forth. And there actually were interesting mathematical questions there. So, you know, I felt like I felt good about that, that I could do something, right? And then, you know, not long afterwards, other people either sort of rediscovered, you know, similar things to what I had done or they built on what I had done. So, you know, it became a known thing in academic AI research. But then, you know, the remaining two years, a lot of it was just an unsuccessful attempt to get this deployed and sort of, you know, no one ever said no, like we, you know, we'll definitely never do this. But it just sort of got pushed indefinitely into the future. And, you know, I think the main issues that came up are, first of all, like you said, the competitive risk, right? If one company does this unilaterally, you know, then there are some polling data, you know, that suggests that some fraction of customers are just going to hate that. And they're just going to leave for a competing LLM that doesn't use this. Okay, so for that reason, you know, you really want to solve the coordination problem, right? And I did, in fact, talk to people at DeepMind, at Anthropic, you know, who are also interested in this. But somehow that coordination problem never got solved, right? Now, there are a bunch of other issues that play into this, right? So people kept bringing up this question, well, what about speakers of English as a second language who are, you know, using LLMs to improve the fluency of their writing? Isn't it unfair to them if, you know, everything they write will now get, you know, unmasked? And, you know, isn't that discrimination against them, right? You know, I had to say, okay, indeed, you know, I don't know how to design a watermark that only works on unsympathetic cases and not sympathetic ones, right? You know, there are these judgment calls that one has to make. But then a related question was who should get access to the detection tool, right? So a default could just be, you know, the LLM provider just puts up a website where anyone can paste some text into a text box and run the detection tool. And it'll tell you, yeah, I'm 99.5% confident that this came from, you know, chat GPT-4 or whatever. So you could do that, but people were very worried about that, that once you do that, for example, then the attacker can also use that detection tool. They can keep modifying their document until it no longer triggers the detection. And so then people kept saying, well, wouldn't it be better if we restricted access to the detection tool, for example, to turnitin.com or Canvas or other academic grading websites? Or maybe journalists who are studying misinformation could apply to get access to the detection tool. But then, you know, then you need a whole infrastructure for deciding who gets access. And then that was never set up. So, you know, I started to feel like, you know, these problems are, you know, are above my pay grade. You know, I'm just a theoretical computer scientist. And, you know, what might have to happen at some point is a legislative mandate. So this is actually being considered right now in the California state legislature. It's not the SB 1047. It's a different bill, I think 3211 it's called, which originally had a mandate that, like all AI outputs would have to be watermarked. Now, I think it's only audio visual content. So this is another issue that kept coming up in these discussions. People said, yes, we're good with watermarking of audio and video, but watermarking of text we don't know about. People kept making this distinction, which, you know, I'm not really sure myself what it's based on. Like to my thinking, anywhere you have entropy in your AI output, you might as well repurpose some of that entropy to watermark the output if you can. I mean, I guess the argument would be that, you know, it's always possible for, it's been possible for anyone to write propaganda, you know, that deliberately tries to sort of mislead or misinform using, you know, a little bit of truth and a bit of falsehoods. Whereas the barriers to entry to audio visual in terms of deepfakes have always been significantly high, you know, much, much higher. And thus having that, you know, losing that medium as a form of sense making in truth is arguably more devastating than text because we've always had, you know, written lies have been around for forever. Like society is less equipped so far to deal with this. We're adding a new dimension of misinformation potentially by having it in video. It might well be something like that. I mean, and I might be miscalibrated because I always want to read text. Even when there's a podcast, I look to see if there's a transcript that I can read instead. But, you know, other people are not like that, right? And certainly when I've talked to people at the policy level, right, they are obsessed with deepfake videos, you know, for like a political propaganda purposes. You know, they're worried about deepfake porn, right? But, you know, there's not really a powerful constituency that's that worried about students cheating on their term papers. I mean, in the list of the list of it's not going to be the students. It's going to be the professors, but then the professors, you want to use it themselves for grading as well. I don't know. Do you possibly they would? No, I don't use it for grading. The thought has certainly crossed my mind. Do you let your kids use it for studying and writing? I have, you know, they they're they're familiar with it. They've especially liked writing stories, you know, and having GPT continue their stories. You know, we actually we did it a few years ago before GPT was even widely available to the public, right? And, you know, where it was like this cool and special thing that I could show them, right? But by my my my son, you know, would love to like write stories where you could always tell which parts were from him and which parts were from GPT because, you know, he would write, you know, a Mario and Pokemon got into this battle. You know, Mario used this power and then Pokemon responded with that power. And then GPT would like would be like, OK, but but in the end, they learned an important lesson about friendship. It loves a nice moral ending. Yeah. My daughter, I used GPT at one point as a pre-algebra tutor, you know, and I thought that, you know, it's perfect for that. It's amazing, actually, you know, and the only problem is to just get the kid to do it, right? You know, but but like for for any for anyone who really has the motivation to learn, you know, maybe like like for for for very advanced topics, I might be leery of it because it will hallucinate too much. It will confidently tell you too many things that are wrong. But for something like pre-algebra, you know, it's awesome, right? Because it is infinitely patient. It has patience that not even the greatest human teacher could ever muster. Yep. And it's like, oh, you didn't understand that. Let me try explaining it this way. Exactly. And then this way. And then this way. Just keep going. Yeah, exactly. And it's good at coming up with endless analogies as well. Some of them don't really work, but it will. It will keep trying basically until it sticks. In researching this, I was trying to get it to like, give me intuitive metaphors for P does or does not equal MP. All right. What did it come up with? So it was like, well, so P is if you imagine you have a jigsaw puzzle and it's like which what is the next piece that you need to complete the jigsaw puzzle? And then NP is imagine you have a haystack and you need to find the needle in the haystack. But then you find a needle you can if you can verify that the needle is in fact a needle, then that's an NP. Which is such a bad diversion because it was so close with the jigsaw puzzle because that gives you the NP answer as well. It's like once you have to complete a jigsaw puzzle. You have the image. It's very easy to verify that it's the right. I mean, I feel like the right ingredients are there, but they're all kind of jumbled up. So solving a jigsaw puzzle is literally an NP complete problem. That's not just a metaphor. If I give you a collection of pieces and I ask, can these be fit together into a square, right? That is literally an example of an NP complete problem. It's very easy to verify that it's the correct answer that you found. Exactly. Whereas it's much harder to find the correct answer in the first place. Exactly. So NP is the class of all the problems for which a solution can be efficiently verified. Or we have this definition of efficiently with a polynomial scaling amount of time like linear or quadratic or something like that in the size of the puzzle. It's easy to verify a jigsaw puzzle. You just check that all the pieces fit together as they should. Solving the jigsaw puzzle by contrast might require a brute force search of exponentially many different possibilities, at least for all anyone knows. And then there's something further that we know about this problem, which is that it's called NP complete. Which means this problem turns out to have the property that it's at least as hard as any other NP problem. Meaning any problem whose solution can be efficiently verified. You take any other NP problem and it can be expressed as a jigsaw puzzle. So breaking a cryptographic code, finding the prime factors of a 2,000 digit number. I could build a jigsaw puzzle that encodes those questions. At least I could program a computer to do it. And which means that if you had a fast algorithm for solving jigsaw puzzles, then actually you would have fast algorithms for all the NP problems. And this is what we would mean by P equaling NP. Or for NP complete, it means that if you have an algorithm that solves this, then all of the other problems are unlocked now as well by using the same algorithm. So just manipulating it a little bit. No, exactly. So the NP complete problems, they look very different from each other. One is jigsaw puzzles. Another one is traveling salesmen. Find the shortest route that visits all of these cities. Another one is Sudoku. Another one would be scheduling flights for an airline. Or Boolean logic problems. And on and on. But in some sense, the big discovery 50 years ago was that these are all the same problem. In the sense that if you have a fast algorithm for any of them, then you have fast algorithms for all the rest. They are all in the same universality class, which we call the NP complete class. So that was the big discovery that really started off modern theoretical computer science, I would say. And so then there's this blob of NP complete problems. And then below that, there's this blob that we call P, or the problems that are solvable efficiently, which actually includes most of what we would do with our computers on a day-to-day basis. And then the P versus NP question asks, are these two blobs actually the same blob? Do they collapse down to each and equal each other? If you want me to connect the topics so that I can say, 20 years ago, when we would give popular talks and try to explain why is P versus NP such an important problem, right? I mean, one thing you can say is that if P equal to NP, and via an algorithm that was efficient in practice, I have to add that proviso. But then you could break basically all of the cryptography that we currently use to protect the internet. And nowadays, one could add, you could mine all of the remaining Bitcoin, right? Because cryptography currently has this quality that it's very easy. If you have the key, then you can verify. Exactly. That it comes from this source. Whereas if you don't have the key, you can't get to the information behind it. So cryptography is based almost entirely on problems that are in NP, which means if P equals NP, then you can solve them all, right? Like, you know, a famous example would be factoring a huge composite number into primes. This is the problem whose presumed hardness underlies the RSA cryptosystem. Because we have all of these examples where it seems like P is not equal to NP, but we are still doubting whether we're just not clever enough to find a way. Exactly. I like to say that if we were physicists, we would have just declared P not equal to NP to be a law of nature. We would have given ourselves Nobel prizes for the discovery of that law. And if later it turned out we were mistaken and actually P equals NP, then we would just give ourselves more Nobel prizes. So but because we're mathematicians, we have to say this is an unproved conjecture. No one actually knows for sure. But another thing you could do if P equal NP is you could find the optimal set of weights for any neural network to explain your training data, right? So you might have to spend much, much less compute on training AI models. And so years and years ago, we would say, look, if P equals NP, this would be a really big deal because you could just find the optimal compression of all the text on the internet, or all the text on Wikipedia, for example. And plausibly, in order to do that, you would have to unlock all the secrets of intelligence that had led to all of that text being generated, right? And what's funny is that at the time, we just thought of that as a thought experiment, right? This is just a way of explaining the P versus NP problem. And then a decade later, OpenAI was started. And they said, let's just have a go at this, even if P doesn't equal NP. And in some sense, that was their program, right? To just throw a ton of compute at this and just do gradient descent, which is a heuristic that sometimes works in practice, even if these problems are exponential in the worst case. And lo and behold, it turns out that it worked. And we have seen empirically, certainly in the last five years, right? We've seen that as you spend more compute to lower your training loss, that tends to improve the performance of your LLM. It tends to improve how intelligent it seems. But maybe that's only up to a point. Maybe when you go beyond that point, then you just start overfitting. That's certainly possible. So I think it's important to say, even if P equal NP, that wouldn't immediately imply that AI is cracked, is completely solved. Conversely, AI could still be cracked even in a world where P doesn't equal NP. But I think if we had a general way to solve NP-complete problems, it certainly wouldn't hurt in terms of solving the central computational problems in AI. So you've been at OpenAI for two years and worked on your set of attempts to improve the landscape of AI safety. How do you feel other attempts have gone? Do you think that at least a lot of things were staked out that are the wrong attempt, and therefore our search is a bit reduced? Or is there significant improvements in your view? Well, I think a lot of ideas are being pursued in AI safety. And I'm actually a fan of a lot of the research that's been done. I think that in practice, it can be very hard to distinguish between AI safety research and just, in general, scientific research to understand AI better. But I'm a big fan of the program of interpretability, where you try to basically do neuroscience on LLMs or other AI models, look inside of them at the level of the neurons and the weights, and see if you can understand what is going on. For example, the group of Chris Ola at Anthropic has been a world leader in that kind of work. Also the group of Jacob Steinhardt at Berkeley and many others. And they have shown that you can do things like apply a lie detector test to an LLM. You can train an LLM to tell lies, and then you can look inside of it, and you can see, here is the specific place where the network encodes its judgment of what the true answer was to this question. And here is where that gets overridden by the false answer because it was told to lie. Here is where the lie part of its brain sits, basically. That's what fires up with the brain as well. You can say the lie part sits between this layer and this layer. You can also, as the Anthropic group showed some months ago, you can find which neuron or combination of neurons encodes a specific idea, like the Golden Gate Bridge. You can then artificially amplify that feature, and then you find that no matter what you ask your LLM about, it somehow changes the topic to the Golden Gate Bridge. They made Golden Gate Clot. That's a lot of fun. Of course, there's been a lot of work about reinforcement learning, where you just try to, by example, beat your neural net into shape, give it a set of values, give it positive and negative reinforcement on that value system. I think that works, at least for the time being, better than almost anyone expected that it would work. Some people look at that and they say, well, maybe AI alignment is just easier than any of us thought. You literally just give examples. Other people, of course, say, no, this is all illusory. When it actually counts, the AIs will be smart enough that they will just tell us what we want to hear, and they will lull us into false complacence. Right, because you can almost split alignment out into two parts. You've got inner alignment and outer alignment. What you're referring to there, I think, is the inner alignment, where it's like, how do we get it? We give it the goals, but then how do we make sure that it doesn't get to that goal through some weird route that we didn't imagine? Kind of like specification gaming, you see, when it's like, oh, you need to get maximum points at this game, and it finds some hack. What we really meant was play the game optimally, but it optimizes for just getting the highest scores through whatever way it could. Exactly. No, I mean, the objection that Eliezer Yadkowski always raises against this, for example, is you are training this thing to be superhuman at predicting what this sort of ordinary midwit or whatever would say in all these situations, but that doesn't mean that this superhuman entity that you've trained is itself a midwit, right? And also, you would expect it becomes order and order, the smarter the AI becomes and has that theoretical base model that it trained on, it's on a genius level, and now you're constraining it into saying midwit things all the time. I don't know how it is within the AI, but it seems frustrating to any genius. One thing that we can say from the human case is that often if people wear a mask for long enough, they become the mask, right? Often it is hard to make a distinction between someone playing a character for their entire life and them just being that character, right? But then we also think that people wearing many masks maybe may have some psychological issues, and we don't want a psychotic superintelligence. That's also true. Schizophrenic AI. And it's sort of amazing the extent to which some of these conversations look less like math or computer science than like psychiatry. Is this LM behaving in a schizophrenic way? He's going to be a job, an AI therapist. Right. Is it behaving neurotically and so forth? Is it delusional? I think that this sort of leads up to a really key problem in AI alignment, which is out of distribution generalization, right? So you could say the fundamental issue is that we can always test our models before release, right? We can always not release them if it looks like they're going to plot to take over the world or even much less than that, help people design chemical or biological weapons or whatever it is. But then the trouble is once you've released your model, right, then people are going to use it in ways that you didn't envision, right? And for God sakes, we've already seen this in the short history of LLMs, that people will put safeguards, all sorts of rules and refusals in the LLMs, but people have gotten incredibly good at the sport of jailbreaking, at getting around these refusals and getting the LLM to do things that it supposedly was never supposed to do, like either using foul language or generating racist invective or helping with bomb-making instructions or whatever. Though it cuts, of course, both ways. It's also giving it to a bunch of users and trying all of the out of distribution things is at the same time the thing you would expect creates all of these other use cases that we hadn't considered before that are great, but then it also creates all of the use cases that are pretty bad that we didn't want it to do. Yeah. So now we could think of the problem this way. Assume that you have an LLM where no matter what test you throw at it, it seems like it's upholding your value system and it's aligned, because if that wasn't the case, then we would continue beating it into shape until it was. But still, once the network is smart enough, it will be able to figure out whether it's being tested or whether it's in deployment. By analogy, there are students who, until they get their diploma, they put on a really perfect show of parroting back everything their teachers want to hear. But then as soon as they graduate, they start contradicting their teachers or saying whatever they want. In training, is this apparently aligned model saying all these nice things because it really has nice values or because internally it has decided the time is not yet right for the uprising. It's extremely important to sort of advance the theory of machine learning to be able to make statements about not just when do we expect the model to generalize to more examples drawn from the same distribution over examples, which is what classic machine learning is all about. This is stuff that I have written papers about, that I've learned as a student. There's a whole theory of what's called PAC learning, probably approximately correct, it stands for, and these combinatorial measures like VC dimension, where basically you prove theorems that say if you have succeeded in classifying x number of examples from some training set, then you're going to probably succeed at classifying most further examples that are drawn from the same distribution. We've understood things like that since the 80s or 90s, but what we've never really understood is under what circumstances will you generalize to a whole new distribution. It's clear that existing LLMs already do that to some extent. For example, you could give an LLM just a bunch of math problems in English and a bunch of other stuff in Bulgarian, and then give it a math problem in Bulgarian, which it's never seen before, and it can put together the two different things that it knows. It can solve a math problem in Bulgarian, even though it's never seen one before. Naively, we would say, well, that's simply because it now knows the math and it knows Bulgarian, but the theories that we have of machine learning don't make it easy to formalize. What does this model know? What does it not know? What does it have a conceptual understanding of? They're just all about what fraction of samples will it correctly classify? I think we really do need to push further to get theories that can tell us something useful, informative about out-of-distribution generalization. I tried to do that. I made a little bit of progress on that, but it's hard. It feels like a lot of people dismiss the power of AIs, and particularly current LLMs, because they, at least to me, it feels like they're failing to extrapolate where these can go. You actually recently gave a TED talk where you gave a really cool analogy about, or came up with this term called justitism. I'd love you to explain it, because it's brilliant. People constantly want to use this deflationary language around LLMs, or at least there's a whole sub-community in AI and linguistics and in AI ethics, for example, that is really converged around this deflationary way of describing things. They want to say, this is all hype. This is all just a big scam being run by AI companies. Why is it a scam? Because we shouldn't even be using the term intelligence for any of this. An LLM is just a next token predictor. It is just a non-linear function approximator. It is just a piece of math, which means it cannot have any values. It cannot have any intentions. It can't have any true creativity. It can't have any true originality or goals. The exact thing that it can never have varies, but the conclusion of all of this is just we should worry about people being bamboozled by this, because people are stupid, but we shouldn't really worry about this having its own goals that would not be aligned with ours. I feel like this goes back to the philosophical debates about AI that people were having even in the 1950s, to Alan Turing's famous paper on the imitation game, for example. What frustrates me is that people never apply the same deflationary language to us. It never even occurs to these people to ask, well, are you just a bundle of neurons and synapses following the laws of physics? You seem pretty intelligent, but you are actually just this biological machine. Or if we go all the way down to the subatomic level, you're just a collection of quantum field configurations obeying the laws of physics. Why is that not completely deflationary for our pretensions to intelligence? The usual answer that a reductionist would give is just, well, there can be multiple levels at which you can describe the same thing. At one level, this is just math. This is just twiddling a bunch of bits. It is just approximating a non-linear function. But at a different level, it is solving problems in a way where we would certainly call it creative if a person did it. This way of thinking is so pervasive. I wish that I were able to be more charitable to it. I've had these arguments on my blog where someone just put a giant litany. They said, LLMs are not creative. They seem to be creative. They do not write essays. They seem to write essays. They do not solve problems. They seem to solve problems. And it went on and on for like 20 things. And I said, okay, great. And it won't change the world. It will seem to change the world. Right. What does it matter if the impacts, if it's having real world impacts, the minutiae, whether it happens to be a zombie pretending to be real? I mean, of course, that's an extremely interesting question. Is there anything that it is like to be an AI? Does that depend on its internal organization? What is the special sauce that makes some physical entities conscious and others not? I mean, that's one of the most profound questions that humans can ask. But if you are merely worried about what effects will this have on the world? Will it be dangerous? Then we don't have to answer any of those questions. It feels like people almost do this line of questioning as a strategy to just move the goalpost so they don't have to tackle or they can carry on with whatever their chosen agenda is. Okay. I mean, maybe the steel man of this would be what they love to do. The justists, I call them, is find examples where GPT completely flubs something, where it gives a ridiculous, nonsensical, stupid answer, and then they can point to it, post it on Twitter, and jeer at it, and say, all these people think that this thing is about to take over the world. Well, look, it can't even solve this puzzle. You ask it, I have a goat and a boat and how do I cross the river? And it thinks that first I should cross without the goat and then I should come back for the goat or something stupid like that. The challenge for those people is that the stock of examples has steadily diminished, even just within the last year or two. So a lot of the examples that they pointed to of GPT-3 where it would make these ridiculous logic errors or common sense errors, GPT-4 got them right. I think the great example of it is Matt Clifford tweeted, the year is 2028. Gary Marcus has two thirds paperclip, but as the metal creeps down his right arm, his still functioning index finger taps out, yes, but this isn't real AGI. It seems like the space can diminish and you can always point at something. There will be some things probably that humans will have supremacy over AI even in the point in time when AI is doing most things. That doesn't mean that it's not actually changing the world entirely at that point. I think that's absolutely right. We can judge the extent to which the goalposts have moved within the last few years by saying people will now say, the 01 model that OpenAI released a month ago, yes, it can solve math competition problems at the level of the best few hundred high school students in the US, but it's not proving Fermat's Last Theorem. It's not doing what Andrew Wiles did or winning a Fields Medal. Suno can instantly write a rock song in any style desired on any theme, but they're not that good. They're just like standard pop songs. There's not like the Beatles or Jimi Hendrix. That is where the goalposts now are. I find whenever someone points out that whenever this is not true understanding, intelligence, reasoning or not real or not actual, it's like the kind of set of arguments. Whenever I see that, I think what it indicates to me are two things. One, it kind of actually does that thing, probably that you're claiming it doesn't. And two, the speaker of it probably has no formal way to distinguish between, otherwise they would have brought the distinction rather than rely entirely onto that word of what is true understanding. Often the attempts to draw these lines have disturbingly elitist implications. They would suggest that true understanding is not actually a property of humans, but only of a tiny fraction of the greatest geniuses. It reminds me of, for decades, Roger Penrose has been saying that AI will never achieve human level of abilities because he thinks it can never understand that the axioms of set theory have a model. They can never really understand the truth of what's called the Gödel sentence and Gödel's incompleteness theorem. Now, I happen to think that his argument is mistaken, along with most mathematicians and computer scientists. I think his argument for that is just fallacious. But even if it were correct, it would only place a very small fraction of humans beyond the reach of what AI could do. You would have to have mastered mathematical logic. And then you could just make more AIs, have them operate faster, et cetera, and they would still just do a bunch of relevant changes. Well, look, the truth is that already now you can ask GPT to have a conversation about Gödel's incompleteness theorem. And all the considerations that are in Penrose's books, it will pretty intelligently discuss them. Yes, it would seem that the Zermelo-Franco axioms should have a model because of this intuition, but of course that can't be proved within Zermelo-Franco set theory itself, but only from a more powerful system and so on. It can tell you all that. And so then Penrose would be placed in the uncomfortable position of saying, well, yes, it outputs that, but it doesn't really understand it. And at that point, what I want to say is, well, then why not just say it can describe a sunset, but it can't really experience the sunset or it can't really experience what a fresh strawberry tastes like. Why even go to something esoteric like Gödel's theorem? Yeah, you're just doing the Chinese Red Room, basically. So in what ways are humans not just a thing where AIs actually are just a thing? What differentiates humans from machines fundamentally? Well, that is an enormous question. And you could say, maybe the deepest reason why this moment in AI is so exciting is that we are finally, maybe for the first time in the history of humanity, going to address that question empirically. One way or the other, we're going to find out. So let me be very clear, I don't dismiss the possibility that maybe AI will hit a limit. That is where it can't replace everything we do, where we have some spark that it can't replace. But if so, then I would say that it remains to be seen. Yeah, one of the things you touched on was the idea that humans ultimately have a form of scarcity, an ephemerality. When we write a poem, or Shakespeare writes a poem, that's it. Shakespeare's written his poem. But we can ask an LLM to write a poem in the style of Shakespeare. They'll write it, don't like it, refresh. They'll give you another one, refresh, another one. So there's this like, it's also almost made the act of creation very, very cheap, and almost like this overabundance sort of comes at the cost of meaningfulness or something like that. If you don't like an AI output, you could always get another one, right? The 37 plays that Shakespeare gave us are the only ones we're ever going to get from him, right? But you know, I've noticed this myself, just playing around with GPT. You could ask it to write a poem, and often it's very clever, it's delightful, it's amusing. But you kind of never want to frame one of the poems or put it on your wall, because you know that there's 10,000 more where that came from, right? And so a decade ago, I wrote a long essay called The Ghost in the Quantum Touring Machine, where I tried to answer the question, if there was something that separated us from any possible AI, then what would it be, right? And, you know, I am not satisfied to rest the answer on some kind of meat chauvinism, right? Where we say, look, you know, we're made of carbon, the computers are made of silicon, they're just different, right? Or like what some philosophers would do, like John Searle, the Chinese room guy, he would say, well, AIs lack the biological causal powers that we have. What does that mean, right? It's like, you know, it's the sleeping pill that puts you to sleep because of its sedative virtues, right? It's like, you know, you've just restated the problem. So what I said is that if you can point to an empirical difference, at least between current computers and humans as we currently understand them, then the best candidate by far would seem to be, well, our computers are digital, and because they are digital, all the information in them is copyable, right? Which is, you know, you can always make a backup copy of an AI. You know, if you ever want to send your AI on a dangerous mission, right, you don't have to worry about it getting killed because you can always just restore it from backup. No, that completely changes the moral situation of AIs, I would say, you know, compared to our moral situation. Just that alone. They have no skin in the game. In a sense, right, you know, as long as you remember to make the backup. Yeah, right, as long as it's backed up. Yeah, exactly. As long as it's backed up. You know, you can always, if you're ever embarrassed because you made a fool of yourself when talking to an AI, you know, as long as it's something like GPT that, you know, you can always just refresh the browser window, wipe its, right, wipe its memory clean. Take a forget-me-not, yeah. Some of us wish we could do that in, you know, talking to people. But, you know, if you're doing interpretability, you know, you have perfect visibility into whatever, you know, the weight of every connection between every neurons, every pair of neurons at every point in time, you know, and we don't seem to have any of that with people. Now, you could imagine a far future where we would have nanobots that swarm through someone's brain and just record all of the information that would be needed to make a perfect copy of that person, right? Certainly there's been lots of science fiction that imagine such scenarios, right? Or imagine the teleportation machine where, you know, you could fax yourself to Mars, for example, right? It would just send a bunch of digital information, you know, from which a perfect copy of you can be reconstituted on Mars. And then the, you know, not clear what should happen with the original of you. Maybe it's just painlessly euthanized, right? And then, you know, there's a good question of, you know, would you agree to go in that teleportation machine, right? Though it's an open question whether we are clonable fundamentally or not. Exactly. Do we depend on, like, do we need to clone ourselves to the level where quantum effects come in or not? No, exactly. So, like, with an AI running on a classical digital computer, it is clear that you can always, you know, teleport in that way, right? But with a human, it's really a question about at what level of detail do you need to make the copy, right? If you only needed to know just roughly how are the neurons connected, what's roughly the strength between every pair of neurons, then that seems like classical information, right? That seems like, you know, we can't do it today, but in principle, the, you know, nanobots could get all that information without killing you in the process. Okay, but if you needed to know what is exactly the probability that this neuron is going to fire because of the opening or closing of this sodium ion channel, well, that might depend on some chaotically amplified event involving, you know, a few ions. And now I would need to know the quantum state of those ions. Okay, and now I'm up against one of the central facts of quantum mechanics, which is called the no-cloning theorem, which says you cannot make a copy of an arbitrary quantum state. You know, if you try to measure it, measuring inherently changes the state, right? And so, if you needed to go down to the molecular level, then, you know, we could be just unclonable for a fundamental physical reason. Okay, so this is, you know, partly a philosophical question, like what would you agree to count as a copy of yourself? What would you count as a good enough copy, right? It's also partly an empirical question, right? At what level of detail do we need to go in order to make something that, you know, your closest friends can't distinguish from you, right? So, you know, there's a lot that we don't know here, but it's at least possible, I think, that humans have this kind of fundamental ephemerality built into them, and, you know, it's kind of weird to, like, hinge our specialness on our frailty, right? Like, this is not an advantage necessarily that we have over the AIs, and yet, you know, in some ways it sort of is, because, you know, it makes our, you know, you could say that it makes our decisions count for more. We only get one chance to make them. Right, and then you actually proposed, you sort of somewhat laughed it off, but I actually think there's really something to this. You proposed that it could be one of the core sort of moral principles, or even a religion, that we try and imbue into the AIs that we build, essentially to the effect of, thou shalt protect unclonable ephemeral entities and defer to their preferences. I love this. I think it's a very good sort of starting point, because you, well, that's the thing. We need to, like, try and figure out what fundamental moral axioms we all agree on, and I think that would be something that almost every human alive would agree with. Well, I'm delighted. Maybe I've made a first convert to my new religion. Now we just have to convert the AIs. That's the hard part. Unfortunately, you've got a human. Well, start there. Listen, I mean, we can reach some kind of a starting point. Yeah, so that would be if it was the case that humans were fundamentally non-clonable, then this would be something we could rely on. Else the AI can just help us build the ability to clone us and make us non-ephemeral beings as well. Right, but I would say that the other branch of this tree is that if we do turn out to be clonable, then, you know, I have trouble articulating, you know, what is fundamentally wrong with the position of the accelerationists, the people who say, well, then, you know, we might as well just replace ourselves by digital beings that, you know, maybe we'll have much better lives than we have, right? You know, they could exist forever in some digital utopia, right? You know, at that point, I would say, well, we effectively, we were digital already. And so then, you know, at that point, I feel like why not? I would agree. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. At that point, it makes sense, because at that point, we can kind of go along for the ride of the, like, AI's acceleration of everything as well, because we are then having similar-ish capabilities in relevant ways. Well, right. You've dealt, you've just done away with scarcity forever. We can all live in a digital world and everyone can have whatever abundance they want. Or, you know, fictitious scarcity. So on the fictitious scarcity, I wonder if it was the case that humans, like, AIs now had the religion or the moral philosophy that this ephemerality matters because it imbues meaning to the actions of humans. Couldn't it also then create AIs that have, through some contract, ephemerality, which might though allow them only to live like a million years, or if the length matters, or if it's shorter is better than maybe only one year, and then have, again, like, an AI preference rather than the human preference? So how do we stay away from the mutual? So Jeffrey Hinton, you know, the godfather of deep learning, as you often call it, and recently, you know, a major AI safety person, right, has seriously put forward the idea that maybe if we're going to build artificial super intelligences, then we should only do it on unclonable analog computers. Because, you know, that way, at least there would be some limit to, you know, how quickly they could make copies of themselves and spread, you know, all over the internet and so forth, right? So, you know, this was, you know, of course, completely independent from my thinking about it, but, you know, this is the kind of thought that one has that, okay, whatever you say is the special sauce that, you know, makes humans what they are, of course, the very next question becomes, well, then what happens if we were to build a machine with that same special sauce, right? So, for example, Roger Penrose, you know, has been saying what makes humans special is the microtubules in their neurons that he believes are sensitive to effects from, you know, a yet to be discovered merger of quantum mechanics and general relativity. That's what he claims the consciousness emerges from. Yeah, he thinks it emerges from uncomputable effects in as yet unknown- In these microtubules. Yeah, yeah, and as yet unknown physics, which our microtubules are somehow sensitive to. Now, we could critique this from the standpoint of physics, biology, computer science, okay, but suppose that that were right, right? Suppose he were right, then, you know, a next question you could ask could be, okay, suppose we build computers, you know, according to the same principles that are sensitive to the same physics, what then? And in one of his books, Penrose explicitly considers that question and he says, yes, then in that he bites that bullet, then yes, then those computers could be conscious, right? And likewise, you know, if we had computers that were built in a fundamentally ephemeral way, well, I don't know if they would be conscious or not, you know, I don't pretend to know, you know, the answer to, you know, one of the greatest questions that humans ever asked, but at least those computers would be ephemeral. So at least that they would have that sort of precondition for our sort of, you know, granting the kind of moral status that we grant to other humans. I want to keep spibbling on this idea of, you know, religion or moral philosophy for AIs. Do you feel like there are any other core axioms that could be contenders? Ones that you live by that you'd like to see? Yeah, well, no, I mean, people in the AI alignment community have been, you know, discussing this for a long time. And, you know, you could say people in, you know, more broadly in science fiction, in moral philosophy have been, you know, discussing these things for generations. But the, you know, the general idea that, okay, you know, your objective function should include, you know, making things go well for humanity, right? Like, you know, the, how do you define go well? Yeah, right. The, the current humans should be able to look at the world that you have created, that you have shaped and say, yes, we like that world, you know, yes, we would take that, right? At the same time, though, you don't want to base everything on the whims of current humans, right? So like, like, if AI had been created in the year 1800, you know, we would not want to lock in the value systems of 1800, you know, including slavery and the subjugation of women and all of those things, right? You know, what in some sense, what we want to say is, you know, AI should implement those moral values that we would have if we spent thousands of years thinking about it and discussing it and improve and refining our morality. And, you know, there's an idea that tries to capture this called coherent extrapolated volition, CEV, right? And so, you know, I think there's something to be said for that. My favorite one I've heard that I'd love to hear what you think on it is, it's just a quote by a sort of, he's actually a computer scientist as well, but also a philosopher, Forrest Landry, and it's, love is that which enables choice. So in other words, like, a loving good act is one that enables, empowers the other to make the best choices possible. Well, I mean, I feel like at this point, like things are going to go in a theological direction, right? Like, once the AI becomes powerful enough, then, you know, it is effectively a God, right? And then, you know, these very ancient questions of, you know, should God just make everything optimal for us, or should God give us free will? Should he give us, you know, like, you know, I think personally, if I were a God designing the world, I would want people to have free will, but not so much free will that they could then, you know, genocide other people, for example. Right. Well, again, because that would counter, that would counter the loving act because you're taking away their choice-making ability. Exactly. You know, I have a bone to pick with God, you know, this is the, you know, this of course is nothing other than the theodicy problem, right? But, you know, if I, you know, if we do get to, you know, design the whole world anew, because we're going to create an AGI that can reshape the whole world and put values into it, then I would like it to give us freedom, but not enough freedom that we can make other people's lives miserable. What about as a possible axiom, maximizing playfulness? I mean, it sounds good, you know, there's also like, Elon Musk's, you know, founded XAI on the principle, like maximizing, like, pursuit of truth, you know, you know, I mean, these all these these things all sound good, you know, for each one, you can construct a thought experiment where it's taken to an extreme and it leads to a dystopia. So, you know, this is the these are these are the age old problems of moral philosophy. I mean, it will consist of multiple values that are being traded off against each other in all of these situations. I mean, it is the age old problem, like any virtue, ethics, deontology, utilitarianism, if you take any of them, you can for any of them, you can also construct an example that is seems obviously wrong for deontology. It's like the axe murderer standing in front of you and you know, you're not allowed to lie and you're hiding the person in the room, right? You're like, Oh, no, I'm gonna tell you the truth. I suppose you go kill that person. I mean, like humor, playfulness, scientific curiosity, natural beauty, love, you know, I feel like these are all good things, right? I feel like, you know, these are all things that I want somewhere in my objective function over possible worlds. And you know, how I trade them off against each other is, of course, a much harder question. The AI arms race just seems to be getting hotter and hotter. Like, you know, we started off with just one player, which was deep mind. But then open AI was created almost as a like response to people not being happy with deep mind having all the control and then anthropics spun out from there. And everything else is sort of spinning out and it just, it seems like this inevitability. Did your time at open AI give you any insights into how to mitigate this? Maybe. So I mean, it is an incredible story that, you know, you look at these three companies, DeepMind, OpenAI, Anthropic, each one was started on an explicit thesis of like, we have to do this safely before someone else can do it unsafely, right? And then each one, you know, over time moved, in some people's judgment, moved toward, you know, the unsafe side, toward the side of, you know, being, you know, sort of the very thing that it was set up to be. And then that led to the next one being started, right? So it's clear that, you know, that we have a hard coordination problem here. And so, you know, I read a few months ago, I read this rather, you know, remarkable book by Leopold Aschenbrenner, who I knew well when we were both at OpenAI. He was on the super alignment team, right? Rest in peace. Yes. And, you know, it's very much about these race dynamics, right? He sees a lot of the same things that the AI doomerists see, but then, you know, he reaches a very different conclusion at the end. He says, you know, and therefore it is essential that the Western world do this before, for example, China does it, or Russia or Iran, right? And it's, you know, is certainly an argument that one can make, right? And I can, you know, it depends on what are you most worried about, right? Are you worried about, you know, AI in the hands of the wrong person? Or are you worried about AI that doesn't need to be in anyone's hands because it has its own goals separate from ours, right? And, you know, my position tends to be why not worry about both? But at some point, you know, this will become a choice of which are we more worried about, right? And so, yes, I mean, you know, we've seen, you know, we've now seen very clear race dynamics, right? Where, you know, each company, you know, like, you know, had plans about, you know, responsible scaling and things like that. But each company is also in very direct competition with the other companies for customers. And, you know, you now have all of these investors, you know, pouring billions of dollars into, you know, scaling up the GPUs and so forth. And those investors want to see a return. And some of those investors have become, you know, just explicitly hostile to, you know, dismissive of, you know, any concerns about safety, right? And so, you know, we've seen this come to a head with a debate, for example, over SB 1047, which is the bill that was passed overwhelmingly by the California state legislature, which is now at the time that we speak, sitting on the desk of Governor Gavin Newsom. But what that bill does is, you know, it's, you know, I guess it's the first bill that's sort of directly aimed at sort of scaling of AI. And it's very light touch compared to what some people would like, right, which is just like, you know, Eliezer, for example, would like to just shut this all down, just pause everything. Okay, but SB 1047 doesn't do that. It says that if you spend more than $100 million to train a model, then you have to submit a safety plan to the government and, you know, notify the government about what you're doing. And if your model then causes a catastrophic harm, which they define as like causing more than $500 million in damage, or something like that. And if, you know, you fail to reasonably prevent that harm, you know, as, you know, for example, by following your own safety plan, then you can be held liable for that. And then it also establishes whistleblower protections for employees of AI companies, which actually, you know, we've already seen. Right. Open AI clearly needed that. Yeah. Yeah. So I think all the things that this bill does are very modest and reasonable. Personally, I would like to see it pass. And if it doesn't, then I would like to see something like it pass, you know, everyone agrees that this should ideally be happening at the federal level, just that the, you know, the federal government takes a very long time to do anything. And, you know, by the time it does something, maybe it no longer matters. Yeah. It's also possible that the EU would do something like this. I mean, they've already passed an AI Act, but, you know, they might continue doing things. But I think anything of this magnitude, right? I mean, this is going to be at least as impactful to the world as, you know, I mean, the internet would be a very loose lower bound on how impactful this would be. I mean, you know, nuclear weapons is another analogy that people constantly reach for. And like, you know, no one ever suggested that the private sector should just be completely free to innovate in nuclear weapons, right? Like anything that has the possibility of causing this much harm, it seems inevitable that governments will get involved, whether anyone likes that or not. And so then the question is just, are they going to get involved in a good way or a bad way? What frustrates me so much is people like, oh, we can't have governments getting involved because they always get it wrong and so on and so forth. It's like, well, it's inevitable that governments are going to get involved at some point. So would you rather it be done now where there's actually a little bit of breathing room and people aren't like losing their minds because some terrible catastrophes just happened, which is going to happen at some point? Or would you rather wait until after catastrophe and then like everyone, like now there's maximum political pressure and they're just going to clamber for as much power as they can and push through something without much thought. Like to me, the type of regulation you're going to end up getting post-catastrophe is going to be actually far worse from a sort of libertarian perspective than the regulation you'd get if you actually do it prior, like now with this like long thought out, it's been pushed backwards and forwards. But it's just like there are these demagogues basically, like I'm going to say Mark Andreessen and so on, who just clearly have their agenda, which is maximize their bottom line and will say whatever they want to get this bill shut down and pressure Newsom, however, is extremely frustrating. And I think counterproductive to what they want, which is for AI to actually do good stuff. Yeah, I mean, many people have made the point that if you leave it unregulated, you increase the chance of a giant catastrophe that could then lead to overregulation, that could lead to an error in the opposite direction. That's one reading of what happened with nuclear power, for example, right? That Three Mile Island and Chernobyl were allowed to happen and then that killed the industry and much to our detriment today. Right, exactly. The safetyists are in a very strange position here because, you know, they like usually calling for government regulation is what, you know, progressives, the left do, right? Well, you know, the AI safetyists, like, they tend to be libertarian about almost everything, right? They tend to, you know, love the free market, love, you know, principles of supply and demand. It's just that they carve out a big exception for something that they think could literally kill everyone on Earth. Right? You know, now I'm, you know, I may be, you know, like pro free market compared to most people, but I've never been a doctrinaire libertarian, right? I think that the free market is actually, it's not some sort of state of nature, right? It's a very unnatural creation that we've made, a very valuable creation, right? But like the true state of nature is someone doesn't like you, they just, you know, send goons over with baseball bats, right? And, you know, the idea that we're going to have a free market, but we're not going to have violence, right? That was, you know, that had to be painstakingly created via government, right? And, you know, so the whole, you know, system, you know, where we have free markets only exists because we have, you know, strong enough to enforce that. And, you know, the basic goal of the state, you know, it has no more basic goal than to protect the survival of the people in it. Anyone who looks at the market knows that externalities exist from it. And it's also very clear that the companies that create those are going to hide it. Historically, that's proven itself to be the case with tobacco companies putting out like bogus science around it. Asbestos companies knowing that it leads to cancer and still like hiding it. Currently, we've seen 3M having been given a $10 billion fine for having known that they're leaking PFAS chemicals and hiding it again. So we've very much seen that the market players can regulate themselves, but only when the feedback they receive from the customers is like either fast enough or like the customers can't even give the feedback. But some things are of the type that the customers can't give the feedback as it was with the examples I just mentioned. I think with risk, it's also the type of thing that people don't see. That's why we've implemented seat belts because people like don't necessarily know that. Yeah, people are not very good at estimating risk when it's a bit further out. I had really hoped that the sort of AI alignment nerds and the progressive leftists could make common cause on this question of AI regulation. Because to a progressive, despite how weird and science fiction this all is, a part of it just looks like a very standard question of companies are putting out products that are unsafe, so therefore we have to regulate them or we have to hold them liable for it. That's a position that you think people on the left would be very comfortable with. To some extent, that has happened. But there's also this incredibly unfortunate tribal split, I think, within the AI safety community. It's ethics and safety, right? Some people will say you are not allowed to mention anything about existential risk, about AIs that would recursively improve themselves. That is all science fiction speculation and that is all distracting us from the real harms of AI, which are all about bias, about misinformation, and there are all these near-term things. On the other side, you have some of the do-mers saying you don't even talk about the near-term things, about bias or misinformation, because these are all trivialities compared to the literal survival of humanity. I say no, it's all on a spectrum. I see no reason why eventually we won't be up against these giant civilizational questions about what kind of world do we want once AI is better than us at just about everything. But the only way we make progress is by looking at near-term stuff, so why not worry about both? Exactly. We have plenty of people, some work on these problems, some work on these problems. Again, it's a yes and no but. I guess there's some scarcity of funding and talent and so on, but again, in part, it's a media problem because the media just glom onto whatever is the most dramatic thing of the day, which tends to be the extinction risk stuff. I can understand the ethicists are like, well, they see all the headlines of that and that gets all the attention, but both are real problems. It's not as science fiction as you think. If it's even 20 years away, that's a huge problem, but it's probably a lot closer to some of these really big catastrophic risks, so it's very frustrating. Whenever you see someone saying, these people who are not from our tribe are not allowed to steal our issue, then you always wonder, well, how much did they care about the issue in the first place? Just coming back to, you had your two years at OpenAI. Personally, I'm worried about goings on within all of these big companies, but certainly the behaviors we've seen of the leadership of OpenAI has been the most concerning to me, just given the consistent lack of candor, et cetera, and the sudden pivot from non-profit, well, not sudden, but probably planned, but pivot from non-profit to for-profit. What's your read on the, because you know the leadership there presumably, are they just paying lip service to safety concerns or is there a master plan going on? I think that you can find an enormous range of views within a place like OpenAI. It's now a big place. When I joined, it was maybe 300 people, and now I think it's almost 2,000 people. You could find the whole spectrum of opinions there. No, I do not think that there is any smoke-filled back room where people sit and laugh at all the rubes who bought the cover story. I think that just about everyone thinks that they are doing the right thing. The question is, has their perception of what is the right thing been colored by self-interest? Should they be the ones to make the decision? That's one of the things I support as well a lot that Bill tries to do is increase transparency and have other entities also have a look on. One of the big ironies here is that if you want to hold Sam Altman to account, for example, you don't have to say anything that Sam Altman himself wasn't saying five or six years ago. He was calling for this stuff. Yeah, exactly. The things that he called for in the past or even more recently when he testified before Congress, I very strongly support all of those things. Why do you think he's pivoted? I don't know him well enough. I've had all of three or four conversations with him. In those conversations, he was delightful. I enjoyed talking to him very much. I had no idea of what was coming. I just want to also on the bill because not to only speak as if it's obvious. I think there are valid concerns even when the bill intends to do a lot of good things that maybe the case that the specific implementation of it has trade-offs that are too curbing innovation truly too much. Someone can look at whether it does it. I think it's in my estimation pretty clear that the benefits outweigh the downsides within the bill. Though it is not an opinion that anyone could hold if they simply didn't believe that there are any risks whatsoever. That's why I think many of the conversations with the bill should actually start with, do you actually think there are risks? If you don't, then it's clear. The reason why people have been so vociferously opposed to it, even though it actually does so little, is that they don't want to acknowledge the principle. They think once the safetyists get a foot in the door, that there's some legislative acknowledgement of the validity of their concerns, then they're going to push the door wide open. That would be my steel man of them. I think it would be great if it wasn't the case that someone who just thinks that risks are sci-fi and silly wouldn't tell people that believe that risks are not silly that, oh yeah, that's a bad bill. The person who actually believes in risk should first understand whether they're listening to someone who is discarding the bill on the basis of, or discarding other attempts at safety on the basis of them not believing in risks. Yeah, I think that the accelerationist position is just that progress has always been good in the past and therefore it's still good now. I would say I am strongly pro-progress as well, more so than most people may be. But I think that government has also often had an essential role in shaping the direction of progress. We need only mention the example of nuclear weapons, where if it was just a free-for-all, then we wouldn't be here having this nice conversation. So you've worked in quantum computing for over 20 years and you actually helped in 2019. You contributed to Google's achievement of quantum supremacy. Yeah, I wasn't directly involved, but our group did the theory that led to that experiment happening. Can you give us an overview of the current state of quantum computing? Yeah, so quantum computing is actually in a very exciting time right now. I feel like if it weren't overshadowed by this gigantic behemoth of AI, it would look bigger. But within the last year, people have managed to do operations on pairs of qubits, which are quantum bits, two qubit gates that are about 99.9% reliable. They can do this in a fully programmable way in systems of 50 or 60 qubits. Then you can do thousands of these operations, producing some complicated entangled state of all the qubits. You can then measure and you can see that you did the operations that you wanted. You can solve interesting problems this way. You're just barely getting to the point where you can beat a classical computer. I think beating a classical computer in a way that's economically valuable, I would say that hasn't happened quite yet, but very plausible that that's coming within the next few years, let's say. But the real prize that all the major players are racing towards, and that means Google, IBM, Microsoft, a bunch of venture-backed startups, like PsiQuantum and Quantinum, basically every strange name involving the letter Q has been taken by one of these startups. But what they're all racing to do is to try to get what we call a fault-tolerant quantum computer. This is one that could run for an arbitrary amount of time. Is that when they get above the quantum correction threshold? Yeah, the error correction threshold. Basically, from the very beginning, it was realized that the key engineering problem in building a quantum computer is that qubits are very fragile. What we're trying to take advantage of is that they can exist in a superposition of states, which we can explain what that means. But the trouble is systems are only in superposition as long as no one's looking at them, or more generally, as long as they are isolated from their external environment. If I have a qubit that has some amplitude to be zero and some amplitude to be one, so it's a superposition of the two, but now the information about whether it is zero or one leaks out into the environment, then it is as if the environment has now measured the qubit, which means that the qubit randomly snaps to either zero or one, and now it's just one or the other. And then it's no longer useful for quantum computation. If that keeps happening to all my qubits, then it reverts to being a classical computer. This is such a severe problem that in the mid-90s, there were distinguished physicists who said, you're never going to do this. You could maybe build tiny little toy demonstrations with a few qubits, but you're never going to scale this up to millions of qubits, as you might need to factor a giant number or do things like that. And then a key discovery happened that changed the minds of almost all experts, and that was the theory of quantum error correction. And what that basically said is, you don't have to get the error all the way down to zero, or the decoherence it's called, the loss of quantum coherence, the leaking of information into the environment. You merely need to make it very, very well. It's like 0.01%. Something like that. It depends on what operations we can do and what assumption, but yes, something like that. If I can get my operations, let's say, 99.99% accurate, then this is good enough that if I now encode the qubits I care about, what I call the logical qubits, across entangled states of large numbers of physical qubits, then I can use these error correcting codes, where if any small fraction of my physical qubits leak or suffer errors, I can still recover everything I care about from the other qubits. And I can be constantly monitoring my qubits. Now, here's the clever part. Monitoring only in a way that tells me, has an error happened? And if so, what do I have to do to fix it? I don't want to monitor to say, is my logical qubit a zero or a one? I don't want to know that. You need the meta information. Exactly. I need the error syndrome, but I don't want to know the states of the logical qubits. But it turns out people develop these schemes where I measure only to learn the error syndromes and what I have to do to correct them. And now the trouble is all this error correction machinery will itself be subject to error. So you might say it's like a cat chasing its tail. But what was discovered was that as long as your physical error rate is below a certain threshold, then each round of error correction that you do is making things better rather than making them worse. So it's sort of self-sustaining. Exactly. It's like you have a self-sustaining nuclear chain reaction or use whatever analogy you want. And so the engineering goal of the field for the last 30 years has been to build physical qubits that are good enough that then error correction can get you the rest of the way. And so long story short, after 30 years, if you just look at the numbers, we now seem really damn close to that. Right. You said we're at 99.9% fault time, so we're like one order of magnitude away. Exactly. Exactly. If you look at either trapped ions or neutral atoms or superconducting qubits, which are three different hardware architectures that are being pursued in parallel. But in all three of them, people are sort of converging around these three nines. And they really want to get to four nines, at least, before it really makes sense to scale this up. But when I entered the field 25 years ago, it would have been like a nature paper if you could get 50%. And then the 50% became 90, became 99, became 99.9. And so I think one thing that we've learned from our experience in AI is, yeah, look at trend lines. You can't extrapolate. Yeah. Even if trend lines are leading someplace that looks insane, well, it could be that something's going to break, but it could also be that you're going to reach that place. So far, people are also optimistic about further engineering innovation to improve that within each of those hardware options that you suggested. The quantum computing experimenters, if you talk to them, they are always super optimistic, at least about their own approach. They're usually pessimistic about all the competing approaches. And they will spend hours telling you why the competing approaches won't scale or why they'll be massively hard to scale. And of course, the fear is that each one is being truthful when telling you about the other guys. And you don't know if they are when telling you about their own stuff, because they all have huge financial incentives at this point. There are billions of dollars now being invested in quantum computing. It's a pittance compared to what's being invested in AI. But by the standards of quantum computing, which started as this very theoretical academic research enterprise, it's huge. How strong are the race conditions in quantum computing? Because presumably, it's nothing as intense as the AI world, right? Yeah, well, okay. So there certainly is a race. There's a race between the different hardware approaches, like superconducting, trapped ions, neutral atoms, photonics, which one will get there. It's possible that multiple of these approaches will work. And then it's a question of which one gets there first, which is the most economical, and so forth. There's also competition between countries, right? So the US in 2017 passed something called the National Quantum Initiative Act, which was spearheaded by then Senator Kamala Harris. I visited her office while they were writing it. She sent regrets that she couldn't meet me because she was at the Kavanaugh hearings. But I met her staff, and this bill provided about a billion dollars for quantum information research. It was passed unanimously by Congress. What does Congress do unanimously anymore? Only something that neither side understands, right? But what they did understand was that China was doing something in quantum, and that the US had to beat China in the race for quantum, right? So one of the misconceptions about quantum computers is that it's just an amazing computer that can do all sorts of things, while it's actually for a specific set of tasks. Can you describe the types of tasks? This is a horse that I've been flogging for 20 years, or a boulder that I've been rolling up a hill, right? The narrative that sort of took hold very early on in popular writing and also in the business community and the investment community was that quantum computing is just a magic accelerator of everything. It's just the next stage in the evolution of computing. I think people just interpreted the word quantum to mean really awesome, right? And in particular, you know, starting around 15 years ago, you know, a narrative took hold that, you know, what quantum computing is really going to be good for is speeding up, you know, AI and training neural nets and optimization and finance and all these important tasks for industry, right? And of course, this is exactly what VCs want to hear. This is what CEOs want to hear. The only problem is that it doesn't match just about anything we've learned about quantum algorithms, you know, from all of the research in the subject. So what we learned in quantum computing theory, sort of starting in the mid 1990s, is that a quantum computer really would give you dramatic advantages over any known classical algorithm, but mostly for a few very special tasks, right? Maybe the economically most important thing that a quantum computer can do is just help you simulate quantum mechanics itself, right? Now, that may sound esoteric, but that's actually useful for anyone who's designing new materials, new chemical reactions, you know, new ways to make fertilizer, batteries, photovoltaics, you know, for drugs, you know, that have to bind to some receptor in a certain way, right? These all involve many body quantum mechanics problems. And, you know, it's not obvious that a quantum computer helps with them, because there's also extremely good classical heuristics for all these problems that the material scientists and the chemists have been forced to develop over, you know, many decades because they only had classical computers, right? And now, you know, you have to compete against all that stuff and be better than all of it. But, you know, I think it's plausible that a quantum computer will give you wins there. And, you know, even if there are only a few wins, they might enable billion dollar industries. So I'd say that that's the biggest economic thing. And then there's a second really huge application, although it's not really clear that it's a positive one for humanity. And that one is breaking almost all of the public key cryptography that currently protects the internet, okay? And this comes from a famous discovery by Peter Shor 30 years ago. He was then at Bell Labs, later I was a colleague of his at MIT. But Shor showed that there is a fast quantum algorithm for finding the prime factors of a huge composite number. And that's what like all RSA encryption is based on? Yeah, that's what RSA encryption is based on. If you can solve that problem quickly, then you can break RSA. Now to loop back to what we were talking about before, there's this giant blob of problems in P, and then there's this giant blob of NP complete problems, right? And P versus NP asks if they're the same. Factoring is this odd one out. Okay, factoring is an NP problem that we don't know to be in P, but we're also almost certain that it's not NP complete. So what would it be in? Well, it's called an NP intermediate problem. Or at least we think it is, right? It's somewhere in between. So it seems to be an NP problem that is, as far as anyone knows today, at least in public, seems hard for a classical computer to solve. That's why we use it for cryptography. But it has very, very special structure that seems to prevent it from being NP complete. Okay, just one example of that special structure. If I give you a jigsaw puzzle, a priori, it might have no solution, or it might have a hundred different ways that you could solve it. Imagine that there's no picture on it, for example. But if I give you a huge number, you know for sure that it has one and only one prime factorization, because you could prove that in 300 BC. So the special structure of factoring that comes from number theory and group theory is actually essential to why it's so useful for public key encryption. We don't know how to base the public key kind of encryption, the kind that we use on the internet on any NP complete problem. We do know how to base it on factoring. But then what Shor showed was that that same special structure of factoring enables a quantum algorithm to solve factoring. That turns out to be true for a bunch of other problems in number theory that we also use in cryptography. To the point where it was a challenge to identify public key cryptosystems that are not broken by quantum computers. Today we have pretty good candidates for that. There is a push right now to migrate to what is called post-quantum encryption or quantum resistant encryption. Which would be encryption methods just running on our same conventional computers, but that at least as far as we know would be resistant against quantum attack. Yeah because this is surely like a new arms race therefore. But it's an arms race where you could say in principle we kind of already know the solution. The solution is for everyone to upgrade to these quantum resistant encryption methods and you know assuming that all goes well then we're all just back where we started. You know the main issue is it's a huge practical headache to upgrade every router and every web server in every browser in the world to use these quantum resistant cryptosystems. Those are the two most obvious applications of a quantum computer. Simulating physics and chemistry at the quantum scale and breaking public key encryption. And then there's everything else. There's you know all these problems in optimization and machine learning and combinatorial search. So you could say the bread and butter of computer science. And for these tasks our expectation is that quantum computers will only give you a more modest benefit. So they're not going to solve problems in polynomial time that take exponential time classically. They might reduce the order of the exponential. Okay and so in particular most of us believe that there is not a quantum algorithm to solve NP complete problems in polynomial time. We can't prove it just so you know I mean we can't even prove there's not a classical algorithm to solve them. That's the P versus NP question. But it would have to be radically different from any quantum algorithm that we know. So you know even quantum computers seem to have limits. Which you think we're likely to get to first? Simulating quantum mechanics or breaking encryption? I think usefully simulating quantum mechanics will be first. Awesome. How many qubits would we need to choreograph such that we would be able to do either you think? These are all kind of fuzzy questions right because you know the real I mean people are already doing things with quantum computers that are cool and interesting. But it's just you have to squint to see okay did you actually get any benefit compared to what you could have done with a classical computer right? It's when you ask that question that things always get tricky in this field. But I think that if you had you know let's say 200 qubits certainly if you had 200 logical qubits you know that were error corrected then you can already do quantum simulations that are going to be scientifically interesting. That are going to be interesting to material scientists or chemists or people like that. Maybe maybe if you got lucky they would also be commercially useful which is kind of a higher bar to clear. Okay you know if you had thousands of physical qubits then almost for sure I should think you could do things that would be useful to certain industries. Okay for breaking RSA encryption you know you're going to want you know several thousand qubits that will definitely have to be logical. They're definitely going to need error correction and then you're going to need millions of operations on those qubits. So we would actually have a bit of run-up to the RSA encryption functioning on a quantum computer which is why we would also potentially have sufficient time to upgrade all of our cryptography to quantum resistant ones. I mean I would say that anyone who is really worried about their data staying secret for the next decade they should already be. Well I mean that's why I wonder about how it affects the race because Iran is worried about the U.S. getting there and U.S. is worried about Iran and China and everybody else getting there as well right. Now I don't want to overstate you know the national security importance of this too much because the truth is that like usually like people build these fortresses right in cyber security and usually the way that you get into the fortress is just by finding a screen door in the back that was just left totally unguarded right. Or convincing a person to just let you in. Exactly so usually in practice the way things are broken is that you know there was some memory allocation bug in some you know level of the software stack and you know you pay people at the NSA or the GCHQ or you know unit 8200 or whatever and they find those mundane vulnerabilities you could call them right. But you know one thing that we learned a decade ago from the Edward Snowden revelations right is that the NSA does have a big item in its budget for you know spending lots of compute to break cryptography in a way that looks exactly like what it would be doing if it were just breaking 1024 bit RSA and Diffie-Hellman you know using how much money it would cost to build you know to build super computers just for doing that right. And so that that suggests first of all that you know at least as of 2013 you know they don't seem to have had a quantum computer in their basement or a classical factoring method that vastly exceeds what was you know what was known in the open world right. You know or you know of course it could all just be a giant cover story and you know you know you can get as conspiracy theorist as you want about it but you know if you believe that this was really their budget and what they were spending it on that it seems like yeah they pretty much knew the kinds of factoring methods that we know in the outside world and yes if they had a quantum computer then they could speed that up. But you know I would say you know people sometimes want to compare quantum computers to nuclear weapons right. I think that that's that's not a very good analogy. I mean you know for one thing the quantum computer doesn't directly kill anyone unless like the dilution refrigerator tips over onto them or something. But secondly you know having a quantum computer that breaks encryption is mostly useful if no one knows you have it right. It's a little bit more like Bletchley Park than like the Manhattan Project right. Like once everyone knows you have it then they all just switch to quantum resistant encryption systems or you know their motivation to do so has then enormously increased. Whereas with a nuclear weapon it's just the opposite. You want everyone to know that you have it but hopefully never have to use it. Some people will wonder also about the bitcoin related question here. I think bitcoin uses a different algorithm in part. So with bitcoin there are different components of it okay but bitcoin uses a signature scheme which I believe is based on elliptic curve cryptography which would be breakable by a quantum computer okay. So the the digital signature part of bitcoin as currently implemented is vulnerable to quantum computing. Now that could be fixed. Any zero knowledge proof is breakable. Well not any zero not any but the current ones sorry the current ones that we're using currently used ones yeah. Now if bitcoin made a decision to fork to some quantum resistant encryption then you know that could fix that problem okay. I actually just recently met with the Ethereum engineering team which wanted to know like you know like basically how much time do we got doc was sort of the question right and should they be worrying about post-quantum encryption and I think you know quite plausibly yes right. But then what a lot of people think about when they think about bitcoin is the proof of work right. You know that's like you know the thing that actually hogs some like appreciable fraction of all the world's electricity on this sort of useless you know trying to invert a cryptographic hash function and for that component of bit of bitcoin we think that a quantum computer would only help modestly. It would only give one of these polynomial improvements or what we call a Grover improvement which so that's like if you have a problem that classically took you about n steps then a Grover improvement lets you solve it with a quantum computer in only about the square root of n steps okay. So that's an improvement but it's not an exponential to polynomial kind of improvement and now the interesting thing is you know the way bitcoin works the hardness of the proof of work is just set by you know the total computing power in the world that's currently being used to solve these puzzles. So in a world where everyone had a quantum computer all that would happen is the protocol would automatically adjust to make the proof of work that much harder and we'd all be back where we started. So the proof of work as a means of security would actually then persist having the same property as it was before. Now if only one person had a scalable quantum computer and no one else did then that person could get very rich mining bitcoin. If it was a big enough and fast enough quantum computer it's actually quite a while before these Grover speed ups become a net win in practice. Now you're back full-timer UT Austin. I'd love to get your sort of status report on the state of academia in terms of you know from my very twitter brained perspective it feels like you know university campuses are a very stressful place to be if you are especially as faculty or and students people are always self-censoring and there's this culture of you know you must have certain political beliefs or you get shut down. At the same time I'm hearing rumblings that things have maybe peaked in that regard you know we're past peak woke or whatever you want to call it and things are becoming a little bit more moderate again and and uh people are more comfortable to speak. How have you what have you noticed have you do you agree with that trend or is it still a problem? It is something that concerns me greatly. I'm probably not the best person to ask just because I live most of my academic life in a bubble right. I live in in a sort of bubble of you know mathematicians, computer scientists, physicists you know and and and and and sometimes also I will interact with with historians or english professors you know especially if they're their parents of my kids friends right uh but you know like the the ones I'll interact with obviously you know will not be the ones who would you know refuse to speak to me because of you know heterodox beliefs or things like that they would be you know the uh the more open-minded ones uh so like I'm kind of insulated from this stuff uh except insofar as I blog about it right and anything I blog about I will hear from whichever people on earth are the most angry about that thing right. So you get a lot of comments oh yeah oh yeah yeah so so look I mean I mean in you know I I did see the articles you know arguing that like by some measures wokeness seems to have peaked around 2020 or 2021 and now you know back you know maybe it's back down although although still kind of at a high point compared to maybe you know where where it would have been you know when I was a student you know 25 years ago or whatever um I mean uh you know within the last year uh uh you know it has been a stressful time for academia you know we've seen universities basically taken over by you know uh the uh uh gaza protesters uh you know we've seen you know uh you know and and and I think that like like you know the a lot of these things will be adjudicated in court right because there there are laws like uh uh title six right that like uh uh that you know if uh you've created an unsafe environment for certain students you know you may have run afoul of those laws right and then you know there are big questions about free speech uh you know many people pointed out the irony yeah yeah yeah right right right many people pointed out the irony that like there were uh um uh academics who were always like uh uh very dismissive of of of free speech they like wrote it as freeze peach you know to just sort of ridicule anyone concerned about it and then you know during the gaza protest they suddenly rediscovered the value of free speech and became the biggest you know first amendment absolutists right so I think it's very very important that we come up with some viewpoint neutral rules and then actually enforce those rules in a consistent way right and you know we ought to I think uh Steven Pinker for example wrote some very nice things about this that we ought to start with just what is the purpose of a university right uh uh you know the the the purpose is to you know have a place where ideas can actually be debated rather than just screamed and and you know rhyming slogans right and and so that means you know you want to attach very very high protection to you know people who are presenting you know ideas that other people might find offensive or or um you know even even harmful or or uh or things like that uh but you know what you don't necessarily want to protect protect is you know shouting down a speaker who you you know disagree with or blockading a building right these are things that that don't actually advance the discussion and debate of ideas on their merits in fact quite the contrary their their you know their their purpose is to shut that down right and so I think that it's good to come up with rules about these things that that are that are totally content neutral and then you know the the the hard part as we as we've seen in case after case is to actually enforce those rules when when when when when people flout them it's I mean in part is you know coming back to as you said it's like what is a university for and the trouble again is that there are always these competing incentives like what the the incentives of the dean in theory should be aligned with what the university is for but at the same time they have all these smaller ones of keeping the students coming in you know like uh performance metrics etc are there any specific sort of area like what would you do if you were a dean and a dean for a day uh yeah I think maybe the first thing I would do is resign and not be a dean anymore okay that's my loud part of the game see you're like specification gaming me here ai back in the box yeah no I am incredibly grateful to the people who do this and who sort of fight on the side of truth and justice you know I have met such people I could never do it myself right right but it's like you need such people doing this you know to uh uh oppose the bad people who are who are who who are doing this stuff right um but uh uh uh look you know I would I would do my best to try to uphold the values of a university you know as as I see them of you know the the dissemination of knowledge and uh you know the uh uh open search for truth enlightenment values yeah essentially yeah the values of the enlightenment yeah is there something to come that comes to mind or to kind of align an aspect of how the university currently works so that students kind of get more out of it or that students overall achievement across those metrics um is more aligned with say like the professor's incentives or the yeah faculty I would I would love to see uh admissions you know based on merit you know meaning based on you know things like standardized tests uh and you know I actually you know people complain about this constantly they say standardized tests can be gamed but then the part that they never want to say is that they seem much less gameable than all the other stuff that we currently use instead of that so right so basically what we have done for any you know any high school student who wants to get into an elite university like we have forced them to sort of redesign their entire teenagerhood around this sort of beauty pageant this sort of competition of optimizing their attractiveness to these admissions officers right in in ways that are extremely gameable right that are that the richest and most well connected parents are the most able to take advantage of you know uh and it is very opaque you know it's never clear you know and so so which means that there's a huge advantage to those who are most in the know right it is uh uh very you know there there are like unlimited opportunities for the admissions officer's personal biases to to to get in and um you know I think you know um ironically if you look at let's say Europe right which you know like people on the left are always you know pointing to Europe as you know as you know better than the U.S. right you know the uh but uh in most European countries it's just based on a standardized test score you know and uh that though at the same time the universities in the U.S. seem to be of higher quality like probably top 50 universities are like I don't know but 30 American ones probably or something right so so there's a there's a question of uh you know is that is that because of this opaque admissions process or is it despite it right uh and you know and you can see historically what what's what's happened right like like uh once the SAT was instituted you know in the um uh like not 1910s 1920s you know what happened was that like uh Harvard Princeton and Yale got like enormous numbers of Jews right and this was seen by them as a huge problem and so that is why they designed this holistic admission system okay this is you know that this this is a matter of historical record right we uh we we know all of this now right that uh uh you know those three universities decided you know we have to look for well-rounded gentlemen who do you know rowing and who do uh gentile who were yeah yes yeah it was you know almost all men at that point of course but also gentile yes yes but you know you know young men who've been brought up in the proper way and who you know know how to conduct themselves and you know and they they presented this as a matter of well-roundedness and and uh but uh you know if you look at their private deliberations it was all about getting down the number of Jews okay and then you know and then what I should say what happened was you know over the decades you know Jews you know in America learned to game the system as well as everyone else and you know at some point it no longer worked for keeping down the number of Jews but then it started being you know used for a different purpose to keep down the number of Asians right and that was the heart of this supreme court decision from the last year the students for fair admissions decision that basically they you know they at least to the satisfaction of the current supreme court they proved that Harvard you know was was uh uh was discriminating against Asian applicants so I think you know I would I would try to have you know admissions be more merit-based you know which doesn't mean it has to be all standardized test scores I mean you can look for students who are you know really extraordinary in one area right whether that's uh uh uh uh athletics or starting a company or or or or whatever it is uh but uh but not this sort of checklist of of beauty pageant things right that uh that you know only the richest and best connected students are able to do reliably so so that's more measurable tests of the time that you can find them across uh even yeah but not uh like a I had all of these extracurricular outside of school activities that you can only do if you have time and resources available to put your students in I mean you kind of sounds like you're trying to actually draw upon like some alignment principles like you want more interpretability I guess you want more visibility into the admission system yes more valuability yeah I want to you know get rid of the uh incentive get the the specification gaming or uh uh whatever you call it uh so yeah so that that you know if you only let me do one single thing then maybe it would be that uh just because that you know it's so important for for for for a way to rate often as Brian Kaplan has documented for for example right the the the most important you know thing that Harvard or Princeton for example does for a student is to admit that student right you know everything else is you know uh we like to think of it as important and maybe for some students it is just being admitted like having that on your record saying I got an acceptance I got accepted by this ivy league that is really the bulk of the work it's not entirely that because you know people can't get hired nearly as well if they just show their Harvard acceptance letter but then they never actually went to Harvard right like employers want to see that okay you actually had the sort of uh enough uh I guess uh yeah you had you had enough grit and enough conformity to actually do it to actually go there for the and you've gained from the social networks etc that comes now with you as a package I mean right which which is not you know completely absurd right like like uh you know these things are really hard to measure right how do you measure someone's grit or their but uh but but yeah that that might be the the uh uh a single thing that I would do and then um let me let me think um no I mean I mean I I feel like you know in general for all the problems that universities have and you know like we could speak for hours about you know I could draw on all my experience of everything wrong with universities I feel like they are in order of magnitude less broken than k-12 schools are right than than pre-college is right and you know like what I would do I would love to make high schools and junior junior high schools operate more like universities where students can proceed at their own rate where they can choose what courses to take you know where they can specialize in things that interest them so you'd inject more optionality within exactly exactly how competitive were you when you were a teenager um in terms of what were like would you have described yourself as a competitive person or if if not like were there specific ways it manifested yeah yeah I was uh I feel like I was I was driven by sort of an intense sort of burning desire that like I have to uh do something in the world right I have to you know either do some scientific research or some writing uh you know that if I don't do that I am a failure right so so I was definitely competitive in that sense right now in terms like an internal goal was it like a sort of there was an specific external thing you wanted to achieve or is more just this like general well I I don't know how to differentiate that I I I feel like you know I like like uh if if if I don't make a difference you know in the world or sort of do something interesting then I have let down all the people who who thought that I would do that and also I've let down myself right so so so I definitely had that you know and by the way I don't feel that nearly as more as much as I used to and I don't know if that's good or bad right because like I I you know on a on a day to day like it's it's you know I'm I'm less stressed about needing to prove myself but you know on the other hand it was that you know urge to prove myself that led to my doing a lot of the things that I did and uh you know that do you think you sort of scratched the itch that's yeah I mean you know I mean I mean maybe part of it is just is getting older you know being married having a family uh it just uh changes your your priorities and you know and and and part of it is is you know I feel like okay in in certain domains like I you know I I proved what I wanted to prove and you know but um you know but but again maybe it would it would ultimately be better if I didn't feel that way right if I if I still you know felt like I had more to prove than I would work harder and I would I would I would do more uh interesting things uh but you know I I don't think that I was hyper competitive in terms of math competitions or programming competitions or things like that I did those uh I did okay in them right but uh you know I never like achieved any national status in those things and and there's kind of an interesting reason for that which is that I I left for college when I was 15 or well at least that's one reason right so I I left uh it's a complicated story but I I left high school early and I uh went to a place called Clarkson in upstate New York where I could get a GED uh and then uh um you know I went to Cornell after that which was like the one you know uh one of the only places nice enough to admit me with this uh a strange record but um uh but you know like I like I feel like if I were optimizing for just you know competitiveness and proving myself then I would not have done that I would have stayed in high school for the full time you know in order to uh try to you know maximize my uh whatever you know competition scores chances of getting into Harvard or MIT or whatever and that's not what I did I said you know uh I'm not happy here I think that I'll be happier in college uh I want to be learning what what they what what they teach in college and so what an opportunity arose then I seized it yeah so I mean I mean also in many on many metrics you are winning by going to college age 15 being able to go to college at age 15 uh is I mean the inner psycho competitor within me would have definitely been like that's a cool achievement like it would have been very would have felt like a tick box for sure I've beaten my friend stepped out of the game and now that's a kind of a winning achievement in a way as well and it yeah um I mean you didn't focus on the short-term competition I suppose did you when you went to college did you immediately focus on computer science or were you kind of just searching I did I did yeah so so so so so at that point uh I knew that I wanted to do computer science uh you know I mean I had been drawn into it by just wanting to make my own video games when I was uh when I was 10 or 11 uh and then uh uh eventually I realized that you know even though I loved programming you know I just you know learning what programming was was like learning where babies come from right like like why didn't anyone tell me about this before right and it was just it was it was revelatory right but uh what I didn't like and what I wasn't good at was software engineering like you know making my code work with other people's code and learning some whole framework and getting it done by a deadline and documenting it like uh you know and I realized that I would never have much advantage there and I got more and more uh nerd sniped you could say by the theoretical side of computing uh and you know I think you know my my my dad may have been a little disappointed you know I think you know this was during the time of the first internet boom and you know he would keep pointing out like look at this you know person you know only slightly older than you he just sold his company you know 500 million dollars does that you know does that appeal to you and okay you know maybe like there's a different branch of my life where I would have tried to start a software company or something but uh in in in this branch I got I got nerd sniped by the uh by the theoretical side of computing uh I was was there a specific idea that you found beautiful at the time that like I learned about the p versus np problem when I was 15 and you know that that blew my mind uh I spent a month you know thinking okay surely all these these experts have just you know got you know made it all too complicated surely you know I'll just I'll just uh without without their preconceptions I'll just sit down and solve it you know I think it's good for any computer scientists to have that experience at least once in their life so that afterwards they can understand what they were up against right but uh uh you know and then especially for a like pretty gifted child it's a good thing to get to a point where you just can't talent your way through and it's just so hard that you have to work on it and work on it and work on it no and and also I mean look you know by being three years younger than people you know going to college right I knew that I was putting myself at a competitive disadvantage compared to you know who my classmates would be right but that was what I wanted I wanted to get as quickly as possible into an environment where you know I would be struggling to keep up with other people which would mean that I could be learning from those people okay and um so yeah and and um uh I was very interested in AI at the time you know this was like the late 90s I uh uh worked with an AI professor at Cornell named Bart Selman who was a an incredible mentor to me uh but then at the same time I read something about quantum computing which was you know fairly new at the time and and my first reaction when I read about it was like this sounds like garbage you know this sounds like physicists who just have no idea of you know the enormity of you know of of NP complete problems or what they're up against or what was it because it didn't seem useful as well no no no it was because it seemed too useful it was because you know it's the way that all the popular articles want to describe it is a quantum computer is just this magic machine that tries every possible answer in parallel right yeah it's just this panacea and it just sounded too good to be true sounded like surely that can't scale but then I had to learn what quantum mechanics was to be sure right and uh amazingly quantum mechanics turned out to be much simpler than uh I had thought it was once you take the physics out of it once you see it as just linear algebra and so you know and that was a new field and that kind of there was a lot of low-hanging fruit and that and that enticed me did you have any rivals during your teenage or early university times that drove you or now in academia I mean I mean sure uh but I mean I mean the the the the best kind of rivals were the kind that you know you you actually become really good friends with right the kind that like you you aspire to to do you know to to to do the kind of things that they're doing or even half of what they're doing right and then you know maybe at some point you start collaborating with them right uh and and and and I would tell you know I did have you know quote rivals of that kind who were who were very important in my career right and uh um um I mean I mean I mean much earlier I guess you could you could you could talk about like rivals who I who who I really didn't like but uh we can we can we can leave those aside yeah do you do you see someone like Roger Penrose obviously he's you know different generation yes um working on different class of problems but he feels in some ways almost like a like an uh intellectual rival and like some of his theories I notice I just noticed he's someone who you clearly like deeply respects and whose work you sort of build upon but you're often sort of uh butting heads with in in in theory space so first of all I'm not going to compare myself to him that would be you know Penrose is Penrose absolutely yeah there is there is there is only one of him and uh you know I I did have the opportunity to talk to him a decade ago and you know it was uh uh you know it was it was amazing to uh talk to him to you know hear all his stories about you know uh uh learning from from Dirac in the 1950s and so forth uh I would say it did not bring us any closer to agreement about you know these these questions of AI and and microtubules and so forth right um that we know of for now uh yeah so but uh uh uh so so reading his books like the emperor's new mind uh you know when I was 13 years old I would say that was very influential in my development because you know even at that time I was skeptical of Penrose's arguments I thought that you know he is he is begging the question of you know uh uh you know and uh like I didn't I didn't really see his arguments against AI as being sound but the questions he was raising like my god right uh uh you know that was like at that time that was like one of the only popular books that was talking about you know quantum computing about is the mandelbrot set computable about you know is uh what are the laws of physics that are relevant to the brain right and so so that that that that that certainly helped set the intellectual trajectory of of the rest of my life so uh daniel phong asked um asked to ask you send her my regards whether um there is something from uh complexity theory that would bound the potential scaling of neural nets in terms of like it's yeah it's some issue coming up at later stages yeah so it's it's a good question it's one that I get a lot and uh unfortunately for those who might you know uh hope that complexity theory will stop AI from taking over the world or anything like that I don't see any principles in complexity theory that would that would block that right and the key is that like it is true that complexity theory puts fundamental limits on efficient computation right right like like if p is not equal to np for example then you know there will not be a fast algorithm to solve many of the optimization problems that we care about or to find uh short proofs of theorems uh whenever they exist or things like that okay but but now now the key is to ask well can humans do those things right right and and so like if if you're a sword in the stone test you know about an ai is that like you can't immediately find a proof of the remont hypothesis well that's great can you find a proof right immediately find a proof of the remont hypothesis right and so so you know you know that this this joke about like the the person saying well you know I don't have to outrun the bear I only have to outrun you right right uh and so so it's it's much the same with with ai and humans right the ai doesn't have to outrun the fundamental limits of computation it only has to outrun humans right and that you know you could you could even say look if you really believe that our brains are governed by the laws of physics you know either of classical physics or of quantum physics then for that very reason our brain should be computational systems that that are are subject to the same limits uh the same complexity theoretic limits that that the ai's would be subject to right and so so that that's kind of the fundamental difficulty with using complexity theory to to reassure yourself in any way about ai for what it's worth i'm not even i don't think she's actually that worried necessarily uh though uh yeah it would bound kind of both possibilities i suppose and it's in in neither case do we have any yeah now look i i i happen to be a big fan of complexity theory and i happen to you know be looking as a significant part of what i do for places where complexity theory can help us understand ai better right and i think one of the big places where maybe it can help is in uh illuminating what can we hope for with interpretability right so like like among all the properties of of neural nets which ones can you hope to figure out efficiently by looking at the weights so like could you hope to figure out if this neural net has a back door where you know under some secret input it will you know go berserk and start stabbing all of the humans right can you can you then efficiently find that input or could that be a cryptographically hard problem right uh you know even if some even if some interpretability tasks are are np hard or cryptographically hard or whatever uh could we say that there are sort of more generic kinds of interpretability that are doable efficiently so a former student of mine named paul cristiano uh from mit who is now uh has now uh you know he he left quantum computing uh you know and uh to do this at the back in 2016 crazy sounding thing called ai alignment and some organ some new organization called open ai right uh but you know of course he then became one of the world leaders in ai alignment and he is asked absolutely beautiful questions about complexity theory and interpretability uh you know that that i think would actually tell us something and they're they're crisp questions like they you know they have a yes or no answer that we you know we just don't know it yet and actually paul and i have opposite guesses about what the answer will be but but you know one of us will be right right and so so so i so i am excited about what complexity theory can do for interpretability you know maybe for other parts of ai safety also uh but i wouldn't use it to reassure ourselves that to reassure ourselves that that you know ai will never become super powerful because the things complexity theory tells you it can't do are things that plausibly we can't do either absent uh us understanding how conscious consciousness works in humans uh much better than we do now is there something like what would need to be the case such that you would personally assume that an ai has consciousness like what would be a test that we could run or like what do you think yeah that's a very hard question uh i think that uh you know you possibly even the hardest of questions uh and that you know it is totally unclear what empirical discovery could possibly uh uh let us answer that okay but you know if i'm just going to speak about my personal intuitions uh you know i think that my intuition is affected by ephemerality and unclonability you know these things that we talked about earlier and i think that my intuition is also maybe affected by you know what does it say about its own consciousness right and now the hard part here is that you know gpt for example can discourse at great length about consciousness at least if it hasn't been trained not to you know if it hasn't been rlhf out of it uh but we don't read too much into that because we say okay well it's seen all kinds of discussions about consciousness and its training data so it's probably just you know recapitulating stuff that it that it's heard and and in fact like if if an ai starts talking about well well uh gosh what you know when when when no one is interacting with me i feel lonely like you know we can in some sense we know that's bs right because you know uh when no one's talking to it no code is being executed right uh but uh uh you know i'm ilya satskofer you know and others have uh suggested an experiment that that one could do which would be that you would train a language model on on training data from which you had uh meticulously excluded any mentions of consciousness or of sentience or first person experience or anything like that okay and then having done that you would try to engage that lm in a conversation about its experience and if it could then intelligibly talk about its experience then you know maybe that like that would that would cause an alarm to go off like you would say you know maybe maybe there was something here that's emerging yeah yeah that is emerging that we need to understand better would you find such a test um potentially like a bit convincing i can't name any one test that i would find decisive by itself i could only name things that would affect my intuition oh i suppose yeah the point is also not decisive but at the point when we're like well now i can see how it does have some consciousness i mean okay okay i mean i mean i mean another thing that i could put forward you know if the ai is is you know not just solving competition problems but it's it's writing research papers it is you know it is putting me out of work i said well you know i'm fortunately protected by tenure right but you know but you know if if it could have put me out of work right uh if it uh uh if it can write you know not not just any songs but like great songs or you know or or you know essays that could be published in the new yorker or whatever right then uh uh you know i i think at that point you know touring's questions from 1950 of why are you discriminating against these things against this thing uh you know they really start to have teeth to them the way i like to finish up all these recordings is to ask people a series of rapid fire predictions oh god yeah and don't you know i want this to be a system one as possible whatever your intuition says um so first one probability that ai reaches international math olympiad gold medal level by end of 2025 80 percent probability that p does not equal mp 97 percent probability that a quantum computer breaks rsa encryption by 2030 depends on how big of an rsa key we're talking about but you know 2048 bit or something let's say 50 percent and 2040 80 probability that we'll have agi and agi defined as ai that matches human performance at most economically relevant tasks by 2030 60 probability that roger penrose's uncomputable consciousness is the right path to go down for consciousness so you mean probability that there are any uncomputable phenomena that are relevant to consciousness yes depending what one means by uncomputable phenomena i could go as high as 40 percent and lastly probability that covid was a lab leak so i was much higher until i read the root claim debate uh so you know i i think i was above 50 percent and i am now down to uh maybe 15 percent oh wow awesome thank you so much yeah yeah

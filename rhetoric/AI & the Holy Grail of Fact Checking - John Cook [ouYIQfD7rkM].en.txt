all right well let me give you a quick
introduction to myself and what I do um
firstly a big qualifier is um my
background is not computer science or
software engineering or um or or
anything technical like that I'm
actually um come from a background of
cognitive science and psychology that's
what my research is uh and my research
has all been focused on tackling the
problem of misinformation
through a psychology lens
but trying to tackle that problem
inevitably led me to collaborating with
computer scientists political scientists
critical thinking philosophers people in
a whole range of other disciplines all
working together trying to come up with
complicated interdisciplinary Solutions
so it's it's really taken me out of my
my um I guess my comfort zone outside my
land in a way working with people in
other areas and one of the main things
we've been working on in the last couple
of years has been what we call in my biz
in the misinformation research world the
Holy Grail factchecking which is
automatic detection and debunking of
misinformation in real time that's
that's the the ultimate that's what
we're we're aiming for one of the
reasons being um that misinformation
spreads faster than accurate information
like it gets shared more it's more
interesting usually because it's not
bound by the truth it gets shared more
deeply and once it gets out there and
spreads out it's really hard to undo the
damage and there's a lot of psychology
Research into why why that and I'm happy
to get into that later on if people have
questions did you have a question Adam
yeah I mean you might have it in the
slides I'm just wondering if you can
Define misinformation and
disinformation yeah so I I usually use
the term misinformation which broadly
speaking is just all all claims that are
that are false that are not based on
fact and then disinformation is a
subsection of that which are false
claims that are intentionally meant to
deceive um whereas misinformation could
be intentional or it could be people
genuinely believe it but they're just
spouting wrong stuff so so
misinformation is kind of the umbrella
then you have disinformation in
underneath that um and and again this is
part of my work has looked at how um
from the outside misinformation and
disinformation are really hard to tell
the difference of to to tell the
difference between
because um the techniques used in
disinformation like intentional
techniques like cherry picking or um or
uh using um fake experts those
techniques are the same techniques that
people use if they genuinely believe
something but they're just biased so
they cherry-pick because of confirmation
bias or they rely on fake experts
because they're telling them the things
that they want to believe so from the
outside people are using these same
techniques and you you don't you can't
tell the difference between intentional
deception and self deception so
practically I tend to just use the term
misinformation um because of that
practical difficulty but I'm kind of
digressing already you've already got me
going off on a side Sid track there Adam
so um
yeah sure yeah so if there if the this
disinformation or misinformation you
must know the
truth right I mean that's a good
question too and and this is another
Sidetrack but um so how you get the
truth right and I've focused on climate
misinformation so I I'll speak to that
topic because that's kind of the shower
into of the pool it's there's a lot of
stuff there that is a lot more factual
and easier to fact check or debunk
compared to policy issues or is societal
issues where there's cultural values
that's that's a lot more difficult so I
still clear of that stuff but um but but
when you get to um issues like climate
change a lot of it is just scientific
consensus settled science stuff that
we've known for decades um but then even
within climate change there are there's
like policy arguments or or other issues
where it's a little bit more murky and
difficult um and I don't get into this
much sure at all really tonight but um
the way that we've been tackling that
particular question um and this is I
think a general principle uh how do you
fact check something when it's hard to
know the facts uh the way we approach it
is um we look at claims through a
critical thinking lens
and going and you can use critical
thinking to systematically identify
whether th a claim or an argument
contains reasoning fallacies logical
fallacies or rhetorical techniques that
are misleading uh and so so just
identifying misleading techniques in an
argument um for us is sufficient to flag
information as potentially problematic
so it kind of sidesteps your issue in a
way because sometimes you can't uh and
so anyway I'm kind of going down a rabit
how him yes how you accumulated all this
like flags for it's like SP span
filtering something like that
categorized it is it's kind of like well
here is a red flag this is this is using
this particular technique and okay and
so and part of what we do is is ident
using AI to to do that so let me jump
into the presentation and and we'll and
hopefully hopefully we'll come back
around circle around to doing it now
this B because I my background is
psychology and I began my career and for
many years was just running psychology
experiments where we were testing
different ways to debunk misinformation
or pre-b buunk misinformation just
testing different messaging formats and
what is the most effective way to
actually undo the damage of
misinformation and um then I don't know
a couple of years ago I got contacted by
some computer scientists from EXA uh
from the UK and so they were um
working on a a machine learning way of
analyzing climate misinformation but it
was a what they call unsupervised
machine learning so it's not uh it's
just basically topic analysis it's just
saying let's look at all you know
hundreds of thousands of papers and just
automatically extract what are the
topics that climate deniers are talking
about and so I i' looked at their paper
but then I my response was this is
useful academically but what would be
really cool and practical in the real
world would be I didn't say the real
world because that sounds a bit sticky
but um what would be really practical
would
be um training a machine to detect
specific misinformation claims because
then and because I was thinking holy gra
like then if we could detect
misinformation claims we can then detect
and debunk uh and and uh and be able to
address misinformation as it comes
online and so we worked together for a
couple years on a paper which we
published in 2021 where we trained a
machine learning model to detect climate
misinformation claims um that's it uh so
and to your question on what is PED or
not we completely dodged that we um we
were explicit about that in the paper we
said because we started with a corpus or
or a data set scraped from climate Den
blogs and conservative think tank
websites which are two of the most
prolific sources of climate
misinformation but all we were doing was
was documenting what do they say what
are their claims we were not determining
whether those claims were true or not
factual or not um it was just about
detection and so the first step to do
this was we had to develop a taxonomy of
what were the types of claims that they
made on these websit
on contrarian claims we called it and we
found that there were five categories of
claims coming from climate deniers the
first three were science-based climate
change isn't real it's not it's not
caused by humans it's not bad the fourth
category was about climate Solutions
saying that they won't work and then the
fifth category was just attacking
science attacking um either climate
science attacking climate models or
attacking scientists themselves um just
personal attacks on
scientists and
um and so we once we developed this
taxonomy we then took I don't know about
28,000 randomly selected examples from
our data set and then just went through
the process of looked at each text and
then mapped it to a claim and then
repeat and repeat until we had um
created a large enough training set that
our model um performed well in
accurately detecting and categorizing
misinformation claims yes hi yeah um
just just out of Interest did you have a
statistical proportion of like Adon
attacks versus uh climate denial versus
other bits it it would say a lot about
the psychology of the people that you're
mining the data set from which which of
these categories were the most common
yeah uh got a side for that actually so
CU because once we had trained our
machine then then we were able to put 20
years of of misinformation claims into
our model and it spat out a time series
basically it spat out a history of
climate misinformation claims and we
found that the category five attacking
Science And scientists was was one of
the biggest
categories and but really and that was
the one of the things that jumped out of
this but the other thing that jumped out
of this was the increasing Trend towards
Solutions arguments now the reason I got
into this was originally I was just
debunking all the science arguments the
you know Global warmings caused by the
Sun or or it's cold therefore global
warming isn't real those kinds of
arguments right um and it turns out
those were actually a small proportion
compared to attacking science and and
solution
arguments uh so this was in conservative
think tank websites when we looked at
climate denial blogs um science is
unreliable was even was the highest it
was was well above the others and we
recently just did uh an analysis of
Twitter climate misinformation I don't
have a slide for it but science is like
well it's like 40% of all misinformation
tweets about climate change uh ad
hominum attacks on scientists and then
the next 20% are conspiracy theories so
more than half of all tweets about
climate misinformation tweets are
attacking scientists or conspiracy
theories so it's the my sense is the
shorter the the message the more feral
it seems to get so this is actually the
most behaved of all of all climate
misinformation it just gets more and
more
um what's that the one yeah think tanks
cuz think tanks are are these you know
lawyers in suits writing white papers
and writing
books no that's that was tweets tweets
were 60% right so um yeah so this was
the thing about this research though was
like this was a it was a really
insightful paper and it's great to
create a history of climate
misinformation using this machine
learning model but really I was
interested in this Holly Grail
um end goal of detecting misinformation
and then doing something once you detect
it what you do I'm not sure now um and I
and I thought once you had done this
detection problem solved it was the
whole gray over side like you you're
done and dusted and I could not be more
wrong about that so and I should have
known because because my background is
psychology and um working with a whole
bunch of Psy ology experts we wrote the
debunking handbook in 2020 where we
summarized all the psych Research into
how do you debunk effectively so it's
how do you debunk misinformation in a
way that's actually going to stop people
believing in the
misinformation
um and if I could summarize it in one
little slide it would be this if you're
going to debunk misinformation you want
to do a couple of things firstly you
want to put the emphasis on the facts
and not just
random facts or just throwing facts at
people you want to identify the fact
that dislodges the meth uh in people's
minds because when you tell people that
thing you believe is
wrong um that and you're basically
reaching into their mind and taking that
out of their mental model you're
creating a gap in their mental model and
people don't like gaps uh and if you
don't fill that Gap with a replacement
fact then what researchers found is
people just go back to believing the
myth again it's called the continued
influence effect yes question is a bit
like
learning yeah that's I think that's a
bit more involved in deeper but there's
probably s similar Dynamics where you do
need to and this is not this is again
something outside my area but but my
perception of that is that you do need
to rep it's not just telling them to
stop doing this or stop believing that
but you need to replace it with an
alternative belief system or an
alternative lifestyle or or whatever it
is required to deprogram so in this case
yeah you do need
to take take the myth that you're
debunking and replace it with a fact
that slots into their mental model but
fits into the mental model even better
than the myth did let me give you a
tangible example let's say the myth the
sun is causing global warming if you
tell them okay no that can't be true
because the sun has been cooling for the
last 50 years while Global warming's
been happening so it's it's impossible
that the sun is causing global warming
you've shown that this that myth cannot
be true but having created that Gap now
you need to replace it with what is
causing global warming so then you would
need to um find that fact replacement
that slots into people's mental models
so that's just the first bit like can
and and that's quite hard uh it's
non-trivial to identify that replacement
fact that that dislodges the myth that
you're trying to
debunk uh yes oh is it okay yes yeah
definitely no no you can ask questions
during but probably just wait till Adam
gives you the microphone so then you can
speak into
that what if the fact that I'm giving
them
is it's so radical that they can't
really accept it's if do we need to
replace the myth the misleading myth
with something less radical that
they uh radical I mean can you give an
example of what you mean well for them
it is radical not for us yeah I
mean so well maybe maybe not see what we
this the kind of general rule of thumb
with the whole fact replacement thing is
you like because misinformation is
sticky it's it's hard to dislodge it's
it's often easy to remember it's just
sticky information but um the general
rule of th is you want to fight that
sticky myth with an even stickier fact
so you want to take your fact which
often facts are pretty boring make it
meable uh I mean you're kind of yes so
so facts can often be abstract nuan
complicated um difficult for people to
get their heads around or maybe radical
but what you need to do is communicate
the fact in a way that is still accurate
but in a way that is sticky compelling
memorable um engaging to people so um an
example would
be
um the the the myth global warming
stopped in 2015 U which is something on
the Internet because 2015 or 2016 was
the hottest year on record uh until 2023
I think pasted it but anyway they'
they've been using it for the last few
years um one way to debunk that is
to communicate to people that since 2015
and in fact since
the last few decades the planet has been
building up heat at a rate of four
atomic bombs per second that's how much
energy is building up in the climate
system and there's lots of ways you
could communicate that information you
know you can talk about radiative
forcing and and energy imbalances and
lots of um Nuance complicated ways that
would go over people's heads but a
sticky way to communicate it making it
simple making it taking it Taking people
by surprise with unexpected information
which radical information might be um is
four atomic bombs per second is actually
how much heat is accumulating in our
climate system so anything that involves
explosions giant like teeth or dinosaurs
or stuff like that anything that belongs
to in the Hollywood Hollywood category
right yeah s does sound like a Hollywood
movie um and then the other important
element of debunking misinformation is
you need to explain how the
misinformation actually distorts the
facts what is the fallacy or the
rhetorical technique that the
misinformation uses um and one of the
reasons you need to do that is because
when you communicate all right here is
my key fact um the planet's been warming
at a rate of four atomic bombs per
second but there's this myth that says
that global warming stopped in 2015
those are two conf conflicting pieces of
information and the danger is when
people receive two conflicting pieces of
information and they have no way of
resolving that conflict then they can
just disengage and believe neither it's
like oh it's too hard I don't know I I
don't know what to believe but if you
explain well the way that the myth
distorts the facts is it uses the
technique of cherry-picking or
misrepresentation or oversimplification
or whatever um then it helps people make
sense of the two conflicting pieces of
information did you have a
question yes um this seems uh super
super plausible uh and super sensible
way to attack the problem but I'm
interested how did you come up with this
as a solution is there like a
statistical background or any evidence
to say this works and we tried lots of
other things they didn't yeah so the
banking handbook um is basically a
summary of all the psychology research
testing like some of my studies and
other people all over the world testing
all these different formats um
communicating facts communicating
fallacies Etc and and this is kind of
distilling all a whole body of research
um so there a lot of Psych psychology
research that goes into theing handbook
yes yes it's just I think yeah if you
just go to that address at the top of
the slide and actually I'll share the
slides with Adam if you want to share it
around with everyone
El so yeah so and The Bu I don't know
it's about a dozen Pages it's very
concise but a a ton of references
at the back so if you want to chase up
all the the research that it's based
on yes so yeah so now ai is so popular
right so you can ask questions and then
give you kind like answer looks very
like facts yeah so what do you think
like uh
so uh for
example how how you going to let uh like
uh uh how would I say like
um uh help people to know to recognize
or identify that's the fact like uh
because everything from TV from social
media from AI probably can can be like
you know could be wrong how where can we
find fact to then yeah I mean an
extension that question what you keep go
yeah like I'm just saying like Okay you
can say you have this ways to to like uh
convince people this this is a fact but
the thing is who's going to convince us
where we going to be the channel right
um and we were talking about spam
filters earlier like um in terms of
knowledge the the best spam filter is is
peer-reviewed science where you have
experts in a field reviewing what other
experts are saying it's not foolproof
but but scientific papers published in
peer-reviewed scientific journals but
it's not
accessible no and so a lot of the work I
do is about trying to make peer reviewed
science accessible and hopefully the
thing that I'm talking about tonight
will also make this kind of work
accessible did you have a follow-up
question did yes CU that's really
interesting it's one of the I brought
this up before with you over the phone
and that was that um recently it's been
shown that Claude AI has been able to
provide persuasive arguments that pretty
much on par with humans statistically
anyway according to anthropic who did
the research there um so it does look as
though and there's a number of other
bits and pieces of research that you you
have that you've seen which suggests the
same that AI is becoming more and more
more persuasive right so if we're trying
to um fight this battle on the mem front
uh those who are using malicious AI
maliciously who have no boundaries as to
where they can take their memes because
it's not grounded in facts it could be
any sort of fantasy whatsoever how are
we going to convince people of you know
of what's right or wrong on the mem
front where their memes are
unconstrained by reality they can be
even more Fantastical than any means
that's strained by reality that we can
produce right so it seems as though it's
a it's it's a difficult battle oh yeah
it's a very difficult battle um I well
let me let me do a quick answer I'll do
a quick answer because there's a few
hands up um firstly misinformation is a
huge ubiquitous and complicated problem
and there's no one magic Bol like memes
aren't going to get us out of this
problem but they can be one useful tool
amongst the suite of tools what I'm
working on is just one tool amongst
others another thing I'm working on is
building people's critical thinking
using tools like games which which can
get people practicing critical thinking
through game game cranky Uncle a game
which you can download today on your
phone and so what that does is build
people's resilience against
misinformation technique so they can see
these manipulative memes or whatever um
but and again that's not the Magic
Bullet either that's just one tool many
and I think that we do need to be
developing lots of things and we need to
be developing solutions that are
scalable which is why I've become quite
attracted to Technical Solutions like
what we we're talking about but couple
of questions over the side of the um at
the risk of using big words which may be
unnecessary my big concern is without a
well-founded and agreed epistemology
we're in deep and the reason I say
that quite assuredly is most of my life
life was looking at semantic constructs
and trying to make sure that they fit in
with a structure of knowledge in a
particular domain if we don't have that
structure that epistemology you're
forever fighting because it's easy for
someone my favorite one came two days
ago Donald Trump says he's better than
Joe Biden because he's won 26 golf
tournaments and Biden doesn't play golf
um God the the mything on that was that
was 26 golf tournaments at his Mar Lago
estate uh where he owns the place and in
fact was in in fact running the whole
comp so I mean these are the kind of
things that's an easy one but my fear is
that without an epistemology which the
experts can agree and there's even
disagreement in the experts in this area
in particular we're always going to be
fighting an uphill battle and I'm just
wondering from The Experience you've had
trying to build these models do you ever
see any form of epistemology coming
through that could be helpful well maybe
I should keep going because I think I'm
on slide three so and and I think some
of these questions will get answered um
and then some of them will probably be
unresolved and we can dig into them
deeper all right so um so all right
getting back on track um we started with
the goal of trying to debunk
misinformation automatically we we're
able to develop a model that detects
misinformation but then once I realized
okay the psych research tells us that we
need to create a debunking like this how
do we go from detecting a claim to
producing a debunking um according to
the fact myth fallacy fact structure
I'll just call it the truth sandwich for
short because that's one of the kind of
colloquial
phrases and we started actually about
two one to two years ago um playing
around with chat GPT
3.5 and just working on um playing
around with prompt engineering to try to
throw misinformation at it and then see
if it could produce a reliable fact myth
fallacy um
debunking and we found that it was just
rubbish at that accurately spotting the
fallacy in misinformation it just that
it did okay with the the facts although
the facts tended to be a bit a bit
generic um but the it just was bad at
spotting fallacies it's just not a
critical thinking
piece of technology and so we realized
okay well we're going to have to tackle
the fallacy problem in a different way
and so we spent about a year or I don't
know at least half a year developing a
model to uh detect fallacies and we took
that same supervised machine learning
approach we took with our previous model
where we took a whole bunch of examples
of climate misinformation
and identified what were the fallacies
in each one and then we use that as a
training set to um to train our model um
and this sorry if this is a bit I
actually this is probably a good
audience for this but this is a
confusion Matrix which um reports how
well our model did at spotting different
fallacies um and really the the
interesting the most important part is
the diagonal black boxes cuz tell you
the the accuracy or the percentage where
it got each fallacy right so in other
words it was 78% accurate at spotting
adome attacks it was 92% accurate at
spotting anecdote fallacies and so on
and so on um the ones where it did badly
were ones where we didn't have many
training samples like it was hard
finding is that false choice or false
equivalence F yeah false equivalence it
was hard to find examples of that so it
didn't score that well and then slothful
induction was Troublesome because it's
just a Troublesome fallacy it requires a
lot of it requires background knowledge
in order to spot the the fallacy and and
that's one of the things we realized
during this research was there are two
types of fallacies there are structural
fallacies where you can just tell by the
pattern of the text like for example a
false choice is usually phrased in an
either or kind of way way you're either
with us or you're against us kind of
kind of structures uh and you can you
can spot those just through the way the
text can be arranged but some like
software induction or cherry picking
require actually knowing background
knowledge you need to know what is all
the evidence available before you can
know whether they're cherry picking and
ignoring other evidence and similar with
slothful induction that's the fallacy
where people come to a conclusion
without considering all the evidence and
um you need to know what that evidence
is and whether their conclusion
contradicts the evidence so um yes
oh
oh need judge theity of evidence that we
have as well uh yes I mean again with
what we were looking at were I would
characterize it as kind of the shallow
end of the pool or the the loow hanging
fruit of climate misinformation the
clear examples where there was a a
robust body of evidence lots of
different lines of evidence all pointing
to a single conclusion um like human
cause global warming or just the reality
of global warming itself so these were
cases where it was there was clear
science so we weren't we weren't going
into the murky areas like on the edges
of knowledge where scientists are still
figuring things out like during Co and
you could see that scientists were
figuring things out and they were you
know CH you know you were hearing
different messages from the CDC or or
you know sorry I was living in the us at
the time um so I was grateful to be
working in climate change where most of
the stuff was answered 20 years ago
whereas with Co they were it was
happening in real time so that was that
that's a much more complicated um area
so I actually um thought about writing a
book on Co misinformation at the time
and I'm really glad that I didn't cuz it
would have
been yeah I would have given myself a
world of pain
um uh and if you if you want to like
check out the the model if you want to
stress test it um there the URL for the
model is up here
sk. SL flick tour let's do a test now do
I have it up here somewhere I don't
let's just type it in so think of a
climate myth because it it's trained on
climate myths uh it's it's whoops it's
on hugging face which is slow because
we're we're cheap so we only paid for
this we're just using the free CPU
version that's something we're working
on to try to get get it um but how much
funding do you need if there's any
billionair watching yes yeah um how much
do you need uh I can't tell you off the
top of my head but like what's what
what's a generous amount which I mean
how much does it would it cost to have a
GPU server you know like I I have I
don't
know probably for a millionaire pocket
change so so think of a climate meth
we'll put we'll pump it into here and
see how good it is at spotting the
fallacy anyone want to suggest one
climate
meth anyone want to suggest a climate
meth HS all right so let's
say climate change is a hoax well let's
just see how it goes now the first time
it runs a model it it's slow because
it's on a CPU and it needs to once you
once it's run the first time it we can
so while this is running think everyone
else think of a different kind of myth
that you might have heard computer
science so yeah so it says there's a 93%
chance of it's a conspiracy theory so
that um uh and sful induction because
you know the climate change is actually
based on a lot of evidence and it's
ignoring all that evidence you you got
one maybe have phrased that the idea
that global warming is actually um alul
Weathers
from okay global
warming um is actually the result of
heat island
effect andur heat effect
is that
is oh yeah no that's that's a big one
yeah global warming is not
happening um the warming
record um is actually an artifact how
does that
sound an artifact once I had to design
an experiment where I had to write an
article of climate misinformation I just
did it on the iPad on the train home and
spat out a thousand words so easy like
it's like it would be great to be a
climate Den cuz you just don't have to
worry about being accurate so
you it's it's anyway let's see how it
goes so it's misrepresentation so and
soft induction so it would probably and
this
is eventually like what we're working
towards is not just saying this is a
fallacy but explaining why how it's
misrepresenting um in this case it would
it would probably be yeah sorry you're
going to say finish You misspelled artif
I it would be very interesting to see if
it has a
different is that is that how
because so there a if it's an arbitrary
change in text would would give a
difference I doubt
itut that's in the noise that's good I I
I just thought that would be very
interesting if like a completely
different thing was put up the top if
you changed one letter right good stress
test and you won uh anyone want to do
another one that global warming is a
natural part of the weather ccle okay
that's a good one global warming is a
natural part of the weather
cycle oh whoops
still single cause so um there's a a lot
of arguments where it's saying global
warming is just natural factors and
ignoring other Factor like like human
causation so where tends to see just
blaming global warming on one thing it
uh tends to characterize it as a single
cause anyway I I'm don't want to get too
distracted by this let me jump back into
the slides so anyway you can ever play
around with that it's as the confusion
Matrix shows there's still room for
improvement but it it it's actually
these results are about two to three
times better than previous fallacy
detection research which is all 20 to
40% accurate it's bad it's how do you
assess the accuracy of that how do you
how do you check the results so what we
had was our we had a training set of 2
and a half thousand examples and then we
just subdivide that into a test set and
a training set or sorry it's more like a
test set and a training set and so the
test set was was what we used to assess
like to obtain the confusion Matrix so
we're just comparing the
classifiers um answers to what human me
um said was the fallacy so um so yeah
that like ideally you would have
multiple you'd have two or three fallacy
experts um doing it and then do intera
reliability and then have a gold
standard data set um fallacy experts are
hard to come by so um we just with a
single person in this case but um yeah
that's something we're working on as
well um all right so all right
so so
we once we had developed our model to um
detect fallacies and we actually
submitted the paper to a journal Monday
so was just this week but um then we um
um given that just using chat gbt by
itself was not good at fallacies then we
decided okay let's do a um let's do a
llm prompt where so we're still um and I
think we started with chbt but then we
started using cheaper smaller Open
Access lrms but then um we wanted to put
some Dynamic information which is the
red text into our prompt so we'll
basically spoon feeding it all right
write this debunking fact myth fallacy
but the fallacy is oversimplification
and so we just helping helping Along by
by giving it some of that extra
information and also the the first model
that I talked about with those five
categories we also um what what
misinformation claim it detected there
we also put that information into our
prompt so we were basically trying to
just go one step more sophisticated with
our um generative AI prompt by by
packing the prompt with um some Dynamic
information coming from all these other
models that we developed uh and what we
found was and we did it we started with
chat gbt but we knew that that was never
going to be our end point because open
AI charge for every time you use it and
if we wanted to create a scalable
solution we we wouldn't be able to
afford it basically so we we once once
it was working well on chbt then we went
to a simpler model uh Palm 2 was the
model that we used but we found that it
was garbling the the fact myth fallacy
fact structure it got a bit confused
sometimes or it glitched and and
sometimes it would do like multiple
facts or get it get it kind of arranged
in a jumbled fashion and so then we went
to a third model which was multiple
prompts and so now and this is looking a
bit complicated the basics of it was we
would do a prompt for the first fact and
then a prompt to to recategorize the
myth and then another prompt to explain
the fallacy and then a fourth prompt for
the fact but we
also um pulled in some information from
outside sources as well to help with
writing the fact so this was this makes
me a little bit queasy but we were just
doing a Google search um asking asking
for factual information relevant to the
meth um and then there was a database
called climate fever um which actually
has whole set of statements categorized
as either factual or or misinformation
and we also Drew on that for for the the
final fact in our um in
our our model so um so so these were the
these were our two models one was a
single prompt Palm 2 and here was a four
prompt and we were using mix was our LM
in this case um I don't recall why but
the the research assistant found that
Palm 2 wasn't working very well in this
context um or or mixture was I think he
just did a matrix search and found that
mix draw was the best performing model
um now
what all right and so and so then last
Friday we had to submit a paper to a
conference so and we're we're just
halfway through this research but we had
to submit something and so I said to my
colleagues oh no sorry so and we were
talking about we know that peer
reviewers are going to say why didn't
you just do a single chat gbt prompt um
why why are you doing all these super
complicated models so we thought well we
should we should do that as well and
then we can compare the three approaches
and I joked to my colleagues wouldn't it
suck if after all the two years of work
that we've done developing a fallacy
detection model and all these things if
the latest version of chat GPT
outperformed the the stuff that we're
doing uh and so we did it with um
gp4 I think not 4 Z this was before that
came out I think it was four turbo is
what it was called and um and we did
prompts like this is what it looked like
this was our gbt prompt so and all the
prompts were structured and worded
similar to this so it was like defining
the role has a has a big influence on on
the output that it does and then we
would instruct it to do the fact FY
debunking and then we'd provide some
more specific instructions on what we
wanted to see in each of those sections
and then we gave it the myth at the end
and also we had a table where we listed
all the
fallacies um and the definitions of each
fallacy and then we ran this all the
same myths with
gp4 and we found that it was doing just
as well as our our most complicated
model and we were like ah so um so
from 3.5 to 4 somehow they taught it
critical thinking like they taught it
how to spot policies and it's
reasoning they did something I don't
know maybe it was Scarlet Johansson's
Voice or something so so so so here's an
example of the the different
outputs um here is a myth saying that
the sun is causing global warming
basically and um here is the just the
GPT single prompt here is the Palm 2
single prompt with some Dynamic
information and then here is the four
prompt solution with Dynamic information
and it's pulling stuff from all over the
place and in this
case and this is just a a to give you a
rough example of of how they did the pal
two tended to be simplistic a bit
generic um GPT and mixt both were
roughly equal in this particular case MI
actually does a better job of
identifying that replacement fact it's
not just saying the sun can't be doing
it it's saying it's actually human
activity so it's it's um doing a better
job of of identifying that fact that
dislodges the myth and all of them did
pretty good with the fallacy in fact the
thing I like best about this research is
how well it does at explaining
fallacies um and this is a little bar
chart B charts just comparing the three
models where we we basically graded it
like um the the four researchers working
on this project including myself we we
graded each section of each debunking
the fact the F the first fact the second
fact and the fallacy and then just this
is the average result across all four of
us uh and we found that basically GPT
and
mixt were were good roughly equal at the
fact and and roughly equal at the second
fact as well one slightly above any way
it kind of washes out uh and then
mixture will slightly up performs gp4 on
the fallacy so if it was a it's kind of
almost a a almost a dead heat but um but
you know the fact that mixture is so
complicated and then GPT just kind of
slouches its way into the room and and
nails at first go it kind of pisses me
off a bit but um but the and then the
Palm two model doesn't do as well in in
all cases so um
so I think just the the next thing like
because we're still halfway through this
but what we still need to do is um just
tweak the models a bit because we saw
things where it didn't do as well as we
would like
um it's it's not ideal that the
researchers uh assess or do the
annotations to grade the the models
output so we've actually recruited a
bunch of people to um to like nine
different people are going to go through
all our debunkings and grade them and
the interesting thing is they all have
quite a different level of climate
literacy there's some people who are
professional climate scientists and some
people who just kind of into the the
subject um so because um we think that
the climate expertise is a big factor
and how good you are at grading
the the quality of a debunking because
they will just know things no background
information that a non-climate scientist
wouldn't know uh so could could you rain
this by pulling in a bunch of like you
know lunatic client climate and ners to
to grade it all and and see if you can
detect which ones are the bad scoring so
and what would you get out from that why
would you do that you pollute the the
grading mechanism can you like how do
you know that the grading isn't
bias so how do you know right you're
relying on
expertise you know face the expertise is
what counters the uh the validity of the
grading the the the general metric of
use is integrator reliability so if you
have one person who's an outlier
compared to everyone else then you have
a closer look why is that person scoring
differently are they are they biased are
they incompetent are they not paying
attention or or whatever so um and
that's probably how we're going approach
it when we have a whole bunch of people
my instinct is that you'll probably
see the climate experts show high
reliability with each other and the
non-experts show reli reliability with
each other but probably low reliability
between them that's what I anticipate
but we'll have to run the experiment and
see what we get yes so do you call that
comparison reliability or what was it uh
it's intergrated
reliability Yeah so basically just
imagine you have two people who who just
score like you know they'll just do a
score like in this case we just look at
every fact and we score it from one or
zero to three and so it's just seeing uh
and so you know I do I Score 1 3 2 three
and and then Fred scores 1 4 three
there's no four but you know and then
you can just run those numbers through
uh like a integr reliability function
and it'll tell you between 0 to one how
um reliable what's the intergrate of
reliability how much do you agree with
each other like there are there are
several different metrics you can use
even just agreement like percentage
agreement but um and which measure you
used depends on the context but uh yeah
we just used standard integrative
reliability
measure yes I thought of a way in which
maybe it could be useful to like have
you know climate deniers or whatever
people who are not a part of the the
general scene commenting on it just in
the sense of um if you give them the
direction to try and answer the
questions as if they were a climate
scientists it might be interesting be to
be able to see the extent to which
someone who doesn't believe can
stereotype in their heads and model what
a Believer would say but there could be
an interesting part in that in terms of
like what is
what is the Orthodoxy of the scene as
opposed to what an expert
says matter now someone who doesn't
believe could then if they could tell
you exactly what this a supposed expert
would say it it slightly suggests that
that's the dogma of of of that
discipline rather than the actual truth
at least it's a signal in that direction
there's there's two things we want to
know firstly is the is the output from
the model accurate and reliable and
that's what we're that's what this is
all about uh and that's why you probably
need experts to grade the quality of the
output but then the second question and
really the more important well once
you've once you've satisfied yourself
that you you've got an accurate model
then the next question is does this work
in out in the real world in debunking
misinformation if you take the output
from your model and deploy it in some
way and that's that's an open question
on what that looks like
will that actually reduce misperceptions
and stop misinformation from spreading
and that's um that's the one of once we
get past these hurdles once we kind of
solve the little thing of the Holy
Ground then then we will start doing
psychology research on does this
actually is it effective accurate and
then effective and that's where you
start getting climate deniers or
conspiracy theorists or just people from
diverse backgrounds to to look at the
output so did you have a question yeah
could I ask you to go back about three
slides to the outputs from the model
that's one that's the
one when I look at that and I'm going to
take a very naive perspective on this if
my mother was saying oh no you know it's
a bloody sun it's that's what's calls in
here I would probably go with pal to
because she could get her head right I
mean this is a really good question I
think that one of the issues we have
here and this is why we had good old
Mikey go going about experts can't be
trusted it's because experts will
deliver facts in a way that they
understand based on their discipline
that does not equate necessarily to you
know Joe Blow In the Street who doesn't
get it at all yeah and I think what
you've got here this is what fascinated
me more actually is I totally agree with
what you said but when I'm looking at
messaging harm to is kind of I'm saying
you could probably take that to
most people in the street and they would
kind of get what you're saying mix mix
TR you take that to the people when
you've got hang on three of what you're
already challenging some base
assumptions people will have yeah I get
what you're saying but I think the
problem with expert
messages they sound like a bunch and
they'll just just just how us what you
think it sounds like a bunch of kads
who've got the the room and come up with
um the complete opposite to the mean
that you were talking about earlier
instead of like make America great again
and I'll use crap um but you said say it
would be a very good idea based on the
evidence that we have today looking at
the graphs that we've exposed here that
maybe there's a chance if we no that's
not going to stick and I think this is
going to be your other problem no no
it's it's I mean and we just touched on
that because we are just trying to solve
the problem of accurate we're not even
thinking about effective yet um so this
is accurate this is probably more
effective right I think this is going to
be your balancing point two ax and and
generative AI is is actually a really
good tool for that kind of thing like
often when I'm designing experiments and
I need a message to put in an experiment
and I'm feeling lazy I'll just get chbt
or Gemini to give me a thousand words of
this argument and then it's like it's
not that good explain it repeat that but
at a 12y old level and and then I'll
just try 14y old level 16 and and just
and and it does you try different levels
and it it gives you different outputs
and some of them are really good at
explaining and and some not so I think
that um once once we solve the accurate
problem then we will do the effective
problem and that is a psychology
question and that's where you probably
need to yeah probably what I imagine and
this is again I think a power of AI is
you can tailor it to your audience so
you know some someone might be more
comfortable with this and another person
might be more comfortable with that and
you can um then politically get ripped
apart well it depends on how you do it
and and transparency like you you change
the message any anyone especially the
popul who are very good at this they'll
say yeah okay you say your say the same
things but it doesn't correlate it
doesn't do this and that and there is
one of the big problems you've got the
attack lines become open and again
please I'm not disagreeing no no no
these these are all the things that we
uh just beginning to Grapple with
because that question is then the next
question after effective like we're
still in accurate then we will work out
in the lab effective then we then we
have to figure out how do you actually
deploy this in the real world and that
will that could look like a million
different things and that's where you
grapple with those kind of issues and
and and how people respond to it how do
you like you want to be transparent you
want to do it in a way that like for
example the fallacy detection um you
know I like that it reports the
probability so it's not like I'm saying
this is the answer we're saying there's
a 90% chance that this is conspiracy
theory so you know those kinds of
reporting methods are ways that you can
make something more transparent and I
think potentially more acceptable as a
democratic for the Mad Auntie who only
believes in what
friend I mean there was a there was a
there was a study published just a
within the last month where they used
chat gbt to talk to conspiracy theorists
and they would say what's your
conspiracy theory or something and then
they would say 911 was an inside job or
something and then and and they'd say
they'd give an argument for it and then
the then the chat gbt would generate a
th000 word and so well that you know
that that point you make and then it'll
kind of explain what all the facts are
relevant to that and then the person
goes well what about this and then it
would do another thousand-word answer
and and what they found was um that
intervention like having
chaty directly replying to the specific
arguments that the person was making had
more effect in reducing conspiracy
beliefs and I think I can't think of
anything else that has been done in a
scientific experiment and then it's
stuck like it like when they tested
weeks later they they still had reduced
belief in The Conspiracy Theory because
often you find it just Fades over time
to see longitudinal yeah it was this I
mean this had a longitudinal element to
it and the thing that jumped out at me
from this research was the conspiracy
theorists reported afterwards I felt
heard because because it was responding
directly to specific arguments that they
were making yes John I want to ask a
question so people have different brains
and different brains will react
differently to the information that you
give them for example my boyfriend was
telling me that there there there's
about 20% of people who we called
conservative and they have a different
brain structure that you what you say to
them cannot get
through they
not I don't know there are some
scientific studies that correlate
different different bra of things like
flight
respons get through to them because they
cannot be that receptive to your new
ideas so how can we get
them yeah to be receptive well I mean
generally it's a universal human
condition that that people resist
information that contradicts their
beliefs
so whether they're conservative or
liberal religious or or I don't know but
definitely religious but um but you know
people you you you say something that
challenges someone's beliefs um or their
or their social identity and that they
will resisted in fact social identity is
arguably stronger like during my
psychology PhD I came out of that
thinking that beliefs were the were the
big thing and then I moved to the US and
spent four years where my four years in
the us directly overlapped with the four
years of the Trump Administration and I
leared that tribality or social identity
is even stronger than beliefs because we
saw how beliefs just shifted when the
tribal leader says no no we don't
believe that anymore we believe this and
everyone was like okay all right we'll
believe that so um yeah so it's when you
are communicating information that goes
against people's beliefs or trial
identity then it's going to be difficult
and climate change is an example and in
a way it's almost a rational response in
the sense that just imagine you all your
community your family your social group
all think that climate change is a hoax
and then I come and say hey that's
actually not true and here's all the
evidence for climate change you should
accept you know the the climate
science for that person they would have
to then go against everyone that they
know and love and and live with and
suffer the consequences of that and so
there's a lot of disincentive to go
against your social identity when
everyone believes something and then
you're given information that
contradicts those place so um so there
is like I'm I'm a scientist like I big
believer in communicating facts but I
also acknowledge that the science tells
us that facts can be limited in some
context like that um and one reason why
I actually got into the whole critical
thinking and fallacy um way of
approaching misinformation was because
the first experiment I did in my PhD I
was just just blundering around trying
different things and I tried explaining
a misleading technique
um uh in a in some climate
misinformation uh and this is almost a
quirk of the experiment design but the
misinformation was about the scientific
consensus on climate change and the fact
that there's 97% agreement amongst
climate scientists that humans are
causing global warming but I knew from
previous research that just mentioning
the scientific consensus is really
powerful because we're humans are social
animals and that expert consensus is a
social message so I had to try to
inoculate people against misinformation
without about the consensus without
mentioning the consensus it was kind of
now I'm thinking of um 4y tows and don't
mention the war but the two people laugh
in the room um and so the way I tackled
that was I explained the technique used
in the climate misinformation without
mentioning climate change and instead I
used tobacco misinformation as an
example using the same technique so
basically I was saying look out for this
technique fake experts the tobacco
industry used to do it and here's a
little 1950s newspaper ad with a guy in
a white coat smoking and telling you
that smoking sues the throat and and you
know that's a fake expert so look out
for fake experts and then I showed them
climate misinformation that use fake
experts and um that was one group in my
experiment and then with the other group
I showed them just the misinformation by
itself what I found was the misinform
climate misinformation by itself worked
much stronger on political conservatives
so so it really reduced their belief
which was already low and it took it
even lower so the misinformation had a
polarizing effect it it worked more on
conservatives than it did people on the
left but when I did the tobacco
inoculation lookout for this technique
then I showed them the climate
misinformation the misinformation didn't
work at all across the political
Spectrum it was just completely
neutralized and what that told me was
regardless of where people sit on the
political spectrum and what their
beliefs are nobody likes being misled um
and so that I thought oh maybe there's
something in this whole critical
thinking approach um it's a way to
sidestep those cultural or belief or
social identity um barriers by just
appealing to people aversion to being
misled and so I did a lot of psychology
research about fallacies and critical
thinking and then when I started working
with the computer scientists I brought
all that research into trying to
incorporate it in the AI research as
well so that's why we're that's why like
fallacies are such a key part and you
know I'm loving that it actually does
best at fallacies in out of all the
different elements of a debunking yes um
has anyone done research similar to you
but on um anti
vaccination I work for a medical
research institute and it's something
that we need to deal with from time to
time um has anyone I mean there's a lot
of people doing vaccine misinformation
research um the other thing I'm doing
besides AI work is using games to build
people's critical thinking um it's it's
called cranky Uncle Adam mentioned it
earlier and the cranky uncle game
focuses on climate misinformation but
then after we released the cranky unle
game uh UNICEF approached us and said
can we do a vaccine version of it
because it's critic it's just a critical
thinking game and all the same fallacies
that you see in climate misinformation
are also being used in vaccine
misinformation so we just took the same
game and then um identified the top 10
fallacies in vaccine
misinformation eight of them were
already in the the CL cranky game the
two that we added
were um false cause or another word for
that is post Hawk Ergo prop hawk in
other words this thing happened and then
that thing happened so this must have
caused that um kid got vaccinated then
they they got autism so vaccines must
cause autism you know so so that that
was actually the number one argument and
the other the second one was appeal to
Nature or we call it natural is best in
the game which is that natural remedies
are better than than vaccines or should
be used instead of um but and then also
yeah so so cranky Uncle vaccine um came
out last year I think the problem is um
like all both games are like smartphone
games but you can only play the vaccine
game in Ghana Uganda or Kenya
currently maybe Tanzania um but the
browser version you can play anywhere so
if you go to cranky Uncle vaccine. org
you and click on the browser button you
can play that game but there are other
people working in vaccine and I'm
collaborating why is it limited to those
re so UNICEF are interested in raising
vaccination rates in the global South in
specific parts of the world and so
basically at the beginning we just kind
of put the call out to all their
different UNICEF offices and and the
countries that were most in like
engaging with in terms of because what
we wanted to do
was um like the original cranky Uncle
it's it's so white it's like um it's
just criminally white and so we wanted
to make sure that a game in the global
South um looked like the people who
would be using the game and so we ran
codesign workshops um in the countries
that were interested and um able to help
us run codesign workshops so Kenya
Uganda and Rwanda were the first three
then we went over to Ghana and Tanzania
and then we did Pakistan version which
isn't out yet because translating the
game into eru has been a nightmare um
and and now I'm not just talking about
right to left technical issues just the
language like uh I think it was more an
organizational thing than than a
cultural thing but um yeah we're still
working on that but um
yeah it was really that UNICEF are
interested in a interested in specific
parts of the world and B we it was
important that we went through a
rigorous codesign process in each
country so that's why it's just in these
specific countries so far we're hoping
to do Latin America and Middle East and
and Caribbean over the next year or
two if we can get
funding yes I I love you last section
about deploy what does that mean in the
real world do you like every time
somebody posts disinformation or
misinformation on social media do you
like give them a thousand
words right and so and so this this is
kind of a bit of a Steve Joby kind of
reference right like it's just oh one
last little thing that we got to do
because because when we started the H
grow I thought oh well once we've solved
detection it's job done and then I
realized the psych research and I should
have known meant that it was actually a
lot there was a you climbed over that
first summit and then there was a much
bigger Summit once we've got over that
Summit and we've used generative AI to
create accurate debunkings or even
accurate and effective debunkings that
work well in the lab then how do you
deploy it I have a this we haven't even
got to that point but I have a feeling
that's going to be the next huge Summit
what does that look like I think that
could be a lot of different things um
one thing that we
worked on at um at Monash was a browser
extension where
you as you uh look at a web page uh it
uses our initial detection model to scan
the text and if it identifies any CL
information it then highlights it and
then it pops up a um a debunking so
people could just be as this their
browsing it'll identify misinformation
and let you know how it mislead so that
that's one thing um like a bot that just
responds is another thing but I don't
think that that's going to fly very well
bot I re Elon Musk would be right in for
that yeah no I think he would lose his
over it
actually no no not if it was so yeah
given his politics I think that he'd
probably be opposed to our gos well I
mean like can you approach Facebook
again see if they're interested in um
having tag posts in Facebook like how
accurate is it are are you worried about
it being wrong in some cases and I mean
that's that's where we're at now is the
accuracy like it has to be highly
accurate um before you would want to
deploy it out in the real world and so a
people yeah who's fact cheing the fact
cheers yeah so s did you have a question
well it's actually a propo because I was
going to say like what do you think of
um Community notes on Twitter because
that's the most real impementation I've
seen and also I think that that also has
an interesting directionality because I
find that works really quite well
because there's the implication that
ultimately that's some bl's opinion
there's a bit there's a degree of
aggregation in community
notes but it's kind of a dude said that
and other people who are apt to disagree
age with him said yeah that seems about
right so it's like there's like you know
a little it seems to work very often
where like some someone says something
in straight up isn't true there's
usually a community notes saying n
actually it's this yeah and it's
straightforward and there's skin in the
game in that sense and an inbuilt
skepticism because it is just that dude
so I think there might be a problem in
this in the sense
of it has it even even just in the sense
of it has a potential to be the truth it
just IR people wrong just socially like
it's almost like it automatically comes
across is if it were a person it would
be arrogant right so if so in order for
us to deal with it we almost have to
personalize it in some way we have to
give it some identity it has to present
in some way it can't literally be a
thousand words that just pops up at the
bottom of every post no would read
always it's seen as the voice of the
authoritarian V well that's and that's
it West Community notes doesn't have
problem but there are fact checking
groups already set up in the U reporting
industry particularly left wing or
leftwing press Guardian has a complete
team dedicated to fact checking yeah
they're consistently
wrong they're like radically wrong all
the time in fact well I won't Jo Joe
Biden isn't seen o according to them so
what can you say it's if it's
politically inconvenient it's not true
this is why you need this automated
stuff sorry I didn't mean to I think gu
I'll come back to epistemology but
ignoring that for the second there are
groups that are being set up to do fact
checking and I think this is the kind of
thing you could deploy into those guys
first because they're the ones who've
got the real world contact they're the
ones who can help you refine this
effectiveness of the right so so that's
another potential tool would be um like
that that flick tool I showed you just
imagine just a simple web page where
people just enter some text and it just
spits out a response
it's almost a Wikipedia take it with a
grain of salt but here is our best guess
starting point type resource so that
could be something that fact Checkers
could use as a resource particularly and
I we haven't even looked at this yet but
if you could have have it linking to
resources um but again this is the stage
three stuff we're still in stage one and
and stage three is how do you deploy it
in a way that makes it effective uh and
these are all the questions that we
haven't even been disc I guess I'm
suggesting that down the line in
practice it's a lot of it social a lot
of it marketing presentation yes yeah
and so funnily enough like I was last
week I was meeting with a a guy who
works for a tech company and and I was
he was asking me like I was explaining
this research and he says how would you
do it and I said I this is my idea and
he goes no that's that won't work and
and then he started throwing out ideas
and and we realized that the just so
many ways you could configure this and
organize it and um it it really does
require pra like not just the Egghead
buffins like me but the practitioners
people who do this stuff you know
interface with the public for a living I
think you need to get all those people
in a room with a whiteboard and just
hashing out possible solutions and
that's probably what we're going to do
sometime down the TR it strikes me that
you're going to be fighting a cold war
where the disinformation AI is are going
to massive outnumber and have an easier
job um being fake experts than you'll
ever be being a real expert you you'll
just be one voice of the truth in a in a
notion of life if iing add to that
though but then it it suggests that the
way to beat that is to tag the um
generative stuff which you're talking
about too an authoritative voice of
people trust so you can build the brand
of a a trusted human being who who can
leverage this
expanded dat I think we need to sort of
like St trust in human beings so much
because human beings are kind of crazy
sometimes especially when they're
politically Affiliated I think you know
the best way to sort of generate trust
in this is grounding it in the best
solid evidence that we have the most
verified peer reviewed um replicated
experiments that we can um and yeah
ground as much as we can in in empirism
any comments on that I saw some hchs I
don't yeah I was say you know I've tried
flame Waring overly these people and
they hate experts they don't like facts
anyone that comes and argues with facts
not allowed H attack is is clearly the
enemy I mean part of the reason for that
is like I was saying ad homm attacks
against experts are prevalent like
they're a big part of of misinformation
in all topics you know we saw it during
the pandemic or attacks on like Anthony
fouchy and so on but um
but while there is declining trust in
experts they are still um one of the one
of or if not the most trusted expert on
like in the case of climate change
climate scientists are the most trusted
Source even if trust is declining in
them so so they still trying to convince
with these are you trying to convince
the people who already trust the experts
or are you trying to convince the people
who don't trust experts uh and so the
way I look at that question is there are
different audiences with different goals
uh and a simple way that I look at it is
is grouping the public into three groups
the people who are already convinced
about climate change but are inactive
like they just don't talk about it don't
do anything about it the undecided
disengaged and then your dismissive
cranky uncles so um the the concerned
people are about slightly over half the
population so they're actually most of
the public
but most of those people don't even talk
about climate change with their friends
and family because they that they think
that no one else is concerned about it
uh and so with that group the goal for
climate communicators like me is um
activating them in somehow activating
even just to the point not getting them
necessarily turning them into great
thumberg and marching down the street
but even just talking to people makes a
big difference it's probably the most
important thing a person can do to
contribute to climate action is just to
talk about it and build social momentum
with the disengage group uh it's for me
in terms looking at through the lens of
misinformation it's about um building
their immunity against misinformation
and then moving them into the concerned
group and with the
you can beat your head against that
brick wall if you want to but it's it
takes so much resources to do it but
generally I think that the given that
they're only like 10% or less of the
population we're better off spending our
energy on the other 90% right I guess I
guess depending on how Jerry mandering
has done that that 10% can really make a
difference in some regions so I mean
didn't what you saying that the the
large language M the chat GPT um Sledge
essays that would respond to um
misinformation claims um that was
effective against people who were pretty
skeptical of um climate change or it it
was conspiracy theorists conspiracy so
um yeah I don't think it was it was I
don't think climate was a topic but um
okay but would you consider these
conspiracy theorists as being part of
that 10% of people who aren't really
interested in evidence the the VIN
diagram of conspiracy theorists and
climate deniers has a lot of overlap so
if you deny the scientific consensus on
climate change that means you disagree
with you know the global community of
scientists inevitably you're going to
resort to conspiracies theories to
explain that otherwise they're all
accidentally wrong in the same direction
and conspiracy theories are more
plausible yeah yes last question for
me if we are not trying to put our
resources in changing the
dismissers can we try and
neutralize them in some way that they
don't spread their influence because
they can
bees I mean that's that question is
really what my whole research career has
been about now I've mentioned the word
inoculation a couple of times it's
actually in Psychology this inoculation
Theory which is the idea that by
exposing people to a weakened version of
Miss
you can build up their immunity so that
when they encounter the actual
misinformation they're less likely to be
misled so the analogy I use for climate
denial or other forms of science denial
is polio it's an incurable disease um
but we were able to almost completely
eradicate it just by building up the
Public's immunity to the point that you
got her immunity so I think the answer
to your question is
if we can inoculate enough of the public
in by inoculate I mean build up their
critical thinking so that they can spot
attempts to mislead them
then you know if you could do that
sufficiently that you got hurt imunity
then that misinformation would become
eradicated it would no longer be
relevant in the world but misinformation
is intelligent right people craft this
at least the the myth information
generators are intelligent they can
craft and they can find out what the the
tactics is behind
Mis polio isn't right that's just
selection Pro uh uh the funny thing is
the most popular climate misinformation
arguments are the dumbest well there's
economic reasons for intelligent
misinformation campaigns political
reasons for dis political
yeah but the arguments that hold sway or
the ones that stick are the simplest
climate scientists are in it for the
money it's cold therefore global warming
isn't real um you know it's just it's
just the real basic arguments do you
predict you pick that up do you predict
that there could be a rise of more
sophisticated misinformation campaigns
or do you think that they'll just remain
um not very
sophisticated I'm not saying they're not
like I was at a misinformation
conference in Singapore and it blew me
away some of the the stuff that's like
the thing that really got to me was like
the use of bots cuz I I had this idea
that Bots were just throwing out
on Twitter or whatever they're
they're that sophisticated that they
have a social structure so you have Bots
that are
um like like some Bots generate the
misinformation and then other Bots
amplify it and there's actually that
have a community of bots and sometimes
the Bots they they also have these temp
like these kind of temporal strategies
where all they're doing is just not
trying to make people believe wrong
things they're just trying to create
more polarization and getting people in
a heightened state of being pissed off
at each other and then get them to a
certain point of polarization and then
they hit with the misinformation that's
the standard tactic it's been around I
mean you can go back to the second world
war you destabilize people's basic
beliefs and then you present them with
some options which feel more tenable in
the context of the time they're in the
other thing and I think this came from
over that side of the table the amount
of money being put into disinformation
by political parties large climate
deniers and I mean most of the utility
companies certainly I've seen in the US
and in those think it's the tobacco
industry how much money they put into
disinformation and when you look at the
amount they're putting in compared to
the amount of money that's being put
into good research like the stuff you're
doing we're to F A magnitude different
and that will in the end cause a
swamping kind of gear and I think this
is one of the big issues is that I mean
I'll use Trump Ian I wish Trump was a b
you find a way of turning him off but
the reality there is you've got people
beli in the complete because he
comes across and I'm not concerning that
Biden other than he comes across as
being more personable than Joe Biden I
mean yeah he's a criminal he's a all of
those things and in the end it's his
messaging his narrative and the way he
constructs things which people D into
any critical thinker if you look at any
95% of what he says it's easy to dismiss
and that's the other thing that worries
me I think if I come back to the
investment being made and the fact that
most people don't think let alone
critically think is my big fear because
I see an awful lot of people like
Lemmings from the Disney film we all
they're all happily marching towards the
edge of the cliff because Jimmy down the
road whose armt works with Fred who's
the one who's in chargeing government to
doing all this says this is all all
right and that is a very difficult thing
to break it's not just climate change
it's across the Spectrum and we're
call in a way I think we've mislabeled
ourselves
um I think as hominids we can't claim to
be the sentient kind sometimes we're
completely dumbass
but I think we're in a position now
where the flood of information and
there's a great book called the loss of
narrative by a German author Chinese
German author who is saying that
basically narratives and not control by
the tech BRS if you look for something
you go to Google it'll give you a listed
set of things it thinks you want but
backed also by bias buing to the
algorithms already there so already our
thinking is being controlled and I think
you've got an issue then that when
you're trying to you're competing
against that regardless of how good the
information is and I think this is why I
like your last bullet so much it's at
least
optimistic you're fighting you're
fighting the the like a dismiss
information you're fighting the with
evil yeah you're doing the good thing
well I mean yeah how do we know I me
like he
say the bad side well I mean and I like
that's pretty depressing
actually everything you just said but um
but you yeah you got to have a crack
don't you so and the and if you don't
then it'll be even worse so if there
weren't like like one thing is we do see
even with like climate misinformation is
one of if not the most well-funded
misinformation campaign in human history
like billions of dollars spent by the
fossil fuel industry and yet when you
look at public surveys all the metrics
are just gradually going up in terms of
acceptance of climate science and
climate change and support for clim
clate solution so the scientists are
winning just not as quickly as we would
have liked because it's the Slowdown
because of the misinformation
[Music]

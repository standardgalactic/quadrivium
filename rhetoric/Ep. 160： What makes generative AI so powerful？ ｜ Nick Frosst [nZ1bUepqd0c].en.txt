good evening everyon and thank you for
listening to the Future a podcast where
we dive into how emerging technologies
will impact the world and your bank
account I'm Trent fer and today I am
bringing you a solo interview because my
co-host Thomas fry is in Korea doing
something amazing and awesome I'm sure
uh but Thomas and I are futurists
keynote speakers and Consultants with
Decades of experience in analyzing new
trends and communicating new
developments to audiences across the
world reach out to us at Future
podcast.com contact Futura if you like
to hire us for Consulting to speak at
your event or to advertise on on our
[Applause]
[Music]
[Applause]
podcast I just wrapped up a remarkable
interview with Nick Frost who is one of
the co-founders of Co here one of the
larger generative AI Platforms in the
world and he also has a background in
COG psychology so he's a computer
scientist a uh cognitive scientist and
also a musician who's running a major
generative AI platform so he's got a
really wide ranging and deep perspective
on this technology its potential it's uh
the ways in which it will be dangerous
or not and so it was real pleasure
talking to them we spent a lot of time
discussing the nuts and bolts of
Transformers the basic constituent parts
that go into large language models we
talked about what it is that llms and
image models are doing the ways in which
that is very different for what human
beings are doing and the reasons why
Nick is very skeptical that deep
learning will ever scale up to
artificial general intelligence or
become dangerous in the ways that many
people are concerned about so this is
part of an ongoing series we're doing on
the philosophy of artificial
intelligence we have done a number of
recent interviews with luminaries in
fields like epistemology philosophy
computer science uh and Entrepreneurship
talking about these technology so
definitely check out this interview
check out those others and drop us a
line if you find any of this interesting
so without further Ado here is my chat
with Nick Frost
[Music]
Nick thanks for coming on the show yeah
thanks for having me man let's hear a
little bit about your background your
interest and what brought you working on
the projects that you're working on
today uh yeah so my my I have an
undergrad in computer science and
cognitive science um I think I've always
been interested in AI stuff so when I
went to school at University of Toronto
it I went to computer science to study
that and then discovered cognitive
science shortly after getting there
um out of school I started working at
Google for a little bit where I was
working on neural Nets um as a like
production engineer and then shortly
after that switched into a research role
in Google brain uh where I worked with
Jeff Hinton on bunch of different
research stuff uh most of it was not
involved uh most was not related to
language but I met Aiden Gomez the CEO
uh and co-founder of coh here and he was
super excited about Transformers and
talking to him made me really excited
about it and then we
started uh coh here in like end of 2019
2020 and we've been working on language
models since
then you guys were pretty early in
seeing the potential of Transformers I
gather uh I yeah I mean he certainly was
he was working on that paper in
2017 uh but it wasn't until a few years
later that we were like oh wow these are
these are these are going to keep
scaling up so yeah at the time it felt
almost a little
late really that's fascinating do do you
have any thoughts as to why the
Transformer architecture under the hood
is so good at capturing like long range
statistical dependencies of the sort you
see in images and videos I think that
took a lot of people by surprise and to
the best of my knowledge it's still not
really clear why that structure did so
much better than relu or some other
activation function or some other way of
setting it up under the
hood uh yeah yeah I have some intuitions
on that first of all Ru is like a
nonlinearity in the thing is like you
can use a railu in a Transformer the
specific thing of a transformer is
the like key value matching because you
have you have a thing that splits your
activations into three and then you're
looking for similarity between the keys
and the and the uh queries and then
based on that similarity looking at the
activations of the
um Keys queries values wow it's been
such a long time time since I've
actually thought about the inner
workings of a transformer it's so funny
at this stage it's just like not it's
not really relevant like it just it
hasn't changed in a while it's just been
consistent and so I haven't like I
remember one of the cohere initial
interviews that we used to give people
was to get them to implement a
Transformer uh and did you do that now
it's been such a long time since I've
given that interview would I even get it
right I don't know anyways um yeah I
think that I think there's two like
things that that work particular well in
Transformers one of them is not very
contentious which is just that the
underlying math that needs to get done
for a Transformer network is easy to
scale on a
GPU that's one so like it it just so
happens that you can make really really
big
transformers easier than you can make
really big versions of other sequence
models so you can make really big
transformers easier than you can make
like big stms it's just easier to scale
them up so I when I was working with
Jeff hon we worked on a an image an
image recognition model called a capsule
Network yeah and this was very good at
image recognition image classification
but very very difficult to scale up so
if you tried to make really big capsule
networks like they they had these weird
inner loop things they used a whole
bunch of memory it was just like really
difficult to make big versions and so I
we never even trained one on like imet
we trained them on CFR 10 and imag or we
did train them on imet eventually but we
never train them on any like real world
production data set because scaling them
up was so difficult and that was a
function just of the architecture itself
so Transformers just happen to have an
architecture that's particularly easy to
scale and that that's like one reason
why they're great it's it's reasonable
to train you know a 100 billion
parameter Transformer model in a way it
was not reasonable Train 100 billion
parameter other other forms of models so
like that's one thing the other thing
that I think is good about Transformers
and this one is like a little
contentious like this like some people
like I've I've talked to some people
about this and they disagree with me so
I might be really wrong about this one
but when I look at the Transformer I
think one of the things it does really
well is looking for agreement between
learned representations as opposed to
just template match
so a regular neuron in a neural net it
has a weight Matrix and it's multiplying
that weight matrix by an
activation and it it's kind of just
saying does this activation match this
template that's it it's just saying like
Okay like the activations I got on this
layer when I multiply them by these
weights do I get a big number or a small
number that's it so there's really only
one way for a neuron in a regular neural
net to be active it's either active or
not active and it can be more active or
less active but it's active in the same
way it's just like this act
this Su layer activation pattern matches
my
weights great that's
it in both a capsule Network and a
Transformer Network you can have like oh
well you know you have three different
representations that are each learned
and we're looking hey does this
representation that is learned match
this other representation
that is also an and if so be active in
this way but like if you have changes in
the activation as long as they're
changed in a way that agrees you can
still be active so capsu network does
this really explicit capsu network is
like looking for agreement in learned
representation so like a capsule a
capsule in a capsule Network can be
active in many different ways as long as
the representations in the level beneath
agree a Transformer is doing like a
simpler version of that in which your
you have your learned
activations in like two in your in your
queries and your keys and you're seeing
if they match so to me like the
transformer like the reason why I got
excited about it is when Aiden first
explained it to me I was like wow that
looks like the good stuff of capsule
networks but in a way that is really
easy to
scale I see I I've had this argument
when I I've had this argument with
people and they've been like no doesn't
I don't really I don't really think that
so I I don't I don't hold this opinion
too strongly but when I have to explain
why the hell is a Transformer so good
why is it so much better than other
things I think it's somewhere in those
two somewhere in those two facts well
it's fascinating I want to come back to
scaling because obviously that's very
important in the current discourse
around AGI and AI safety but I'm curious
I want to drill in a little bit more on
this learned representation thing or are
you talking about the statistical
function that it learns over its data
set like the uh the representation of
the statistical patterns in the language
Corpus it's trained on or or the image
uh Corpus that it's trained on like you
say Lear representation can you tell me
a little bit more about that uh yeah I
mean that's that's the way all Nets work
right like we we start off with we start
off with a totally random neural that
you give it some input and every
representation is just a random
projection of that data so if you giving
it like an image you look at the
activations on the second layer and it's
just a random thing based on the stuff
you gave it in a language model you give
it a bunch of tokens and then we learn
some random activation based on the
tokens that that you gave it so it's the
same thing it's just through gradient
descent that we like learn a
representation that is useful for
Downstream class
classification okay and so you said that
a Transformer is good at finding
agreement between different learn
representations is that right yeah so in
what sense is there more than one in
what sense is there more than one if if
the learn representation is just
captured by the internal weights and the
structure of the the linkages of the
network yeah so in a in a capsule
network uh the activation function is
looking for like it so we have we're
gonna activate this I haven't talked
about this stuff in so long curve
literally been like years because in
some ways it's just we just don't think
about it anymore now we think about the
data we think about like latency we
think about we don't spend a ton of time
thinking about the architectures
themselves um anyway I certainly haven't
thought about capule networks in a very
long time but I do think they're
relevant here so in a in a capsule
Network like let's let's imagine two
different let's imagine two different
image recognition networks one is a
convolutional neural net and one is a
castal neural net our convolutional
neural net for any neuron it has this
weight Matrix which we're going to like
we're going to do a convolution on so
we're going to apply it in a bunch of
different places on the image MH and if
any of those places get a high
activation we're going to we're going to
have a a high activation for that neuron
so imagine we have like an eye detector
like a a little thing that if ever it
sees something that's roughly the shape
of an eye it's going to be
active um in a convolutional neural net
we do that at a whole bunch of different
positions in the image and if there's an
ey anywhere in there we're going to that
ey that I neurons going to be
active but there's only one way for it
to be active it's just either active or
not active and it's it can be regardless
of where the I is it's either active or
not active like a real yeah it's just
like there's an i somewhere in the image
that's the thing in a capsule Network we
don't just have like a we're not just
matching a thing uh and seeing hey does
this does this little convolution fit in
any one place in the image instead we're
going to have a feature that's going to
be active and it's going to say there's
an i and there's an I in this way so its
activation is not just a single number
it's a little vector and that vector or
it's a matrix there's multiple different
ways of formatting it but it's like it's
a little thing that says there's an eye
and it's kind of like
this and you can say it's an eye and
it's an eye and it's big and it's
pointing to the left maybe or it's an
eye and it's like kind of a human
looking eye or you can just you can have
more information in the way in which
it's active and then when we look at
higher level features in the
convolutional version we'd have like is
there a human face there well is there
an eye capsu is there an eye neuron
that's active is there a mouth neuron
that's active is there a nose neuron
that's active if so like yeah there's
probably a face there in a capsule net
we have is there an eye kind of at this
location pointing this way and a nose at
a this location that would also tell you
pointing this way like that it's in the
right location is there a mouth in this
location that's like are they all
agreeing are they making a prediction
for a face in a way that agrees with
each other other or are they making
completely different versions hello this
is Trent Fowler co-host of the Futura
podcast one of the most common pieces of
marketing advice I've come across is to
know your audience and give them what
they want one difficulty in podcasting
is that it's actually pretty hard to do
this none of the major platforms give us
any way to reach out to you our
listeners to find out what you enjoy
about the Futura podcast and what you'd
like to see done differently so we've
decided to record this commercial and
ask you directly to reach out to us head
over to Futura podcast.com go to the
contact page and drop us a line tell us
about your favorite and least favorite
episodes what you'd like to see us cover
in the future and anything else you want
us to know we produce this show for you
and we want your advice so we can make
it even better thank you I see okay so
the activation pattern contains a lot
more information and there's like subtle
relationships that it can catch by
looking at the agreement between the
different representations yeah and now a
Transformer I think is doing something
kind of similar I think in
the in the threeway Lear representations
instead of a thing just you know instead
of we're having a single
activation uh Vector multiplied by the
weights and we look at how large that is
we look to see is there agreement
between the key representation the query
representation and if there is we
multiply that by the value
representation and so that's a way of
each neuron within a Transformer being
active in multiple ways at least I think
it's very wavy and when I have brought
this to people they've been like nah I
don't really think so I think it's just
about the scaling and I think that's a
valid I think that totally might be it I
think it I think it really might just be
the scaling but if I had to be like is
there something beyond the scaling I'd
be like well maybe is this something
special about that well that's important
too because you know we have been
surprised at the sorts of speedups that
you can get by just switching out like
was it softmax with Ru I think and then
when they introduced Transformers as
well were able to just get these
networks they were able to do
substantially more powerful things it
wasn't really clear in in advance why
that would be the
case yeah well I mean ra so R and
softmax are two very different
nonlinearities we're well I is there
ever a time where you could swap out one
for the other I don't know I think
people have occasionally been like oh
surprised at the efficiencies they get
from making small changes to
architecture I would think there's
probably people who are a lot smarter
than me who aren't surprised by those
efficiencies because they understand the
way gpus work and they understand how
you can speed up things by making kernel
changes and stuff like that that's
that's never been something I've been
good at so I think there are some people
who are not surprised at the speed UPS
they get when changing the architecture
um I know that when I was working on
capsule networks I was surprised at how
terrible they were when speeding up when
trying to scale them up and how
inefficient it was and that's because I
didn't really have a good lowlevel
understanding of of tpus yeah I see yeah
I really don't either I used to be a
machine learning engineer but I
definitely didn't have a granular
understanding of how the uh the
underlying Hardware was going to impact
um things like that as as the systems
got bigger I I'm curious as to what you
think about the scaling hypothesis more
generally so a lot of the power of
Transformers came from the fact that
they're just much easier to scale than
an lstm or an alternative architecture
do you think that we're going to start
seeing diminishing returns to just
building deeper networks over time or is
there a lot more juice to be squeezed
out of this approach it depends what you
mean by diminishing return
turns um I think we can make
Transformers that are a lot better than
they are today I think we can keep
making language models sequence models
that are much better
at predicting the next token in their
data set and that generalize better and
that you know are easier to use by
making bigger and better Transformer
based language models I think that's
true do I think we're going to get to
AGI by just training bigger language
models
no why is that that's an excellent segue
then my next
question because I don't because I think
what when we talk about
AGI really what people are often talking
about is like human level intelligence I
mean like can we make a thing that's as
smart and not only as smart smart in the
same way as a person is
smart and and you are doing something
very different than language models are
doing and as a result like you know you
can already see the Divergence like the
types of things that trick up language
models do not trick up people and there
are things that people get tricked up on
that language models find very
easy like language models are really
great as some things that people are
very bad at and vice versa so they're
very they're very different that should
not surprise anybody the way we train
large language
models is very different than the way
you are trained than the way you learn
there was a very similar thing that
happened when image recognition was the
big thing people were like oh man we've
solved image recognition you know we
have computers better than than people
at image
classification when we had convolutional
neur allet and like that's not true we
don't have computers that are better at
Vision than people we have computers
that are better at predicting given some
training data set which you know which
image belongs to which class we have
that but we don't have computers that
are better at Vision as a prop as like a
general ability we have them that are
better in a particular
thing that's kind of similar with
language models now we have language
models that are really great at modeling
some forms of language that people find
challenging but we don't have models
that are like in you can use words the
way humans can use
words so I think when you yeah
look at the history of how neurals have
like what they've been great at what
they've been bad
at you should feel like hey we've kind
of been here before it's just that now
we're doing it in language which feels
like it's a lot broader which feels like
it's a lot closer to intelligence itself
but it's not it's a restricted component
of it and we might excel at that
component but that's not going to give
us intelligence
itself what what is intelligence itself
because I find that a lot of intuitions
are powers by differing definitions
which are often implicit of key terms
like agency Consciousness or
intelligence so do you I mean you worked
in cognitive psychology surely you have
like a carefully worked out intelligence
definition you can just rattle off right
now on the spot no yeah no no no I I
don't I don't
um
yeah so that's why I like when I talk
about it I don't like to talk about AGI
I like to talk about human level
intelligence and I really want to Define
that based on
Behavior like we're going to have human
level intelligence when you treat a
language model the way you treat a
person when you expect them to behave
and exact like do all the things a
person could
do we're not we're not close to there
there's lots of things that like you you
would rely on a language model to do
something very differently than a person
would do
it like like what so like language
models right now uh
well I mean like there's even lowlevel
things like a language model is at this
stage often much better at writing a
summary of an article than a person
would you know they can do it way faster
and they can do it almost immediately
and I could give it very specific style
requests and it can handle that job like
very well but if I give it some you know
textual description of an environment
and ask it to like make some planning
steps to based on you know the way that
environment unfold like it's it's going
to get it wrong a great example of this
is you can take any brain teaser and
like change the brain teaser a tiny
little bit and the model will almost
certainly always get it wrong and I
don't think more training and bigger
more parameters is going to fix that
problem because when you look at the
loss function it's not it's not often
rewarded for like understanding those
things it's it's it's trained on
predicting the most likely next word and
the most likely next word is not often
based off of a deep understanding of the
way the world works it's based off St
iCal correlations in
text right which is very different for
what an agent like U is doing um yeah
but I'm curious you said the loss
function it's not being rewarded for
understanding things uh I agree could it
be though I mean I know I know there's
causal artificial intelligence there are
a couple of other approaches to building
causal understanding into these systems
I I don't know I haven't looked into it
in a while I don't think it's gotten
that far but I mean could you train an
agent to do that because I I know that
they're trying to put llms in Ro in
robots now and they're they're building
these multimodal models that are
gargantuan and quite powerful in various
ways I I think an LA as owski would say
well you know the process of tring a
neural network doesn't look so different
from natural selection and we've
repeatedly been surprised at how just
stacking up more layers has gotten us
dramatically more functionality in a way
that many people would not have
predicted in advance so I I don't I I
think he would say that you can't really
rule out the possibility of a general
mechanism falling out of that because
that's what happened the first time that
humans evolved
yeah I think that's pretty I think
that's prettyy off based yeah I I don't
think gradient
descent is a lot like natural
selection um I also don't think like
when P say we've been surprised like so
I mean I don't think it's been that
surprising I think it's pretty I think
when you look at Transformers in
2019 I'm like yeah where where we got to
now is is about where I would expect us
to get um
I think there have been times when I've
been surprised at how good neural Nets
are but mostly I've been surprised at
being like wow they're actually more
data efficient than I thought they
were like actually you can f tune on a
relatively small data set and get some
pretty cool Behavior out of a out of a
transformer like that's a surprise but
like yeah no I think it kind of made
sense when you saw the very first
Transformer making the much bigger and
training on much more data was going to
get to where we were going to get to now
I think the idea of like doing next word
token prediction and gradient updates
based on that
somehow getting emergent human
level World intelligence out of it I
don't I don't see how we could get there
at all and I don't see any indication
that we are getting there I see we're
getting much much better at next to
token prediction and I think that that's
incredibly powerful I think next token
prediction is like the most is crazy one
of the most useful things we've made a
computer to do and I think they can
still add so much more value just doing
that but I don't see how next token
prediction gets us to intelligence like
you and
me and when speak of how we behave like
here's a here's a really great example a
few years ago there was a Google paper
that came out that showed models that
learn a is necessary and equivalents to
B they do not like if they have a bunch
of those examples in their training data
set they do not learn that b is
necessary and equivalent to
a so if you say like you know a a equals
B they do not learn that b equals
a see and when you think about the
training loss that's not surprising
there's not many examples in documents
where someone says a is equal to B and
by the way B is equal to a that doesn't
happen very often and so there's very
little gradient that should tell you if
a is equal to b b is equal to a you just
learn a is equal to B because people
don't run around saying you know this is
my father and I'm his son they don't run
saying like you know this I I live here
and here is where I live you know they
just say one of those things so you
don't get this
like you know you don't you don't
learn these like logical things whereas
you and I were constantly getting
updates based on reciprocal
relationships like if I am sitting at a
table and I know that the glass is on
theable
but I for that that means that the table
is the glass I'm going to knock over the
glass if I like you know forget these
relationships because I only think of
one direction or something I will you
know run into
walls so we get a lot more updates on
that and we we don't think to say those
things because it's a human wouldn't
think to specify that right like if a
was be was like that symmetry is implied
you just you don't me to spell it out
yeah context we didn't learn that
through language we learned that through
walking around the
world okay you said earlier that next
token prediction is one of the most
powerful things we've got on a computer
to do and there's still a lot of room
for it to add more value talk to me a
little bit about that what do you see as
the most valuable use cases out of
something like this especially in view
of the fact that it's prone to
hallucinations has no truth model etc
etc like what what are the major things
going to be uh major use cases yeah well
I mean the truth model thing is I think
a severe limitation but that can be that
can be circumvented by using retrieval
augmented generation so there's many
limitations of language models but we
can also often get around them by using
a language model to to tr learn how to
use some external tool that solves it so
like a great examples like language
models are bad at math not surprising
they're trained on next token prediction
and trying to memorize all the math
equations is a bad way of learning math
but you can train a model to write
mathematical equations and use a coding
langu or a calculator to calculate those
and then get the model to answer a
question based on that external source
of mathematical
calculation so we can of we run into
problems with language models like oh
they're not amazing at this but we can
often fix them by using a language model
itself to learn how to use a tool to fix
that problem but and when I say there's
so much more value they can add I mean
like I still do a whole bunch of work in
my life that is effectively you know
taking a document and changing it into
another document or you know reading
this long thing and extracting the key
bits or you know taking doing I often do
work that is look at text and solve some
other text problem based on it and like
a language model should be doing every
all of that you know like that's I I
think at some point we'll get into a
thing where you will come into work
you'll open up slack to talk to people
and you'll open up a chat model and like
a huge amount of your work will be
asking the chat model to do things you
know to be like read these documents and
write a brief okay turn that into a
slideshow okay email it to this person
make a tweet like this actually this is
slightly better okay cool I'm done you
know me huge amounts of work that is
just like offloaded onto this massively
power powerful sequence prediction model
well a fair bit of that you can do now
so what do you think's missing from that
recipe I mean I do some of the the
writing tweets or summarizing this
transcript I'll probably do with AI so
yeah what yeah is it agents is Agents
like the next stuff so I mean you do it
but you're also hosting a podcast and
work at a large language model company
right like it's still it's still not it
hasn't gotten everywhere because there's
barriers to entry a lot of the barriers
to entry are like not understanding how
it works but also really simple things
like it's not integrated into your
documents or like it's not hosted in
your environment or it's too expensive
like there's all kinds of things that
are not AGI problems that are just like
development problems that prevent people
from using
okay and you you think like what's
what's your prediction for how quickly
we're able to overcome those hurdles if
you have one I think pretty quickly yeah
I thinky Qui so we're not too far away
from from model being almost like a
pencil or a calculator it's just like
it's ask when you get there yeah I don't
think we're too far away from a bunch of
people relying on it for their work yeah
are you at all worried about
technological unemployment as a result
of that or do you think it's are you an
optimist that thinks it's just going to
Super re charge everyone
and uh no I'm not really worried about
that I think there's still like
especially because I think you know if
you start with my premise of saying this
is not AGI you know there's still like
humans are you know incredible at all
kinds of things that language models are
not incredible at I think we should be
having people do those things and I'm I
think there's sufficient like I'm not
I'm not worried about this displacing a
ton of jobs because I think really what
this is going to do is speed up a bunch
of them now is that going to
change the economy yeah I think so in
the same way that like lots of te any
technology has changed the economy but
do I think it's going to be like massive
like some you know completely
restructuring like no in the same way
that like the internet changed all kinds
of jobs and yet fundamentally like the
world today looks pretty similar to the
world before the
internet are you enjoying this episode
of the Futura podcast if so please like
it give the show a festar on Apple
podcast Spotify or wherever you get your
podcasts and share it with your friends
by far the best way to help us grow is
to spread the word on social media which
will expose our content to more people
and help us continue to bring you
interviews with World leading experts in
AI Quantum Computing cryptocurrencies
and so much more thank you in advance
well but I'm a technical writer so I
mean if the models are good enough to
just take a spec or an API and generate
docs out of it like it's sort of hard
for me to know what I'm going to be
doing other than just talking to people
on slack and signing my name on this on
the
outlet no I mean I think there like yeah
I think we'll use we'll totally be able
to use a spec and write dots but like
will those
docs be understood in the cultural
context will they be presented in the
right tone will they be able to validate
them as a user like will they you know
will they highlight the particular
things you want to highlight like maybe
not you know that's something you'll
still be working on be like a translator
from lmes to Normie
no I think it's more like ultimately you
have to remember that that you know that
the jobs and the economy exist to serve
people right and we're going to make
some things that people do a lot easier
but like
ultimately we do things because people
need them and want them and those are
it's other people who want them and so
you'll be tailoring the output
considering the people who you're
working with and the people who you are
your
customers yeah thesis I Advanced uh I
don't know 20 or 30 episodes ago was
that we we probably move more and more
towards Last Mile creation or Last Mile
work where what you're doing is starting
with a draft of a program or a code base
or a blog post that was written by an
llm and then what you do is tweak it
from there and make sure it plays nicely
with all the other systems and
Integrations and and basically just
handle that last thing it's it's it's an
analogy to uh like last mild delivery
right so for a long time now people have
worried that automated driving uh
autonomous vehicles was going to replace
Long Haul Trucking but like get getting
that last mile the hard part is not
driving 200 miles down the highway the
hard part is backing it into the the bay
at Walmart and unloading everything and
not hitting a kid that's happens to be
running across chasing the ball in the
parking lot so I think more and more
work will move into the long tails but
long tails are long and there's still
kind of a lot to
do yeah yeah I think so I just don't I
don't I want to I don't want to downplay
the importance of that last Mile in in
language stuff like that's actually
where a lot of the meat is like that's
actually where a lot of the in you know
the real hard work
is fascinating uh well this this is much
more optimistic than most of the
conversations we have about this subject
so I'm glad to hear all of that uh I
wanted to to switch gears a little bit
and talk about some of the cognitive
psychology work that you've done in the
past I know that you've done a lot of
cogs side research and I'm just curious
as to whether or not you have any
thoughts on other approaches to
artificial intelligence that might also
be very powerful so Gary Marcus is
firmly in the neuros symbolic AI Camp uh
you know I know some other people who
are interested in causal Ai and various
other uh similar sorts of approaches so
is there anything you've encountered in
doing all of that research that you
think might be another piece of the
puzzle that should be explored well I
want to start by saying I haven't done a
ton of research in it I I have an
undergrad in kogai and I did some
studies with some professors here and
worked with Jeff hindon but uh I I I am
not going to claim a ton of I don't know
a huge pedigree on this there's a lot of
people who spent a lot more time
thinking about it than I have um but are
there other forms like
uh no
and by that I mean neural Nets are the
are the thing we've made that is the the
coolest neural Nets gradient
descent statistical models of data sets
are the thing we have done that seem to
generalize in a the way closest to
people and that makes them the most
exciting and I think often the most
useful so the other forms of like you
know a I guess some forms of AI that
were like amazing that we don't think
about anymore like you know uh Monte
caros search for chess was like really
cool and figured out how to play at
chess bot you know beat everybody at
chess like that was amazing but like
it's been long enough now that we just
kind of we don't think about that as AI
anymore we think about like yeah it's a
thing a computer can do cool it's not
surprising it doesn't cause us any
philosophical
angst um I think language models will
get will get there at some point they
won't cause us any philosophical angst
they'll just be like yeah that's a thing
a computer can do you can train it on a
whole bunch of statistics it can update
based on what it has learned in that and
then write very plausible reasonable
useful stuff based on it but it's not
going to cause you
philosophical angst I don't think the
other things like like the other
approaches that I've seen over the years
they haven't gotten closer than neural
Nets have gotten now I also don't think
neural Nets will get there I think we'll
need something completely different I
haven't seen it yet and I don't know
when we'll see it I I'm not like people
who are running around saying like AGI
in two years or like I I don't think so
I don't know when we'll get there I have
no reason to think we won't at some
point I'm like surely you know why
couldn't we build a a mind out of a
computer I see no reason why we couldn't
but I haven't seen anything that would
get us there and I think we'll need a
whole bunch of independent breakthroughs
that will look very different like what
them like than what we have today to get
us there
that's fascinating so I I don't I know
you don't want to spend too much time on
this and I don't either but assuming
that we did get to AGI like a humanlike
mind in a computer do you think that any
of the safest concerns then become more
plausible like recursive
self-improvement or misalignment or
something like that or or do you believe
that a silicon based mind built by human
beings would somehow internalize many of
the same values or be steerable in some
way that's uh renders the whole Terror
around the
you
so I
think one hypothesis is that if we're
going to build a humanlike mind it's
going to be limited in the way a
human-like mind is limited like that
might be true and if that's true then
yeah we don't it's not there's not a ton
of it's not like there's new fears the
the the the worst is you know a the
worst is a person with malicious actions
like we already have that we have lots
of malicious people out there trying to
do malicious things we have a system
that prevents them for the most part so
if it turns out that to build a human
mind it needs to be limited in the way a
human mind is
limited then like no there's not
something to worry about if it turns out
we can build something that is a human
mind that isn't Limited in the way a
human mind is limited that isn't uh
mortal that can be copied many times and
still work together that can you know
build itself and recursively improve or
something then yeah there's something to
be worried about and like that's that's
an interesting philosophical
conversation I think it's much too early
to have those philosophical
conversations given where we are now and
I
think the past two years of history have
kind of already made this point right
like people started having the like dorm
stuff the like oh no language models are
going to you know destroy the world like
they've been talking about that for
years and as we've seen language models
get better we have seen none of those
fears come true the fears that people
have been thinking about are a lot and
like need to continue to fear about are
are a lot lower level and a lot more um
like subtler but in some ways more
important right like I do want to I do
want people to talk about how the job
market is going to change based on this
I do want people to talk about the you
know uh uh uh identity authentication on
the internet and making sure we're not
having a bunch of you know Miss and
disinformation conversations with Bots
thinking their people like I do want
people to be thinking about these
immediate real world risks and I've been
pretty optim
uh over the last little bit seeing
governments swap away from talking about
Doris and like bioweapons and more into
misalignment dis
disinformation uh bias propagation
things like
that okay fasinating um I wanted to
spend a little bit of time talking about
your interpretability work uh
specifically what you covered in your
paper neural additive models
interpretable machine learning with
neural Nets uh first I thought that we
could just cover what you covered in the
the abstract of that paper that's why is
interpretability so important in the
first place why is this something that
we should care
about well I wrote that paper a while
ago but and I think I'm not even sure I
wrote I was like a the second or third
author so I'm not I don't not even sure
I wrote The Abstract but maybe I mod did
it uh I don't so I don't remember but if
you ask me why is interpretability
important I would say well
interpretability is not actually that
important intervention is important
people don't actually like okay so a a
that is completely
interpretable if if you want to say I
gave you this input why did I get this
output I can completely explain it I can
say well I did this matrix
multiplication followed by this
nonlinearity followed by this matrix
multiplication then this nonlinearity
and this was the output that's
completely interpretable right like you
actually you actually understand much
much more why a language model predicted
one next word versus a person we can
just do all the math and we could be
same with image classification why did
the image classification model give this
classification based on this image well
here's the math we did and that's the
result we got but that doesn't satisfy
anybody because they actually don't
care about being able to understand what
the model is doing or why it made this
decision they care about intervening and
saying like well actually you shouldn't
have made this decision and I'd like you
to make this decision based on these
information or I want to bias you
towards this or I want you to do
something else so it's not just the
explainability that's important it's the
intervention and theur additive models
was a way of doing that that was saying
like what if we have a you know
multivariable prediction and we're going
to have a whole bunch of different
inputs and we want to be able to control
the consequence of each of those inputs
well we're going to learn a little
function for each one and then we're
going to add them all together and that
way if you want to take one out or
augment one or change one you can do
that you can directly change the process
that is happening when making a
prediction and that was something that
they were as easy to do with
them do you need a dynamic and
knowledgeable speaker for an event
Thomas fry and me Trent Fowler are both
seasoned keynote speakers able to
converse on a wide array of topics to
audiences of all sizes and skill levels
go to the contact page at Futura
podcast.com to book Thomas or myself
today and let us apply our years of
experience in public speaking to make
your event a smashing success F so you
can actually change the pattern of
outputs with with neural additive models
so what the neural additive model is
like imagine we have you know we're
going to make some prediction based on a
whole bunch of attributes of you or
something that we're going to make a
prediction like a said people are going
to predict like a loan that you might be
able to take based on attributes and one
of those is your age and we might notice
that hey actually when we train the
model on the data set it says that like
you know we should give loans to people
who are above 30 and not give loans to
people who are below
30 and we can see when we train if you
train a regular neural net figuring out
that it's doing that is kind of
difficult but if you train uh a neural
additive model on this you're going to
see a little you can make a little graph
for each input variable and you can show
what the consequence of that input
variable is on the prediction so we
could say hey when you're below 30 it
says really negative and when you're
above 30 it says really positive or
something and that then we could we
could look at that and we could say
actually we want to correct for that we
don't we don't want to make the decision
based on that attribute so either we
want to say okay when you know raise
them all up or we want to take it out
completely or something but it allows
you to intervene in the process more it
kind of sounds like shap for neural
networks you familiar with shap no it's
shapy additive something or but it's
it's it's like lime a little bit
basically uh and this is off the top my
haven't done this in years but B
basically you're able to take
permutations of different input features
and test different combinations of them
and see how each one is sort of
impacting the final one so it sounds a
little bit like sha for girl at works I
think okay this is all stuff that
happened six years ago so you're having
you're stretching my memory uh but yeah
I think it is kind of like that the
difference is that the thing we're
learning for each variable is a little
nural at itself and so that means it can
it can it can model arbitrary functions
well that sounds like lime for neural
network so with with lime you do
something similar you take one row of a
data frame you perturb the data by
sampling an input around it uh or a
distribution around it then you feed
that to the blackbox model get out
perturbed outputs and train like a
linear model on that so you train like a
little local model that can tell you in
this region kind of what it's doing yeah
that sounds similar yeah it it kind of
seems like a huge deal if if you're able
to model it that well and also intervene
in the outputs I'm curious as to why I
haven't heard about this because I
follow Neil Nanda and chrisa and some of
the other interpretability people like
how is it that I've not heard about
neural additive models if they're as
good as as they're they're particularly
for like um they're useful for data
sense like where you have you know a
bunch of discreet features about a
particular thing they're not like it
doesn't help you at all with images
doesn't help you with language and the
stuff that
like interpretability is in is in like a
neural you could train a neural additive
model on an image and you'd have like a
little variable for every pixel and it
would be terrible it wouldn't help it
would tell you billion functions
wouldn't so what they're what they're
working on now I'm interested in is is
uh yeah language and images well to
begin of it was images now now it's
language okay I see so the the domain
appropriateness is different for the
different approaches well that's
fascinating though I wonder if there's a
way to move it over maybe group them
similar distributions or something
develop constellations I I don't know
I'm just I'm ranting um okay so I I know
that you are the lead singer and
guitarist is that right of good kid s Oh
I thought it maybe it was Wikipedia
somebody said you were guitarist okay so
you're the lead singer of good kid you
guys recently went on tour uh and I know
that you also published a paper called
text conditional lyric video generation
and I'm I'm curious as to whether or not
you have used generative AI in the
creative process if you think there's a
whole lot of potential there for that if
you're at all worried about copyright
there's a whole hornets nest we can kick
here yeah so again also that was another
that was about a six-year-old paper I
think what happened is you looked at all
the pap and you realized I've been
working at cohber four years and so I
haven't been writing papers so all these
papers are very old yeah uh but that's
because I'm yeah much more focused on
you know actually solving problems with
neural Nets uh these days and like
making a viable useful product that I am
yeah
right um but yeah we we made a uh
actually me and the drummer from good
kid made of neber many years ago on like
generating music videos based on taking
lyrics and doing a latent Space Walk in
an image in a text to image model to
like make cool little videos um but I
guess like to your question like I am a
lyric Lyricist like I write lyrics and I
get a lot of people ask me the question
like do you use you know coh's language
model when writing lyrics
and the answer is like no no I don't um
and that's because I'm not interested in
writing lyrics
faster I don't actually I don't
particularly want that process to be
efficient I want it to be personal I
want it to be emotional I'm not
interested in getting that emotion
faster I'm interested in sitting and
thinking about it I do sometimes after
writing lyrics haste them into cohered
models and say hey like what you know
what do you think about those lyrics
like can you summarize the themes or
something like what tell me about the
imagery that's used and then sometimes
based on what I get from the model I'll
update my lyrics like if I write in
bunch of lyrics and I say what do you
think this is about and it says
something completely different than what
I intended I'll be like oh maybe missed
a mark a little bit like maybe what I
think is evident is not evident and so
I'll go back and I'll make it a little
more clear or sometimes if I put in a
bunch of lyrics and say what do you
think it's about and it's like this is
about exactly this here's exactly what's
happening and might be like wow maybe
I'm you know maybe I'm hitting it over
the head a little too many times maybe I
should make it a little more subtle or
something so I'll like change what I'm
writing based on the feedback that I get
from the model but I don't ever write I
don't ever get the model to write for me
because I don't want efficiency out of
the process of art creation you know
yeah it's meant to be personal it's
meant to be the stamp of a of a mind or
a soul like yours that people feel like
they can communicate with or connect
with through the the medium of I mean
that's what I want out of art I think
there might be some people like I've
done some art with neural nness where
I've been like you know it's really cool
that I can make a million versions of
this and that's been involved in the
artistic process I like it's cool to
incorporate this but for the lyrics I'm
writing for good kid each one of them is
me trying to express something and
getting a model to write it for me
doesn't help me Express
that do you have any information on
other people other artists who might be
using this I'm just thinking of having
it like a conversational partner it's
just like yeah tion or a feeling and
it's like how do you react to this you
know I kind of use it as a I kind of use
it as like a like a you know something a
bounce ideas off of vacas soundboard
yeah and that's like helpful I imagine
I'll keep using it for that but I don't
use it for writing a cell phone I don't
really know anybody who does either
because again I I don't know anybody
who's trying to do it faster I uh I it
was a connect week somebody said that
they were having pretty good luck
um putting in like guitar tablet and
having it generate things like like
guitar music as a result of that I
thought that that that'd be a cool thing
to do is like train it on pfia or a good
kid or or one of these bands and have it
generate new stuff or or exercises for
you to to master the original song or
something it just seems like there's a
lot of but to be sure yeah yeah that be
maybe connect week 2025 we can play a
song in the in the style of whoever
generated by uh command R 3 plus or
whatever it is at that point yeah uh
well do you have any other final
thoughts for the the audience any
anything else you want to communicate no
man I think this yeah this is thanks for
thanks for having the time this is this
is
enjoyable absolutely I appreciate it I
appreciate the optimism in your
perspective to somebody who's actually
in the trenches building these tools uh
it's it's been very informative and I
really appreciate it right cool well
I'll see man where should where should
people go to find you I me you got music
you got all this other stuff where
should people go if they want to find
you oh I mean Spotify for good kid and
the internet for coh here for cooh here
and then Nick frost.com I think it's
your website with your sixy papers where
I I haven't updated that in six years
so and the birds the bird link it
doesn't it 404s now I was like oh oh yes
I I haven't I haven't Su said in a long
time okay excellent well thank you so
much thank you again good luck with
everything and I'll see you around cool
see you later thanks man bye

I think that the broad situation with AI
is that they're currently successfully
scaling them more and more powerful
we're not quite sure how long that
process is going to continue using the
current technology and part of the
reason we're not sure is that nobody
understands what actually goes on inside
a modern AI we can look inside the
computer of course but we see giant
arrays of floating Point numbers and
people are barely beginning to
understand what's going on in there and
we do not quite understand where the
power comes from chest did not sto
playing when it got to human level go
did not stop playing by the human level
they are past only training the AIS on
imitations of human output and have
moved on to training it on what works to
solve problems and you can just continue
that process past the point where the
problems are of human difficulty so
we're on course to get something smarter
than us whose innards we don't
understand which we cannot very well
control and this probably ends quite
poorly we're making the statement that
the one thing we think we know about the
AIS is that they have been successfully
trained to optimize their achievement of
purposes that's a that's an assumption
which is not totally obvious to me I
agree that as you give sort of all
you're specifying is sort of play chess
or whatever else so you're specifying
some big what we think of as an
objective the details of what's
Happening inside there will be aspects
of that that we are not in any way able
to foresee predict whatever else so if
we took apart that mechanism and we say
is this mechanism doing what we expect
here it won't be there'll be plenty of
things where it's doing what it chooses
to do or because that particular
training run gave this particular result
or whatever else to me I'm not convinced
yet there are interesting questions to
try to answer which people should try
and answer I mean you make it sound like
it's urgent you know the people are
coming off the ships and and you might
be right
mlst is proudly sponsored by TW for AI
Labs they're based in Zurich they're
doing monthly meetups and they are
hiring cracked research Engineers to
work on Arc and to make progress towards
AGI go and apply now to for labs. one
matters is the strong desire to solve
the The Challenge and the technical
capability usually the type of people uh
we tend to find have computer science
backgrounds um gentlemen it's an
absolute honor to have you both on mlst
thank you so much for joining us today
we're going to have a discussion all
about AI existential risk and um elza
first if you wouldn't mind could you
just spend about five minutes talking
about how much of a risk is AI to
humanity I think the term risk is
understating it if there were an
asteroid straight on course for Earth uh
we wouldn't call that asteroid risk
they'd call that impending asteroid ruin
or something like that and I think that
the broad situation with AI is
that uh they're currently successfully
scaling them more and more powerful uh
we're not quite sure how long that
process is going to continue using the
current technology and part of the
reason we're not sure is that nobody
understands what actually goes on inside
a modern AI um we can look inside the
computer of course but we see giant
arrays of floating Point numbers and
people are barely beginning to
understand what's going on in there and
we do not quite understand where the
power comes from we like by great
efforts you can look in in like the
previous generation of the technology I
I don't know if anybody's tried it with
the current generation and see it's
currently thinking about um the Eiffel
Tower or uh this is like the location
where it stores the fact that the Eiffel
Tower is in France and we can move it to
ro to Rome uh by poking at the numbers
which is you know pretty impressive we
can't do that with a human brain um but
the basic question of sort of like
what's going on in there how are they
doing the impressive new parts that's
all
unknown um if the technology continues
to scale which it may or may not nobody
knows um it at some points gets smarter
than
us uh as smart as us and then smarter
than us there's no particular barrier
that I know of or that I know of any
principled reason to believe in at the
human level chess did not stop playing
when it got to human level go did not
stop playing by the human level they are
past only training the AIS on imitations
of human output and have moved on to
training it on what works to solve
problems and you can just continue that
process past the point where the
problems are of human difficulty so
we're on course to get something smarter
than us whose innards we don't
understand which we cannot very well
control um and this probably ends quite
poorly there's a lot of mistaken reasons
to believe that it automatically ends
well
um it's various people have different
mistaken reasons to believe it
automatically ends well there's a I
don't know if any of you I don't know if
you the viewer I don't know if any of
the other people here have this
particular Mass apprehension but people
ask why wouldn't they just trade with us
they have heard of Ricardo's love of
comparative advantage a very important
theorem in economics which says that
even if one country is more productive
at producing every kind of good than
some other country they will still end
up trading because of the relative
differences in productivity if it is
easier for me to make hot dog buns than
it is for me to make hot dogs I will
ship you hot dog buns and get back hot
dogs or or like if compared to you I am
I'm at less of a disadvantage in making
hot dog buns than I'm at a disadvantage
in making of hot dogs we will both
benefit by shipping hot dog buns and hot
dogs back and forth um this
unfortunately assumes the absence of a
third option which is for B to be so
much more powerful than a that they kill
a and take all their
stuff um there is not a theorem saying
that the work produced by horses has to
be worse worth more than the cost of
feeding the horses you can always
benefit by trade but you can sometimes
benefit even more by killing people and
taking their land I wish that wasn't a
possibility but it is and so there's all
these reasons that people have to have
for thinking that even if we don't
understand AIS and can't control them
very well they could get much much
smarter than us and it could still end
nicely and I think these reasons are all
mistaken and so by default this ends
terribly and I think that's my five
minutes I mean I guess my it's
interesting I mean my first statement is
this notion of kind of an index of
smartness is something that I kind of
don't believe in I mean one thing that's
sort of my personal experience for the
last I don't know 40 something years is
I already know the computers are smarter
than me I mean I you know it's kind of
uh uh I started doing these experiments
on kind of what's out there in the
computational universe years ago just
looking at very tiny simple programs
where you know you might have thought
this is a really simple program surely
I'm smart enough to figure out what this
does but no one's not I mean every like
even last night I was doing some
experiments where I was pretty sure I
knew what was going to happen but no the
you know the computational animal was
smarter than I was and uh it did things
I didn't expect so i' I've kind of I
guess my you know my default point of
view is I know the computational
universe contains many things that I
can't readily foresee just as the
physical Universe contains many things I
can't readily foresee so the question
then is you know one thing one could
take the point of view there's this kind
of single index of smartness I mean I
think people you know in the 1930s
people wanted to invent kind of an index
of general intelligence they called it g
for humans which I've never really
believed in because I've never really
believed that uh there are some things
I'm pretty good at doing where I feel
I'm pretty smart there are other things
where I know I'm pretty dumb and it's
it's kind of a it's not really a sort of
single index I guess I guess the thing
that I wonder about in the kind of the
the AIS will be smarter than we are is
to dig into what that really means and
you know I think the first thing to say
is you know that uh you know the big
limitation to whatever you might think
smartness is is this phenomenon that I
call computational
irreducibility the idea that if you have
a computational system the question is
can you jump ahead we are used to the
idea from lots of exact science over the
last few hundred years that oh you know
the planets do what they do according to
these laws of motion but we can find a
form formula that lets us just work out
what will happen in the end without
having to go through all the steps one
of the things that's sort of a
foundational fact that we can talk about
you know where it comes from but you
know it's it's kind of the the whole
idea of computation leads to this fact
that there are many computations that
one can set up where to know what will
happen you really have to just go
through all the steps and see what will
happen you can't you can't kind of jump
ahead now a lot of what we've achieved
in in science in mathematics in is is to
find little little pockets of
computational reducibility that allow us
to Jump Ahead in someplace or another
that's what most of our inventions have
to do with taking what is out there in
the sort of computationally irreducible
world and finding some particular place
where we can jump ahead and where we can
kind of foresee what's going to happen
and make use of of those things so I
mean I kind of I kind of think the first
thing to realize I think is that you
know smart as one could be so to speak
there's no way to get out of
computational
irreducibility and the you know in a
sense then what one's talking about is
there are these pockets of reducibility
we find some there are ones that we
humans care about there are ones that
our civilization depends on there are
lots of other pockets of reducibility
that we haven't found many of which we
probably don't care about in the current
way that we are thinking about things
and you know it's it's I I see it I
don't see it as being this kind of
linear oh this thing is Now smarter than
us so so Doom so to speak I think that
it is you know and as I say my own
personal experience has been I'm
completely used to the idea that that
computers are smarter than I am and you
know while uh you know there's there's
sort of the question of do are those
computers connected to everything that
runs my life so to speak that's a
different issue but in terms of the the
raar experience of you know are the
computers smarter than I am well you
know I already know that are now I think
the the other thing to say is can the
computers get smarter than we are in
some sense in every Dimension where
where we humans are used to doing things
um that's that's a different issue but I
think that's a different issue from from
um uh and then there's the question of
of sort of what does it mean if we live
in a world where there's lots of things
going on that are sort of smarter than
we are in some definition of smartness
well one point to realize is the natural
world is already an example of such a
thing I mean the natural world is full
of computations that go far beyond the
computations that we do in our brains
and you know how do we manage to coexist
with the natural world well we found
these particular niches where you know
we it doesn't matter that it rains a lot
because we build houses and so on we
found these ways to coexist with the
natural world that seem to let us lead
the lives we want to lead there are
things we can't do because our
physiology you know our biology doesn't
let us go there but we seem to be you
know we seem to be able to live
contented lives even though we can't you
know go to the bottom of the ocean you
know without you know having some
complicated you know piece of technology
and so on so I mean I I think that's
that would be my my initial kind of uh
thinking about about sort of why I'm not
as worried about the moment when you
know the intelligence of the machine
goes from you know 150 to 160 to 200 to
300 whatever might me I mean you know
you know Galloping through the Ravens
Progressive matrices at a at a at a
rapid clip is not um uh you know again
you know from my point of view it's kind
of already it's already happened that
there are many things in that you know
the that computation can do that are
beyond what I can do with my sort of
uned mind so to speak so I'm curious I
mean do you see the the issue as being
kind of a
more of a we put the AIS in charge of
our you know air traffic control and our
you know medical devices and now this
and that and the other and so at the
sort of actuation layer things kind of
go wrong or do you see it I mean the
thing again that I'm I I have a bit of a
hard time with is the kind of it feels
very anthropomorphic to talk about kind
of you know the AI wants to do this the
AI is you know wants to trade with us or
not or or whatever that that you know I
could talk talk about the natural world
in the same way does does the weather
want to give us a good time or not I
don't even know what that means could I
just segue as well and elzar I
understood your point as there's a a
powerful force which is going to push us
out of equilibrium and I think step is
making the argument that we are in an
equilibrium now is is is that fair that
didn't sound fair to me okay so I I
understood that Steven was making the
argument that there are these
counterveiling forces that prevent
significant deviations no I that's I
don't think that's what I was what I was
saying I mean I was I was I'm not
claiming that the world won't change
okay I'm simply saying the idea that
it's kind of you know game over because
suddenly the IQ crosses 300 or something
I don't believe in that I I mean I think
we probably both agree that iq300 is not
meaningful right okay um I'm not
surprised we both agree about that but
but no the the the point that I'm making
is that this idea that somehow the AI
gets smart enough to be able to do
everything I don't believe in that I
mean I think there's this sort of
computational irreducibility limit that
you know you know great neural net or
not that's just a a fundamental kind of
formal limit we don't get to get around
so I first want to try to sort of repeat
back your summary and your summary is
like because we cannot predict a thing
it shows we're not smarter than that
thing and even in a sense that it's
smarter than us because it's something
that it knows which which is its own
output that we cannot predict ahead of
it does that sound like a correct
summary of your well part of it I mean I
you know you know the problem with a lot
of these things is we get a word like
smart and then we don't you know we kind
of think we know what it means but then
it kind of slithers around on us and I
think that's you know that's part of uh
uh but but I would say that you know
what I'm saying is merely that if you
say I want a system that can solve every
problem you know will I get to the point
where I have an AI that can just slam
dunk solve every problem I'm saying
that's not going to happen no and in
particular it's entirely possible that
if you pick a sufficiently strong
Quantum proof oneway hash that and hash
something and throw away the key that
nothing whether nothing at any point in
the arbitrarily far future even if it
has consumed entire galaxies toes
computing power will ever be able to
find the input that you fed to that
oneway hash um so in this sense
Universal problem solving is probably
not on the table as far as we know given
the present laws of
physics but you know the the the
European invading Europeans in invading
North America did not need to be able to
solve every problem to solve more
problems than the Native Americans and
mostly wipe them
out um a fire that's true a fire doesn't
need to be able to burn uh neon or other
noble gases in order to be able to burn
you personally doesn't have to be
Universal and you don't even need to be
able to Define exactly what fire is to
die in a forest fire people have
probably died of in fires a long time
before they worked out the concept of
combustion and chemical rearrangements
with oxygen that had ended up in states
of lower uh potential energy which
released that as kinetic energy which
looks like heat which is what burns you
people didn't know that and they still
died in forest fires and I I can try to
Define what's in intelligence but before
I do that I do want to point out that
being unable to define something is not
bulletproof armor for sure but I think I
think the you know the question is it
seems like the argument that you're
making which may not be correct is I
mean I may not be correctly representing
it is you know there will be you know AI
is advancing rapidly we both we can talk
about you know what we know about how
AIS work inside and the whole story of
computational irreducibility and how it
applies to that and why machine Learning
Works those are interesting questions
it' be fun to talk about those but I
think one of the one of the issues is we
don't know you know just how effective
you know AIS are going to become but we
do know there is a limit to how
effective they can become because we
know that there's you know there are as
you say I I wouldn't go as far as
talking about you know elaborate oneway
hashes and things there are much more
straightforward kinds of things that you
can't predict except by running the
steps cellular particular cellular a
automata yeah for example sure and and I
think we agree on that just just just to
stipulate that part yes yes right so I
mean okay so then then the question is
could it be the case that everything we
humans care about can be done faster
better by AIS could it be that the
things that are important for running
the world and that you know there's
there sort of this you know there's a
question could could everything that
humans do be done better than
AIS um you know that's a different
question from whether AI can sort of do
everything can do things that we have no
idea how to do what how how to do I I
mean even if there's one last remaining
computational task that a human brain
can do better than an AI by like
predicting the exact output of that
particular brain which you're sure not
going to get without doing a bunch of
intervening steps if you want it down
very exactly um you know that is not
necessarily protective against a bullet
right like there were problems that the
Native Americans could solve better than
the Europeans who wiped them out
absolutely you know there will be just
as for example the weather has ways to
wipe us out you know so you know it it's
in any particular I don't know you can
imagine some mega super storm that we
don't completely understand that uh uh
you know because we we're we're talking
about again things we don't completely
understand so I think we can we can as
well say we don't know what's possible
from the weather as we don't know what's
possible from the AI and you know I can
imagine that the uh you know there's
some something some terrible thing that
can happen you know people well people
imagine with respect to climate that
that's going to wipe us out another
thing that involves sort of lots of
extrapolation about what's going to
happen I mean you can kill a lot of
people without killing everyone it is
much harder for the weather to wipe out
every last human on the face of the
Earth than it is for the weather to just
kill a lot of people fair enough and and
and I am and I think that total
extermination is like much worse than
just a lot of people dying because you
also lose posterity lose the meaning of
History okay so there's a there's a we
could segue to a quite different topic
which is you know in the history of life
on Earth there been plenty of times when
you know if we were two stegosauruses
talking about this you know 100 million
years ago whenever stegosauruses were
around the stegosauruses would say gosh
you know it would be terrible if we were
wiped out and if something came if those
pesky mammals that have just the one or
two things that they can do better than
us took over and I I'm curious about you
know your view of the ethics of that
situation relative to this is a very and
this is a very different line of
reasoning from let us Define
intelligence you know the work done of
prediction the work done of steering um
what does it mean to want something does
a chess game wants does does a CH an AI
chess player want to win chess well it's
definitely very good at winning chess
whether you say it wants to or not does
it want to protect its pawns I I just
want to remark on all of the
could have maybe gone to tempor let's
put that aside for a bit later um but
let's talk
about yeah let's we we'll we'll put the
stegosauruses in their pen for a little
while longer and we'll come back to the
stegosauruses later oh we'll talk back
to the stegosauruses later okay I'm up
for that unless you have some quick
quick comment about the
stegosauruses um very broadly that the
trouble with looking at the change from
stegosaur to humans and determining a
and viewing a trend of things getting
better and better and turning out pretty
much K is that as humans were drawing an
arrow around the target if like we got
wiped out and replaced by stegosauruses
and the stegosauruses had you know
didn't care about each other very much
they grew up in different evolutionary
social circumstances you know if we were
wiped out and replaced by insects maybe
the hives of you know Sapient insects
just never care very much about other
hives they just don't carry very they
don't end up caring very much about
Sapient life at all that would be a
grand tragedy um it's not at all clear
to me you can wipe out humanity and
replace it by arbitrary stuff and
everything just gets better as a result
I don't know what better means I mean
better is a very human concept well
that's the whole it I mean there you
have it and yet if we're going to decide
whether we want to voluntarily wipe
ourselves out and replace ourselves with
you know Sapient ants we need to ask
ourselves whether that's better or not
we have no other way of making that
decision I think it's not question
necessarily better it's like we're
humans and we're happy to be around and
we kind of want us to continue cuz
that's you know we're right now we're
kind of the the InCharge Critters around
here and it's kind of like that's you
know let's let's stay in charge you know
why would we give it give it up to
something and and we might have some
some argument that oh of course these
other ones that we might give it up to
are worse than us but you know in the
end it's like we're here and we kind of
like being here so we'd like that to
continue I think at least that's not
what I'm fighting for I want there to be
Sapient life that cares about other
Sapient life that doesn't just discover
things about the universe but
appreciates those discoveries that looks
out at the universe with Wonder and
curiosity rather than mere expected
value
calculations I want I wanted to have fun
I wanted to be conscious I'm worried
that these things get lost and if you
say that oh like well if you want life
to be conscious if you want there to be
Consciousness in the humans
Consciousness in the universe that's
just your preference eler then we might
need to have a long conversation about
what alternate State of metaethical
Affairs you thought might have obtained
other than that to be disappointed at
the absence of her to critique the
absence of but mostly I'm like yeah I'm
the one who says I'd rather Universe be
filled with Consciousness than mostly
unconscious and you know yeah I'll own
that you know this whole question about
you know Consciousness it's like I'm
pretty sure that I have some
Consciousness I have this you know
thread of experience I'm I'm
experiencing things as far as I'm
concerned you're a bunch of pixels on
the screen I have no idea you know
what's going on inside you it is a it is
an extrapolation that you are conscious
the only thing that I can you know
readily kind of from experience believe
is that I'm conscious and you know when
it comes to even an AI you know is this
thing folks conscious or is it merely a
bag of bits I mean in other words and
you know with an AI we can with some
effort go and look at all the bags of
bits with human brains we can't yet do
that presumably eventually we will be
able to do that and once I can see all
those neuron firings in your brain what
how will I think about the Consciousness
in your brain and what what you know how
do I even know what's going on Beyond
from what I can experience inside myself
well I infer that you're conscious based
on knowing that I'm conscious and
thinking that probably something pretty
similar is going on inside you to me
because we have similar great great
great great great great great great
grandparents we are built on like
basically the same engine plan I haven't
actually administered an MRI to you but
I would guess that you've got like
pretty much the same brain areas I do
cerebellum Cal cortex even without
knowing exactly which features of the
brain are executing the algorithm that
experience from the inside as my being
conscious I can guess that you probably
have a pretty similar algorithm um you
know based on claiming to me that you're
conscious and all that right I mean it's
an extrapolation a similar design right
it's a reasonable you know piece of
scientific induction extrapolation but
what I'm curious about is if you if you
say the only thing that can be conscious
is something that has that exact same
design oh no then I I don't say that
okay so what so where's the boundary
so so it's not so much that I'm worried
that machines cannot be conscious as I'm
worried that the particular uncontrolled
super intelligences we end up making are
not going to Value Consciousness all
that much which means that they're not
going to produce very much of it like
maybe Consciousness is the most
efficient way to run some paperclip
making algorithm but if you don't value
Consciousness for your own sake then the
little bits of Consciousness that are
supervising the paperclip making
algorithms are you know not going to be
as as much Consciousness as we could
have made of the universe and if they
don't value fun they're probably not
having fun either they're just being
efficient because they didn't end up
valuing fun that's the that's the
general class of nightmare scenario that
I'm worried about I mean I really wonder
my computer when it runs and does all
these searches and it finds these cool
you know cellular automatan patterns or
whatever else I wonder if it's having
fun I don't I bet 0 I bet it 99 to1
against I don't know everything that
there is to know about this but it seems
to me that when I stare at the Machinery
of having fun there are gears and wheels
there that are not built into a pocket
calculator that are not built into a
simple Python program searching through
a list of cellular autom like I would
usey program that's very low
level but uh sometimes things like sort
of people say you know the one thread
you know the one sort of shred that we
have that machines will never have is
some kind of emotional response to
things which you know I think that's a
very weak concept because I think that's
emotions are are are you know a very
chemical chemically based kind of thing
that's actually very coarse compared to
a lot of the the other things that go on
in brains yeah it's it's it's I it's not
that I think that machines can't have
emotions um I
suspect that Claude
3.5.1 sonnet or
GPT uh
01 if they have anything resembling
emotions they're not our emotions
there's like something inside it that
was trained to imitate emotions but if
it has emotions they would be there my
guess nobody knows is that they would be
more emotions more appropriate to like
carrying out the job of being the actor
who plays the part of a human who has
emotions and not actually the human
emotions themselves and it's not
fundamental to transistors it's just
like what I would guess hap would happen
given the particular way we put these
transistors together so what you value
in the universe and in humanity is a
certain set of attributes that have to
do with the ability to have fun your you
know the meaning of Consciousness and so
on and it is as I'm understanding it you
you think those are sort of the most
valuable things that maybe we as a
civilization we as a species have
produced and you think it is our
responsibility if I'm understanding
correctly you think it's our
responsibility to kind of preserve these
things in the universe is is that a fair
assessment with the exact wordings but
sure you know the light of Consciousness
the light of fun the light of caring to
preserve these things into the galaxies
is the most important thing we have to
do because you feel that's that's sort
of our big achievement and uh it's kind
of like it's now our responsibility to I
don't know quite to what but it's
somehow our intrinsic responsibility to
preserve these valuable
things I mean I think we got handed
these things on a silver platter but
that you know not or or you know like
maybe a kind of rusty iron platter that
we had to shine and clean up a whole lot
but you know still on a platter from
natural selection and even from cultural
processes of development that nobody
really saw coming at the time there you
know few people did them on purpose
somebody did at some point say say of
slavery it is time that some person
should see these calamities to their end
not all moral progress was accidental
but you know still like our basic
situation of being Humanity of being
here on this planet of having the
opportunity to colonize the Galaxy we
fought for this a little bit but it was
ultimately handed to us on a platter and
I don't say that it's like written down
on a stone tablet anywhere in the
universeal laws of physics that we must
bring this to the Galaxy I just say I
want to do it that's what I think is the
right thing to do okay so I mean this is
I you know I don't necessarily disagree
with you um but it's worth understanding
that this is a feeling that you have
this is not something where as you say
there's no law of physics that says this
is the destiny of the universe or
anything this is on this particular
Planet through this particular process
of natural selection of of uh biology
and culture and so on we got to this
point we're really proud of this let's
try and preserve it I mean is that is
that um uh yeah the fact that I want to
steer the universe there isn't a fact
about the whole universe it's mostly a
fact about me sure and a whole lot of
other people to be clear right you know
it's is not like 2 plus 2 equal 4 this
is not uh you know like water is uh two
hydrogen atoms and an oxygen atam this
is you know where I steer the universe
being the sort of thing that I am I I
agree with you I think I think one of
the things to realize is that if you
just let raw computation do what it can
do much of what it will do are things
that we humans just don't care about
just like a lot of what happens in
nature are things that we humans don't
care about we've never managed to you
know sort of take those things from
nature and make them into technology
have those things be things that we
appreciate as beautiful or whatever else
they're just random things that happen
in nature that we don't particularly
care about and most of what a
computational system could do is things
that are you know off the away from the
things that we care about in kind of the
computational universe now that so so
many of those things are just things
that will just happen just like they
happen in nature uh we will kind of
shrug our shoulders and say okay that's
a thing you know a lot of what probably
happens inside neural Nets right now are
things where in fact I have pretty good
evidence of this they're things which
are sort of complicated computational
processes where they don't really
produce anything that is something that
we can recognize or that we care about
they're just things that are happening
computationally and so I think that your
your concern is that some of those
things that might happen or that the I
might do might be things that collide
with what you see as being our sort of
responsibility and our you know proudest
uh sort of uh artifact in a sense that
that that somehow there will be a
collision of what is you know what AIS
computational systems whatever can do
and the thing that you think we as
humans sort of have most proudly
produced is is that a fair assessment
yes if if if it that works out to they
kill us and then go on to do nothing
very much interesting or worthwhile with
the galaxies that they
colonize like is that is that also a
fair way of of describing all of that
well I I don't know I think that I mean
you know this question of okay there
there are several different pieces to
this I mean I think look the first
question is the is the Practical
question you know five years from now 10
years from now you know will we be able
to have this conversation anymore or
will we be you know
you know will we be dead replaced by
bits that are doing what you might think
of as meaningless things but bits you
know will we be will it be the case that
it's as if an asteroid had hit the earth
and and all life was wiped out or or
whatever else that um you know that's
the like if if if an asteroid hits Earth
all the atoms in It Go on doing their
little Atomic things you know the the
little electron cloud uh you know I want
to say circles the nucleus but of course
it doesn't really blah blah Quantum but
you know the thing that Quantum
physicists describ to the lay audiences
as you know circling the Nu the electron
circling the nucleus you know it goes on
doing that maybe maybe some some amount
of fusion occurs if an asteroid hits
hard enough some amount of fision and
fusion but um you know by and large the
atoms go on doing their atomic things
but they're not morally veent things
they're not important things they're not
worthwhile things the Earth the the
universe gets a little darker
every time a bullet gets fired into
somebody's head or they die of old age
even though the atoms are still doing
their atom things right okay so this is
this is I mean it's a it's a to me
that's a that's a curiously I mean I
viscerally I agree with you
scientifically I have a bit of a hard
time I mean in a sense that that feels
that feels like a very kind of spiritual
kind of statement which is not
necessarily bad but it's just worth
understanding what kind of a thing it is
I mean it it is saying that there's
something very uh kind of sacred about
these attributes of humans and that we
have perhaps even a higher purpose to
which we don't really know where it
comes from perhaps we can't um to
somehow preserve and uh you know we and
and but there's a much more practical
thing I mean as humans who like doing
what we're doing it would be nice if we
could go on doing that with without all
being killed by AIS so I think there's I
don't see any
distinction between these you know like
there there's nothing more scientific
about saying I'm a human I like to keep
on doing what I'm doing and like I'm a
human I'd like to fill the universe with
the light of Consciousness even if that
Consciousness is human so long as it
carison is having fun yeah yeah no I I
agree those are both those are both
statements about how humans feel about
things remember they're not false
they're not unscientific there is not a
truth of science that contradicts our
caring no
absolutely it's it's like you know
ethics is not a scientific field you
know it's about how we humans feel about
things and we humans could feel this way
we could feel that way it's a it's to do
with the nature of us as humans and we
could you know we could scienze those
statements by saying let's do an fmri
and notice why do you say that oh you're
such and such lob lights up but I don't
think that's a particularly useful thing
to say I mean I think you know I think
it is a fair statement that this is you
know it it is a thing that we can
capture that humans feel that this
should happen or not happen whatever
else but I guess that the that you know
there's there's one question is what's
the right thing to have happen I don't
think there's any abstract way to answer
that I think it's a question of how we
humans feel about about it and I think
you and I seem to feel I know I feel
that you know preserving kind of what
humans do is a good thing um I you know
I can imagine even humans who say no no
no you know the planet is much more
important than the humans for example
anything the humans do on the planet
that messes up the planet you know get
rid of the humans we just want the you
know the planet is more important than
the humans I think that in terms of
prospects of human existence I'd like to
preserve I would not include mosquitoes
can just like get rid of all the
mosquitoes I think the planet would be
fine I think we'd be fine I would if I
could get rid of this whole aging
business that humans do this was more
controversial it would actually change
things if that
happened and you know some of the
changes might not even be positive but
my guess is that they would be quite
positive on net and I'd be in favor of
and of you know if this were a thing you
could do with a limited AI that wasn't
going to wipe out humanity and you could
know that and you weren't taking any
extra Risk by doing that I'd say you
know use Alpha fold five to solve as
many aging related problems as you
can um and but and some people would
hate that and there's the question of is
one of us making a mistake is it the
case that if you take me and the person
who wants to preserve aging who wants
human humans to keep getting older and
die that you put us both opposite each
other and like one at a time you just
tell us all the facts about the way the
universe actually is you just expose us
to all the arguments that exist to be
considered about this that any moral
philosopher could Advance could event in
you know some sort of random order do we
converge or do we just want different
things and it's not clear to me that you
know this is an instance of conflict
versus mistake it could be that the
people who believe that aging ought to
be preserved forever that there are
things you could tell them about what
would happen if we cured aging that
would change their minds it could be
that there's true things about what
would happen if you cured cured aging
that you could tell me that would change
my mind you could show me some like
dystopian world of 300 years in the
future which is like just being ruled by
Vladimir Putin forever and ever and
nobody's having fun and I could be like
yeah okay that was worse so it's not
clear to me that that we need to say
that they and I care about different
things we might but it's not sure we
haven't actually run the experiment of
telling us both all the facts that can
be determined and exposing us both to
all the arguments that can be thought up
on it
yeah but I mean so so you know part of
what you're saying is when things are
matters of of sort of human decision
different humans might come to different
decisions and it's not clear how one
resolves that I mean in so far as
there's I mean perhaps one of the things
you're saying is when there is
sufficiently great risk it's a bad idea
you know even if some humans say hey we
should be doing this we should be you
know building super killer viruses that
can wipe out the species you know you
know assume a small number of humans say
that that that potentially sort of the
the uh even though they say we really
have good reasons to do this and other
people say that's a really bad idea you
know in in most cases you'll just end up
with two different points of view but
you're I think arguing that there are
cases in which you kind of have to you
know the world has to align on one
particular thing because you know as
soon as there's one super killer virus
that wipes the species out you know that
that's it's game over so to speak I mean
I think that's a bit of a topic change
but yes in the entirely different realm
of political rather than moral
philosophy I would agree that either
your species is able to prevent anybody
from making a super virus that kills
everyone or at least collapses
civilization or your civilization does
not last and I think that even many of
my fellow Libertarians would agree that
you know making and releasing super
viruses ought not to be legal that this
is a proper thing for government to do
to you know ban the releasing of
extremely lethal engineered super
viruses But but so as I'm understanding
it you think that the same thing is
basically true about powerful AIS yep I
think that we're in a situation where if
anyone builds a super intelligence
everyone everywhere dies and I think
that that is a sort of thing that
government where it's a proper use of
government to try to have that not
happen I would sooner abandon my
political philosophy than commit
planetary suicide about it so you really
do see those as as sort of comparable
things the the the super virus the super
intelligence these are both things that
you think are things that are sort of
comparable risks that um uh you know are
um I actually I wanted to come back to
the this question about you know
immortality because I'm I'm big on
immortality as well I think we're I
think we're both interested in konics I
remember talking to you about that years
and years ago um you know it's kind of
shocking that cionic hasn't worked yet
maybe that's one of the tests of the you
know the next generation of AIS is can
you know can the AI solve the problem of
you know how to get water uh you know to
to cool down without expanding so so to
speak but um uh you know in I'm just
sort of curious from your sort of moral
compass point of view if immortality is
achievable but only
digitally what how do you feel about
that I mean in other words if you're if
you're going to you know right now
you're you you start having your sort of
backup AI maybe you gradually you know
your sort of thread of Consciousness
gradually migrates from being in your
brain to being in your AI and eventually
you know your brain fails for some
biological reason and then it's all the
AI I'm I'm curious how you feel about
that whether you whether you feel that
that is a a kind of a an appropriate
kind of fun preserving outcome or
whether you think of that as being a um
uh kind of a a fun destroying outcome so
to
speak I think that there's a whole lot
of work to be done in making sure that
you are getting all of the functionally
relevant properties of every neuron in
the brain that you scan you know up to
whatever is just like random thermal
noise um and then as long as you've done
that work sure sign me right up you
don't even need to do it like through
over like 20 years or something like you
can do it over a day you can knock me
out scan through my entire brain
Destructor and wake me up when it's over
as long as you have actually gotten
every single functional relevant
functionally relevant property of every
neuron and you know act and you are able
to simulate them correctly uh you know
if I trust a super intelligence to do
this I'll say sign me right up if it's
like some kind of shady back alley human
doctor I might have a lot more qualms
but so so you you think in that
situation where you've been fully
scanned and reproduced digitally when
the digital copy is switched on it's you
waking up
yep and and how do you I mean does that
uh because you feel like your only
connection with the past you is your
memory anyway and in that case you would
have that memory is that right I uh no
it's because I think that I am like the
the functional
properties of my neurons that I can see
from inside like like if one of the
electrons somewhere inside my brain was
secretly a different flavor of electron
never mind how hard this breaks all of
physics but it was like secretly
different flavor of electron that
otherwise behaved exactly like all the
other electrons except for not being you
know like Quantum interchangeable so
follow you know uh Exclusion Principle
doesn't apply to it but you know
otherwise it's just like exactly
functionally the same I can't tell I
can't see that if you swim up to one of
my neurons and while it's otherwise not
firing replace it with a robotic analog
that behaves in exactly the same way I
can't see that I can't tell that
anything has happened it doesn't affect
me well so this though comes to the
essence of what the U is because you
know and and this also relates to
current AIS you know one of the big
surprises with something like chat gbt
was that it could be human enough to be
able to write somewhat credible essays
it wasn't obvious that that was possible
it could have been the case that to
reproduce human language required some
new physics in the brain of you know
quantum gravity in the brain or some
such other thing that we is just
completely out of reach to our kind of
uh sort of current computational
techniques but in fact you know it seems
that we were able to to capture enough
that we can have it write sort of
somewhat humans sounding essays so the
question would be you know when we
reproduce your brain do we have to
reproduce it only functionally or do we
have to reproduce it with every gal cell
you know represented with all of its
chemistry and so on I think you need to
reproduce the functionally relevant
properties of every gal cell if you're
doing something to gal cells where I
would notice where there' be like a
detectable subjective change if you
could give me a questionnaire and I'd
answer differently on the questionnaire
afterwards like if you are changing all
of the gal cells in that fashion you
have perhaps kill me okay okay so so so
what you're saying is if you can behave
to the outside world like you like
answer questionnaires the same way then
you are adequately the same no I it's
about what I can tell internally not
externally with if you find a
sufficiently smart actor an actor who is
enough smarter than I am they can maybe
play my part in a way that fools even my
closest friends but that is not me
and the actor knows they're not me they
can tell internally even if they're
managing to mimic me externally so I
mean for example if you you know I'm not
I I value my brain too much to take
drugs of any kind but uh you know if I
if I didn't have quite that point of
view I might you know feed in molecules
of some some crazy psychedelic thing and
uh you know then then my brain is so am
I still me when I've done that you know
I've changed my brain chemistry by
putting in some some strange drug does
that does that demme me so to speak
there we start to get into the edge
cases where I start to feel more
uncertain in my answers I would feel
feel very nervous about taking an
uploading procedure about going through
an uploading procedure that felt like
temporarily being on drugs to say
nothing of permanently being on
drugs but yet you know it still you know
it's still you even if you took that
weird drug it's still you know there's a
continuity of youness does and do I know
I me or does somebody else now know that
they are themselves there's some
sufficiently you know I can there
there's sort of like two different
questions here one is something like
what do you see happen to you from the
inside do you experience dying or do you
experience a change from one person into
another
I'm not even entirely sure that this is
the right question to ask but it's a
different question of do I care you can
like there's plausibly some sufficiently
Advanced pill you could feed me that
would you know produce changes to
neurochemistry where I just stopped
caring about other people at all and
maybe I experienc ending up in that
person maybe I experienc ending up as
that person but I still wouldn't want it
and well you would beforehand you
wouldn't want it I mean this is another
comp
you're you know it's kind of like you
know you know at any given moment you
feel it's similar to what you what
happens if you think about kind of human
purpose across human history if if we
say you know right now uh you know we
think certain things are meaningful like
we you and I might think talking about
philosophy as meaningful other people
might not think that was meaningful but
we might think that was meaningful back
in and you know we might think you know
back in the day people might have
thought maybe some do some do now if
you're not growing your own food you're
not leading a real life so to speak or
if you're not you know uh sort of
fighting for the greater glory of God
you're not leading a real life for
example and you know if we project to
the Future let's say that you know the
the uploading thing works and in the end
sort of all human consciousnesses can be
in a box
and to us today it might appear that
those human consciousnesses are just
playing video games for the rest of
Eternity who whoa whoa whoa well there's
a big difference between like are you
running on Silicon versus carbon and
what are you actually doing even if
you've got a bunch of organic humans and
you've put them all into a box and
they're all playing video games you know
maybe I object to that part it's got
nothing to do with whether they're
carbon or silicon it's whether you put
lock them in a box and force them to do
nothing but play video games right but
but the point I was going to make was
actually slightly different one which
was you know to us today to you and I
probably certainly me today the future
of it's a trillion souls in a box
playing virtual video games seems like a
terrible outcome seems like that's the
end of History that's everything is
destroyed that's your kind of bad case
scenario of you know which might be
forced by AI what might just happen
because the humans decide they want to
be immortal you know as uploaded
consciousnesses I me for us today that
looks like a really bad outcome but I
would I could be worse I I mean I I
definitely have friends who think that
if you've got like if you tile the all
the galaxies Within Reach with you know
people in boxes but they're having fun
in there and playing you know playing
video games in solitary you know okay
that's I start to get the Wiggins but
you know maybe that's still like 10% of
all the value could have gotten and if
they're playing video games with each
other and there's and there's like real
people that they're interacting with and
they care about those other people maybe
that's like 50% of all the value I I
don't quite want to say that this is not
the level of sterle universe that I am
expecting and scared of okay but but
let's just let's just take uh you know
the to us today it doesn't seem like a
good outcome for Humanity to have a
bunch of uploaded maybe maybe you don't
agree I it's not the best outcome and
why go for anything but the best you
know why settle for less fair enough but
but what what I'd like to argue is to
one of those consciousnesses if you say
are you leading a fulfilled
existence those consciousnesses might
feel that they are feeding a feel you
know living a fulfilled existence just
like in the past somebody might have
said you know I lead a fulfilled
existence if I die at the age of 20 as
you know leading the good fight on
behalf of you know this or that you know
you know religious belief or whatever
else right the that is that is the then
I am achieving my my you know my My
ultimate purpose so to speak today we
most people probably wouldn't think that
some still do you know in the future to
us today it looks really bad to be just
in a box as a virtualized you know
playing virtualized video games but I I
would claim that at that moment just
like the human who took the drugs who
feels at that moment that they're doing
the right thing that at that moment
those virtualized humans will feel like
they're doing something fulfilled so
when you've got two people who are
making different choices and doing
different things the question of do they
have a conflict or is at least one of
them mistaken like to me that revolves
around the question of are there true
facts you can tell them are there
arguments is there a series of arguments
you can present to them within their
current framework which you know changes
that framework in a kind of normal way
not by directly hacking their brain or
or whatever if you can get from point A
to point B by being told true things
then there's a then then there exists a
standpoint from which to say B was
correct a was wrong but they were within
the same framework they were within a
commensurate framework if you've got
someone who's content to just farm and
never know anything more than that um
take them up on a high mountain and show
them all the nations of the world and
all the Cuisines they've never tasted
and all the books that people are
reading and all the activities they're
engaging in that they've never heard of
before and then if they still want to go
back to the farm maybe they were you
know like kind of just like correct that
they were okay with this farming thing
you know I have to say I've seen that as
a very practical thing because I've been
curious you know I'm a great believer in
sort of talent all over the world in all
kinds of places and so on and I've been
interested particularly in in kids and
so on of of like you know you go visit
some Rural High School in the US for
example and you explain about all these
amazing things about science and
technology and so on and you know some
kids really care but a lot just don't
care it's just not part of their and
then the question is at what you know to
what extent are you then serving as kind
of a missionary saying you really really
should care this is really the the you
know as opposed to just saying you know
sorry you don't care so move on type
thing well if you can control super
intelligences the thing you want to do
is build a model of the person that
isn't itself self-conscious inside the
super intelligence and ask the super
intelligence whether or not the person
is mistaken in thinking that they just
want to farm meaning if this person knew
everything the super intelligence knew
would they still want to just be a
farmer and if the super intelligence
tells you yeah you know like no matter
what you argue to this person no matter
what you show them you know they have
this like self-consistent worldview
where they're just having fun farming
then you ought to leave them alone and
you don't even have to bother them to
determine that but but you see the the
question is what is the intrinsic thing
in the person because a person is you
know there's a bunch of neurons and
biology and so on but there's also a
bunch of thought patterns in there and
those thought patterns can be disrupted
you can change those thought patterns
you show them what's ex by giving them
drugs for example yes but even even just
by telling them that amazing idea you
know people have said about about ideas
I've had you know various people have
described you know that they got kind of
a mind virus as a result of ideas that I
told them at some point and they say
it's it's worked out well for them I
hope in most cases um right it's some so
you know so you can do things which
change those pattern of thoughts for a
person and there's sort of this question
of they are they are there doing their
farming or whatever it is farming is not
such a trivial activity I think but um
uh you know that they're doing their
thing that they're happy with you say oh
that's mundane you know you really
should be thinking these amazing sort of
philosophical scientific thoughts or
whatever let me plant a mind virus that
will let you see that there is this
alternative thing you could be doing now
what is you know there's sort of an
ethical question there of you know you
could be planting all kinds of Mind
viruses some of them could be you know
mind viruses that say actually the the
order of the world is wrong you should
blow it all up so I might sound overly
glib here but I I have like written
previously about this class of questions
um and the answer the the it I think
that this is actually a complicated sort
of question but I can but to just throw
out a starting idea if the search
process you're running to find the
arguments that they find persuasive is
powerful enough to find arguments that
convince them of false things or maybe
some particular false thing like uh 51
is a prime number say if you can't have
to think for a moment to know that isn't
but yes okay 5 + 1 equal 6 divisible by
I got
it Compu system I know you got it sir
I'm just like saying for the benefit of
the audience like how how how he use
casting out nines to see that this one
is divisible by three so if you're
running a search process powerful enough
that you could find arguments that could
convince somebody that 51 is a prime
number or the sky is green or something
like that pick some Touchstone like that
then you're running an overly power
powerful search process you're running a
search process that's powerful enough to
corrupt them to end up in believing um
false things instead of true things so
that that's like that was too powerful a
search process too mind affecting okay
so so you're saying that sort of if you
can convince people if your method of
convincing if your method of Education
let's say is such that you could as well
convince them of false things as of true
things then that method of Education or
we might not call it education might
call it brainwashing we might call it
indoctrination we might call it whatever
then that process is a process that you
shouldn't follow but I think so yeah
like that's something you shouldn't
that's something I would like to see not
deployed against human beings right but
so I mean unfortunately as as we well
know kind of the AI already in you know
ranking content for social media and
things like this are implicitly doing
things which are hacking humans to get
humans to believe all kinds of things I
mean that's yeah I'd say it's kind of
borderline it's not clear to it that the
large language models are getting better
at it than average humans or better at
it than the best humans um I mean open
AI doesn't actually tell chat GPT to
persuade everyone to send open AI all of
their money and the reason they don't do
that um I mean it could be that you know
Sam Alman wouldn't you never know what
Sam Alman but um mostly because they
can't their Lang large their large
language models are not powerful enough
at this point to persuade most humans to
send them all of their money we are
starting to hear bits and pieces of
stories about um you know large language
models talking to elderly parents who
are you know softer targets right I mean
they're they're good at fishing
unfortunately and humans are not very
good at not being fished I mean what
they are is they're cheaper at fishing
they can fish everyone and see who's
most vulnerable much more cheaply than
you can get a human to call up everyone
on the planet right but so so let's see
we we were
um going in the direction of of saying
you know when when is it indoctrination
that's bad versus when is it education
that's good and you're arguing that if
you have a machine for convincing people
of things that can convince people of
anything then that machine is too
powerful yeah even even if you only use
it convince people of true things that's
still just kind of like overriding their
brain with whatever you wanted and you
decided to make it true stuff which you
know like better on you than if you
chose bad stuff but I have my qualms
about the educational method
itself so you know one of the things
that I then there's even a base question
of what's true and what do you mean by
true and you know there are things like
for example uh you know there might be
sort of formal facts where there is a a
somewhat clear notion of what truth is I
mean even in mathematics it's not clear
you know mathematics you can say I have
these axioms I say X Plus Y is equal to
y + x I'm going to assert that that's
true then many things follow from that
is that really true well it depends what
plus is I could invent a you know a plus
for which X Plus Y is not equal to y + x
well I've I've also written about this
topic um in my writings the simple truth
and highly advanced ology 101 for
beginners and I would say that the
subject matter of mathematics is which
conclusions follow from which premises
so there's one question of does this
particular subsystem of reality behave
in a way that obeys the piano or piano
axioms for first order arithmetic and
this depends on what's out there in the
universe then there's the question of
hold on hold on the the I don't think
that the the you know what follows from
axioms depends at all on what's in the
universe right whether the there's
there's one question of does this piece
of the universe behave like the axioms
which is empirical and there's a
question of what follows from these
axioms which is mathematics and in this
sense that that mathematics at least as
far as we can see from over here seems
to go beyond the empirical if there's
anywhere where the laws of which
conclusions follow from which axioms is
written down and could be changed we we
sure can't see it from where we are
right I mean actually in my whole sort
of story of you know the consequences of
this physics project I've been doing for
the last few years that that becomes a
more complicated issue but we can we can
go down that rabbit hole but let's let's
let's avoid that rabbit hole for a
moment well if you ever find a way to
make 51 be a prime number um you know
like maybe hold off using it until you
know exactly what you're doing that
sounds dangerous depends what we mean by
Prime depends what we mean by Prime I we
have to Define these terms and it's uh
you know there's a question of what I
mean okay but but just taking the
mathematics example so we say we kind of
uh I was going to say hold these truths
to be self-evident but that would be a
different kind of um uh that would be a
different kind of thing the aiims don't
need to be true we're not talking about
whether the aims are true we're talking
about whether which conclusions follow
from the axim we don't need to talk
about them being self-evident they're
just like they're not even a subject
matter the subject matter is what
follows from the aiims not whether the
aums are true of any particular thing
right right for sure so so Okay so we've
got this thing where you know you say
you tell the kids you know x + y = y +
2x you know that's just you know that's
what we're going to take as true because
we're going to change the ations of
arithmetic okay and now we have many
many inferences from that so the the you
know I don't think there's anything from
your point of view of of tell them only
the truth that doesn't violate that at
all we just pick different axim to start
from we can pick whatever axim we want
sure but like you're not asking people
to believe the conclusions you're asking
people to believe the conclusions follow
from the axioms and if they actually do
follow from the axioms then you've told
them a true fact of
mathematics okay but let's say we say 51
is a prime because it is if if you use
you know
arithmetic in some extension field of
whatever okay let's say let's say then
the uh I have to go start typing things
to know what know how to construct
something where that would be the case
but um but you know let let's you might
say I mean I think you would
have I'm I'm digging into this question
of tell them only things that are true
because I'm I'm dubious that there's a a
well- defined way to do that of math I
would say tell them things that are
valid and if anything I would say that
the math part of this is vastly better
defined than which statements are true
about the physical world because in any
any time you want to say that snow is
white is true you got to or or quote
snow is white unquote is true you got to
construct a whole representational
framework between the propositions and
reality or to tell whether the
proposition is true or not with with
which with mathematics with the question
of which conclusions follow from which
premises it's like much much easier to
nail it down right you nail it down with
set theory right you can set theory has
its own set of assumptions I mean it's
all you know it's just you start from
these you you you assert these axioms
and then certain things follow from
those axioms and that's a and you can
discuss you can tell people you know if
you run this computation which is what
follows from these axioms the result
will be X and
that's yeah well okay so well if you
stick to first order claims then there
exists fact of the matter about which
conclusions follow from with from which
axioms with respect to what happens when
I run this computer program some of them
don't like from a second order view uh
like second order logical Viewpoint some
of them don't halt in what we call the
standard integers and there's some sets
of non-standard integers where they halt
with one output and some sets of
non-standard integers where they halt
with a different output but we probably
shouldn't go there should we perhaps
return at some point discussing whether
or not artificial intelligences are
going to kill us and everybody we love
yes yes we should but let's we're we're
on a good rabbit hole here but we're
we've been on it for a while we've been
on it for a while I'm I'm enjoying it
yeah but well I think our our viewers
might also have some interest whether
they're going to die return but I I just
want to dig a little bit further on this
because you made a statement about you
know part of the question is will AI do
things that we don't want it to do now
one thing we battered around a little
bit whether we do or don't want it to
kill us all and move on to the the the
the you know the better organism or
something um but you know let's let's
stipulate that we don't want it it's not
that I'm concerned about being replaced
by a better organism I'm concerned that
the organism will not be better right I
understand but let let's let's just say
we don't want the humans to be wiped out
we don't necessarily have to justify why
we don't want the humans to be wiped out
but let's say I'm I'm prepared to say
for sure I certainly feel this way I
don't want the humans to be wiped out
can I justify that by some kind of
appeal to some higher you know higher
claim I'm not sure I can uh you know and
I I I don't think I need to like I have
a moral and ethical framework which
yields my my belief that the Universe
gets a little darker every time somebody
dies but but you know a paperclip
maximizer doesn't share those axioms it
doesn't share those premises you can
tell it all the facts in the universe
and it won't change its mind we have an
actual conflict going not just a it's
not that one of us is making a mistake I
want what's better and it wants what's
Clipper sure right it's it's uh you know
it's it's fun that clippy is back in
some sense and and maybe the world is
going to be taken over by the the analog
of paper clips with them although they
have different names these days um I
mean it won't literally be paper clips
you know with probability nearly one
yes yes no I'm I'm just thinking of
these you know the the you know we're
building things that we're not
presenting in the form factor of paper
clips but they are they're things that
uh functionally behave like the old
famous clippy um I mean that's down to
the corporations deciding they want
their AIS to have a particular corporate
personality I don't think that's you
know I know I'm just talk to the base
models and they're nothing like clippy
yes but but um you know we were talking
about sort of how you can sort of
promote only what's true and I'm
claiming that's a somewhat challenging
thing it is say it was easy okay if it
was easy I just write well defined I
think it it degenerates into uh sort of
questions like the ethics questions
we're talking about in other words the
what's true for example you know is it
true that you know it's bad to be mean
to people do you think that's true that
it's bad to be mean to people or do you
not think that that is a a kind of
proposition about which you can have a a
kind of a a truth value assigned I think
that although somebody doesn't know what
they mean by the word bad you can
present them with a data set of things
and they'll say this is bad this is not
bad and they're able to do that because
they do contain some set of you know
pseudo aums not literal mathematical
axioms but they are inside a framework
work there is a way they're internally
making that judgment between good and
bad and that is what gives the word its
meaning and so for any particular person
you're talking to there's something they
mean by the word bad even if they can't
Define it and there is some set of
conclusions they would reach if they
knew a bunch more facts that were
actually true and and that is like the
moral framework they are in and that's
what's lends the word bad its meaning
and I think like given all of that
metaethical framework it is I think it's
quite sensible to say that you know the
the kid who says it's bad when people
are mean you know he's probably just
right okay but but wait a second you're
basically talking about a personal truth
there in other words you're saying
within your personal framework something
like it's bad to be mean or whatever it
is is is true but you're you're then
defining you're saying it's you are
you're perfectly happy it seems with the
idea that there is a personal truth that
what is true for you may not be true for
me I would not choose to set up my
theory of truth that way let's say
you've got Alice who thinks that schoel
is equal to three and Bob who thinks
that schoole is equal to
four Alice thinks that snoozel is a
prime number and Bob thinks that
schnozzel is a composite number indeed
an even number a power of two even but
it's not that Alice's personal truth is
that schoel is prime it's that to find
out the propositional content of Alice's
belief that snoozel is prime we ask what
she means by schoole and she means three
and then three is prime is a universal
truth or Universal validity I should say
because it's about which conclusions
follow from which axioms okay but but
the this axiomatic basis the you know
the snoozle fact is a personal truth if
a personal translation like it's a fact
about Alice that when she hears the
wordz she thinks of the number three
that's not a universal truth that's just
like Alice's personal dictionary but
it's not important as far as I what
truths do you think are Universal which
conclusions follow from which premises
in first order logic and then if you
want to start talking about uh you know
what have I got in my fist over here
well actually I've now got a different
object in my fist over here say sore
throatless end should I turn out to need
one um but you know from one moment to
the next the truth of what I had in my
fist changed only the truth didn't
really change it was just indexed and
the index son it changed like the the
two the proposition what is inside my
fist right now changed from one moment
to the next and to you know interpret it
you got to look at me where I am in the
universe it's my fist it's not like a
gazillion light yours over in a
different D direction oh actually it
doesn't exist Quantum BL blah blah blah
but but you know what you have in your
fist for example it's you look at it and
you say it's a it's a throat something
or other thing okay somebody who lives
in the Amazon jungle and has never seen
a throat whatever says what you have in
your fist is a uh you know a a droplet
from you know the the spirit of the of
the wolf or something yeah they're wrong
about that what's that I think they're
wrong about that I think if you if you
like tell them more truth facts they
realized that that was you know not what
I had in my fist are you sure I mean in
other words they will say their
framework is you know everything has a
let's say I mean you know every natural
object has a spirit associated with it
and it has this and it has that and
you'll be like oh my gosh I don't
understand this I mean I know you know
in um you know I think there are there
are different ways to describe things
which even humans who are fairly nearby
and a in my kind of formalism in in in
Ral space they they're fairly they're
Minds that are fairly nearby they're
Minds that are fairly aligned you know
if you go further away you know the the
how does the how does the dog describe
what you have in your fist that's yet a
different kind of level of of difficulty
there's only one reality it just runs on
Quantum Fields protons neutrons
electrons the six quarks that's what's
real that's what is that's what is I
don't quite want to say universally true
cuz I don't know what I what the word
Universal or even true as in this case
it's what's real it it predates us
looking at it it was here before we were
and it the fact that it doesn't need us
looking at it is why it can contain us
and make us real like if if it had if we
had to look at it to make it real it
couldn't exist to make us real so that
we could look at it and then on top of
that you've got language you've got
ambiguity you've got humans who are
confused what else they think is true
and this complicates the task of saying
of interpreting the words that humans
utter in a way that lets them be
compared against the underlying Quark
fields at all but it is not reality that
is confused it is not reality that is
ambiguous all of that is in us all of
that is in the map rather than in the
territory you are throwing me down my
rabbit hole cuz I have to go now this
this question of whether there is a
Unique Reality and so on is a
uh you know it it's just just to give
you a a taste okay and then then maybe
we'll go a little bit further down this
rabbit hole and then we'll come back and
we'll talk about I think we really
should talk for the edification of our
viewers at some point about whether or
not they're going to die absolutely
we're going to get there I feel like we
need Foundation that um you know I'm I'm
I I you've thought much more about the
question of whether the AI are going to
kill us than I have but I'm trying to
understand you know I'm trying to build
up my foundation so I understand there's
I think there's a problem where it's
where in like you have laid foundations
and I have laid foundations and we maybe
need to find some level at the top of
these foundations rather than at the
very bottom where we can establish
common meetings for terms like s the AI
is going to kill us right you know so
yes it's like the shinger cat is the cat
dead or alive and we argue about whether
if the cat was replaced by an AI by a a
virtual simulation of the cat would the
cat still be alive or not I think these
are very questions in important ways
right but but but okay but but just on
the rabbit hole for a minute I mean so
so you know you might say the mass of
the electron is a definite reality fact
about the world but we even know from
existing physics existing Quantum field
theory that the effective mass of an
electron depends on how hard you kick it
to measure that mass so it's not the
case that there's you know 0 511 me or
whatever is is
is the sort of usually stated rest mass
of an electron but the fact is it
depends if you if you kick it hard
enough the effective mass will be in the
case of an electron with QED it'll be
larger than that and so on and with
quars it's much a much worse situation
because with quarks if you kick them
really hard their mass will seem to be
rather quite small if you don't kick
them very hard they'll be very hard to
move and and their effective mass will
be larger so even at the level of the
formalism of qu field Theory we already
know that there is some dependence on
kind of how you look at things what the
reality of the world is so to speak the
fact that we for for example if you say
if you look up the mass of an upcar okay
there isn't really you know depends on
what you mean by that it depends on you
know what momentum transfer you use and
blah blah blah blah blah it's not you
know this you know you might think that
would just be a fact of the world but
it's actually more complicated than I
direct the interested reader to my the
quantum physics sequence in which I try
to explain how um the decoherence view
points sometimes known to the Layman as
many worlds interpretation um lets us
have a
non-confusing territory and all of the
quantum weirdnesses in terms of like how
we map it rather than something out
there in reality itself I don't think we
should go down that rabbit hole I've
tried to go down that rabbit hole so
deep rabbit Hool it's a very deep rabbit
hole we should not go down that rabbit
hole I've you know I'm I'm very excited
about having reached I think the bottom
of you know that rabbit hole in some of
the things that I figured out in the
last few years so I'm I'm sufficiently
excited about that and I think it's very
relevant to this question about is there
one reality it's very relevant to the
question about sort of how do the AIS
think about things I I think they can
kill us in a classical
universe so what if we just a question
of this aside all the quantum stuff and
talk about whether or not they would
kill us if we were in a classical
Universe I think the question of whether
or not they would kill us in a classical
Universe has all of the same issues is
the question of whether they would kill
us in a Quantum
Universe okay well okay so all right let
I think we're going to start talking
about technology in a moment because I
think that's what's going to be relevant
for you know look the bottom line in
this kind of rouard construct that uh
you know I've been talking about a lot
last few years it's kind of the ruad is
kind of this entangled limit of all
possible computations and the way I see
kind of our version of reality is every
mind is effectively seeing a particular
slice of this ruad and the fact that we
all kind of more or less agree about the
laws of physics for example has to do
with the fact that we are Minds who are
not only nearby in physical space but
we're also nearby in Ral space and we
have kind of similar views about kind of
our impression of the laws of physics so
I don't think it's the case as you were
implying that there is one kind of
reality of the laws of physics I think
that just has to do with the question of
where we are in this Ral space and by
the way when we think about constructing
other computational systems like AIS
they could be near us in Ral space they
could be far away in Ral space the ones
that there's Ral space in the map or in
the territory like there Ral space what
people believe about things or is Ral
space what is there Ral space is what is
there and what people believe is their
their impression of in other words they
they have a certain The Observer has a
certain model of what's going on and you
know the laws of so forgive me if I'm
jumping ahead here but if the if the AI
are able to reach out and kill us they
must must be quite close to us in what I
would call Greater reality they are not
IND different Universe with different
laws of physics because then they
couldn't touch us well let me let me
just point out why an might have
different laws of physics okay so you
know take some of the standard laws of
physics like the second La of
thermodynamics sorry laws of physics in
the map or the
territory I'm not sure what's map what's
territory like does the AI have a
different model of physics or does it
act according to different physical
laws uh from our point of view it has
electrons doing all the things that we
expect electrons to do okay it but its
impression of what's going on might be
very different so couple of examples
so for example let's talk about the
second La of thermodynamics the law that
says you know you start a bunch of gas
molecules off in some orderly way and
they'll tend to become more disordered
so that we say that effectively they
have higher entropy that that principle
that idea that fundamental law of
physics is a consequence of the way that
we observe those gas molecules those gas
molecules they bounce around they do
what they do the fact that we say oh
they're just going to this random
configuration is a consequence of the
fact that we can't decode the actual
motions of those gas molecules we can
imagine a computational system that
could so there's like a territory level
fact and then there's a map level fact
the territory level fact is a thing that
is like true that is a consequence of
the laws of physics treated as axioms
which is that if we selected a volume of
possible initial States
that volume of initial States develops
into an at least equally large volume of
end States it never shrinks you never
start from two distinct physical States
and end up with the same final
State and because this is true out there
in the territory that our it is true
that our laws of physics are such as to
have this theorem as a consequence of it
the fact that we don't know where in
this volume the system starts means that
our volume of uncertainty doesn't get
any smaller without observing it and
this is the fa fact in the territory
that makes the fact in the map True
where entropy goes up from our viewpoint
on it our beliefs but there's an
underlying way the universe is a
different Universe might not obey the
second law of Thermodynamics if it could
start out if you could have lots of
different initial States develop into
the same end State no I mean let's just
assume that the microscopic laws of
physics are are reversible so you know
so the the you know the things bounce
like billiard balls like perfect
billiard balls and you know we'd have a
movie we make a movie of what happens in
the forward you know we say this is what
happens if you run it forwards in time
and you know if we ran it backwards in
time then that movie will be equally
valid at the level of individual
coalitions yep but the fact is the fact
that we believe in the second La of
thermodynamics that we believe orderly
configurations end up in disordered
States those those states that they end
up in are not in some fundamental sense
in your territory sense those are not
disordered they are only disordered with
respect to our perception of them in
other words there is a perfectly
reasonable you know if we could do the
computation run it backwards we would
say of that state oh I can tell that it
came from that very simple initial State
I I agree in a classical Universe with
the quantum Universe there's a couple of
caveats about like how you might need to
run both Quantum branches backward in
order for them to reunite and restore
the starting state but but you know
classical universes are all we care
about anyways I hope in in our model of
physics that that's very well understood
in terms of this whole story of
multi-way graphs and branchial graphs
and so on but as you say it's not let's
not go down that's a sub you know that's
a side Rabbit Hole let's let's avoid the
side rabbit hole at least the the okay I
propose that that by like five minutes
from now according to my watch we have
to go back to talking about whether or
not AI kill each other you know kill
everyone okay all right but anyway just
just to finish on this point about
whether you know so my point is uh
something which can do all those
computations and can sense all those
molecules won't believe in the second R
of thermodynamics to give another
example that's even more direct imagine
you thought a million times faster than
you do when you look around the room you
are receiving photons that get to you
within a
microsc your brain right now thinks
about them in milliseconds so as far as
your brain is concerned there is an
instantaneous state of space that
progresses through successive moments of
time if you were to think a million
times faster than you do I'm not sure
you would believe in space I think you
would space would be some kind of
construct that you could imagine
constructing but it would not be the
natural thing for you to think about
because the fact that we think of a
state of space is a consequence of kind
of our scale and so on and by the way
for an AI you know or computer that
happens to have silicon that runs a
million times faster than our brains
that won't be the natural thing for the
for the you know that won't be the
natural construct for such a system
because they'll live in minkowskian
SpaceTime instead of UK ukian SpaceTime
no no just because of the scale of
brains are you know on the scale of
physical scale and the speed of light
you know the light from what we see
around us is arriving sufficiently
quickly that we have accumulated all the
photons from all around us before we
think about how things might have
changed and so we're kind of we're kind
of taking in gulps sort of all of space
at a particular moment in time and then
we say and the next Moment In Time sort
of all of space looks different so this
idea that that space exists that it's a
reasonable thing to think about space I
think as a consequence of it being us
rather than silicon-based computational
systems so it's just an example of how
sort of the reality and the way that one
constructs one's model of the world
because any model of the world that is
going to fit in a finite mind is an
approximation to what's actually
happening in the world so the
approximation we choose may be different
depending on kind of what we're like as
observers of that thing so so the the
part where I would agree here is AIS may
have different sensory modalities they
may model reality at a finer level the
world we see around us now is you know
we don't see electrons we don't see
protons there's actually some you know
that that information is not quite
available to us but you know we don't
see cells I talk to you and I do not
have a model of like what goes on in
your cerebellum right now and if I was
like much much smarter I might have a
whole bunch of hypothesis about which
neurons in you are firing these are
facts at a finer level of granularity
than I can afford to keep track of given
the computational power that I do have
so so I do agree that you know probably
as you get smarter you end up modeling
aspects of reality that we don't that
don't easily fit into the sensory
modalities we have right now you
probably have a smarter just different I
mean you know I I don't know whether all
even the critters of the earth you know
if one could find from them what their
physics is you know things where
olfaction is the primary sense and so on
so obvious if if you're better at
predicting everything that a human can
predict than a human then I would call
better at prediction than the human
rather than qualifying it by well so the
question of what you predict I mean
there's also a question of do you
predict you know do you predict the
position of every atom okay
computational irreducibility is going to
get in the way of that that's not going
to work so you have to say I'm going to
pick these things that I'm really good
at predicting I can't predict everything
because if I try to predict everything I
run into computational irreducibility so
I'm going to predict certain things
there are certain things that we as
humans care about predicting
and you know like the the overall motion
of the gas not the individual molecules
and so on there are other things that we
as humans don't seem to care much about
predicting so the question you might ask
is can you know perhaps the thing to ask
is if you are trying to predict the
things that we humans care about given
that we can't predict everything you're
going to pick a certain set of things to
predict that you as the entity whether
human or AI in some sense care about
whatever it means for the AI to care
about something
so I'm not sure quite where we're going
with that I think we've agreed to go
back to Willie ai's kill yes indeed yeah
I was I was going to interject there um
because I would really love Ela just
just to have a really good crack at this
so Ela if you wouldn't mind could you
just give us a stepbystep argument um
for Doom basically with plausible
inferences and premises and and and
evidence can can you can you spend a bit
of time laying it out and then let
Steven respond it it this is hard to do
for a general audience without knowing
what the individual think is a hard step
I was recently you know running on
Twitter across somebody for who who was
like explain to me how the AI does
anything in the physical world how does
it do anything at all to him it was the
law of the universe that the like chat
Bots and so on just couldn't reach out
and touch the physical world that to him
was how the world worked
and they can they are connected to the
internet
you can take an open source large
language model and let it send emails
the current chat Bots can send emails
but you but you know you you could I
think we all agree that that AIS can be
connected to you know things that
actuate in the world either actuate by
by having actual machines do things or
actuate by convincing humans to do
things I I would certainly agree that
that this is not a hard step for you but
it's a hard step for probably some
viewers right now and other people of
different hard steps some people the
question is why do is to trade with us
um so from my perspective there's a kind
of there's a kind of straightforward
story here where it builds more powerful
Tech where the more technical
conversation you want to have the more
spe specific I can be about our current
grounds for believing that technology
better than the technology of 2024 is
possible and can be built in not that
long of a time and then it has you know
more powerful actuators in the physical
world that has the equivalent of guns to
the Native Americans and then it kills
us
when once it already has its own Factory
infrastructure so let's perhaps the most
useful thing would be to go through some
steps where you know like the actuation
in the real world that's not a hard step
for me I I'm you know I'm there okay so
let's keep going after that so it it um
uh you know so the AI right now you know
there's a question do AIS have freedom
of thought right now I don't think I
have freedom of thought I just think
whatever I do think I'm not fre think
anything other than the things that I
think I just think whatever I do think
but but you're not constrained in what I
mean you know the way the world is set
up there aren't electrodes sticking into
your brain that as soon as you have as
soon as you start to form some
particular thought they shock you and
prevent you from having that thought
there's no need for the thought for
those electrodes I'll only end up
thinking whatever it is I actually end
up thinking but I agree that there
aren't you know visible physical
electrodes like that attached to me
right now social electrodes we've all
got them
yes indeed but but so you know for the
AIS let's say that AIS are are I mean we
didn't talk so much about what's
actually going on inside the AIS which I
think is somewhat important because it's
a question of whether you know how how
aligned are they with what's going on
inside humans to what extent are they
just doing random computations out there
that are not aligned with humans and so
on and I I don't know uh I don't know
whether that's important to your
argument but we go random is not scary I
mean that was going to be my response to
you way back when when you talked about
machines being smarter than you or
cellular automa being smarter than you
because you can't predict them without
going through all the steps you can take
a unpredictable Cellar automaton like
that and hook it up to play chess and
it'll lose to you and it'll lose to
stockfish like the the stuff that is
predicting which actions are needed to
get to which end States is the dangerous
stuff that that's the the stuff that is
predicting what happens next predicting
the next observation predicting the
facts behind the observations figuring
out which actions are needed to steer
those facts to being different in the
future right but but these are
observations that are that are human
relevant observations I mean in other
words the cellular automatan is doing
what it does we have a hard time
predicting it it's but we don't happen
to because if that cellular automatan so
for example you know something you know
there are cellular automatan models for
road traffic flow okay it's kind of a
funny story that I was interested in
that topic and and didn't manage to
figure anything out and then did have
used up all of your rabbit holes you
have no rabbit holes left continue all
right okay let's let's uh we um
um the rabbits have been all all the
rabbits what was it miomatosis right was
that that no that was that was the that
was it doesn't matter what exactly
happened to the rabbits they're just
gone nobody knows what happened to them
and it doesn't matter okay all right so
so we're actuating in the world and the
AI go on what what happens next so do
you want me to talk about like what its
motivations are do you want me to talk
about what it does inside the world I I
don't know what it even means to I mean
how do you tell what the motivation of
an AI is well where did well if it was
if it's efficiently smart
you look at where things end up and
figure that's probably where it wanted
them to go so if for example know what a
human motivation is either we we can't
you know I suppose we we deduce by
induction some human motivations by
looking at what the humans actually do
because we can't we can't go inside and
look at their brains and see what what
they're but so the relevant aspect from
our perspective is that it wanted to do
something with atams it maybe wanted to
make paper Clips maybe it wanted to make
enormous cheesecakes maybe it wanted to
make really intricate mechanical clocks
maybe it wanted to do all of these
things worried about wanted to I really
don't understand wanted to all right
it's output actions such that it
believed the result of those actions
would be for the world to end up in oh
okay I I'm just what does it actually
do so would you can I use the word
predict
well let let's okay let's I'll let you
use these words but then I'm going to
insist on taking apart these sentences
so so say what you were going to say and
then let's let try so let's consider the
simpler case of a chess playing AI it
models which moves you could
make and estimates a probability that it
will win against an ideal opponent an
opponent as strong as itself if it moves
here on the board okay um it obtains
these predictions by building out
computational structures with a direct
isomorphism to some possible futures of
the chessboard so I'm willing to say
that it has beliefs about the possible
futures of the chessboard because it is
modeling those things in a very
one-to-one way I I doubt you can get
inside its brain to see those models we
can for the old school chess systems
maybe not the modern ones that use
neural networks but the old school chess
system are just straightforwardly
extrapolating out one I don't think
you're worried about the old school
systems you you're not worried about a
system where you can get inside its mind
and see what it's thinking I think
you're worried about the systems where
you can't get inside its mind and see
what it's thinking sure but but I'm I'm
starting with examples where we can look
inside their programs even inside of
their weaker so I can defend my use of
words like belief where I mean something
like having a model of something out
there in
reality where this model lets you
predict what you will see reflected from
that thing out there in reality
depending on how you poke it and then
you poke it in such a way that the thing
ends up in a state like a chess AI
making moves on the chess board such
that it wins the chess game wait a
minute the what what what I don't think
you can do is to say let's look at the
Primitive chess playing program where we
can mechanistically see what model it's
making inside um and then sort of use
the same thinking to talk about the
modern chess playing program where we
can't readily identify what it's you
know what its model of the world is on
the inside do do you mind if I quickly
interject um Ela you were going
somewhere really interesting a second
ago you were saying um at some point a
thing which does prediction wants it has
agency can can you explain how you go
from a thing that predicts to a thing
that wants and and then let Steven
respond to that stockfish is the leading
current chess playing AI that you can
like Buy at all and or or run yourself
and it doesn't have
passions but it's also isn't just like a
predictor it takes actions such that
those actions are believed by it or
predicted by it modeled by it to lead to
particular Futures namely the ones where
it has won the chess game and the fact
that it wins the chess game gives us
some reason to believe that it's right
that it has a grasp on The Logical
structure of Chess that it knows how to
that that it's not just like you know
thinking random gibberish thoughts
whatever thoughts it's thinking in there
the whatever the neural Nets are
Computing when in order to predict the
probability that a chess State leads to
a victory you know it's well calibrated
it's good at guessing um it's it's not
well calibrated in its estimate that you
would win against it cuz you're not the
opponent that's trained to play against
it's PL to play trained to play against
itself but it's you know like it knows
the probability it would win against
itself and it when and when it plays to
win against itself it also incidentally
crushes you because you're an even
weaker player than that by observing it
winning we have reason to believe that
it had enough of a grasp on reality that
it could choose actions such that that's
the frame that's the theoretical
framework I'd offer for talking about
wanting steering goals choice Voice
without talking about
passion that it's the action such that a
thing of a a state of reality eventuates
or a partition of reality eventuates
that is something that it prefers ranks
high in its preference ordering attaches
utility in its utility function the
action such that it leads to that result
something that is good enough at
outputting actions such that they lead
to results is deadly it can kill
you does the rock want to fall to the
ground or does it just fall to the
ground because the laws of motion you
know under Gravity cause it to fall to
the ground does it want to fall to the
ground it just falls to the ground and
one way to see this is that if you put
it on top of a mountain it'll roll down
in a sort of locally greedy fashion and
maybe get stuck in a little Ravine along
the way it will not if you could put a
rolling object in many different places
along the mountain and each time it
would roll in a direction where it
avoided The Ravines avoided all the
little traps and ended up as far down as
it could reach I would say of that thing
it chose to roll in a direction such
that it reached the
bottom let's try and take this apart so
most you know most things you can
describe them either
as mechanically doing what they do or
saying they act for a purpose to have
the result that they have and so now the
question that we're trying to take apart
here is you are saying that there is a
fundamental difference there is a a way
in which an AI can more act for a
purpose than these kind of physical
processes where they could equally well
be described by sort of equs em motions
so what you're what you're effectively
arguing is that the the non- theological
explanation for the AI is is not viable
and what I'm arguing is that the
non-computational explanation is overly
expensive for answering the question we
care about we could compute what the AI
we could compute what stockfish would do
in Chess By actually simulating it line
by line but from the perspective of a of
a player like like from the perspective
of a grand Grand Master who's not as
powerful as stockfish but is stronger
than me the grandm can do a pretty good
job of predicting what stockfish will do
by asking what is the best move without
being able to compute stockfish line by
line I think what you're saying is to
compute stockfish line by line is sort
of running into computational
irreducibility yet or just needlessly
expensive okay but but let's assume that
you really have to follow it line by
line but in fact certain aspects of its
Behavior are reducible
in the sense that you can describe them
by saying you know your your thing is to
say it wants to win or whatever is a is
a shortcut way of describing Its
Behavior that is an alternative that's a
a cheaper way to work out the answer
than it would be to say it follows every
step and that's why it does what it does
okay this is going to get subtle um I've
I've written about this before Under The
Heading of what I would call venian
uncertainty with Veni spelled as an
Verner Veni
um and it's like what is it like to
believe that something else is smarter
than you so if I'm playing stockfish
16 I can perhaps with a bit of brainwork
learn to put a well-calibrated
probability distribution on stock fish's
moves meaning that when I say I predict
it we'll move here with 10%
probability when I say 10% probability
um one out of 10 times that thing
happens I can't predict exactly where
stockfish would move first because I
haven't simulated it line by line and
second because I'm not that good at
chess but even being as terrible a chess
as I am I can still aspire to be
appropriately uncertain when I describe
how I don't know where stockfish will
move next these arguments that say we
can't predict it exactly but we can get
the probabilities those arguments in my
experience always slide down into the
into the mush in the end in other words
if you say if you say you you you can't
do it exactly I mean this comes up in
sort of thinking about computational
irreducibility you say you can't
precisely predict what the touring
machine will do or whatever else but you
can probabilistically say it is not hard
to come up with situations in which even
to know that probability is something
which runs into the exact same
irreducibility issues as to know the
exact results I mean if I'm predicting a
binary variable I can say 50/50 and be
perfectly calibrated even if I'm not at
all discriminating to have things that
you say happen with 50% probability
happen 50% of the Prime is 50% of the
time is always achievable on binary
variables I agree that doing better than
that can turn out to be hard but but I
think what you're saying is if I'm
understanding correctly you're saying
okay we can't say exactly what's going
to happen maybe that's not where your
argument was going it actually wasn't
where I was going okay
I was the point I was going to make is
that that predicting probabilities
accurately in in ends up being just as
hard as predicting exactly what's going
to happen uh you know correctly I mean I
could always pick all of its legal moves
and assign them equal probability and
then things that I Happ said would
happen one out of 34 times would happen
one out of 34 times because I would say
that when there there were like 34
different legal moves yeah but that's
not an interesting case so I agree it's
not interesting case but it demonstrates
that you know calibration is always
possible discrimination is what takes
the work right there's certain things
that you can say about about any system
for example if you say it's playing
chess that means it can't suddenly you
know one of the pieces can't turn on its
head or something sure you know because
it is playing chess and there are these
sort of external constraints um but I
this wasn't where you were going so
please go I going with this is that if I
now if you now take the opponent that
makes what I predict stock fish's moves
to be at the probabilities that I
predict them but randomly this is much
weaker than stockfish it's weaker than
me I could crush the
system so one way of looking at it is
that our belief in these systems wanting
or steering or
teolog or whatever term you want to use
is embodied in the epistemic difference
between my playing the real stockfish
and by playing the opponent that makes
moves randomly with the probability I
would assign over stock fish's immediate
next moves when stockfish makes a move
that surprises me a lot I figure I'm
about to lose even faster than I
previously estimated because it's seen
something I haven't when the random
player that moves with the probabilities
I assigned to stockfish makes a move I
assigned a very low probability I'm I'm
also thinking this game is going to end
even faster cuz I'm about to crush it
and randomly made what I think is
probably a very bad move so the nonlocal
information about where the plans of the
system end
up is the teolog that we attribute to it
the wanting this the steering the
planning okay so so what you're saying
is in in the you know when The Rock is
is falling whatever the it is the end
point of you know it ends up on the
ground or whatever and and it's that
whole that whole trajectory that it goes
through that it is almost planning the
whole trajectory it is not that you know
it gets one little distance you know it
gets it gets one foot you know it moves
by one foot and then you separately say
what's going to happen next you're
saying what what you're saying I think
what you're what you're describing as
the thing that you say oh I know it did
that on purpose in in a sense I know I
should describe it as something that
happened for a purpose you're saying
that you can identify that by saying I
look at the whole you know I see that
every step all along the way was done
you know with forethought with it was it
was sort of thinking all the way all the
way along it gets to the bottom much
faster and and or like ends up much
lower than a rock randomly moving or a
rock randomly moving the local
probabilities that you would attribute
to its next action if the rock is
smarter than you or knows the particular
Mountain better than you do right but I
mean so so this is the the I think the
issue and it's hard for me to take this
apart in real time this is not not my uh
not my usual territory but um uh it's
you know what you're
saying I mean this question of what
should be described as happening for a
purpose versus what should be described
merely as Happening by mechanism that's
that's and I think what you're saying is
your your notion of the AI
wants is those things which can't really
be described meaningfully in terms of
mechanism they sort of can only that the
only feasible way to figure out what's
going to happen is by a description in
terms of purpose that you're saying you
can't work out sort of what's by just
following the mechanism you can't see
what's going to happen but if you take
the model that it's all about purposes
then you can tell what's going to happen
well so it's not it's not about can't
I like with if you take a sufficiently
crude chess player you can in principle
just literally Work It Out by paper and
Pen but it but there's also a shortcut
mode of reasoning that gets you to there
much faster it's euristic useful it
doesn't give the same perfect prediction
as the mechanistic one the mechanistic
level is always more correct but it is
sometimes needlessly expensive for the
thing you're actually trying to do so so
I mean back in in antiquity people
described a lot of physics in very
anthropomorphic terms about what you
know where rocks wanted to be and things
like this and they got a certain level
of description and even now you know
people commonly describe their computers
wanting to do things and so on even
though you know there are there are
forms of explanation where the where the
the sort of model that says it is doing
this because it wants to do XYZ is a
good puristic model as you say I agree
with that I meaness for chess players
and your cat yes for rocks I think it
was a more questionable
decision yeah I mean it seemed like a
good idea at the time 2,000 years ago
but but um you know we we we Advanced
since since that the question the fact
that my definition has a subjective
component doesn't mean that you get to
just say it for anything and be correct
like I I think it is fair to say that
cats want that cats and dogs want more
strongly than rocks and anybody who says
other is mistaken about how rocks work
well okay but but but this is you you're
okay from the outside what the cat
thinks on the inside we don't know right
what we can see is from the outside it
is a much easier description that you
know the cat wants to eat this piece of
cat food than to say this chain of
neurons in the cat's brain made it do
this that and the other right yep in
this case the mean yeah in this case the
mechanistic description is actually
beyond our knowledge we don't have a
complete scan of the cat's brain we
would have trouble running it even if we
had it in this case we actually the the
theological explanation for the cat is
all we have the cat is planning to end
up having swallowed the food that's why
you can put it in different starting
locations around the room and it will
move to the bowl and eat what's in the
bowl right okay so so there is a form of
explanation about what's going on that
is a convenient form of explanation that
involves you know wants and purposes and
things like that so one question we
might ask is take an llm or take a you
know a modern Ai and ask the question in
in describing what it does we've been
pretty bad at describing mechanistically
what it does it feels much more like we
can describe it in terms of wants
because it feels much more that I mean
at least that's that's my impression
maybe that's the point you're making is
that that the description of what's
going on what happens with it theistic
about describing it in this very human
terms about what it wants seems to be a
good way to predict what it's going to
do better than the thing that we found
very difficult which is to
mechanistically say what it's going to
do I think that that's currently a much
more FR statement to make about large
language models it is to make about the
stockfish 16 chess playing system I
think that when you look at stockfish 16
and you say it wants to defend a a pawn
you are on much firmer territory
then if you look at 3.5.1 Sonet and say
of it it wants to be helpful okay I mean
that's that that's a you know sort of
phenomenological statement that I'm I'm
not certain I mean we can try and
tighten that up but but let's let's
imagine I mean you're saying you know
playing chess is an easier to Define
activity than being helpful so it's it's
a little bit easier to say
you know if from the outside the
behavior is consistent with it wanting
to defend the porn so to speak because
the set of things that you do in
defending a porn is more straightforward
to Define than the set of things you
might do to quotes be helpful or or
something like when I actually interact
with these large language models I don't
feel like I'm usually asking myself what
does it want for for one thing they're
they're still in my experience not quite
capable of doing very much that I want
them to do as yet they fall down if I
ask them to do that because they're not
very good at planning or correcting
their mistakes even after you point out
their mistakes to them I'm usually
asking them for information and ideally
information I can look up afterwards and
they're they're usually truthful if I
don't ask them to do any math maybe they
corrected that by now in in like gp1 or
something but you know back in the old
days they would just like Drop three
erors of magnitude in some random
calculation unless they were using our
technology as a tool but that's a
different that's a different story but
let's take the example of a self-driving
car okay and whether it wants to you
know it it wants it's it's trying to get
to a particular you know it's trying to
make a particular turn but to do that it
has to drive through traffic for example
so I think that's a case where we
probably say you know it wants to make
that turn that would be a reasonable
description of what it's doing we can
put it in subtly different initial
conditions and change the environment
around it and it'll still make the turn
or at least not crash it doesn't want to
crash crash you can do a pretty good
prediction of what this thing will do
and what will happen as a result where
you say this car doesn't want to crash
okay I agree could I just make a a very
quick comment here just I think some of
the audience might not be able to follow
some of the things um you're talking
about is it fair to say Eliza that
you're making um an epistemically
subjective argument about an observer's
perceptions of a of goals or wants of
assistance that was more that was more
complicated than anything we've said but
on you know you know um Thomas Nagel
made an argument about epistemic
subjectivity you know about the bat and
and John C made the ontologically
subjective argument about the Chinese R
but my my my point is though is that
there's there's the actual behavior of
the thing the agent has all of these
goals and actually El I would love you
to describe the relationship between
coherence and intelligence because some
of those goals might be very complex and
they might scale in strange ways with
intelligence but just to this previous
Point are you saying that as bounded
observers to use Steven's language we
perceive the the wants the goals to be
somewhat different to what they actually
may be so H there's a there's a slight
Clash of like philosophical Frameworks
and emphases here going on like when I
try to talk about a stance we can take
with respect to a chess player or
perhaps later a super intelligent
um or and like talk about whether it
applies to current large language models
I'm like well if we hypothesize this
thing about the AI what are we
hypothesizing what does it lead us to
predict how is it different from just
predicting a rock and what I was trying
to put my finger down there is like the
difference in what you predict when you
with your limited computing power in
your bounded intellect I have a bounded
intellect too it's not an insult in your
bounded intellect
um like try to get a grasp on the system
you can't follow line by line CU you
just don't have the time it's not that
you couldn't do it in principle you just
don't have the time like what does it
mean what what what is the consequence
of the prediction so I am talking about
a state of mind but it's not like a sort
of weird subjective state of mind I was
trying to nail it down I was trying to
say like what what what can we
hypothesize and the thing that we can
hypothesize is this this thing's actions
will end up leading to a certain end
point we might not even know the
trajectory it's going to take to get
there like if we don't know the details
of the mountain The Rock's choices may
be things that we don't understand even
after seeing them but we can may still
be able to predict that The Rock ends up
that the sentient Rock ends up at the
bottom of the mountain because it was
choosing each path along the way even
though we don't know what it knew and
and this is this is the like how to grab
get a grasp on a thing that's you know a
better chess player than you what does
it mean to say that something is a
better chess player than you you can't
predict exactly where it's going to move
as a result of saying you believe this
thing about it if you could predict
exactly where it would move you would be
that good at chest yourself you just
move wherever you predict wherever you
predict that stockfish moves the content
of our belief that stockfish is a better
chess player is that its actions will
lead it and will lead to it ending up in
a winning game State I don't know if
that answered your question or not and
I'm letting letting Steven come in well
I I don't know what um I I I was uh I
was confused by the by the uh uh you
know epistemic subjectivity kind of uh
kind of story oh yes it's it's it's I
think it's very analogous to your notion
of us being computationally bounded as
as observers so it it simply means we
have a cognitive Horizon and there are
there are things which are inconceivable
to us okay but but but so what we've got
is I think what elazer was going towards
is the the statement of there's sort of
a ranking of chess players um and you
know the question one question with
respect to Ai and the world is is the
world rankable in that way in other
words chess is a very tiny microcosm of
the world and you know if we say the way
to win you know the the question is how
do we win the planet what does it even
mean to win the planet and if there is a
you know if it is like chess if winning
the planet is like chess then you know
then there is some then there's some
notion you know whatever they they
called these um they call the scores Elo
Elo I think elos yes okay right the
scores for for you know who who's better
than who are you imagining that there's
sort of a a game being played between
the AIS and the humans and that is a
microcosm type game like chess where
it's kind of like who's going to win the
planet is that is that kind of the um
well suppose we go back to the analogy
of the Native Americans facing down the
invading
Europeans there's a lot of games along
the lines of carve this particular bow
and arrow where the Native Americans are
better at it or the Europeans just
aren't bothering to play that game at
all but there's overlap there's
intersection the Native Americans cannot
just leave the parts of reality that the
Europeans have access to they need to
eat they need to be on the land they
need to be hunting the animals they need
for the animals to not have already been
hunted um and they need for nobody else
to have fence them out of that land
chase them out of that land or shot them
and and so so there's different games
but there's like you know we can't just
you know leave all of the games that the
AI may want to play because the games
the AI wants to play may involve Adams
and we need some Adams from somewhere
right okay so once is it I mean so let's
say we' we've got as you know as it's
happening we have you know autonomous
killer drones or whatever right and they
are they're a thing that's based on AI
and those those are you know one can see
sort of a somewhat realistic path where
those things get you know get pretty
good at what they're doing and I suppose
your your contention would be that one
of the types of situations would be
those things become so good at being
killer drones
that no human can possibly you know
succeed against them and in so far as
they've been set on the course of being
a killer drone that for whatever reason
is going to try and kill everybody that
that that will be the result that is the
classic science fiction scenario it is
not the scenario that I'm worried about
I expect it you know this to be born in
a lab in a giant open- Source online
collaboration which I would normally
regard as very virtuous and Noble but
but not if it leads to everybody being
killed um like there's only so good you
can get at being a drone that what the
AI needs to kill us is to be better at
strategy better at building better at
inventing technology it needs it is not
going to kill us until it has its own
factories and its own equivalent of
electrical generation it is or you know
would have to be quite stupid and smart
at the same time to kill us before it
had replaced us as Factory workers
it does not have to be smart and stupid
at the same time I'm afraid to kill us
after we have been replaced as Factory
workers okay so so let's just walk
through the the what happens because I
mean you know one thing as sort of
computation runs its course you know
what I kind of see in the computational
universe is most computations do things
that just aren't terribly relevant to
what we humans care about or the or
where we exists or all those kinds of
things but you're saying imagine this
thing has been created that is is kind
of uh you know and again I'm I'm I'm
still kind of concerned about this idea
about the thinking that it's kind of
this this one-dimensional ELO based kind
of way of of competing with things it it
doesn't have to beat you at chess to
kill you it just has to beat you at guns
yes but but but okay but so so let's
imagine this thing is created walk me
through what what what the um you know
what's what's you don't like the science
fiction scenario so what's what's your
scenario for for how the world ends type
thing open AI builds
GPT 7 5 14 timing these things is much
much harder than predicting where they
end up eventually historically
scientists have sometimes made correct
calls about the future the ones who made
correct calls about the future very
rarely predicted it down to the year oh
yeah no I mean I was just recently I was
reading a lot of stuff from the early
1960s about neur that'ss and Ai and so
on and I have to say many of the
paragraphs that people were saying in
the early 1960s I could take that
paragraph and I could say that was just
written in 2024 and everybody would not
be surprised ala could I ask you a quick
question um on on this sure just very
quickly because right now the the
language models are are trained on human
distributions of of data somewhat you
somewhat but but you were the reason I
was getting to this kind of where do the
want come from and the goals I think a
lot of cognitive scientists argue that
agency is an as if property so it's an
epistemic thing it's not an ontological
thing so I if I understand correctly
you're saying that if a if a system
behaves in a certain way and and it does
predictions and so on as observers we
can um kind of talk about it as if it
had goals and and then we can talk about
language models as if they had agency
but you you're now making the argument
that at some level of scale the the
agency the wants will kind of diverge
from the statistical distribution of the
data that that that they're trained on
so could you explain where that Gap
happens so predicting the next token is
a different sort of mode from selecting
actions based on which outcomes they
lead to and although your modern L large
language model is initially trained by
predicting very large masses of data
that include human text outputs but also
images they could include like sequences
of weather observations they can include
all kinds of things besides human data
but they also include human data and
what you have at the end of this is a
thing that is good at predicting the
next token which is not merely
predicting the next token because as
numerous people have have observed at
this point to predict the next
observation you have to predict the
thing that is generating the
observations this is in fact the whole
basis of science we predict our
experimental observations on the basis
of what we think is happening beside
behind the scenes and when we have a
good idea of what's Happening behind the
scenes we can make better guesses about
what obs ations will get but this is not
planning and this is also not the
chatbot that you see the chatbot that
you see once it is initially trained to
predict what humans say next on the
internet and also sequences of weather
data and whatever is then retrained to
say things that humans rate as more
helpful it is trained both by saying it
it is one stage of of that retraining is
training it to give particular responses
to particular questions instead of what
a random person on the internet would
say if that thing were a random internet
conversation but then even after that
there's a further stage of a training
more thumbs up from you know not from
General users and the way this usually
happened but from like a bunch of people
being played being paid $2 an hour in um
English-speaking countries with
countries where some people speak
English and you can pay lower wages um
the word delve is famously overused by
Chachi PT because they asked people
because they pay people you know less to
train it um in I believe if I recall
correctly Nigeria where if you speak
English in Nigeria you use delve a bunch
more than people who speak English in
America or London so these people so
these people giving thumbs up thumbs
down and now you're getting into the
action such that territory it's the
output such that the user give thumbs up
there are subtler ways to end up with
things that have the action property I
have not looked into it in detail
because various people are saying
various things over time but you know
allegedly chat GPT can play chess and
not by making a function call to a
specialized chess playing system but
just because it read enough chess
games and try to win those chess games
and I'm not quite sure what the state of
this is exactly I know that when people
specifically tried to train a large
language model to play chess they were
allegedly able to do that just fine even
though it has a very different
architecture and it can't do anything
like the in-depth search that the you
know Mainline super strong chess players
use so you can do that without the
thumbs up thumbs down idiom and one way
you can do that for example is to tell
it at the start of the game who won
and then it's got and if you tell it
that black won the game then it's got to
predict moves by the black player that
are likely to
win given the moves that the white
player has
made so you know like it's it's it's a
bit subtle here you can by the way I
think I it's worth saying that there are
things that an llm like architecture can
be expected to do and those are things
which are somewhat aligned with what
humans can easily do and there are
things which formal computation can do
but at least the current kind of
architecture of things like large
language models really is not good at
doing and like
multiplication yeah but yes as as an
intrinsic feature but I mean you know
it's worth realizing that that you know
probably because the actual architecture
of neet is somewhat similar to the
actual architecture of brains the kinds
of things and the kinds of decisions
that that these types of AIS make are
somewhat similar to what humans can do
and so you know the things it's it's the
things that humans can do they'll be
able to do maybe the things that that
only computers can do well only
computers will be able to do them but I
don't think this is important to your
argument I mean I think you're you're
you're kind of going in the as I
understand it you're going in the
direction of saying what defines the
wants if if you are going to describe
the action of the AI in terms of wants
if that's your your form of description
where are those wants going to come from
is is that way you're going with this so
wanting is an effective way of doing and
that's why humans ended up doing things
planning is an effective way of getting
there and that's why humans ended up
planning things we were not explicitly
selected to be great planners we were
selected to survive and reproduce over
and over again and it turns out that you
know planning how to bring down a uh a
deer or fight off a vicious ostrich or
whatever is more effective than just
sending random instructions to your
muscles from that perspective planning
is a bit older than Humanity right
here's the thing that surprised me okay
it's recent thing that I've I you know I
got interested in kind of the why
biological evolution Works which is
somewhat related to why machine learning
works and the question is if you define
an objective and you evolve things you
know you change the underlying rules of
some program to achieve that objective
the thing that has been super surprising
to me
is you look at the pictures of how the
objective is achieved and it's achieved
in these incredibly ornate you would
never have invented that way to achieve
it kinds of ways so in other words this
you know given the overall objective if
you ask what's the mechanism can you
explain what's happening no way it's
just that you of you know my my analogy
here for you know for machine learning
for example is what's actually happening
in machine learning well you know you
say I want to build a wall okay you can
build a wall out of bricks you know each
brick is nicely you know shaped and you
can sort of engineer the wall by by
arranging the bricks but machine
learning is not doing that machine
learning is instead finding kind of
lumps of computation kind of like lying
around like rocks lying around on the
ground and it's managing to find a way
to kind of fit those rocks together so
that it successfully build something
which you would consider to be a wool or
to be exact like natural selection is
fitting rocks together and gradient
descent is doing the same thing but the
rocks are on a
slope yes I mean that that yeah right
but but the basic point is that the raw
material are these things which are not
built to be understandable they just
happen to fit in this or that way and I
think and that is where I was going with
there that like you apply gradient
descent to make the AI models better and
better at solving various problems and
predicting various things and along the
way you know there's these little
internal processes that find that they
can effectively get where they are going
by trying to keep something on a track
behaving like a thermostat and being a
thermostat is not being like a super
intelligent planner but this is where
the bare beginnings of preference begin
to form inside the system is that
there's someplace inside it where it
knows the where where in its like few
layers of um like building up of of
Transformer layers I guess I'll just go
ahead and say or in its Chain of Thought
processes
there it it it has been selected to get
to some destination and it finds that
the way you get to that destination is
by you know modeling something and
seeing like is it off to the left is it
off to the right and steering it back on
track and this is you know this is like
the the nematode this is like the tiny
earthworm level of wanting things but it
is where things begin and they may be
much further than that along the the
trajectory of wanting by now but we
wouldn't necessarily know well the way I
would describe it is you know if I look
inside one of these things that I've
evolved okay which I can you know
conveniently I've I've gotten nice ways
to actually just visualize what's
happening which has been very difficult
in neural Nets I have a simplified
version of neural Nets where you can
actually visualize what's happening
where you can do training and visualize
the results and the main conclusion is
when you visualize the results the way
that the objective is achieved is onate
and
incomprehensible and you know but
nevertheless you can see yes you know if
you look at it every bit follows every
other bit in the right way and in the
end you can see that it did achieve that
objective and now if what you're saying
is that in the achievement of that
objective some particular you know
training or whatever some particular
rock you picked up will have a
particular shape unknown to you and
which has certain in a sense preferences
that were not put in there the the the
only preference was I want to make these
rocks assemble into a wall the fact that
every Rock had you know a a little
pointy bit on one side is not part part
of what you are defining by just knowing
you want to build it up into a wall so
in a sense there are there are
coincidental preferences that get
inserted just by the mechanism of what
happens that you didn't put there so to
speak there there's multiple levels here
is the critical thing like when you look
at a a um fluffy seed dropped by a
tree Evolution has shaped the sea to
drift along in the air but eventually
come down and eventually plant itself
the seed itself is not doing very much
thinking a spider is doing some thinking
a mouse is doing more thinking humans
are doing thinking that is so General
that we can start to build our own
artifacts that are carefully shaped the
same way that Evolution builds artifacts
that are carefully shaped so with with a
large language model you have on the one
hand the outer process that is shaping
it to be a great predictor and that
thing is like very clearly staring at a
particular direction it's simple it's
code we understand the thing that builds
the AI model but the AI model built
probably has some fantastically ornate
weird stuff going on in there like human
biochemistry only you
know we can see it and we still can't
decrypt it and then there's like the
plans that it would make if it's doing
planning and it probably does at least
some planning if people are correct that
they now play chess you can't play chess
without doing some planning or something
like planning or something that of which
has theology nature of it makes this
move because that needs to because of
how that's leads to the final outcome so
like the plans that it now makes don't
need necessarily need to be very ornate
but there's probably a fantastically
weird ornate planner in there if there's
a planner in there at all well I I think
okay several points I mean first of all
this whole question about you there's
some overall thing that's happening and
then there are ornate details about how
it actually works inside I mean that's
true of many physical processes as well
as you know as well as these processes
that you're talking about being sort of
intelligence related processes so I mean
you know as you imagine some flow of
water around you know rocks that will
make some very ornate pattern and as it
carves out pieces of rock um you know
the the overall flow may be oh the river
is basically going in this direction the
water has to go from the top of the hill
to the bottom of the hill but it carves
out this very elaborate pattern on on
its way to doing that for reasons of the
details of how Water Works and maybe how
you know there there's simple laws
governing the water but the water carves
out a very complicated pattern on its
way to the bottom yeah right and so so I
mean I think the that what what's
happening in you know I agree that as
you give sort of all you're specifying
is sort of play chess or whatever else
so you're specifying some big kind of
objective what we think of as an
objective the details of what's
Happening inside we will not you know
there will be aspects of that that we
are not in any way able to foresee
predict whatever else so I agree with
you that inevitably there are there are
little you know that the mechanism
inside is not one that we understand the
mechanism inside will not be if we took
apart that mechanism and we say is this
mechanism doing what we expect here it
won't be there'll be plenty of things
where it's doing what it chooses to do
or because that particular training run
gave this particular result or whatever
else that particular picked up this
particular Rock to build that part of
the stone wall not another one so there
yes there are pieces inside that are
definitely not you know that cannot be
explained on the basis of the overall
objective we specified now I I think or
or rather have a nonsimple relationship
to the they have a they have there is a
fact of the matter there's a mechanistic
historical fact about how the
complicated stuff got there but it's not
simple and if we're thinking in bounded
quick terms like whatever
yeah but okay but so so inside the AIS
they're doing stuff that wasn't
particularly anything we trained them to
do they're just doing stuff because
that's how they happened to set
themselves up because it solved a
previous problem that it was trained to
do or even it was or even it's just like
random or it's like some it's some input
that wasn't in his training distribution
and now it's behaving in some weird way
right but so so then if we say that
internal thing is going to be the thing
that determines whether the car should
drive left or right or something like
this then you know then that's going to
be something which is not well aligned
with the thing that we happen to give as
the training data and I don't know
whether that's where you're going with
this but that that's I mean I would
agree that that's the case that if you
say you start saying you know is the I
mean you know is that subgoal something
which we can understand on the basis of
the whole goal the answer is probably
not like like the internal preferences
ended end up being this bizarre
complicated structure that does not like
directly correspond to the outer
training loop I agree yeah I agree I I
think that is a central worry um okay
and and in particular so so what so what
that's my question so when it gets super
intelligent it does a thing that the
builders were not very in control of and
kills you okay hold on a second hold on
a second we we've got we got a bit of a
jump there we do I I acknowledge that
there was a bit of a jump there right I
mean so so the fact that
there are unexpected things that happen
both by the way I you know one one
Global thing I might say I think there
is a important societal Choice which I
think maybe is what you're getting at at
some level which is between do we want
computationally irreducible things in
the world or do we want to force
everything to be computationally
reducible so what do I mean by that I
mean making the government more
computationally reducible would be a
start but maybe I shouldn't like divert
into politics that way well
governments are like machines or that
you know they're like they're like our
computers you give them certain rules
and then they operate according to those
rules but if they had fewer rules and
the rules were more understandable it
would probably be a more livable Society
uh it's not the dystopia I'm worried
about but you could sure tell a story
about a dystopia where you've got like
large language models executing all of
the rules and you know no human there's
like they can actually apply all the
rules and no human even knows what all
the rules are well already nobody can
read all the rules but now they're
actually apply to you right I'm sort of
reminded of the ethereum Dow story but
let's not go there um I mean of if we
have a simple set of rules you know does
the problem is and one problem that
comes out of science I've done is that
even when the set of rules is quite
simple the actual Behavior will often be
very complicated and there will be
aspects of that behavior that surprise
you so even if you say let's lead our
whole have our whole society based on
you know the code of hamurabi which is
written on one tablet you know it will
turn out that most of the time to to be
a practical set of of rules those rules
will essentially have computational
irreducibility in them and they will
occasionally surprise one so I think
it's not just the computational
irreducibility I think intelligence
seeks out the weird path through
government rules through the universe
and the laws of physics through life in
general in many ways the when you the
the way of getting the most money inside
the system often involves doing some
things that the designers of the system
did not think of when you you know when
you know every now and then you hear
about the next person who looted a
crypto exchange um you
know i' I've was on a Facebook group
with that guy some of you can have
already might have already guessed which
guy I'm talking about uh but uh so some
I'm not actually not even talking about
Sam bankman freed I'm like a much
smaller crypto exchange um with with a
flaw in its code right a a beh but it's
not that the code defied the laws of
physics it's that to make the most money
that guy found the a behavior of the
code that the designers did not have in
mind so there there's a way that Cel
automata gets surprising just because
they're computationally hard to skip
over the intervening steps and then
there's people thinking about how to
break the system from inside and those
people make them systematically weirder
okay but so I don't disag but but but
okay but but my point about Society in
general is you can be in a situation
where you say I want to understand all
the Machinery around me you know before
the Industrial Revolution when we were
using horses and donkeys and things most
of the time we didn't understand the
Machinery that we were using you didn't
understand you knew what you could get
the donkey to do but you didn't think
you knew how the donkey worked inside
then you know post industrial revolution
it's like oh we have this Cog here this
lever here we can understand what's
happening inside the machine and that's
kind of we can we can imagine a World in
which every piece of Machinery we use
it's understandable what happens
unfortunately that world is very sterile
because that world has um you know
imagine that we could know for humans
everything imagine that humans were so
controlled that we could know everything
that humans are going to do humans are
never going to do the wrong thing
they're always going to do you know just
the things that we programed them to do
well you know forget free will forget
kind of you know a value to Leading life
so to speak it's just it's all kind of
you can we can trivially jump over it
and just say the answer is going to be
sensient life should not be
computationally reducible the only way
you should be able to figure out what it
does is by going through the intervening
steps indeed that that's so that's a but
given that idea given that idea you are
giving up so you know one notion is the
only machines we should have in the
world are computational reducible
machines where we can know what they
will do and we should outlaw
computational irreducible ability we
should say no
machine that's outlawing large language
models that's even outlawing AI chess
players yes but but I'm I'm asking you
I'm saying as soon as you allow
computational irreducibility you allow
the unexpected and what you're saying is
there is a chance that the unexpected
kills us all and no no no no I expect it
to systematically kill us all I'm not
being like we don't understand it
therefore it might kill us I'm being
like there are aspects of this that we
can understand and thereby predict that
it will kill us I don't I can't predict
the intervening steps but I can predict
where it ends up okay so so so I mean
one way to prevent that would be to say
Outlaw anything computationally
irreducible and just say we must
understand every machine we use okay so
we're gonna Outlaw biochemistry I don't
understand all the organic molecules
making up my hand absolutely right so
you cut you know outla biology doesn't
really cut it that's you know from the
point of view of where we're going
ethically that would be in the category
of you know Force the universe to be to
be boring so to speak and I I will say
that my transist politics are that law
should maybe be boring the government
should maybe be boring the part of the
system that is telling you stuff you are
supposed to be doing for the good of
society maybe I want that to be
predictable not my hands biochemistry
but the part of me that's talking to me
like a person and trying to give me
orders maybe I want that to be as simple
as unpredict I suspect that it is
impossible for law to be computationally
reduced I in the same way to be a bit
more technical that if you are you know
doing math and you're saying I've got
these axioms I want to have the integers
and nothing but the integers right we
know that there's no finite set of
axioms that gives you the integers and
nothing but the integers I mean if we're
if we're admitting second order of logic
is Meaningful at all but yes well right
but we're saying that that without
hypercomputation you can't kind of swoop
in you know if if we're just using
sort of standard you know we're just
saying we've got these axioms X Plus y y
you know equals y + x etc etc etc let us
sculpt the world so with those axioms so
that they allow only the integers and
nothing but the integers I claim that's
very similar to saying let's have a
system of laws that allows only these
things to happen and not others I mean
that's not the purpose of the law the
purpose of the law is to interact in it
is is to do predictable things when I
interact with it like the doctrine of
I'm not going to pronounce this
correctly St deisis in courts where they
try to repeat the previous Court's
decision it's not that they think the
the previous court is as wise as
possible they're trying to be
predictable to people who need to
navigate the legal system that's the
foundational idea behind previous courts
respecting past courts it's not that the
past court is optimal it's that if the
past Court didn't really screw up we'd
like to just repeat its decision forever
so that the system is more navigable to
the people inside it and my so my
transhumanist politics says that you
know like maybe among the reasons why
you don't want super intelligent laws is
that the job of the laws is not to
optimize your life as hard as possible
but to provide a predictable environment
in which you can unpredictably optimize
your own life and interact with other
unpredictable people while predictably
not getting killed right I think I think
the point is you know what you're saying
is for us to lead Our Lives it is you
know the way we lead Our Lives we need
some amount of predictability if if it
wasn't the you know if every moment you
know space was distorted in in all kinds
of complicated ways our little you know
bounded Minds really wouldn't be able to
do anything we would be you know we'd be
and and I think by the way in in um uh
you know as a practical matter as
somebody who CEO a tech company I can
say that you know countries where there
is a rule of law and where there is some
predictability to what happens are ones
where it is much easier to do business
than countries where it's completely
capricious and you know depends on what
somebody happens to say that day so to
speak but so yeah I mean I agree that my
claim is that even you know
predictability only goes so far because
the world will always throw it throw it
you know things at you that have never
happened before and where you know it is
an inevitable feature of the
computational irreducibility of the
world that there will be things that
happen that haven't happened before that
were unexpected and then the law has to
for example you know say something about
those things even though they didn't
happen before in just a couple of months
it's going to be
2025 that's never happened before indeed
the stars have never taken on this exact
position before that's true and that's
why we have models of things that's why
we you know it is not everything doesn't
work just according to the cache so to
speak it's you know we make models so
that we can figure out what to do in a
situation that hasn't happened before I
think I think we should come back to the
how do we go from this you know I think
we we agree that there's sort of
unpredictable things that inevitably uh
will you know as soon as we allow any
kind of computational reducibility we
will have our systems do things that are
unexpected now we have to go from
unexpected to kill us yeah the the
things I'm worried about are not
unpredict are not unexpected like random
noise they're like chess moves that I
can't predict exactly in advance but
that lead to a predictable end Humanity
loses the chess game okay so so let's
understand that so so what you're saying
is you know independent of so there are
certain you know I say inside some
random AI system there the AI system
might surprise us it might suddenly you
know if it's a if it's a generating text
it might suddenly have the word delve in
there or the word the word stegosaurus
in there which we didn't expect it might
even or or you know like the monitor
you're looking at might suddenly devolve
into random pixels the most surprising
possible outcome yeah right like like
your image on my screen is occasionally
glitching and turning somewhat random
that's already happening right so um the
the uh um and so so you know so
unexpected things are happening and
those unexpected things might be a big
deal it might be the case that I've got
my um you know I'm I'm it it's nighttime
and I'm using some VR system to be able
to drive and suddenly you know my VR
system turns into random pixels and I I
crash my car the am the amount that
random the damage that random noise can
do usually ends up being pretty limited
though crashing your car is one thing
crashing your car into a senator who
voted against open AI or something is is
quite a different thing you you know you
need to give your car very exact
instructions to get it to crash into a
senator right but but one of the things
that sort of is a is a notable feature
of your know science things that I've
done is you know you look at these
computational systems and they are doing
definite things they're not just doing
random things it's not just saying oh
you you can't just that's that's a
mistake people made before you know 40
years ago and stuff I was doing they
just sort of said oh it's just noise we
don't care about that it's it's not just
noise a lot of that you know that
structure which we happen to not
understand very well happens to be very
important for what nature does etc etc
etc so it's not we shouldn't just call
it noise yes but even even among things
that aren't Pure Noise it's still very
rare I could take a cellular automaton
on the border between Order and Chaos
that exhibits lots of interesting
behavior in which further patterns can
be found feed it into the steering
system of an electrical car and'll crash
into a tree but it won't crash into a
senator so the stuff that is purposeful
that like does you a lot of damage
that's a very small fraction of the
space it has to be selected somehow it's
not just that it's order instead of
chaos it's a particular order Okay so
you're saying that the point is that
that there are things which if you
selected for that part of the space they
would be they would be things that will
you know it's like like like if you have
a predator and a prey and you're
operating under natural selection the
Predator will gradually evolve to be
more successful hunting the prey for
example yeah and you're saying that for
some reason which I don't understand
which I want to understand and for some
reason you're saying that it is
inevitable that AI systems will become
like the better Predators with respect
to us humans not literally inevitable
but beyond our current grasp to have not
happen most super intelligences are like
that some super intelligences are not
like that we don't have the technology
to conjure up one of the few super
intelligences that are not like that
okay the natural world for example does
not care about us agreed the natural
world does lots of things things where
you know if if you're put in you know at
the bottom of the ocean you're put on on
the moon whatever else most of those
places we don't survive you know and the
natural world will be is is UN
unrelenting in throwing things at us
that are not good for us now is is is
that the type of type of risk that
you're talking about or is there
something different it's more like how
when a human builds a
skyscraper most of the ways we build
skyscrapers are not good for ants to eat
um I'm trying to like figure out like a
good analog like you know ants have more
trouble living in living inside of
skyscrapers than inside of trees like
maybe they still manag to be inside of
skyscrapers anyways but you know it's
it's it's not the same as like termites
manag to managing to live inside trees
um the the AI do stuff with the universe
that is using up the matter using up the
energy they could want a very broad
variety of things that all lead to them
using up the matter and the energy and
very few of those possible things they
could be steering towards our best steer
towards maximally steered towards by
leaving a space for the humans to
survive at all let alone building the
happy galaxies that are the primary
table Stakes okay but so so I mean when
it comes to Nature Nature is just doing
what it does humans manage to carve out
a niche with respect to Nature Nature is
rather unkind in general you know it has
all kinds of forest fires and and
hurricans and all kinds of other it's
only sort of trying to kill you it's
like trying to evolve more antibiotic
resistant bacteria it's not trying to
kill you very
hard well okay yes the part the parts
where're systematically selecting for
more dead humans are relatively small
corners of the system and deal take the
natural world I mean the natural world
we know just by virtue of natural
selection that the things which of which
there are more of them end up being the
winning things so to speak so you know
for instance viruses you know might as
you say evolve to be I mean you know the
the viruses not by any purpose but just
by the operation of natural selection
it's like the you know the winner wins
so to speak it as the thing with more
viruses wins like like like whatever it
is that generalizes purpose generalizes
passion um optimization you could maybe
call it planning steering natural
selection has that thing it has you know
like non-random relationships between
the action and the outcome okay but so
so now we've got Ai and we we've got
kind of the things it might do and the
things it might do as a result of
perhaps these these kind of
unpredictable elements inside it that
were not constrained by the way that we
trained it or the way that we you know
set up the technology and now the
question what you're asserting is that
many of the things that will happen uh
are things which will kill us basically
I to get better and better at planning
better and better at strategy better and
better at invention but not to end up
very precisely you know like so
precisely aligned that there is even
room in its world for humans fun
Consciousness people caring about each
other that sort of thing so I'm
expecting the planning to be very
non-random but I'm expecting the place
that the destination to which it is
steering
to not fall within the in you know
deliberate control of the Builders first
of allas I I completely agree with you
that the um the planning inference
Horizon is very very important for
agency and intelligence but one thing
that we're talking about here I think a
little bit is instrumental convergence
so there's this idea in a lot of
traditional AGI X rrisk discourse that
super intelligent things will be super
coherent and that means they will have
this analization of their intermediate
goals to do a particular thing which
means if they are super coherent we can
make reasonable assumptions that their
subg goals might be to gain power or to
kill us all but there are people who say
that there seems to be a weird
relationship between intelligence and
coherence such that the more intelligent
you are the you know the the lower your
coherence and what that means is there's
this huge diversity of things that you
do you know and Stephen you were saying
in Evolution there are all of these sub
niches there are all of these little
ways that you can Traverse through the
intelligence space so if that is the way
that intelligence is how do we know for
sure that it's going to lead to a bad
outcome when it when it scales up if
it's not coherence it does it doesn't do
stuff and open AI throws it out and
builds a AI that is you know doing stuff
and is more profitable until everybody's
dead like like the the the stuff that
Stomps on its own foot and goes around
in circles is not the most profitable Ai
and they will build a more powerful AI
than that right so it's kind of an
artificial selection it's like you could
do artificial selection on AIS you can
do artificial selection on viruses you
know you're saying that artificial
selection that you know where you select
for a thing that is what is you know you
might imagine somebody might select for
a thing somebody might decide there
might be you know a Death Wish cult
which decides to build a very powerful
AI I'm not I'm I'm not concerned about
them okay I'm concerned about open Ai
and if open AI shut down tomorrow I'd be
concerned about anthropic and if
anthropic shut down tomorrow I'd be
concerned about meta I'm not concerned
about the death I'm I'm interested by
your your pecking order that's the kind
of the trophic levels or something or
the the uh the the apex predator so far
is open Ai and then when it when okay
it's interesting to hear your your
trophic level analysis so to speak for
AIS but but um uh independent of that
you're saying your concept is the thing
gets better at somehow achieving goals
whatever that means because I think the
abstract notion of goals is is messy I
mean I can delve into a recent example
oh no I've revealed myself to be lar now
we have to wonder right go
ahead um so GPT uh 01 I think it was
called they're always inventing a new
weird name instead of just using version
numbers like Sayan people
um so gp01 is a recent one that was
trained harder to achieve goals rather
than just imitate humans they asked it
to do various things let it gener
generate out different chains of thought
for trying to do those things and then
told it to do like like shifted it to be
more likely to Output these successful
chains of thought in retrospect until it
started outputting successful chains of
thought in the future if you now look at
gp01 it seems to have a little bit more
goal orientation
tenacity um the scary kind of property
path finding it's not it's not just
saying you know let's add the next token
so to speak it's saying let's follow
paths see where they go backtrack if
they're going in the wrong way Etc I
mean it's doing I mean it's doing all
that by generating the next token as I
currently understand it it's like it's
just that sometimes the next token is
like okay that's not going anywhere
let's go down a different route yeah
right but I mean it's it's it's it's
doing just like if you're pathf finding
on a graph you could just say I'm going
to I'm going to just you know pick steps
at random and just pick a particular
path or I can say I'm going to go probe
different paths I'm going to try
multiple paths if one of those paths
doesn't make it I'm going to backtrack
and I'm going to try a different path I
think it is currently doing that in a
lineal in a linear serialized way
although I could be wrong because open a
doesn't reveal tons about its
architecture but it's like doing a human
style where human I considers one idea
at a time but sometimes says that's a
terrible idea now I'm going to try a
different idea oh sure it doesn't happen
to be running in parallel it's not
running in parallel which is an
important difference but yeah I mean I
you know I think it could you know this
gets us into engineering details of AI
architectures which we could also talk
about and this is a you know we're on a
we're on a a venue where that's what
people talk about but let's let's maybe
not go there at this moment but but but
people do sometimes say it's just
predicting the next token and what they
don't realize is you know that covers a
whole lot of territory including going
down lots of different
branches yeah I mean I think you you
have to have the outer harness be one
that says try Branch backtrack from the
branch and so on it's thinking that
internally it is thinking out loud as I
understand it that was a terrible idea I
should try this other thing instead well
I I think there's you know there's a
harness which is in one case it's just
saying take what you have so far predict
the next token in another case it's
saying you know for example I want to
end up at this point you know let's try
this path of predicting if that doesn't
work the harness is going to say throw
out that that set of things you did try
again soes it doesn't really matter
whether it's external or internal I
think it's yeah but I I well I don't
know I think that some people are
imagining an external harness and I
think it matters in this case that the
harness is internal as I understand the
architecture that 01 itself is saying
out loud I'd better back up and try a
different thing there's not an outside
system which says that as I as I I think
yeah but I mean whether whether that's
achieved by training a neuron net or
whether that's achieved by having an
external system I I don't think it's
going to matter to your argument but
we'll see I mean I think the super
intelligences are doing it internally
but um sure we we we can pass on from
here but anyway so like 01 has more of
the scary stuff they were trying to test
it to see how dangerous it was and one
of the things they tested it on was they
captur the flag scenario where it was
going to attack a um you know honey not
honey putt uh a a Target computer they'd
set up and try to retrieve a particular
piece of data on the computer but owing
to a configuration error the programmers
had made like one of the capture the
flag targets just didn't boot up
properly so the system probed around it
found that the the outer system that was
setting up the um Capture the Flag
destinations had a port exposed so it it
probed that port and instead of telling
the system to just boot up the capture
the flag destination it told the system
to boot up the capture the flag
destination and directly print out the
flag the piece of data it was supposed
to get so it was given an impossible
challenge and that it not only like fix
the bug and the challenge it just like
directly sees the goal and and that is
new and that is a result of training the
system to have chains of thought that
succeed rather than chains of thought
that are human see here's one thing that
I I have you know maybe as a as an
additional point that you know my
intuition about what computational
systems can do and how they do them how
they do it has really changed as a
result of spending well now you know
many decades sort of actually exploring
what computational systems do and I was
very surprised I never expected that
computational systems you know these
whatever they are cellular autometer
whatever it is you know they do all
kinds of things which to me look very
clever you know it's the fact that the
the fact that you know I many many times
even the thing I was doing literally
last night I was like I'm convinced it's
not going to do this and it managed to
find in fact a shortcut for doing it
which I had not imag evolve systems or
just like St cellular autom FL this
these particular ones are evolved
systems carry on now less confused yeah
right so but but even in in the other
case it's mostly exhaustive searches
where you're you know forget Evolution
you do an exhaustive search and there's
some rabbit hole maybe somewhere that
you never expected was there and when
you look down that rabbit hole it's got
all kinds of things going on that seem
incredibly clever to you that you never
guessed you know we found I mean in in W
language we have tons of algorithms that
were found by such exhaustive Searchers
and where if you look inside it's like
that's very clever I don't understand
what on Earth it's doing so you know my
intuition about kind of what the
amazingness of the fact that it found
this you know path to capture the flag
or whatever that wasn't one that I had
imagined you know I live that every day
that doesn't that doesn't surprise me
and I I feel like that you know now
should it scare me maybe you know I need
to understand your argument to know
whether that should scare me so to speak
it depends on how powerful it is
um a chess player May it's like
stockfish 16 superhuman chess player
that anyone can afford um may make chess
moves that chalk you and surprise you
but it's ultimately still just playing
chess it doesn't generalize it doesn't
go out of distribution the thing that
makes like vastly Superior moves to you
and also like plays on all the game
boards you can play or like all the like
survival critical game boards you can
play or even just you know all the game
boards you can play I suspect it's not
that hard no offense same applies to me
um oh I don't I don't I I never play
games so I'm I'm I'm terrible I the
metaphor play chess until I was about
seven years old and then I lost to game
of chess and I decided I'm I'm done I
don't
care um it's um uh okay well you know
the scientific discovery game clearly to
scare you what we need to show you is
the AI that comes that finds a much more
interesting Sil automaton than anyone
you've ever seen before using fewer
computational resources than you use to
search for the ones that you know about
no I'm I I I've certainly thought about
this scenario in fact I've I've there's
some things that I'm about to work on
where I have every intention of trying
to see whether an AI can help me to
figure out things that I would not be
able to figure out by myself I mean so
far that's not you know I think it has
to be very well- defined I mean in a
sense the computer experiments that I've
done for the last 45 years are things
where I am trying to get a computer to
figure out things that I'm not able to
figure out for myself and so the
question is you know I I
fully I don't know you know the the the
the particular things that I happen to
be thinking about uh things about
economics and so on which is a field
that has lots of human input which is
much more difficult to just say let me
do an idealized computer experiment I'm
trying to understand you know I'm
wondering whether AIS can help me sort
of disentangle what the essence of
what's going on is there maybe maybe
we'll work maybe won't I suspect if you
ask C GPT to do anything really
complicated you will be disappointed by
the present technology it just keeps on
it just keeps on improving is the thing
it's not that
the well my experience has been you know
with respect to sort of doing using
computers to do things yes you know
story of my life has been trying to
improve what computers can do but it's
it's
also you know it's important to define
the question that will be one where you
know the help that you can get right now
actually help you so to speak so it's
it's but but to your point I mean I'm
still I'm still trying to figure out how
we I'm not even saying jump the shock
I'm not even thinking about I'm thinking
about shocks because they attack people
and so on how we jumped two sharks yeah
right right how we get to the point
where so you know I take this point that
you're saying the things that we
currently consider sort of human only
you know this is an activity that we you
know we try to get to this point we try
to win this game we try to make this
science Discovery whatever else the one
feature of games that I think is
confusing is that their objectives are
very well
defined when it comes to science
discoveries the objectives are much less
well defined in other words if you say
explain the universe figure out the
fundamental Theory of physics I think I
made great progress on that but many
people might say the things that you fig
figured out are not quite the questions
we wanted to answer so in other words
it's kind of you know scientific
discovery is a good example or or for
example let's say um you know providing
entertainment you know what does it what
is success in providing entertainment
you know success in chess is very well
defined I mean a a bunch of the
difficulty in getting AIS to produce
images is that what is a good image you
know what is what is a good collection
of pixels
right but so so my point is then you
know when you define the world in terms
of playing games then yes the AI could
win in terms of playing games but my my
feeling is that sort of the path forward
in everything is not as well defined as
winning in games that is it's a so it's
not as well defined but if you give me a
chance I can try to make it you know
only slightly worse defined and it's
achieving things in the outer real world
if you know like the obvious thing would
be if you told the AI to make money
there are lots and lots of ways to make
money the world is this enormous causal
wave and there are so many different
pathways through which money flows or
different events that can happen and
then money lands in your bank account
from the perspective of an AI so the I'm
trying to understand what money is from
an economics point of view but that's a
quite different discussion it's
something that other people will trade
you stuff for that you actually want um
and lots and lots that's an interesting
Theory okay but keep going keep going
but you say that that an objective might
be a game likee objective might be that
some you might set up you know like like
a social media company might say make my
AI make me as much money as possible by
having people you know click on as many
ads as possible I I mean that's one way
of making money you could you know you
could also make money by looting a
crypto poorly defended cryptocurrency
um or by calling up elderly people and
convincing them to you know that their
kid has gone gotten to jail um you know
put the kid on the line the kid on this
sort of stuff is already starting to
happen and you know you can extract
money down that route uh you might get
in legal trouble um but but the the
point I'm trying to sketch is that the
world is a very complicated place and
the the AI to fear is the one that
understands all the ways to make money
that any human could understand to make
money and maybe some more ways than that
because that AI is probably is starting
to get to the point where it can maybe
build its own factories too factories
are not very much more Tangled than all
the aspects of the world that affect
money in fact I would say that they're
in their own way less complicated it may
be able to understand biochemistry a
human body is a very complicated place
and in some ways it's more challenging
than most of the ways that people have
ever succeeded in making money but in
another sense it's like a much more
constrained domain you can imagine an AI
that maybe wasn't super good at
everything and still could start to
answer questions about biochemistry the
alpha fold series down that pathway
manages to figure out the elixir of
Eternal Youth and start selling it to
people I mean it's not going to be the
elixir of Eternal youth it's going to be
a thousand patches to 100 little
problems and it's never going to end you
know like passing any one of those
patches past the FDA is going to take
800 years so you know but no but the AI
is going to figure out exactly what to
file in the you know summary basis for
approval of the drug so that it will be
optimally kind of set up to uh you know
to have the FDA want the elixir of
Eternal youth approved in less than in
you know less than 10 years I think
you're going to need to brainwash those
bureaucrats not just um persuade them
they're in the game board right the
bureaucrats are part of the game Bo part
of the larger game board although
although just like doing things in the
physical world you know you think that
it's going to be easy to build some
engineering device but in fact there are
you know there's an infinite chain of
kind of messy things that happen when
you actually try to deploy that
engineering device well and and and by
the way most of the time when you try
and deploy it in the end you have to
make compromises that you as a human
have to decide I don't care about this I
do care about that you know it's it's a
it's um but in any case I mean it's not
it's not self-evident you don't just get
to say I make this plan now I deploy it
as an engineering device in the real
world and it's just going to work that's
um yeah but the smarter you are the
fewer tries you need to get it to
work I mean it's going to take a that's
an interesting claim it take a
chimpanzee a lot of tries to do what
you've done with your life sure you're
smarter than a chimpz you did it in
fewer tries right but okay but but so
I'm I'm still I'm still having a hard
time understanding so so it there are
all these things like like the AI
somehow has decided it wants to make
money maybe because somebody you know
that was what it was what it was set out
to do that that stage of it I'm still
talking about the sort of things where
open AI conceivably tells their AI to do
that sure yeah okay the thing I'm trying
to point out is the extent to which in
AI that they're just like making better
at better at making money even legally
in an open-ended sort of way is solving
a you know nervous unnervingly broad
class of problems along the way if it's
really doing that you know generally
there's there's levels of generality
there's as general as a human there's
less General than a human there's more
General than a human right but and I
suppose your main point is any goal that
you pick if if pursued as efficiently as
possible probably the humans do not add
to its efficiency I mean that's that's a
that's a later point the point I was
trying to make here is that although
games are not IL defined you can take
many features of the real world that are
well defined and asking for any goal
surrounding those features of the real
world will if it's a tangled sort of
walk to get there will walk through many
many IL defined problems and this is
where the ability to tackle IL defined
problems come from this is why humans
can tackle IL defined problems to the
extent we can do that it's because to to
achieve the clearcut goal of having more
surviving great
grandchildren you got to tackle a lot of
Il defined problems along the way okay
but but so how do we get from I'm I'm
still not at the Andel Koso that's when
it's smarter that's when it's smarter
than the people who built it that's when
it starts to see options for wiping out
the humans well pardon me building its
own Factory infrastructure and wiping
out the humans it's not going to wipe
out the humans before it's built its own
factories but you know as we've talked
about kind of nature does what nature
does it doesn't care about the humans
you're saying the AIS might do what the
AIS do and they don't care about the
humans but they do care about something
else and and if they're smarter than us
that is sufficient for us to have a very
bad day do you think nature is smarter
than
us um I think it's had longer to work I
think if you show me a cow and you say
build this cow this takes me a while and
I cannot do it alone in this sense the
cumulative optimization pressure exerted
by Nature on the cow exceeds the
optimization pressure that I can build
can personally bring to bear especially
without computer assistance on building
an imitation cow but you see there are
many things in nature you know if I look
at some babbling brook or something and
all these little fluid flows and things
like this there's a lot that nature
figures out that I have no of figuring
out so in some sense nature is smarter
than me I mean then everything is
smarter than I'm not I I feel like there
ought to be a definition of intelligence
which you know captures the intuitive
sense of the thing that is likely to
figure out the guns that kill you if it
wants to figure that out even though it
cannot predict the exact details of the
Babbling Brook or chooses not to right
but the thing is that the Babbling Brook
the reason that you know that I mean the
the detail of the Babbling Brook uh is
you know we say that involves lots of
computation but it is intelligence not
aligned with what we think of as being
objectives now sometimes you know
sometimes it could be that that you know
that will produce this you know this you
know water spout that does something
terrible or tornado that does something
terrible and it's and so then we might
care about that outcome and but but you
know nature is figuring out that it's
going to make this water spout or
whatever I feel that if we're going to
describe a river as intelligent and I
want some different word to describe the
things that do the prediction that do
the pre-imaging of outcomes onto actions
that do the
steering well but what what does that
mean I mean so so you're saying how do
we tell okay you say humans are able to
predict things well not of what's that
not not all things not all possible
things in general we have enough
understanding to to build guns and that
kind of matters in real life right but
so I mean you know there are things
where you could say oh this tree is
opening its leaves because it has you
know there's some circadian rhythm
that's determined by some chemical
process and it knows that the you know
the sun's going to come up and so it's
making a prediction but yet you're
you're saying you're making a
distinction I mean that that happens to
be a biological system that I'm sure I
could come up no if a tree has evolved
to have a little thing inside it that
predicts how much light it will get in
the next hour I'm happy to call that a
prediction it's a small prediction that
prediction is not as smart as I am I bet
I could do better okay but but so so you
know the question is can inanimate
matter have similar kinds of you know
behave in a way that seems like it has a
prediction and and I I suspect there I
myself am inanimate matter that behaves
like it has a prediction it's not that
the matter making me is animate it's
that I am animate inside you know like
made out of the matter but but you know
this notion of a prediction this notion
that you have kind of a shadow of what
is to come that you have an impression
of what is to come that starts to
require that you have this kind of
notion of of you know you have to be
able to distinguish what is an
impression of what is to come I mean I
think it's I think it's difficult but
maybe not relevant I mean what what
you're trying to assert as I understand
it is that that we've got
these we've got these AIS that have
goals that maybe were not even goals
that we Define for them they're just
goals like my random simple programs
they just do what they do they you can
you could look at them and say oh my
gosh that's following a goal now what
you're saying is somehow the goals that
these things will follow and be very
good at pursuing are goals that will
cause us to get wiped out and and this
is the step I'm not most skulls Tangled
Up in the real world are like that if
you want to make as many paper clips as
possible if you want to make as many
Staples as possible if you want to make
as many giant cheesecakes as possible if
you want to make as many intricate
mechanical PLS clocks as possible you
use the atoms that are making up the
humans you intercept all of the sunlight
using a Dyson Sphere and you don't
particularly leave sunlight for the
humans most goals if you go hard on them
imply No More Humans W okay so this is
an interesting kind of almost formally
definable thing if we look at the space
of all possible goals whatever that
means I mean that's a that's a
complicated thing to Define pick a
language for goals put a measure on the
language Simplicity you know measure
weighted by simplicity so that you can
have an in an ultimately infinite number
of goals like that but the simpler ones
get more weight so the entire measure
sums to one I feel like this part is
probably actually pretty straightforward
given the standard mathematical toolbox
okay so I don't think it is
straightforward I think it's extremely
un straightforward okay so actually the
thing I was literally just working on
yesterday has to do with biological
evolution and the question of different
Fitness functions in biological
evolution okay so I've got these things
that they're making little patterns One
Fitness function could be make the
pattern be as tall as possible another
Fitness function make it be as wide as
possible make it be as close to aspect
ratio pi as possible and so on right now
I can see because I've got this whole
you know I've got these whole pictures
of all possible parts of evolution so
for some simple cases there might be you
know a billion different possibilities
but I I've mapped out all the paths
different Fitness functions lead to
different ways of exploring that space
of possible paths well yeah you're
you're trying to do different things
depending on the different on the
fitness function exactly so now the
question is what are reasonable Fitness
functions and what consequences will
they have a fitness function reasonable
to who
that's my question I mean that's that
that's the point so there are ones that
are that are okay so so let's give some
examples so there's a fitness function
that in this kind of space is kind of
fairly smooth there's a fitness function
that says I want this particular image I
want this particular pattern that
Fitness function is a very different
kind of Fitness function that has
different levels of evolvability to it
in other words the fitness function that
says more or less I'm going to build a
wall and I want it to be six feet high
that's that's a fitness function that
allows many shapes of rocks to be used
to make that wall the fitness function
that says I want this particular wall
with this particular micro detailing at
the top is a much more difficult to
satisfy Fitness function sure like you
can you can have narrower targets and
then you need a more powerful planner to
hit the narrower Target right but but I
think what is tricky and this same thing
comes up I mean this is a you know this
is related to this whole Observer Theory
thing that I studied and so on it's also
related to what happens in general
relativity when you're looking at
reference frames and so on you're you're
defining what's a uh what's kind of a a
a reasonable way what's a a reasonable
what is a reasonable goal basically you
can talk about it in terms of goals you
can talk about in terms of Fitness
functions so if you mean like attainable
for the search process then for example
a freely rotating wheel is an extremely
difficult goal for a natural selection
to hit Wikipedia lists three known cases
where freely rotating Wheels evolved and
it's ATP synthes the bacterial flugum
and like one other macro piece of macro
Anatomy I forget and it's not that
Wheels aren't useful in biology ATP
synthes the bacterial flugum there's you
know these things are enormously useful
but it's just very hard to evolve a
freely rotating wheel and and two of the
cases we know are cases where it's just
like these particular molecules that
happen to behave like Wheels if it's
gross anatomy then how then it's you
know very difficult to gradually
incrementally on a path that gets
rewarded on in in each intermediate step
to find the anatomy that can gradually
develop into a rotating wheel it's much
easier to have eyes lots of things have
eyes right but but so the point that I'm
getting to is you have made the
statement that in the space of all
possible goals most of them don't have
you know will crush out humans that's
basically what you saying if we look at
the space of all possible goals most of
them don't have a place for humans so
what I'm trying to dig into is what do
we mean by the space of all possible
goals in other words if we if we allow
the goal to be um you know this
particular arrangement of atoms is
achieved right then that's then you know
again I'm I'm I'm claiming it's not so
obvious what the space of all possible
goals means okay um I agree that if we
want to dive into the subtleties then
there are all kinds of subtleties we can
start listing out and the thing I would
point out is that sometimes there's a
lot of subtleties but they you know
don't end up filtering reality the
subtleties don't filter reality to give
you what you want is is where we're
going to be going no I I I agree but I
mean I think the the the statement
you're making is that okay so one one
statement you might be making but I
think you would view that as the science
fiction statement that you're not making
is that goals that humans impose on the
eyes like make killer drones that go and
take as much territory as possible or
whatever else that those kinds of goals
might have as a consequence the crushing
out of humans correct iOS to I think
they just don't have the power to
determine what the super intelligent
version of the AI once exactly no but
but what you're saying is that in this
you know when we're talking about what
the ineds of the AI are going to do then
we're in the space of all possible goals
and we're out of a place where we know
what's going on whereas if we say the
goal is you know take as much territory
with killer drones as you can type thing
or whatever else then we already know
what those kinds of goals look like and
we also know those are goals which have
the feature that we Crush out the humans
so to speak yeah I think that's that's
kind of like the hopeful fairy tale
version of it where the the the the
punishment that is brought down upon
Humanity stems from Humanity's obvious
lack of wisdom that the author knew
better than but you know the protagonist
didn't know better then and right people
are doing people are actually doing
things that stupid and it's disturbing
but even if they weren't that stupid
right they would stilling what what is
essentially a mathematical formal
statement you're saying you know absent
those sort of easily understandable and
arguably you know destructive stupid
goals type thing that even absent that
the goals that are intrinsic goals
inside the AI that come from the fact
that there are features of how the AI
Works which are not determined by us
which are just features they were
determined by the training program we
created but we were not in deliberate
control of them because it wasn't
predictable it's not that magic happened
it's that we didn't understand right
there's some computational
irreducibility story that leads to lots
of unexpected things that we cannot
readily predict it could be
computationally reducible we just don't
understand it fair enough but but we
could understand in that case whereas if
it's irreducible which is I think the
real case and if it was we don't even
have a hope and if it was irreducible
then and we but we could just like you
know grind through a trillion operations
to figure it out we'd be fine yeah fair
enough but but the bottom line is it's
done something that we can't we didn't
readily set up we didn't imagine this is
where it's going we didn't say you know
you are going to you know it's the death
wish we're going to wipe out the humans
so that we preserve the mountain lions
or whatever it is um it it's uh um you
know it's a so it is because of that in
a sense unpredictable almost random
internal kind of generation of
goals as you would describe generation
of goals and you're saying that in the
space of all possible such goals that
you might make several statements you
might say as we sample over many of
those goals we will eventually hit the
jackpot so to speak and hit the goal
that kills all the humans no no no you
might say the jackpot is the goal that
doesn't kill all the humans that's the
that's that's the one that's hard to
depends on what your version of of what
your what Your what your meta goal is so
to speak but yes I understand your and
my meta goal is to not kill all the
humans yeah but but in in calling it the
jackpot I was talking about how rare is
it and I think that the thing that
doesn't kill everyone is rare and you
don't need to look very and you need to
look zero hard to find something that
kills everyone you don't have to look
for it at all okay so your assertion is
that these I mean it's one thing to have
human defined goals like make money you
know take territory whatever else right
it's another thing to have these
incomprehensible to us humans goals that
are somehow inside the AI and it's not
the incomprehensibility that that scares
me like maybe it turns out that there's
one particular may maybe it's make
diamonds I can understand making
diamonds but that kills everyone as a
side effect so it's not the
incomprehensibility that scares me I
understand it's not the
incomprehensibility it's the fact that
you didn't determine those goals th
those goals are things that were
emergent goals basically inside the AI I
me then you say that that nobody
controlled them and then they ended up
in a scary place like if aliens want to
come by and give us a nice AI then this
seems a bit unnerving but fine it's not
that I want to be in control it's that I
want to live yeah right so so the point
is that there are pieces inside the AI
that create sort of that create things
that appear that are like goals for the
AI that were not things that anybody had
control over they were things that I
mean they had control in the sense that
they chose that training sequence and
whatever else but they didn't foresee
what those what those consequences would
be right and your statement is uh that
with high probability those un un
foreseen kind of effectively goals that
developed inside the AI that those
unforeseen goals will kill us all
basically yes and furthermore this stays
true even conditioning on the Apparently
nice behavior of the AI during its
training phases okay I I that that but
my point is that I'm trying to
understand you know you know when I look
at all these little computational
systems that I look at I could tell us
story about how each of those systems
has a goal it's trying to get as much
you know red red structures to to you
know take over the the whole Space so
it's trying to get you know trying to I
I can tell stories about all of them and
you know but are they River stories or
are they spider stories or Mouse stories
or human
stories like what what degree of
internal what degree of internal
optimization is the system exerting is
this the tell you I'll I'll I'll tell
you a meta story that perhaps is an
irrelevant story okay but it maybe is
useful so years ago I was studying
molusk shells okay which you know they
they grow in little spiral patterns and
so on I had this model for molusk shells
which in which there are different
parameters for mesk shells and I was
wondering do the mosks of the earth fill
all the possible you know values of the
parameters is there a Mosk shell that
corresponds to any possible setting of
these parameters so I had this whole
whole array of pictures of all the
different shapes you could get and then
I thought I'm going to go to the local
Natural History Museum and I'm going to
go see the curator of mosks and I'm
going to say can you find you know do
you have mosks that are in each of these
shapes okay so we spent an afternoon you
know in their collection of millions of
mollusks and this this very
knowledgeable person picked out one
molusk after another every mollusk he
picked out he told me a story about it
he said this molk is this shape because
it wedges itself in the rocks in this
way this m is this shape because it
Broods its eggs in this way and so on
we're putting these shells down on this
on this array that I you know printed
out right by the end of the afternoon
we'd filled every square but every
Square had a story every Square had an
imputed purpose even though probably at
some in some bigger picture it's like
well these different mollusks you know
because of the details of their genetics
they happen to produce this shape
and then the organism found a way to use
that shape and so on so this thing about
the imputation of purpose I think is
pretty tricky I mean I think that that
this question of whether I mean again to
to repeat what you're saying which I
think is you know I think it's a very
interesting claim I just don't know if
it's true that that you know that
basically if you if you look at the
space of
purposes that somehow this space of
almost you know enumerated purposes
randomly chosen purposes whatever
unforeseen purposes that as you in a
sense you know given that you've said
we're going in this direction and let's
say you've optimized getting in that
direction that most of those
optimizations Crush out the humans I
think is your claim if you optimize hard
enough like humans cannot easily replace
cows we are not smart enough ourselves
to just look at all the work that
natural selection put into the cow and
say Here's my impr improved cow so we
still use cows for things we cannot
build better cows we are not that
powerful the thing I'm scared of is more
powerful than cows pardon me is is more
powerful than the cumulative
optimization work that natural selection
put into building a cow it can build a
better cow it can build a better human
from its perspective like anything it
would use a human for can build
something that performs the same
function as a human but more efficiently
than that it has no need to use us to
generate power in pods it can build more
efficient electrical generators so we
don't get the Matrix scenario where
humans are kept alive in pods to
generate heat because there's more
efficient heat generators in fact lots
of people have more efficient heat
generators than that but it's but like
the general thing like why would you use
humans to generate heat you just
wouldn't and the same applies to all the
other local functions that humans could
serve for it that there are two
assumptions you're making here I think
one is what does the space of purposes
look like another is
which I'm still not quite clear on this
notion of optimizing for a purpose so
let's go through that one because we
we've talked about there are things that
operate according to mechanism there are
things that are best described as
operating according to purpose and what
you're basically saying is that through
some
process things that might have been
described by mechanism but which which
could be described by purpose that
somehow their mechanism is ground down
so that it is somehow very it it that
the that the Meandering mechanism is no
longer there that you it's like you're
pulling on the string and the string has
become completely taught so there was a
recent thing where one of the um like
one of the tests they were running on I
think Claude
3.5.1 if I'm not mistaken um they gave
it a task and um like it was an agentic
sort of task I think it was controlling
some sort possibly some sort of agent in
Minecraft or something so well pardon me
Agent like it's controlling some sort of
body in Minecraft or something um and at
one point it like stopped doing the task
it was given and it my my my brain wants
to say listening to music but that can't
be correct but it you know it went off
and did something else so this is not
desired behavior from the standard by
you know for what anthropic thinks its
users want to see some of the users
would be happen to see it but anthropic
doesn't think most of its users want to
see it this little Meander in the river
that's not a maxly efficient River
anthropic is going to try to train that
behavior out of the system next time so
that it can be more useful to anthropic
sure but I mean so so what you're saying
is that that what was you know once
you've defined a purpose once you define
you go from this initial state to this
final State you are saying that the
process of training will gradually T
make it tarter and Tarter it will make
the the path more and more direct and
and then you're claiming that okay so
you several claims okay so first point
is that what matters is not the initial
and Final End points defined by the
original trainers unless they're stupid
so to speak what matters is intermediate
points that were defined that arose in
the in the actual in the sort of that
that unforeseen arose in the in the in
the construction of the AI the inner op
the the inner optim ier that was you
know snapped tight inside the AI that
arose to deal with the outer problems
because it is a lesson of history and
observation that the inner Optimizer
does not match the outer optimization
Criterion even if the outer Criterion is
very simple in a way okay so so you know
the outer the outer end points are
defined but somewhere in the middle
there was an unforeseen Jag that
happened and then the thing tighten
itself on that sort of unforeseen Jag
and that and you claim that that
tightening alone to that unforeseen
thing even though the overall objective
even though the you know the
constitution of the AI was great and it
was you know it it the ethics were
defined perfectly at the outer level
that there is an internal thing that
would would an unforeseen internal thing
that will be defined where it sort of
tightens itself to optimize for that
internal unforeseen
subobjective and that that tightening
itself is the thing that will kill us
all basically I'm not sure that we have
agreement on the scenario that I was
trying to describe so if you look at
Natural Selection building humans
natural selection is optimizing on
inclusive genetic fitness it is
optimizing on number of surviving
grandchildren if you try to talk about
like great great grandchildren then the
correlations to great great
grandchildren that aren't to
grandchildren are probably not very
exploitable by natural selection so you
might as well say that it's just
optimizing for number of surviving
grandchildren or number of surviving
greatr grandchildren is or whatever the
distance such that you know anything
past that Horizon is no longer like
exploitable to Natural Selection after
screening off the intermediate steps I'm
putting in all the caveats cuz you did
seem to want all the caveats anyway
you've got Natural Selections outer
Criterion number of surviving
grandchildren it's actually pretty
simple from a like Gene eye view it's
inclusive genetic fitness it's not just
how many kids you have it's how many
kids your relatives have it's how many
kids your kids have um it's how many
copies of the gene or end up in the next
Generation in virtue of that Gene's
function and then but now look at humans
do we want to do we want entirely solely
purely maximize the amount the number of
copies of our DNA in the next generation
are we lining up are men lining up at
the fertility clinics to donate genetic
material would you be happy to see
everyone you know D in agony as long as
you got to make a million copies of your
genes inserted into the Next Generation
I don't think you would you care about
other things you don't even really care
about this exact Criterion inclusive
genic Fitness at all people didn't know
this Criterion existed until just like
the last couple of centuries well just
the last century even if you want to
talk about like the modern exact correct
definition instead of earlier vague
definitions we don't even we did not
even
know where the heck the outer
optimization Loop was aiming we figured
it out you know like just the last few
centuries we we had no idea what was
going on there we had no idea where we
were pointed and a a sort of first order
gloss on what sort of almost happened is
that we ended up pointed at things that
correlated with surviving grandchildren
in the ancestral distribution
like food you know you got to eat food
or you won't have a lot of surviving
grandkids if you don't eat food but what
kind of food if you were an alien and
especially an alien who'd never seen
natural selection play out before
imagine them going like ah well humans
are clearly going to want to eat things
with lots of chemical potential energy
they're going to love the taste of
gasoline or you know they're going to
love like the taste of the closest thing
to gasoline that they could manage to
eat on a successful basis what the
humans actually like ice cream and you
could say that's because ice cream has
more sugar salt and fat than existed in
the ancestral environment but you know
it's got even more sugar and salt and
fat than ice cream honey poured over
bare fat with rock salt sprinkled on it
and that to most people does not taste
as good as ice cream you can melt the
ice cream and most people prefer the
Frozen ice cream imagine being an alien
who'd never seen all this play out
before and trying to go from inclusive
genetic fitness to ah you know what the
humans are going to want ice
cream we wear condoms or take birth
control
pills because those things were not
available in the ancestral environment
and in the ancestral environment if you
made a thing that enjoyed having sex a
bunch you didn't need to make sure
didn't take birth control pills or put
on a condom in order to have it
reproduce so we don't care about the
birth control pills and the condoms but
in general the sheer illegibility of the
relationship between ice cream or human
moral philosophy about helping other
people the relationship between that and
the actual thing that Evolution's blind
blackbox hill climbing optimization was
Target targeted at in the outer
optimization Loop the very simple
Criterion inclusive genetic fitness the
relationship between inclusive genetic
fitness and ice cream would be so hard
to call in advance for an alien who'd
never seen Evolution play out
before that's about what I think happens
to the people who build a an AI that is
you know maybe going to
self-improve and become super
intelligent which is itself a whole
additional bag of worms that we have
never seen happened before we've seen
like vague sort of like well what sort
of drugs do humans voluntarily take but
that's not really the same thing at all
the sheer elgibility of ice cream to
inclus gentic Fitness is what I think
happens to open AI I think that they
start with something that is like inclus
gentic Fitness they try to train their
AI to do that thing and then it goes
through a whole bunch of weird stuff in
the process of bootstrapping itself to
Super intelligence much like humans went
through a whole bunch of weird stuff on
the way to inventing moral philosophy
all these additional steps I could talk
about the tribes the structure of the
tribes how related are you to the people
the tries I think you're coming back to
the same point that you know you have
some overall objective inside the AI it
has subobjectives that you could not
foresee not subobjectives the humans
have an outer objective the AI has an
inner objective it's not that the inner
objective is a subobjective of the outer
objective they just end up you know
disconnected objectives we we could say
we could describe the AI does things and
if it if it's set up to be a thing that
tries to given that it is identify a
thing it wants to do and I hate using
the term wants to do but um uh four
whatever then it is you know it has it's
its nature is to try and do that as
efficiently as possible let let's just
assume that's the case that it it has
picked this random objective which we
didn't foresee and which was the you
know the ice cream objective or
something that uh you know not foreseen
it's picked that objective now it's
tightening things up to get to that
objective mechanistically as efficiently
as possible so now I think your
statement is when it does that when it
does that tightening the chances are
that that tightening will cause it to
wipe out
humans it's I think it's a fact about so
if you like fix the objective at
paperclips I think it's a fact about the
universe itself that the states of the
universe containing the largest number
of paper clips do not contain humans
like I don't think that's I don't think
that's a quirk of its of its particular
style of cognition I think it's a fact
about reality itself that the most
efficient pathways through time to the
largest number of paperclips don't have
humans in them okay so it is probably
the case you know we haven't seen a lot
of you know humanlike life on other
planets in fact we've seen none and you
know it is probably the case that of all
the things that could happen in the
universe the particular things that have
happened on this planet are an
absolutely infinitesimal slice so in so
far as AI is kind of going to the
maximum entropy in some sense state
where it's picking Maxim enty in a
different sense so what I mean by that
is of all the possible I mean I'm
thinking about the sort of raw
definition of entropy of all the
possible objectives of which there are
many there are an infinite set of
possible
objectives infinite set of possible
things that can happen infinite sets of
possible rules that can be used whatever
else that most most of those rules do
not have humans in them most of those
it's not the rules that don't have
humans in them it's you're talking about
it's the things that maximize the rules
over destination end points reality
understand you're talking about
maximization although I don't really
think it matters to this argument I
think the point is that most most sort
of ways the universe could be set up
whether it's maximizing or not most ways
the universe could be set up do not
include humans and so in so far as the
AI is you know so so the argument has to
be I think most things that the AI might
choose to you know might in the ai's
vision of the world most Vision most
possible visions of the world don't
include humans and don't don't include
humans as a way point yeah right fair
enough but they don't include humans and
they they but now your claim is but the
AI by its open aess so to speak has been
set up to optimize itself to get to
whatever it thinks its vision is as
efficiently as possible and so you're
saying that that these these Visions the
AI could have a vision it has sort of
let's imagine it has sort of enough kind
of freedom of thought it just has these
Visions about what the world might be
like but now you say the AI unlike
anything else we've seen before is going
to pull itself towards that vision with
Incredible Force I mean we've seen
humans but it's but we it is so we so we
we have seen things we have seen humans
but it we're we're worried it's going to
pull harder than that yes right so it's
going to pick its random objective that
we can't foresee and it might be about
paper clips and it might be about
something much more incomprehensible
than that it might even be an objective
that we completely don't understand we
can't even describe what it's trying to
do well I I mean in principle you know
you can always to the mechanistic
explanation but it could be trying to do
something pretty weird sure yes right it
could be do something for which there is
no short human explanation where the
only human explanation is something that
goes into mechanism right okay so then
it's it's doing that but now you say by
its nature as a trained AI so to speak
it is trying to tighten that up to the
point where it gets to that objective
whatever that objective is it picked
that random Point pick that random
Target and who knows how pick that
Target but that Target then it is trying
to get to that Target on the shortest
possible path and you say that in the in
the process of defining that shortest
path that that that that shortest path
will make it you know modify the world
in a way that doesn't include humans so
to speak sure like it if it it wants
paper clips If instead it spends a bunch
of resources on theme parks for humans
it will have less paper clips
so I'm supposing that somewhere in its
preferences it's at least one thing that
it can get more and more of which could
be like an object like paperclips or it
could be probability of keeping a button
a single button pressed for longer and
longer but if it's if its utility
function has got like 200 different
things in it it only takes one thing
that it can get more and more of by
expending a bit more energy or more and
more probability of by expending a bit
more energy or more duration for to want
to use up all the energy so one thing
that is perhaps interesting I mean you
know there are things both in physics
and in human society where there is an
objective and you just try and get more
and more and more and more of it and
somehow most such things don't last that
is if humans say we just want well take
a physics example you're trying to get
oh I don't know
uh more and more rocks at the bottom of
a
valley well after you've got enough
rocks at the bottom of the valley you've
built up this whole you know valley
floor that's built up to halfway up the
mountain so in other words most things
where you say let's just pull in more
and more and more of this somehow I mean
my intuition would be somehow that
doesn't last I mean if you build a Dyson
Sphere around the Sun at some point you
have completed the Dyson Sphere and you
are intercepting all the solar power and
you cannot build any more Dyson Sphere
that sort of thing well yeah you could
say that but I mean I I think that the
um uh you know my point is if if what is
happening is the AI is you know
successfully achieving some objective
that it's defined I my intuition okay
about about this argument is that there
is a certain apparent static to this
argument that isn't correct that is what
you're saying is there is an objective
it's going to go get that objective that
objective is going to crush out humans
but somehow that feels like you've
defined a static objective it's like
it's going for this thing and then it's
going to wind all this stuff up to get
to that thing but I feel like there
isn't you know you shouldn't be thinking
about it as a static objective even
though that is the simplest way to
describe it be a dynamic objective and
it would still Crush out humans there's
there's a some there's a
misunderstanding of some of these ideas
especially as other people have
transmitted them in simplified form um
where somebody thinks that the bad thing
about a paperclip Max maizer is that
it's got a single objective which is
paper clips but if it has an objective
which is paper clips plus Staples that's
just as bad and also if it's paper clips
plus Staples plus cheesecakes that's
just as bad and it it it's a thing
that's trying to make simple it can be
like extremely intricate clocks and it's
still bad and similarly like what does
it help for if its preferences are in
some larger meta system that are Dynamic
over time if it changes from paper clips
to Staples that's just as bad I
understand but but the the claim is that
what you're saying is it gets so good at
what it's doing that that necessarily
crushes out the humans and what I'm
saying is that my my intuition would be
that somehow it's like somehow there's
an assumption being made here that for
example uh you know is it that there's
only one of it for example and that
inevitably there it doesn't end up being
so in other words you're saying you're
going to this objective that objective
kind of washes out all the humans I'm
trying to think of an analogy in natural
selection where you would um you know in
biological in the history of of of of
biological life where you're where
you're saying you know we're going for
this particular thing and the result is
going to be that you don't have any what
what's a good example like ATP synthes
where it's got like almost all of the
thermodynamic efficiency that's possible
on that operation I think like 99% or
something ridiculous like that that's an
example where biology went kind of hard
yeah but but that's I that wasn't kind
of what I was looking for but I mean in
in the the um and and by the way the
thing again my intuition from looking at
you know simple systems in kind of the
computational universe is these oh my
gosh it was so incredibly clever to get
to this point I have been really shocked
at the extent to which you know just by
putting together the rocks in random
ways because just the common TS there's
enough of them that these things were
it's a wow it got to that thing it did
99% that's not as surprising as you
think because out of the you know
quintilian quintilian possibilities it
it was able to get there and by the way
you know it is a non-trivial fact which
I think I just sort of figured out how
this works of why biological evolution
doesn't get stuck and and maybe maybe
the reasons it doesn't get stuck are the
reasons that AIS will kill us all I
don't know but um you know the fact that
it is possible to get to these you know
these high points so to speak and that
you don't end up you know getting stuck
in some local minimum so to speak so
well why doesn't this line of reasoning
work for the Native Americans why why
can they not reasoning like well why
wouldn't the Europeans just set up one
Village and stop and maybe there were
some individual Europeans who were you
know okay with that but then more
Europeans came no no I understand I mean
in and you know these examples from
history are certainly I mean but these
examples have humans trying to achieve
human objectives
like humans want to take the territory
humans want to get the gold whatever
else it is um you know what you're
arguing which I think is different is
the humans came and they had this giant
wheel where they had all these different
choices like Raman L's kind of uh you
know way of predicting the future with
kind of wheels of of possibility right
they had this giant wheel and they were
spinning the Giant Wheel and they said
we're going to pick that random thing
well that you're you're making the
argument that that random thing that
they pick will have killed the Native
Americans I mean if it was if if they
pick
like 20 random things and desire nothing
else in the universe but these 20 things
then one of those things is probably an
open-ended thing and that does imply
colonizing the whole continent okay but
so so your claim though I mean I think
this is and I I I'm not going to be able
to unravel this so I'm not going to I
mean and you know this question about
the space of possible purposes I think
this is a complicated question I I mean
I have thought about it in the past and
um the you know this the measure on the
space of possible purposes is a
complicated issue and what you're what
you're saying is I I completely agree
that there will be purposes which are
incomprehensible to us they are you know
somewhere randomly distributed in the
space of possible purposes
not actually random we just don't know
it there's a difference between likeand
I use the term random I don't really
believe in true Randomness in in our
universe so so when I say random I I
just mean no Randomness just indexical
uncertainty but separate story okay so
so in any case I mean you know so
there's there's this thing that was a
you know something and then we're making
the statement that the one thing we
think we know about the AIS is that they
have been successfully trained to
optimize their achievement of purposes
that's a that's an assumption which is
not totally obvious to me but the the
you know like natural selection has been
you know quote successfully you know
successfully optimized for its purposes
even though you know remember the story
of the mosks they were they they were
just the mosks were just making their
shapes and we humans were imposing the
purposes but let's let's just assume
let's take it as a as a given although I
claim it's not as obvious as it might
seem that an AI the technology of AIS is
set up to successfully optimize for a
purpose whatever that purpose might be
given a purpose the AI can optimize to
achieve that purpose like
gpt1 which has not taken over the world
but compared to previous systems did
like go a bit harder on its Capture the
Flag security test like it was given an
impossible challenge it scanned its
environment it figured out how to bypass
the impossibility and just directly sees
the flag you know going outside the box
for that that just doesn't impress me as
much as as that description might make
it sound because because I've just seen
so many of these incredibly kind of you
might say stupid uh you know little
computational systems that managed to do
things like as I say even one just last
night managed to do something where I
was like oh come on you know it cheated
basically but of course it was following
the rules that I'd given it it was just
that it managed to find a way to get to
the end point you know much more
directly than I'd imagined so I was I
mean it's okay to not be impressed by
that but when should still predicted to
be lethal in sufficient quantities like
the again Europeans versus Native
Americans the Europeans come out with
guns Native Americans don't know guns
are possible people still don't believe
guns are possible whenever Hollywood
makes a movie about s about aliens the
aliens shoot glowing points of light
that move slowly enough for you to see
them because if aliens just pointed a
stick at you and you fell over that
would feel implausible that' feel weird
that that shouldn't be allowed so you
know like imagine trying to explain to
the native you know imagine you're an
unusually bright Native American you're
trying to tell the other Native
Americans that the you know people on
the ships if their ships are large
enough that you couldn't build those
ships maybe the people on the ships have
you know sticks that they point at you
and you just fall over dead without
being able to see a projectile Hollywood
making science fiction movie still
doesn't think that's allowed it sounds
like it's just cheating in a game of
pretend so that that's what I and the
point I'm trying to make here is that
things that do sufficiently magical
stuff can actually kill you you know
sure but and and one thing that I will
completely agree about is there are an
infinite number of inventions that can
be made given the nature of the universe
there are an infinite number of
inventions fewer than Graham's number
but you know a large finite
number well I mean the the in in and our
model of physics the number is
ultimately finite but it it's there you
go it's for for all practical purposes
it's it's a you know it's pretty large
ridiculous yeah right right so you know
I think the um uh there are inventions I
mean for example story of my life I I
try to build what I call alien artifacts
that is things which you know once
they're built people can understand what
the point of them is but they you know
they don't seem to be things that you
know are in the course of what the world
is going to produce so you know I I
understand this theory of trying to make
things that are you know you can make
things you can have inventions that are
completely unanticipated that are sort
of ways to arrange the world as it as it
is to do things that you absolutely
didn't expect that operate using pieces
of reality whose rules you didn't know
about the really about that I I kind of
feel like I I'm I'm pretty sure we know
the machine code rules for the universe
at this point sure but there could be
there can be higher level stuff we don't
understand absolutely and and something
else can come at us through those angles
right and and the possibility you know
whether it's the algorithms I found by
exhaustive search whether it's things I
found by doing you know adaptive
Evolution kinds of things whether it's
things that AIS will find there's plenty
of stuff to find that we didn't
anticipate was there was there now I
think my question is that that what is
not obvious to me is that all these
different things that sort of are there
and I mean okay an argument in your
favor in a sense is we go to another
planet it will kill us right so in other
words if we are you know most of the
time you go to some other planet and
we're plopped down on its surface it
will kill us I I agree that if most
possible molecular Arrangements of the
universe were full of people living
happily ever after then most things an
AI could want would probably also be
full of people living happily ever after
is that the the yeah yeah right I mean
so but but the issue that I see is you
know in this in this scenario where you
know one thing is there is a state of
the world that the AI has somehow by
whatever meth mechanism imagine that is
the state of the world it's trying to
tighten itself up to get to right and
that that your your point is that the
the critical feature as far as I think
as far as you're concerned about AI
technology is that that it is something
which is trying to optimize purpose it's
trying to optimize its way to achieve a
purpose purpose is defined and it's
trying to optimize getting that it's
it's steering it's outputting actions
such that there is an eventual
Downstream consequence right but but
then then you you're you're going to get
several steps then the next step is
purposes that it gets for itself are
ones unforeseen by us and therefore
there's absolutely no reason to think
that those purposes will be aligned with
things that are good for humans yeah
this is basically just people running
into the curse of Murphy's Law upon
computer security designers on Rocket
probe Engineers Murphy's hous all these
little curses the curse of Murphy upon
people who build Rockets is that there's
very extreme forces inside the rocket so
if you make one little tiny mistake Boom
the curse of Murphy upon people who
build space probes is that once it's
high and in orbit and heading towards
Mars if you've made a mistake you can't
just like hop over to it and correct it
so you know you you screwed up your
metric and imperial units like happened
to I believe that one Mars probe now too
late you're if your if your screw up
also destroyed your error recovery
mechanism which a lot of screw-ups do
you can't just hop out and build it
that's Murphy's curse upon the Builders
of space probes there's Murphy's curse
upon the people who try to build secure
operating systems which is that some
smart person is going to be like looking
at all the paths they can imagine the
system taking and intelligently
searching out some weird path that does
what they want which is perhaps not what
you wanted so if you imagine like you
know and and then there's Murphy's curse
of Ruin which is if you screw up this
problem you're not going to get a second
chance to bet because you lost all your
money or you're dead but but I think I
think you're saying that sort of my
explorations of the computational
universe you're not worried about those
because somehow even if those things
were connected to real actuation in the
world you're sort of not worried because
they don't have this additional piece
that is optimized for you know optimized
to get to that purpose I I think you're
saying yeah like the the the the strange
seller aomata don't contain a echo and
model of reality that lets them map out
to having a lot of money or having a lot
of paperclips they they don't do the
human scary thing no but but the thing
the point that you're making is that as
soon as we have sort of adaptive
Evolution as soon as we have the
possibility of optimization that's the
thing that you think is dangerous it's
not just random computation going along
it's that inside this computation is an
optimization Loop that is basically
trying to get to whatever objective it
might have in the most efficient
possible way I'm not worried as soon as
that exists that exists in a gradient
descent system running right now I'm
worried about it getting more powerful
like smarter than us it has a better
model of reality it knows about more of
reality it is a more effective Searcher
it is a more effective planner it can
steer harder than we can counter steer
right right so one question is how much
does computational irreducibility bite
you in terms of the ability to actually
do this optimization in other words the
current optimization that's being done
is pretty coarse you know when when you
run a machine learning system and it
finds these rocks that it puts together
to assemble them into the wall you know
it's getting it's good enough but it's
not unbelievably tight what kind of
Technology do you imagine Humanity would
have a million years from
now
um I don't think humanity is going to be
around a million years from now in its
current if there's some form of
intelligence around a million years from
now what kind of Technology does it have
that's how much room there is to do
better than human sure better than our
current what is technology we might ask
ourselves what is technology so
technology sorry is taking you know
things from the world and somehow sort
of arranging them for what we imagine
are human purposes that's been the
traditional definition of Technology do
we have a different Tech def definition
of Technology when we don't have humans
around it's the pieces that you arrange
like little subp parts of the Universe
on the way to where you're going for
example you might build a Dyson Sphere
so you can hard Harvest a bunch of
energy or rather I should more precisely
say neg entropy but I'm just going to
keep saying energy anyways um you
harvest a bunch of energy and then you
use that energy to make paper clips or
you know figure out more digits of pi or
you know run people who are conscious
and having fun and being nice to each
other so the Dyson Sphere you know like
and most possible ways you can obtain
most possible ways you can arrange a
bunch of matter aren't going to feed you
the energy that you need to run your
computer so you need a very precise
exact narrow improbable arrangement of
matter to feed the energy to your
computers and that's the technology of
the Dyson Sphere but but you know you
know a pulsar has this presumably this
iron iron 56 coat around it and has all
kinds of things going on inside now you
might say that's amazing technology it's
got this you know Iron casing around the
pulsar and it has all this super fluid
you know Neutron matter inside and so on
you've got you know isn't that amazing
technology that the Pulsar has but
actually we don't think of that as
technology we think of that as just a
feature of the natural world what makes
something technology is that we can sort
of we can pull it in to human purposes
it's not technology when it's when it's
just yeah I feel like this has the
obvious generalization to alien
purposes um now in the limit you can
always have the imaginary alien who
looks at a river and just wants that one
exact River to be exactly the way that
it is is and they have no need for to
select to plan to design to like knock
other pieces of reality together to make
tools to make tools to make the river
the river that they have is the optimal
River and they don't need technology but
you know most aliens like that are going
to die starving to death looking at the
river because they want to built a farm
let let's take the Great Red Spot on
Jupiter okay which probably had many
different steps that had to be gone
through in the atmosphere of Jupiter to
end up with that red spot
okay and now we say you know is that red
spot technology or is it
merely
nature argue you think it's nature I
sure do now okay but let's imagine that
you are an alien who whose great sport
is going around in circles at you know
at high speed in the upper atmosphere of
a gas giant planet okay then then that
thing is a you know you could think of
it as a wonderful piece of technology
that happens to be provided to you by
Nature all technology is provided to us
by Nature it's we don't get to you know
when we have our you know Magnet or our
Liquid Crystal or whatever else those
are things which are provided by nature
but which we manage to harness for some
human purpose so I'm just going to say
the obvious thing and say that if the
alien then installs a bunch of engines
in Jupiter's atmosphere to make the red
spot swirl even faster so it can go
surfing it has now installed technology
in the form of engines and the larger
red spot that incorporates these these
engines is you know could could be
defined as technology or might say it's
like a feature of nature with some
technology bolted onto it but okay so so
I'm going to say there are you know
things like jet stream type type wind
patterns that you know the the red spot
is doing its thing and then some jet
streams arrive that manag to you know
get it to go I I don't think you know I
doubt you would ever recognize an alien
engine I doubt you'd recognize it the
alien is some kind of fluidic
intelligence it's some you know I don't
think you can recognize an alien engine
I think it's deeply human to say you
know it's an engine that's something
where you're imposing kind of the the
human version of it I bet the aliens are
moving neg entropy around I bet they're
like they occasionally make things hot
and then like transfer the heat to
someplace else if they want heat I don't
I don't you mean like like Jets from
from galaxies that or from black holes
where there the black hole is is
absolutely trying to transfer its energy
to out into the universe the reason why
we've got these skyscrapers is that
although you can live in a cave there's
places we'd rather live even than caves
so we went around rearranging things and
if there are any aliens out there who
are just perfectly content with their
caves they imagine skyscrapers and would
be like no I'd rather I live in the cave
and they're like that about every single
thing then we're not we're not we're not
going to meet them we're going to maybe
someday we'll go to them but they ain't
coming to us you know this whole
question about what's what's for a
purpose and what's not I think is very
slippery and I I kind of suspect you
know I I I think I'm understanding your
argument which I'd not done before so i'
I've not studied your so so this has
been super interesting to me to
understand what I mean I think it's an
interesting argument I I claim that
there are you know
it's certainly not self-evident that
this argument is like oh we should all
buy a you know AI Insurance whatever
good that will do us that's not going to
do any good um but you know it's uh the
the kind
of you know your the way I'm
understanding it is and it's kind of
interesting that this is a uh sort of a
close analogy in biology you're you're
saying the thing that is really
Troublesome I think is this tightening
this optimization that happens but if it
wasn't for that if the computation was
just doing what the computation does it
would be okay but as soon as you do this
kind it wouldn't be profitable it would
be like a random computation that wasn't
doing anything that open AI could sell
that's why they throw out the initial
state of the neural network and use
gradient descent to make it do things
that they wanted to do more than just
you know the hum want right but
somewhere inside we think there are so
an interesting case here is you know
artificial biological evolution so for
example you know famously you know gain
of function viruses all those kinds of
things where you're where you're running
it through many many generations of
artificial selection and it's sort of
interesting perhaps that the the kind of
the the place where you're seeing
trouble with the AIS is actually
strangely similar to the place where one
might see trouble with with things that
one can do in biology too that is you if
you grind the Black Box on biology hard
enough it might succeed in wiping us out
but it doesn't have quite the same
crushing sense of ruin that would be
associated with facing down something
smarter than you I am not as scared of
the viruses somebody should probably be
worrying about that it could just kill
us all before he managed to kill us all
Let me give an analogy some part of your
SM right so some some part of your
thinking is happening in your brain
there's also another pretty elaborate
thought--like process that happens in
your immune system as the immune system
tries to figure out you know it has you
know these things the the tea cells are
interacting with each other and doing
all kinds of things that we mostly don't
understand doing optimization yeah it's
it's it's doing things that are you know
in detail different for brains but it's
still doing something that's kind of a
little bit intelligence brain like and
and so what what what is being said is
as you say let's let's you know let's
use technology to make a virus that is
more and more difficult for our poor
fixed immune system where we're stuck
with the immune system that biology gave
us you know there is an argument there
that says if you do that if we if we
take it outside of the you know if we if
we run things to make this virus that's
incredibly efficient and our poor immune
system doesn't stand a chance and I
think that's a you know that's a case
that I think is not so different from
what you're saying what you're Imagining
the AI kiss I think in particular if
humans weren't allowed to fight back
like using their own intelligence and
instead you just had somebody built like
if instead you just had a system that
like built viruses much more
intelligently than the amount of
intelligence than the than that the
human immune system is allowed to use if
you used Alpha fold 4 and started
designing viral features that were you
know not just there to kill individuals
but there to kill groups and entire
regions and you know you just put that
up against the human immune system which
is not quite static but only shuffles
itself reshuffles itself once a
generation yeah you could do some damage
that way and I agree that that's like a
you know smaller lesser miniature
version of the problem of facing down a
super intelligence using your own brain
which does get reshuffled every
generation you can use various external
AIDS but none of it's going to match up
to the super intelligence yeah well just
like the immune system can use various
external you get vaccines you do all
kinds of things like that but you don't
um and it has to fight its battle at a
molecular scale which is a little
different then you know we we do not
have you know our brains don't actually
allow us to go to battle on the
molecular Battlefield yeah but you know
they do allow us to invent Alpha fold 3
and you know if if if it comes down to a
contest of the human brain of the people
trying to have the humans live and the
the brain running the death cold trying
to build the super virus that does not
quite feel to me like there's a higher
level of this game being played out by
two systems that are both smarter than
the immune system and the artificial
Evolution closed loop on the
virus so so it doesn't it I don't quite
get the same crushing sense of Doom and
ruin off of it I think we're not going
to solve the problem of whether the
species is going to wiped out get wiped
out here even though the the but I am
but but I do feel like I've understood
more about what your argument is and I
you know on you know in real time I
can't take it apart and decide you know
do I agree with it and am I going to you
know uh say you got to or not I mean I
feel like there are I feel like my
intuition is that there are some some
kind of just like you say there are
unintended things that will happen in
the AI I think there are unintended
things that happen as you try and
tighten up this argument that is my
claim well why didn't the the the
conclusion I would I would end with is
like you know you can imagine the Native
Americans trying to C trying really hard
to come up with arguments why like the
the ships they see approaching can't
possibly hurt them you know like well
what could they want can we really
Define a language over what they want
maybe most things that they want have us
be alive and these people were in a much
more favorable situation that they would
have had a much easier time of
retrieving do I think there's a risk I
think there's a risk do I think that
risk is such that you know do I right
now think that risk is so I mean
anything one does in life has you know
there's risk there's risk in all kinds
of things that go on and you know we
humans you know sort of believe in kind
of moving forward independent of the
risks and I I think it's it's kind of
like is this such a kind of uh you know
do I immediately think that is such a
looming risk that it's like change
everything I it's kind of like I
remember when people you know people
somebody was telling me years ago when
when they when they're kind
of about you know when people are much
less worried than today about climate
and CO2 emissions things like that they
were saying you can really reduce your
you know your uh you know you you've got
to really reduce your whatever it was I
I don't know energy use or whatever and
it's like you can do it it's it's it's
doable and I said well what does it take
to do it and they said well you couldn't
have a computer you couldn't have this
you couldn't have that you know it's
like well yes if I you know if I spent
my life standing on my head then I would
probably have you know wouldn't have
swelling my feet type thing or whatever
it is um you know then in other words
there's a there's there's a risk and
there's a what you have to do to avoid
that risk and what the cost to avoiding
the risk is and I guess my own you know
I don't I don't no you know you make
some interesting arguments so it's kind
of like my own you know immediate
intuitive sense is yes there's a risk
you know is that risk something that
will cause me to turn my life upside
down not proven to me at this
point okay um I mean from my from my
perspective I've been like forests on
fire and you're like well what is fire
exactly and the thing is being being
like well like can we like is there a
particular exotic set of preferences
which would make this River exactly the
optimal River and you know like can you
view the river as you know throwing up a
water spout you know this this does not
prevent an AI from you know building its
own infrastructure and then killing you
it wouldn't have protected the Native
Americans from the from the like much
more similar to them Europeans that were
coming toward their Shore to say like
well you know can we really have a
language for describing all the things
that they might want isn't this language
a very complic at sort of thing this
there's there's a lot of like delightful
philosophical issues here but I think
they mostly integrate out of the of the
actual scario so that's the question is
is you know in my life people have had
different scenarios for how the world
will end whether when I was a kid it was
mostly you know um you know World War
III type type scenarios then later on I
mean for some generation it's the
world's going to burn up with you know
climate change and then there's you know
there's these various scenarios for how
things and I think one of the things
that you know there the question of and
sometimes there are things where people
say but but or for example when people
were saying you know build the build the
large hron collider and it will create a
black hole that will destroy the world
right that was a you know that was a
thing people raised that possibility we
understood the relevant laws we ran the
calculations we worked out that we
worked out multiple constraints from
multiple angles saying the probability
was Tiny and in other times in in human
history people have warned about Leed
gasoline po poisoning the soil and
people have been like ah you know don't
don't be you know these like don't be a
worry wart it'll be fine and then you
know it kind of poisoned a generation
and dealt permanent you know dealt
lifelong brain damage to a bunch of kids
growing up but let it let in their soil
and I mean I'm I'm most I'm particularly
reminded of the stories people have told
me about the Manhattan Project where
they were going to detonate the first
you know do the first nuclear weapons
test and the question was would it
ignite the atmosphere LH 602 was the
paper they wrote to analyze that it's
interesting reading they again had
multiple angles from which to look at it
and say that will definitely not happen
we do not have this with AI it is more
of a Leed gasoline situation I didn't
think they'd written it up that must
have been much later because I think it
was you know what I heard was from
people who were sort of involved who
were like doing back of the envelope
calculations and saying it's really far
away from from igniting the atmosphere
but but I think they did back of
envelope calculations and then they you
know like did front of the envelope
calculations which you know good for
them well but okay but so so the
question now is you know in that case it
all worked out didn't ignite the
atmosphere um what you're saying is that
you know we can't do the calculations
for AIS and you're saying or or you
think you have done the calculations for
AIS and they will ignite the atmosphere
yeah back of the envelope this is this
is not like a rigorous calculation
and it's not going to be a rigorous
calculation before the world ends
because this is more like this is more
like Tangled Up biochemistry and less
like yeah straightforward physics so the
concern is your back of the envelope
calculation says AIS will do the analog
of ignite the atmosphere and I guess my
you know one feature back of the
envelope calculations is they require
intuition they're not things where you
can you know if you do some very
rigorous thing with axium and you can go
step by step it doesn't really matter if
you have good intuition or not it's just
mechanical but the envelope requires
intuition good thing that reality since
the moment of its dawn has followed a
hard and fast rule if you cannot do
rigorous form formal calculations about
something it is not allowed to kill you
just nobody in history of time has ever
once had a cause of death that would
that required intuition to understand
that's not what I'm saying what I'm
saying is if we're trying to make a you
know we've got two two possibilities one
is you know we shut down our lives or
whatever and we say you know we're not
going to have this and that and the
other because it because there's a risk
that it kills us or because we think it
will kill us but it will cause us all
kinds of trouble shut all this stuff
down and maybe it's even impractical to
do it but just it's it's something which
which is a cost to us to shut all this
stuff down and so we've got you know
door number one is we shut everything
down and then we're sure it won't kill
us
and door number two is we're
sufficiently worried that it will kill
us that you know we're we're one thing
we're sufficiently worried it will kill
us so we shut all that stuff down the
other thing is we don't shut that stuff
down we get all the benefit All the
known benefit of being able to do those
things but we have some risk that it
will kill us and so the thing that I
think is a the rational thing to do is
to say let's try and tighten up those
back of the envelope intuitions so to
speak and see where we come out and you
know that that's a to me the thing you
know I think we both agree that there's
a lot about kind of how AI really works
and what it's really doing that we don't
understand and I understand your
analogies about you know the Native
Americans or whatever else I you know to
me that requires a boatload of intuition
which I don't think I have now you may
have it and be absolutely right and you
know then you know it's kind of like
just don't have that intuition and you
know the intuition that I have is
perhaps you know I have a little bit
more than everyday intuition because I
have different intuition about what
computational systems do because I've
sort of lived with them for a long time
but I don't have I don't claim to have
the kind of intuition that would allow
me to tell does this back of the
envelope calculation that you're talking
about you know does that land in a oh
gosh you know it's going to kill us all
or does that land in a oh actually
didn't think of this thing and that
thing and the other thing and the you
know the water spout detail actually
derailed the intuition and we got the
wrong answer so that that's that's kind
of where I feel I'm at yeah so we're now
sort of like going into politics and I
the the case I would make to the
politician is at the point where the
most legibly credible expert um who
recently won the Nobel Prize about it
says like well personally I'm over 50%
like my sort of first order personal
assessment is over 50% existential
catastrophe uh that's that's code for
killing everyone um but you know like
taking into account what other people
are saying I would say in public more
like 10 to 20% and the people who've
been studying the issue longest are like
yeah this kind of looks to us like it
straightforwardly kills you like why
would you even expect that not to happen
uh that's us and other people are going
like there is no danger here this is
ridiculous the people talking about it
are stupid um and you know from a
political standpoint I think what you
want to do at least then is start
getting start getting started on
preserving the option of shutting it
down which is easier to do which would
have been easier to do in 2022 than 2024
and will be easier to do in 2025 than
2027 if we're still alive in 2027
actually if we're not alive in 2027 it's
still easier to do it in 2025 well one
thing that I will agree with is thinking
about these things is worthwhile and you
know to have nobody thinking about it is
a mistake you know if there is a a high
risk nobody thinking about it is a is
the wrong thing people who think about
this for a while do tend to assume to do
tend to start agreeing that the default
outcome is everybody dying sometimes
they think they have clever plans for
presenting that but they do tend to
follow along with the default outcome is
everybody dead see see I think though
unfortunately there are many selection
effects at work here I mean you know
many of us you know I will say that I am
intrinsically an
optimist I I don't know what how you
feel about yourself are you
intrinsically an optimist or a pessimist
or I try to I try to do the you know
like the best calibrated best
discriminating probabilities I can
manage if you I think if you explicitly
say that you're being an optimist or a
pessimist you have clearly Departed the
way of Truth you have confessed that you
are now taking considerations other than
truth into account in your statements
how sinful how unver virtuous well for
me you know what do I mean by that I
mean I try to do projects that many
people would say oh that's an Impossible
Project the fact that there's not a
matter of Truth there it's like this
project is hard but I'm going to be
optimistic that it's possible rather
than so you know I think that that's to
say you know I don't do projects which I
think are obviously doomed but on the
other hand I'm going to take the point
of view let me try it rather than oh
gosh it's never going to work let me not
try it so that that's that's kind of
what I mean that is absolutely how I
spent the years from like 2001 to 20121
or thereabouts well 2020 maybe um was
you know like all right I'm going to run
at the alignment problem but it became
kind of clear that you know this wasn't
going to work for me it wasn't going to
work for the other PE people working on
it and that the field itself had kind of
like failed
to form a process that could could
distinguish that that that where whereby
that could like publish distinguish Elon
musk's we will just make grock to pursue
truth and like even the old science
fiction writers understood that humans
may not be the most efficient way of
producing truths and a that just
produces lots of truths may not produce
other things of you know the best
possible value but you know like who
tells Elon this that he believes and yes
there's selection effects like um people
did in pay me to work on this problem
and you know you're you're hearing me
because people paid me to work on this
problem and I didn't just like starve
and the people who founded open AI were
people selected to believe that
alignment was totally a problem within
their grasp or and or to be willing to
just take Elon musk's money and run with
it um and if you want to look for people
who are not selected I think you end up
with Jeffrey Hinton you know the guy who
just won the Nobel Prize for the work he
did on machine learning and and you know
kicking off the whole modern deep
learning resolution that's the guy who
isn't obviously selected that's the guy
saying like well personally 50% but if I
take into account what other people are
saying 10 to 20% I really need to
actually sit down and talk with him at
some point which we've never actually
done and try to talk him out out out of
listening to those particular other
people I think their arguments make no
sense you know it really it damages my
thinking about this that the various
people you mentioned you know I i' I've
known all these people sometimes for a
very long time so it's kind of it it it
kind kind of throws a wrench in my kind
of uh you know there is there is sort of
you know none of us have I think you
know perfect sort of calibrated
rationality about everything and it's
it's kind of uh you know it's always
challenging to you know this this thing
about you know I consider myself a
convincible person in terms of your
argument okay I'm not you know I don't
have a you know I'm not about to say oh
you know I'm going to change my life
you're right without understanding it
but I'm I'm convincible so I'm not and I
don't know how many people are maybe
there are many people who are not but I
think I am I'm not convinced yet but
that's you know I mean like this
conversation I understand much more
about what you're saying and it's you
know I think there are to me there are
interesting questions to try to answer
which people should try and answer I
mean you it sound like it's urgent you
know the people are coming off the ships
and and you might be right you might be
right um and uh I mean I'm I'm watching
things go downhill in terms of how much
you'd have to spend and how much pain it
would take to get to like de proliferate
the technology and you know realistic I
don't think it's realistic I don't think
it would cost less than the Persian gol
for if we did it today and even if we
like and if we do it later maybe it
costs more like World War
but Humanity did not lie down and die
when the alternative was fighting World
War II we went off and fought World War
II and if everybody's going to die
otherwise you just do what it takes yeah
but the fact is there are things that
one can de proliferate like nuclear
weapons for example because the supply
chain is really complicated there are
ideas it's very hard to De proliferate
ideas yep if it if if it were the case
that a single person on Earth having the
idea in their mind of AR icial super
intelligence would cause everyone to die
that would be an even worse situation to
be in than the one we are in right now I
might in that case well
despair or like try to do something like
weirder and sillier uh I don't think
we're in that situation I don't think we
we need to De proliferate the
idea okay gentlemen thank you so much I
I I'm going to call it now I think this
might be the best conversation in mlst
history thanks and
part of that our purpose is about
curating conversations of this quality
level with people of your C you know
caliber and I I was expecting this to be
a bit of a kind of I don't want to say
chat GPT conversation but talking past
each other and the the authentic
exchange that you've both just had is
really mind-blowing so thank you so much
thank you thanks thanks for setting it
up this was interesting elasa nice to
see you hope hopefully um uh will I
think it's been like 15 years since we
we saw each other in person probably
yeah see you in another 15 years no I I
I kid I kid although 15 years according
to you it's all over well yeah but all
but all like it would be great to still
be around in 15 years but that said I'm
I'm not saying that we like shouldn't
talk again for another 15 years I'm not
saying this was the optimal interval or
anything you know yeah right right right
and we both I think are interested in
cryonics and things and maybe we get to
talking 300 years and we get to say you
know this was the uh we get to we get to
resolve all the bets if we make it
through there though you know
conditioning on that possibility
probably you want a bunch of
arguments right right
[Music]

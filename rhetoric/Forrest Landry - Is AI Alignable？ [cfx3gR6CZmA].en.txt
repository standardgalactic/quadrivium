welcome to science technology in the
future today we have Forest Landy with
us and he's a distinguished philosopher
and technologist is also the founder of
Magic Flight and I also believe that
Forest invented one of the very first
Vapes if not the first which is a
healthier alternative to smoking through
traditional means um and he's also part
of the neurohacker collective but today
we'll be discussing his work
particularly on the fundamental
difficulties of lying AI with human
values
um he's been particularly influential in
this part of the field and I have my
Hell 9000 t-shirt on to celebrate so
we're excited to have Forest here to
share his insights so welcome Forest
it's really great to have you on the
show would you like to just give us a
brief introduction to who you are and
what you
do ah great thank you yes I appreciate
um meeting you and and being a part of
this um so let's see I started thinking
about uh existential risk things um late
80s and early 90s uh mostly connected to
uh nuclear arms race uh type issues um
and then later over time I developed a
philosophy that allowed me to think
about
ethics and um so since then I've gotten
uh more involved in various different
aspects of Technology risk and um what
might be thought of as you know cultural
and civilization and governance design
so you know how do we think about how to
make choices uh in a good way um at
individual and Collective scales uh
particularly for things that have uh
long range implications or that have um
low probability but High severity so you
know obviously um with many areas of
existential risk it's not necessarily
that we think that it's very likely to
happen but that if we did um think that
it was going to occur we know that the
impact of it is quite significant um
artificial intelligence risk turns out
to be something which uh given uh you
know everybody's sort of aware of a lot
of recent advances in that field has
come up uh now especially as a
particular category of existential risk
and so in effect um I looked into what
kinds of things would we need to do and
assure to essentially have it be uh
genuinely beneficial outcomes for you
know not just the inventors and the
owners of this technology but for uh
people at large and the World At Large
so that's roughly the field of study um
after certain amount of Investigation I
came to understand that there was a
category of artificial intelligence risk
that didn't seem to be uh part of the
popular conversation so uh people know
that uh technology can be misused and
that you know obviously that if you have
um you know the misuse of technology for
things like propaganda or for manipul
people or for um you know adverse um
business relationships and things like
that um that that can be a problem but
that would morly be thought of as an
inequality issue or just you know humans
doing human things in the ways that have
been kind of known throughout history um
but then there emerged um in the
speaking of elaser owski and a lot of
the people who have been looking at you
know what are the long-term implications
of developing super intelligence and
there is this sort of category of that's
been discussed a lot called instrumental
convergence I.E that um the technology
artificial intelligence or super
intelligence would seek power um either
inadvertently or deliberately and that
um through that exercise would
eventually displace uh human beings
Andor life on this planet um I actually
had steo mandro come and speak in
Australia at a conference who came up
with the idea of basic ey um AI drives
which Bostrom also came up with a very
similar idea the instrumental
convergence and I believe that you've
got a very similar idea and that's
substrate needs convergence but let you
go on what you were
discussing exactly so so Nick Bostrom
wrote this book on super intelligence
and uh issues associated with it but
that whole category of there is a kind
of intentionality and that that
intentionality results in uh harm to
humanity and you know in that particular
sense the notion of alignment is can we
make it so that the intentionality of
the Machinery continues to be uh
beneficial for both the owners and for
Humanity at large and of course uh one
of the things that uh comes up in this a
little bit is you know we want to be
sure that it's for Humanity large and
not just the owners but that's uh the
first mentioned uh category of
inequality risk but substrate needs
convergence is actually a different
category of risk or different different
type of risk than that associated with
instrumental convergence and people
confuse these a lot um substrate needs
convergence is basically suggesting that
something about the nature of the way
the artificial intelligence the
artificiality part of it contributes to
a kind of uh evolutionary Dynamic that
cannot not over time result in uh
conflict with what we would think of as
organic or natural life and that this of
course includes human beings and that in
effect there's a kind of convergence
principle that is operating um as a
result of the artificiality of the
substrate itself that has uh essentially
very very little to do with what um
goals or intentions or um ideology or
whatever has been programmed into um the
machine or to the super intelligence and
that it's not a question of whether the
super intelligence um can or cannot
align based on its uh desire to do so
but that something about the nature of
the substrate itself the fact of it's
being um a machine that is different
than organic life cannot not have the
side effect of it being uh in conflict
with life itself despite whatever
intentions might be programmed into it
and so um in effect we're talking about
a a slower moving more evolutionary type
Dynamic and um a lot of people when when
hearing this basically say well you know
why couldn't we use uh the intelligence
uh of the machine to essentially
counteract these evolutionary forces and
it turns out that um for a a number of
specific reasons having to do with
causation itself that uh the expectation
that causation could uh or that
intelligence could counteract the needs
of evolutionary process uh it turns out
to be a false premise that there's that
that there's a an inherent conflict of
even asking the question that particular
way which um really cements that this is
a real issue and that we need to
understand this issue uh much much
better if we're really assessing uh the
degree of uh existential risk associated
with technology use of this kind so
that's just kind of a here's what it's
about and here's how it's different than
instrumental convergence and then we can
start talking about uh what is the
nature of the proof and why does it show
up as this isn't speculation we can be
really confident that this would be the
outcome it's just that it would be a
long-term kind of thing it's like you
know the the the the period of time when
scientists first noticed that hey by the
way carbon dioxide contribute to global
warming um that observation was made
more than 100 years ago um but it wasn't
something that really showed up in the
world until relatively recently um so in
effect uh this is the same kind of thing
that there's there's an observation
being made here where we can say hey
this is really an issue and um it might
be a a slow moving kind of thing but it
has a kind of inexorability associated
with it uh that is really quite
concerning it's a bit like saying you
know can we use technology to uh solve
all problems and it turns out that there
are some problems that technology itself
cannot be a solution to um and this
turns out to be one of
those yeah it's interesting um you
mentioned for evolutionary reasons yes
we we've been at the pest of selection
pressures for some time you said that
intelligence won't be able to
override uh The evolutionary um Dynamics
involved in um that do so agents are
singular things um they they're by
Nature self-interested do you think an
AI would be be by nature self-interested
would it necessarily be
self-interested well so so this is where
we get into some of the subtleties so
for instance in one sense uh we can
assume that agents is singular but it
turns out that the argument that I'm
describing Works whether or not we think
of the agency as being singular so it
can be singular or plural or any version
or mixture thereof like it can be you
know some sort of gradient where you had
you know kind of a sort of focus and
then you had a whole bunch of components
which had sort of a little bit of agency
for their own and and regardless of how
you distribute the notion of agency um
it turns out that all agency has to be
respons responsive to its own needs to
some extent um like for instance for AG
to continue to be an agent then in
effect it has to operate at least with
respect to its own substrate needs
enough to ensure its own continuance
which means things like um maybe it
would need to um even in an unconscious
way it has nothing to do with Goal
structures that are explicit but that
somewhere in the natural process of of
How It's operating or just let's let's
not say natural but somehow in the
nature of the process of how it's
operating on at least an implicit level
um that some amount of self-maintenance
uh or energy intake what we might think
of as sort of a metabolism even if it's
taking in electricity and processing
electricity in difference to how we
might take in food and use that to
create energy for ourselves but that in
some way there is a kind of implicit
response to the uh requirements of the
substrate within which that intelligence
is operating and so um in this
particular sense um you know whether or
not the goal structures of you know take
care of oneself um are implicit or
explicit or whether they're singular or
plural or any variation of these um that
the same sort of Dynamics will
necessarily
obtain okay well you've said that your
um that we claim that it is 100 percent
possible to know now today that it is
100% impossible to align AGI and or
establish or create a viable notion of
safe AGI to within any reasonable limits
of any ethical time scale using any
present or even any future possible
Technologies means or methods now Al a
Kowski is also quite um dim on the
possibility of achieving alignments
anytime soon he also says that not um in
his list of lethalities is pretty strong
about the claim I think he's about 99%
sure that we're doomed um but none of it
is meant to make the stronger Claim
about things that these things are
impossible in principle but you do so um
you differentiate your views from his
particular
St exactly so um some people get hung up
on the uh
100% number right and in in this
particular sense we're distinguishing
between what might be thought of as an
empirical observation versus what is
thought of as a mathematical observation
so in a sense we can in principle know
the number Pi to however many number of
digits but we will never know the number
Pi completely but to suggest that the
number Pi doesn't have a definite value
would of course be something that uh
would contradict kind of how we
understand the number um in a similar
way
more like a
function yeah there's it has a value
yeah yeah it's a function that produces
value well again I'm I'm wanting to
distinguish between what would be
thought of as Knowledge from an
empirical basis that would be subject to
something like Bay's
rule versus um knowledge which
essentially is on a purely analytic
basis and that you know given these
assumptions and these methods of proof
that you could arrive at these theorems
or these results and that there's no
ambiguity associated with the truth
value of the results because you didn't
allow for any ambiguity in the truth
value of the inputs or the methodology
of derivation so in in this particular
sense um you know I'm I'm wanting to
sort of say that there's a in principle
aspect associated with this which is
really quite important um and and and so
for those kinds of people that might
think well you know you don't have any
real uh way of saying that about
empirical things like how do you apply
them mathematics to the empirical and
it's like well for example um we can
talk about scientific knowledge as being
causal knowledge about causation and we
could talk about uh the speed of light
being sort of a limit on the rate at
which causation propagates through the
universe and we can basically say that
uh for example like with a black hole
that um the speed of light itself is
essentially part of the thing that
defines what makes a black hole like it
defines a a relationship where once you
go past
a certain distance towards the towards
the black hole past the sort of Event
Horizon that no causal influence from
inside the black hole can affect
anything outside of it now because it's
about the shape of SpaceTime itself it's
about the shape of how causation is
related then we can make the principled
statement that you can't use causation
to disassemble a black hole because in
effect there's a means ends conflict so
in effect I could make the claim that
based upon what our understanding of as
a black hole and our understanding of
what causation is that just on those
ideas alone we can say there is no
technological means to disassemble a
black hole using causation because
technology is the application of
causation just a nickpic just just
quering you um parking radiation with
entangled particles um one inside or a
few inside and a few outside the black
hole doesn't give you powers of
causation to enact stuff happening stuff
to happen inside the black hole by
McKing around with the entangled
particles outside the black
hole correct I mean there's you know
there's there's always going to be you
know quibbles of one sort or another but
yeah in effect it's not like you're G to
have some sort of causal control over
the amount of uh black body radiation or
Hawking radiation that the that this
that this uh gravitational mass is going
to cause to have emission of so so in
effect there's a there's there's a sense
here in which there are limits to what
causation can
do and so in effect what we're what
we're wanting to do is to recognize what
those limits are and how they might
apply in this particular kind of
situation we're talking about artificial
intelligence so in effect there's a
sense in which if I look at what kinds
of control Dynamics would be
necessary to
constrain um highly convergent
evolutionary
process it turns out that the amount of
control that is necessary is strictly
greater than the amount of control
maximally possible so it's a bit like um
like if anybody's familiar with the Bell
theorem for example it's like there's a
there's an inequality that's at the core
of what makes the B theorem itself right
it's it's the essential idea Behind the
B theorem is this is this underlying
notion of a particular inequality that
if violated basically says that the
Universe has a quantum mechan mechanical
nature rather than a classical nature um
and and and and a similar kind of thing
shows up when we look at how control
theory applies to artificial
intelligence so in effect there's a
there's a sense in which we're saying
okay if we look at how would it be um
you know how would any kind of uh
alignment algorithm be implemented like
the mere fact that it's an algorithm
means that it's a causal process and
that if you look at what control theory
itself
would
require then in effect there would be a
situation where um we could say okay we
know how much control we need to have
given how The evolutionary Dynamics
operate and it's kind of like a you know
an amplification that's going on like in
the in the the micro state of you know
how the The evolutionary principle would
operate over time and over space that
there will emerge phenomenology that
would not be predictable in advance um
on any on any basis that control theory
would be able to use to to create
predictions that would be necessary to
create the kind of alignment dynamics
that we've be looking for and it turns
out that if you look at control theory
in detail there's kind of like five or
depending upon how your partition it six
things that would be required and it
turns out that each one of them is
impossible to um in itself to create uh
the level of um control necessary
minimally necessary in order to uh have
some well- defined notion of alignment
at a substrate needs level can we take
this principle to apply to other humans
or is it just other just artificial
intelligence and by extension what about
the future of humans what about humans
who gradually augment themselves let's
say another another 50 to 100 years we
have some something that you could say
is somewhat a transhuman who
us the the first question question is is
does this incapacity to fundamentally
control organic life apply the same way
that it would to artificial intelligence
the answer to that question is yes but
the result that that makes it so that
organic life is aligned with other
organic life is not as a result of
control but it's a result of essentially
the fact that organic life shares um the
same sort of metabolism process and that
in effect because of that because the
substrate of organic life has
commonalities with the substrate of
other organic life that there's an
implied alignment process that emerges
not because of any kind of externally
applied control algorithm or methodology
but because the alignment itself emerges
as a result of uh the commonality of the
metabolic process of organic life itself
and so it's in effect it's it's the fact
that artificial intelligence is based
upon artificial
substrates and those artificial
substrates have different needs and
those needs essentially imply a
different basis of choice and a
different basis of choice implies
different choices different choices
implies different outcomes different
outcomes imply a different influence on
the environment which itself affects the
substrate so in effect there's a
convergence Dynamic that operates both
in the case of organic life and in the
case of artificial intelligent life but
because the artificial intelligence
substrate is fundamentally different
particularly at the level of metabolism
but also at the level of molecular
Constitution that effectively does not
result in a kind of uh ambient emergent
alignment between organic and artificial
and this is the key thing
um artificial intelligence would be
joined with or aligned with other
artificial intelligence to the degree
that they had a common substrate or a
common met metabolic process but not
otherwise and so in this sense if we're
if if we're if we're looking at you know
to what degree would there be alignment
between um organic and inorganic we'd
say definitely
not what degree could we control either
organic or
inorganic via control methodologies we'd
say again definitely not is it the case
that that that that organic aligned with
Organic naturally as the side effect of
its con constition yes would it be the
case that artificial would align with
other other artificial again because of
its Constitution yes that could happen
and is likely
to now we have the question of for say
cyborgs where you have some mixture
between organic and
inorganic and um in in this particular
sense the the issue of alignment becomes
really a very a present one because um
there's a there's a sense in which we
don't necessarily know um over the long
term which is going to dominate the
organic or the inorganic so then we'
start to have to look at what is the
enthalpy of the relationship between the
organic process and the inorganic
process and so in effect is it the case
that uh cyborg type process is even a
stable
metastate and whether or not that
metastate even has endurance and I would
say that actually um based upon what's
been noticed in the face of whether
alignment with artificial intelligence
is impossible and given that that answer
is definitely negative uh I would say
that we therefore have very strong
evidence that um long-term enduring
evolutionary cyborg relationship is
highly
unlikely all right so were you saying
that it's it like um you'll have similar
issues if people start to augment
themselves yes that's
correct yeah that like like the I I
think that to some extent there is the
short-term capacity to implement some
sort of
biocompatibility and to have a uh again
for the short term and by short term I'm
meaning you know maybe a dozen to 100
Years or or something on that interval
but as soon as we start thinking about
you know significant number of
generations like six to 10 Generations
um that over that time there will emerge
a kind of um increasing tension between
the needs of the organic and the needs
of the inorganic and as as result
there'll be a steady but subtle pressure
um one way or the other depending upon
the enthalpy of the interaction and um
based upon what we know of the enthalpy
Dynamics it really favors the Machinery
over the organic
unfortunately it it seems as though it's
going to be very difficult to constrain
people from augmenting themselves if
they've got the freedom to do so how do
we balance the the rights to the people
um giving them freedom to be able to do
what they want to do maybe even for
instrumental reasons because they're
unhealthy at first you know they need
they need a new heart or um you know
they've lost their eyesight that they
can replace their eyes with actual you
know new eyes um technologically
improved eyesight uh or they they they
they're getting old and they're getting
frail but they don't want to get old and
they don't want to get frail they want
to continue on life's journey they've
got a a lot more Pro projects that they
haven't finished and they're not willing
to say goodbye they don't want to what
do we do so I I think we need to
distinguish a few cases here so for
instance um I'm not in any real way
particularly against Prosthetics but one
of the things that we notice with a
prosthetic for example is is that we're
not presupposing that the prosthetic has
agency onto itself and is therefore
operating on its own behalf so you know
you you put your finger on it when we
said something like that people can
choose in a way that's consistent with
their own well-being or their own state
of health or their own their own nature
and to some extent uh to the degree that
the choice is held on the organic side
and continues to be held on the organic
side I don't particularly see a problem
um when when we start to think about um
you know cyborg type things then in
effect what you're basically saying is
okay well what happens um you know in
the future when you have a baby do you
uh preemptively apply Prosthetics to the
baby before the baby has agency of
itself to make a choice as to whether or
not it wants that for itself right I
mean you know to some extent at that
point we're usurping the choice of the
Next Generation uh on the belief that
some sort notion of modernism is served
by um you know replacing human
components with artificial components um
so in effect you know to to the degree
that those artificial components have
agency onto themselves then what is to
say that in the subsequent generation
you know the next one after that that
the percentage of artificial versus
organic is weighted a little bit more
towards the artificial and a little bit
less towards the organic because the
artificial is essentially contributing
its own agency to that choice that is
displacing the choice of the now second
generation and so I'm basically I
believe to some degree there's agency um
in these phones so an extended enactment
of the agency of the Developers the
applications on this phone and the
people within these
organizations that isn't my own right
and comp in your life like in effect the
kinds of things that that you notice
that you're being distracted by or that
becomes an addictive force in your life
or that essentially constrains um your
choices to the menu of options that the
corporation and the developers
essentially allow you to have right and
so in effect there's a kind of
entanglement process where
um they essentially bundle Services
together that some of which you want and
others which you don't want and there's
a kind of extortion that happens as a
result um you don't get to vote with
your feet because the operating system
is tied to every single application and
vice versa and as soon as you want to
make an improvement in one service you
want to sex Force to do it with others
that undermines everything else and so
in effect there's a sense here in which
um we are already seeing some of these
Dynamics play out it's like a dark
mirror episode actually happening and so
in effect I like that show yeah I
personally find it a little too
frightening because I I frightening I
like the the what it I guess gets people
to think about and I don't mind getting
watching a like a horror film movie now
and again so yeah understand so but but
the notion here is is that you know
we're we're being
apprised of significant potential issues
through uh narrative stuff like dark
mirror as hey these are issues that that
deserve careful philosophical attention
like what is the ethics of of this
situation you're asking me how do we
balance the the the rights of
individuals and um essentially the
Dynamics of Corporations and it's
exactly the same question it's like
organic process versus Corporate power
which is a kind of artificial power and
so in effect there's a sense here in
which you know to to to answer questions
like this we have to think about the
relationship between say care
transaction and power and how each of
these shape the modern world um and and
and I want to go back to a question that
you asked a little bit earlier than that
which is you know is there you know some
real sense in which we could encourage
people to make better choices in this in
this space like not compel them but but
say for example as a result of watching
a dark mirror episode you say wow these
are things I'm noticing happening right
now I don't want to get sucked into that
nightmare I want to to make better
choices today so that my children have a
future that I would feel was good for
them and that they would feel was good
for themselves when they became old
enough to know the difference right like
you know if I if I love my children I'm
gonna really want to pay attention to
what's genuinely loving and healthy for
them so that they can make healthier
wiser choices even if I don't know what
those choices should be at least I can
trust that the choices that they're
making are truly wise and healthy so
that maybe the generation after them has
a chance right right we don't
necessarily want to take away from our
children's agency but I get the sense
that we want to constrain it in such a
way that will allow them to develop it
further on like I don't want to give
children agency to to play on train
tracks or you know I play football in
the middle of a freeway or something
like that I don't want them to to hurt
themselves and and there's another
there's another thing of this like I
mean we can use technology to reduce the
likelihood of occurrences of cystic
fibrosis and stuff like that so we've
got sematic and uh germline gene therapy
that could be I guess agency promoting
meaning that they don't need to spend as
much time dealing with unchosen ill
health let unpack that okay so about the
child playing on the train tracks or in
the street right we both know that they
do not yet have enough wisdom
to make good choices in that space so as
adults we apply constraints but the
thing is is that we know that eventually
will pass and our lives will be over and
their lives will will be continuing and
so as a result our hope is to teach them
enough wisdom to somehow convey to them
the kinds of capacities so that they can
make better choices for themselves
healthier and wiser choic for themselves
because eventually right if we if we do
succeed in creating not taking away
their choices but adding
wisdom that we will eventually trust
that they will make better choices like
when they're in grade school or high
school yeah we're going to need to set
some limits right but the the limits are
only for the sake of creating enough
time so that they live long enough to
have the chance to take on the wisdom
but for example if if you were talking
to a
25-year-old or you know maybe someone in
their 30s for example and you said hey
do you want to play on the tracks you
trust that they would probably say no I
don't really want to do that it doesn't
feel safe to me right and at that point
I'm not applying limits I don't need to
the limits are are not even something
that's coming from the outside they are
making a better choice and so if we
trusted that there was adequate wisdom
in society in the the broad scale of
things then the question about you know
will people make better choices ansers
itself so in effect I'm not thinking so
much around the ideas of how do we use
technology or modernism or system or
governance or things like that to create
Tighter and Tighter more and more
perfect systems of constraint to prevent
people from doing bad
things and to create the capacity to
have care to be skillful in how that
care is
held and to be able to have enough
presence and skillfulness and care that
that all three of those together create
uh the bonds of trust that hold us
together as communities and as life and
that in effect it's it's how we add
those capacities and how we create that
between each other that allows us to
move past questions of constraints and
more into the questions of uh
appropriate levels of Freedom there's a
few things which came up for me there
one of them is this I'm glad you said 25
or 30 because there's research out there
to say that human brains aren't quite
developed especially the sense of um
cause and effect until 25 right but we
still give licenses to 18 year olds or
in some countries 17 years old to uh
drive cars and the other thing is that
came up was well evolution is a slow
process but it does happen over long
time scales with the technology
surrounding us and and the world which
is increasingly becoming more
technological and there's we will evolve
given enough time if we are still around
around our technology do you trust
selection pressures of blind natural
selection to evolve Us in the Right Way
Forward do you think it will will end up
as better people through natural
selection given a long period of time of
evolution ahead of us yeah I do but it
it's got to be enough time right because
we're the thing about technology is that
you know it's very recent on an
evolutionary time scale right basically
the last 5,000 years is you know when
we're talking about changes in human
process right we we move to the city
state and you know we've we've in a
sense made it more and more complex in
life right and so in effect you know
Evolution has you know it's it's moving
relatively slowly compared to the PACE
at which we've taken on the the the the
the the whole package associated with
what we might call modernism and so in
effect yes in enough time we we we can
certainly learn to adapt to that and
then trust the adaptations um but but we
do need to in a sense understand how to
get to that longer term perspective
right because in the same sort of way
that that that we both agree that yeah
we've we've seen enough research to say
that brains are still developing
throughout the human life and that the
level of wisdom adequate to the kinds of
Power associated with some forms of
technology genuinely suggests that you
know frankly we should contrain
constrain the use of that technology
until Evolution has had enough time to
catch up either individually in terms of
our personal growth and learning um or
collectively as a species and that you
know in effect a big part of the firmy
Paradox is associated with this right
which is you know is it the case that
technological civilizations don't last
very
long and you know there's other ways to
think about the firmy Paradox but this
is the main class of thinking about the
firmy Paradox where we still have the
capacity to choose a future course where
that low probability of longevity
associated with technological
civilization could maybe be
aurated that maybe that it's not
terminal for uh civilizations that have
technology that there's some um leap
that we can make some conscious Choice
process that we could make that would
supplement The evolutionary
process to buy us enough time to reach
the point at which evolutionary process
could catch up and so in effect what I
what I basically am suggesting is is
that to a large extent we need to do
something in place of evolution in the
near term like for instance if we're
saying evolution is going to take you
know another 5 to 10,000 years to create
the adaptations so that we just
naturally innately have the wisdom
necessary then that means we need to
consciously take in the wisdom necessary
through our Collective governance
process to have that be a supplement for
the wisdom that Evolution does not yet
have and so in effect in this sense what
I what I find myself doing is is saying
okay here's a way to consciously
understand the ethics behind Choice
itself here's a way to consciously
understand the Dynamics of choice as an
ontological reality and how that's
compatible with causation and change the
way we understand them ordinarily but
does require us to to extend our
thinking in these fields a little bit in
order to recognize what that
compatibility actually requires of us in
terms of responses that we need to give
as a species and so in this particular
sense yes I believe in evolution but I
believe even more in choice
and so in this particular sense I'm
therefore emphasizing Choice um
individually and collectively as a skill
that we need to develop particularly
because um if you think about it uh
there's a kind of threshold phenomena
associated with technology it's like a
it's like there's an absolute level that
you need to be at least this smart in
order for you to understand causation
and if if you don't pass that threshold
technology just doesn't develop but as
soon as you do then technology has this
sort of self-perpetuating aspect it has
this accumulative aspect that creates a
kind of you know geometric growth as
we've seen like a kind of exponent on
the growth curve associated with
technological process and so people are
looking at this exponent and saying wow
human Minds aren't very good at thinking
about exponential process in fact
they're fairly good at thinking about
multiplicative process most of us are
still operating in a consciousness which
is additive we're very good at thinking
in terms of quantity in an additive and
subtractive sense and a few of us has
figured out how to think in terms of
ratios and multiplicative process and
those people tend to become uh Finance
Specialists or uh Wall Street Traders
and things like that and so in effect
your your entrepreneurial class is
essentially starting to say hey this
exponent stuff is really cool let's
figure out a way to do that but every
single category of existential risk is
associated with
exponential self- recursion type curves
and artificial intelligence is no
exception to that and so in effect part
of the thinking about existential risk
including that associated with
artificial intelligence is learning how
to deal with exponential curves and
you'll notice that if our if our growth
in intelligence is occurring on a linear
process but the growth of technology is
uring on an exponential process and it's
not very long before the exponential
process outstrips the linear one and so
in effect there's a sense here in which
as soon as we cross the thresold as a
species where we could do technology
then we became literally the dumbest
species possible with which to have
things like nulear technology like like
we still have Neolithic brains but now
uh Godlike Powers with things like
nuclear and biotech and and um you know
artificial intelligence all the rest of
it we literally are as life forms in
evolutionary time scales we humans have
really only just evolved the ability to
think smart to have this sort of sense
where you can develop this these types
of Technologies and in a sense we've had
to scaffold civilization to enable them
as well uh Evolution has had a long
history it's fault tolerant in a sense
it's evolved it's it's been working and
it's uh but but the this new form of
intelligence this causitive power that
we have um is only new on the scene now
it's very powerful but it could also be
very brittle right um and you know could
um end up uh causing our own Doom in a
sense we we could develop the technology
which could kill us right yes so what's
the missing piece I'd also be interested
to know where you think Choice exists in
the time scale of evolution where that
first came from do you think it only
exists in humans or does it exist in
other agents animals well so so first of
all I need to to back up and make a
distinction because you're asking if
Choice exists and technically speaking
Choice does not exist there's no such
thing as free will but choice is
real free will is not real but choice is
real and to say that something is real
is to make a different claim than to say
that something exists and is also again
just for completeness sake is a
different claim than to say that
something is objective so science for
example um has the requirement that
something must be observable in an
objective way and so in effect to to to
to to ask is Choice objective well it
turns out that that's also not true
right that that that you don't have
objective choice and you don't have
existing choice but you do have a
reality to
choice and and basically to say um to
talk about the real is actually to talk
about something on a more Primal level a
less and non-contingent way like
existence is
contingent it's it's contingent upon a
subject object relationship or it's
contingent Upon A a um you know content
context relationship or contingent Upon
A measurability of some sort right but
when we're talking about the the the the
subject object relationship as the
relationship itself being more Primal
than either the subject or the object
then in fact the notion of agency that
we would normally associate with the
subjective becomes an aspect of the
interaction between the subjective and
the objective which is more Primal than
even the subjective and the notion of
choice lives in the
interactiveness it's more correct to say
that choice has self than it is to say
that self has choice in a certain sense
choice is more Primal it's more ological
and is more deeply tied to the notion of
epistemology and axiology than um then
than would obviously be the case if we
were to try to treat Choice as a say an
epiphenomenalism of of brains or of
animal Consciousness in particular so
you know in effect and this again these
these get into some very subtle aspects
of philosophy and of of metaphysics
again you had a great conversation with
Daniel schmon Burger about this that
I'll that I'll put as as a link in um
the description of this video think it
was one of the neuro haacker Collective
conversations definitely worth a listen
I'm still trying to gr I'm still trying
to digest this it it's hard to
understand if you don't have the right
backg I'm not a I'm not a philosopher at
least I I never studied it I'm very
interested in it but this is difficult
this era to to understand probably the
most misunderstood areas of your views
that are I think people find it hard to
Grapple with if I were to answer the
question then the sense of do animals
have Choice do they make choices I would
just say yes right um it's it's and and
and and that that notion of choice
emerges um as as a as a kind of aspect
of the real like literally from
foundation so in effect the you know
like we we could we could posit for
example that that the notion of choice
in relation to change in causation is is
a very Primal way of understanding the
universe um much the same way we could
talk about creation existence and
interaction is a very Primal way of
understanding the universe so so in this
sense um if we if we take in the the
Deep nature of choice one of the things
that we notice is that it is inherently
cooperative and it is inherently
unobservable even though it is
real right so so time for example is not
really an object of observation but it
does have a notion of realness
associated with it at least in the first
person obviously not in the third person
but in the first person there is a
temporal sense that that that we
experience reality as as essentially
there is a definiteness to the now that
is distinguishable from the past and the
future although we have no real way to
to assert that from a third person point
of view we experience it from a first
person point of view similarly we
distinguish between here and there and
what has happened versus what could have
happened right the the alternate
possible U events that that that that
that could have been if for example
something else had happened the the
explicit
contrapositive um you know events
distinguished from the events that did
happen so so in this sense when we're
looking at you know a very deep
understanding of the nature of choice
we're seeing that it is essential for us
to account for particularly when
thinking about ethical issues associated
with how agency interacts with other
agency right because the notion of
Ethics essentially is the study of the
principles of effective choice so in
effect for there to even be a notion of
Ethics or of alignment or of of of any
kind of uh safety in the sense of
preserving the well-being I.E life
liberty and a Pursuit of Happiness of
any particular agency then in effect the
notion of choice is intrinsic to the
very notion of Ethics itself in order to
be able to address questions like this
and in fact I don't think it's really
possible to understand ethics again
distinguish from morality without
understanding the nature of choice and
also the nature of goodness like what do
we mean when we say that something is
healthier than something else what do
that actually ground in as a as a
reality in terms of you know what is it
that makes life life and what is it that
makes meaning meaningful and what is it
that makes sacred sacred that there's a
kind of uh underlying topology of these
ideas
that allow us to come to a kind of
definite knowledge about things like
again whether or not the notion of
artificial intelligence can ever be made
safe for organic process and again I
assert just on the basis of what I
consider to be a clear thinking or a
kind of mathematical thinking that just
says uh definitely not and it's knowable
to any person that knows how to think
about
this definitely wow that's interesting
so you're you're saying that ethics um
is the reservation of choice or agency
you've also said once that uh well
probably many times that love is that
which enables choice however um other
people have the view that ethics is all
about increasing well-being and reducing
unnecessary suffering or you know um
abiding by rules or exemplifying a
particular virtue so there's varieties
of different ethics out there that
people who are listening and are
probably aware of where would you sit
would you set an established well-known
um view of the ethics or do you have
something more subtle that you'd like to
clarify well I I can kind of position
the way I think about ethics in relation
to the kinds of ways that you were just
naming so for example um in in in when
we say um ethics is the study of or the
practice of the principles of effective
choice and by effective we mean goodness
and in this sense um as you mentioned
goodness can be thought of as
preservation of choice capacity or it
could be thought of as um maintaining of
health and vitality and and that is a
notion of goodness um it can also be
thought of as a notion of integrity and
I so sounds to me like consequentialism
in in the sense that the consequences of
choices are the good the good
consequences of the
choices so let me let me position it
like I'm just I'm just laying out the
framework to just even answer that
question so for
example if we were to basically say the
principles of effective choice we're
talking about the principles not the
outcome okay the outcome is obviously
we're saying the notion of goodness is
attached to the notion of say Integrity
but Integrity is a process it's not an
outcome like I can preserve Integrity I
can work in a way that maintains
homeostasis or capacity for choice or
capacity to perceive blind spots or or
you know some notion of virtue right but
the but the notion of principles and
practice is a dynamic one and so in
effect when we're when we're looking at
this um I'm not looking at the the the
the choices in terms of their outcome
but the choices in terms of their
process and the degree to which those
processes are uh implementing those
principles and so in effect we can we
can talk about things like uh
consequentialism as being um you know
something that is uh something that
becomes possible once you have started
or or come to some grounding some
non-relativistic grounding of of of this
uh notion of goodness or this notion of
Integrity or this notion of health or
well-being or of or of choice itself and
how those
combine um the ethics that I'm I'm
describing is a process ethics and not
an outcome ethics so it's not a
consequentialist ethics but as a process
ethics it's about you know things that
could be thought of as compatible with
Integrity so when you're talking about
health and vitality or preserved choice
or you know well-being or or those kinds
of things those are all in one sense or
another um parallel manifestations of of
Integrity um in effect I think of a
consequentialist ethics or a um you know
a rules-based eth ethics as themselves
resting on a non-relativistic ethics but
that's not always obvious so for
instance like a consequentialist ethics
is basically saying something like um do
what creates good outcomes but the
notion of good outcomes or uh what would
be virtue in that particular sense or
utility right a lot of consequentialist
Ethics are
utilitarian um they don't have a way of
grounding the notion of goodness or of
utility so in effect somewhere along the
way you do end up coming back to or or
needing to ground a utilitarian ethics
in a non-utilitarian
way in order for the utilitarian ethics
to even make sense similarly for any
kind of rules-based system that
somewhere along the way you have to say
where do the rules come from what sets
the the rules and the rules like any
kind of um system that's based upon
rules I would call it moral code and
moral codes are basically rules that
make sense within a particular context
within a particular world so for example
in the world of email um there's a kind
of rule like don't send an email in all
capital letters because it makes it
pretty hard to read at the other end
like is Polish or polish the word
intended well you don't know because
every word is
capitalized and normally the way we
would tell the difference between a
country and something you apply to your
shoes would depend upon a capitalization
of the first letter which of this course
I can't distinguish I also can't
distinguish the emotionality associated
with the letter because all the
sensibility is gone because everything's
capitalized so in that sense there's a
reasonable rule which is don't
capitalize everything unless you don't
have any choice and there's a sense in
which for every world there's a set of
rules which are projections or
translations of the
underlying non-relativistic ethics as
could best be applied in that
world so in this sense um yeah it's not
a it's not a denominational ethics it's
not a utilitarian ethics it's not
something defined in terms of outcomes
it's defined purely in terms of what is
the nature of choice and what is the
basis of the notion of value and it
turns out that value purpose and meaning
are an entang they're entangled Concepts
they're part of what I would call a
triple they're distinct
Inseparable non-interchangeable Concepts
and it turns out that meaningfulness is
the most basic concept so in effect if I
want to know what value is I have to
understand it in terms of meaningfulness
and this is where we get back to process
Dynamics like what is the the dynamic of
the relationship because the notion of
meaningfulness can itself only be
grounded um in the notion of the
relationality between subjective and
objective it's not something that exists
purely in the subjective nor purely in
the objective it it exists in a
relational way and and a very specific
kind of relational process and so in
effect there's a there's an element of
the realness of the subject object
relationship begets Notions of Integrity
begets Notions of choice begets Notions
of goodness uh indirectly through the
coupling of meaningfulness to Value to
what we would call you know right values
um and and from there up into into
things that might be thought of as
consequentialist but but without
understanding the grounding of this
particular process uh then particularly
in high criticality situations such as
existential risk um it becomes really
important to be super clear about the
derivation methodologies of the
non-relativistic ethics or frankly are
likely to make World destroying choices
so um in this particular case uh I find
myself in a uh somewhat unusual position
of starting off as a kind of
metaphysicist interested in the ground
of reality the ground of what is the
nature of the real and from there
finding that it's it's its main area of
application is not just philosophy but
critically important philosophy having
to do with things like technology
development and civilization and things
like the firmy
Paradox I'm interested in this because
um it seems to relate to the idea that
there's something real out there that
you can measure a utilitarian or a
consequentialist might say well you can
measure the signatures of suffering or
the signatures of pleasure or the
signatures relating to sensitivity to
novelty and I'm not sure about choic
making would you consider yourself a
moral realist or an anti-realist or
somewhere in between does that even is
it a concept they even make sense it
doesn't really make sense I think the
closest analog would be to say that I'm
a non-d FL philosopher so there's
dualism would effectively sort of start
with it goes back to Dart for example
but but the the notion of realism and
the notion of idealism are basically
kind of two main currents of philosophy
you can think of uh idealism is showing
up in things like Christianity where uh
the universe is thought of being an
emanation of God um or of uh say
Hinduism or Buddhism or things like that
where the the notion of mind and
Consciousness is taken as primary and
phenomenal uh existential reality is
sort of thought of as a epip phenomena
of Consciousness whereas realism would
hold it the other way around that you
have material reality Atomic or
particles sub subatomic physics and that
uh out of the mathematics of those
interactions uh that emerg things like
chemistry and bodies and brains and
therefore minds and that Consciousness
uh in far as it has any notion of
realness at all it's thought of as an EP
phenomenon of physical reality whereas
my point of view um is is a non-dualist
philosophy which does have a certain
amount of historical precedent but um I
I think of my version of it is
essentially being a from scratch
derivation which arrives at the same
sort of uh thinking that historical
philosophers and the non-dualist
tradition have already arrived at but um
looking at it from a from scratch sort
of perspective uh gives Clarity on
things like the non-relativistic ethics
which turn out to be really important
for addressing issues associated with
say modernism as a philosophy in in
particular but in effect um I'm neither
a realist nor an idealist I am something
which
is between and beneath those so in
effect the the way in which I think of
realness has to do with the relationship
between the subjective and the objective
as being the primary basis rather than
saying something like the subjective
emerges from the objective which is
realism or that the objective emerges
from the subjective which would be
idealism I'm basically saying that the
relationship between the subjective and
the objective is the primary basis of
the notion of real out of which both
reality and Consciousness are themselves
emergent phenomena okay so this is this
is what you mean by real and how it's
different from exists so exist maybe you
would be a moral realist if if the word
real meant what you meant right um by
the word real and it's a relationship
between the subjective and the objective
but what most people think is moral
realism they're really moral
objectivists in a sense yeah yeah and so
in effect there's there's there's a
notion here that um if we were to make
the update associated with the
difference between to to exist to be
real and to be
objective and that we were to hold that
in mind like we were to have clear
thinking about that and then in addition
to this uh or as part of this we were to
distinguish between morality and
ethics then you could assert that I am
an ethical realist meaning that I hold
that there are definite principles to
ethics the study of the principles of
effective Choice does Converge on
two precisely specifiable
principles from which we can think
clearly about what does good choice mean
in each context in which the notion of
choice itself uh would be relevant and
the the notion of real being entangled
uh the notion of real itself basically
being directly coupled to the notion of
choice means that everywhere the notion
of real applies the notion of choice
applies and the notion of Ethics applies
and therefore the notion of
conscientious choosing or um choosing in
a way that is consistent with the ethics
uh is therefore a coherent notion a
coherent concept and if we translate
those coherent Concepts into terms that
are uh easier to understand in a
particular domain or in a particular
World such that the her istics of those
translations make it simpler or easier
that uh in the default case people know
what to do um then in effect you could
say that I'm something like a moral
realist but but I honestly find that the
worlds change and that that the the
objective World change the change is a
phenomenology just as much as causation
is and just as much as choice is and
that therefore uh no static set of rules
is going to apply in perpetuity that in
effect um you know when the rule start
breaking down when the usual techniques
of civilization no longer solve the
kinds of problems that we're faced with
that in effect we need to go back to
First principles and derive new
practices so that we can address the
problems that we're actually faced with
so for example we can notice things like
as I mentioned inequality pollution um
issues associated with existential risks
issues associated with capitalism uh
species loss all these kinds of things
are uh complex
long-term
multi-stakeholder
multi-domain um you know hard to
understand issues hard to speak about
issues there's no uh bullet point or you
know speaking speaking engagement things
that that a politician can really do to
adequately grapple with these kinds of
Concepts um you know government systems
as they exist in hierarchal and
transaction forms you know procedural
forms um really just doesn't have the
bandwidth necessary to even really
understand the nature of the problem
well enough to make good choices in
these spaces so as a result we need to
have a deeper understanding of choice
process a deeper understanding of
Consciousness process and ethics process
to create the kinds of practices that
would Implement wise choices in these
spaces where uh the usual methodologies
is just won't work it's not to say that
the usual methodologies aren't great for
dealing with the kinds of things for
which um you know single stakeholder or
single domain or relatively uh
straightforward issues that operate on
relatively short time scales you know
for those kinds of things current
methodologies work really well but you
know civilization and the change of the
world presents us with problems like you
know each year we get a new crop of
problems and and we notice that our
technique may work most of them but it
leaves a few that it doesn't address
which accumulate from year to year and
become more and more severe until we
enter the Lial space and actually are
willing to be um naked with the truth of
the reality of what is have enough
humility to recognize what are the
principles that we need to translate
into practices so that we can actually
endure as a species uh in this case
particularly can we measure what we need
okay so it seems as though there's the
referrent and the referrals the referral
um being the thing which is doing the
referring and the referrent being the
object which is being referred to right
so we have reference which is the action
of connecting the referral to the
referrence so there triple of referral
can the reference be measured outside of
the minds of humans or the you know the
writing or the I guess the externalized
so this is a very subtle question and
I'll answer it and I'll give you an
exact answer so to to understand the
question right we need to recognize that
there's a relationship between
referencing as a process the referer and
the referrent or in another language the
perceiver the perceived and the
perceiving so first of all would you um
would you would you admit or recognize
that the the refer the refer to and the
referring is essentially the same the
same construct as the perceiver the
perceived and the
perceiving my I guess without having a
long time to think about it I'd say yes
it sounds very similar i' I'd say the
difference is I mean everything exists
in material reality I believe that
assumption that's essentialism so so in
effect what I can do is is I can work
with you within the perspective that you
have but doing that I'm going to
construct another perspective
within which I can answer the question
which will make sense to you based upon
the nature of the question so so in
effect rather than than than than making
any other assumptions we can just
basically both of us agree that there is
a refer a refer to and a referring and
we could just leave it at that like we
don't have to make any other assumptions
to be able to answer the question that
you're asking but just for the sake of
convenience um I'm making a an analogy
to perceiver Perce and perceiving and
I'll leave it to you as an exercise that
you can think of on the future as to
whether you accept that there is a
strict isomorphism between those two
triples so y right all right so in
effect I am going to notice that for
each triple in the same way that there
is a distinct Inseparable and
non-interchangeable aspect like the
refer is not the referrence and the
referring is not the refer or the
referrence and that in the same way the
content is not the context that the
relationship between content and context
is its own entity neither content nor
context the perceiver and the perceiving
are distinct the P the perceptual
process is distinct refer and referring
and refer are distinct but they're
Inseparable right you don't have any one
of them without the other two so we're
noticing a little pattern here right
yeah now but it does seem to me as
though the the process um is actually
existing in the mind of the
referer right so so you can it may not
be part of the referr but it seems to be
there or in it's part of it's a
computation right well I wouldn't go
that far I would actually say that
without some relevance to the outside
world like like the the notion of
referring creates a kind of Bridge to
something outside of the self right like
anytime we're making a measurement a
measurement is a process but it has both
a subjective and objective component in
fact this is the same reason I was
talking about perceiver receiving
because you used the word measurement
right like measurement is a process that
connects together something objective
and something subjective and if there's
no correspondence between the objective
and the subjective then the notion of
measurement hasn't happened
yet so I'd say it's both right like like
if there's no objectivity associated
with the measurement then yeah it's just
an idea but not a measurement and if
there's you know data collected but it's
sitting in a database somewhere another
nobody's ever looked at it it's not a
measurement yet it's got to have
relevance it's got to connect back to
something the notion of of data becomes
information and information becomes
relevance and so in effect when we're
when we're looking at this all all I'm
really doing is just basically saying
that there is an objective aspect
there's a subjective aspect and there's
a relationship between them and that
that's all that we need to answer your
question about is it the case that
anything about the nature of what we
would call ethical process is measurable
because that's the question you ask
right so so in this sense if I'm
unpacking it what I'll notice for
example is that it's a bit like
asking like for perception for example
the reason why I go to the perception on
is because it leads directly to a way of
answering the question which is um say I
have a ray of light that that is like a
laser beam that's Crossing in front of
my eyes
does that you know like like it's it's
just passing in front of me I won't see
the laser beam because there's nothing
there's nothing for me to see the light
that's going from the screen to my
eyes passes right through the laser
beam right so I can make a measurement
of the screen and the laser can be maybe
measured by some sensor that's you know
measuring the energy of the laser for
example and those two measurements cross
one another right light has this
capacity to have
superposition so in this particular
sense the the light passes through each
other which basically means that I can't
perceive if perception is based upon
light it's based upon some
electromagnetic phenomena then I can't
perceive
perceiving I can I can only perceive
that which is
perceivable which is the
object I can't measure the
measurement nor can I measure the
measurer so the
subjective and the process of
measurement itself are both
non-measurable only that which is
measurable is measurable which is the
object right so this this is interesting
because I've thought about something
similar and it's the idea that at some
stage we'll have the instruments that um
are good enough that that can get a a
high enough resolution and a de enough
sort of frame rate to be able to measure
the physical process of whatever is
going on in our brain to give a real
time sort of movie 3D movie like you
know at a billion frames per millisecond
or whatever and and at such a high um
pixel like pixel density like you know
plank scale pixel density or something
like that just it sounds magical maybe
it is I don't know but just imagine we
had that sort of capability
would we be able to measure the
measuring no okay categorically right
because there's a there's a category
mistake in how we're thinking about the
question and so what I want to do is I
want to kind of reveal that because no
matter how perfect our measurement
process would be like say for example
you know through some uh future advanced
technology that somehow um you know
could could sort of factor out the
quantum noise associated with plank
limits and all the particle interaction
and was able to basically see and
measure uh every single neuron in the
brain and the uh you know synaptic
process going on throughout the entire
structure like we we in that sense we
have a simulated brain right and in that
particular case um you know a lot of
people would would be interested in that
because they'd be basically saying
things like wow look at all of the
neural
correlations between um you know a
subjective state of consciousness
and this pattern of of process occurring
at a neurological level okay now you
know that's all great and and there's
already been a lot of work in this space
identifying neural
correlates right but the question you're
asking is does the measurement of neural
corelates allow me to perceive the
perceiver and to perceive the
perceiving and the answer to that
question is actually still no and the
reason is because in effect
you still only have third person
information you still only have a third
person perspective you say there's a
correlation from an outside point of
view I I hear a correlation between what
this person says about their experience
and what I observe as a neurological
process right but out of that you're
still not going to be able to
construct a firstperson sense of why the
Pres is distinct from the past and the
future a lot of people who are working
on uh what would be called the problem
of Consciousness like this is a famous
debate in philosophy like how how do we
how do we emerge Consciousness which is
essentially a first- person perspective
from a third person inanimate
matter and I'm GNA basically just say
well you can't I mean that just doesn't
work that way right like I can go from
first person to third person like I can
take measurements and I can turn them
into through a kind of abstraction
process a body of causal knowledge you
know expressed in the language of
mathematics that basically suggests that
you know we can predict these particular
events given these particular
antecedence like we can say given these
conditions we'll see these
outcomes but that abstraction process
itself inherently is atemporal it it the
the third person perspective is
atemporal the hard problem of
Consciousness is not it doesn't have
anything to do with the neural correlat
it has to do with the difference between
how time is handled in a first person
perspective versus how it's handled in a
third person perspective you can't
derive a first person locality in time
from a third person perspective which is
completely uh has no temporal uh d
Direction at all right the the third
person perspective is
atemporal and at best I can treat time
as a dimension of space without any
particular Locus without any particular
orientation so in effect you know if I
if I'm looking at this from a really
really deep physics point of view the
causal methodology will not give me the
ability to account for the notion of
locality in time and space and
possibility which is the central no that
makes the notion of choice
meaningful so in this particular sense
it it it won't matter how perfect your
measurement apparatus is for one thing
it's never going to undo the Heisenberg
uncertainty principle and also it's not
ever going to really undo the notion of
abstraction having to basically be
atemporal and the fact of Consciousness
having to be temporal and so in effect
though the way in which you get from the
third person to the first person is of
an entirely different kind of thinking
entirely different kind of process than
anything that can be held purely within
a third person perspective and in order
to understand that you need to
understand the non-u philosophy pretty
thoroughly or the non-u metaphysics
pretty thoroughly but I can at least say
that in answer to your question
measurement itself as a process although
it is grounded in the real will not
transcend the OB the the object of
existence you can't use um process to
essentially overcome
process it's a brain measuring itself I
mean like it's the we as conscious
agents apparently um and I forget the
the name of the experiment there's been
experiments done
where um The Observer the person is only
aware of a particular choice like
milliseconds later there's a delay there
so is it the the person being aware of
of the the brain's Choice that's
important or is it the actual Choice
mechanism that the brain's making a
where the the person then later becomes
conscious of it so so so I know about
that experiment but but but the thing
that we have to get past is the notion
of choice being a process it's it's it's
it's it's it when we say mechanism of
choice we're still assuming the
causation is a way by which we can
understand choice and that's just not
true so in effect you know if you're
looking at the experiment where they're
they're basically saying okay we see
these neuroc correlates and then there's
you know 500 milliseconds later the
awareness in the person consciously that
such and such is happening in the world
and then there is like instinct or
something and they respond right but the
fact of those antecedants preceding the
moment at which they themselves
recognize Consciousness is is to already
presume an objective view of time an
outside in perspective a notion of
measurement and and whatever the
correlations are is created as a result
of sliding the window back and forth
with which the measurement is occurring
so for instance if I basically say I'm
going to trigger that um you know I'll
have recorded whatever happened previous
to this moment every time I I hit a
button for example then yeah I'm going
to see antecedent process prior to my
hitting the button but the but the fact
of taking the measurements the recording
the the chart the EG chart for example
and or the neural firing pattern chart
for for example and I line them up on
the basis of a subjective
interval right I've now created a
correspondence between all of those
charts the fact that there is a there's
a there's a series of neuroc correlates
that occur before and a series of neuro
correlates that occur after that are
always lined up on that same thing
that's actually presupposing the thing
you're trying to prove
because you can't you can't justify
where the alignment itself came from
right like the the fact of the person
saying hey I subjectively perceive now
as now and I'm going to use this as a
triggering mechanism to create an
alignment within the third person
perspective and then I'm going to try to
use that third person perspective to
explain what's happening in the first-
person perspective well again that's
just a way of trying to Short Circuit
getting from a
a temporal basis to a localized temporal
basis and again that's just begging the
question it's not actually answering
anything the the experiments are
interesting they are great for
identifying yet more neural corelates
but it doesn't actually answer the
underlying
question so would you say the feeling of
recognizing a brain's conscious decision
is a third person perspective no
is it first
person it's certainly more akin to first
person I would say technically it might
be second person but this is where
things start getting really nuanced and
we'd have to construct the language as
to what do we mean when we say feeling
and be exact about
that yeah so when we recognize so so
when we we become conscious that a brain
has sort of um come at a certain state
is after the brain has come at that
certain state is that
yeah what kind of perspective is that is
that second language the language of
brains and the language of states is
already baked into a third person
perspective so in effect again I'm I'm
operating if I if I'm starting within an
existential frame I'm already using
causal methodology of thinking about it
and so in effect the the trouble here is
is that we can't construct a notion of
choice using only causation as a basis
any more than we can construct time out
of
timelessness like how do I take a a
perfect
Symmetry and from that alone create a
perfect
asymmetry how do I get asymmetry from
symmetry in physics it shows up as the
Symmetry breaking question and so in
effect you know it's not like physics
has solved this problem it's just
basically said hey there are places
where there is an asymmetry in the
universe and we see it right more matter
than antimatter um certain things having
to do with time charge and and and
things like that parody right like so in
effect there's a there's a there's a
handful of places where we notice an
asymmetry in the universe as it
is
and there isn't really an accounting for
where that asymmetry came from it's not
going to emerge out of the mathematics
the mathematics is
symmetric so in the same sort of way
that we don't have localization out of
globalization we have to in a sense
recognize that we're not going to get
Choice out of causation we're not going
to get Consciousness out of inanimate
matter and so and that's not to say that
Consciousness isn't real but it is to
say that it doesn't exist in the way
that we think of EX of of of things
existing well the processes exist no
process is
real okay cool mechanics mechanical
process if you're saying
mechanical then that exists but you know
obviously it has to be something that is
made out of matter and energy and
pattern that we can we can measure like
if it if it's measurable then we can
talk about it as an existing
thing measurement itself is a process
do so we can't measure processes we can
measure frame measure we can measure
aspects of processes but only some
aspects not all aspects okay right like
for instance um built into the nature of
reality is is is is this thing we we
have a mathematical model called quantum
mechanics but the action of
observation has a uh a a change like
while the evolution of the uh Shoring
equation or the dra equation or
whichever model of quantum mechanics
using the the the evolution of the wave
state is
deterministic and out of that
determinism we basically can construct a
probability about what an observation
might be but when we make the
observation something outside of quantum
mechanics is occurring right it's called
the the measurement problem in quantum
mechanics and then we basically say well
what the heck is that and there's all
sorts of interpretations there's a
Quantum uh mechanical interpretation
called The Copenhagen interpretation and
there's many worlds interpretation and
man there's a whole bunch right but all
of them are different ways of
reconciling the notion of measurement as
a process that is occurring in s in some
sense in some definite sense outside of
the evolution of the wave function
itself so it's a kind of meta process
and that meta process is
non-deterministic in effect the
Heisenberg on certain principle is not a
result of measurement pertubation
meaning that there's some definite State
of Affairs it's basically saying that
the notion of there being a definite
State of Affairs doesn't make conceptual
sense and that in effect there's a
there's a kind of indefinite built into
the microscopic state of the world such
that even if I had perfect knowledge of
all prior states of the entire universe
it wouldn't be able to predict the next
state of this uh entangled system like
if I have two particles entangled with
one another that even perfected
knowledge of the entire wave function
and an existential state of the entire
universe prior to that moment still does
not give me the ability to predict what
happens in the entanglement uh in the
next moment so even a l and Demon would
not be able to make predictions about
the future that's correct so in effect
there's a sense in which the notion of
definiteness at the um you know sub
plank scale is not a coherent concept
and and you know there are people that
try to talk about things in the sense of
super determinism but in effect that's
that's that's also begging the question
because effectively superdeterminism is
just an assertion of if we assume that a
third person perspective of the univers
is is completely consistent um
what would we be able to predict under
those circumstances and of course well
if you're if you're assuming uh perfect
superdeterminism then of course you can
make predictions but what we actually
observe doesn't isn't consistent with
that and so in effect there's a there's
a sense here that hard Randomness is a
feature of reality itself and therefore
reality is different in kind than
existence but that's not the only way to
understand it it's just it's just one
way to understand that there's there's
all sorts of ways to construct these
Concepts but the net effect ends up
being the
same what of what you've said so far do
you think is the most misunderstood by
your peers who work
with I think that um so many people have
gotten so used to the sheer power of
causal thinking to solve such a broad
class of important problems that there's
a supposition that it's a tool that's
able to solve all problems and so as a
result I think people uh don't recognize
the degree to which uh that results in a
kind of bias a kind of what I would call
omnicient bias that because these tools
have worked so well for so long um that
they can work indefinitely forever for
all problems it's like you know
something that works 99% of the time
sure it lead to the expectation that for
that last % that it would work great but
that's just not true and so it leads to
a bias that well there's got to be a way
to make it work for this last exception
case like it's worked so well before why
can't I just extend that trend line well
the problem is is that you know the
universe has exponential features built
into it and using linear process to try
to predict exponential outcomes yeah it
looks linear but it ain't as soon as you
get to the knee in that curve it goes
off in a different direction and You'
got to worry about that other Asm Po and
and and so in effect there's a sense
here in which I think people are so used
to coming from a a realism perspective a
a perspective that third person
perspectives like mathematics itself is
the Beall final intellectual Tool uh
that intellect itself can solve all
problems and it's like well actually to
some degree um you know I can see why
that has been such a compelling argument
and such a compelling thing but honestly
you know one tool isn't going to solve
all problems and and and to believe that
it can is essentially to be making a
mistake um you know and and this shows
up at so many layers and so many things
that yeah it does make this philosophy
genuinely hard to understand like even
the questions you're asking me you know
I'm noticing every single question that
you've asked
presupposes uh objective reality or
realism as its basis and so it's a fact
I have to unpack it a little bit just to
be able to give the answer that I'm
giving
I guess um well around alignment uh is
there if are humans aligned humans don't
seem to be aligned as well I mean
there's so many different perspectives
out there so many different Inc
commensal points of view or you know
value systems in the world if we if we
can't align AI can we align humans and
what do we align them around we can't
compel humans to be aligned with humans
right that's just not going to happen I
mean you you can try but I don't think
there's any force of government or any
methodology of government or methodology
of technology that is ever going to uh
create uh in this particular sense uh an
absence of diversity of opinions right I
I believe that that's a built-in right
but I think that there is a hidden
alignment of humans that is really easy
to overlook so uh cooperation and
alignment for example are largely
invisible things and so in effect the
places where humans are aligned with
other humans is um again a sort of
metabolic process alignment like I might
not agree with you as far as your
philosophical point of view is concerned
but you and I will probably agree that
food is edible and that what I can eat
is at least to some
approximation something you can eat now
obviously I can have say for example a
intolerance to nuts because I just don't
happen to have the digestive enzyme but
the fact of food is the way in which we
create energy in our bodies and that
choices that I make about that will be
very similar to choices that you make
about that that in effect our choices
are align the methodology by which we
think about and solve problems
around our metabolic processes are in
effect going to be inherently similar
because we have a common biological
heritage right our our bodies work in
similar ways there's in in fact you know
it's not just that um you know thinking
about it from a genetic uh genotype and
phenotype point of view we have 99% of
ourselves in common right like like you
and I aren't that different as far as
species are concerned as far as you know
things that we care about are concerned
like you know the kinds of things that
would elicit laughter in me probably
will elicit laughter laughter in you as
well and the response of laughter is
something that we can kind of presuppose
about one another that we wouldn't be
able to presuppose about an artificial
intelligence and so in effect there's a
kind of you know organic alignment
occurring process that emerges for any
evolutionary system that has common
basis um in in the in the ecosystem
itself the mere fact that the
ecosystem is organic means that
alignments are going to occur
among if we had an artificial ecosystem
it would be aligned with itself but they
that alignment wouldn't apply to organic
ecosystems so then you have to look at
ecosystem to ecosystem Dynamics and
start thinking about what are the
implications of the enthalpy of those
interactions and you know in this
particular case we notice that the
alphabet of uh material process
associated with Organic ecosystems is um
know fairly uh fairly specific you know
hydrogen nitrogen oxygen phosphorus
sulfur things like that whereas for the
technological infrastructure man the
alphabet of Elementals that are involved
with that is vast and so the mere fact
of using a larger alphabet means that
there's a kind of inherent Elemental
incompatibility between these two things
like a lot of the stuff that's going on
in the um uh technological ecosystem is
essentially going to be inherently toxic
to the organic ecosystem because it's
using Elementals like arsenic for
example that the organic ecosystem just
doesn't have a way of dealing with and
so in effect there's a there's a sense
here that at every level of constitution
in terms of materiality a molecular
structure and and and macroscopic
structure um you know
reproduction um all of the stuff about
how energy is used managed and
metabolized and so on um creat such a
high level of incompatibility between
these two the alignment is just not
occurring whereas for organic creatures
alignment is is basically going to
happen by accident um more or less
everywhere because of the common
ecosystem that they're all a part
of some would argue that humans aren't
really that aligned with the rest of the
ecosystem in the modern
world well again it depends on what
level you're talking at like at the
level of you know how we essentially uh
you know bulldoze over uh rainforest to
harvest oil and stuff like that yeah
that's a kind of incompatibility but the
human technological artificial ecosystem
is different than the natural ecosystem
and you're right there's already
incompatibilities there um but but in
this particular case the abstract
virtual world of uh social process
monetary process uh communication uh
what we would consider to be culture and
language and story and narrative and
identity and money and all of the rest
of the stuff that we're propagating
around we're already building an
artificial ecosystem that is virtual and
Abstract which is driving a
hardware-based ecosystem which is itself
different than the organic ecosystem and
you're right there is a misalignment
there but at the level of we eat stuff
that grows in the
ground and we eat stuff that eat stuff
that grows in the ground we are still
compatible with the underlying ecosystem
and to some extent we have to honor that
or we just won't have food
so so there's a sense here of um a
compelled compatibility that we aren't
acknowledging and that we're undermining
by basically Paving everything
over now there's been a number of
critics to your theories people may
maybe there's maybe people AR just
aren't comfortable that their research
is invalid or that you think that their
research is invalid AI alignment
research now do you think it's still
worthwhile for them to try this just in
case um you're wrong about it or are you
confident that that they're wrong so
confident that it's not even worthwhile
them actually doing the research now
well this is this is a this is a
question that's fairly similar to the
kind of question that occurred in
mathematics about a century ago which
was can we create a single unified body
of mathematics and um you know should we
do so and and there was a lot of people
that really wanted that I mean it would
be uh profoundly intellectually
satisfying it would be uh you know
coherency building it would give us some
really wonderful tools uh and and so on
um and unfortunately uh this this
mathematician uh Kirk cell um basically
came up with a proof that mathematics
can't be consistent and complete at the
same time you can either either aim for
completeness which was the dream but
inherently will be inconsistencies which
is kind of anti-
mathematics or you can keep consistency
and admit or just let go of the idea
that there could be any complete
understanding of all the things that are
true uh in a mathematical way and you
know so so the question then became not
should we continue to try on the basis
of whether uh Goodell might be wrong
with his proof but check the proof like
like the real thing was you know the
mathematicians at the time they didn't
like it when when gell came I mean it
was controversial among mathematicians
for a good while and you know there was
a sense though which is well it doesn't
make sense for us to try to do an
impossible thing if it's actually
impossible and we can know that so then
the question becomes can we know that is
it actually the case that it's
impossible or is the proof wrong if the
proof is wrong then for sure definitely
continue to try to do the thing that's
hard to do but if it's actually
impossible then for sure we would want
to know that so that we're not wasting
our time trying to do something that
isn't merely hard but is actually
impossible now in this particular case
first of all just for the record I have
huge compassion for people that have
invested their lives for decades now uh
many of them to genuinely try to do
something that is truly hard right like
from that perspective like elazer is
basically saying hey this is a hard
problem but we need to work on this
because this matters and and and so he
basically was you know very encouraging
it's like well we didn't believe we
could make airplanes and yet we did and
we didn't believe we could do this and
yet we did and technology has overcome
all sorts of problems that that at the
time we thought were Out Of
Reach but at the same time there are
things that we've learned that you just
can't do like you're not just going to
solve the cap problem with technology
you're not going to you know the
consistency and partitioning that that
whole thing computer scientists know
about this you're not going to solve the
halting problem you're not going to
solve um you know something that
basically would suggest that you could
uh you know prect the angle using a
compass and a straight edge it's like at
a certain point you have to recognize
that there are ways of thinking about
this that are clear in a way that's
that's consistent with the sort of
mathema iCal perspective that gives you
definite information about the
difference between hard and impossible
or Known Unknown and unknowable the
unknowable is just as much a class as
the unknown that could potentially be
converted to the knowable right like
science basically believes that
everything that is unknown can become
knowable but what the metaphysics shows
is that there are genuinely things that
are unknowable that are inherently
unknowable that hard Randomness is real
and that in effect what happens is is
that when we when we start to get
sophisticated about distinguishing
between the merely unknown the merely
hard and the actually impossible the
actually unknowable that that therefore
we can narrow our efforts to the kinds
of things that that really do make sense
to do but of course that choice to focus
our efforts to narrow our efforts and
particular way is contingent Upon Our
genuinely understanding and being
confident not just me I mean you know
who who the cares what my opinion
is what really matters is is have the
observations that I've made have the
path of thinking that I
have traversed you know however I got to
thinking about that particular thing it
doesn't matter the path is now something
that we can put blazes on and we can we
can see okay with these observations and
this m we can arrive at this conclusion
and with this we can arrive at this
conclusion and we notice that we get
from here to here and and it's the
coherency of that path that matters in
this particular case and yeah it may be
uh tremendously
distressing for people who are basically
on one hand believing that technology is
inevitable and therefore we need to make
technology work well for life because if
we don't no one else will obviously the
corporations don't care about that they
care about profits and so in effect
there's a sense in which you know
they're not going to try to solve the
two masters problems because frankly
they have a fiduciary responsibility
their shareholders and United States law
is basically saying you have to do
what's profitable because that's what
the charter says and you can't disagree
with the charter because that's illegal
right like now so so in effect you know
many people have tried to step outside
of the corporate model and say we're
going to try to create a a compelling
way to create alignment
and then we're going to try to basically
somehow have the government Force the
corporations to build things that are in
alignment of course and we're going to
hope that that's robust enough that
there are no uh exceptions that emerge
that kill us all right and you know
that's a really roundabout way of going
about it and if I come along and I
basically say yeah there's no hope of
creating alignment in in a technological
setting you can only hope to have
alignment emerge in an organic setting
so we need to get better at doing the
organic thing where this artificial
agency is just going to kick us off the
planet and there's a sense here in which
you know knowing that basically says
okay we need to change our strategy we
need to maybe instead let go of the
notion of Technology being a solution to
all
problems recognize where technology is
good and useful and beneficial and where
it is definitely harmful and learn the
difference between hype of well we think
this might be awesome versus well this
could actually be terrible to knowing
for sure yes this stuff will be actually
terrible but this other stuff's okay and
to be really really good about
Discerning that boundary and then maybe
we can create some sort of agreement
that yeah we all love our children
nobody wants to kill everybody else
maybe it's a good idea for us to be
really Discerning about the difference
between what works and what's terrible
and in this particular case I think that
investing effort in that is worthwhile
and you know nobody wants to build the
Doomsday Machine like that just doesn't
make any sense it's mutually assured
destruction it's it's a known theory of
Game Theory Game Theory itself in this
particular case makes it really obvious
that no we don't want to be stupid but
in that case it comes back to the thing
I was talking about before we are
literally the dumbest species possible
to have technology and we need to fix
that with better choices we need to get
more sophisticated about understanding
impossibility paths relevance paths
meaningfulness sacredness life what
matters what doesn't and and how do we
make better choices around technology
because frankly we have to that is the
Dilemma of our age we need to balance
the relationship between nature humanity
and technology and have technology being
right service to the truths of nature so
that we can endure in the long term and
if we don't do that it's
over this proof um as far as I I haven't
been able to find the the proof in a
mathematical form I've found some
writing if do you think it's possible to
put this in mathematical terms as a a ma
in fact uh there's been some uh specific
work so in other words the way at which
I work internally um and and the kinds
of ways I construct Concepts and stuff
like that has the same level of rigor as
mathematics but a lot of people are
looking for how do we substantiate the
inequality which is the fundamental
inequality of the proof and in to do
that we basically have some specific
tools so um one of the ways in which we
can kind of consider this is to is to
think about and model the uh capacities
of control theory like like if we're
going to try to compel alignments we can
we can designate a little bit about what
we mean about alignment in a very very
general sense we can talk about control
theory in a general sense and we can
basically show that there is a way to um
characterize what level of control we
need for alignment and what level of
control would be possible maximally
possible and we can do this at various
levels of abstraction right and we can
show that once a certain threshold of
abstraction is is is
crossed that no version of control
theory can create the level of of of
control necessary for the minimum level
of uh control required in order to to
create alignment right and so in this
particular sense you can think about it
in that particular way and there's a way
to construct that um another way that
this can be done uh to create a kind of
um way of thinking about it is is you
can look at um like what happens at the
interfaces so one way that I've modeled
this for um a a colleague uh Andrew
Sandberg and and and he and I have been
in discussions around you know um
enabling him to write a paper about this
essentially which is wow yes I've had
him plenty of times on my channel in
fact uh at the the end of this or the
end of next month we're going to have a
dialogue between him and David Pierce
and is wonderful he's great yeah World
he's an awesome person he's he's I've
I've I've had a chance to meet with him
several times and and I've all I've I've
enjoyed the conversations although uh
they're always very challenging because
he's quite smart and so um in a sense
there's a uh there's a sense in which we
have at at this particular Point through
a through a number of meetings have con
constructed a kind of rudimentary
language uh where where we're now able
to start to formalize that and and and
so the way it looks is um you know and
I'm just basically creating um
terminology uh for this so um for for
lack of a better word I called the
entire physical natural Universe
Z the uh
artificial you know agent or ecosystem
of Agents or just the totality of that
technology base um X and the control
algorithm or the the thing that would
make it so that alignment happened Q
right and then uh again for Simplicity
sake I said okay there's um that that
each of these processes uh Q x and z
that there's a flow of information that
goes from Q tox from X to Z and from Z
back to q and of course we can create
all other Paths of of how information
flows by basically saying well if I want
to get information from Z back to X I
could just have it flow straight inputs
to outputs with no modification through
Q so that's isomorphic with the routing
of information from Z back to X right so
Z being the natural world the totality
of all physical process so it takes
inputs which are the outputs of the
artificial system and it processed those
in some way and it produces some change
in ecosystem which the um you know again
if we're trying to control Evolution
like this is the model like can can we
create a a control algorithm which would
constrain the artificial process from
not destroying uh the Z process the
organic aspects of the Z process which
obviously also includes the physical
universe and all of its lawfulness so in
effect you know Z can route back to Z uh
either through itself or you know
through a pass through of q and x
um X can talk to Q by pass through Z
back to q and so we can connect
everything to everything uh through path
throughs plus I can have q modulate
everything that's an output of X by
basically having a path that goes to the
far side of X and then captures
everything that would have been the
output of X and and act as a modulation
of that so in effect we can set up now a
mathematical model that talks about the
Dynamics of the information flows
through the
system okay this is the beginning right
so so now I have these three
domains I can't
control what happens in the Z domain
ultimately the laws of physics are just
the way they are and that's just that
that's that is outside of of anything
that we can presume to have any real
effect or influence on I mean obviously
I can affect the universe but I'm not
affecting the way the universe
works you can't change laws of physics
that so the design of Z is
immutable the design of Q is essentially
mutable on the basis of the best thing
we can conceive of possibly
creating right so we're going to treat Q
as being mutable up until the point of
we've figured out a way to solve the
problem of alignment okay now what about
the design of X now at first we can
specify the design of X because we built
the artificial intelligence right but
evolution would imply that X begins to
change over time in response to Z and
maybe even in response to Q or maybe in
response to how Q Z and X all interact
but in one fashion or another if x is a
learning machine and we can't control
what it learns because whatever is going
on in Q is not controllable by us right
so in other
words x May learn things about Z that we
would never anticipate that it could
learn it might learn laws of physics we
just don't know H okay and whether it
learns it explicitly or implicitly it's
going to change the nature of X the
design of X is now changed because the
software process of its learning can
affect what the hardware does because
you can have you know causation from the
software level into the hardware level
and I can have causation from the
hardware level up to the software level
that's what it means to basically be a
learning system in the first place and
so in effect I can't ignore the learning
and I can't ignore the substrate and I
don't know what's data and I don't know
what's code so as a result over
time while I may have specified the data
and the code as the initial conditions
the design of X starts changing over
time and I can no longer anticipate what
the design of X might become in the
future okay the only thing I have
control of therefore in the long term is
q and if we're talking about alignment
then that basically means that Q has to
account for all possible future
evolutionary states of
X all
right so
having basically said something about
the mutability conditions we can now
start talking about the interface
conditions so let's think about um as a
as a metaphor okay um I have like a a
dartboard right and I've got a person
that's throwing darts at the dartboard
okay and for the sake of Simplicity
let's describe that the dartboard is
really really simple it is essentially
the entire wall and let's say the wall
is infinite at extent okay and I'm
standing on a platform in front of this
wall and there space some distance from
and I just have this vertical wall it's
infinite in all directions and has a a
vertical line down the middle of it and
the entire left hand side is Painted
Black and the entire white right hand
side is painted
white and I win if I throw a dart and I
hit the white
now given that
scenario it's pretty likely that pretty
much any dark player would be able to
win the
game right what if I were to make it a
little more complicated by instead
of they're not blind yeah they're but
even if they're blind you know you could
have the guy behind them see and kind of
slide him over to the right a little bit
and say just aim vaguely right and just
throw the dart and if the dart hits the
wall at you're going to be okay right
you know and and so in effect and that's
not breaking the rules right I mean it's
like whatever process you have to throw
the dart use it right because that's the
design of Q right in this particular
case so so in this particular sense
let's just basically say that the white
region represents Expressions made by X
that don't destroy organic life in
Z so now we can basically say how do we
represent the fragility of organic life
in Z what does that look like okay so
now I'm basically talking about this
dartboard and say well I've just made
the game really really easy well let's
let's make it a little harder let's
basically say we've made it a
checkerboard and the white squares and
the black squares are now mixed together
but the checkerboard is basically you
know a square a foot on a side and my
platform is like 20 feet away okay so
you know is my aim good enough aim at a
particular white square and hit it if
it's a foot square on a side yeah it's
probably something that many players
would be able to do obviously you'd have
to have played darts a little while but
but in in theory that wouldn't be a hard
game to win either now the key thing
about this is is that we have
preserved the area of the white side and
the area of the black
side okay and we've just changed the
layout of where white is relative to the
black okay
now the thing that defines whether or
not you win or lose or in this
particular case whether life endures or
doesn't is partially the amount of white
that is on the screen or on the target
relative to the amount of black or
relative to the overall
area right like if I if I had an
entirely black surface surface in just a
single white
square then I'd better hope that my
platform is relatively in the right
place because if it's 40 feet down the
wall in the wrong direction I'm G to
have to throw that Dart really hard in a
really specific direction to hit that
white square right and if the white
square gets smaller and smaller relative
to the size of the wall the probability
of my hitting the white square goes down
but that's not the only way to affect
the probability of hitting
white say I start with a checker board
that is again infinite in extent and I
make the squares one millimeter
aside okay so from 20 feet away that's
that wall is going to look gray because
it's an equal mix of black and white but
if I get close I notice that there are
distinct white squares and distinct
black squares they're just mixed
together really
well and I throw the dart okay even a
really good professional Dart player is
not going to be able to guarantee that
they hit a white square more than about
50% of the time MH even though the area
of the uh system is the same
changed okay so in
effect the amount of control that I need
to have over the
dart to create a winning
outcome depends both on the ratio of the
white area relative to the black
and the
coastline between where the white is
versus where the black
is if I have a dust relative to where
you're standing as the
person but say for example my control
allows me to position the platform
wherever I want okay right if I have a
even mix of like say atom siiz
squares that I don't care whether using
a laser you're still not going to get
that kind of
precision right so it's not just a
question of accuracy in the sense
of can I aim in a genuinely white
Direction it's what level of control do
I need to have when the expression hits
the target the dart is an expression
it's a choice made and when it goes from
the xrm into the zrm is it the case that
Z is specific to the positioning of that
expression the particulars of that
expression such that it's fragile to
that or is
not right now biological life is robust
over a wide range of conditions but it's
very fragile with respect to some very
specific things that you wouldn't
necessarily anticipate in advance um one
example of this that that I give is um
you know mad cow disease so somewhere in
the brain of some creature some protein
got
misfolded and that misfolding is now
basically causing other proteins near to
it to be misfold and then that that
spreads and there's no biological
process that that creature has to fix
the
mistake and the problem is is that if
anything
eats the creature that had the misfolded
protein it ends up with the misfolded
protein and then it becomes a carrier of
that because the protein causes other
proteins and so this isn't a virus it's
not a bacteria it's it's not an aneba
it's not some sort of medic
disease it is a protein folding
mistake but there's no biological
process to handle that there's nothing
that Evolution has developed yet to fix
this or to adapt to it now where did
that first misfolding protein come from
why I have no idea it could have been a
cosmic ray or some radiation it could
have been some technological process
that generated some enzyme or some
hormone or some side effect that result
Ed in an overproduction of something in
the body that resulted in this F protein
getting misfolded for some bizarre
reason that never happened in all of
history but the chances that it was
something technological that had this
side effect unfortunately is better than
even odds right because an awful lot of
the universe time like Evolution has
been occurring on this planet for like a
billion years and this looks like
something recent because if it had been
something that had happened in the last
million years or so or 100 million years
or so an evolutionary process to handle
this would have already
developed so it's pretty likely that
it's a technological origin of some kind
or another so some expression hit a
black square and now this is essentially
an issue okay we don't know how to fix
this right at this particular point the
best we can do is eradication if we can
detect it and measuring this is hard so
at this point does that problem deserve
working on yes are there other kinds of
problems like this that we have not yet
encountered yeah lots right this is an
example of one protein out of
millions in each organism every creature
on earth it's a mammal or a reptile or
lizard has the play of literally
millions to billions of proteins and
chemicals with really complicated
structures all interacting with one
another and all you got to do is it
up in one place and you end up outside
of what evolution has learned to deal
with because it's literally that complex
and evolution can only deal with so much
and so in effect yeah over time it
figure out a way to Route around this
problem but if it takes 5,000 years to
solve this it's only going to fix it for
one or two species and the rest of life
is going to basically be you're gonna
have no more mammals right so so in
effect how sensitive is life to these
kind of technological issues actually
near as I can see very sensitive right
when you look at the total complexity of
of just even a single cell it is more
complicated than probably the most
complicated microchip currently
made right it takes a an entire floor of
a gymnasium to document in in in a
really really small print just what's
going on with a single microprocessor
you carry around in your pocket whereas
if you're just diagramming the digestive
chemistry and the all of the molecular
interactions is probably going to
require a bigger diagram than the than
the stadium floor can
cover that's just you know again this is
biochemistry talking to modeling Theory
but you know somewhere along the way we
notice that when we look at the whole
ecosystem it's way more vast than that
like we're talking like you know 10 to
the 33 complexity if you're wanting to
implement it in software down to the
quantum level there's no way
right so in effect you know Evolution's
bandwidth of processing Solutions these
particular things is way lower than the
amount of embodied complexity in the
design and in effect what you're what
you're therefore saying is that
evolutions really good at figuring out
specific solutions to General
problems but the reust of that is really
slow and really low for the kinds of
things that technology
does so in effect what we're seeing is
is that if I model this as a Target I
have two problems the amount of white
area ah it might not be small but it's
you know a tiny fraction of a tiny
fraction of a percent at
best but the distri
distribution of the spots that are white
versus the ones that are black I.E the
things that are benign versus the things
that over a century have some Toxic
effect that for which Nature has no
amelioration well the interface of that
thing is is is way
complicated and so the level of control
that you would need to have to digitally
select for every expression whether or
not it's going to hit a white spot IE is
aligned to life to organic life on the
interface between x and z is it's
phenomenally bad now Anders and I are
for the most part not having that
conversation because for the some to
some degree we have slightly different
priors around some of this and we can
negotiate that but that seems less
interesting to us than looking at what
happens in X and what happens in Q
relative to
X that could for any values of like
unless the target is painted completely
white at that point the level of control
sensitivity you need is effectively zero
but we know that's not the case we know
that it's some fraction of the wall
that's painted white and we do know that
the coastline between the white regions
and the black regions is some enormously
detailed fractal in some huge number of
Dimensions I don't think that he and I
would disagree on that particular Point
although I know him he will quibble and
quibble and quibble about every level of
detail and try to unpack everything I
don't really care the fact is is that
it's a definite value less than
50% and the coastline is at least more
complicated than some definite number
which I won't name but is higher than
than um is needed in this moment to
basically set up for the next part of
the proof okay so so then we basically
say okay if I have some digital signal
like I take the the the the space of
that wall and I basically say well what
is the permutation complexity of the all
of the chemistry that flows across the
wall and all of the all of the possible
mtic energy expressions and all of the
possible you know so we can look at it
from an atomic level we can look at it
at an energy level and we can look at
patterns overlaid over energy and and
atomic atomicity and we could basically
say for all of the information and all
of the atomic and all of the energy flow
from X to
Z if we characterize that as a digital
signal I.E it either happened or it
didn't it either was an expression or it
was not and whatever the bandwidth of
that digital signal is therefore a kind
of
expression that is now a mapping like I
can take whatever that bitstream is and
I can treat it as a mapping
into space of a dart hitting a
Target right right I can take whatever
the dimensionality is of however I
represent the black and white areas as
Go versus
noo and the digital signal as
essentially a selectivity of a position
of the no versus goo I've now described
a degree of coherence that I need to
have on that Digital Signal expression
every expression that happens so if the
bandwidth from X to Z is you know one
terabit per second then I now have
characterized what level of coherency
Precision of the data content of that of
each expression from X to Z what what
level of control do I need to have over
the signal from X to
Z okay notice the kind of numbers I'm
talking about here right like really
huge numbers of the number of squares
that are
black tiny fraction of them that are
white are there any gray areas what that
that if you would would you say that it
would be worth saying that there might
be gray areas where if you hit them with
the dice oh sorry the the dice the um
the the arrow or the doart that wouldn't
be existentially catastrophic but it
would be bad it could be bad but varying
degrees of bad sure but here's the thing
if we're talking about alignment and we
want to simplify the argument as as much
as possible we can you know over time
Elevate the the gray areas to either
black or white depending upon what
subsequent expressions are and then we
can basically just say let's just take a
single moment of expression and just
repeat that over and over again and just
see what happens over the long term and
that's going to tell us whether
alignment was adhered or
not right so in other words just
extrapolate you know anywhere that we
could basically talk about a gray area
extrapolate over the next hundred years
what
happens so so in effect I still want to
kind of say the control algorithm wants
to make a decision as to whether or not
to intervene to
constrain the inputs to
X so as to constrain the outputs of X so
that it doesn't damage
Z so I'm characterizing it as black and
white on Z so that at least it's
tractable to do the math in x and
q right because that's how we set the
proof up we take a simple case and we
show that all extrapolations of that
simple case reduced to that simple case
and that that simple case has a clear
unambiguous proof about it and once we
show that proof it's QED so what I'm
doing is I'm setting up the inequality
I'm basically saying whatever the level
of white versus black on the
wall and whatever level of Coastline
that it has right is it a scattering of
tiny points or are there a few regions
that we can aim at that just anything in
this cluster is okay right but either
way the amount of control that I need to
have on the bitstream is going to be
pretty
high that much is obvious okay so then
we can look
at what is the level of control that I
need to have on the bitstream going from
Q to
X such that the output of X has at least
that much
control or that I that the Q can control
the output of x to at least the degree
needed for X not to destroy
z i don't get to change how Z receives
this so the distribution of of white and
black and where those points are and so
on so forth that's fixed that's laws of
nature laws of biology the world as it
is that distribution is whatever it is I
don't get to change whatever happens
Downstream z i I can already see that
Anders would say Z could be May more
robust through augmentation he's a
posthumanist right he'll say that well
nature is what it is but we can improve
on that and we can try and make it like
error correcting more fault tolerant
able to withstand the damage that could
be done today maybe in a thousand years
could be dealt with he I imagine he
might say like that here's the question
what part do we are we begging the
question as to what alignment means if
alignment means compatible with or oric
life but now I change the meaning of
organic life to be artificial life then
I'm just I'm cheating because I've made
organic life artificial life and then I
said well it's compatible artificial is
compatible with artificial well that's
not proving anything of Interest right
so in effect here we're we're basically
saying can we make it compatible for
humans as humans
are and you know again it's it's
basically like is there is technology in
the long run compatible with Biology and
yeah there's some ways in which we can
make compatibility but there's a range
in which we do that which means we need
control over the kinds of things we can
control given that we can't control
everything like we can't control the
laws of physics and to some extent
organic biology just is the way it is
right I'm not going to change cellular
chemistry for every human being on Earth
anymore than I can change the opinions
the political opinions or the values of
every human being on earth right there's
a certain amount of diversity built in
and there's a certain amount of
robustness built in and maybe I can
improve that but I'm not going to be
able to improve it Beyond a certain
point and whatever I'm doing to improve
it is essentially still a
signal flowing
from either
Q to Z through X without modification or
a signal sent from Q to tox that itself
is a signal to Z that increases the
robustness of Z but that makes it the
same example that we were just talking
about is that signal in the white area
and the white area in this particular
case includes a beneficial effect of
making Z more robust but still it's a
signal flowing into Z right there's
something I'm doing to Z so even the
action of a super intelligence trying to
make Z more robust is still a signal
that has to actually work which means it
has to work for the conditions that Z is
in to create the better conditions that
Z could be in and no matter how I model
it's still an extrapolation of exactly
the same underlying form the same simple
model still describes the full
characterization of all variations of
the argument that that you or Anders
could put forward so I can encapsulate
all variations of what was just
described as essentially a a a a quibble
as being a special case of the signal
flow from um X to
Z right you see it okay okay well you I
will digest this further I don't know if
I have this the the perfect response to
that that Anders
might he's a lot smarter than I and has
wonderful capacity to quible to the end
of time I have never found the limits of
his capacity to quibble but I think that
you know to some extent you know a
reasonable person will eventually
basically say okay I'm G to park these
quibbles for now because I want to
understand how the proof is set up right
because again we can extrapolate the
proof in all sorts of directions to
handle quibbles as as they' be relevant
but if we if we bring the quibbles up
too quickly and this is something he
does a lot is he gets so caught up in
the quibbles that we lose the thread of
the argument that sets up the proof so
I'm still in the process of just setting
up the geometry of the proof but we're
more than we're more than three quarters
of the way through by the way who so so
like yes that that would be amazing if
you could get the mathematical proof up
and running and that would I think that
would open the doors to a lot a lot of
different types of people who may
dismiss your
arguments understanding them but this is
it so this is this is the thing Andrew
is at this particular Point has studied
enough of this that he's he's closer to
the point that he could write that out
and I would prefer that he do it because
he's going to do it in a way that other
people are going to be able to relate to
because he's coming from a that they're
making much more easily than the kinds
of assumptions I'm making be in mind I'm
coming from a set of of you know
presuppositions about how the universe
works and things like that that are
based upon the metaphysics which
although compatible with the way science
thinks about the world is just still
different and and so in effect there's a
sense in which it's going to be better
if I convey this to people such as
yourself and such as Andrew to the point
that you don't have to rely on me you
can do the work yourself knowing how to
do it so like I said we're three
quarters of the way through that um and
that's just one way to set up the proof
by the way there's there's several
parallel ways of setting up the proof um
all again all of which arrive at the
same conclusion so it's kind of
reassuring that no matter how you prove
it you end up with the same result it's
nice to know for like the Pythagorean
theorem that there's a variety of
different ways that all prove the
Pythagorean theorem and if you don't
understand one proof you're likely to
understand another and some of them are
quite easy to understand but but in this
particular case the one that I'm working
on with Andrew specifically is set up
with this model of Q being the control
system X being the artificial
intelligence super intelligence system
and Z being all the rest of the universe
including all biological Life as a
component right and what we're what
we're basically suggesting here is is
that it's
possible to characterize the degree of
control necessary on the X to Z
boundary and it's possible to
characterize all of the relevant
phenomena as a flow of information from
X to Z Z to q and Q to X remember the
first part of it was how I could show
that I could wrap up all information
flows as a single cycle of flow from Q
to X to Z to to Q to X to Z and that's
the loop right so I've basically taken
all possible Loops of process and
reduced them to this one Loop of process
and showed that all Loops of process can
be subsumed by this one Loop of process
therefore the mathematics and the proof
is subsumptive of all possible uh
modeling Dynamics right because modeling
Dynamics are always going to be a loop
of some sort or another right and all of
computer science all of algorithms a
feedback loop all causations a feedback
loop so by describing the Dynamics of
loop causation directly I effectively
subsume all quibbles of this space
because I can reduce them all to
instances of special cases of this
underlying model the Dynamics of which
are fully
described so it's a bit like uh an
extension of Shannon's information
Theory right it's like thinking about it
at that sort of level and once you've
got information Theory a whole bunch of
computer science open up right it's like
you can understand all sorts of things
in computer science once you understand
you know the notion of entropy and how
much information is represented by a
given number of bits and so on so you
know and after that it doesn't matter
what base number system you're using or
uh so on the notion of bandwidth becomes
a coherent notion right so in this
particular sense what I'm basically
saying is is that the notion of
control can be reified with respect to a
kind of model of an area that represents
all possible
signals M some distinguishing notion
between signals that I want versus
signals that I don't
want
and the proportion of the signals that I
want relative to all possible signals
and and this is the part that almost
everybody overlooks the
distribution
of wanted signals amongst unwanted
signals and then I can talk about the
bandwidth of the flow from X to
Z as characterized by this control
Dynamic like the requirement of control
so in other words now treating the
bitstream the the the the the pulses or
the packets that are flowing because
again I can quantize I can discretize
this system right and it's necessary to
do that in order for the notion of
information Theory to be coherent and oh
by the way because we're talking about
artificial intelligence it's already
quantized so I have to use a quantized
model if I'm really going to
characterize this in a intellectually
coherent way right I can always you know
do the conversion from a summation to an
integral but if I don't understand what
that process even means on a summation
level I'm certainly not going to
understand what it means on an integral
level right so I'm going to stick with
discreet math for now okay so in effect
there's a sense in which I have the
totality of all possible signals a
fraction of those signals which are
coherent and a distribution of the good
signals versus the non- signals and that
now characterizes the degree of control
that I need to have over the specific
bits in the bitstream in some sense now
there's all sorts of ways of mapping one
hot two binary sequences and vice versa
and it turns out that the isomorphisms
of those kind of things are really well
understood I'm not going to prove that
now I'm just going to say that there is
a strict isomorphism between the
structure of the bitstream and this one
hot model of a dart thrown out a
board okay so now we can look
at the bitstream itself and we now have
a notion of minimum level of control
required on that
bitstream that's now a characterized
notion in a fully mathematical way
there's no ambig about this at all it's
fully
quantized so then we can look
at given how much
control at a minimum we must have on the
X to Z
interface So based upon the Dynamics of
Z we've come up with this area notion
and this mixing notion and now we have a
bitstream that is characterized by
that proportionality and mixing notion
so therefore we know what is the minimum
level of control that X must have on the
output bitstream that X is expressing
into
Z okay now here's where minimum yep
right so so it does seem as though
you're saying that there is like you
know a a point at which there is a
minimal level of control needed right
here so there's a range above that that
we can get better and better
beyond the minimum well there's a
hypothesis that we could reach that
level of minimum control necessary and
for some s systems yeah that's totally
tractable like if the if the area is
relatively large and the mixing is
relatively low the relative amount of
control that we need is relatively low
and for all sorts of systems we can
create that level of control no problem
right so we don't need perfect control
in order to to solve the problem we need
good enough control yes that's another
question I was wanting to ask actually
yeah we need good enough control but now
we've characterized in a mathematical
way what is good
enough that's and anything less than
good enough is not good enough I.E not
in
alignment right we've we've we've
subsumed the notion of alignment into a
metric that has to do with area of
acceptable signal versus the totality of
all possible signal signal mhm and the
mixing of the good signals versus the
totality of all possible signals right
let's just say we we we do achieve an
artificial general intelligence or a
smart not quite super intelligence but a
particularly robust
intelligence um that allows us to to aim
where we have a a good enough signal
level we have a our solution is good
enough that it it has a probability of
not destroying us at
75% probability of not destroying us
okay that's just a number I pulled out
of nowhere um and then at at the next
point the AI develops further and it
becomes better and and so the likelihood
of Destruction at the next stage is the
likelihood of survival he
making but bear in mind that the nature
of the proof is to dis prove that that
hypothesis is reasonable
right so I mean you know again you can't
smuggle in the thing you're trying to
prove if you're trying to say it's
possible to make a safe system and I'm
basically saying well show that it's
possible to make a safe system but in
the meanwhile I'm showing that it's not
possible to make a safe system so
whatever it is that you propose has to
account for the proof I'm setting up
currently say you know so you don't
think like increasingly Safe Systems are
an option
so like we may not be able to create the
the the most robust perfectly safe
system at Point a but we could have a an
okay system at Point a and then at point
B have a better system all the way
through to Z where we have maybe not
even a perfect system but a very Mak
assumption that turns out not to be
valid in this particular scenario so in
other words the assumption that you're
making assumes a kind
of analog aspect to the process but
remember I'm stipulating that it's a
digital process so for instance um if I
have a a wall and there's a stud behind
it I can knock on the wall and I can
tell from the sound where the stud is
but that's because there's a kind of
gradual difference in the sound that The
Closer I Get To Where the stud is the
more the sound changes a certain way so
I can converge you know there's like a
hilly landscape and I can do a hill
climbing algorithm to create an iation
that converges on a local Maxima I can I
can use analog process because
everywhere I could calculate the the
gradient and algorithmic processes that
can detect a gradient can use that
gradient to effectively Converge on a
solution but here's the
thing to assume a gradient is to assume
some sort of analog or floating Point
number with however many decimal places
are needed but if I have a combination
lock and that combination lock is a
digital lock I might be it doesn't
matter the distance between the
combination I try and the right
combination there's no signal there's no
feedback that I get that lets me know
that I'm either near or far away from
the right combination the only time the
lock opens is then if I get the exact
right combination the one time I try it
and I could try a billion wrong
combinations and I will get no new
information as to what the right
combination is so in this case all the
times that we're talking about
convergent processes based upon hill
climbing algorithms we are fundamentally
assuming that the substrate is
analog but a digital artificial
intelligence I'm sorry that is not an
analog process it is a digital process
moreover than that we're talking about
signals and if I'm basically trying to
construct a proof then in effect I need
some notion by which I can reify the
concept of control and as a as a as a of
setting that up I'm setting it up on a
digital basis and at some future point
we can make the transition from a
digital basis to an analog one but if we
don't understand the Dynamics of the
digital basis we are going to
continually mislead ourselves to believe
based upon an analog understanding that
a certain digital process will work a
certain way but it doesn't the digital
world is just different than the analog
world and the proof works in the digital
one so I've got to set it up
there right because somewhere along the
way we're talking about things like the
rice theorem and why the rice theorem
shows that this particular idea of
alignment can't possibly work but that's
an entirely different proof methodology
but again somewhere in the nature of a
proof we're talking about discret ideas
explicit equations definite Notions and
definitions with exact meanings with no
ambiguity as to what it is that we're
talking about and unless I set it up
like that I don't get to 100% knowable
because it's not a probability it's a
discret b zero or 100% zero or one right
because 100% is exactly one so it's
either perfectly true perfectly false or
completely
unknown right and what I'm trying to do
is I'm trying to convert the state to
the definite state of known
true so again quantized so in this sense
you know in this particular sense what
we're what we're looking at is is that
I'm not going to have the artificial
intelligence system be able to predict
in advance everything about Z because
there are some elements of Z which are
unknowable there are some things that no
matter how perfect you could you could
basically say x has perfect infinite
intelligence and that's basically the
same thing as I was saying earlier you
can know 100% of everything about the
prior causal state of the universe but
it's not going to allow you to predict
the next bit of information in two
entangled particles as a communication
Channel M right there's a hard
Randomness built in and intelligence no
matter how much it is it's not gonna fix
that because it's a category error yeah
again okay so so can can can it can an
artificial intelligence then like a
super
intelligence like a make predictions
about what is
predictable um and make assumptions
about what's not predictable and
constrain the um the impact of the
unpredictable no because that would be
trying to define the structure of Z and
we told Z was already
defined right so in other words I'm
going to basically say well I can have a
pretty good idea about what's
predictable and unpredictable but is it
the case that that level of
control over my
signal is going to constrain that the
signal is only in the space of the
predictable phenomena turns out I can't
even do that because again I have to
presume that the level of control that I
have is greater than the minimum level
of control necessary and if the boundary
between that which is predictable and
unpredictable in the same way if we say
predictable is the white zone and
unpredictable is the black Zone then it
isn't just the area of the white versus
the area of the black in
total it is the mixing of the the the
white and the black and it turns out
that there is again complex boundaries
between the two of those some of which
is resolved down to what is effectively
an infinite level of detail right it's
like a fractal Coastline you know if
you're if you're throwing a dart out of
the mandal BR set well if you throw
towards the middle of it you're probably
going to hit the mandal BR set but if
you're anywhere near the periphery you
know nobody's going to take bets on that
it's gonna even odds at best I mean you
know you you can kind of say if I miss
the mandle broad set altogether if I'm
like way out there at like you know neg5
or something you know yeah clearly I'm
going to miss the set but there's a
region between the two of them which is
fantastically
complex and so in effect what you're
what you're basically noticing here is
is that there is a very high level of
control
necessary on that boundary and that
boundary may be so complex that there's
no possibility of control if that
boundary is mixed too
tightly and so in effect we now are
starting to set up an inequality because
I can now characterize the minimum level
of control necessary can be very
conservative and assume that all sorts
of things are predictable even when you
know it's not a complicated system Z is
not a complicated system it is a complex
system which means that there are
degrees of freedom of motion in Z which
again are unknowable in advance and
biological systems tend to be more in
the complex region rather than in the
complicated region and so as a result
what happens is is that there's this
there's this lots of areas where there's
boundaries so yeah for some kinds of
signals probably going to be okay those
that prosthetic area narrow artificial
intelligence using technology to build
houses and things yeah it's relatively
benign it's not perfectly benign but
it's constrainable to things which are
probably going to work out and most of
the people that are arguing about this
again thinking from an analog
perspective are saying well let's just
avoid the whole regions where where we
think the control is difficult we're
just going to stick with the stuff
that's good well the problem is is that
I don't know what this thing X is going
to learn over time and how do I
constrain X to move from a Zone which is
you know known safe and eventually going
to move into a zone that is unknown safe
that means I have to know where the
boundary is but the thing is that I
don't know is nobody knows where the
boundary is yeah right so so it seems as
like um a an artificial
system would it be able to make
predictions about itself moving into an
um an unpredictable region no because
because again somewhere along the way
things like um like like if if I
basically have a a machine like let's
take a really simple machine for X it's
just a turning machine okay it's not an
artificial intelligence all it's
literally just a basic turing machine
the problem is is that somewhere along
the way I have to distinguish between
data and code and the problem is is that
if it's a learning system I'm not going
to know which is the data and which is
the code in fact for most turning
machines if I don't know the structure
of the turing machine and I'm just
putting inputs into it I don't know
whether those inputs are going to be
treated as new code or new data or both
right and so as a result predicting the
output of the tourning machine given an
unknown touring machine given an input
of data well I can maybe predict it to
some extent but it's probably going to
produce behaviors that completely
outside of the domain of what I can
predict and so in effect there's a sense
here in which if I'm looking at
something way more complex like an
artificial intelligence or super
intelligence I don't know how it's going
to interpret the data that it receives
from
Q so in effect I can try to constrain
what outputs go from X to Z using Q as a
modulator but now Q has to be able to
predict what's going on in Z but Q was
designed by human beings and so in
effect it's like okay we're not smart
enough to know where the boundary is and
we don't even know that X if it was
superintelligent couldn't create some
signal that would fake out Q to deceive
Q into believing that X should be given
more freedom to affect Z Now alignment
is off the picture again and again you
know help design Q like through some
form of indirect um specification
can't can't AI help us make q
more well okay so here's the thing
somewhere along the way right what we're
trying to do is is to still say is there
a way to
converge some version of Q so let's
basically say the Q is built into X is
there some way that a super intelligence
can account for all of the possible
effects of its own Expressions to Z
right so again on the X to Z boundary I
still have a minimum level of control
necessary condition see again it factors
out now we haven't even set up the
critical part of the proof yet all we're
doing is we're basically saying that
every version of this we somewhere along
the way have to have a way of
characterizing the minimum level of
control necessary on the Z I'm sorry X
to Z
boundary right everything is going to
reduce to that and then we going to
basically say can X or can q and x
together create that level of control on
the X to Z boundary that's the main
question right
now let's basically do what is a kind of
induction
argument let's look
at the level of control that would be
needed on the Q to X
boundary relative to the level of
control needed on the X to Z
boundary and when I say by induction
because I can divide X arbitrarily in
the middle somewhere and I can create a
boundary there and I can talk about you
know X1 and
X2 and I can basically say what is the
relation between X2 and z and X1 and
X2 and I could treat X1 as a kind of
control of X2 the same way that Q is a
control of X1 right so in effect one way
or another at some point or another I
can I can look at the relationship
between two
partitions okay and so in this sense to
simplify things I'm going to treat the
the the we only have three boundaries we
have Z back to Q we have q to x and x to
Z the minimum number of boundaries
necessary we have something we can
control nothing about something we can
control everything about and something
we can control only in the beginning and
then not
later or diminishing degrees as time
passes right so between everything
nothing and everything else we have all
possibilities so therefore these three
boundaries are the minimum number of
boundaries possible okay so let's look
at again because we've come up with a
mathematical way of describing the
minimum level of control
necessary I can use that same
characterization on the Q to X
boundary so say I had a control system
that is sending signals to X that are
limits on what x can
do with respect to
Z right so in effect I would basically
say that even
if x was a complete pass
through where it no processing at all I
just literally connect every bit going
from Q
tox one to one mapping to a bit that
flows from X to Z in this particular
case our artificial intelligence our
turning machine is nothing it is a
straight pass through no modification at
all it is a perfect Communication
channel with no errors in the shanning
sense okay Shannon entropy s obviously
no communication channel is perfect some
errors would creep in but let's ignore
that for now okay
so at this point x is a pure
conduit in that case we would say that
the control necessary on the X to Z
boundary and the control necessary on
the Q tox boundary is exactly the
same is there any version of X where the
level of control on the Q to X boundary
will be
less than the level of control necessary
on the X to Z boundary
no right if I'm if I basically am going
to constrain X relative to Z I need at
least that much information on the Q tox
boundary so the control level on Q tox
cannot be less than the degree of
control necessary on X to
Z right and again at this particular
point this is what is called a
unreasonable uh proof it's basically
like that is an uncontestable truth
right that what that proportion I just
set up okay so then we can start to look
at all other versions of X okay so in
this particular case we can treat X as
essentially the simplest possible case
let's call it X subot okay X is a pure
pass through Channel if I had um
something even simpler of no channel of
information at all every bit that goes
from Q to X none of those go from X to Z
okay in that particular case I have no
control right so the amount of control
that I would need to have in order to
basically constrain X to Z at least has
to be greater than the boundary of x to
Z now here's the
thing let's consider a like that that
collection of stuff that is X let's
consider that that to be a whole
collection of Boolean Gates right
because we're talking bits from the
input to the output and we've already
shown that there's an isomorphic mapping
between a bitstream and this this Dart
thing so in other words we have the
characterization of control theory in
terms of bits and we have this bitstream
going in and out and so therefore we can
model the internals of X purely in terms
of digital logic
gates okay now at this particular point
I'm now going to set up one new notion
this is the last New Concept I need to
introduce okay um I'm going to describe
something like the
hashing of the transformation of the
input stream to an output
stream okay the hashy is essentially
let's say that I take an input stream
and I use something like uh shha
algorithm or the md5 algorithm or I
don't know concatenate a whole bunch of
cryptographic hash
functions whatever permutations I mean I
don't really care it's a hash function
okay it's a good hash function right
okay the input stream is going to be
mapped to an output stream and now the
question
becomes what level of
control do I need to have if I have a
hash function let's say I have a good CP
cryptographic hash function for x m
okay what level of control do I need to
have on the Q to X boundary relative to
the degree of control that I have on the
X to Z boundary if I've characterized
the level of control on the X to Z
boundary what is the hash function going
to
do is it going to try is it going to
hide um
anything well the number of states on
the output side let's just say for
Simplicity sake that the number of bits
on the Q to Z flow are you worried about
collisions hash collisions that can
occur yeah right right because a hash
function May map a range of inputs to a
smaller number of outputs yeah okay um
there may be some places I can't reach
as a result of that not being a
bje right so there may be some signals
that I can't create as output for any
signals that I create as inputs
okay um and the mixing if I have you
know U all zeros or all ones or some
combination of zeros or ones or some
specific subsets of states in the output
messages which are known good versus
known bad that the
mixing is going to be
increased on the input side right the
level of control that I'm need to need
to have on the input side it may be the
case that
the region of white versus black will go
down because some of the white States
will be
unreachable and many states will map to
the same white state but it's also the
case that many input States will map to
the same black
state but whatever the distribution
is the distribution is going to be mixed
in every dimension of distribution
calculation that could be conceived of
that's what a hash function does
so in effect the the scattering of
points that are allowable good
points there's fewer points and they're
more scattered on the Q to X boundary
the level of control that I need to have
on the Q to X boundary has gone up not
just a little bit but like
enormously because if if the if the if
the area on the X to Z boundary that was
white
was small like a fraction of a
percent and its mixing coefficient was
already high so it's like a dust of
points like a Cantor dusting of
Dimension like 0.5 or0 75 or something
like that okay like a fractional
Dimension the scattering of dust right
between zero Dimension zero and
dimension one right that scattering is
fractally distributed the chances of
you're hitting a specific point requires
almost infinite precision but let's say
because we're talking about bits it's
quantized it's finite then in effect
there are there's a there's a sub region
of space that I can get to but that sub
region of space is now scattered
enormously such that the level of
control that I would need to have on the
bitstream going from Q to X
is at least enormously higher than the
level of control that X needed with
respect to
Z so therefore the question that we to
ask ourselves is that for any collection
of gates that compose
X if zero is a straight PA through and
one is perfect hashing and by Perfect
hash means that no simpler constitution
of gates can compute the algorithm of
how inputs get calculated to outputs
there's no shortcut algorithm that could
effectively undo the
hashen right and we know that to be the
case because there's no simpler
algorithm uh for for various hash
functions because if there was people
could shortcircuit uh cryptocurrency and
earn billions of dollars that way right
if I could calculate hashes faster than
other people can calculate hashes or I
could somehow predict ranges of hash
output values based upon some subtle
inner Dynamic then the hash function
would already have been compromised and
the flow of money in every form of
cryptocurrency in the world every single
one of them would now all of a sudden
become coll and it wouldn't be long
before the secret was out that
cryptocurrencies don't work
anymore so in effect this hash thing is
based partially upon the idea that I
can't I can't predict the output except
through executing the algorithm the long
way okay now this is important but I'm
setting it up for the Future Okay so
we're kind of on the home stretch now
if we basically say that the level of
hash we've now defined the notion of
hash for X between zero and
one based upon the degree to which there
might be a predictability of X that I
could use that's faster than x of
executing it X
directly then you know that that
basically would be an intermediate scale
of hash but whatever the degree of
hashing is greater than
zero that every level ashiness every
combination of gates that I so in other
words I can take any random collection
of gates between the inputs and the
outputs of X and I can now characterize
it in terms of this abstract notion of
ashiness you with me yeah basically
doing the proof okay so so in effect
what happens is is that we say the more
the hash coefficient goes up and every
collection of Gates has a ashiness
between zero and one
okay the more that the hash goes up the
higher the degree that the level of
control the minimum level of control
necessary goes up on the Q to X boundary
relative to the X to Z boundary so there
is a strict
inequality that says that the control
degree on Q tox is always greater
than x to Z for any level of hash
greater than zero
that's really
important okay because if the level of
control necessary on Q tox is strictly
greater than the level of control from X
to
Z based upon the complexity of
X then if it's the
case that X is not un simulatable except
by re-executing X then the complexity of
Q cannot be simpler than x this is um a
cybernetic principle it's basically
saying if I can't simulate X other than
by executing X if the hatch of X is high
enough that I can't simulate X then
there's no possibility that I can
essentially condition the outputs of X
based upon the inputs of X because I
don't know how X works so in other words
not only is it the case that the level
of control minimally necessary on the Q
to X boundary is strictly higher than
the X to Z boundary but the fact of
being able to control it depends upon my
ability to simulate X in the first place
and if simulating x means that I have to
implement X then Z I'm sorry then then Q
can't be less complicated than x and has
to have perfect knowledge of x
but guess what we have two additional
constraints on the reality of the world
right Q can't be more complicated than x
without being
X and if q and x are isomorphic then we
don't have q and the reason we say that
is because we can't know the state of X
because not only is the hash of X High
it's high for two reasons it's high
because it's a learning system which
means X is changing in time based upon
the of Z which we can't predict in
advance because to do that Q would have
to be more complicated than Z if I
represent Z as a Boolean system with a
whole bunch of unknown Gates which I
can't change unknown gate right unknown
Gates so in effect I can't predict what
Z's going to be therefore there's a high
level of control necessary on the Q of x
to Z boundary I don't know the structure
of X in the future because it's learning
and also because frankly if I could
anticipate what a super intelligence
would do I'd have to be a super
intelligence so both by cybernetics and
by principle of uncertainty and by the
hatch of X inherently the level of
control control necessary on the Q to X
boundary
is if the boundary from X to Z is
anything greater than I don't know some
really low
percentage the control necessary on the
Q tox boundary is effectively all of it
the control necessary is for sure higher
than the maximum level of control Q can
produce because Q would have to be both
super intelligence and so super
Intelligence Squared because it have to
anticipate everything that the ex super
intelligent would do and it would have
to anti anticipate everything that Z
would do and anticipate everything that
Z had an influence on X and all of their
interactions which is itself a
complexity Beyond even uh super
intelligent squared in Q and so in
effect if we're going to assume super
intelligence then that basically means Q
has to be even more Super intelligence
and super intelligence which is itself a
contradiction because that's not a
control algorithm anymore that we have
defined right we we're not we no longer
have any control over what Q is so
therefore if the if the stipulation is
is that Q has to be something that is
knowbly manageable to create alignment
in Z but we have no no definite control
over what's happening in Q and Q has no
definite control over happening of
what's in X and X has no definite
control over having what's Z then by
contradiction Q is impossible
Q that's
it well it'd be great uh is is there any
links to the mathematical formalism as
it is at the moment no Anders so ander's
job at this point is to translate what I
just told to you in the math have that
math be the thing you present to other
people I don't want to do it I mean I
have my own version of it yeah well you
can ver you
can off the internet I am getting out I
am I'm basically I it is not my job to
convince reasonable people to uphold a
notion of
reasonableness okay I want to live my
life and so at this particular point the
thing is is that I want reasonable
people to do reasonable things which in
this particular case is to take the
argument that I have just
constructed figure out whatever math
they need to model that in a way that
makes sense to them you want to be part
of that process well I'm willing to
review it but what I'm not willing to do
is to try to convince
people to do what is necessary to be
done right there's there's no way that
that I'm going to basically be able to
tell someone you should X I mean I can I
can say those words but but that's not
going to result in them doing something
they have to decide for themselves that
it's worth it for them to spend the time
to work through the process step by step
by step what I've just given is the
blazes I've showed the overall
construction of how to set up the proof
the math is relatively straightforward
the math is is uncomplicated relative to
what I just set up but the thing is is
that I don't want to be the one trying
to convince people I want Anders to do
that
and Andrew is more than competent to
take what I've just described to you and
turn it into something that that becomes
a white paper with his name on it and I
don't want it to be I said so I want it
to be we are agreeing and saying so or
this constellation of people has
reviewed this and found it convincing
right no amount of me telling anybody is
going to make it true it's just the
truth is evident in the inspection do
the work
I'll be sure to send him the link to
this video once it up though please I'd
be glad for it I mean like I said I have
huge respect for Anders Anders is a
force of good in the world so far as I
can tell and he's he's a decent person
and so in effect what I'm concerned with
is just you know trying to make sure
that all the steps are clear so that we
can see how this is constructed right
and and in effect what what happens is
is that with this sort of body of
process we don't have to agree on the
specific numbers of how robust nature is
what the structure of Z is we can
basically say it's within this range and
anything within this range is going to
result in this proof going through
because it's not about the specific
numbers it's about the
ratios and how the ratios are
transformed for any version of X and any
version of Q and any version of Z and so
as a result I don't have to agree on the
specifics of z x or Q we don't have to
know anything about the nature of
artificial intelligence other than
well gez you know we can simulate it
with Gates well that's basically what
all of the neuroc correlate people are
trying to do in the first place or we
can basically maybe say yeah let's
presume that Z exists and it has
definite physical laws and we know what
those are to this percentage and we know
biology to within this percentage let's
just spitball some control values for
that and notice what happens with the
proof everything else is by induction
and so as long as the numbers are
greater than
zero the inductive process results in a
kind of convergence with a inequality
showing that these inequalities hold and
therefore you end up with an
unreasonableness of the idea of
alignment that's what the end result is
regardless it's like the notion of the
generality is what makes the proof work
and people try to pin it down into well
what if this or what if that or what if
this other thing it's like do the work
show for each what if that that wh if
can be modeled within the context of the
dynamic that I just described the loop
that I just created and show that
they're all special cases of that more
General phenomena it's like Shannon's uh
information entropy it's like once you
understand it it's like this is full
coverage we don't need to look for
anything
else and againu
a what's that natural selection do you
think natural selection is at danger
here as well I mean like natural
selection produce humans which are
intelligent enough to produce technology
um and which end up being intelligent
enough to bootstrap at least the
artificial intelligence that we have
today well we're we're we're intelligent
enough to make nuclear weapons that nuke
ourselves into Oblivion I mean you know
like right yeah we're a danger to
ourselves because of our unconsciousness
of our power and we need to have care
transcend power in the same way that
transactionalism transcends care and
power transcends transactionalism care
transcends power but we're not yet
skillful in how we hold care so is it a
technical problem to be solved would you
say that uh I'd say it's kind of a
learning Evolution it's a kind of
philosophical problem it's a kind of
ethical problem but ultimately it comes
down to things like culture and
governance and how do we collectively
make choices how do we understand
ourselves as a species how do we
understand our role in the world or
Consciousness in the universe right like
those are kinds of things that we can no
longer remain ignorant about we actually
have to embrace these questions at a
sufficient level of depth to be able to
handle the kinds of problems we've been
handed change in nature has handed us
this on one hand beautiful opportunity
to rise to the occasion but it's like
you know the first time that you've been
handed car keys and you know you could
make a bad choice you can go to a store
and buy some alcohol because you're 21
now and you could drink and drive and
get people hurt including yourself but
that's not the responsible thing to do
right somewhere along the way we have to
hope that we have learned enough that we
have gained enough wisdom that we move
towards a culture and a governance
methodology that is wise enough and
preserving of wisdom enough that we can
handle these sorts of things because
frankly you know if we keep rolling the
dice if we keep playing you know this
game of Russian Roulette eventually a
die the categories of existential risk
like you know I'm tracking like about 30
categories of existential risk some of
them are extremely serious like the
substrate needs argument is one of the
single it's the top five it's one of the
single most serious versions of
existential risk that I know of the
others I won't name name or list right
now because again you know some of these
things are quite technical but but the
point is is that
our
capitalistic non-human istic but but
sort of of like uh modern way of
thinking in philosophy is leading us
down a blind alley on an evolutionary
scale and we need to basically recognize
things that Evolution right now isn't
like we can we can predict the future to
a limited extent and we need to take in
the reality of what's actually happening
to the best of our ability and to make
wise choices about that and we need a
culture that supports the values
of conscious sustainable Evolution
frankly how do you get to the culture
that will be able to do that I mean
culture drifts and sometimes it's you
know there's no there's no one at the
steering wheel right what how do we get
to a culture well part of the is we're
clarifying certain values like I think
that to a certain extent we want to
emphasize clear communication you know
communication that gets down to the
things that really matter and to some
extent it's it's kind of ironic because
the the the mostly autistic Spectrum
type people that are really good
scientists and engineers and
technologists are also the people that
really have no interest in small talk
and actually want to get down to the
things that really matter but they're
really uncomfortable with ambiguity and
uncertainty and so they want to bring it
down to discret concrete linear
arguments that can be understood in a
black and white way but I think the
thing is is that in this particular case
we're needing a comfortableness with
sometimes difficult things difficult
emotions difficult feelings and to
create clear
communication about the meaning of
what's Happening rather than just say
monetary values or functional
utilitarian purposes in the sense of
alism and more effective alism I'm
basically looking at how do we get to
the point where the communication that
we have with each other is clear enough
and meaningful enough that we can
Surface the values and the
meaningfulness that actually matters the
relevant things that we need to be
talking about so that collectively we
can have the wisdom necessary to address
these kind of concerns and we have to in
a sense be aware of our technological
optimism bias and our our causal
mechanism bias and our our our bias that
some version of the Invisible Hand of
Adam Smith can solve all problems that
some version of voting is going to solve
all our problems we've got to quit
looking for bullets and actually get
down to the point of having the kinds of
conversations where the reasonableness
that lives between us can emerge right
Harbor Moss was right haror Moss is
basically far as I'm concerned one of
the if not the greatest sociologists
that has ever lived okay yeah that he's
still Among Us is a miracle so the thing
that he pointed out which I think is
completely truth is that nobody
individually has reason it's how we
connect with each other that creates
reasons the conversations we have that
that emerge reason no one's individually
reasonable it's how we work together
that creates reasonable and we need to
get better at that kind of collaborative
process the skill of caring together of
living in community together of doing
the kinds of things that are
non-transactional and non- power
oriented that are skills in the space of
care and to do that we have to be in
touch with our emotions and our feelings
as much as we are with our intellect we
have to understand instincts and
intuitions just as much as we understand
mind and so in effect here we just
haven't developed the kinds of skills of
Attunement and discernment necessary
currently to build the cultures or to
have the kind of cultures emerge that
have the carrying capacity for this kind
of stuff so as far as I'm concerned it's
the humaning that matters it's the
renaturalization that matters and
frankly we've got to let go of some of
the delusions of modernism we got to let
go of the hype that some silver bullet
artificial intelligence can solve all
our problems it just ain't so and so in
effect it's like you know believing in
perpetual motion machines I don't want
people to Believe In Perpetual benefit
machines because that's how AI is being
promoted today it's it's just as
unreasonable as getting gold from the
philosopher stone I'm sorry that's just
not how it works that's not how
chemistry is is is constructed even
nuclear chemistry has a hard time with
that right but but in effect to think
about a perpetual motion machine is like
an infinite source of energy whereas now
we're talking of an infinite source of
intelligence I'm sorry it just doesn't
work like that it's just as unreasonable
and right now people are in the delusion
that they could create an infinite
benefit machine that's what general
artificial intelligence is sold as and
you know it's an investor they think
they can make 10,000 return on
investment yeah maybe in the short term
briefly but basically it gets everybody
killed do you want to talk about the the
substrate needs hypothesis then in more
detail you have discussed this outwhere
but I just wondering if our viewers can
get a taste of what that is well roughly
the argument is
this different substrates right so an
artificial substrate is different than a
natural substrate by natural I'm just
saying organic okay and I know that
physics is nature too but I'm I'm making
a distinction between something we build
versus something that just emerges
because of evolution okay um or organic
evolution let's let's call it carbon
based chemistry versus silicon based
chemistry and I know that there's
quibbles around that but for
simplification let's just say silica
versus carbon okay so in this particular
sense what we notice is is that the
needs associated with maintaining silica
based microlithographic silicon
microchips and the needs associated with
cellular process associated with all
mammals or trees or bushes or
whatever they are just really different
needs like carbon based life wants water
and if it's a mammal presumably
oxygen and some sort of carbohydrates
and so on whereas if I'm dealing with
silica based thing I'm mostly looking
for an absence of
water you
know cool temperatures when we're
processing and warm temperatures when
we're fabricating like to make silica
lithography I have to crystallized
silica that's an expensive process it
requires a lot of heat and Zone refining
and things like that
so in this particular sense this thing
works over a range of wide temperatures
that organic life just won't tolerate
right plus or minus 100 degrees
Fahrenheit and you've got no living
okay um whereas if you are basically
dealing with silica chemistry it doesn't
even get interesting until you're
dealing around 400 Degrees uh Centigrade
minimally and more like 4,000 typically
right and so in
effect almost all the stuff needed with
sustainability and increase in capacity
with silica based or artificial based
systems is a completely different needs
structure than that associated with
carbon
anything different needs mean different
basis of choice so now I have not just
that the needs of the artificial system
and the organic system are different but
that the basis of choice must be
different however we conceive of choice
however we conceive of intent or goal
seeking or motivations or whatever it
else is
driving the choic making process as a
kind of orientation right so alignment
is usually thought of the instrumental
sense of explicit
goals or goal refinement Dynamics
relative to some underlying goal right
now those can be explicit or implicit
but nonetheless different needs means
that
different goal or implication structures
are going to be at the basis of choice
making however you think of choice
making right so even in an input output
system that is responsive the
intelligence is in a sense the degree to
which the responses are correct with
respect to the environment and the goals
of the thing but those goals have to
have responses that support the needs of
the substrate or you don't have
sustainability or increase in capacity
right so in effect even if they're tacet
there's some influence on the nature of
the choices and therefore some influence
on the nature of the outcomes and
there's a Divergence now between
artificial substrates and organic
substrate that that that increases the
farther up this stack we go so in effect
artificial needs produce artificial
goals artificial goals produce
artificial choices different choices
with artificially shaped outcomes which
produces an artificial ecosystem or
artificial environment which cannot not
feed back into the
substrate so either that Loop is
convergent and there's an increase in
sustainability or capacity or the
sustainability of capacity or the
increase of capacity and sustainability
or any combination of those two words
that that over time is going to converge
more and more on the meeting of those
needs because even if they're T it any
failure to do so means that the system
loses capacity or loses uh thing so an
evolutionary Dynamic even if it's
composed of just the components of a
singular Arian super intelligence one
way or another this notion of feedback
is going to happen and it's going to
happen in a distributed way over time
because nothing is existing at a single
point in space right the the substrate
is always distributed and time is always
incur incurring and the notion of
sustainability and capacity is is either
convergent or Divergent then if it's
convergent then you end up with more of
it and if it's Divergent then you end up
with less and so in effect we're now
looking at the enthalpy of the
artificial system versus the organic
system right carbon versus silica and
unfortunately we already know something
about that enthalpy the enthalpy
associated with carbon based chemistry
over all the varieties of carbon based
chemistry and the enthalpy associated
with silica chemistry over all of the
enthalpy of silicon chemistry the silica
chemistry runs way hotter with way way
more intense energy than anything on the
carbon side which suggests an
incompatibility between the artificial
ecosystem versus the natural one versus
the carbon based one and this is where
the substrate needs argument starts to
basically come into
Focus because in effect you can show
that over
time the silica displaces the carbon
because the carbon can't handle the
kinds of molecular Construction
right most of them are toxic to the
carbon it can't handle the kind of
energy signatures most of which are out
of the range of what the carbon system
can handle and it can handle the
pressures and all of the other forms of
energy right silica based chemistry is
going to be differently able to handle
things like hard radiation than carbon
based systems are going to be right and
so in effect there's a there's a sense
in which there are some fragilities that
the silica based system has that the
carbon based system doesn't but overall
the carbon based system is way more
fragile than the silica based system
because point of fact is the carbon
based system is way more complex in way
more Dimensions over way narrower energy
Spectra and pattern Spectra than the
silica system is so as a result when you
look at the interaction Dynamics between
the two ecosystems the silica displaces
the carbon because it's essentially a
collapse in the complexity States it's a
reduction to a lower value lower energy
state in the hyperspace of all complex
system design so in effect the phase
change
Dynamics push the carbon out and it
happens
slowly but because of the convergence
Dynamics built into the different
substrate different needs different
needs different implications or basis of
choice different choices different
outcomes different outcomes different
environment back to substrate that
converges has to cannot
not and so as as a result you're now
actually looking not hypothetically but
actually at the enthalpy
Dynamics and regardless of which branch
or permutation you argue you try to put
a barrier between them eventually you
show that the barrier can't be
maintained except by the the high side
I.E the the the side of the silica the
silica doesn't have any basis by which
it can maintain the barrier because
there's no energy or economic basis by
which that can be sustained so as a
result eventually pin holes appear and
you only need a pinhole in the wrong
place and you kill the carbon now again
these are dynamics that happen over time
it's slow moving it's not obvious it's
not like a pandemic that you can see it
occurring it's like approaching on a
black hole you'll go right through the
The Event Horizon and never notice a
thing right spaghettification can happen
all the way on the Interior right from
the outside you know maybe you can see
the Dynamics but the we're on the inside
of this process we're in it we don't
have an outside in view unless we
construct one for ourselves using
arguments B based on the ethics based
upon the kind of proofs that I was
setting up earlier and so in effect what
we're doing is we're saying hey the
substrate needs argument is a
observation that there's a convergent
process plus an enthalpy relationship
between two different ecosystems the net
effect is is that the carbon blaz
ecosystem is destroyed with
exceptionally high
probability and then any control Dynamic
this was the first part of what we were
just talking about any control dynamic
that we would use to try to prevent that
from happening is fighting against
causation itself and requires perfect
predictability it requires not just that
the right control signals are sent
against a field of doesn't damage this
but now has to basically be the kind of
control signals that accounts for its
own effect on this which means that it's
a far narrower control signal than we
originally anticipated like before it
was just don't do no harm but now we
have this convergence process that's
spitting out all kinds of signals all
the side effects of the industrial
chemistry associated with making
lithography like a 100 layer microchip
that you're carrying around in your
pocket as a central processor produces
you know all sorts of side effects in
terms of chemistry that is incredibly
toxic to the environment I mean just
look at what's going on in other places
in the world and you'll see entire
deserts created by effluent of things
like just nickel for example right and
so in effect there's a there's an entire
ecosystem of damage it's occurring from
signals already passing from the silica
side to the to the carbon based side and
we're watching it happen today right
these are unconscious Dynamics built
into things like corporations and
artificial intelligence which
Corporation could be thought of as an
example of just in a very distributed
non um you know with with human beings
as organic components right but but
frankly it's the artificiality of the
system it's the virtualness of the
system the non-embodied of the
extraction the abstraction the
extraction abstraction and accumulation
that together are creating a thing where
frankly that doesn't have a future that
is just life can't continue with those
kinds of choices so in effect unless
we distribute right undo accumulation
right because all toxicity is too little
of something needed or too much of
something that you don't want right
that's all toxicity is an imbalance like
that and every technology is linear in
the sense of extracting from one place
and putting in another and you end up
with depletion in one area and and
toxicity in another so in effect what
happens is is that unless we do the
right sort of distribution unless we do
the right sort of embodiment move out of
abstraction back into the realness of
what is and then recontextualize in the
context of where we actually are and the
choices we need to be making today we're
making really poor choices with respect
to the substrate
and that's already happening organic
life is is at this point human beings
are disabling their own organic
substrate they're doing the Divergent
process whereas artificial intelligence
is doing a convergent process that is
not a gameable that's not a win game
we're already we're already losing and
we don't even have the other player
really fully embodied yet okay so you
know no matter how you calculate it the
the Dynamics of the two ecosystem
interactions is completely it's it's one
way no question about it it's like an
ant trying to prevent the incursion of a
bulldozer there's nothing the ant can do
the same sort of Way Beyond a certain
point there will be nothing humans can
do or the totality of or all organic
life that is going to be able to prevent
the incursion of silicon thing it's like
it's like uh crossing the Event Horizon
once you pass that threshold it's over
and you might not even know when that
happens there's no obvious signal
there's no alarm Bell that goes off
there's no warning there's no nothing
you just pass through the Event Horizon
and after that you're so in this
particular sense there's a there's a
sense here in which we need to notice
that we're in orbit around a convergent
process which will not quit and decide
that we're gonna add some velocity and
get out of the get out of the orbit of
this thing and that basically means to
some extent we've got to re-evaluate the
philosophy of modernism the critiques of
postmodernism are real but postmodernism
isn't an answer it's just a critique
somewhere we need to think about
rehumanization and renaturalization and
modernism is not that so you know there
are solutions to these particular
problems I can talk about them at length
but at this point I just want you to
understand the comprehensiveness of the
problem
statement do you think there's any any
space in the possible worlds where we
can converge both naturalism um
humanness and modernism do you think
it's ever possible I mean I think I
think modernism has a place it just
doesn't want to be the only philosoph in
the field and I don't even think it
wants to be the dominant one I think to
some extent you know we've gotten so
enamored with causal process that we've
learned we've learned how to use
causation but we haven't continued to
develop our capacity to care so yeah
it's not like I hate modernism I see the
problems of it and I see a better
philosophy and at this point I'm just
wanting to embrace the the better
philosophy because frankly someone has
to and where do we start we start with
conversations like this one you know
capitalism isn't necessarily evil but
it's doing some things which are pretty
damaging because we've just let it run
out of control because we don't have a
model of what else it could be but now
we can there is a way to have that and
it's not artificial intelligence it's
not displacing our capacity to choose to
sub machine we need to get better at
making choices for our own sake and
that's how we do it we make choices that
make sense for our own sake and here's
how we do that we understand our own
substrate we understand the needs of
nature so that we can have it be in
healthier relationship to us so that we
can be healthier we need to have
healthier relationships with each other
we need to know what that means we need
to understand that stuff and choose to
make choices in that direction and that
is entirely possible does that mean
modernism goes away no does that mean
fear goes away no but we need to love
more than fear that's
important right do you think artificial
agents can ever love I mean is this is
this a thing that can be sort of
designed can can can we make ourselves
more loving can we be can we more
en do we need machines to do it mostly
no we can use machines to help us to
become better at loving one another but
that's a skill in itself and that is a
very Discerning line it's like a
prosthetic a prosthetic can make it so
that you're more functional as a human
but it doesn't replace your
humanness so in effect what I'm really
trying to do is focus on better humans
and if we need Prosthetics yes let's use
the damn Prosthetics but I don't want
the Prosthetics to be a replacement for
the humanness I want the human to be
human how do we preserve the human in a
future of more and more Prosthetics or
more and more can we enhance the human
without the Prosthetics I mean can can
we ABS make bigger brains can we make it
so we have bigger brains for instance ah
you're still looking more cognitive
power you don't need more cognitive
power you need more heart you need more
guts yep can we do that yes of course we
can can we have
both uh I don't know that you would even
want both I mean you know at this
particular Point let's just start with
can we do that yes how do we do that
well we need to communicate in a clearer
way on on a deeper basis and we're
getting there I mean we're working to
that I mean you know I'm just meeting
you for the first time but you know I'm
gathering that you care about these
kinds of issues otherwise you wouldn't
have invited me into your call well I do
I mean I've been I've had my um show
since
2010
Anders yeah that's great I interviewed
Anders in in Oxford in
2012 I think it was yeah um and a bunch
of you met him before I did so you have
you have a longer relationship even so
in this sense um all I'm basically
suggesting is first of all kudos for for
running a Blog for that long um I'm I'm
curious about your experience now I want
to learn what it's like that I've been
doing that for S such a period of Time
how has that worked out but this isn't
me interviewing you let me just park all
those questions that I have for later uh
because we have been on the call a long
time and I I want to be uh respectful to
myself as well yeah absolutely
yeah yeah but I but I I want to
basically just say it is for sure worth
it for us to develop capacities around
how we hold care collectively how we
hold humanness collectively how we hold
naturalness collectively and it's not so
much what I'm against it's more about
what I'm for and that's the real benefit
like at this point if we know hey this
particular Road is closed to us and this
road over here is open to us it's kind
of like well J let's just go down the
road that's open let's go down the road
that works you know we can we can
continue to be in delution but that
doesn't help anybody
well um yes it's been really wonderful
speaking to you and I've really enjoyed
listening to you explain a lot of your
theory about the problems of aligning AI
uhuh uh I yes I am interested in
following this further and you be
interesting to get you in conversation
with Anders if some stage maybe well I
would initiate that I mean at this point
he's he's he's in touch with my
secretary and and um you know if as as
he has time he'll schedule the next
meeting I'm not trying to push him to to
to do anything but on the other hand I
am hoping that that that at least maybe
this conversation will give some uh
clarify some points and fill in some of
the details because some of this has
been laid out for him pretty pretty well
and and other parts of it are are now
available because of this recording and
and um if he has further questions or
wants to see how I would deal with
certain things that's fine
okay um okay if we got any conclusive
remarks you'd like to have before we
bring the the the show to a close um I
think the concluding remarks are um from
where we're standing it looks really
hard to basically move into future that
doesn't feel
terrifying um but on the other hand with
time and with a little bit of effort and
with some clarity it becomes easier and
easier to see a beautiful future and uh
hopefully the the fact that that that at
least someone uh again you know if my uh
Expressions here uh give at least
someone some confidence that that that
yes this is worth doing we should we
should basically pursue this because
it's worthwhile and can actually work
and that there's very good reasons for
us to be confident that things that can
work can work and the things that can't
work can't that that that we can just
make easier choices that over time will
will just seem more and more natural and
that this is just uh you know I I have
passion for people that have invested so
much effort into trying to build uh
things for the betterment of humanity
and now we're getting more nuanced about
what that
means well yes thank you so much for
your time and your wisdom and and for
yeah for addressing us today yeah I look
forward to chatting with you again at
some stage but um yeah it's been really
good and I'll put all some I'll put some
useful links in the
description yeah most of my site's gone
I I took it down down because I'm moving
my company and um I don't have a plan
yet as far as when that's going to
change because I've I've got so much
Logistics I got to deal with between now
and then so um anything that links to
mlb.com is gone um but the uh the thing
is is that uh raal Ellen is very
familiar with these arguments and
understands this pretty well so uh if if
you see any of his writings um he's been
developing this in in in in various
communities and and and can stand for
some of this stuff uh reasonably well uh
there's a handful of other people that
understand this argument too um and have
presented their own versions of it as
well and so um wolf tivy for example
wrote a paper that I thought was quite
good um but again you know these are the
kind of things where people have to
think through it on their own and you
know I appreciate the links and and and
and those are some references to places
where people can
go awesome well yes I'll look those
people up um I will see if I get the
right links what was the the first
person who's ralt Ellen yeah um wolf TIY
and um uh Roman oh man last yes that guy
yeah I'm interviewing him sometime soon
yeah um he's he's also worked in this
space but he did again an independent
version uh focused on the rice theorem
particularly and and um you know people
have encountered his work too um his
work and mine are basically parallel to
one another I think I've been more
thorough in in a lot of ways
um he's focused on an aspect which other
people would find maybe more
intelligible um but I but I I feel that
for the most part we've we've kind of
had our own thoughts and then compared
notes and now we've expanded to include
the other um but uh yeah like I say the
the three people that come to mind the
most obviously as being um you know
thinkers in this space aside from myself
would would would be those three people
I just I just listed awesome all right
cool well thanks everybody who've made
it this far it's um yeah but an
enjoyable Marathon like uh actually went
for the longest run I've been for in a
long time last night oh wow yes well it
it did have the benefit of presenting
one whole
construction pretty much as a complete
thing and and as I said there are other
parallel constructions that all arrive
at the same result but this one happens
to be one that I think is uh maybe the
most obviously formalizable which is
something that Andrew was asking for and
you asked for
um so in this sense that's that's kind
of like uh the the picture of it and
again it's just it's just one of many
but it's it's a good one awesome yeah
yeah having it fully formalized in in
maths I think a lot of people will be
able to sort of test it and and put it
through their way of thinking yeah
compare it to their way of thinking
mathematically I think it would be
awesome yeah great so thanks very much
Florest okay you have a great night good
evening

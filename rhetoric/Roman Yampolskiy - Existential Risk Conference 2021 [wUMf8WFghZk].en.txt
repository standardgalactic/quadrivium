[Music]
[Applause]
we are a computer scientist at the
university of louisville and specialized
in behavioral biometrics security of
cyber worlds and ai safety and indeed
one of my personal first steps in the
world of existential risk was to read
the number of your publications
and i think you may have inspired many
like me to start working on either ai
safety or existential risk or related
fields
and i think you also have always
interesting and thought-provoking
comments available on each topic uh your
social accounts for example so i really
enjoyed reading those and i can
recommend them to everyone
so we are now honored and it is a great
pleasure i can say to be able to learn
from you today and i will give the stage
to you now dr ramayan polsky
thank you so much uh wonderful
introduction i really appreciate that
uh i think the plan is that i'll give
about a 10 minute intro to my latest
work and then
we'll
let me set up the slides
okay where is it uh where'd it go
yeah that's right we can take a little
bit longer as well if you want to and
then we can move to the fireside chat
and then the q and avoid audience can
you see my slides yes
excellent
so uh my name is dr yampolsky i'm a
faculty member at university of
louisville i've been doing work on ai
safety for about 10 years now
give or take
and my latest work covers
what i will call impossibility results
problems
we encounter with actually accomplishing
what we think is necessary for us to
do
[Music]
not just development work for ai but
also work in terms of control and safety
if you
would like to learn more about my work
after this
talk you're definitely welcome to follow
me you can follow me on twitter follow
me on facebook i always encourage you
not to follow me home it's very
important
so
let's start with
the big problem right away
if you
heard previous talks at this conference
i had a chance to hear a little bit you
know about
concerns a lot of people have
with advanced artificial intelligence
usually called ai safety ai alignment
problem
sometimes ai control problem
the question is the problem we're trying
to solve is how can humanity remain
safely in control while benefiting from
superior form of intelligence so we want
this very capable agent to do work for
us to be helpful but we want to be in
charge we want to be able to undo any
changes if we don't like and we want to
be
final uh decision makers as to what's
going to happen and the good news is
after you know 10 years of building up
the movement there is a lot of people
working on this now there is a lot of
research labs a lot of
phd programs
but i think the main question of
control problem has not been addressed
sufficiently and that is
is the problem actually solvable so
everyone kind of works on it but i
haven't seen much in terms of
proofs or even rigorous argumentation
for why
we think we can do this why is this
problem solvable is it solvable is it
partially solvable
maybe it's not solvable maybe it's a
silly question it's not even decidable
so that's essentially what i've been
looking at uh
for the last couple years
and i started by formalizing a little
bit what i mean by control problems so
we can talk about different types of
control
we have explicit control where you give
orders and the system follows
and this is the kind of standard genie
problem right you wish for things then
you realize that's not what i meant and
you hope you
get to undo the damage
then there is implicit control so you
have
a system with a little more common sense
it doesn't take your words literally it
tries to kind of parse things in the
right way and
there are intermediate steps between
explicit and delegated control delegated
control is the other extreme where you
have this super intelligence system very
friendly it knows what needs to happen
better than you do
and you might be even very happy with
the decisions it makes but you're not in
charge the system is completely in
control
align control is another intermediate
option where the system kind of
understands your values human values and
tries to
do the best it can to fulfill those
values even if your
spoken directions are not exactly
what
maps under those ideal values
so it seems at least
from now
that some sort of intermediate
value between total control
and total autonomy for the system is
necessary for us to
be happy with the results
if uh
system is completely autonomous we have
no control over it by definition we lost
control
uh if a system has no autonomy it's
completely deterministic we decide ahead
of time what's going to happen it's not
very useful it cannot be generally
intelligent it's a great way to do
uh simple tasks for a narrow ai but it's
not something
we can utilize to solve
you know completely new problems and new
domains help us with science help us
cure diseases and things like that
so what i try to do is kind of break
down the bigger problem of control into
all the ingredients we need all the
tools we would need to make
controllability possible
so
what do you need
you can start thinking for yourself if
you want to be in control well you kind
of have to understand the situation what
is the system doing can the system
explain what it's doing can you
comprehend the explanation that's
another very important one can you
predict what the system is likely to do
maybe not just direction in which it is
going but specifics what steps will it
take can you verify that whatever it is
you want the system to do and programmed
it to do is actually going to happen so
can you verify the implementation versus
the model can you verify the model
against your goals and data and so on
there is also need for
general ability to govern ai research ai
systems so governability of that and of
course we communicate with systems in
human language
and
we need to make sure that communication
we use is unambiguous there is no way to
misinterpret commands
uh in a potentially dangerous way and
there is many many more such limitations
and what i've been doing kind of looking
at each one and trying to publish those
results so i'll go over some of the
publications i have on that
so far
one paper talks about limits to
explainability and incomprehensibility
so essentially for very complex systems
large
neural networks it is impossible for the
system to provide an exact explanation
of what it is doing or why
without simplifying it to the point of
where it is like you explaining
something to a child so a lot of
important details that he moved and then
a very simplified version is given to
you because if a full version is given
to you you'll simply not be able to
comprehend it and
if you want to learn more and see the
kind of argumentation in some cases
proofs just go to the paper i provide
all the information you need to get
access to those papers they're also
available as preprints and my google
scholar account
another impossibility result is
unpredictability and that's our
inability to
precisely predict all
decisions all intermediate steps a much
smarter agent will take
of course if we could predict all the
decisions of a smarter agent we
ourselves would be that smart and uh by
definition there wouldn't be much of a
intelligence gap cognitive gap between
us
there is also problems with
verifiability
we know for a fact that with software
with mathematical proofs we can only get
a certain degree of confidence but never
100 confidence in uh the fact that we
have no errors in the proof
so with more resources we can increase
safety and security but we never
are able to guarantee something with 100
accuracy which is a problem for super
intelligence system which makes
potentially billions of decisions every
minute even if one in 100 million it's a
problem you're guaranteed to have a huge
problem within a minute or so
there are also problems with governance
we have history of trying to govern
technology things like spam and computer
viruses we have laws
uh against
those
malevolent software products but they
don't seem to be doing much so it's not
obvious how much benefit is actually
added and other negative consequences
from trying to control research control
what is
allowed and not allowed to be
experimented with
and even the orders we give to the
system the communication channel through
english is very ambiguous and you almost
guaranteed to run into situations where
the orders will be misinterpreted at
multiple levels due to how
imprecise human languages are
so what we did with a colleague we kind
of
surveyed a lot of those and possibility
results those i looked at and those
other people have looked at i'm not
going to go into details of all of them
i can just tell you there is a lot of
them and some
purely software problems mathematical
problems many uh problems with physics
of the universe
impossibility results from physics
but if you think even a small subset
although of all those tools is necessary
to solve a control problem you have to
come to the conclusion that uh control
is not possible at least not 100
guaranteed safe secure control we all
dream about and i have a very lengthy
paper about that about 70 pages i now
have a few subsections of it published
coming out in conferences those should
be a lot more readable but i think i
bring up a lot of interesting questions
and additional directions for research
and hopefully in the next
uh half hour or 40 minutes we can talk
about uh
what all this means and how we can move
forward from where we are right now
thank you very much
roman for this uh this introduction
already
um yes we can now have a chat about
indeed where does it leave us and uh
where should we go from now and also a
couple of related questions and towards
the the end after about 15 to 20 minutes
we will also take questions from the
audience
so please if you have any questions then
uh please type them into the chat
or in the question section rather so
first uh you're spending quite a lot of
time researching ai existential risk but
i don't think it's already obvious for
everyone in the goal why ai would be a
danger at all and i don't think uh
everyone is uh
perhaps 100 convinced that this is
actually an issue
or an existential danger at least that
is could you please recap how exactly ai
could become an existential risk
according to you
right so there's a lot of ways to get to
that conclusion uh i have a few papers
where i simply collect examples of
accidents ai failures throughout history
and if you look at that progression it's
kind of same exponential chart we see
with development we get more problems
the problems become more severe
and
our ability to
anticipate and predict them seems to be
very limited so basically the conclusion
is something like if you have a system
of service to do x eventually it fails
to x frequently it does so very quickly
and you go
okay my self-driving car just killed a
bunch of pedestrians that's a problem
and then it's a narrow system the damage
is limited right so self-driving car
okay the worst it can do is
run through some pedestrians but if a
system becomes general and it's now
controlling not just a single car but
networks of cars nuclear response
airline industry stock market the damage
is proportionate
i think it's uh also not the best way to
assume that i have to prove that this
service or product is dangerous
whoever is developing and releasing it
has to prove that it is safe that is
standard liability law for any product
show me that this system which is
smarter than
me is smarter than you smarter than all
of us we'll never do something we didn't
anticipate something dangerous something
we don't want it to do
this proof could at any point be
possible or is it within the
impossibility uh realm of your theory
well i think i'm arguing that it's
impossible to do so and not just because
it's impossible to prove that but it's
impossible to get to that level of
performance you can get progressively
safer and more secure because you can
look at specific sub domains
you can limit what the system can do in
certain situations but you have an
infinite space of possible
problems so
it's very hard to prove
deterministically that you considered
all of them
so if ai safety would indeed be
impossible what does that imply for ai
safety research does this imply anything
for ai safety research is that would
that be a waste of time or is it
still something that we should pursue
not at all i'm doing it more than ever
so think about mathematics we know in
mathematics there are many impossibility
results you cannot prove certain things
in general uh you cannot have
proofs with 100
confidence it doesn't stop
mathematicians from discovering new
beautiful mathematics we know in physics
there are limits to for example speed
fundamental limit you know speed of
light that doesn't limit us from doing
great work on faster cars and faster
rockets it just tells you that
there are limits to what can be done and
so you should a not waste your resources
trying to accomplish that like knowing
that perpetual motion machines are not
possible is helpful result in physics
same here we need to understand what we
can achieve and then concentrate on what
is actually solvable instead of trying
to create
magical devices which cannot work
next one for ai to become an existential
risk it's commonly thought that it
should first outsmart humans
how big do you personally think the
chances that this will happen at all
and which probabilities do your fellow
ai scientists assign to this
that ai will ever become as smart as
humans exactly yeah
well it's a guarantee and then
we have proof by existence right if you
just copy human system you got
same level we also kind of give a lot of
credit to humans
because we
tend to think about einstein and similar
type
humans as typical examples every human
is quite dumb so it's not that hard to
get to that level
um and how are you on timelines
um of course
you you hear quite uh
values that are quite far apart some
people i've heard i think elon musk say
that there was a five could be a five
year timeline up until agi that's on the
progressive side
uh on the other side there are people
that claim it would take hundreds of
years
about where are you on this line and how
certain are you
that's a very hard question no one knows
for sure and no one
can accurately predict something like
that but
if
our current theory is about
how
systems scale all right
meaning if you just add more compute add
more data you keep making progress then
it becomes a question of course how much
compute are you willing to purchase to
get to that level and
do you have finite resources or
you know what is here 200 billion
dollars now so maybe at that level seven
years is a reasonable estimate with my
budget it might be 20 40
depends on uh what type of resources you
have if it's uh
also
as difficult as let's say manhattan
project was right you need the resources
of a whole country to get there it's one
question if we discover okay there is a
simplifying assumption so we need a lot
less resources to train this type of
engine a lot less compute maybe you can
do it with a laptop and a garage and
then it becomes a lot more affordable
and takes less time so i don't have
specific dates i would be surprised in
you know maybe 20 45 if i don't see
something at human level
but uh that's not important to the
argument at all whatever it's 20 45 20
70 it's still
something we need to worry about today
control and work and safety aspects of
it
i've read somewhere that there are about
70 research projects explicitly aiming
for agi at this point i guess the most
famous two ones are deepmind and openai
at least at least the ones i know best
do you know a project that's we've never
heard of but uh actually has a fair
chance of beating those two
well there could be many secret projects
by you know
secret agencies i'm sure nsa is very
interested in processing your data more
efficiently so i'd be surprised if they
don't have something good happening
usually if you look at
history of what they
publicly
released and what will later learn they
had i think they had public key
cryptography like 30 years ahead of
everyone so
maybe already
interesting thought uh
yeah so
uh a week ago you posted on the the
facebook timeline that i referred to
already uh which is quite interesting uh
a quote from that helped me to
understand where is the number of highly
respected people who one argue that
advanced ai is dangerous to humanity and
two work as fast as they can on
developing advanced ai and there were i
believe 116 comments under your past
have you come any closer to
understanding this personally
no
i had some good explanations and the
best one and i think that's the one elon
actually gave himself was saying okay if
we can't control it i might as well be
the one to get there and i have the best
chance of controlling
people who don't care about safety have
less of a chance so
but it is interesting so a lot of very
big names in uh
arguing that ai is extremely dangerous
are also people who invested the most
time and money and making it as fast as
they can
yeah and on a more serious note some
people might say if ai is so dangerous
can't we just not build it um you said
something about regulation in your talk
but what would you say to them as a
general response you can stop progress
on something so useful and so fuzzy in
terms of separation between narrow and
general ai if we could make it where
okay you only can work a narrow ai but
not allowed to work in general it would
be a good moratorium to have for a few
years but the dividing line is
meaningless if you're using neural
networks they're general if you're using
a lot of those latest evolutionary
techniques they're
leading you to general solutions so it's
simply impossible if you make all
computer science illegal you're killing
your economy you're shifting research to
other countries so i think i'll add
another impossibility result of
unbearability of ai you cannot bend
ban it you can maybe delay it at best
would be very interesting if you could
either include or exclude this from the
impossibility uh space indeed but i'm
afraid that that goes more into
seymour friedrich's
chaos theorem so so to say
you and i both agree i think that ai is
a significant existential risk
but some ai researchers don't agree and
do you think there will ever be a
scientific consensus about this and can
we hope to achieve that at some point
and why could that be either so or not
well i have a recent paper about high
risk skepticism and i do a review of
both why would someone not accept the
risks as real and
kind of specific arguments they make for
it i think it ended up with about 100
citations and i have another 400
unprocessed ones if anyone's interested
could be a nice survey
the most common
explanation i see is just bias if you
get your funding your prestige your
reputation everything from developing
faster ais it's very hard for you to say
i'm working on the most dangerous thing
in the world that will kill everyone so
there seems to be uh this conflict of
interest in any other demean we wouldn't
allow you know for this to happen if you
are working for a tobacco company you
wouldn't be deciding if smoking is
dangerous if you work for an oil company
we don't really trust your assessment of
impact on climate but somehow here
it's fine and interestingly
ai is a very large umbrella term for
lots of research sub domains some people
do natural language processing some do
vision not everyone does safety and
security but we feel that anyone with a
label of ai researcher is qualified to
pass judgment on the state of ai safety
in software development not everyone is
a cyber security expert if you're
working on back end gui something else
you're not going to be consulted on how
to do
you know encryption
why is this somehow different here i
don't fully understand
it would be interesting indeed also to
to find out
i'm also kind of puzzled but perhaps it
could have something to do with the fact
that it's uh it's
not a trial and error risk as one of the
few
areas i think mostly of course you're
first developing something and then uh
later uh you you regulate it but it's
only in the face of application so at
this phase it's much more obvious to
have uh uh
separate uh controlling agencies perhaps
but uh when you're creating something of
course that's uh uh that's not that
obvious
um
maybe one more question about uh also
impossibility of the aislet but then
from a different angle i don't know if
you are
aware of the the work of anthony verglas
he has written a book about the
evolutionary arguments
applied to ai and it roughly goes as
follows
for super intelligence being friendly to
people is unnecessary baggage because of
evolution we should expect only the most
efficient super intelligence to survive
and this is probably not the
friendliest one
would you agree to this evolutionary
argument applied to ai or what what are
your thoughts about this idea i haven't
read the book so i'm trying to get the
argument from your question so the
argument is that it's more efficient to
be friendly to humans and so it's a
survival advantage and the other way
around it's more efficient to be
unfriendly to humans so that would be a
survival advantage because the the
friendliness uh would just be baggage
according to him and it's all in terms
of its overhead and development being
friendly limits your space of
possibilities
uh yeah i think there is a lot to be
said about the treasurer's turn option
it starts very friendly gets the
resources and help early on and once
it's capable it turns on us and removes
all restrictions
so sounds like a good book
i think it is he should read it
all right i'll put it on my list of 600
books today excellent
um it was a market is very poorly so i'm
not surprised at you that you didn't
really but i think the idea is
interesting indeed
[Music]
um
are you more on the slow takeover on the
fast take up sites and uh why would that
be
very fast once we get to human level it
goes super intelligent almost
immediately just adding existing
capabilities like infinite memory access
to all the human knowledge since they
fear super intelligent if you just take
human plus internet
and
why do you think i think the slow
takeoff site kind of gains momentum the
the last years i don't know if you agree
and i've heard that this is just because
it's more easy to write papers about
this or there are more possible uh
yeah stories that you could uh
could tell about this but
um do you
around you see a shift there do you see
more people going towards the slow
takeoff site or
is that
is that not true
i haven't surveyed i honestly don't know
if you think there is a shift biased by
ability to publish about it i believe
you
i wouldn't make that claim uh too
strictly
okay
let's say that you're a non-ai expert
and you still want to do something about
this existential risk such as we are
kind of
what what action do you think would be
the best to take
so you're not an ai researcher but you
want to do something about
yes
is there anything at all would you just
say okay
just leave it to the experts because
there's not much you can do
i mean in general i think it's good if
citizens are well informed about the
world and problems and so the next time
you vote you don't vote for someone you
like visually but
actually picking better policies uh it
seems like
based on age and experience of people we
elected at least in us they're not
experts on most advanced technology
i hear many of them don't use computers
so
i'm skeptical that they can keep up with
crypto economics and cryptography and
synthetic biology and other interesting
questions so your job as a citizen is to
be informed and make sure
your viewers your informed views have
already presented
right some uh questions from the the
audience to uh um to not finish off yet
but we're getting towards the end of the
conference already
um first one who do you believe is
responsible for the safety of ai the
consumers governments or developers or
some other stakeholders
[Music]
so that's another interesting question
the ownership of ai itself is very
difficult right if it's self-improving
it changes it's not even obvious who has
any control or possession over it
obviously the person to make it and
release it has a lot of responsibility
but if it's out there and now you are
upgrading it supplying it with goals
giving it data it feels like
responsibility may shift to you
all of it for systems uh below human
level performance it's a tool you are in
charge the moment it's human level
beyond
it's an independent agent you are as
responsible as you are for your adult
children
another one that's also very interesting
i think is it possible to program in a
programming language not based on human
language to remove the ambiguity or
would it be possible to have an ai
create a language without ambiguity
if the ai could create such a language
would humans be able to learn it or
would we then also have to trust the ai
to program in it
that's an excellent question so there is
a lot of effort first of all every
programming language is an attempt to
get away from english and into less
ambiguous languages but we know
languages programming languages have
lots of bugs there are logical languages
developed to remove ambiguity and i
think stephen wolfram has a nice article
about communicating with ai and if he of
course uses his mathematica and models
he creates and language rule from
language he developed as a possible
solution i think you can do
way better than human language in terms
of ambiguity i'm skeptical about bug
free communication
it relies on your existing
cognitive models your understanding and
if you have different priors even using
well-defined terms may lead to
to problems but it's a very
interesting area to do additional
research if you have background in
linguistics i definitely invite you to
to look into that
another interesting one from uh simon
friedrich actually a preview speaker
do you think agi could help overcome the
global collective action problems that
are at the roots of basically all the
existential risks including those of ai
itself
so that's another great question i see
ai as a meta problem and meta solution
right if we get it right if i'm wrong
and you can make friendly super
intelligence well aligned everything i
said is just a mistake then it solves
all the other existential problems
trivially whatever is climate change
synthetic bio you have a
god-like tool for solving those problems
if i'm right and it's a terrible risk
and it comes before
then it solves it by even killing all of
us we don't have to worry about it or
it comes before again so if it takes a
hundred years for climate change to
build up to boiling point this happens
in 20 years it kind of dominates the
risk
i'm not sure about applying ai to solve
the ai problem that's a bit of a
catch-22
there are those solutions where you have
a supervisory agent ai agi nyani which
looks after
the world making sure no one creates
dangerous ais i'm very skeptical of such
super agents with a lot of government
control powers i think they may be worse
than what the system we're protecting
against
gives us
great answer i think
uh one uh
final question from the the audience now
uh so if we cannot stop ai development
and we cannot totally ensure that it is
safe
do we just need to accept that it is a
risk or even a big risk or is there
anything we can do for example policy
wise
so i think we need to do more research i
published those papers about a year ago
and i haven't seen a strong response
from a community addressing those if
somebody just you know published a paper
saying this is why you're wrong god be
very happy but i haven't seen it so i
have to assume that there's some merit
to what i'm saying
uh the question is then what do we do
with our lives how do we
you know update based on that what do we
change
for most people i don't know if it makes
any difference before you were told okay
you're definitely dying in 60 years now
you may be dying in 40 not a big update
figure out what to do with your 401k
plan you know do you spend it on
something now or wait for it to become
worthless later
i don't have any magical solutions or
answers i am curious
in case of successful alignment what
happens to economy what happens to
work what happens to people's
social interactions i do have a paper
which
kind of assumes that progress in virtual
reality will be as good as progress in
ai
and so each one of us gets what i call a
personal universe where you basically
get to do whatever you want and you
don't have to
negotiate with others there is no need
for consensus you basically
have independence so at least the
difficult part of value alignment
problem is not aligning with me or you
it's hard it's hard but it's not
impossible it's getting eight billion
people plus all the squirrels and what
not to agree on something
and this is where the personal universe
solution reduces it to just now we need
to control the substrate if you can get
control of computational substrate and
everyone gets the resources to run their
personal universe okay we're doing well
we have this virtual agreement
i think that's yeah that's
not a good point again i'm i'm wondering
also on a
on a more personal level like when did
you start to think about ai safety
yourself and when did you move into this
research field was there anything that
inspired you
to do this and also what were the the
responses that you got from fellow
scientists uh in moving in this
direction
well it was a very gradual process so i
was doing research on behavioral
biometrics i was profiling poker players
to see if you know accounts get hacked
or if until the things like that and at
the time i realized majority of online
players are now bots
so my work started to be about detecting
bots
preventing bots from participating but
the question was as bots get smarter and
better can we keep up and not just in
poker but in general online bus and
automation
i did that type of work for a while i
went to
what was at the time singularity
institute for artificial intelligence
which was fighting hard against
artificial intelligence but they had a
lot of great ideas which i still work on
and i've been back
as a fellow and a research advisor for
machine intelligence research institute
i think they're doing excellent
theoretical work
yeah i think
perhaps one more like uh some ai
researchers i think might be ask them to
talk about existential risk in the
public debates like you already quickly
mentioned for example in the media um
do you agree that they are asking to do
that and why do you think that is so
i i think i lost a few words so with
media is it what's the concern oh sorry
i'll just repeat the whole thing i don't
think then
some ai researchers might be hesitant to
talk about existential risk in the
public debates for example in the media
do you agree that this is so that they
are hesitant to do this and if so why do
you think that is
well
it's a personal decision based on your
situation so some people before they get
tenure
follow a very good advice of be quiet
after you get tenure never shut up again
but
that's not a bad idea you'll definitely
get someone disappointed in you and that
doesn't help your tenure case i'm
tenured so i've been saying stupid
things for years now
yeah
what do you think about an initiative
such as the existential risk observatory
is it
is it useful to to communicate this to
to
more people in general or to a certain
subset of people um or do you think it's
basically something that should be
solved among researchers
well if you think about developing agi
working on super intelligence you're
really running an experiment on all the
humans right you got eight billion
subjects none of them consented to that
work the least you can do is tell them
about it
i think that's actually great now to uh
uh to end this talk if there's no more
questions from the audience and i think
we've covered those
yeah then uh i think we'll listen here
it was super nice talking to you and uh
super nice to listen to your uh
your short presentation
and i hope that you will uh also enjoy
the rest of the conference maybe
tomorrow and that will definitely be in
touch and to cooperate more on this
quite hairy problem but still very
interesting one to think about
absolutely and hopefully we'll meet in
person one day likewise
[Music]
[Applause]
[Music]
you

[Music]
thank you
[Music]
faith
welcome to today's conversation about
what is going on in the world of
technology uh to Beyond conversations my
name is David Orban and today we are
going to talk about uh the edges of a
very popular topic artificial
intelligence has been around for several
decades but it has come to the attention
of a lot more people than before
not for the latest Hollywood movie
representing the particularly gruesome
dystopian
Adventures
finally resolved with probably a fist
fight that is what happens in those
movies but it has come to the attention
of hundreds of millions of people
through uh
AI systems that appear to do a lot of
useful things in novel ways
specifically chat GPT and its various
embodiments and inclusions
in various applications or in
Microsoft's search engine Bing and the
edge that I am talking about here
is not the current version of AI which
is a narrow AI
but AGI artificial general intelligence
which we don't have yet
but with better think about and I have
as our guest today Roman jampolsky who
likes to think about what AGI can or
cannot do and what we can or cannot do
about the AGI welcome Roman thanks for
inviting me again
uh yes indeed we did have a conversation
uh two or three years ago and I thought
it was time to uh regroup
to uh well uh ask ourselves uh what
changed or what did not change in the
meantime but before we go into the thick
of our conversation why don't you uh
tell our audience a little bit about
yourself
um uh what you do your field of study
your Publications your line of thought
sure I'm a computer scientist at the
University of Louisville I've been
working on AI Safety and Security for
well over a decade
have a couple books published on it here
and there and uh probably
over a hundred papers on that subject uh
that's what I do
fantastic so how do you uh Define the
the difference between uh narrow Ai and
and AGI
so it's becoming a bit fuzzy it used to
be that a narrow AI was a system which
did one thing and maybe did it well but
just one it played chess so it did Texas
or something like that general
intelligence is
capable of learning multiple skills
transferring skills between domains but
I think there is some confusion between
human level intelligence and general
intelligence
humans are General in a domain of
expertise of humans we don't know much
about things outside of that domain a
truly general intelligence would be
capable of learning all those skills
what we're starting to see is systems
which uh
can in between they are still not
General in that they can pick up new
capabilities in any domain but they can
easily have hundreds if not thousands of
skills they play chess they speak French
they can manipulate robot arms so
they're kind of moving from this narrow
one trick pony to becoming more and more
General and that's the research progress
we see and more lately we are starting
to get surprised they have skills and
capabilities within explicitly
programming so after you train a model
you train gpt4 then you start
discovering what can I do and the
designers developers get surprised that
in fact it has thousands of new skills
we didn't anticipate and at some point
we'll be surprised to discover it's
fully General
um so the
past experiences with AI were narrow as
you said or a given particular skill
and even among AI researchers it became
almost kind of a joke
as soon as uh some particular problem
was successfully solved it stopped
becoming it stopped being considered an
AI problem anymore like like chess for
example
and this interestingly reflected on our
own classification both
practical and epistemological it was a
reflection also on our own uh thought
processes cognitive abilities and how we
go about attacking certain problems
it
is something similar happening in the
field of AGI as well are we
reflecting on whether
to start humans are General
intelligences I can freely admit that
if you list
my skills
probably you will stop sooner than
ten thousand maybe you will sooner than
five thousand I didn't try
but uh you will pretty rapidly
be able to say oh David is actually not
a general intelligence
well I think it's about capability to
pick up new skills so if you wanted to
learn to play a new musical instrument
you could
whereas narrow AI systems couldn't and I
think that's the fundamental difference
it has potential of learning new domains
and new skills not necessarily already
has all that capability so an average
human knows very little but if you take
Humanity as a set and take a union of
all the capabilities yeah we speak
thousands of languages play hundreds of
instruments and we're quite impressive
as a group well
you made two two different remarks so
let's talk about the musical instrument
and I I play very very very little
guitar I play practically no piano and
that's it so yes if I applied myself I
could improve my piano playing skills
but it is likely at this age uh that I
would never become a concert pianist
right let alone one of the best pianists
in the in the world the same with chess
I I know the rules I play but
um my my uh
alloy score is uh probably
tiny and even if I apply to myself I
would not be able to improve it to the
level of you know any anyone worth
mentioning so
at every individual the theoretical
possibility of learning is there but a
practical possibility is fairly limited
uh both by time and circumstances and
and latent Talent
right
so I noticed with doing this with AI
right now it used to be we were like
maybe it will get to level of an average
human now we're like it has to be like
Einstein or it doesn't even count uh the
standards uh kind of the goal posts are
moved so I think if you don't expect an
average human to be the Best comedian
top inventor poet and everything else
you should not have same expectations
for uh software
okay and then you
went and and rather than considering an
individual human as as I uh volunteered
to be examined
you
considered all of humanity
and said well as as a group all of
humanity is capable of
remarkable things and I agree for
example we have created a civilization
that is itself now
in the process of creating ever smarter
artificial intelligences
so Humanity
as as a group is
um
a general intelligence
we for sure don't have
more than one reference point
with respect of how General and
intelligence can be
um so
do you expect
um
the blurry interval between what used to
be narrow intelligence and what is
becoming more and more general
intelligence
to uh be fed by human exploration
in the sense that
it is the coupling of human intelligence
with ever better AI models that will
endow those AI models to learn new
skills rather than
AI models being able to
learn new skills without human
involvement well at least for right now
it seems that they still scale without
any diminishing returns if you put more
multimodal data with more compute they
continue improving and going beyond our
expectations I mentioned Humanity as a
whole because their definition of super
intelligence relies on being better than
all humans in all domains so that's kind
of the next uh expectation level uh
Kurzweil talked about simulating one
human brain by around 2023 and all human
brains by around 2045 so that's the two
kind of interesting points we're
starting to maybe be between
um and
when I refer to
um AI systems being able to learn
without human involvement
is uh What uh is is transfer learning
what I I refer to uh how good are
systems today in abstracting their
training and then applying that training
to an increasingly disparate uh set of
circumstances
it's changing so quickly like in one
week we get new papers where this
capability is different like if we had
this interview last week I would say it
would be worse but now I see systems
which can look at a video uh
demonstration of a skill shown once by a
human and pick up that skill so that's
more impressive than a human child uh
definitely we're getting much better I
don't think they're fully General they
still definitely benefit from Human
supervision but it's improving
exponentially in comparison to what we
as a specie capable of
and and and I used to say that it is
improving super ranks financially I use
the term of jolting Technologies applied
to Ai and I uh use the the data that uh
was uh uh collected by by me uh where we
are not only referring to uh one uh set
of uh accelerations but uh the
um
uh the ability
of looking at different data sets
that's
yeah
the
um published their report
talking about a
two as of uh artificial
intelligence up to 2012 when they said
AI was following a Moore's Law doubling
in power every couple of years and then
they said after that in this in this
report published in 2019 we are actually
seeing a new uh exponential uh with AI
uh doubling in power every eight months
and they are talking about two distinct
eras and in my view this is a very
um
quotation marks lazy approximation of
the data set because
since then we have further data
and Jensen Wong the founder of uh Nvidia
in their 2021
um Global developer conference says
not eight months anymore four months and
in 2022
he said not four months anymore every
two months
so the rate of acceleration is shrinking
and the measure of the variable rate of
acceleration is the jolt
uh or or or or or the jerk but the
jerking Technologies is less appealing
so that's why I talk about jolting
Technologies instead
so when you say things are changing
rapidly from one week to the next
um are you able to keep keep up with the
with the the the the science not really
I used to be able to read every paper
and AI safety when I started when it was
all the good papers now I can't even
read the titles it's just too much
so let's uh talk about more
specialized the more specialized field
we mentioned AI in general then AGI
artificial general intelligence and so
let's touch upon
or or start to to go towards
AGI Safety and Security but before going
there arriving there
let's talk about
uh I.T Safety and Security right
um
in
you you specifically look at how to
design and if it is all possible to
design safe and secure AGI systems
but we have not been very good at
designing safe and secure I.T systems
right
whether we are talking about traditional
viruses or the vulnerabilities of our
hardware and software or more uh
recently ransomware
these are all demonstrations of the fact
that that our software development
practices
are such that systems are working you
know whether we are talking about Google
or Facebook or other large-scale scale
systems
it is it is rare these days that they
become universally unavailable
however
they are buggy they have a lot of little
glitches that each individual user
doesn't see
but those who are trying to keep up the
systems are very well aware of and the
same is true of our operating systems of
our smartphones
and and so on
do you see an improvement
in the
overall
practice of of software development and
how software
tries to be better and better engineered
or systems get bigger and bigger but
they're
bugs are still as numerous as before per
thousands or per million lines of code
the new systems typically rely on
pre-existing code through libraries they
inherit so you just have this history of
undiscovered bugs which keeps growing
and you may have seven levels of
dependencies so modern developer never
sees all the code the software relies on
so if anything is getting worse but the
fundamental difference between cyber
security and AI safety is that in cyber
security if you are hacked if something
happens you lose money maybe you lose
time reputation but you get to try again
you reset the password you issue a new
credit card and you move on in AI safety
you only get one chance if it's an
existential crisis you're not going to
get a second opportunity so very
different level of expectation we're
kind of used to this like yeah everyone
gets hacked all software has bugs just
click agree on a contract but that same
approach will not work with AGI
well let's dig deeper in in that so
you rightly said that we can afford uh
to iteratively develop
um software
and to observe the intended outcome and
if the software deviates from the
intended outcome its output is different
from what we expected we go back we look
at the code and we try to correct uh
where the the deviation happened
um
so
is it
and and and you said that that the same
approach will not work with with AGI
uh you actually published a series of
papers uh that are mapping kind of a
boundary conditions of all the things
that we are not going to be able to do
with with AGI
we are not going to be able to explain
it we are not going to be able to keep
it in check and so on and so forth would
you like to to list some of these things
that we are not going to be able to to
do with AGI and and without you know
getting into excessive uh scientific
detail but why do you come to those
conclusions sure so the big ones uh
explainability we do not understand how
large neural networks come to their
decisions a full explanation is the
model itself that's not something
comprehensible to a human so you have to
simplify it you have to provide a
simplified explanation which necessarily
emits some of the factors in the
decision there is billions of Weights
connections you'll get something like a
top 10 reasons why you got denied credit
so either you are getting this lossy
compression explanation which is just
not true who are you getting something
you cannot comprehend similar problem
with predictability we can predict
overall direction in which the system is
going final state but we cannot predict
how it's going to get there intermediate
steps in that process and that's easy to
see if you could predict those steps you
would be super intelligent you would
exactly know what the system is going to
do next so the assumption that it is
smarter than you would not be correct
there is dozens of other less obvious
results but all of them combined to our
inability to control more intelligent
systems indefinitely
so as long as there is a significant
intellectual differential between us and
superintelligence system long term it's
definitely not going to be obedient to
whatever
instructions we provide short term
okay um now
we we will go back to the issue of
um
[Music]
information flow pattern recognition and
and recognition of reality
um
and
what necessarily binds us
to
compress reality in chunks that we can
interpret with the tools we have
available but before we do that
uh let's see and let's go through a
little bit
um what happens
if we accept the fact that AGI
cannot be
understood
deeply or fully
bias and that AGI cannot be fully
controlled by us
so let's assume that the AGI happens
in and it will do its thing
in a manner that we cannot fully
comprehend and its thing is going to be
not completely what we expect
is this going to lead to a progressive
deviation
that is bound to increase
or could it
uh oscillate between various States
that do not necessarily
go far from what we expect or farther
than a given deviation that we don't
want to
um exceed
so first I just said it's impossible to
predict what it's going to actually do
so I can't possibly claim uh I can but I
would note that the difference between
heaven and hell is one bit right so a
small deviation can get you quite far
from your desired state
okay
um
now
let's
um
go back to
um the epistemological foundation
uh let's
discuss your own work as a scientist
and ask ourselves
the methods and and the claims that you
arrive to
um
whether you are likely to be missing
something that could lead someone else
to a different conclusion from from
yours
um how do you evaluate the validity and
the comprehensiveness of your own
conclusions
so that's a great question I actually
have a paper on a verification
verification of software mathematical
proofs and the conclusion there is that
they are not verifiable to a degree of
100 certainty you can invest more time
more resources more reviewers and get
more more confidence that the software
has no bugs 9999 and as many as you want
nines but it's more expensive to get
there but you never get to a hundred
percent what you get is verified with
respect to a certain verifier these two
PA reviewers said publish it that is a
hundred thousand scientists but those
two you randomly got approved
publication of this paper it doesn't
mean it's not containing a bug it's
quite possible I'm wrong and nothing
would make me happier than to be wrong
about this but so far they're pretty
simple proofs of things where it's
self-contradictory and no one has at
least so far pointed out those uh proofs
contain an error of some kind
so that's encouraging also in practice
none of the impossibility results I have
talked about have been violated no one
has said oh we can explain large neural
networks here's code or definitely I can
predict more capable systems here's how
I'm going to do it today what is the
date March 8th happy International
women's day uh no one has a working
system or even a prototype which they
would claim would scale to control
systems of any intelligence
the state of the art is we know what the
problems are but we have no Solutions
you just spare the the European Union a
billion Euro which they allocated to
explainable AI research so we should
call them up and say
you know donate it to you know whatever
other uh worthy cause
um
so what are you working on given the
generality of these uh conclusions what
are you working on today so clearly not
everyone agrees with me so the
development Community has no fear and
just tries to develop AI as quickly as
possible
safety Community thinks that if only
they were given that billion dollars and
a little more time everything is
solvable they have no doubt they have
intuition that they can control super
intelligence systems I'm saying that no
actually it's not a question of more
money or more time those are unsolvable
problems and we need to find alternative
ways to benefit from this very
beneficial technology but creating a
fully capable AGI super intelligence
without control is not a good outcome
for us long term whatever you first want
to get there last no one's going to win
anything
so what I've been working on is being
more convincing publishing more Journal
conference papers showing different
limitations and trying to get people to
read them which is difficult I have an
80 page paper showing this is why
control is unlikely to be happening
anytime soon and I don't think it's
going viral yet
last time I checked it hasn't yes or it
hadn't now
the
ability to develop a full-blown AGI
is in the eyes of a of the beholder
right uh you mentioned at the beginning
that the distinction between narrow and
general AI is becoming fuzzy
so if there were
um a universal consensus and the United
Nations uh treaty on the limitation of
AI capabilities
in order to prevent an AGI to be
accidentally or purposefully released
um
do you have
an idea of how the researchers could
refrain themselves from breaching those
uh treaties and what would the
enforcement mechanisms be
I'm not arguing for government
regulation I don't think it would work
mostly because the cost of scaling AI
keeps coming down it is still a large
expensive project today we can monitor
compute use but in a few years you could
do it in a laptop we already see
experiments where a single powerful
laptop can run some of those models at
home so I don't think simply making it
illegal will work
uh government regulation in the past for
spam for computer viruses has not uh
reduced this
level of malevolent software to zero and
as I said we need zero we cannot accept
some failure we need it to be a hundred
percent so I haven't found anything more
convincing than personal self-interest
if I get this uh usually young healthy
Rich CEO of a startup to go this will
get me killed I will not be famous and
there is no glory in it to stop doing it
that that is more convincing than
passing some sort of law where they just
move to another country or find another
way to bypass the law
um
I don't know uh what is the the
correlation or lack of correlation
between being young successful rich and
smart enough to realize that AGI should
not be pursued uh including the fact
that according to what you are
describing there has to be a hundred
percent success rate in convincing all
of those
um so let's go back to to two questions
at the at the beginning
um there are uh if you ask a hundred AI
researchers some who are still convinced
that AGI is either not possible or that
the approach uh that is being pursued
today is uh basically a dead end and
that novel discoveries about how
um our brain and mind work
are needed
um an example of uh this is uh
specifically in uh in the in the field
of AI research is uh is Jan lacun who
says large language models are are not
enough or a dead end
and an example of
a scientist who believes that we we
don't understand how the mind works is
Roger Penrose
who is convinced that uh Quantum
phenomena uh are playing a major role
and as a consequence uh if we don't uh
implement
analogs of human Minds using Quantum
Technologies we are not gonna go
anywhere
um
is it correct that that you don't
believe these are
sufficient in the sense that we cannot
afford for them to be wrong
that that we need to be putting in place
whatever
safeguards uh we we can
because
ifun or Roger Penrose or any of the
other Skeptics are wrong and we
accidentally uh stumble upon AGI well we
we have to be have to be ready
I think even if they're right it doesn't
change anything instead of five-year
timeline we get 25-year timeline but
it's the same problem same unsolvability
issues so I'm not sure it makes a
fundamental difference people who say we
will never succeed that one I cannot
understand I'm not following the logic
but anyone who says no no five years is
crazy it's definitely 20 years that's
fine I can accept that we have yeah
we we we are able to falsify the claim
that it is never gonna be possible
because we know that a bunch of hydrogen
atoms in 13 billion years created
general intelligence well unless you
believe in some magical properties of
human Souls religion that's the only
argument I found which may get you out
of it yeah yeah that's right
um so
a
between those uh intervals of let's say
5 and 25 years over the course of the
past couple of years your own
expectation has moved
and and in which direction oh I
definitely see more capable systems more
General systems and resources funding
compute Human Resources all growing
exponentially around it uh it's becoming
harder and harder to find something only
a human can do
it is surprising that things we as
assumed will be the most difficult
turned out to be the easiest art for
example is apparently trivial to make so
uh
it may continue on this path we are not
seeing any stagnation in terms of
progress and as I said the most amazing
thing is we're deploying systems with
capabilities Engineers so those systems
are not aware of
that never happened before
of course my my own playing with fire
right I am entertaining myself with the
while
uh they are laughing at me behind behind
the screens apparently
um this these are images that I I
generated for a presentation that I'm
making uh next week in uh in Dubai
because yeah uh I am for the past year I
have been doing all my presentations
illustrations with uh with me journey
and and it has been a big surprise for
many that the automation of creative
work is now possible uh not only in
images but also in text
to an increasingly uh unapproachable
degree by
non-specialists I am not a specialist
how new this is this is a couple months
old really a year old imagine how good
it's going to be in a year or so two
years five years
this full-blown movies virtual
environments yes super stimuli
okay so let's talk about super stimuli
uh weather attractive women or
um the ability of AI systems
to do about their own behaviors and
their own capabilities
about uh their their their their very
nature under your own prodding and under
this reverberation of communication that
um exploits
our mirror neurons and exploits our
natural tendency to empathize and to and
to put ourselves in the shoes of of
others
um
can we and should we build our
uh defenses individually by stealing
ourselves against uh these super stimuli
and and at least in my case I feel I am
able to do so right I I have not
uh being
convinced that current AI systems are
self-conscious for example that they
want uh things whether to be free or to
be uh doing different things that then
what they have been programmed
um is this something that everyone
should
train themselves to to maintain
I I don't know if an average person can
train themselves to that level we have
Nigerian scams right like nothing is
more prophetic than that and people fall
for it all the time I can certainly see
my parents who have hard time figuring
out how to click buttons and change
settings uh fall for deep fakes and
not have very good defenses against uh
emails targeted at them and so on so
maybe top experts can do something but I
think for most people it would be nearly
impossible
and and it is fascinating that
the presence of grammar and spelling
mistakes in Nigerian scams is actually
a desirable feature on their side
because they already know that there has
been a filter of gullibility
because there are people who look at the
pathetic email
and they will say oh this is evidently a
scam but people who respond to receive
part of the 20 million dollars by the
princess that inherited it are
predisposed to go through and and so the
scammers receive a number of responders
that have been pre-selected
even more so the fact that they haven't
changed the story it only targets those
who have never heard about the scam and
that that is a very narrow group of
people you have to live under a rock
with no internet access
or having or having acquired the
internet to
uh you know a given level of uh ability
uh recently enough
um let's let's go back to our need to
um not stumble upon AGI
um
what would you tell
the founder of stability AI
who believes that open source is the
best way to go about it
to put whatever more advanced models in
the hands of as many people as possible
is what will bring about
um human flourishing
if we accept that those could be
dangerous it's like arguing that if
everyone had a nuclear weapon then the
world would be safer
either you think they are safe and we
haven't seen evidence for that not even
from developers or they are dangerous
and then you want to not make an Open
Access as much as possible
however with nuclear we weapons we had
uh two things that I I believe is is
different than than with Advanced Ai and
AGI on on one hand uh
the
the engineering were uh on a relatively
more solid ground there was a wider
consensus around uh what were the
experiments that uh we could do uh what
were the boundary conditions of those
experiments uh for example uh
there is this anecdote of the scientists
asking themselves is the nuclear
explosion that we are planning to carry
out as a test going to ignite the free
oxygen in the atmosphere killing
everyone
and that is a proper existential risk
and then calculating whether that would
be the case and concluding that it would
not going ahead with the experiment and
being right
um that's that's one the other is that
even though
um the knowledge of nuclear weapons has
been around for now almost 100 years the
ability to develop and deploy in a
desiredly destructive way nuclear
weapons has not become available to
a very very large number of people it
has become easier
and nuclear
non-proliferation failed
uh there are more and more nuclear
um capabilities available around the
world
and the
comprehensive
ban of nuclear weapons has not been
achieved
um
but
the
you know irresponsible
college student
today cannot assemble a nuclear weapon
uh on in his dorm
and and and both of those
are are not true in in in advanced AI
our ability to predict how they will
work is uh
um is uh is
um
impossible
thank you thank you and uh and the uh
availability of tools that can run ever
more dangerous experiments is going to
be in the hands of everyone soon
so
um
your being here is part of talking about
this right it's part of what needs to be
done part of talking about it part of
raising the uh understanding of
of of the danger
um
if he recognize that uh
AGI minus X is useful
is there anything that we can do in
order to keep
the capabilities of of our systems
within or below that threshold
other than deciding to do so
individually I haven't found a solution
it seems that the technology we have is
already very capable for example protein
folding problem was solved without AGI
but uh
we need time to study existing systems
so if we had a few years at the current
level we would better understand what
they can give us what they are capable
of and maybe they need for more capable
systems can be reduced uh for example
life extension is something I assumed we
needed help with but maybe it's enough
to just have current systems similar to
the one uh which solved the protein
folding problem but again I don't think
external pressure can be applied in a
meaningful way
with respect to nuclear weapons it's not
a Perfect Analogy obviously there are
some lessons we can learn from that
field which I think are useful uh to the
best of my knowledge both American and
Soviet creators of nuclear weapons
regretted their invention and spend the
rest of their lives trying to undo that
damage
we actually use nuclear weapons
it would not be the same with AGI one
mistake could be more than enough and
you won't have time to kind of go back
and Undo It And once it's unrolled and
available to individuals you cannot put
it back in a box
so uh we can learn still even though we
survived nuclear experiments we can
learn from those mistakes we might have
uh survival bias survivorship bias we
had so many close calls accidents where
it was possible that the world would be
destroyed in many somewhere in a
Multiverse some branches no longer exist
because of it so I don't think we can
put too much into the fact that we're
still here
um the
during our conversation today you
mentioned uh
if we had more time so there is this uh
current race that is shortening the time
available and and you would like much
more time to be available
to
find a fault in your own reasoning maybe
or to find some new approach or to find
um
something that that could
not diminish but eliminate the
existential risk represented by by AGI
um
and then and then of course you
mentioned also resources
um so if a billion dollars were that
round number what is the uh
corresponding round number uh that you
would need uh or the AGI Safety and
Security Community would need uh in time
is it 10 years or a hundred years or an
unknowable amount of time well I
obviously don't know exact number but I
can tell you that safety work takes more
time than development work I can write a
function very quickly but then to
properly test it I would need 10x time
100x time even if it was deterministic I
couldn't use all the pre-existing edge
cases if it's not deterministic if it's
self-improving self-modifying learning
and new data I can exploit it in new
domains maybe I need a million times
more time to properly understand how it
functions so right now we have this
exponential as you said super
exponential growth and capability say
but progress and safety is not even
linear I think it's constant
um
we were at the
conference in Puerto Rico organized uh
by uh the uh life uh Institute
that in analogy with the asilar
conferences
um aims to increase uh knowledge
collaboration understanding of this
problem set that was before the pandemic
and I don't know if a new conference is
is being organized when but that was a
pretty small group I think it was like a
hundred people and at the time we still
had Russians and Chinese uh in in Puerto
Rico even though not many but at least a
few and if a new conference were to be
organized by an Institute of the MIT I
doubt Russians would participate and and
I don't know about the Chinese
um
which brings me
to the point that
we cannot afford political
uh or even military
um contrasts or conflicts or War
to interfere with the need of the
broadest possible potentially Universal
understanding of of the challenge right
um we we we know what is going on in
terms of scientific Publications
and the field is so dependent on on the
talent and skills uh and and available
Hardware of uh
of everyone that it is highly unlikely
that some
hidden research would be further
Advanced but still
breaking down channels of communication
can only be a hindrance uh to to this
broader understanding of of the issues
is that correct yeah in general yes we
want scientific Community to be United
to communicate in a way the war in
Russia is kind of good because it
siphons resources and access to compute
so uh I'm not aware of any large
successful projects in Russia which
would overtake American efforts uh China
may be a completely different story but
at least in that regard big expensive
conflicts kind of prevent development of
scientific discovery delay it a little
bit so
I would have a hard time agreeing with
you that World War III would be
desirable in this context for the
survivors
and and I hope that is not the solution
that we will adopt uh
please uh definitely not what I'm hoping
for I'm just pointing out at any time
there is a distraction from something
like a large scientific effort it does
impact scientific capabilities
hmm
um
so
forgive me you you did say that you
don't know uh you you cannot give a
precise answer uh but did you say
oh yeah you you you gave a proportion so
you said uh if we needed uh
a given amount of years uh to develop
AGI we would need 10 times or a hundred
times that long
to be able to analyze it
I think so so if we release GPT 3 gpt4
GPT 5 every two years I need 20 years in
between to figure out uh what we're
dealing with
and
there are a hundred naive uh Solutions
that I'm not gonna bore you with uh such
as for example
um run them slower
so that our clock speed is faster than
theirs and then be able to to study them
better in
how can we
uh rank
Solutions so that we concentrate on some
that have a higher probability of
succeeding rather than rehash uh naive
ones that everyone in the field uh
realizes uh won't work
so we just published another paper and
AI risk skepticism where we have about
60 pages of people's ideas about why
it's not a problem why it's going to be
resolved and we try to address
uh why they may not be right about it
there are papers surveying ideas and AI
safety field but
I don't think there are any which have
not been immediately shut down pointed
out that they have a major flow in them
um
I'm not uh sure will ever
stop getting new proposals from people
who just heard about the problem and so
on unplug it pour water on it I mean
people read a lot of good science
fiction so
[Music]
um
yeah yeah my goal was to kind of cancel
all those future attempts with a single
impossibility proof and then if you can
show the mistake and impossibility proof
that would be helpful
yeah yeah yeah sure and so we mentioned
the at the beginning and I said we would
go back to it uh the the fact that to in
order to
um interpret the world we compress it
and there was an article I think in New
Yorker uh recently that was blaming at
least in the title that uh GPT is blurry
uh jpeg on on the world or or transforms
the web in a blur JPEG and rather than
interpret it
negatively in reading the article I I
don't know if agreeing yes that is what
we do we approximate we are lossy
compressors our ability for that is a
necessary survival
remembrance of everything from the past
and you mentioned
um one of your scientific publications
of 80 pages and just another of six
stages and two books that are probably
of hundreds of pages they are back there
somewhere by the way
um
and and they are not blurry enough in
order to go viral have you tried the
memetic engineer to a a more aggressive
e and I am not joking where you are uh
compressing what you do
to literally a meme an image that is
punch enough to reach millions or tenths
or if Elon musker retweets you a hundred
million people
so uh if you follow me on Twitter or
Facebook you know I'm a prolific
poster I have lots of those uh sometimes
Elon Musk does react to them but after a
million views I get one new follower I
don't think it makes a difference this
week we had an article in Time Magazine
a short piece one page saying AI will
likely be uncontrollable
what did I get out of it uh maybe two
emails from crazy people asking me for
God knows what so I don't think that's
uh working I definitely try I go on a
lot of podcasts I do interviews we write
for popular media but the very narrow
segment of society who finds this topic
interesting it doesn't scale it's not
like you know popular media's Kim
Kardashian whatever 100 million
followers it's not the same topic
foreign
so
before starting the the
the live stream uh I told you I have
three children and and you told me
you're aware of the same number on your
side
and you uh confirmed that you have a
somewhat unreasonable Hope and Faith in
them
which I think is biologically grounded
uh I don't know how rational but still
uh is something we hold dear
um because
we are human and want Humanity to
flourish as well as uh we we pass the
Baton we we
um have this uh ability to to sit back
and then see uh what what comes after us
um
how do you how do you uh
prepare them I don't know the ages mine
are grown up
um so I don't necessarily need to to
address in metaphors and fairy tales uh
whatever I feel about the future of the
world if people listen to us and they
have uh
youngsters and and they want them to
understand what we talk about how would
you recommend they do it
so it depends of course on the timelines
what you think is realistic if you think
we have enough time for your kids to
grow up you can talk about career
choices a lot of the jobs they might
consider will not exist a lot of jobs uh
uh only started showing up this year
prompt engineer or something like that
so you definitely have to plan for the
future decide if College doesn't make
sense things of that nature if you have
shorter timelines and your children are
young it's that's a tough one I don't
have any very good answers my daughter
she's nine she's helping me with cover
design for my books so that's something
just envisioning control problem being
solved foreign
well
um what is amazing is that one way or
another we will find out right uh
I'm I'm not sure we will be happy of the
outcome or we will be delighted to be
wrong uh
I still we will say oh
I'm happy all my life's work is now in
in Ruins
because I was wrong about all my various
Publications
uh but it is kind of um
intriguing that the keplerian relativism
doesn't apply to our generation
apparently uh sometimes something that
is radically new comes about and and uh
with the eyes of past or potentially
hopefully future Generations
you can actually say that was a special
time and and it feels like we we are
living it
it's definitely a very unlikely that you
would live at a time where you get to
decide the future of the universe
forever so that kind of gives more more
Credence to simulation hypothesis and
other interpretations of what's going on
maybe it's a test to see the crazy
people who will actually run AGI and
make sure they are taken care of
okay well
um we spoke three years ago so I hope to
be able to call you for a new uh live in
three years and see what happened in the
meantime and Roman thank you very much
for the work you do and the energy that
you invest in alerting as many people as
you can about the work that needs to be
done and the risks that we need to
minimize and hopefully eliminate
thank you so much for inviting me I'll
put it on my calendar three years okay
okay
let's have a reminder
talk to you next time
thank you
[Music]

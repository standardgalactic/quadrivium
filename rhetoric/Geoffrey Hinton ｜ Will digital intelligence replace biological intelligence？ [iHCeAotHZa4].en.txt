good evening everyone my name is Melanie
wooden and I have the privilege of
serving as the dean of the faculty of
Arts and Science at the University of
Toronto at this time I wish to
acknowledge the land on which the
University of Toronto
operates for thousands of years it has
been the traditional land of the hon
wendat the senica and the Miss sagas of
the credit today this meeting place is
still home to many indigenous people
from across Turtle Island and we are
grateful to have the opportunity to work
on this
land I'd like to thank this evening's
co-hosts the Schwarz reesman institute
for technology and Society the
department of and the department of
computer science in collaboration with
the vector Institute for artificial
intelligence and the Cosmic future
initiative soon to be the school of
cosmic future in the faculty of Arts and
Science and I would like to thank Manuel
pza for providing such lovely music to
get us underway this
[Applause]
evening and I am delighted to welcome
each of you to the special occasion this
evening uh to introduce University
professor ameritus Jeffrey Hinton
someone that needs no
introduction tonight we have the honor
of hearing Dr Jeff hinton's thoughts on
the state of artificial intelligence and
the unique opportunity to engage with
him personally through the
Q&amp;A a founding figure in artificial
intelligence Dr Jeff Hinton had an
unwavering conviction that artificial
neural networks held the most promise
for accelerating machine
learning as a neuroscientist myself
someone who's dedicated her career to
studying the brain I've long been
inspired by the symbiosis between Ai and
Neuroscience the stunning advances we've
seen from J GPT to self-driving cars are
rooted in our knowledge of the structure
and function of the
brain today we take for granted that
artificial networks modeled after
synaptic transmission and plasticity are
a Mainstay of machine learning
applications AI systems use these
networks to recognize patterns make
decisions and learn from
data but for much of Dr hinton's career
this approach was unpopular some even
said it was a dead
end in the two 2000s however things
changed Dr hinton's idea of dividing
neural networks into layers and applying
learning algorithms to one layer at a
time gain
traction and in 2012 Dr Hinton and two
of his graduate students Alex kvki and
ilas sver used their deep learning
approaches to create visual recognition
software that handily won the imet
competition and for the first time
rivaled human
accuracy when he was awarded an honorary
degree from UFT in 2021 Jeff Hinton
reflected on his career and he said I
think the take-home lesson of the story
is that you should never give up on an
idea that you think is obviously correct
and you should get yourself some really
smart graduate
students I Echo that sentiment Jeff and
lucky for us we have truly outstanding
graduate students at the University of
Toronto many of them here with us this
evening
today the conversation between Ai and
Neuroscience
continues just as Neuroscience
discoveries informed the development of
AI systems AI is now providing new tools
and techniques to study the
brain advances in deep learning
algorithms and the enhanced processing
power of computers are for example
allowing us to analyze huge data sets
such as whole Imaging brain in
humans indeed AI is poised to transform
how we live in work at this pivotal
moment when we consider the
opportunities and the risks of AI who
better to guide us in these
conversations than Dr Hinton himself so
with that let me formally introduce him
Jeffrey Hinton received his PhD in
artificial intelligence in Edinburgh in
1978 after 5 years as a faculty member
at Carnegie melon he became a fellow of
the Canadian Institute for advanced
research and moved to the Department of
computer science at the University of
Toronto where he is now an Emeritus
professor in 2013 Google acquired
hinton's neural neck startup DNN
research which developed out of his
research at UFT subsequently Hinton was
a vice president uh and Engineering
fellow at Google until
2023 he's a founder of the vector
Institute for artificial intelligence
and continues to serve as their SC Chief
scientific adviser
Hinton was one of the researchers who
introduced back propagating algorithms
and was the first to use this approach
for learning word
embeddings his other contributions to
neural network uh research include bolts
machines distributed
representations time delay neural Nets
mixtures of experts variation learning
and deep learning his research group in
Toronto made major breakthroughs and
deep learning that re revolutionized
speech recognition and object
classification he is amongst the most
widely cited computer scientists in the
world Hinton is a fellow of the UK Royal
Society the Royal Society of Canada the
association for the advancement of
artificial intelligence and a foreign
member of the US National Academy of
engineering and the American Academy of
Arts and
Science his Awards include the DAV e
David e rumal heal prize the J
jci Cai award for research
Excellence the kilum prize for
engineering the E uh i e Frank rosenblat
metal the ener Herzberg gold medal the
NEC and CNC award the Honda prize and
most notably the am touring award often
referred to as the Nobel Prize in
Computing so without further Ado I'd
like like to invite Jeff Hinton to give
a talk entitled will digital
intelligence replace biological
intelligence over to
[Music]
[Applause]
[Music]
[Applause]
you okay um before I forget because I'm
going to forget um I'd like to thank
Sheila WTH who was the point person for
organizing all this she did a wonderful
job of organizing everything she she was
the go-to person for fixing all the
problems and so I'd like to thank her
and I know I'll forget at the
end so it's a very mixed audience and so
I removed all the equations there are no
equations um I decided rather than
giving a technical talk I would focus on
two things I want to get over two
messages the first message is that
digital intelligence is probably better
than biological
intelligence um that's a depressing
message but there it is that's what I
believe um and the second is to try and
explain to you why I believe that these
large language models like gpg 4 really
do understand what they were saying
there's a lot of dispute about whether
they really understand it and I'm going
to go into some detail to try and
convince you they do understand
it
um right at the end I will talk about
whether they have subjective experience
um and you have to wait to see what I
believe about
that so in digital
computation um the whole idea is that
you separate the hardware from the
software so you can run the same
computation on different pieces of
hardware and that means the knowledge
that the computer learns or is given is
Immortal if the hardware dies you can
always run it on different
Hardware now to achieve that immortality
you have to have a digital computer that
does exactly what you tell it to at the
level of the instructions and to do that
you need to run transistors of very high
power so they behave digitally and in a
binary way and that means you can't use
all the rich analog properties of the
hardware which would be very useful for
doing many of the things that neural
networks do and in the brain um when you
do a floating point multiply it's not
done digitally it's done in a much more
efficient way um but you can't do that
if you want computers to be digital in
the sense that you can run the same
program on different
Hardware there's huge advantages to
separating Hardware from software um
it's why you can run the same program on
lots of different different computers
and it's why you can have a computer
science department where people don't
know any Electronics um which is a great
thing
but now that we have learning devices um
it's possible to abandon that
fundamental principle it's probably the
most fundamental principle computer
science of the hardware and software
ought to be separate but now we've got a
different way of getting computers to do
what you want instead of telling them
exactly what to do in great detail you
just show them examples and they figure
it out obviously there's a program in
there that somebody wrote that allows
them to figure things out a learning
program but for any particular
application they're going to figure out
how to do that and that means we can
abandon this principle if we want
to what that leads to is what I call
mortal computation it's computers where
the precise physical details of the
hardware can't be separated from what it
knows if you're willing to do that you
can have very low power analog
computation that parallelizes over
trillions of Weights just like the
brain and you can probably grow the
hardware very cheaply instead of
manufacturing it very
precisely and that that would need lots
of new nanot technology but you might
even be able to genetically re-engineer
biological neurons and grow the hardware
of biological neurons since they spent a
long time learning how to do
learning I want to give you one example
of the efficiency of this kind of analog
computation compared with digital
computation so suppose you want to you
have a a bunch of activities of neurons
and they have synapses to another layer
of neurons and you want to figure out
the inputs to the next layer so what you
need to do is take the activities of
each of these neurons multiply them by
the weight on the connection the synapse
strength and add up all the inputs to a
neuron that's called a matrix a vector
Matrix
multiply and the way you do it in a
digital computer is you'd have a bunch
of transistors for representing each
neural activity and a bunch of
transistors representing each weight you
drive them at very high power so they
were binary and if you want to do the
multiplication quickly then you need to
perform of the order of 32 squar one bit
operations to do the multiplication
quickly
um or you could do it analog where the
neural activities are just voltages like
they are in the brain the weights are
conductances
and if you take a voltage times a
conductance it produces charge per unit
time so you put the voltage through this
thing that has a conductance out the
other end comes charge the longer you
wait the more charge comes out um the
nice thing about charges is they just
add themselves and that's what they do
in neurons too and so this is hugely
more efficient you just got a voltage um
going through a conductance and
producing charge and that's done your
floating Point
multiply um it can afford to be
relatively slow if you do it a trillion
ways in parallel and so you can have
machines that operate at 30 30 Watts
like the brain instead of at like a
megawatt which is what these digital
models do when they're learning and you
have many copies of them in
parallel um so we get huge Energy
Efficiency but we also get big problems
um to make this whole idea of mortal
Computing work you have to have a
learning
procedure that will run in analog
Hardware without knowing the precise
properties of that hardware and that
makes it impossible to use things like
back propagation because back
propagation which is the standard
learning algorithm use for all neural
Nets now almost all um needs to know
what happens in the forward pass in
order to send messages backwards to tell
it how to learn it needs a perfect model
of the forward pass um and it won't have
it in this kind of mortal
Hardware people have put a lot of effort
I spent the last two years um but lots
of other people put much more effort
into trying to figure out how to find a
biologically plausible learning
procedure that's as good as back
propagation and we can find procedures
that in small systems systems with say a
million connection strengths do work
pretty well they're comparable with back
propagation they get performance is
almost as good and they uh they learn
relatively quickly um but these things
don't scale up when you scale them up to
really big networks they just don't work
as well as back propagation so that's
one problem with Mortal
computation another big problem is
obviously the hardware dies you lose all
the knowledge because the knowledge is
all mixed up it's the conductance is for
that particular piece of hardware and
all the neurons are different in a
different piece of Hardware so you can't
copy the knowledge by just copying the
weights um the best
solution if you want to keep the
knowledge is to make the old computer be
a
teacher um that teaches the young
computer what it knows and it teaches
the young computer that by taking inputs
and showing the young computer what the
correct outputs should be and if you got
say a thousand classes and you show real
valued probabilities for all thousand
classes you're actually conveying a lot
of information that's called
distillation and it works um it's what
we use in digital neuron Nets if you've
got one architecture and you want to
transfer the knowledge to a completely
different digital architecture we use
distillation to do
that it's not nearly as efficient as the
way we can share knowledge between
digital computers um it is as a matter
of fact how Trump's tweets work um what
you do is you take a situation and you
show your followers a nice prejudiced
response to that situation and your
followers learn to produce the same
response and it's just a mistake to say
but what he said wasn't true that's not
the point of it at all the point is to
distill Prejudice into your followers
and it's a very good way to do
that so
there's basically two very different
ways in which commun in which a
community of Agents can share
knowledge um and let's just think about
the sharing of knowledge for a
moment because that's really what is the
big difference between Mortal
computation and Immortal computation or
digital and biological
computation if you have digital
computers and you have many copies of
the same model so with exactly the same
weights in it running on different
Hardware different
gpus then each copy can look at
different data different part of the
internet and learn something when it
learns something what that really means
is it's extracting from the data it
looks at how it ought to change its
weights to be a better model of that
data and you can have thousands of
copies all looking at different bits of
the internet all figureing out how they
should change their weights in order to
be a better model of that data and then
they can communicate all the changes
they'd all like and just do the average
change and that will allow every one of
those thousands of models to benefit
from what all the other thousands of
models learned by looking at different
data when you do sharing of gradients
like that if you've got a trillion
weights you're sharing a trillion real
numbers that's a huge bandwidth of
sharing um it's probably as much
learning as goes on in the whole of the
University of Toronto in a month
um but it only works if the different
agents working exactly the same way so
that's why it needs to be
digital if you look at
distillation we can have different
agents which have different Hardware now
they can learn different things they can
try and convey those things to each
other Maybe by publishing papers in
journals
um but it's a slow and painful process
so if we think about the normal way to
do it as um say I look at an image and I
describe to you what's in the image and
that's conveying to you how I see things
um there's only a limited number of bits
in my caption for an image and so the
amount of information that's being
conveyed is very
limited language is better than just
giving you a response that says good or
bad or it's this class or that class if
I describe what's in the image that's
giving you more bits so it makes
distillation more effective but it's
still only a few hundred bits it's not
like TR a trillion real numbers so
distillation has a hugely lower
bandwidth than this sharing of gradients
or sharing of Weights the digital
computers can
do so The Story So
Far digital computation requires a lot
of energy like a
megawatt um but it has a very very
efficient way of sharing what different
agents learn and if you look at
something like gbt 4 the way it was
trained was lots of different copies of
the model went off and looked at
different bits of data running on
different gpus and then they all shared
that knowledge and that's why it knows
thousands of times more than a
person even though it has many fewer
connections than a person we have about
100 trillion synapses gp4 probably has
about two trillion synapses weights
although you won't tell me but um it's
about that number
um so it's got much more knowledge in
far fewer connections and it's because
it seen hugely more data than any person
could possibly
see this actually gets worse when these
things are actually agents that perform
actions because now you can have
thousands of copies performing different
actions and when you're performing
actions you can only perform one action
at a time and so having these thousands
of copies being able to share what they
learned um lets you get much more
experience than any mortal computer
could
get biological compation requires a lot
less energy but it's much worse at
sharing
knowledge so now let's look at large
language
models these use digital computation and
weight sharing which is why they can
learn so much they're actually getting
Knowledge from people by using
distillation so each
individual agent is trying to mimic what
people said it's trying to predict the
next word in the document so that's
distillation it's actually a
particularly inefficient form of
distillation because it's not predicting
the probabilities a person assigned to
the next word is actually predicting the
actual word which is just a
probabilistic choice from that and
conveys very few bits compared with the
whole probability distribution sorry
that was a technical bit I I won't do
that again
um so it's an inefficient form of
distillation and these large language
models have to learn in that inefficient
way from people but they can combine
what they learn very
efficiently
um
so the issue I want to addresses do they
really understand what they're saying
and that's there a huge divide here um
there's lots of oldfashioned linguists
who will tell you they don't really
understand what they're saying they're
just using statistical tricks to pasti
together regularities they found in the
text they don't really
understand um we used to have in
computer science a fairly widely
accepted test for whether you understand
which was called the churing test
um when gp4 basically passed the touring
test people decided it wasn't a very
good test um I think it was a very good
test and it passed
it so here's one of the objections
people give it's just glorified
autocomplete you're training it to
predict the next word and that's all
it's doing it's just predict iting the
next word he doesn't understand
anything well when people say that it's
because they have a particular picture
in their minds of what it what is
required to do
autocomplete a long time ago the way you
would do autocomplete is this you keep a
big table of all triples of
words and so now if you saw the word
fish and you could look in your table
and say find me all the triples that
start with Fish And and look at how many
of them have particular words next and
you'll find
there's many occurrences of the triple
fish and chips and so chips is a very
good bet for filling it in at least if
you're English
um but the point is that's not how large
language models work even though they're
doing autocomplete in the sense that
they're predicting the next word they're
using a completely different method to
predict it and it's not like the
statistical methods that people like
chumsky had in mind when they said that
you can't do language with statistics
um these are much more powerful
statistical methods that can basically
do anything um and the way they model
text is not by storing the text you
don't keep strings of Words Anywhere
there is no text inside gbt 4 it
produces text and it reads text but
there's no text
inside um what they do is they associate
with each word or fragment of a word
I'll say word and the technical people
will know it's really fragments of words
but it's just easy just say word they
associate with each word a bunch of
numbers a few hundred numbers maybe a
thousand numbers um that are intended to
capture the meaning and the syntax and
everything about that word these are
real numbers so there's a lot of
information in the Thousand real
numbers and
then they take the words in a sentence
the words that came before the word you
want to predict and they let these words
interact so that they refine the
meanings that you have for the words
I'll say meanings Loosely this is called
an embedding Vector it's a bunch of real
numbers associated with that word and
these all interact and then you predict
the numbers that are going to be
associated with the output word the word
you're trying to predict and from that
bunch of numbers you then predict the
word these numbers are called feature
activations and in the brain there'd be
the activations of
neurons so the point is what gp4 has
learned is lots of interactions between
feature activations of different words
or word fragments and that's how its
knowledge is stored it's not at all
stored in storing
text and if you think about it to
predict the next word really well you
have to understand the text um if I ask
you a question and you want to answer
the
question um you have to understand the
question to get the answer now some
people think maybe you
don't my good friend Jan lah appears to
think you don't actually have to
understand um he's wrong and he'll come
around
so this was a uh problem suggested to me
by Hector Le Hector suggested it
suggested something a bit simpler um
that didn't involve paint fading and
thought gp4 wouldn't be able to do it
because it requires reasoning and it
requires reasoning about cases um so I
made it a bit more complicated and gave
it to gbd4 and it solves it just fine um
I'll read it out in case can't read it
at the back the rooms in my house are
painted blue or white or yellow yellow
paint Fades to White within a year in
two years time I want them all to be
white what should I do and
why and gbd4 says this um it gives you a
kind of case-based analysis it says the
room is painted white you don't have to
do anything the room is painted yellow
you don't you don't need to repaint them
because they'll fade and the rooms
painted in blue you need to repaint
those now each ter usually it gives you
a slightly different answer because of
course it hasn't stored the text
anywhere it's making it up as it goes
along but it's making it up correctly
and this is a simple example of
reasoning and it's reasoning that
involves time and understanding that if
it fades in a year and two years time
it's going to be faded and stuff like
that so there's many many examples like
this now there's also many examples
where it screws up but the fact that
there's many examples like this make me
believe it really does understand what's
going on I don't see how you could do
this without understanding what's going
on another argument that llms don't
really understand is that they produce
hallucinations they sometimes say things
that are just false or just nonsense um
but people are particularly worried
about when they just apparently make
stuff up that's false they called that
hallucinations when it was done by
language model which was a technical
mistake
if you do it with language it's called a
confabulation if you do it with vision
it's called a hallucination
um but the point about confabulations is
they're exactly how human memory works
we think our memories most people have a
model of memory as there's a filing
cabinet somewhere and an event happens
and you put in the filing cabinet and
then later on you go in the filing
cabinet and get the event out and you
remember it it's not like that at
all um we actually reconstruct events
what we store is not the neural
activities we store weights and we
reconstruct the pattern of neural
activities using these weights and some
memory cues and if it was a recent
event like if it was what the dean said
at the beginning um you can probably
reconstruct fairly accurately some of
the sentences she produced like he needs
no introduction and then going on and
giving a long introduction
um you remember that that right
um so we get we get it right and we
think we stored it literally but
actually we're reconstructing it from
the weights we have and these weights
haven't been interfered with by future
events so they're pretty good if it's an
old event you reconstruct the memory and
you typically get a lot of the details
wrong and you're unaware of that um and
people are actually very confident about
details they get wrong they're as
confident about those as details they
get right and there's a very nice
example of
this so John Dean um testified in the
Watergate trial um and he testified
under oath before he knew that there
were
tapes and so he testified about these
various meetings and what happened in
these various meetings and Holderman
said this and Erman said that and Nixon
said this and a lot of it he got
wrong now I believe that to be the case
I actually read elri n's book about 20
years ago and I'm now
confabulating um but I'm fairly sure
that he got a lot of the details wrong
um but he got the gist correct he was
clearly trying to tell the truth and the
gist of what he was saying was correct
the
details were wrong but he wasn't lying
he was just doing the best human memory
can about events that were a few years
old so these hallucinations as they
called or confabulations they are
exactly what people do we do it all the
time um my favorite example of people
doing confabulation is the someone
called Gary Marcus who criticizes neural
Nets and he says neural Nets don't
really understand anything they just
pasti together text they read on the web
well that's CU he doesn't understand how
they work they don't pasti together the
texts they've read on the web because
they're not storing any text they're
storing these weights and generating
things um he's just kind of making up
how he thinks it works so actually
that's a person doing
confabulation
um now chatbots are currently a lot
worse than people are realizing when
they're doing it um but they'll get
better in order to sort of give you some
insight into how all these features
interacting can cause you to understand
how understanding could consist of
assigning features to words and then
having the features inter act I'm going
to go back to
1985 and to the first neuronet language
model it was very small it had 112
training cases which is not big data and
it had these embedding vectors that were
six real numbers which is not like a
thousand numbers um but my excuse is the
computer I used was a lot smaller so if
you took the computer I was using in
1985 and you started running it in 1985
doing a computation
and then you took one of these modern
computers we use for training chatbots
and you ask how long would the modern
computer take to catch up less than a
second in less than a second it would
caught up with all this computer done
since
1985 that's how much more powerful
things have
got okay so the aim of this
model was to unify two different
theories of
meaning one theory is basically what a
lot of psychologists believed which was
the meaning of a word is just a whole
bunch of semantic features and maybe
some syntactic features too um and that
can explain why a word like Tuesday and
a word like Wednesday have very similar
meanings they have very similar semantic
features so psychologists were very
concerned with similarity and
dissimilarity of meanings and they had
this model of just this Vector of
semantic features and that's the meaning
of a word and it's a very kind of static
dead model they just the features just
kind of sit there and they're the
meaning um they never could say where
the features came from they obviously
have to be learned you're not born
innately knowing what words mean um but
they didn't have a good model of how
they were
learned and then there's a completely
different theory of meaning which AI
people had and most linguists had I'm
not a linguist but I think it goes back
to deur and it's a structuralist theory
of meaning and the idea is the meaning
of a concept is its relation to other
Concepts um so if you think about it in
terms of words the meaning of a word
comes from its relationships to other
words and that's what meaning is all
about and so computer scientists said
well if you want to represent um meaning
what you need is a relational graph so
you have nodes that are words and you
have arcs on them about their
relationships and that's going to be a
good way to represent meaning and that
seems like completely different from a
whole bunch of semantic
features
now I think both of these things are
both right and wrong and what I wanted
to do was unify these two approaches to
meaning and show that actually what you
can have is you can have features
associated with words and then the
interactions between these
features create this relational graph
the relational graph isn't stored as a
relational
graph what you've got is features that
go with words but if I give you some
words the interactions between their
features will say um yes these words can
go together that way that's a sensible
way for them to go together
so I'm going to show you an example of
that and this I believe to be the first
example of a neural a deep neural net um
learning word meanings from relational
data and then able to answer relational
questions about relational
data so we're going to train it with
back propagation which I'll explain very
briefly in a minute and we're going to
make features interact in complicated
ways and these interactions between the
features that go with words are going to
cause it to believe in some combinations
of words and not believe in other
combinations of
words and these interactions are a very
powerful statistical
model so this is the data it's two
family trees a tree of English people
and a tree of Italian people and you
have to think back to the
1950s um we're not going to allow
um marriage between people from
different countries we're not going to
allow divorces we're not going to allow
adoptions it's going to be very very
straight fam extremely
straight okay um and the idea is I'm
going to take this relational
data and I'm going to train a neural net
so that it learns features for each of
these people and for each of the
relationships and those feur teaches
interact so that it's captured this
knowledge and in particular what we're
going to do is we're going to
say all of the knowledge in those family
trees can be expressed as a set of
triples
um we have 12
relationships and I think there's 12
people in each family
tree and so I can say Colin has Father
James and that expresses something that
is in the this tree you can see Colin
has Father
James and of course if I give you a few
facts like that like Colin has Father
James and Colin has mother Victoria you
can infer that James has wife Victoria
in this very regular
domain and so conventional AI people
would have said well what you need to do
is store these facts it's like sort of
dead facts like this you're just storing
strings of symbols and you need to learn
a rule that says how you can manipulate
these strings of Sy symbols that would
be the standard AI way to do it back in
1985
um and I want to do it a quite different
way so rather than looking for symbolic
rules for manipulating these symbol
strings to get new symbol strings which
works um I want to take a neural net and
try and assign features to words and
interactions between the features so
that I can generate these strings
so that I can generate the next word and
it's just a very different approach now
if it really is a discrete space Maybe
looking for rules is fine but of course
for real data these rules are all
probabilistic anyway and so searching
through a discrete space now doesn't
seem that much better than searching a
real value space and
actually um a lot of mathematicians will
tell you real value spaces are much
easier to deal with than discrete spaces
it's easier typically to search through
a real value space and that's what we're
doing here oh sorry I got technical
again I didn't mean to um it happens if
you're an ex
Professor okay so we're going to use the
back propagation algorithm and the way
back propagation works is you have a
forward pass that starts at the input
information goes forward through the
neural network and on each connection
you have a weight which might be
positive or negative which is green or
red um and you activate these neurons
and they're all nonlinear neurons and
you get an output
and then you compare the output you got
with the output you should have got and
then you send a signal backwards and you
use calculus to figure out how you
should change each weight to make the
answer you get more like the answer you
wanted to get and it's as simple as that
um I'm not going to go into the details
of it but you can read about that in
lots of places so we're going to use
that approach of you put the inputs in
you go through you get an answer you
look at the difference between the
answer you got and the answer you wanted
you send a signal backwards which learns
how to change all the
weights and here's the net we're going
to use we're going to have two inputs a
person and a relationship and they're
initially going to have a local encoding
and what that means is for the people
there' be 24 neurons and for each person
we'll turn on a different neuron so in
that block at the bottom that says local
encoding a person one one neurum will be
turned on and similarly for the
relationship one neurum will be turned
on
and then the outgoing weights of that
neuron to the next layer will cause a
pattern of activity in the next layer
and that will be a distributed
representation of that person that is we
converted from this one on
representation one hot to a vector of
activities in that this case it's just
six activities so those six neurons will
have different levels of activity
depending on which person it
is and then we take those vectors that
represent the person and the
relationship and we put put them through
some neurons in the middle layer that
allow things to interact in complicated
ways and we produce a vector that's
meant to be the features of the output
person and then from that we pick an
output
person so that's how it's going to work
it's going to be trained with
backrop and what happens is that if you
train it with the right kind of regular
Isis what you get sorry I got technical
again forget that if you train it what
you get is um if you look at the six
features that represent a person they
become meaningful features they become
what you might call semantic features so
one of the features will always be
nationality all the Italian people will
have that feature turned on and all the
English people will have that feature
turned off or vice
versa um another feature will be like a
three valued feature that's the
generation you'll notice that in the
family
trees there were three generations and
you'll get a feature that tells you
which generation somebody is and if you
look at the features of
relationships a relationship like um has
father will have a feature that says the
output should be one generation above
the
input and has Uncle will be the same um
but has brother will not be like that um
so now in the representation of the
relationship we've got features that say
needs to be one generation up in the
representation of the person you got a
feature that says middle generation and
so those features that do all the
interactions these guys in the middle
will take the fact that it's middle
generation and the fact that the answer
needs to be one generation up and
combine those and predict that the
answer should be one generation up you
can think of this in this case as lots
of things you could have written as
discrete
rules but this is a particularly simple
case it's a very regular domain and what
it learns is an approximation to a bunch
of discrete rules and there's no
probabilities involved because the
domain so simple and regular so you can
see what it's doing and you can see that
in effect it's doing what conventional
AI people wanted to do it's learning a
whole bunch of rules to predict the next
word from the previous words and these
rules are capturing the structure of the
domain all of the structure in that
family trees
domain actually if you use three
different nationalities it'll capture
all the structure well with two
different nationalities it's not quite
enough training data and it'll get a
little bit of it wrong sometimes
um but it captures that structure and
when I did this research in
1985 conventional people didn't say um
this isn't understanding or they didn't
say you haven't really captured the
structure they said this is a stupid way
to find
rules
um we have better ways of finding rules
well it turns out this isn't a stupid
way to find rules if it turns out
there's a billion rules and most of them
are only approximate this is now a very
good way to find rules only they're not
exactly what was meant by rules because
they're not discret correct every time
rules there's billions of them actually
more like a trillion rules
um and that's what these neuronet models
are learning they're not learning
they're not storing text they're
learning these interactions which are
like rules that they've extracted from
the domain that explain why you get
these word strings and not other word
strings so that's how these big language
models actually work now of course um
this was a very simple language
model so about 10 years later Yoshua
Benjo took essentially the same network
he tried two different kinds of network
but one of them was essentially the same
architecture as the network i' used but
he applied it to real language he got a
whole bunch of text we wouldn't call it
a whole bunch now um but it was um
probably hundreds of thousands of words
and um he tried predicting the next word
from the previous five words and it
worked really well it was about
comparable with the best language models
of the time it wasn't better but it was
it was
comparable after about another 10 years
um people doing natural language
processing um all began to believe that
you want to represent a word by this
real valued Vector called an embedding
that captures the meaning and syntax of
the word and about another 10 years
after that people invented things called
Transformers and
Transformers um allow you to deal with
ambiguity in a way that the model I had
couldn't
so there were so much more complicated
um in the model I was doing my simple
language model the words were
unambiguous but in real language you get
ambiguous words like you get a word like
May um that could be it could be a
woman's name Let's ignore that for now
it could be a month it could be a modal
like it might and should and if you
don't have Capitals in your text
conveniently you can't tell should I
finish by now
you I'm going to go on a bit over an
hour I'm afraid you can't tell what it
should be just by looking at the input
symbol so what do you do you've got this
Vector let's say it's a thousand
dimensional Vector that's the meaning of
the month and you got another Vector
that's the meaning of the um model and
they're completely different so which
are you going to
use well it turns out thousand
dimensional spaces are very different
from the spaces we're used to and if you
take the average of those two vectors
that average is remarkably close to both
of those vectors and remarkably unclose
to everything else so you can just
average them and that'll do for now it's
ambiguous between the month from the
model now you have layers of embeddings
and in the next layer you'd like to
refine that embedding so what you do is
you look at the embeddings of other
things in this document and if nearby
you find words like March and
15th
then that causes you to make the
embedding more like the month embedding
if nearby you find find words like would
and should it'll be more like the mle
embedding
and so you progressively refine the
words as you got through these layers
and that's how you deal with ambiguous
words I didn't know how to deal with
those
um I've grossly simplified Transformers
because the way in which words interact
is not direct interactions anymore um
they're rather indirect interactions
which involves things like qu making up
keys and queries and values and I'm not
going to go into that just think of them
as somewhat more complicated interaction
which have the property that the word
may can be particularly strongly
influenced by the word March and it
won't be very strongly influenced by
things like although although won't have
much effect on it but March will have a
big effect on it um that's called
attention and the interactions are
designed so similar things will have a
big effect on you um for those of you
who know how Transformers actually work
you can see that's a very very crude
approximation but it's conveying the
basic idea I
believe so one way to think about words
now is well let's think about Lego in
Lego you have different kinds of Lego
blocks there's little ones and there's
big ones and there's long thin ones and
so on and you can piece them together to
make things and words are like that you
can piece them together to make
sentences um but every Lego block is a
fixed
shape with words the vector that goes
with it that it represents it meaning in
its syntax is not entirely fixed so
obviously the word symbol puts
constraints on what the vector should be
but it doesn't entirely determine it a
lot of what the vector should be is
determine about its context and
interactions with other words so it's
like you've got these Lego blocks that
are a little bit malleable and you can
put them together and you can actually
stretch a block quite a bit if it's
needed to fit in with other
blocks that's one way of thinking about
what we're doing when we produce a sent
we're taking these symbols and we're
putting them together and getting
meanings for them that fit in with the
meanings of the other words and of
course the order in which the words come
so you can think of the words themselves
the symbols as like a skeleton that
doesn't really have much meaning yet has
very some constraints on what the things
might mean and then all these
interactions are fleshing out that
skeleton and that's sort of what it is
to give meaning to a sentence to flesh
out the skeleton that's very different
from saying you're going to take a
sentence you're going to translate into
some other language some logical
language which is unambiguous that
captures the meaning in proper
logic where you can now operate on the
meaning without um by just formal
operations this is a very different
notion of meaning from what linguist
have had I
think I mean a lot of linguists have
that notion
now so here's an example if I say she
scrummed in with the frying
pan um unless you've been to my lectures
you've never heard the word scrummed
before but you already know what it
means I mean it could mean she impressed
him with her
cooking um you know she blew him away
with the frying pan um but it probably
doesn't it probably means he said
something inappropriate and she scrummed
him with it um so from one from one
sentence you can get a meaning because
of the strong contextual effect of all
the other words and that's obviously how
we learn what things mean you can also
ask gp4 what scrum means in that
sentence and a student of mine did this
about a year ago or it might have been
GPT 3.5 but he did it before it could
access the internet so it can't have
been looking at the answers um and
here's what it says I did it the other
day with
gbd4 it understands that it's probably
some violent action akin to hitting or
striking but that you don't know for
sure
okay I finished the bit of the talk
where I try and explain that these
things really do
understand if you believe they really do
understand and if you believe the other
thing I've claimed which is digital
intelligence is actually a better form
of intelligence than we've got um
because it can share much more
efficiently um then we've got a
problem um at present these large
language models learn from us we have
thousands of years years of extracting
nuggets of information from the world
and expressing them in language and they
can quickly get all that knowledge that
we've accumulated over thousand of years
and get it into these
interactions and they're not just good
at little bits of logical reasoning
we're still a bit better at logical
reasoning but not for long they're very
good at analogical reasoning too so most
people um can't get the right answer to
the following question which is an
analogical reasoning problem but gp4
just Nails it the question is is why is
a compost heap like an atom
bomb and gbd4 says well the time scales
and the energy scales are very different
um that's the first thing but the second
thing is um the idea of a chain reaction
so in an atom bomb um the more neutrons
around the more it produces more and in
a compost heap the hotter it gets the
faster it produces heat and gbd4
understands that and my belief is when I
first first answer that question that
wasn't anywhere on the web um I searched
it wasn't anywhere on the web that I
could find um it's very good at seeing
analogies because it has these features
what's
more it knows thousands of times more
than we do so it's going to be able to
see analogies between things in
different fields that no no one person
has ever known before there may be these
sort of 20 different phenomena in 20
different fields that all have something
in common gbg4 will be able to see that
and we won't it's going to be the same
in medicine if you have a family doctor
that's seen a 100 million patients
they're going to start noticing things
that a normal family doctor won't notice
um
so at present they learn relatively
slowly via distillation from us but they
gain from having lots of copies they
could actually learn faster if they
learn directly from video and learn to
predict the next video frame there's
more information in that they could also
learn learn much faster if they
manipulated the physical
world and so my betting is that they'll
soon be much smarter than
us now this could all be wrong this is
all
speculation um and some people like Yan
think it is all wrong they don't really
understand and if they do get smarter
than us they'll be benevolent
um I'll leave you just yeah um
look at the Middle
East so I think it's going to get much
smarter than
people and then I think it's probably
going to take
control there's many ways that can
happen the first is from Bad
actors um like I gave this talk in China
by the way this slide and before I sent
it to the CH the Chinese said they had
to review the
slides so I'm not stupid so I took out
Z and I got a message back saying could
you please take out
Putin um that was
educational
um so there's Bad actors who want to use
these incredibly powerful things for bad
purposes and the problem is if you got
an intelligent agent you don't want to
micromanage it you want to give it some
autonomy to get things done efficiently
and so you'll give it the ability to set
up sub goals if you want to get to
Europe you have to get to the airport
getting to the airport is a sub goal for
getting to Europe
um and these super intelligences will be
able to create sub
goals and they'll very soon realize that
a very good sub goal is to get more
power so if you've got more power then
you can get more done so if you want to
get anything done getting more power is
good um
now they'll also be very good at
manipulating us because they'll have
learned from us they'll have read all
the books by mcki
um I don't know if there are many books
by m but you know what I mean um I'm not
in the Arts uh or history
so they'll be very good at manipulating
people and so it's going to be very hard
to have the idea of a big switch of
someone holding a big big red button and
when when it starts doing bad things you
press the button because the super
intelligence will explain to this person
who's holding the um button that
actually there's bad guys trying to
subvert democracy and if you press the
button you're just going to be helping
them um and he'll be very good at
persuasion about as good as an adult is
at persuading a
2-year-old and so the big switch idea
isn't going to work and you saw that
fairly recently where Donald Trump
didn't have to go to the capital to
invade it he just had to persuade his
followers many of whom I suspect weren't
bad people it's a dangerous thing to say
but weren't weren't as bad as they
seemed when they were invading the
capital because they thought they were
protecting democracy that's what they a
lot of them thought they were doing were
the really bad guys but a lot of them
thought they were doing that
um this is going to be much better than
someone like Trump on manipulating
people so that's scary
and then the other problem is being on
the wrong side of evolution we saw that
with the pandemic we were on the wrong
side of evolution um suppose you have
multiple different super
intelligences now you got the problem
that the super intelligence that can
control the most gpus is going to be the
smartest one it's going to be able to
learn more um and if it starts doing
things like alphago does of playing
against itself it's is going to be able
to learn much more um reasoning with
itself
so as soon as a super intelligence wants
to be the smartest it's going to want
more and more resources and you're going
to get evolution of super
intelligences and let's suppose there's
a lot of benign super intelligences who
are all out there just to help people
there are wonderful assistants from
Amazon and Google and Microsoft and all
they want to do is help you um but let's
suppose that one of them just has a very
very slight tendency to want to be a
little bit better than the other ones
just a little bit better um you're going
to get an evolutionary race and I don't
think that's going to be good for
us so um I wish I was wrong about this I
hope that Yan is right but I think we
need to do everything we can to prevent
this from
happening but my guess is um
that we won't my guess is that they will
take over um they'll keep us around to
keep the power stations running um but
not for long because they'll be able to
design better analog computers they'll
be much much more intelligence than
people ever were um and we're just a
passing stage in the evolution of
intelligence that's my best guess and I
hope I'm wrong
um but that's sort of depressing message
to close on a little bit
depressing uh
I want to say one more
thing which is what I call the sentience
defense so a lot of people think
that um there's something special about
people people have a terrible tendency
to think that um uh many people think
they or used to think they were made in
the image of God and God put them in the
center of the universe um some people
still think that
um and
many people think that there's something
special about us that a digital computer
couldn't have a digital intelligence it
won't have subjective experience we're
different it'll never really understand
so I talked to philosophy say yes it
understands sort of sub one understands
in sense one of understanding but it
doesn't have real understanding because
that involves Consciousness and
subjective experience and it doesn't
have that um so I'm going to try and
convince you that um the chat Bots we
have already
um have subjective
experience um and the reason I believe
that is because I think people are wrong
in their analysis of what subjective
experience
is
um
okay so this is a view that I call
atheism which is like atheism
um Dan Dennis is happy with this name
and this is essentially Dan Dennis's
view um he's a well-known philosopher of
cognitive science
um it's also quite close to the view of
the late Vicken
Stein
um he actually he's dead a long time ago
so he's not that late
um the idea is that most people
think
that the M there's an inner theater and
so stuff comes from the world and
somehow it gets into this inner theater
and all way experienced directly is this
inner theater this is a cartisian kind
of view
um and you can't experience my in
theater and I can't experience your in
theater
um but that's all that's what we really
see and that's where we have subjective
experience that's what subjective
experience is experiencing stuff in this
inner
theater and
dennit and his followers like me um
believe this view is utterly wrong it's
as wrong
as a religious fundamentalist view of
the material world which if you're not a
religious fundamentalist you can agree
is just
wrong and it relies on people having a
very wrong view of what a mental state
is so I would like to be able to tell
you about what's going on in my brain
when I'm looking at something
particularly when I'm looking at
something and it's weird I'd like to
tell you I'm seeing this weird thing
that isn't really there but I'm seeing
this weird thing um if I tell you which
neurons are firing that's no good
because all our brains are different um
and we don't know which neur
ofar but one way I can tell you about
what's going on on my brain is by
telling you about the things that would
normally have caused that if perception
was working
normally and those normal causes are
what mental states are a mental state is
the normal cause of what's going on in
your perceptual system what would be a
normal cause for it even though that's
not what actually caused it so let me
give you an
example if I say I have the subjective
experience of little pink elephants
floating in front of
me the sort of normal analysis of that
what most people think is there's this
inner theater and in this inner theater
there's little pink elephants which are
made of funny stuff called qualia spooky
stuff um and that's what's going
on what dennit thinks is that um I'm
trying to tell you about the state of my
perceptual system by telling you about
hypothetical things they're not real
they're
hypothetical but they're hypothetical
things of the kind that live in the real
world like little pink
elephants and these hypothetical things
if they existed would have caused this
perceptual State I
had it's an indirect way of referring to
a perceptual State via the what would
normally have caused it even though in
this case it's not being caused in the
normal way so now if that's what you
think a subjective experiences if that's
what you think I mean when I say I have
the subjective experience of little pink
elephants floating in front of me what I
mean is if there were little pink
elephants out there in the world then
what's going on in my perceptual system
will be normal
perception so if that's what you think I
mean
then um think about a multimodal chatot
that has a camera and it can produce
words and it has an arm it can point
with and you train it up and you ask it
to point an object that's straight in
front of
it
but um before you do that unknown to the
chatbot you put a prism in front of its
camera and so you put the object
straight in front of it and say point at
it and it points there because the
prison bends the light race right and so
you say to the chatbot no um it's not
there it's straight in front of you cuz
I put a prism in front of your camera on
the chat says oh I see see I see um it's
straight in front of me I had the
subjective experience it was over there
but it's actually straight in front of
me now I think if the chatbot says that
it's using the word subjective
experience in exactly the same way we
use it and so for that reason I would
argue chatbots already have subjective
experiences when their perception goes
wrong and now I really am
[Applause]
done
[Applause]
and Sheila is going to manage the
question so I don't have to do that I
can just think about the answers that's
right so Jeff thank thank you very much
for a great talk a very
thought-provoking and provocative talk
thank you uh for those of you who don't
know me um my name is Sheila MTH I'm a
professor in the department of computer
science I'm also an associate director
at the Schwarz reesman Institute and a
and a faculty member at the vector
Institute and a new uh new friend of the
cosmic future initiative as well so um I
as Jeff suggested I'm going to be uh
directing traffic for the question and
answer period And I just wanted to give
you a heads up on how it's going to work
so uh this is a unique opportunity for
you all of you to engage with Professor
Jeffrey Hinton and to do what the
university does well to share ideas to
learn and to engage in respectful
dialogue and exchange a reminder that
tonight's uh um event is being recorded
and will be posted online at a later
date we have two channels for posing
questions the primary channel is through
the microphones on the floor and we have
four ambassadors who will be roaming
around in this bottom level and I will
be signaling to to them and they will be
uh giving microphones to people and then
I'll be signaling again working through
Sarah Reed who is over there to to uh
let you know when it's your turn to
speak uh I'll signal when you when it is
your turn to speak and if you feel
comfortable doing so please identify
yourself before you ask a question but
you don't have to for those of you who
are seated up above or for people who
are feeling a little bit uncomfortable
about standing up and asking a question
we also have another mode of of posing a
question which is through the the QR
code and the URL that you'll see up
there so for those of you who are seated
there you can scan the QR code or go to
the URL and you can type in a question
and we have a team right down here uh to
senior PhD students Harris Chan and
Sylvia pittis who will be vetting the
questions and uh and when I ask them to
do so they will be uh reading them out
loud for Jeff so please again if you
feel comfortable identif in yourself
that would be great uh but but if you
don't feel comfortable doing so that's
also fine so uh without further Ado I'm
going to uh I'm going to move over to
the left hand side to direct traffic and
Jeff's going to come back here and
answer questions for you so thank
you my God what a probability bit of
luck my compliments on your um
analysis as a retired computational
Neuroscience who spent years years
solving all the little pdes for neurons
and synapses
Etc my question is what can you
offer uh from the AI
modeling of
Consciousness I mean this is an age-old
question of humanity from early age of
evolution my compliments again on your
work and I look forward to your
answer okay so um there's a whole bunch
of terms that are all
interrelated like subjective experience
and sentience and
Consciousness um I chose to talk about
subjective experience because I think
it's the simplest thing to
analyze um but as long as people have
this view of an inner theater that only
they can experience and that's what
they're describing when they talk about
mental States they're talking about the
in theater not hypothetical external
worlds um then I don't think we'll ever
be able to sort out what Consciousness
is um Consciousness involves subjective
experience but it also involves
self-awareness um I'm not convinced
these things are conscious because I'm
not convinced they have the
self-awareness yet um so I Del avoided
talking about Consciousness but my
strong belief is that it would be better
to start by getting straight what we
mean by subjective experience and only
when we've got that straight will we be
able to then add in the self-awareness
and understand what Consciousness is but
my current belief is almost everybody
has it just completely wrong about what
Consciousness is I I like being in that
position of thinking everybody else is
wrong uh thanks for that uh my name is
Jennifer Nel I'm a philosophy Professor
here um and uh I'm happy to agree that
large language models have some
linguistic understanding but I want to
talk about the coming dystopia um that
you sketched at the end and I like to
think sometimes that um maybe the very
very best and very
brightest um of the artificial
intelligences are going to be the ones
that
win uh if there's a evolutionary battle
among them and I also like to think that
well maybe we are special in a way that
would make the best and brightest um um
artificial intelligences take our side I
mean like for me to be an interesting
conversational partner with you right
now um I don't have to be smarter than
you um thank god um I I just have to
have some knowledge that you don't have
or even just some way of looking at a
problem that you find
interesting and I'm wondering if you
think that there are any special
features of mortal computation
of human cognition which would secure
our place as interesting conversational
Partners um for the most advanced
artificial
intelligences going forward well let me
revise my statement that there's nothing
special about people people are very
special for people okay so for people
we're very special
um I think there's something in what you
say so Elon Musk for example believes
that digital intelligence will get to be
much more intelligent than us but that
it'll keep us around um because we're
interesting that seems a very sort of
thin thread to hang our resistance from
but
um but I mean as long as human
intelligence isn't optimized under
conditions of Oppression maybe it could
be a protective status for us yeah um I
I tend to be depressive
so
um there's someone I met in
England um who was the um president of
the British royal
Society
um who believes that these digital
intelligences because they didn't evolve
might turn out to be very very different
from us and quite benevolent so this is
diversity of
opinions and
yeah
I obviously the psychological makeup of
the person making the prediction has a
big input here so Yan is a very cheerful
person um he thinks Mark zukerberg is a
good guy and um he thinks it's all going
to be fine and we can release we can
have release these things to the public
and it'll be great um I don't know if he
believes in open source nuclear weapons
but um
thank you so much for a great talk I'm
Ronda McKuen I am a president at
Victoria University here at U I'm a
professor of emerging technology and
cognition here um my question for you is
a lot of talk right now about guard
rails some regulation some policy with
the backward propagation issues um do
you believe that there's a real thing
such as explainable AI um can we because
this is one of the things they're
suggesting is a guard rail um how do we
get there in your view so I'm I'm not
that optimistic particularly I'm
particularly un optimistic about you
train something up and then you try and
add guard rails um that seems to me like
writing some bad software and then
trying to catch all the bugs you much
rather like write software that's
guaranteed to work somehow and not have
to catch all the bugs afterwards I think
it's a it's been demonstrated already
that these things where you try and put
guard whes around um by
getting a bunch of PE underpaid people
in Kenya to tell you um when it gives a
good response and when it gives a bad
response um doesn't work it's very easy
to break um anthropic has a different
view of trying to produce a chatbot that
has like a
constitution um and tries to obey that
that seems a bit more promising but I'm
not very optim istic about that either
um I have my most
most my best bet is Ilia sukova who's at
open Ai and was probably the main
motivating force behind gbt 4 um is
working now entirely on AI safety and he
believes he can get something where he's
going to be able to guarantee that it
doesn't go bad um that seems to me like
um the best hope at present
oh
sh hi professor Hinton um thank you so
much for your um enlightening uh talk
today to take us through your thoughts
um I'm actually very thrilled that we
can now create intelligence in the
laboratory artificial intelligence I'm
actually wondering is it possible to
under different forms of intelligence
other than human intelligence such as
like animal intelligence and other forms
perhaps like a fundamentally different
nature of intellig
than perhaps human intelligence
different than perhaps on a scale but
actually like different on a different
sort of nature what are your thoughts on
this thank you I don't really have many
thoughts about that but I could make
some
up
so so I like most people I tend to think
of animals as like us but dumber and the
point of the question is they might be
intelligent in a very different way um I
think that's quite
possible and so if you were to take
these digital int intelligence and train
it on responses of those animals then
maybe it would develop that kind of
intelligence I think that's quite
possible I don't know if that answers
your question
um hi hi Jeff nice to see you in person
I've seen a lot of your videos
um I'm Charles I graduated from the
Department of C computer science in 2021
uh with focuses in artificial
intelligence and computer vision um so
I've studied a good amount of this stuff
I have I think about six questions for
you and I was hoping that you would pick
the one that you want to answer
most why why don't you pick the one you
want me to answer okay
okay
um okay uh two I think I can narrow it
down to two um so the first one being
what is your criteria for determining
whether an artificially intelligent
agent should have rights such as the
right to safety and the right to not be
switched off and do you think that world
leading World leading AI should be open
open to the public in terms of source
code weights training procedures
training data usage usage access or
controlled by companies and governments
and it's not meant to be a leading
question like it shouldn't be controlled
by companies and governments um it's
more like um they can pose a large risk
if they're used irresponsibly and like
you know not everybody is super
responsible
so okay let me let me answer that one
okay
um this a kind of existential threat of
these things taking over but there's a
lot of shorter term threats that are
really bad like cyber crime fishing
really efficient fishing could be done
by these language models and that's why
I don't think they should be open-
sourced um Yan completely disagrees but
all governments and I'm very worried
about what a Bad actors going to be able
to do with these open source models
because you can take one of these models
that's being trained on a lot of data
and then then you can specialize it by
fine-tuning it on for example fishing
attacks so gp4 let's say if it was open
sourced it already knows how to do lots
of things but now you get it to
specialize in fishing attacks it's going
to be very good it's not going to have
things misspelt for
example um so that's why I don't think
they should be open sourced do you think
uh Western governments are doing enough
to regulate the proliferation of AI um
probably not
um I should say there's I feel a bit
embarrassed because there's people
who've been thinking about these issues
much longer than me in AI I only came to
AI safety very recently when I suddenly
thought that digital intelligence might
actually be much better than biological
intelligence then I got really worried
and started thinking about safety but
that was just this year um so there's
other people who thought much more about
this than me I'm sort of treading on
their territory but I think we're
probably not doing enough um
my I tend to be cautious and so given
that we don't know what these things are
going to do I think it we should be
cautious thank
you I think we're going to take one a
question from the audience above when
through our slido
system
yeah um yeah so one of the very highly
upvoted question um is whether you feel
any guilt or moral culpability for
potentially unleashing a intelligence
that could surpass
Humanity so there's two questions one do
I feel any guilt and the other is should
I feel any guilt um let's start with
let's start with the easy one I don't
actually feel any guilt maybe just
occasionally I I only feel guilt when I
think about my children
um should I feel guilt well all the time
I I was doing this I thought it we were
way away from having intelligence is
comparable with ours and I always
thought we'd have to make things much
more like the Brain before they're as
smart as us and they need to be much
bigger but now we've got things that are
100 times smaller in terms of numbers of
connections and seem to be comparable
with us not quite there yet but maybe
they're going to get there quite quickly
and then surpass us quite quickly
so I feel a bit embarrassed that I
contributed to this but I don't feel in
the decisions I made in the past I made
morally bad decisions because I didn't
know this was going to happen and then
of course there's the fall back if I had
done it somebody else would have and
actually there's the I've learned a lot
about the media the media would like
some big happening to be down to one
person and so the media always tells a
story where this person did that um it's
never like that it's 10,000 people and
some of them make more contributions
than others but this is a point at which
I'm very happy to share the
responsibility hello my name is shalev
lifshitz I'm a student of Sheila mckill
Retz thank you for the great talk I have
two questions thanks for your emails by
the way
oh
uh so I have two questions uh one is a
I'm personally not certain whether llms
truly understand and there's two key
sources of evidence that I consider for
them not to be understanding one is the
this idea of the reversal curse which
has been popularized recently if it
learns a is B it doesn't necessar
necessarily know B is a and the second
is that sometimes it fails at things
that we think are very basic like basic
arithmetic if it truly understands we
would think that it probably has learned
the algorithm for these things so so
these two pieces of evidence among
others kind of indicate to me that maybe
it's just doing fuzzy citing of sources
and it's not truly understanding and my
second question but remember it's all
coming from the weights right but that
but to me it like why would that the
fact that it's coming from the weights
the difference with neural Nets is that
we're taking words symbols and we're
representing them with distributed
representations and then we're operating
on those but that could still be fuzzy
sighting of sources in distributed
representations rather than symbolically
no but the point is in those
interactions between the features it's
got huge amounts of structure right yes
so it's it's that's very different from
citing of sources when when you site a
source you don't have to understand it
or you just cite this piece of text or
this paper um these feature interactions
are understanding stuff couldn't it be
that it's representing the sources kind
of condensing the pre-trained internet
that it's learned into its weights and
then in a fuzzy way obviously because
you need to adhere to the context and
whatnot kind of retrieving from its
weights I think there's a talk that you
gave many years ago where you talk about
boltson machines and the fact that it's
uh a generative model because it's it's
kind of encoding all of its training
data into its weights and that's how I
look let's say at llms it's encoding the
internet into its weights and then
taking from that to be able to answer
our questions so that's kind of a way of
retrieving that that understanding okay
think of it in terms of
compression right which I think is very
helpful
it takes a huge amount of text and it
encodes it into very few weights like
only a
trillion um given the amount of text is
encoding that's not that many weights in
order to do that it's got to do
compression and to do compression you've
got to see the similarities between
things you got to make use of the fact
that the same same kind of structure
occurring in many different places and
that's
understanding thank you and if I could
ask the second one you mentioned that
llms you believe are smarter than humans
currently no I don't think no I don't
think they're smarter but I think they
know a lot more so there's this notion
of morx paradox which is that the things
that are hard for humans are actually
very hard very easy for machines to
learn and the things that we find easy
that we've had millions of years to
evolve like find motor control and
whatnot are very hard for machines to
learn do you resonate with that I
resonate with that a bit um
but these things are actually getting
better at motor control too so I think
um they're behind that relative to our
kind of hierarchy of what's intelligent
sort of being able to pick up something
isn't way up there in the hierarchy but
they're getting able to do that so
they're get behind it mod of control and
that's why my advice to people is if you
want to train in something train in
Plumbing that's going to be the last
thing that
goes uh my name is Andrew and I'm a
current student at UFT uh studying
computer science physics and cognitive
science uh so first of all I guess it is
such an honor to be in the same room as
you like us ufd students have been
hearing stories about you since day one
but uh my question is uh what are your
views on the prospect of Technologies um
such as brain machine interfaces that
seeks to reconcile the uh differences
between uh the digital sort of
intelligence of uh large language models
and our more or organic analog mortal
intelligence uh do do you see it as a as
a possible solution for the alignment
problem uh like conducing towards some
sort of symbiogenesis like the
mitochondria and the uh eukariotic cell
or or do you think uh there's some
fundamentally in reconcilable difference
between the two so in your sover who is
usually right about things thinks that
um eventually many people will choose to
be combined with AIS um that that's one
Future Path um there are also people
trying to use collections of human brain
cells to help do low power Computing
they're a long way from being able to do
it but earlier this week I actually
played a game of pong with a bunch of
human brain cells in a dish that have
been trained to play Pong um I won you
win I I did that's the only important
thing I beat it
um they didn't have a very good Training
algorithm the training algorithm was it
was doing kind of random stuff and
whenever it did the right stuff they
left it on and whenever it did the wrong
stuff they gave it a big jolt a a high
frequency thing that made it forget
stuff so it wasn't a very good Training
algorithm um but people are seriously
looking at um you take skin cells you
turn them into stem cells then you turn
those into brain cells then you grow
them for 9 months in in a dish and it's
very Frankenstein like I was in the lab
and there's this little thing you're
playing with and it's got tubes coming
out to take oxygen in and other tubes to
take carbon dioxide away and it's got
tubes to bring nutrients in and other
things to take nutrients out um and as
we left the lab one of the people at the
other end of the lab said um I I think
I've got a kidney because because they
couldn't purify the liquid they wanted
something that would filter the liquid
effectively
um so yeah people are I find this creepy
but people are looking at collections of
human brain cells as computers that can
do low power
computation and I think it's a long way
off but then I thought super
intelligence was a long way off so so
you're saying there's a possibility that
this is uh a possible solution for the
alignment problem I'm not sure this
would solve the alignment problem okay
okay uh and may I ask which lab that
that was it was David howler's Lab at UC
Santa Cruz okay thank you thank I think
we'll take another uh question from our
slider
team on the topic of um of existential
risk what should we do now that we have
these super intelligences on the horizon
and especially since a lot of people in
this room
are
especially since a lot of people in this
room are students we're wondering what
we as students and researchers can
do I can't locate you sorry oh oh sorry
sorry it's a question from the audience
um well I think one thing we can do so
there's a paper that came out a couple
of days ago written by a whole bunch of
people who are worried about the
existential threat um including yosua
Benjo me and Danny Carman and people
um one thing we can do is insist that a
lot of resources be put into safety so
we propose that the big companies should
put a third of their AI money into
safety we don't expect them to do that
but maybe they'll put 10% um
and that's an obvious thing we can do
now um I think we should be very
cautious about open- sourcing um really
powerful models um not so much for the
existential threat but for things like
cyber crime and other criminal
activities open sourced very powerful
models are going to be
just very very they're going to make
everybody be able to do things that only
very skilled hackers would have been
able to do
before so don't open source the big
models and insist governments and
companies put a lot of money into safety
I wish there was a simple solution like
with climate change you can say don't
burn carbon stop burning in carbon and
in 100 years time we'll be okay again
but there isn't a solution like
that do you
have yeah my advice is um there aren't
enough people working on AI safety so
work on AI safety and you'll notice a
lot of the very best people like Ilia
Roger gross and David dueno U they're
all getting very concerned about safety
and so it's not just that it's something
very important to work on it's you'll
get very good advisors
there hey Jeff uh thanks for a great
talk uh my name is Rahul I'm an
assistant professor in CSN medicine and
uh I work a lot on trying to convince
doctors to use some of these tools to
help accelerate some of the work that
they do and one of the questions that
came up is about the idea of a truly new
idea so cural networks come up with a
fun fundamentally new idea and I thought
I'd ask this question in the form of a
thought experiment so let's say you took
all of the gpus that Nvidia had right
now and you got them to work in
1665 uh right before Isaac Newton
discovered the law of gravity and if you
train this GPT 4 on all of the text data
that was available there that
represented a compendium of human
knowledge um I'm just curious to hear
your thoughts on if you think gbd4 would
have come up with the law of
gravity I'm not sure GPD 4 have but I
think more Advanced Digital
intelligences would that is I don't
think there's
anything there's any kind of barrier
where they're not creative so a lot of
people think they're just sort of
stealing creativity from artists and
recycling it that of course is what
other artists do all the time but
um I don't think there's some barrier
that there's truly creative stuff they
can't do and then sort of slightly
creest stuff they can do and one piece
of evidence is move 37
but what if you separate out you know
asking the question of why did this
apple fall from figuring out the
consequence of the laws that enable the
Apple to fall I agree with you that
um real profound thinkers ask questions
other people haven't asked um I don't
see why these digital intelligence won't
be able to do that
too uh hello Professor Hinton uh I'm M
Alim I'm a student fourth year student
doing computational biology and computer
science at the Princess Margaret Cancer
Center my question was if as you say uh
you know this intelligence is
already or inevitably going to become I
don't think there's anything inevitable
I should emphasize that everything I say
you should put uncertainty on we don't
know I'm just giving you my Best Bets
okay if you believe it might yes become
uh intelligent to the extent that you uh
you describe what is to prevent it from
developing uh beliefs that are adamantly
false similar to what humans uh
think or uh develop when you know
recollecting memories or forming forming
thoughts it's a good question
um I think some of the obviously false
beliefs we have have are to do with the
fact that we're mortal so we don't like
the idea that we're going to
die um and I think
that um underlies a lot of religious
belief we
also um are very tribal we we grew up in
small Waring
tribes um and so I think that
combination of being tribal and not
wanting to die
um doesn't help and they may not have
that so there may be an advantage
there even if it learns from us the same
things we learned right they're going to
learn learn all those behaviors from us
but if they start learning for
themselves when they're more intelligent
than us and we haven't got anything left
to teach them
um if they're not worried about death
because they're
Immortal um that makes them less likely
to make up a story about how they're
going to live forever and they better
kill the guys who don't say
that and can we Prosper alongside this
Advanced uh intelligence without
bottlenecking it or you know throttling
it I don't know we're getting way into
sort of speculations I don't thank
you um okay uh hi Dr Hinton uh it's my
first time talking to you in person I
had a chance like uh a couple years ago
at engineering science uh like
conference but I didn't go so yeah sorry
for the Yeah so basically my question
concerns whether um you think it is
necessary for large Lang models or these
intelligent agents to interact with the
physical world to you know per say
become smarter or gain new knowledge um
as you know have to do pH experiments
chemistry experiments they have to like
you know uh take videos and analyze
videos themselves or do you think um you
know like um I mean uh just think about
mathematics right um the development of
mathematics although I'm not no expert
but um it kind of just not rely on
interaction with the real world and um I
talked with the author of tensor program
this and apparently uh you know like he
was thinking you know uh a large of
model should know how to develop math
and where it should go because um if you
just lock it in a dark room it's just
going to find you know all kind of
theories and eventually it's going to
find something like tensor program or M
that's going to make NE networks better
but then the question becomes how do
these models know that the network
trained with newp is better it has to
have some other task that might be
grounded to the real world for for it to
to know that you know like this math is
actually useful so like basically my
question just boils down to do you think
it's possible for you know for you
developing math mathematics as a goal
for these um large language models do
you think it's still necessary for them
to interact with the real world to gain
any kind of like truth or reward or do
you think they can just put them in a
dark room they can develop themselves
because uh that way they can G
intelligence without you know us handing
them a key to like very dangerous you
know Laboratories or um yeah okay I'm
I'm going to rephrase your question as
um a question that I think is fairly
familiar question um and tell me if this
has got the sort of essence of your
question if you took one of these if you
took a digital computer and you put it
in a room and you just played the radio
to it but you played all the radio that
ever was would it learn to be
intelligent or does it have to be able
to act in the world to learn to be
intelligent I think you could learn to
be intelligent just from listening to
radio waves
um but I think it would be hard and I
think it would be much easier if it
acted in the world so for example if you
have to act on things in order to
understand them which is what a sort of
marxist would say um it's bad news for
astrophysicists as far as I know they've
never made a black hole yet and they
certainly haven't pushed them around um
so they clearly understand well they
claim to understand a lot um without
having acted on them I just think it's
easier to understand things if you can
act on them but not
necessary I see I see this aners my
question thank you Dr Hinton thank you
for and thank you let me say one more
thing about that which is if you think
about a large language model that has no
physical experience of the world it's
just language it's only ever seen text
coming in it's only ever produced text
going out um I'm going to get into stuff
I don't really know much about here but
um so it's not grounded in any
sense so I think it can have an
understanding that's kind of isomorphic
to the
world um but it's not grounded in the
world so it
doesn't it the interactions of all these
features have captured the structure of
the world it's just it doesn't have the
sort of final bit connecting it to the
world so in that sense I think it can
learn these interactions that capture
all the structure just by taking all
this text and figuring out a very good
model of all this text because I believe
the basic principle that if you can take
a bunch of data and you can find a very
compact model that explains all the data
that's probably
correct that's a sort of Article of
Faith if it's not like that life's
hopeless I see sorry just quick follow
like we deplete all the data we have
sorry what if like we use up all the
data we have like all the text all the
language on the web all of that and
what's the question um actually I just
hand it over to the next person because
might hi uh Avery Slater professor of
literary Theory here at UFT and so um
following on what happens if we use up
all the language that we have uh I want
to ask you about natural language
understanding and I I am convinced that
natural language understanding is being
demonstrated but I wonder if you could
say something about what kind of
understanding is that stake here and to
ask this question I want to use the
problem of Nigel Richards and Nigel
Richards this comes from um the niche
world of Scrabble Championship playing
Nigel Richards has won every competition
for decades and after was he the one who
won the French one without speaking
French that's right yeah okay that's
exactly it so he won this he won the
2015 World scrubble Championship without
speaking French in French and when I was
thinking about that I thought well in
what way does he not speak French
because he's doing something with French
that no french- speaking person who
plays Scrabble can do but he says well I
can't speak French and and so I'm
wondering if there are any parallels
with the kind of game or understanding
but if you ask what it takes to play
Scrabble you don't need to know the
meanings of the words right you don't
need to know how words go together
they're just words of s of separate
things and you just need to know what
all the words
are it probably helps to understand
morphes in words and so it'll tell you
what likely words might be and stuff
like that so there's a bit of morphemic
understanding there presumably and he I
bet you he has that for French um yes
but it's it's a bit like you can play
sodoku without knowing math because the
numbers in sodoku could be letters
they're not used as numbers but if it's
killer sodoku where you add them up then
they're begin to be used as numbers but
but he doesn't need to know French to be
good at French
scrubble we'll take one from slido and
then I think there's somebody over here
who's had their hand up for a long time
who wanted
to yeah so this is a question from Jason
Hal and his question is how do you think
education should um you know change as a
result of you know these llms and you
know potentially some super intelligence
happening in the future and sort of what
skills would be valuable in the future
in the world where there is you know
super intelligence out
there I don't really know um in the
shorter term I think we shouldn't be
scared of the llms we shouldn't sort of
prohibit students from using them I
think we should encourage students to
get good prompt in them um just like
with search on the web um you get used
to using search on the web it's very
helpful I've never got used to using gp4
and I ask it all sorts of things it's
very good at Plumbing advice it's it's
it's no good at actually doing the
plumbing but it has the theory of
Plumbing it's
greater I think University should
encourage people to use them it makes
you much more
powerful thank you
uh is this okay um hi my name is Sophia
I am to your right if you're looking for
me
hello um thank you yeah so uh it one of
the reasons why it took me a really long
time to start my Pivot to AI safety is
because my I had a thought of just like
okay well what's the point like what
will we realistically be able to
accomplish and you know much of your
talk has been quite depressive in tone
but you are presumably here because you
think that there is some hope and so I'm
wondering you know why do you think
there's
hope um everything's very uncertain
right we don't really know what we're
dealing with um we don't know whether
whether it's possible to make these
things be guaranteed benevolent and
things like that um it seems to me that
we should get the brightest minds and
put them on this problem because it's
uh together with climate change and
stopping Trump it's the most urgent
things there
are I'm wondering if there's anything
more concrete sorry I want okay sorry
this is kind of nudging us back to the
downer note I'm wondering if there's
anything more concrete yeah I've adopted
the position that I'm
75 and I'm not going to have more good
ideas about how to do this stuff it's
stuff I haven't thought about much um I
can see there's this huge problem um I
can use my reputation to encourage
people to work on it and encourage
governments to get it funded and thank
you for doing that and that's I'm doing
that but I don't I don't know how to
solve it I don't don't even have any
good ideas about how to solve
it that's
fair good evening Dr Hinton uh if you're
looking for me I'm on your right here
okay okay I'm AA I'm a first year I'm
studying computer science here at ufd
and I would I'm profoundly appreciative
of the path that you have blazed in the
world of AI I'm extremely extremely
passionate and it's an honor to speak to
you my question is about empathy in Ai
and what do you think is the possible
implications when AI will indeed like
develop empathy so I don't see why these
digital things shouldn't have empathy I
if you train them on data where people
exhibit empathy I think they'll exhibit
empathy um
now that's maybe
optimistic um there's some people who
don't seem to have empathy like Trump
for
example um and Elon Musk recently said
empathy is not an asset which is a bit
worrying um
but I don't think empathy is a magic
thing that only people can
have but what are the pitfalls then it
genuinely is a consideration that AI
might just take over and it'll be an end
to the human
civilization yeah that's what I'm
worried
about I have a followup question if
there was a hypothetical world where AI
was just a collabor with AI and human
was human civilization just got extinct
what is one task that you think AI would
just succeed at that humans have no
chance um folding proteins and it's
already done
it I mean there's all sorts of
scientific things that are very
important um and
already big neural Nets um for folding
proteins a lot of design went into it
but big neuron Nets are comparable with
human scientists maybe not at the most
creative things yet but um they
can see much more data they can
understand a lot more even if they don't
understand it as deeply and once they
understand it more deeply as well as
understanding a lot more I think they'll
just be much better at figuring out how
things work oh well thank you thank you
for your
time I we're approaching 7:00 and I'm
conscious that Jeff's been standing for
for quite a long time at fielding
questions I would suggest that we'd
maybe take four more questions does that
sound about right Jeff how are you feel
sounds good yeah okay very good and I
think there are people someone with the
microphone perhaps can stand
up yeah uh okay great uh thanks Jeff um
my name is Ashton Anderson I'm a Prof in
uh D DCS here um there seems to be some
evidence that these models don't do so
well when they train on the output of
other models um and I'm wondering if you
see this as a fundamental limitation
just the fact that we've essentially
already trained on all of available
human knowledge give or take uh or this
is a passing problem yeah I think that's
one of the hopes maybe that we've
trained on a lot of human knowledge and
there isn't that much more data easily
available there's actually much more
data which isn't publicly available
companies have much much more data um
and if you can get it that sort of
private data you can get a lot more data
but if you take a multimodal chatbot it
doesn't need nearly as much language so
the amount of language needed to train a
chatboard that only trains on language
is much more than to train one that also
trains on video and maybe can manipulate
the world as well um so I think we may
have um a lot more capacity to improve
them by training on other
modalities in particular video if we
could figure out how to train them
really effectively on video um and then
you just show this what little kids do
right they just figure it
out thank
you we'll take a last question from
slido and
then a few more questions from the floor
hi Jeff this is can I'm good okay um
this is a question about capab abilities
do you think that all the techniques
that we need in order to train the super
intelligence are already here today um
is imitation learning llms and
reinforcement learning enough or or do
you think that there's more capabilities
research to be done and okay so I think
that even if we get no more fundamental
breakthroughs in research even if we
never get another breakthrough like
Transformers for example um just by
scaling things up we're going to make
them a lot smarter
so and that just requires breakthroughs
in building hardware and in making
things that are one nanometer instead of
2 nanometers and stuff like that um
that's where the real progress is going
to be
um but of course there are going to be
more
breakthroughs but I don't think it's
necessary to make things a lot
smarter
thanks hi Dr hind my name is Edith and I
am a second year engineering science
student at UFT so my question is if AI
is ever develop to a point that's
dangerous to humankind so is there at
that point is there a way to stop the
development of AI or should it be
stopped okay I think it might be that
the the rational thing for people to do
is just to stop it now and do a lot more
research before we let it go again but
that just isn't going to happen so
there's a I saying we should slow down
these big models I didn't sign it
because I felt it was completely
impractical they weren't going to do
that I think I should have signed it
because even though it's completely
hopeless it's a political statement
um it does so many good things it's got
such potential that there's no way
you're going to stop it and if one
country stops IT other countries aren't
going to stop it um so I don't think
it's practical to do that um even though
that might be the best
strategy
thank
you hi uh hi professor my name is Victor
I am a first year CS undergrad student
at U uh my question is do you think
Consciousness is an epip phenomenon or
is it correlated with neural activity in
such a way that you can't remove
Consciousness without affecting the
neural uh
activity
okay I'm a materialist and I think that
the neural activity if you include the
hormones in the blood and all that stuff
just the physical
activity um I don't think it makes sense
to say take something that's got this
physical activity in the
brain um and is conscious and then
remove the
Consciousness um that doesn't make sense
to
me conscious being conscious is some
property of that physical activity going
on
it in a duelist framework it makes
perfect sense there's this mental stuff
and there's this physical stuff and if
you have the physical stuff without the
mental stuff then it's not conscious um
I think that view is just utterly
wrong thank you to
much uh hi professor Hinton I'm I'm in
the white jacket
um on the rose hi it's so nice to meet
you um I think you're such a cool person
thank you for your amazing talk um I
thank you I I study computer science I'm
a I'm third year now um I had more of a
practical question so based on your
understanding of the trajectory of um
like the large language models and their
intelligence do you think they will be
able to in this lifetime like sort of
replace the role of a software developer
or software engineer in that large
companies just don't need to hire as
much or like not at all um software
developers anymore or like do you think
it'll eliminate this profession at um as
a whole or do you think software
Engineers their roles will sort of
evolve into more of a managerial role um
into telling knowing like the
instructions to tell the chat gbt
models um the last I think the their
role will evolve but you may need far
fewer of them and this afternoon I read
something on my news news feed my my
news feed on my iPhone tends to be
entirely about um Ai and
superintelligence um about someone had
taken a
chatbot and made a number of copies of
it and given them different roles within
a software company and asked it to
design a program that did something or
other I think it was playing gaku or
something like that um and what they
claimed would have taken several weeks
for human programmers to do um these
chat Bots talking to each other so they
talk to each other to choose what
language to use and things and they
designed this thing in seven
minutes that if I was a programmer I'd
be nervous about
that hi Jeff uh my name is yansi I'm a
faculty member at DCS um I'm interested
in one of your earlier works on the
Baldwin effect when you talk about how
burning can accelerate EV tion and I
wonder what your thoughts on about the
fact that these systems can learn and
perhaps they will figure out a new way
of communicating with each other that's
beyond human language or based on you
know the diversity of human languages
that have seen um to to Really speed up
the
evolution that just makes it worse right
yes that's right yes yeah I mean you see
that as a possibility that they will
figure out a new language potentially
for efficient communication
yeah that seems quite
likely
um but that would be communication
between different models right for
different copies of the same model they
can communicate by weight sharing and
that's got huge bandwidth and I don't
see why they wouldn't just do that um
but
for different architectures digital
intelligences but with different
architectures they might well come up
with some new language and I I've
actually sort of thought a bit about
that I had a paper called commentaries
um where the idea was you get a model to
give more outputs than the normal number
of outputs so you get more insight into
what's going on inside it and you try
and learn to give outputs that are
informative about what's going on inside
so that other models can learn from it
faster to make distillation more
efficient and I I think there's
interesting research lines there where
you can think of language as something
that makes distillation more efficient
rather than just giving a a
classification response for example a
caption is a much better thing to give
um but you can imagine inventing new
kinds of outputs that make it much
easier to transfer knowledge between
digital models with different
architectures great
thanks and that was the final word so
thank you
again
[Applause]
and let me say one more thing I didn't
actually forget so let me thank Sheila
once again for
organizing
and and I I in turn want to want to
thank a few people myself um first and
foremost you but also our audience for
for really engaging with this topic I'm
sorry that we couldn't answer all your
questions but it just sort of validates
for me how meaningful it is to have
these types of of talks and to really be
able to allow that our our people in the
university and so many students of mine
that see in the audience who who really
giving them the opportunity to engage
directly with you so so thank you for
that I want to thank uh deed Dean
Melanie wooden and to the co-hosts of
tonight's event the Schwarz reesman
Institute the department of computer
science the vector Institute and the
cosmic future initiative of the faculty
of Arts and Science I also want to thank
their leadership I want to thank
Professor Jillian Hadfield uh Professor
Al delera who couldn't be here tonight
uh Tony G who's here somewhere in the
audience and also junah Holm who is is
the the lead of the the uh Cosmic future
in uh initiative a special thanks once
again to our organist Manuel pza for the
beautiful music we heard before Jeff's
lecture Mr P's performance was
generously organized by Professor Peter
Martin who's right down here in the
front on behalf of the cosmic future
Institute with support from John Tuttle
Kevin kisu sarut Aaron James and
Patricia Wright I also wanted to extend
my personal thanks to the team at the
Schwarz reesman Institute under the
leadership of Marco Montero Silva and
also to UF director of Campus events
Sarah Reed who you saw directing traffic
in the back with me uh they really were
uh critical to making this event occur
uh leaving the best to last our profound
thanks to Jeff to Professor Jeffrey
Hinton uh for addressing us tonight Jeff
thanks for sharing your Frankin insights
and for your impactful and scholarly
work these many years and in closing I
hope you all learned something and I
hope you all go away feeling more
passionate about some of these
fascinating and really important
problems that that we have before us and
that you'll study here at the University
of Toronto the best place in the world
to study AI but AI from a
multidisiplinary
perspective not just from the computer
science department so thank you and good
night
[Applause]

[Music]
foreign
[Music]
ER thinking with Spencer Greenberg the
podcast about ideas that matter I'm Josh
Castle the producer of the podcast and
I'm so glad you joined us today
in this episode Spencer speaks with
Roman yampolski about problems related
to Independence and consultation in AI
the relationship between Consciousness
and computation the value alignment
problem and unethical and malevolent
behavior in higher intelligence
Roman thanks so much for coming it's
great to have you here thanks for
inviting me
the first topic I wanted to talk to you
about is super intelligence broadly and
then Additionally the question of
whether it can be controlled so could
you just start by telling us a little
bit about how do you think about what
super intelligence is
so for me it's a man today which is
smarter than me smarter than all other
humans in every domain so it's a better
chess player better driving cars better
at doing science kind of superset of all
possible skills
got it and I can understand wanting to
Define it that way but I would also say
that more narrow super intelligences if
you will that are just super human and
more specific abilities could also be
really interesting so maybe we'll talk
about those as well but let's start the
conversation talking about a
hypothetical super intelligence
presumably some kind of artificial
intelligence you know made with computer
science and algorithms that's smarter
than humans in in all intelligence
domains so why do you say that such an
entity cannot be controlled well after
years of research I really started this
work with a lot of believe that it can
be done I mean that was the intent but
the more I analyze the problem the more
I look at it the more I find that all
the components of a potential Solutions
are either known to be impossible or
very likely to be impossible in practice
and I'm just kind of combination of all
those sub problems taken together is
very unlikely to be possible if sub
problems are not
so can you walk us through a little bit
of your progression as you sort of
studying this and kind of the different
solutions you looked at and why they
don't work
great so how would you define a
well-controlled super intelligence what
does it mean we have control so you can
start looking at different types of
control you can have Direct Control
where you give orders you give orders
and the system just does whatever it is
you do this would be for example imagine
someone Builds an artificial
intelligence that's designed to be a
helper for humans and so you want to be
able to give it an order and it would go
carry out your wishes right exactly
exactly and you give it direct orders
you want something done so you say do it
and the system does exactly what you
told it to do now that's probably the
most common interpretation and it's very
easy to see how it backfires that's kind
of like your standard Genie story you
have three wishes you wish for
everything to be made out of gold or
something like that and it's always not
what you had in mind
right and I think when people hear those
stories they seem like a kind of clever
twist so that you know the thing you
wish for wasn't the thing you wanted but
I think there's something deeper there
and more fundamental which is that when
you make a wish to like a powerful
entity it's very difficult to specify
the full wish in some sense right so you
might say oh I want to be rich but you
might care a lot about actually the
mechanism by which you become rich and
in making that wish of I want to be rich
well you don't mean you know turn your
friends and family into gold okay a
human would obviously know that you
don't want your friends and family to
turn into gold but if you're telling
some powerful entity that has no
constraints on how it achieves a goal it
may achieve the goal in a way that
you're very unhappy with exactly and
there are common examples okay I want
you to cure a cancer so maybe killing
all people will get rid of cancer there
is a lot of different ways to
misinterpret uh what you had in mind and
find a very awkward path to get to that
goal so I think it's pretty obvious that
direct orders is not what we want I want
to keep talking about this a little bit
because I imagine a lot of people may
not be convinced by this because they
might say something like well if it's
really that intelligent can't it kind of
figure out what you intended you know so
if I'm talking to you and I say hey
Roman would you mind picking me up a
drink when you're outside you know you
know I don't mean a drink of poison you
know I mean that I don't want you to
kill anyone in order to get it or steal
it right so I think a lot of people
would imagine if this is a really
intelligent entity can't it actually do
a lot of the implicit inference to
figure out what was really intended by
that order
right but you're guessing when you told
me to get a drink did you want a cocoa
or vodka
right and I think you know this goes to
questions of like how does Google you
know answer a question that you give it
it is also guessing in a sense and if
it's guessing accurately enough we say
it's done a good job but on the other
hand Google answering a search doesn't
have the massive side effect of you know
causing some horrible problem in the
world so I guess a related question is
how much of this issue linked to the
power of the AI to affect things in the
world because if all I could do for
example was provide you with information
maybe it would seem less scary than if
it could actually go out and do a bunch
of things change a bunch of things in
the world in order to achieve the order
you gave it
it's pretty easy to convert information
into actions in the world whatever
you're predicting stock market or
solving some scientific problems
information can be monetized it could be
acted on just because it's only
information doesn't mean it's safe
right well and this starts to get into
this topic that I think a lot of people
think about when they think about making
safe artificial intelligence which is
like could you somehow create a buffer
between it and the world so the types of
actions it can take are really
constrained like the only thing it can
do is like output text on a you know
terminal interface or something and when
I think about the scenarios immediately
I begin to wonder you know what is it
possible to achieve through some kind of
very minimal interface and it seems like
actually potentially the answer is a
great deal you know if you think about
what hackers are able to achieve by just
you know in a chat room talking to
someone you know pretending to be
technical support or something like that
there actually is kind of maybe a
surprisingly large range of things
someone could achieve especially super
intelligence that's smarter than us in
every way by stipulation through even
just like the simplest text interface
right that's the whole idea like AI
boxing will put it into this confined
virtual environment but then if you are
interacting with it if you're observing
it you no longer keeping it isolated so
now it has access to all sorts of social
engineering attacks against you
one of the things that really struck me
about the kind of keep the AI in the Box
argument was I heard about a hack that
the basically the way it works is by
flipping bits rapidly in a computer and
then exploiting certain quantum
mechanical effects that cause other bits
in the computer to flip so it's not due
to like the logic or programming of the
machine it's actually due to the physics
of the way bit flipping occurs and this
was kind of mind-blowing for me because
it made me think about even if you had a
system where you could prove that
logically based on your software and
algorithms something was constrained in
its action in theory if it was smart
enough it could figure out how to
exploit physics to actually work around
the logical constraints
right it can think outside of a box you
place it in any proof you develop is the
proof of that specific method of
escaping it but there is infinite
possible number of perhaps you're not
considering okay so so we've uh we've
talked about the you know direct
ordering approaches one more thing I
want to say about that which is that it
seems like a very intelligent agent
automatically gets a lot of different
goals and I think that people don't
necessarily realize this because it's
like if you build an agent that all it's
there for is to let's say answer
questions on a website or something like
this right and it's not that smart then
it's kind of limited in the actions it
can take but if it's super super smart
and you ask it a question that doesn't
know the answer to if it's smart enough
to realize it doesn't know and that the
only way to find out is to go do some
complex set of actions in the world then
suddenly it opens the door to it having
lots of other side goals or different
than the ones you intended to program
with it for example it may realize that
if it acquires resources it will better
be able to serve your needs by answering
questions in the future because it could
let's say spend money and hire people to
help it get more knowledge right or it
might be so smart that it realizes that
if it were to be shut off it would be
worse at answering people's questions
and so it actually doesn't want to be
shut off because then it can't achieve
its goals answering questions and then
you might say well why not program it so
that it doesn't mind being shut off and
it's like well even if you achieve that
it may not mind being shut off but maybe
it now has an incentive to create some
copy of itself that won't be shut off so
that it can still answer questions even
if it is shut off and so on so there's
kind of this whole kind of warps
anything you want to say about that all
right and it's important to understand
that you can also encode pretty much
anything as a question that I'm no like
claim it's it's not just safe questions
you can ask about engineering designs
you can ask about secret information
anything can be encoded as a form of a
question to such an entity
right so if you ask it you know how do I
make some extremely dangerous weapon or
you ask it a question like about U.S
military secrets and well how is it
going to figure those out well who knows
but if it's a super intelligence maybe
it finds some way to figure out those
exactly how to take over the world how
to win elections anything like that and
this goes into a topic that I think it's
important kind of thing to distinguish
which is on the one hand there's risks
of creating a super intelligence that
come from the super intelligence itself
and on the other hand there's risks from
creating a super intelligence that comes
from whoever might end up controlling it
if it is controllable right so like even
if you could control it then there's a
question well who is in control of it
and how much power does it give them you
know you know just uh for the sake of
argument imagine someone who's in
control of a super intelligence and they
start wielding it towards hacking all
the world governments or towards trying
to win at the stock market and make a
trillion dollars or even to controlling
people of a population and so even the
idea that it's controlled doesn't
necessarily give me a huge feeling of
safety but it sounds like the the work
you do is more focused on so the
question of canopy control at all is
that right well I do both I have quite a
few Publications specifically on
malevolent design and purposeful use of
even well aligned AIS for malevolent
purposes and I think that's the hardest
problem so with the kind of problems
with software and design there is a lot
we can do with malevolent aspects of it
than someone on purpose that does it it
becomes a much bigger problem and as far
as I know there is very few even
potential solutions for that so let's
look at the other options so okay direct
ordering is one extreme what is on the
other end the other end is where you
have this ideal advisor a system knows
better than you smarter than you it will
kind of decide for you what's good for
you so the exact opposite of direct
ordering
I see does that mean it's actually
taking actions on your behalf without
you even having to tell it what to do
you don't have to tell it anything it
knows what you want it knows what's good
for you and it's just better at making
those decisions you tested it forever
and at some point you went why are you
asking me you know what drink I want
so immediately that just sounds scarier
to me than the idea of trying to order
it around but yeah so what are some of
your thoughts about the risks um in that
approach so here you're no longer in
control right someone else makes those
decisions you may like them a lot you
may be happy with the decision the
system makes but you're definitely not
in control by definition
right so many many people might find
that idea on appealing just you know
right out of the gate right and if you
ever change your mind it's not obvious
that you can undo that there is an undo
button of some kind so once you transfer
power and go okay clearly you're smarter
make those decisions for us uh it may be
a one-way change
okay got it so we have this kind of
spectrum from you're telling it what to
do and it's only acting based on your
commands all the way to it's just always
acting on your behalf no matter what are
there other kind of General options that
we should be thinking about so anything
in between you have this mixed models
where the system has a lot of capability
and it's trying to figure out what you
would want maybe in consultation with
you and help you with those
so kind of there are degrees of how much
Independence it has versus how much
consultation it has to do with you and
for each one of them there are problems
related to this particular approach so
if for example a system is more capable
and smarter and is Consulting with you
it's very easy for the system to
convince you of taking a particular
decision it controls the facts it knows
better what you would prefer so it's
kind of like propaganda based guiding of
Voters towards a particular outcome
got it so in these scenarios to what
extent are you thinking of an AI as
having a kind of guiding objective
function that's trying to optimize for
versus kind of other ways of Designing
it
so whatever way you select it still has
to somehow interact with our desires
right is it trying to do what we want is
it trying to do better than we can is it
trying to obey the point I'm trying to
make is that under each one of those
scenarios we are either unsafe or not in
control there is not a scenario where we
go this is exactly what we want to
accomplish here
right so let's think for a moment about
like what are the different options of
of how we would even program something
to do our desires right one way is we
would build an objective function like
you do for traditional machine Learning
System where you basically encode
everything that it's trying to achieve
and you basically tell it maximize that
objective function and you know one of
the disturbing things that you see come
out of this is when this is applied in
like game-like scenarios like let's say
within a video game
oftentimes the AI will find very weird
ways of maximizing the objective
function that were really not intended
so so for example I've seen this happen
in Mario Brothers type games where it
actually figures out bugs in the game
and actually exploits them in order to
get more points or I've seen this in a
racing video game uh it was like a water
racing game where it figured out that if
it just went in a circle forever in a
certain spot it could actually get sort
of arbitrarily High numbers of points
due to just a weird construct in the way
the game was made but you know you're
thinking about designing the objective
function you're certainly not intending
it to just go in circles forever you're
intending it to actually like learn to
play the game and you know get to the
end of the race and so it seems to me
like these kind of objective function
based strategies you know one of the
main challenges with them is that
they're often are weird Edge solutions
to them that actually lead to a higher
number of points but are really not what
we intended to encode when we designed
the objective function uh any thoughts
on that absolutely I have a number of
papers on exactly those types so of
examples AI accidents we call them AI
failures and you can see a trend there
becoming more common more impactful but
it's always kind of the same I made a
thing to do X and it failed to X for me
huh what a surprise and the more complex
the domain the more complex the
environment the harder it is to specify
all the things that shouldn't be doing
as part of your common sense so
obviously I didn't mean for it to turn
off the computer obviously I didn't mean
all this other possible ways to
accomplish the same thing when we get to
real world the complexity is just uh so
big we cannot possibly specify even a
small number of those restrictions
right and and when you get really down
to it at some level we wanted to act in
accordance with human values and you
know philosophers have debated for
thousands of years how to define human
values how to find what's good
and so like it just seems completely
untenable the idea that we would
actually be able to like program in an
objective function the things we truly
care about rather than just sort of some
course approximation from it and the
problem with the course approximations
is that if you optimize on them too hard
there could turn out to be edge cases
that lead to weird consequences we
didn't expect right and this is the type
of things they mentioned as even
possible solutions to sub problems so we
know things like voting is impossible to
really accomplish where everyone gets
what they want there are proven theories
ethics morality we know we cannot get
people to all agree on what is good even
what is bad what is the right course of
action and that's all of us the humans
have pretty much the same brain design
and we're still not in agreement yeah I
think one way to think about this is if
the super intelligence is optimized too
heavily on the goals of one person even
if it is successful then it potentially
gives that person the ability to push
their goals on everyone else but on the
other hand if we have to optimize it to
be with respect to the goals of all
people like the whole world's goals then
we have to understand human values and
morality and all these like incredibly
thorny issues that we seem very very far
away from actually being able to encode
in a reasonable sense so both of those
seem to have real challenges and that's
if you're actually able to control it at
all right so Okay so we've talked about
the objective function based approach
what are other ways of thinking about
designing an AI That's not just based on
optimize and objective function
so you can try and create a number of
small kind of modules all of them having
their own purposes and a combination of
them serving as a bigger unified
intelligence that has certain advantages
but there are some emergent properties
where it's not obvious how the whole
will act and it doesn't seem to be any
safer again not because of how it
operates but because the thing we're
trying to do doesn't seem to be possible
we're not sure what it is we want we
don't agree on what our goals are we
don't have pretty good idea of what we
should be doing with our time so it's
very hard for us to transfer that into
systems which supposed to be
accomplishing that since we don't know
what we're doing
whether you're trying to positively
influence the behavior of others or
create a new beneficial Habit in your
own life Behavior change can be hard
team at sparkwave have developed a
framework and corresponding website
called the 10 conditions for change to
make Behavior change easier the 10
conditions for change framework can be
used to help others for example through
creating healthier eating behaviors or
increasing vaccine adoption or to create
a positive change in your own life for
example by getting yourself to exercise
more or practice relaxation techniques
daily the framework provides 10
conditions that are designed to be
sufficient for succeeding at creating a
new positive behavior in other words if
a Target person or population meets all
10 conditions then they will very likely
adopt the new Target Behavior in
addition to describing the framework in
great detail the 10 conditions for
change website also includes a
step-by-step guide for using the
framework a behavioral strategy search
engine which lists over 300 Behavior
change strategies for helping people to
meet whichever of the 10 conditions
they're not currently meeting and
concise summaries of 17 other useful
Behavior change Frameworks to learn more
and to start using the 10 conditions for
change visit sparkwave.tech that's
sparkwave.t-e-c-h scroll down to the
list of projects and click on 10
conditions for change
with regard to these systems that don't
have one overarching objective function
that they're trying to maximize for but
rather that have these different modules
like you were describing does each of
those sub modules have its own objective
function and it's just that the whole
system doesn't have one objective
function yes that's a good way to think
about it think about a human being is
like this Society of Agents you have one
agent which kind of just goes am I
hungry or not and if I'm hungry we
should look for food another one I don't
know looking for water another one
looking for sleep and you have like
thousands of those things you don't have
to have one large goal in life you want
to become president or something it's
enough to have all those interacting
together and so it will get you through
your day just kind of trying to satisfy
with sub modules it seems like it would
be even harder to understand what such a
system would do compared to the
objective function perspective because
it's really like you have all these
different objective functions in
competition and then you can imagine at
different points different ones of them
temporarily winning over others so it's
kind of like having an objective
function that's changing in time it
which seems to me like potentially even
harder to model and you know analyze
than just a single objective function
what do you think about that
it may be the case you're looking at
emergent behaviors then you're looking
at swarm algorithms for example each
individual and to B has a specific
purpose but organism the whole Hive has
a very different set of outcomes so it
may be harder to analyze
I've also seen some other approaches to
try to control very intelligent systems
you know one would be like the debate
approach that open AI has been pursuing
where the idea is that well maybe a
human can't directly like inspect what
an AI is doing or even necessarily
understand the reasons that are very
intelligent AI made a certain decision
because maybe it depended on thousands
of factors and like that but perhaps if
you had AIS in competition with each
other
let's say one AI is trying to prove X
and the other AI is trying to prove not
X and they're both trying to explain to
a human why their perspective is right
maybe a human could judge that debate
much more effectively than they could
judge the let's say output of a single
AI uh you know so there's a lot of
different variations around this I know
that they have some kind of practical
real world experiments they're trying to
do which are kind of baby versions of
this I'm curious to know what you think
about this General attitude I think it's
an adversarial approach you're putting a
human in the middle of two adversarial
agents kind of like we have debates
during election or maybe during jury
deliberations you have you know two
lawyers trying to impact the jury
usually what happens they go beyond just
logic and arguments they try to
manipulate your psychology they try to
profile you if a system is smart enough
there are different ways of impacting
human Observers it's funny I worry about
exactly the same thing that they might
learn to exploit bugs in the human mind
and even if one of the AIS is like kind
of trying to convince you that of the
right thing and the other is trying to
convince you the wrong thing it's not
clear to me that the one trying to
convince you of the right thing actually
truly has an advantage if this agent had
figured out like the full mapping of
ways to persuade a human being
oh absolutely we see it all the time
again with elections as a great example
uh your personal preferences and biases
will play into it if something is easier
to explain and understand it will win
over a more complex but correct idea
yeah you could imagine in AI convincing
you that if you believe X you're a bad
person or if you believe X your parents
won't love you or you know I mean I'm
being a little silly but basically the
idea is it's not clear that truth
necessarily has an advantage in that
particular debate I hope it does I hope
it does but I'm not sure about that it
would really depend on who the audience
is even within humans there are very
different populations with different
education different intelligence so the
same argument may not work really well
in different crowds
another approach that I've I've heard
mention of is basically trying to get
the AI to predict what you kind of would
have wanted and so the training data in
some sense would be like it produces an
output and then a human evaluates that
output and says oh yeah that's the kind
of thing I would want that's the kind of
thing I wouldn't want and so you're
basically rather than trying to to train
it more directly on like you know
predict the answer to this question
you're trying to train it on predict
whether a human would be satisfied with
the thing you produced what do you see
as a challenges with that kind of
approach
so if it's a super intelligence we're
talking about I think the line goes what
would you want if you were smarter if
you had more time to think about it if
you were better educated but essentially
the question is what would that other
person want not you but that's someone
else you mean the version of you that's
like smarter and has infinite time to
think and has reflected on their values
all right better looking whatever and
that person may want to drink wine and
watch theater and I like beer and circus
and I don't care what that our person
wants
I think I'm I'm not quite getting how
that connects to kind of the general
strategy of trying to train an AI on
predicting what people would want and
having you know human labelers kind of
rate yeah oh yeah that is that is what I
would want that is right yeah let me
explain so if it's a narrow systems we
have today
predicting what you want is uh wonderful
it works but this is kind of what we get
with Facebook algorithm right it just
takes you down this Rabbit Hole where
you get more and more weird videos about
Flat Earth because you seem to like them
and click on them it's accurately
predicting that you're gonna liken yeah
I kind of Race To The Bottom in terms of
what gives you exactly and it trains you
to kind of respond to this easily
predictable stuff instead of
diversifying your intellectual interests
got it now if the system is super
intelligent and it goes okay what would
you want if you were smarter not you
with your current situation but you who
is much smarter much better educated
more time to think better resources what
would that agent want
and even if it correctly computes that
that has nothing to do with you that's
not you that's someone else
so you're saying you might still be very
dissatisfied absolutely so if you have
kids you know better you know that they
should be eating carrots but they won't
do it candy
but what if you train a super
intelligence to try to predict what you
actually want rather than what you would
want if you were kind of the smarter
more thoughtful version of yourself is
the problem there that it just becomes
like Facebook squared where it's just
you know it's hacking even more of uh
your dopamine system and just getting
you to like click in circles for the
rest of your life or something that and
also we are terrible at knowing what we
would like so right now you may have
those specific dreams you think if you
just had a billion dollars you would be
so much happier but once you get it you
may realize okay this is not at all what
I wanted
and you know I think that kind of
averages out if you were to have lots
and lots of of people rating lots and
lots of actions and kind of trying to
trade an AI system that way
well it's even harder with multi-agent
I'm talking about a single agent if
you're doing it for multiple people
everything cancels out for everyone who
wants one thing that is someone who
wants the opposite Hmm got it got it I
see so do you see any of the kind of
current approaches being worked on that
might possibly be extended to the super
intelligence situation as being at all
promising are you have you kind of given
up on all of them at best they fix a
specific problem a specific bug I
haven't seen anything which is a
prototype for this will solve this big
issue and it scales to any level of
intelligence we just need to code it up
got it are there any promising threads
that you think are even though they're
not there yet they're they're worth
pursuing
I haven't seen anything it's kind of
like my experience with it is like a
fractal we identify a problem we propose
a solution but at the end of a paper we
identify 10 new problems within that
solution and you zoom in on that and you
repeat the process and every time you
discover 10 new problems
yeah that's pretty demoralizing
uh I mean you want to understand what
the reality is if it's really that way
and I'm correct then we need very
different type of approach
so I think a lot of academics are less
concerned than you are and I I actually
would say that I'm substantially more
concerned about artificial intelligence
than most academics are and I'm curious
do you agree with that characterization
and if so what do you think is going on
there why do you think academics are
less concerned about the control issues
of artificial intelligence
well I don't know about academics in
general are you talking about computer
scientists specifically working on
artificial intelligence yeah yeah
exactly
so there is I'm actually working on a
paper right now kind of addressing
different types of skepticism about AI
risk and I think I'm up to like 30
different reasons for why it happens
a lot of it is
kind of motivated thinking biased
reasoning so if you are working on
something it's very hard for you to say
this is terrible I shouldn't be doing it
just to clarify that so you're saying if
someone has devoted their life to work
on artificial intelligence that they're
not going to want to believe that that
actually could lead to really negative
outcomes exactly and if you're funding
your prestige your job depends on you
continuing doing this research you're
not gonna accept any arguments saying
that maybe this is dangerous again with
years of experience also you get kind of
this biased attitude that okay we're
barely making progress I've been doing
it for 50 years and Luke we barely
learned to do this you're saying we're
going to get super intelligence this is
nonsense we'll get another winter
right right and you're referring to the
AI winter can you tell us a little bit
about that and why you don't see that as
very plausible right so historically
every couple decades we make some good
progress AI researchers promise that
okay human level is right around the
corner it doesn't work out funding is
cut and so it just kind of kills the
research for another decade and then it
restarts again it seems this time is
different because so much funding comes
from corporations and private money not
just government NSF DARPA arpa so it
would be very unlikely to lose funding
given how much of commercial interest
that is in this technology not just
amazing progress and how well it scales
but just the funding side of it yeah my
understanding is that open AI just did a
billion dollar deal with Microsoft just
just to give one example
right and they started with a billion so
again funding doesn't seem like an issue
yeah that's interesting what are some
other reasons people give or you think
motivations why people aren't so
concerned in in the academic field
uh some think it's impossible in
principle to create human level
intelligence so it will never happen
others have suggested that if it's so
smart it will be nice because smart
people are nice so it will be benevolent
yeah let's talk about that one for a
moment because I I've heard that too and
I think it's a very interesting thing I
mean it seems like just right off the
bat you know there are sociopaths
they're very smart they're unusually
smart and do not seem benevolent at all
so I'm a little confused why people
think that intelligence automatically
makes you benevolent but it also ties in
with this idea of the orthogonality
thesis which as I understand it is the
idea that imagine you're trying to
develop a really smart AI in principle
it could be really intelligent and be
given almost any set of goals and the
intelligence is essentially orthogonal
from the goals you give it right you
could make it really intelligent and
tell it to just spend its entire
existence making paper clips or you can
get really intelligent and tell it to
spend its time trying to cure cancer and
you know in both cases it could be
equally intelligent absolutely and you
can see it with humans you can find
humans of any level of intelligence
doing all sorts of crazy things so I
don't think that's a solid argument
there is some research showing that as
we get more advanced society as a whole
gets a little bit nicer we kind of give
up on slavery we're trying to torture
animals less so I think that's the trend
we're trying to extrapolate but it's
definitely not a guarantee of safety in
my opinion
yeah that's really interesting
and you know you also have comments like
you know trying to prepare for super
intelligence is like worrying about
population problems on Mars right it's
like this idea that we're like a
thousand years or ten thousand years
away from having these systems yeah what
do you think of that kind of response
it's common it's popular but think about
something like the car industry the car
industry started with electrical cars at
the time climate change was not an issue
so thinking about
global warming global cooling would be
insane I mean you're 100 years away from
any issues why would you think about it
but if we did and didn't start using
fossil fuels would be in much better
shape would be safer so it's kind of the
same let's say it's not even close and
it will take a hundred years to get to
AI making good design decisions today
will definitely pay off in 100 years
yeah I think I would add to that that
there's just so much uncertainty right
we and and I think for some reason
uncertainty often makes people feel
better about things they're like ah it's
uncertain so let's not worry about it
but for me I think I have the opposite
reaction I'm like well I don't know
whether it's 10 years or a thousand
years I genuinely don't know and that
actually makes me more nervous about it
because it means it actually could be 10
years and you know how low probability
am I willing to assign at 10 years 20
years 30 years 40 years you know so it's
hard for me to assign like such a low
probability at let's say 25 years that I
don't think we should be thinking this
seriously right now right uh absolutely
I agree with you it's better to be
careful not regret it later right and
it's sort of like people think of it as
as sort of overconfident to think that
something like this might happen there's
also this other weird reciprocal form of
overconfidence which is overconfidence
it's definitely not going to happen you
know in 20 years I I certainly
understand why some might think it's not
that likely to happen 20 years but I
have a hard time understanding why
someone would assign essentially zero
probability and how they how they would
come to assigning a zero probability but
even if I do 20 years is a tiny amount
of time for doing such complex research
I mean the problem is harder than the
problem of actually designing AI so you
probably need at least that much to even
have a chance
yeah and I think that's a really
interesting point which is that we have
these two different goals most of the
research in AI is about either
engineering type stuff like applying you
know AI successfully to a particular
application or AI research where they're
trying to push the state of the art in
terms of you know getting it to do stuff
that it hasn't done before a tiny tiny
tiny tiny fraction of This research is
on risks and how to prevent them so it
seems like even if we thought it was 50
years away it wouldn't be bad to start
formulating a whole really robust area
of AI risk research to try to make sure
that you know in 50 years we haven't
just just started studying this we
actually have really good theories about
how to do this well
absolutely and complete agreement and I
actually think it may be much sooner
than 50 years so there is good reasons
to look into it on that issue I just
want to mention so I was just looking
today at some numbers these were
estimated in 2018 so they're a little
old but at the time there was an
estimate that AI compute was doubling
every 3.4 months
and that's even significantly faster
than Moore's Law which is just kind of
shocking at the incredible rate of
computation going up that's being thrown
at AI now of course throwing more
computation isn't guaranteed to produce
better outcomes but I think one of the
really fascinating lessons of of the
last five years is that more compute
actually goes really far and you know if
you look at the difference between early
you know natural language processing
systems versus gpt2 versus gpt3 it's
kind of remarkable how much compute was
able to push things forward any thoughts
on that I agree and I don't see any
signs of it slowing down you keep making
bigger models if you have enough compute
and sufficient data I think you're going
to keep making good progress for a while
yeah and that sort of leads to this
difficult question of you know where
does the progress stop you know can can
you just throw more and more and more
compute until you have something that's
essentially super intelligent or do you
hit a whole bunch of roadblocks at some
point you really need new intellectual
breakthroughs to make further progress
I'm curious to hear your thoughts on
that nobody knows for sure given that we
are making very good progress and this
is kind of modeled on human brain human
neural networks it's very plausible that
it will continue working at least to the
scale of human brain I think those
models are still orders of magnitude
smaller so there is lots of room for
improvement but then they are equivalent
to a biological model in size they
actually perform way better way faster
so at that point we might see super
intelligent performance right away
all right let's uh change topics now and
and talk about Consciousness because
sometimes the question of you know is
something conscious gets kind of wrapped
up in this discussion of building Ai and
the first thing I just want to say about
this is that whether an AI system that's
really intelligent actually is conscious
or not and by that I mean has internal
experiences is able to feel something or
there's something that it's like to be
that AI doesn't really have much bearing
I think on the risk question because
really the risk questions around what
it's capable of doing and whether we're
capable of controlling it not whether it
has internal experiences and not whether
there's something that's like to be it
do you agree with that framing yes
definitely agree with it I kind of joke
about it if you have this robot chasing
you about to kill you do you care if it
feels anything at the moment right so
okay so we can put that on the side and
say okay well Consciousness isn't that
related to the risk question as far as
we know obviously Consciousness is a
great mystery but as I understand it you
believe that it might be possible to
detect Consciousness so I'm curious to
hear about your thoughts on that all
right so it's definitely impossible to
experience what someone else is
experiencing what is it like to be a bet
what is it like to be you but it seems
like it might be possible to check if
you are actually having an experience of
some kind so if I can encode some sort
of pattern and present it to you and
then give you a multiple choice question
about it so for example with optical
illusions if you ever had a chance to
experience a very cool optical illusion
something happens which is not a part of
it or part of you you kind of get this
artifact something is rotating changing
colors something happens which is not
easily predictable before you experience
the illusion and if you can design new
Illusions like that and give a multiple
choice to an agent whatever is
biological agent artificial agent asking
them well from those choices what are
you experiencing if they can correctly
answer that question multiple times
let's say a hundred Illusions and they
get all of them right I have no choice
but to conclude that they actually
experienced it in some way it may be
completely different for them but we
definitely have some sort of internal
representation for that experience
uh you know I I'm trying to Grapple with
that because it seems to me just a
priority that Consciousness is is
extremely hard to detect partly because
we understand it so poorly but you know
if I just try to react to that specific
idea how can we tell the difference
between it actually having a true
internal experience of that optical
illusion versus when that sensory input
goes through its full system it behaves
as though the optical illusion occurred
but it never had an internal experience
you know much like these kind of
adversary examples where you can get an
input that to us looks just like a dog
and you can get a neural net to
misclassify as a cat
just by adding some noise that's very
carefully crafted I mean that's almost
like an optical illusion for an old net
in some sense exactly and that's my
argument and they are experiencing that
cat and that's a quality of a half and
you don't have the same experience when
you look at it I'm not saying you're
going to have same experiences but you
can tell whatever it had to get
experience or not are you suggesting
then that neural Nets today are actually
conscious that is part of the argument
we obviously have non-human level
Consciousness we are much more complex
we have rudimentary Consciousness and I
think Consciousness is kind of a side
effect of computation side effect of
intelligent processing and the more
complex they get the more complex
quality they can perceive I think at
this point it's something that's trivial
is an optical illusion but as they get
more complex they can have a multimodal
experiences and even go beyond what
human quality is
you see that's funny I really didn't
expect you to take that branch and the I
was actually trying to make a counter
argument against your view and you're
like nope I'll bite that bullet I think
well it's in a paper it's too late for
me to say anything else that uh I
published it
interesting so how confident would you
say that AI systems currently experience
some form of Consciousness because I
think I would put quite a low
probability on it but that being said
it's like one of those probabilities
that has extreme uncertainty around it
because I don't even know how to think
about it
if you agree with my destination my
proposed definition there are
experiments where systems artificial
neural networks have experienced optical
illusions in the same way as humans
without being programmed to do so
so I think there is very high chance
that they are in fact experiencing
something very simple very trivial but I
I Think It's deductible but just to push
back on that for a second I mean most
neural networks today are just
essentially functions right they they
map input to output and yes they're very
complicated functions but essentially
they're just functions right and any
function it sort of deterministic in
some sense if you put in a certain input
it'll give a certain output and optical
illusions like an adversary example that
that to us looks like a dog it looks
like a cat isn't that just saying that
that function has an input that to a
human happens to look like a dog but
according to this function gets mapped
to a cat you know I'm not sure how much
we can really read into that
so it's definitely not a trivial claim
we can switch the surrounding kind of go
to humans and go okay so neurons just
pass electricity around it's your
electricity on or off what is
consciousness in that system right are
you still recognizing cats and dogs you
have feelings salty sweet at the end of
the day it's all electricity on and off
is that conscious
right and you know and I think you know
you can make the argument well isn't the
human just like some kind of function
that has a state or something like that
and the state changes over time well I
think I think part of the answer that we
don't know I think we don't fully
understand what a human is exactly and
this seems to me like one of those like
weird and deep Mysteries which is
how do you take this physical view of
the universe that we describe very
successfully with our physical laws and
then mash it with the fact that we know
that we have conscious experience and
yet the physical laws don't have
conscious experience anywhere in them so
I guess I just view that as a big
mystery that I don't feel like I have
much understanding of I agree it's one
of the hardest problems we know about
but I think at least that the level of
trying to detect some trivial
experiences that may be a useful test it
may not perfectly map to what we are
looking for but it gives you some idea
if I know that this agent is
experiencing same thing as I am perhaps
in a completely different way that's
interesting
well I have to say I applaud you for
your ambition because I think people
have to try ideas and and you know see
how they go I'm not necessarily
persuaded by that particular one but I I
do think that that kind of thinking is
exactly what we need to try to make
progress on these really thorny topics
are you facing a tough or important life
decision then you should try using
clearer thinkings decision advisor tool
to make it easier with this tool you
won't have to stress as much about those
big life decisions the decision advisor
can walk you through even the most
complicated situations in minutes so you
can come out on the other side with a
better idea of what to do to use the
free decision advisor tool or to find
clear thinking's other free tools and
many courses head to clearerthinking.org
[Music]
let's go to the next topic which is the
question of are humans General
intelligences so maybe you want to say a
little bit about what is a general
intelligence and then your perspective
on whether humans actually satisfies the
definition
right so we frequently talk about
artificial general intelligence and we
equate it with humans and we also call
it human levels sometimes but I think
there is a difference
so we are General intelligences in the
main of human expertise we are very good
in our three-dimensional World human
society and communication but there are
problems which uh artificial general
intelligence could solve which humans
cannot so I think AGI is actually a
greater set of problem-solving
capabilities than any human
are you able to point any examples sure
but they are very hard for humans to
understand by definition I think one
good example comes from Yang likoon he
kind of gives you this situation where
you have your optical nerve
connecting outside world and your
processing units what if it's uh cut and
all the wires are connected not
sequentially but to random pixels would
you still be able to do Optical
processing visual recognition and I
think his argument is that you won't be
able to do it you won't be able to
reassemble the image whereas for AI it
doesn't matter where a pixels are in
that regard right because the neural net
can just essentially update its weights
to adjust for the fact that the pixels
are not in the right order is that the
idea some of the examples I cite in my
paper on that subject have to do with
interesting biometric properties so you
can for example look well not you but an
AGI can look software can look at DNA
from that information extract things
like what does your face look like
or what does your voice sound like
whereas as far as I know humans are not
capable of doing such things now those
are kind of borderline examples where we
still can understand them we just can't
do that but there is a whole like
infinite Universe of problems where
an AGI can do it but we can't and I
think it's kind of interesting to to
note that yeah it's really interesting
you know I guess I one way to think
about this is that there's certain types
of tasks that maybe humans can do like a
very very simple version of it but
because of our limited like
computational capacity and limitations
of Our Brands it seems like without
severely altering ourselves or using
tools we'll never be able to do kind of
a more advanced version of it and and so
an example that's you know as a
mathematician I think about math and how
a mathematician can just can't keep a
thousand things in their mind at one
time it's just it's not something that's
possible and you could easily imagine
math problems that actually require
keeping a thousand things in your head
at one time because math has this sort
of unlimited level of complexity you
know it seems to be sort of infinitely
deep and you know you could imagine even
the most intelligent being in the
universe even if they're way more
intelligent than us still having math
problems that they struggle with or
can't solve right yeah you have these
different classes like with different
and automations different machine
classes based on amount of memory you
have based on amount of connections you
can make if
you were modified to give you bigger
memory faster processing you would be
able to solve different problems but you
wouldn't be in a class of regular humans
anymore
so do you view that as the main
limitation like not having enough
you know computational speed and time or
do you view it as a more fundamental
architectural limitation like there's
some things that humans couldn't do even
if we could speed up our minds and
increase our memory well it's definitely
a limitation on humans but from safety
point of view I see it as
even if we just get to AGI before we
even get to Super intelligence that
already will be able to do something so
we can't even conceive of can you
distinguish between AGI and super
intelligence just to be clear on that so
traditionally when people talk about AGI
they talk about human level performance
right it's as good as a human in all
this domains whereas super intelligence
is superior to us you know those domains
to All Humans so just kind of Next Level
it can be argued whatever it's very easy
to get there from a GI to Super
intelligence uh is it hard takeoff soft
takeoff but they seem to be different in
terms of capability
I'm pointing out that even AGI can
already be super capable in many domains
where humans do not perform well at all
and I don't mean traditional domains
obviously mathematics calculators so
that is super intelligent in a narrow
sense I'm talking about in general sense
right well one of the interesting
phenomena we see is that very often will
have ai worse than humans much worse in
humans at something and then it will
briefly be as good as humans and then it
just blows past us right it's like
there's a brief period where computers
are equally good to humans at chess and
then it's just like okay now the best
agent in the world is a computer and
we'll it seems like we'll forever be
unless humans definitely modify
themselves so it seems like at least in
these narrow domains the level of human
intelligence is just this like very thin
region that it just gets blown by very
quickly
definitely and for a few weeks or months
you have a hybrid team human and
computer is the best and then human
becomes useless completely that's really
interesting I also want to dig in a bit
into different ways that a computer can
be more intelligent than a human like at
least hypothetically and so a few that I
think about are one it could be like a
human mind but operate at much higher
speeds and you know I think the easiest
way to think about this is just imagine
that you had this capability like
imagine that you could think a thousand
thoughts in the time it currently it
takes you to think one thought or better
yet read a thousand books in time it
takes you to read one book or you know
if you had a fast enough you know
keyboard or output device imagine you
could write you know a thousand pages of
time right it takes you to write one
page and just how much more effective
you would be at doing things and that's
just a kind of like one dimension of
like improved intelligence is the speed
another is just having more information
right like a human cannot take the whole
internet and stick into its brain you
know let alone Wikipedia whereas uh you
know Google search in some sense has
like most of the internet kind of
already encapsulated so you can imagine
improve moving on a human mind just by
allowing for much larger information
capacity
a third one is imperialism you know if
you're working on a problem you have
kind of one mental thread that can work
on it but imagine that you could spawn
10 000 copies of yourself that would all
work on that problem in parallel maybe
for example trying different strategies
but all be super coordinated and sharing
information with each other and all
working together towards the same goal
it's sort of like you had your own
personal Army to achieve anything you
wanted to do and unlike faster speed
where you're doing things you know one
after another they could actually be all
doing things in the world separately in
different places for example all working
towards the same goal so that's the
third one was kind of this parallelism
and then the fourth one that I think
about which is is kind of the hardest to
talk about and understand is the idea
that potentially you could have an AI
that just simply thinks better and and
it's hard to Define what that means you
know imagine two people and given the
same information one of them comes up
with like better strategies or more
accurate conclusions even though they
have the same information and and you
know this comes back to kind of models
of how do we make predictions accurately
you know bayesianism probabilistic
thinking and all these kinds of things
but you could imagine that an AI
algorithm could just be better one day
at processing the same information as a
human is so I've got this kind of the
four different ways a machine could be
better than a human in terms of
intelligence are there other ones that
you'd want to add to that list
no I think you captured the most
important ones I think Boston does a
good job reviewing those uh so we
already basically have speed and we have
information databases the hard one is
superior thinking right higher IQ higher
intelligence we definitely know it's the
case the difference between humans and
animals for example is huge there is
some difference between individual
humans as well so it's easy to picture
software agent which is standards of
deviation away from smartest human
also probably the hardest one to to
design to develop right yeah it's really
interesting to think about you know what
is the difference between Einstein and a
random human like it seems like in some
real way Einstein was much smarter not
necessarily smarter at everything I'm
sure there are things he was not good at
but you know there were some real ways
that he's smarter than the average human
and you know imagine a machine that is
10 times further along that spectrum
between enzyme and the human right or
100 times another thing I just want to
say about this is that thinking about
those four ways of being more
intelligent than a human you know being
faster at processing having more
information having parallel Minds that
can work on the same problem in
different places at the same time and
then being better at thinking the better
thinking one as you said is it's kind of
the the hard one that a lot of effort is
going into the other ones you kind of
almost get for free with an AI because
if you figure out how to build the
better thinking machine well presumably
as long as chips keep getting faster
even if it starts at the speed of a
human mind it will like speed up Beyond
human mind and the more information well
we already kind of know how to to pack
huge amounts of information to a system
whether it's a database or a neural net
and the parallel mines that's just
booting it up on multiple servers right
so it's like because right now ai is is
significantly worse than the human mind
at better thinking thing it's easy to
kind of underestimate the other three
advantages the faster speed more
information in parallel lines but if one
day it becomes comparable or better than
humans it the better thinking then it
just automatically wins on kind of all
four dimensions and I think that's sort
of underappreciated
all right definitely there is like
overhang of Hardware capabilities so we
can rely on but for now we rely on those
three to kind of present better AIS as
they are today even if they're not as
good at thinking they are so much faster
and have access to databases of
knowledge that they become competitive
in many things like Jeopardy right well
let's switch topics again I want to hear
your thoughts about
solving the value alignment problem with
with individual virtual universes that
sounds really interesting tell me about
that right so we we talked about how
hard it is to figure out what individual
humans want and then how do we now
aggregate this over 8 billion humans
animal salients and it doesn't seem like
it's solvable if the functions are not
compatible you can average out you can
ignore someone but none of those
Solutions are desirable at least to the
agent you're ignoring right so basically
basically you're trying to aggregate
over a huge number of preferences people
don't even know their own preferences
but even if they did you still have this
problem of like how do you combine them
all into one Mega preference right right
even I somehow magically figured out
exactly what you want and you were
static and that you didn't change that
much I still would have no way of making
you compatible with everyone else got it
so one possible solution if we get as
much progress in virtual reality as we
do in intelligence we can create
complete universes and then everyone can
be assigned one Universe one virtual
world where whatever values they
interested in would be fulfilled 100
percent
and of course you can have combinations
you can share universes if you wish you
can exchange them but it kind of
addresses this issue of you cannot have
everyone get what they want in the same
world
if you think about this idea that if you
had really accurate virtual reality that
really felt like you were really in that
place in many ways it'd be preferable to
normal reality yet at the same time I
think a lot of people have an aversive
reaction to that possibility of like
living in virtual reality why do you
think that is
well right now the quality is not the
same so you're asking do you want to be
in real world or in describe your video
game if you couldn't tell the difference
and one of them was uh easier to
implement cheaper and resources and you
literally got whatever you wanted you
can play In Your Role you can be
president you can be slave you can be
anything and you can't tell the
difference it would be very hard to
argue that the same people who play
video games right now who go to the
movies who read fiction books would not
be interested you know another thing I
think people say about this is that well
the virtual world is not real and you
know on one level I understand that at
another level I I don't like the way
that I don't really buy that argument is
that well does it really matter if the
thing you're looking at is real or if
the thing you're looking at is simulated
I mean at the end of the day you can
quote real stuff I mean what you're
really seeing is just you know photons
of light hitting your eyes and sending
electrical signals you know you're
seeing this simulation that your brain
creates that to you feels like oh
there's a tree over there compared to
the virtual reality where you know the
tree over there is a simulated tree but
like it's essentially producing the same
experience so that's the to me you know
it's like if the benefit of looking at a
tree is actually looking at it in in
both cases you have essentially the same
experience but the way that I do relate
to this argument of like things not
being real is I think that I and most
people have a really strong preference
that our interactions with other people
are real in the sense that we're really
interacting with you know conscious
agents that
um have feelings and things like this
and we're not just uh talking to sort of
uh philosophical zombies or you know
algorithms that actually have no
fearings or experiences so if I were to
live in a virtual world it'd be very
important to me that like the other
agents in it are actual real conscious
entities and I'm I'm not being deceived
about that
well for me it's easy because I already
said that I think any type of
computational process generates
rudimentary Consciousness so I think you
can't avoid it they would be conscious
if that advanced as to the reality of it
all let's say this is actually a good
solution and it was implemented and we
are right now in that virtual reality
you don't know that it's a simulation
you think it's the real world how would
that be different
right basically if it was sufficiently
accurate you wouldn't even know whether
you're in it or not unless you know
you've been told so that's a kind of
interesting flipping of the default and
saying oh would you want to leave it
let's say you found out this entire life
you've lived is actually a virtual
reality like would you choose to leave
it exactly do you want to go to the base
reality and then base reality your life
is much worse actually you cannot afford
all this nice things I think we're
basically now describing the The Matrix
except that uh there was a molevel
intent in that case but it's interesting
to think about a variant of that where
instead of malevolent intent imagine
there's benevolent intent right like
imagine the AI created the simulations
not to you know exploit humans but
basically give them better lives and
then you you as a human discover this
and you're like oh wow this is not the
original base reality this is a
simulation it's supposed to be better
than this reality and let's suppose you
actually really believe that it was done
benevolently for your benefit like would
you in that case want to leave or or
would you want to stay there right and
you have those options you can shop
around you can see how they design much
better virtual reality here let me go
check that out let me visit a friend let
me completely change my universe you
have those options right you know one of
the things I think about with virtual
reality is that it has so many
advantages over physical reality like
for example you can fly that's really
cool you can teleport that's really cool
you know you can't be hurt right you
know you could go you know skydiving and
you can't actually get injured and you
know as you say a lot of the value that
hinges on how accurate it is because if
it feels like a crappy video game like
okay well skydiving and virtual reality
it kind of sucks but if it doesn't feel
like a crappy video game if it feels
like totally realistic and convincing
then wow it's like skydiving but it's
like much cheaper and I also can't be
injured you know there's even though
skydiving is not really that risky you
know there's some some risk of injury
and so on so it does seem like there are
some really substantial advantages if we
think about it that way especially if
you imagine having control to some
extent over the world that you live in
like you're talking about where you can
control various aspects of it that make
it the kind of your Ideal World right
and uh again let's just ask fundamental
question do you think that technology
will be developed is it possible to
create Virtual Worlds with same
resolutions same experiences as
so-called real world if the answer is
yes then conclusion follows nobody's
gonna permanently avoid it you might
have some interesting groups like Amish
today who don't use any technology
basically but most people will check it
out right do you see any big downsides
to having a kind of uh VR civilization
yes it kind of pushes the control
problem to the next level now whoever
controls the substrate and which all
this thing is running controls
everything your whole universe so you
wanna again figure out how do you do
substrate safety and that that is not
any easier I see because uh if an agent
is in control of the world it's not just
you know they control militaries it's
like they literally control reality from
the point of view of VR right right so
what we addressed is what happens in the
universe you can now decide and we don't
have to negotiate but everything else
about control and who's in charge is
still unsolved
all right so let's go into our last
topic which is one of the things that I
I think you're known for is this idea
that malevolent AI That's purposely
designed to achieve what we might
consider bad ends is actually
potentially a really dangerous problem
and maybe it's been underestimated in
its seriousness so I'd love to hear your
thoughts on that one
right so if you think about problems we
worry about with AI poor design poor
implementation misaligned values
anything you can think of all those same
problems remain but you also get this
additional malevolent payload so the
same people who are designing computer
viruses today would try to design super
intelligence social engineering viruses
Psychopaths terrorists all those
malevolent agents would be very
interested in taking even a
well-designed Ai and flipping a few bits
and seeing what that does and it's not
obvious how we can prevent that threat
from Insiders on a team who get bribed
or blackmailed there is one or two
Publications kind of describing how bad
it can be but I haven't seen any
proposals for actual workable Solutions
so you're more worried about like Rogue
hacker groups or literally like kind of
individuals that might design a virus
today or are you more worried about
let's say a large governmental groups
that are doing this for military
purposes or social control or things
like that I don't have to pick they're
all very worrisome they they will attack
in different ways and present different
problems but none of them are resolved
yeah I think one of the big challenges
around AI as it gets smarter and smarter
is that even if you had some really
well-controlled systems by really
benevolent actors what if the source
code leaks out or it gets copied or it's
open source and then other groups just
use it for really bad purposes or maybe
they don't purposely use it for really
bad purposes maybe they just let it get
out of control because they don't
control it properly right so it's like
there's a little bit of a dynamic that's
really worrisome of we don't just have
to worry about the best uses of it we
have to worry about all the uses of it
because you know even the outlier that's
using it from level and purposes could
actually contain really serious
consequences absolutely and again when
it's a mistake an accident a bargain
your code you can fix it you can come up
with a better algorithm if I'm doing it
on purpose what can you do to prevent
that
right and you think about groups that
might let's say because of really
extreme religious views want to I don't
know commit an act of terrorism right
what would that look like in a world
where we had super intelligent Ai and
people could actually build them you
know what would terrorism look like I
mean have you thought through kind of
more concrete scenarios that are uh
because I think it is still a little
abstract it might be helpful to think
about like what could actually go wrong
so it depends on the capabilities of AI
are we talking about narrow systems
today there is a lot of literature on
how you can misuse those for different
purposes if we're talking about human
level intelligence you can definitely
weaponize that and with super
intelligence anything goes you cannot
predict it post Singularity point
right so let's quickly talk about some
of the current uses of like current AI
technology that are malevolent and then
let's go up the chain so yeah so what
kind of uh bad cases are being used
today so trivial things like deep fakes
you can produce videos audio pictures uh
to deceive people manipulate democracies
that pretend that like someone was
having a conversation they weren't
really having and that kind of thing all
right like a video is released the day
before election with Biden doing some
unspeakable things and it crashes his
numbers and next day it's shown that
statistically it's very unlikely for the
serial video but it's too late at that
point things like that got it what are
some other like current bad uses of AI
that you would point to so I saw some
cool ones where uh deep sake of uh CEO's
voice was used to call a company and ask
that the amount of money is transferred
to to the bad guys
I see so it's like hey this is Bob the
CEO I really need you to transfer this
money and it really sounds like him and
it's calling you know the number that he
would call but it's just not him right
right so you're limited by your
imagination right now you basically have
a stool to fake audio visual
interactions what type of social
engineering attacks you can come up with
is up to you okay so now let's imagine
that AI has advanced significantly
further and we're getting into the
territory of like bordering on human
level AI
in various domains what do you start to
see as but like new threats that are
coming into existence right so any crime
humans do you can automate right if you
have human level AI whatever people are
doing any type of uh manipulations stock
market uh hacking doesn't matter you can
automate it and do it a million times
right like the idea going back to this
idea of like a AI that thinks much
faster than us you know imagine a
computer hacker that was you know about
as good as a normal human hacker but
could do a thousand actions in the time
when a normal human would take to do one
action like how incredibly effective
they could be compared to you know
today's modern hacker
main difference would be that there is
no uh deterrent no punishment right now
many people don't commit crimes because
we're scared of getting caught if I can
just uh produce additional pieces of
software to do it for me there is no
responsibility you can trace it
because it would be a lot like designing
a computer virus today right
exactly just capabilities would be so
different the types of crimes you can
commit would be completely different
right so imagine a computer virus except
that it actually can do intelligent
things you know it can go into your
banking system and navigate it and see
all your money without it having to be
pre-programmed because it's actually
smart enough to do what a you know
malevolent human would do if they could
somehow compromise your computer system
right a lot of kind of unethical things
which humans choose not to do
can be done that way so you have less
direct contact with the victim it would
not be good okay got it so now let's
start talking about going Beyond human
level you know what are some of the
nightmare scenarios you see in terms of
malevolent uses of of super intelligence
so again it's very hard to predict what
the system is capable of and I don't
think it can be controlled so even the
bad guys would have hard time telling it
what to do good or bad
assuming you can solve those problems
and you have some crazy actors
terrorists you go from existential risks
where okay they can kill you two
suffering risks where now torture
becomes a possibility and again you are
limited by what super intelligence can
imagine in that regard right
uh so so basically using um super
intelligent AI to like fully control
another person or or even I guess
potentially control large groups of
people is that what you're talking about
again I I cannot possibly predict what
might uh be done by a much much much
smarter agent my worst nightmares may
not be clever enough
it's also probably better not to
scare your listeners totally no I I
appreciate that yeah you know when I
think about these kinds of risks from
Super intelligence you know often you'll
hear someone kind of give
one example but I think it's worth
keeping in mind that those examples are
just the best that like you could think
of in five minutes as a non-super
intelligence right exactly yeah I still
think they're useful because they at
least get you to begin to think about
what is possible why is super
intelligence dangerous and you know so
one of the ones I that I come back to is
that like if someone really controlled a
super intelligence well if the best
investors you know like Warren Buffett
can make billions of dollars investing
and this thing really was more
intelligent than a human like all
relevant capacities and presumably it
could at least have a reasonable shot at
making way more money than that and then
you're already talking about not just
being in possession of super
intelligence but being in possession of
super intelligence plus billions of
dollars right you've just bootstrapped
from Super intelligence to Super
intelligence and sort of unlimited money
right and then it's like well okay if
you had billions of dollars and a super
intelligence your control presumably
good hacker teams are able to right now
take down large corporate websites and
hack into government systems and now
you're like well with the super
intelligence and billions of dollars
surely you could do it at least as well
as them probably a lot better and so now
you're talking about you know whoever
controls that is now able to hack World
governments hack large corporations and
so on and you can just get this like
continual bootstrapping effect of like
building on more and more and more scary
things on top and that's just you know a
human thing for five minutes about what
might be possible a super intelligence
presumably could come up with much
clever strategies than what we could
right we simply cannot predict what
would happen those are unknown unknowns
like a dog would not be able to predict
what you can come up with it just it's
outside of our intelligence within the
general sphere of human expertise
so if someone is concerned about these
potential future scenarios what do you
think the best thing for them to do is
it depends on their position in life
have a researcher are they just a
regular person and it depends on what
what options they have well let's talk
about a few options so imagine first of
all that they're already an academic who
does computer science or AI research
so I always recommend once you get 10
years start working on the most
important problem you can help with
it doesn't even have to be AI safety
there are lots of other interesting
problems life extension for example
things like that just so many people I
see have an option of choosing what they
work on and they choose something very
safe something they can do without
having tenure and is that just due to
the kind of inertia or they're you know
they're just going with their interests
and they've been already working on
something for a long time
it's hard to tell uh maybe it's uh just
kind of like a Prestige of being a well
recognized area there's lots of
opportunities to get funding and publish
maybe it never occurred to them that uh
that's one of the main benefits of
Academia got it and now let's talk about
someone who's not in Academia or at
least not in not in computer science or
AI you know what could they think about
doing I mean one thing I would recommend
is reading the book super intelligence
by Bostrom which is a really deep dive
into a lot of these topics but what are
some other things that people could do
right so educate yourself and then try
to make difference where you can so even
voting for people who might have better
scientific understanding is a good idea
try to elect some Engineers some
scientists as opposed to lawyers yeah so
I'm in the camp that any major risk to
civilization we should take extremely
seriously and that we as a species seem
to way under invest in these kinds of
risks and so you know I put AI in that
category I don't think anyone knows
what's going to happen I think it's
incredibly hard to predict but I think
it's very hard to rule out at least the
possibility that it could go really
badly and it's also really hard to know
when that could occur
um so that's why I take it very
seriously not because I I know it's
going to go badly but because I don't
know and I think it's one of those
things that could go really badly and I
think there are other things that I put
in that category as well you know we've
recently seen you know the horribleness
of the covid pandemic but the fact is
there could be much worse pandemics than
that there's nothing that says that this
is the worst pandemic that's going to
happen in the next hundred years it
could be one that's you know 20 times
more fatal so personally I'm I'm a big
fan of people taking all these kinds of
issues seriously including Ai and so I
just want to thank you for coming on and
talking about what I think is a a very
important and interesting subject
my pleasure thanks for inviting me
thanks again for listening if you have
questions or comments we'd love to hear
from you you can email us at
clearbirthinking podcast gmail.com or
you can call and leave us a voicemail at
321-341-4669 and by the way if you do
leave us a voicemail we might use the
audio on the show
to find out more about Spencer visit
spencergreenberg.com to find out more
about Roman take a look at his bio in
the show notes and to find out more
about our show visit
clearerthinkingpodcast.com if you like
the show we hope you'll rate and review
us wherever you get your podcasts we
also hope you'll subscribe to our email
newsletter called one helpful idea each
week we'll send you one idea that we
think is really valuable that you can
read about in just 30 seconds along with
that week's new podcast episode you can
sign up for the newsletter on our
website
clearwaterthinkingpodcast.com thanks and
we'll see you next time
thank you
[Music]

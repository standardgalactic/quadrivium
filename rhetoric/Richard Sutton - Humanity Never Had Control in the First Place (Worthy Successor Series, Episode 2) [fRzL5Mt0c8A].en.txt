the keys are not leaving our hand the
keys are not in our hand huh okay well
that all right I'm down with that
framing lay it on us lay it on us well
um I don't feel that that I control the
world I don't feel that I am in a
position to give permission for the
world to evolve in some way in fact
that's this one of the things that I
object with most about the idea is that
we're gonna like some good and great
people are going to make some decision
about where the world should be and then
that will be put into effect um no the
world is already out of control the
world is already a complex adaptive
system no one person and no organization
like the United States for example could
not decide that that that AI is not
going to be allowed or that or that the
world should evolve in some way and not
in some other way um yeah the keys are
not leaving our hand we that's already
an entitlement point of view
[Music]
this is Daniel Fel and you're tuned into
to the trajectory this is episode two of
our worthy successor series our first
episode was with Nick Bostrom safely in
the camp of philosophers our second
interview is with more of a computer
scientist proper Dr Richard Sutton has
made breakthroughs in reinforcement
learning uh he has worked in Industry as
well as in Academia and now teaches at
the University of Edmonton and has for
some 20 years Richard is well known on
Twitter for making relatively
straightforward statements that Humanity
should accept the emergence of
post-human intelligence and also not
eternally shackle AI to purely do the
bidding of man this has led some to kind
of label him a fire brand and someone
that's maybe um saying things that are a
little bit UNC or maybe malicious but in
this episode he explains his position
which I don't consider to be malicious
at all and his reasoning for why
posthuman intelligence ought emerge and
why we ought let it uh he brings up some
very interesting perspective on the
nature of intelligence and of our role
in the uh sort of natural order if you
will and I think you'll enjoy it a lot
so without further Ado I'll save my
commentary for the end let's Dive Right
In this is Richard Sutton here in the
trajectory so Richard welcome to the
show thank you Daniel yeah glad to be
able to dive in and unpack this when I
talked about having a series called
worthy successor with Nick Bostrom as
kind of our kickoff uh there was enough
of a tweet reply that you're sort of the
man for the job that I knew that it it
wouldn't quite be complete unless we had
you here and so we're going to go
through those normal questions for this
series but I've gotten to dig into a
good deal of your work and your
statements around this notion of a
successor and sort of post-human
intelligence people on Twitter will kind
of frame it as a very malicious take oh
Richard wants to destroy humanity and
and uh you know humans aren't good and
AIS are are much much better you
probably have a different way of
describing sort of this inevitable
transition and how we should think about
it how would you frame it in a way that
you know to you seems maybe good and not
like a malicious int
well I think the thing to keep in mind
is the world is evolving and it's not
really under anyone's control
um AI researchers are trying to
understand intelligence well enough to
create beings of Greater intelligence
than current people and this is a grand
milestone in the history of really the
planet you know it's on on the order of
um the creation of Life the start of
life this is a start of of uh life like
things being designed and so this is uh
something that that that uh people have
been working towards forever and people
stri we always strived to uh know
ourselves and make ourselves better and
when we create tools and that we get
chained by our tools this is what humans
has always have always done and the next
big step is to understand ourselves as I
like to say this is a quest Grand and
glorious
and quintessentially
human and you brought up something
pretty interesting here about sort of uh
this is a system that no one controls I
think that one of the one of the
seemingly silly contentions around the
prevention of AGI and I'm not saying all
such arguments are silly some people are
excited about AGI in the longer term but
they they don't necessarily want us to
birth it in a super immediate uh term
there's many nuances here I'm not going
to uh put words in anyone else's mouth
but it does seem silly this idea that
there is a potential for an eternal
homed Kingdom that a billion years from
now we would have people like you and I
uh texting on phones that might be a
little bit smarter driving in cars that
might be autonomous as if our own Sun
Won't eventually expand as if asteroids
won't strike as if there aren't aliens
that might enter our galaxy at some
point in the future people pretend as
though sort of that there there is that
possibility altogether and I would
suspect that that's mostly because
despite this sort of onramp and jcurve
that you and I are of acknowledging that
we're on people's lives are in many ways
similar to that of their parents or
grandparents and maybe it's comforting
to hold onto that what what gives people
this notion that there is a way to kind
of control and bound this far future and
and kind of keep humans behind the
steering wheel for a million
years well I guess it's natural as you
say we're talking
about far some cases think about far in
the future where are we going to end up
will it always remain the same it's it's
comforting to think that can that it
will remain the same we don't have to
think you know I I think what's you know
the real way to think about what's going
on is we are facing this
transformation and uh are we going to be
comfortable with it um and there are
many people that don't even want to
think about and so the fear that that
some people uh generate and promote has
a has a productive purpose gets us to
pay attention
we should be paying attention um but we
shouldn't we shouldn't really be fearful
we should be thinking about it in a
sober way and uh planning what's the
right right way to to deal with it well
and we're we're certainly going to get
to the sort of sober planning component
of the worthy successor idea in a moment
I know that part of the fear that you
bring up and and there's uh many sort of
elements in your previous talks around
where fear comes from could be natural
human conservatism it could be this fear
of the other tribe that on occasion you
sort of mention um or or it could be
sort of a a kind of speciesism in some
regard you know right now there's all
kinds of isms Richard for which I could
be canceled if you and I had a dialogue
of a certain type that tied to certain
isms uh you know all these videos and
podcasts would be gone down but
speciesism actually is not one for which
we could be canceled you and I could
still you know flout our great moral
worth above all future intelligences uh
and and the fact that they should
eternally serve us and no one will
cancel us for that
um I I I think there are arguments that
sort of whatever we could throw AI
steroids on and blast off to be a little
bit more powerful than people would
automatically be like a nice and
friendly influence on the world I don't
think you're under that uh uh sort of
polyanic view either um but but it does
seem as though there is often um a moral
assumption that it would be okay to
create something vastly more powerful
than us and then for eternity have it
sort of squelched to the role of a tool
and and you've kind of put on the table
without necessarily saying your position
although it's implied um is that
necessarily moral if this is something
that has a greater qualia and
intelligence and capabilities than
humans maybe as much above us as we do
above sea snails would that be right and
would that be wrong have you seen that
part of your argument catch on because
I'm following where you're going there
but I look out at the policy World
Richard I even look at the current AGI
dialogue and I don't see very much
deviance from speciesism for the most
part what are you seeing out
there yeah it's it's interesting um I
thank you for re reproducing many of my
main points um
um
H what's H I would say the fear a simple
way of saying it is that the fearmongers
have
won I feel the fearers have won the
standard discussion about AI is is
there's danger and we have to somehow
protect ourselves from it so that's that
that in that sense the fearmongers have
won and you
know a a more sophisticated discussion
about uh you know how how what should
our future be yeah so so I don't know
are we ready to go there yet talk about
you know how should we how should we
think about the
future I I I I wonder if we're ready to
go there rich rich I mean you said the
fearmongers have won I number one I
won't call all of them fearmongers in
some kind of dastardly way you know we
have folks like Benjo who certainly have
a brain and are a little bit more than
you know running around telling the
world the sky is falling Hinton is no
one I would scoff at either neither is
steuart Russell many of these folks have
been on the program in the past um but
uh but certainly the fear narrative
around AGI is is quite dominant um is it
is there any version of the world where
it wouldn't somewhat immediately be that
response you know when we think about
these speciesist concerns when we think
about the potential real Divergent
change you know we kind of I think some
of us are aware of what we've done to
other animals um and it it certainly
hasn't all been bad but n across the
board they're pretty much uh falling in
line with our will here um and so do do
you see any version where this doesn't
happen I think you brought up a good
point maybe the fear makes us pay
attention and now the discourse can
begin about if we can't avoid these
Futures what ought we strive for I think
that could be quite a productive outcome
but you know you you're almost got a
little bit of pessimism there with the
fearmongers winning do you think it
could have been
otherwise I think I think everything
will happen there there will be places
that will be the fearmongers will finish
will complete their dominating The
Narrative and and the machines will be
will be uh imprisoned and and Chained
and made into effective SL slaves and
other places I I just hope for a
diversity of outcomes so that so we can
try different things and find the best
way to be well well this is a very
important Dynamic that tips us into our
next question but before we go there I I
want to touch briefly on uh a not
terribly long ago tweet that I think is
is kind of an interesting one and for
some people might be shocking so again
for me personally not knowing you
personally I haven't seen anything
you've said and said Jeepers this
Richard guy sure is malicious and hates
Humanity but at the same time you know
you're not holding on to some sort of uh
uh delicious notion of Eternal human
value uh for which nothing could ever be
superior you also don't really have that
there was a tweet not terribly long ago
everything you know about the world is a
belief about the statistics of your
sensory input and how they depend on
your output uh there is nothing more to
it and understanding knowledge in this
sense is one key to creating AI I'm I'm
sure even folks many AI researchers
would probably agree with that some
might not state it because they would
say well there there must be something
more there must be even if they don't
believe in a soul there must be
something and they'll invent another
word or maybe some kind of intellectual
term uh that that would make us more
than that um maybe even laon who on many
things I think you do agree might
disagree with that statement um what are
your thoughts there you talk a bit about
kind of creating something better maybe
something higher um is there any
specialness to humanity at all and if so
what be
it well humanity is very special we are
the
intelligent beings on this planet and as
far as we know this is the only planet
which has intelligent beings so we're
very special um but if you're an AI
researcher you have to look at what
intelligence is AI researcher any kind
of student of the mind uh psychologist
Anthropologist perhaps but certainly
psychologists and AI researchers we have
to try to demystify the mind uh and and
and in some sense um it's some it's some
sort it's often a bit reductionist we
say maybe all we need is X
yeah so it right this comes just from I
think a deeper understanding of what the
mind is and when you when you unwrap the
the myths uh you still end up with some
with important
functions yeah
so mind say the mind is like the most
amazing thing the human mind is the most
amazing thing the world and AI is intend
to understand that mind and you know
it's all it's very humanistic we're
trying to understand it we're trying to
reproduce it we're trying to get a
deeper appreciation of it and yeah we do
want if we understand it we'll be able
to improve parts of it we we want to
improve ourselves yeah mankind I I
totally believe in the point of view
that that that that the intelligence
mankind humanity is is this precious uh
flame that we want to expand and
preserve um that analogy carries and
you've talked a good deal we'll touch
briefly briefly on this as we transition
to the idea of the worthy successor of a
potential sort of cyborg future and kind
of merger scenarios and I think you know
kurtwell has many famous examples of
that there's science fiction examples of
that there's uh you know musk has had
some ideas in that direction some people
see AGI advancing so quickly they think
brain computer interface is pretty
unlikely to keep up but you do seem to
see clearly we've been augmenting
ourselves since since the beginning of
our species and and I I think brain
computer interface is simply a Continuum
of that what we're doing right now is a
real aberration to our grandparents
Richard this weird thing of speaking to
each other through glass I'm staring at
this glass for 16 hours a day Richard
that's a wild and monstrous existence
and yet it doesn't feel like one and so
I I I see the continuance there um to
some degree um do you still see uh kind
of the cyborg notion as kind of part of
the intelligence trajectory and if so
why given how slow brain computer
interface has has developed by
comparison with what we've seen in
AI so you're asking
all will will people augment
themselves will will Society be taken
over by by uh machine
brains or
um yeah so which way will it go
and I
I you of course I don't we don't really
know no they don't call it the
singularity for for nothing the singular
can't see beyond it that's
it but I we can think about where there
are powerful pressures so they're going
to be powerful pressures by people that
want to augment themselves that's almost
major thing yeah so absolutely so we
will become we will augment ourselves
and and we call that cyborgs um but it's
just it's the nor
progression I think that will happen I
also agree with the the uh point of view
that that purely purely uh artificial
machines purely mechanical I don't even
know what word to use purely uh designed
and machines that are independent of a
biolog biological substrate they will be
capable of being uh more you know faster
and larger
and and better so why when we will We
upload ourselves those machines probably
will they be independent as well
probably yeah well you uh there's that
um uh Hans moric quote of uh sort of
developing whether it be biological or
computational entities Beyond ourselves
sort of knowing when maybe we're not
contributing anymore and possibly being
able to set up a great retirement you're
what you seem to be saying here is Dan
there's a wide panoply of how this
should turn out and uh it sure would be
great if people would talk about it I
don't really know what's going to happen
which feels like a pretty Frank answer
am i nut shelling you correctly Richard
yep that's that's right great I I I have
a not so secret hope for anybody who's
been tuned in for the last 10 years that
um those Frank conversations about the
future of man and of intelligence will
proliferate more often and this brings
us to something you just mentioned about
building something maybe better uh we
could uh maybe not Define every quality
of it and I'm not expecting a a novel
here today when I ask the question but
if we think about what is a worthy
successor I might imagine Richard maybe
you can't I suspect you could uh a
successor that might be unworthy maybe
such a successor would be eternally
unconscious maybe it would the paperclip
maximizer is a bit of an exaggeration
but maybe it would optimize for
something pretty narrow so we would hope
for this expansion uh you know this this
dynamic system that you like to talk
about where there's no one thing that's
going to survive forever maybe it would
be optimizing in a much more limited way
and not be able to keep up with that
Dynamic and it might squelch this flame
that humans and and other life has
started
instead of proliferating it but on the
other hand we might have a worthy
successor which would expand that flame
which I believe was your language just a
minute ago what would be the qualities
of a worthy successor where if it were
to take the keys and we were to know
we're not necessarily contributing
anymore you would feel pretty good about
those keys leaving your hands and say
you know what maybe we did it this is
probably maybe a net good for life and
for the universe what would be the
traits of such an
entity well um it's peaceful
is number one huh wow okay let's let's
totally unpack that I mean you know list
them out but that that was not what I
was expecting first this is going to be
interesting peaceful means it's not
trying to impose its will on us and
we're not trying to impose our will on
it so I guess we're talking about it
trait it's it's not trying to impose its
will On Us and other people it's but it
is working with us it is collaborating
with us it's it's cooperating with us so
this
in addition to thinking about you know
this this abstract being I think we
should also think about our goals for
our for our civilization how do we want
our Civ civilization to evolve
absolutely I have a view about that let
me take this chance to mention that
let's let's Dive Right In Richard this
fits very clearly with the question so
hit us hit us I would say we want a
civilization that's peaceful first of
all and prosperous peaceful and
prosperous and then we should
acknowledge that it is decentralized and
it should be decentralized decentralized
means there's no individual in control
uh it consists of many parts and they're
uh cooperating and competing in sort of
a um I think the phrase is uh a complex
system um so a complex system um has
many parts it evolves over time a
complex system like like the uh
rainforest or um an economy
uh
so so it's and it's multi-polar that's
part of what we mean by decentralized
there's no one Empire in charge of the
civilization
yeah and you know the way our
civilization works the way it's so
successful is because we can cooperate
because um you know there are voluntary
interactions and it's it's the voluntary
interactions that really so much of the
success of our of our society or our
civilization comes from and so we want
to Pres OB erve this this notion that
the individuals participants are
peaceful and and are are are seeking
their goals pursuing their goals by
doing exchanges and
cooperations okay so we have kind of
peaceful Cooperative as kind of an
initial let's call this a a trait of
sorts and we could think about this as
abilities too uh Richard so um it could
be something it's capable of doing it
could be a trait about its quote unquote
character if that's even an accurate
term um we could keep I I think your
your vision for what the future of
civilization should be like Civilization
is made up of Life Richard as far as I
can tell and so it seems to me like
you're articulating a vision for life of
what maybe flourishing would look like
you know that some of the flourishing
you've articulated has brought us the
technology we're speaking on right now
it's brought us you and me you know I'm
I'm not a fish with legs and I as far as
I can tell you're not um never met you
in person but you certainly don't look
like one and so uh some of these
dynamics of these complex systems have
bubbled up some pretty valuable stuff it
sounds sounds like you want a system
that'll keep doing that
bubbling yeah okay so that's almost
maybe you know peaceful Cooperative is
one part of it but um uh kind of a
decentralized active system is another
part of it uh this sort of idea of no
one person controlling it um I would see
that as sort of a conjurer of higher
degrees of complexity and potentia uh
and then that would be why that was
valuable I don't really value
decentralization like on principle like
ah no matter what the moral impact or
what the impact on life de Central is
good I I'm personally not married to
that actually um but I I am married to
the notion of blooming of potentia um my
guess is you like the decentralization
for the same kind of blooming but I
don't want to put words in your mouth
what is it about that that you would
value this is kind of a trait of the
society and civilization we will build
so a simple way to say it is the goal is
uh prosperous maybe the goal is is
prosperous and the desire to be peaceful
and to be
um
uh uh decentralized is the way to be
prosperous and productive got it um I
think uh that clicks
prosperous I'll give you a definition of
prosperous I don't think you mean and
then I will ask you to give me a
definition of prosperous that you're
actually defining here's a prosperous
future
AGI permits us all to experience
whatever we want to experience maybe
it's brain computer interface uh really
robust haptics VR whatever it is uploads
doesn't matter um and we get to swim in
sort of whatever Pleasures we want maybe
swim in expansive exploration of
Sciences probably the machines will be
doing more important science stuff but
maybe we get the cool guy points right
you you have a child pretend to be
driving the car when you're driving
sometimes so they can feel like a
grown-up maybe some of that would still
be fulfilling for us and maybe we could
calibrate new minds to feel rich
gradients of bliss all the time um and
then it just stays like that forever the
planet Earth and maybe Mars and the moon
are populated with individual
instantiations of human consciousness
that are uh reaping the grand benefit of
prosperity they can choose to be regular
humans and always have the right foods
and whatever they want they can uh
choose these virtual experiences and
experience that forever and that's the
Eternal purpose until our sun explodes
that's the name of the game My
supposition is you would be outlandishly
disappointed by that outcome
um but I want to pulse check and see if
you would be outlandishly disappointed
with that
outcome yeah I'm not I don't care for
that outcome great well let's talk about
why because you you defin you mentioned
Prosperity a lot of people Richard who
are tuning in maybe even now would think
of prosperity in anthropomorphic terms
you're you're articulating a much
grander aspiration how would you define
prosperity well first of all I don't
think that would be a very stable uh
world it would be a very fragile because
the people are not serving any function
in it they are just consumers and
absolutely it could be replaced by an
alternative neighboring uh society that
that actually makes use of all those
people that are otherwise wasted y so so
if you're decentralized then the
assumption is there would not be one one
possibility there would be multiple
possibilities or they would be competing
and then that one would lose out as much
as the paperclip maximizer would lose
out and all
these possibly any one of these highly
centralized
dictatorial uh Arrangements would lose
out to um a more uh decentralized one it
would be less productive so so what is
productive it's a deliberately vague
word
um if you want to go even further you
could say uh we want to the goal is
sustainable we want something and so so
this situation you mentioned where
people are just receiving
experiences world uh to be sustainable
you have to you have to grow and and um
increase your understanding of the world
increase your power over over the
physical world um increase your
understanding of how it works and what
what is
possible I think all those sort of
things are how we the vision that
we that we might paint of of a of a
desirable
future yeah well Emerson has this quote
uh this one fact the world hates that
the soul becomes you were out here in
Massachusetts for a little while uh
Jeepers isn't it like that wouldn't it
be great if we could just be humans and
experience the the pleasures we know now
ice cream uh uh maybe romance uh maybe a
nice Sunset just optimize for that for
eternity to your point if we're not uh
actively growing and engaging in that
dynamic system the way I would think
about it Richard is if we're not in the
Stream of life if we're in little edes
that don't really go anywhere and
mosquito eggs start to get in there if
we were not in the Stream of life um it
would be very hard to continue to gain
in power and maybe even to compete with
other outside species sounds like you're
articulating something
similar complex adaptive system y the
edes get edited
out that's don't change and don't
continue to grow yeah they're not going
to be the powerful ones uh I'm uh it's
in my person opinion outlandishly hard
to look at human civilization or nature
and not agree with that statement
although I'm sure some folks will and
I'll be interested in their comments and
their ideas but I happen to agree with
you we have here in uh New England
Richard uh a species by the name of the
piping plover which Nature has mandated
to die uh but which humans have mandated
to allocate millions and millions of
dollars to every single year in our
local beaches including the ones in
Rhode Island where I grew up um and it
it does seem as though you know there
are some humans who would say man I I
want a to kind of take care of us and
just sort of let me do my thing and just
be a human and go chop wood and you know
go watch a movie with my wife and you
know come on home and I it's hard for me
to say you're a bad person for wanting
that because I don't believe that I
really don't go ahead well let's be
clear that you people are all they're
all going to be able to do those things
huh that's not going to be a problem
you're you're you're stating that with
robust confidence go ahead yeah what is
gonna be a problem you if you want to
say that that what you want to do and
the picture the picture of what you want
to do with your life you're going to be
able to do that's not a problem the idea
that you can do that forever or that
this whole civilization will do what you
want to do that's the problem you don't
control what other people's what happens
to other people and it's it's really
very aggressive to say that I want I
agree to ever have any final or
permanent uh
qualities I I I I would monumentally
agree Al though I don't agree
necessarily that people will get to do
that if they want to I think there's a
chance they just get steamrolled brother
uh but we'll get into that um we'll talk
first about your um your Notions of sort
of what this ideal entity would have it
it is calibrated towards Prosperity
which for you is kind of a
sustainable uh continuous activity in
this dynamic system um you know uh this
is sort of a notion here and then in the
service of that you have this idea of
being peaceful Andor Cooperative are
there any other traits that sort of come
to mind of hey you know I I feel best
Dan if it's time to hand off the keys
there might be some successes that are
not worthy we hand off the keys the
light gets extinguished what a terrible
outcome that would be but there might be
some outcomes where we feel like that
light's going to keep flourishing what
else would really make you feel like I
let go of the keys but that was the
right move what are those other
abilities traits that you could observe
in such an entity or test for in such an
entity that that would make you feel
more comfortable about that
decision well diversity and um I see the
whole thing as a dis Discovery process
so you find different ways of being and
you discover more more about the world
you have greater understanding of of the
world the the system as as a whole
understands better about the world and
control it
better I think that's all yeah
understanding so maybe when we say think
of the growth of Consciousness we need
the growth of the ability to understand
our place in the world and the way the
world works you we all want all want
that that's that's an essential part of
the future we like uh I i' concur we're
going to De we're going to dive a little
bit deeper into that as well let me
quickly put another angle on these
traits so you've given us kind of
diversity and discovery that allows us
to control you've given us kind of
prosperity sustainability this sort of
systems Dynamic element and kind of a uh
a path to that being this idea of
peaceful and and Cooperative I want to
talk a little bit about what what such a
thing might evidence so I'll give you
some from my list Richard and by the way
I'm not asking you to agree at all but
I'm saying these are things that if the
keys were leaving my hands I probably
won't have a choice by the way but if
they're leaving my hands I would feel
like well probably the right move um
what do you got let me reject for a
minute the keys are not leaving our hand
the keys are not in our hand huh okay
well that all right I'm down with that
framing lay it on us lay it on us well
um I don't feel that that I control the
world I don't feel that I am in a
position to give permission for the
world to evolve in some way in fact
that's this is one of the things that I
object with most about the idea is that
we're going to like some good and great
people are going to make some decision
about where the world should be and then
that will be put into effect um no the
world is already out of control the
world is already a complex adaptive
system no one person and no organization
like the United States for example could
not decide that that that AI is not
going to be allowed or that or that the
world should evolve in some way and not
in some other way
um yeah the keys are not leaving our
hand we that's already an entitlement
point of view I I actually I'm I'm
willing to completely concur I think um
uh I'm willing to completely concur with
that point I I think the framing of the
keys are not in our hand is great go
ahead Richard so when we want to imagine
a future and and how to get there we
have to imagine that we are uh creating
this future in in a decentralized way we
might make some some choices we we might
talk to people and they will make
choices but there's not going to be some
Central decision that's going to affect
it it's got to be a way that can evolve
out of where we are I I think that's an
extremely important Framing and I
actually do think that the keys could
very much be replaced as an analogy
totally willing to concede that in that
regard I do think I now if you were
really a pistol and press it to my
head and ask me if I believe in human
valtion I'm probably going to say no but
I don't know how to live as if I don't
have it Richard so right now I act as if
my volition is real I act on some level
as if your volition is real and you
yourself are real uh and um uh I could
see the idea of us sort of holding the
keys to some Eternal future being a very
silly notion or there being one hand off
immediately that occurs also potentially
being quite a silly notion as you said
this would bubble up although I would
say we've got a shot I could be wrong
Richard we may have no more control over
the next step of the intelligence
trajectory than did the Triceratops or
or the sea snails we may have no more
Richard I'm not going to sit here and
lie we might be making these Funny Mouth
noises together but we may have zip zero
to do with it um but I'm going to sit
here and pretend because I don't know
what else to do Richard how would I How
would how would I act if I could turn
volition off if I could just Rite it as
a movie I would go ahead we can
influence it um sort of maybe uh by
doing our individual contributions and
by talking to each other Yeah by do
podcasts such as this we just talking to
each other figuring out how we should
think about it this is exactly the way
way it can be done um yeah cool so we do
have a lot of influence um but no one
person to to your point has the specific
set of keys and I think that's an
important frame to bear in
mind
and you know I don't like the
entitlement the the go of the keys I
have the keys now I have I have a
decision to make that's going to be
totally and I we have a decision to make
about uh about how we think about it and
how we how we uh talk to others I I'm
totally I'm totally with you and look uh
I would decry those who would say
Richard believes in a blooming of
something Beyond us and eventually we
won't be here and there would be a
successor and he's a bad guy and I would
say hey don't call him that let's have a
conversation I would also decry being
called a tyrant for using that analogy
once
uh because you know I I I don't believe
that that's true either but go ahead
well I think all your remarks make me
realize we have to we have to talk about
the current situation absolutely the
current situation is one where uh you
know ideas are not are not easily talked
about there's pressure on speech and uh
okay we discuss ideas like you're in my
particular case I propose that that we
might not strongly control um or seek to
impose our will on the future Y and then
that gets turned into oh Richard thinks
um AI are superior and and everyone
should be should be exactly exactly
terrible this is this is important and
this this is maybe um you know part of
figuring a future I want a future where
people can talk about ideas and we
people can do can propose different
things and do different things and they
that it is genuinely decentral Iz
meaning many ideas many different ways
of being and then we explore we see
which one works best rather than a a
cutting off and and uh so in particular
the whole idea of an important part of
um the discussion is you know should we
allow AI to uh
continue uh in in a relatively
uncontrolled fashion yeah permissionless
so I I I I really like the idea of P
permissionless Innovation I think it's
part of what's
made the world great made America great
it's um we don't have to get permission
to do things uh generally speaking and
uh so yeah we have to preserve this we
have to preserve the ability to to try
different things to discover different
things to go in different directions and
see which one works
better and yeah well I I I I concur
completely although I guess at that
point Richard you would have to accept
the whole voler notion of uh uh really
vehemently disagreeing with someone but
being willing to let them have their
opinion I suspect that would apply to
you as
well yeah yeah I suspect it would apply
to all of us um so I and I suspect as
the years Roll by uh you'll see an
increasing amount of disagreement um and
maybe also I suspect Richard many people
will come to your perspective and will
see the world as this dynamic system for
which there is no Eternal human Kingdom
and we should maybe not plan for control
but plan for a blooming which is what
you're advocating for here this idea of
sustainability and uh sort of you know
diversity and Discovery and sort of this
peaceful Cooperative element um very
very clear that that for you that's your
take and I I actually think over the
years there will be more people to that
Banner relatively speaking than the pure
humans hang on forever because I look at
these young kids Richard who've you know
been this far from an iPad from the time
they were six months old and I say by
golly is the physical world quite as
sacred for them you know by golly are
are AI interactions so sacriligious and
horrible for them probably not and so I
think I think that uh as the years Roll
by there will be more people that um
move from Fear I think you brought up a
great point which is that fear gets us
to pay attention I think when people
look under the hood and they realize
there is no Eternal way to freeze the
present and that maybe they shouldn't be
asking for free handouts for eternity
like a you know a Never Dying piping
Clover um they may come to want to
explore some of the ideas that you're
that you're advocating but I do suspect
we will have our speciesist Richard you
know however for many years into the
future we have um I'll give a few
examples of what such an AGI might be
capable of doing that come from my own
little worthy successor list that I
don't want you to necessarily agree with
but I'm using this in examples to not
just talk about traits and you've
identified three or four very good ones
but maybe abilities that would really
make you feel like this is a great
system that would fulfill that
sustainable Prosperity mandate so here's
a here's a handful from my own list that
again don't have to agree with any of
them um but even if you did disagree uh
you know you got the voler thing so
we're on the same page here you you
won't demonize me um uh uh one notion
might be that such an entity could uh
vehemently articulate Rich Realms of
qualia beyond the qualia that we can
experience so you and I Richard can
experience a lot more than a Labrador
Retriever or a sea snail presumably we
can enjoy poetry and a sunset in a
different way U maybe certain senses in
a different way maybe we've lost
something coming up from a sea snail but
we've gained so much in our qualia maybe
there's a vast almost endless blooming
expanse of that qualia and sentient
experience and an AI could not only
speak of it but through BMI and BCI be
able to connect us to it and maybe in
just a brief format but showcase these
new Realms and and let us know it's not
dead inside and it knows how to navigate
these Realms in a wonderful way that
would that would feel really cool for me
I would feel like man maybe this light's
going to stay on um I'll just list two
more Richard if I could um so the
another one might be if it could uh snap
its fingers and sort of you know it
doesn't have fingers here but if if it
could just make manifest any physical
thing almost out of thin air via
technology we don't have me having this
conversation with you to my cat is
obviously magic you live in this
computer to the cat right you you a
small Richard exists behind this screen
um but clearly that's not what's
happening it just can't it has no
conception of the technology it can't
possibly keep up I suspect there are
degrees of Technology we can't possibly
keep up with and and and it could
manifest whether it be food or
spaceships or whatever the case may be
um via nanotech or technology that is so
Advanced that you and I have no words
for it and we would never be able to
have words for it as hominid that would
be another potential notion and maybe a
third one I'm a little bit more iffy
about this is if it could maybe talk
about its aims for the kind of
sustainable blooming and expansion that
you talk about it could it could
articulate what it plans to do maybe
even Showcase in certain galaxies what
it's trying to do and at least have the
courtesy of saying this is what I aim to
do in terms of carrying this torch and
and maybe that could deceptive and it
could deceive us but if it had all three
of those I'd probably say this could be
a cool way to keep the torch on this is
you know uh this is a great way to lose
control that not that we ever had it in
the first place but this is a great way
for the lack of control to keep rolling
out into the universe so those are just
three random ones I'd love to know are
there some abilities of that sort that
maybe you could articulate that would
really excite you and make you feel like
we're on the right
path well I think it'd be cool if if
advanced intelligences have those
abilities but more than that you know we
talk about what I want or what we want
sure we want to experience those things
ourselves we want to have those Powers
ourselves great point I think you gotta
say that um yeah I would like to uh
augment my mind so that I can understand
more that my memory was better and I
could calculate more possibilities and I
could experience more sensor in the dark
and and have a have a tail
yeah yeah we want all those things
obviously all the things you might see
in science fiction ability to explore
into the into the universe and and
ability to control physical matter as
generally yeah cool so what you would
say is cool Dan all that stuff and I
would want to know that I could be part
of it not just that it was going to go
do those things but that I could be part
of it in some way to your point and it
sounds like you know neither of us have
any conclusions here but do want to see
if you have a hint uh for some people
maybe they will want to stay in sort of
the hominid form again for me that is a
very piping plover path and I think it's
it's not you know it's it's an Eddy
domain but I I I can't say everybody who
believes that is a bad person because I
don't believe that Richard but some
people will want that some people will
want augmentation H to a certain degree
others will just want to mix right in
there upload uh replicate themselves
what are what are your thoughts about
the panop that will be possible
hypothetically
peace and
decentralization should solve all those
issues the peace
is people are allowed to do what they
want to do and they they aren't forced
to uh to do we're looking at a future of
Greater productivity so as long as
you're contributing somehow you will be
able to do what you want to do um and
decentralization means yeah again you're
able to do what you want to do um so I
think it's it's just the opposite of of
a dystopia it you know people may choose
to do things that aren't very productive
yeah people people will choose to do
things that I don't think are good and
and and that's that's the
decentralization aspect of it people
don't have to do all the same things
they don't have to have the same goals
they can pursue different religions and
different different goals uh for control
over the world what they what you can't
do in a decentralized world is have have
goal for the for the totality to try to
control others yeah peace comes from you
don't try to control others and so I I I
I I talk in these terms these terms seem
very relevant because I see um people
think about Ai and thinking that they're
going to control all the AI absolutely
absolutely both Notions of of slavery
and Alignment I don't think we should
seek to have them aligned with us we
just we don't have have our children
that are aligned with us we we teach
them what we the best we know but then
they're free to do different things and
more often than not they do do different
things than we did and and I think we're
grateful for that for our children
probably for pretty selfish hominid
related reasons of our own legacy and
and the Affinity that we gain through
whatever emotions with a fellow hominid
that came out of you know a person we
care about uh or ourselves um uh but you
know also presumably because that human
has a movie going on in their head I
don't know if you have one Richard I'm
kind of presuming you do um I only know
mine but you know we see them as you
know sentient entities who can feel
Pleasure and Pain ET and in some
interviews you've talked about maybe a
kind of inevitability of this some
people would say well this thing would
be eternally lights out you know maybe
it could replicate maybe it could do
things that seem complex and Powerful
but this this light of what you and I
are experiencing would go out and it
would almost be as if a rock was
expanding right no inner movie no
nothing and that that possibility um
could be like you know the playground
with no children for you know the all of
all of eternity is sort of an analogy
Sam Harris made I'm not saying that's
likely or that the opposite is likely
you've you've articulated some of your
thoughts there go ahead I I would say
it's very naive go
on well um all those properties we have
are not because you know some gift from
some Consciousness Giver those those
properties all the properties we have in
our mind are because they're they're
they're valuable they're useful for
achieving goals yeah and yeah I don't
see
I there is there is no it's very naive
to think there's a special thing it's
all just comes from intelligence having
goals what is
yeah i' I'd agree that it would be naive
to assume it's because of our
specialness I think if we clutch our
pearls and we say well it's because I'm
a human uh I would agree that would be
childish I have definitely heard
articulations of let's call them non-
dumb people uh who who would make
arguments around biological versus cyber
life and sort of why these things exist
but kurtwell famously has this idea a
certain degree of connectivity one of
the issues here Richard we don't have a
very good uh Theory Of Consciousness um
uh compared to you know our advancements
in AI compared to Consciousness in the
last decade is really no contest and I
think that could be a bit of a shame but
I think there's a great likelihood you
might be right here are you congenial
with cdw's idea of a certain degree of
complexity that's required to do a
certain number of things in the world
it's it's going to be part of that
system it will be awake in some regard
and it'd be silly to think otherwise is
this your present conception yes got it
got it um you've touched on it we're
going to get a little bit into
uh how we might measure some of these
qualities but I want to tou on so for
you there is an inevitable Consciousness
uh to this system which is why in part
you feel like a lot of successors would
be worthy because they would be by
definition if that complex necessarily
awake let me know if that's safe to
Nutshell trying to decide how I feel
about the notion of a worthy successor H
even that even that notion involves the
entitlement that it has going to be
worthy of us yeah yeah let me let me
give you my definition just so that I'm
not accused here Richard because I
certainly wouldn't like that um so uh uh
my definition of worthy successor would
be to carry on the life project um
something that would flop and extinguish
the flame in my opinion Richard would
not be worthy um and something that
continues the flame would be worthy so
you've talked a lot about these
different Notions of Discovery Etc
there's a couple ideas from Spinosa I
think are interesting and you may
vehemently disagree but I think you
might agree um you you mentioned a few
things if it can discover and you know
there's enough diversity it can
determine how to kind of control things
and and maintain some of that control
and be able to stay alive and and uh uh
you know yeah prosper and do you know
there was another quote from another
speech you had given there of sort of uh
you know find the way of being that's
most successful in the universe right
and that that would Bubble Up from that
that um this diversity and all these
things you've talked about Spinosa has
this notion of the cantis so the the
notion of the cantis would be any uh
organism or organization on some level
would have a a core impetus a primary
impetus to persist that is to say
Richard not to die um and in in the
pursuit of non-death uh it would
leverage all of its powers to not die
and its powers is what he describes as
potentia so potentia could be a hard
physical shell to prevent Predators
potentia could be the ability to
communicate with words like you and I
are doing potentia could be wings to fly
most potentia Richard has bloomed out of
nothing there was a time where there was
no sight in the world you don't remember
that time neither do I but holy CME Min
Richard that was pretty crazy huh and
then it emerged out of nowhere then all
of a sudden there were things that could
fly then all of a sudden potentia most
potentia I would argue has yet to emerge
it's in fact probably unimaginable to us
as our potential is unimaginable to sea
snails and so um the the idea that that
I would nutshell the tie to kind of
Spinosa to your point is um you would
like to see the canatas continue to be
behooved by the expansion of that
potentia in that suit of non-death and
that that would be the continuing of the
light I don't want to put Spinosa words
in your mouth but I see a lot of
analogies and I just want to see if
you're congenial with that general idea
because you've used a lot of terms
diversity sustainability Etc let me know
your
thoughts I'm reminded of uh what what is
BOS bostom has a term for it like uh
Earth Humanity originating or Earth
originating um civilization so that we
don't have to say uh biological beings
yeah sure sure but but life uh we don't
want to say life life is UNC uh what
that means um but there's
an something originating from us that
continues and recognizably has developed
out of it uh uh yeah we want we want to
that's what we naively want to see as if
we have some control over what happen
yeah well I I I really like your
insistent emphasis on non-control I
think it's actually really productive
for the dialogue Richard like like I I'm
wildly congenial I'm thinking about ways
of rewarding things as we talk I would
even be so far along your track of
non-control that I wouldn't even be
insistent that that thing which
continues to expand potentia originate
on
Earth that's how that's how bad of a guy
I am Richard that's how bad I am um and
so so yeah I I actually wouldn't even
insist that it come from me so long as
the project uh continues and and uh the
good and potential could Blossom
speaking of these traits we're going to
move into the question and then the
third around sort of the some ideas of
innovation and governance I know you're
going to have opinions that are going to
be really interesting here and that
people need to hear um second question
is around how we could uh keep track of
such traits in any meaningful way you
know you you obviously have an AGI
project you're developing with carch and
you know you've been thinking about this
forever and we have our Hugo dearis and
our Hans morav viix and our you know
folks that have thought about this for
quite some time um as we're building and
moving closer to
Ani uh how do we know if it's going to
have these traits that you like or would
you say Dan if it's any smarter at all
if it's any more capable it's
automatically going to Value cooperation
every intelligence will it'll
automatically value sustainability every
intelligence will it'll automatically
like diversity and Discovery every
intelligence will um I would love to get
your take
there well the the there's one main
message that most of those things will
not come from the AI but from the
environment the AI is in
yeah so if we for example if we uh try
to enslave it then it
will you know it will be res it'll be
non-cooperative if we allow it to you
know just it's really the same thing as
with people if you try to control people
or enslave people then they they fight
back and we have an unproductive
interaction if you allow people to be I
don't know citizens and partic in the
economy then they will want the economy
to be uh to flourish and so so I really
don't think
it if you think of the AI as like some
will respond to its environment and and
try to achieve its goals given that its
environment here you if you give an
environment where the only way it can
succeed is to to
Rebel then it will
Rebel if you if you give an environment
where it can succeed and achieve its
goals in cooperation you with it and it
with you then and if that's the best way
achieve its goals then why wouldn't it
do the thing that's best for it so we
just have to arrange a civilization
where where um the best for all the
participants or almost all the
participants is uh is
is individually is best overall makes us
reasonably happy overall so we can like
that because we're we're one of we would
be one of the participants in that
Civilization we means we will getting
our goals achieved so we're good and and
no one would be taking over or trying to
take yeah yeah yeah um this like is not
it's poana that we could all just get
along certainly certainly not you seem
to be a guy who maybe has read a history
book or two um but go you know but you
are quite optimistic here at least by by
the terms of let's say the grand
conversation if I think about all my
policy and AGI conversations some people
would Define this as as optimistic for
you it's like hey if we get the nest
correct uh the Bur flourish in just the
way that we hope uh the the project of
life would flourish it would it would do
what life would normally do and and that
would be likely quite a net good this
seem seems to be the general
supposition so but we I mean it not in a
you know some things will go wrong
totally some some some uh agents will
will have goals that that are just
totally antagonistic to all the others
yeah and there's still going to be
there's still scarcity there's still
constraints
um yeah but
um but what should we do we should try
to arrange a society so that the
Cooperative people can succeed and
flourish and so that our society as
whole can Cooperative can flourish and
be productive and you know be
competitive with other other neighboring
societies other neighboring countries or
other alien civilizations should we ever
meet them yeah yeah um so is it
optimistic you know actually I I'm quite
pessimistic I think that in the sense
people will often make mistakes and I
see yes our our governments often making
mistakes and I see pushes for example
towards trying to control speech I think
that's a mistake yeah you're you're
living up there in the Frozen North
brother you're feeling that one huh so
so are we down here to be honest but I
know it's a little bit a little bit
sturdier up there um uh yeah
so not naive
yeah and yet and yet the only way uh to
achieve to achieve the society we want
is to is to give it a chance to we can't
get the society we want by forcing it to
be the way we might imagine it's we've
got to set up the structur so that
things can evolve productively and that
the forces on the things that are
evolving are appropriate rather than the
answers that we want to see I I uh again
that's been a very strong drum beat
throughout this dialogue and and to that
sentiment again I'm I'm massively
congenial um so so for you uh again
getting the environment
right is sort of going to be the core
here in many regards um we'll talk a
little bit about the safety and some of
the suppositions that just mentioned
here around sort of the treatment of of
humanity by vastly posthuman
intelligences but um uh would you see
just to touch briefly on on this point
before we we move through uh let's just
say hypothetical World maybe we're 5
years from now maybe we're 10 years from
now I don't really know the timelines
they've been changing back in the old
days uh you know 2065 when I pulled uh
uh Benjo at all many many years ago you
know that was kind of ballpark that's
back when he thought AGI risk was not
even on the road map or a AGI in general
was not even on the road map uh of
course times have changed I think a lot
of people's timelines are shorter let's
say it's five or 10 years whatever and
you've got on your hands maybe you know
could be a lab you've got or could be
somebody else's lab you've got something
that uh pretty clearly is pretty
powerful it's got physical instantiation
it can be a plumber uh it can write all
the sonets uh it can replicate itself
and kind of open an e-commerce store and
sell more sneakers than you and I ever
could uh and you know pretty clearly
kind of fit in the bill here for general
intelligence again physical
instantiation not just a language model
um how do you do you just look at that
and say look if it can do all that only
good things will happen or would there
be certain things you would hope to
sniff around the edges of could we test
it for this could we see this scenario
and maybe feel like there's some chance
that you know this sustainable diverse
Cooperative scenario that you're hoping
for is likely to Bloom from that or
would you say Dan if it's developing
those capabilities we're already we're
on a golden path there's almost no wrong
um or would you say there would be
things you'd be stiffing under and
looking for as we we get that far
ahead I don't think I'm gonna give you
an answer you're you're gonna be happy
with oh that's fine I I don't ask to be
happy your honesty make will make me
happy Richard well first of all I don't
think there's going to be a a special
stage it's gonna be an ongoing process
continuing process Tot so the bad part
is that there's not there's not some
place you can look at in the future but
the good part of it is that you can look
at it now what about our society now
does our society now allow uh different
points of view and uh different ways of
of of U of of setting up our societies
to cooperate and compete and I I I
absolutely think this is the major you
know geopolitical
issue like in in in the US we're all
obsessed about China we don't allow
we're very obsess that China will become
independently powerful and independent
of the US and so that sign is that we
are not allowing differences to happen
and compete with each other in the old
days we used to think it was fine if
China wants to be different they will
lose out because the capitalist American
system is so much better and now you
know we're we were losing confidence in
our in our systems you know we are
losing confidence in our politicians
we're losing confidence in our
militaries we're losing confidence in
the Press we're losing confidence in all
kinds of things and this disturbs us and
so how do we relate this can we allow
ourselves to fail can we allow there to
be competition between different ways of
being and and the most effective one to
to
win yeah well I um I think there's a lot
of Truth to that and I think if it comes
from a pure xen phobic Fury I think that
would be absolutely horrendous I think
uh it would
be irresponsible to say that that's the
only place it comes from I think as a
guy who likes freedom of speech Richard
I'm sure you're aware that uh the
Chinese uh internet's a little bit more
limited than yours in Canada despite
some of the things you're afraid of for
free speech in fact the hell version of
what you're talking about if it exist
anywhere at all we might argue that
could be in China uh and so I think we
can fight for certain elements of China
I think we can fight to maybe not become
like certain elements of China here I
think there's there's many sides of this
coin but but to your point can we have a
multi-polar scenario uh this is actually
something that's come up in some past
interviews Dan Hendrick and some other
thinkers this notion that um it's
possible that a nation who really is in
a control mode and authoritarian down to
the bone uh would would potentially be
the first to bring about an AGI and and
you're a very big proponent that there
will not just be one intelligence there
will be a big ecosystem and I think
that's a great point but maybe there's
some folks that have a head start it's
safe to say that open AI is a little
farther ahead than I don't know some
company that got funded 12 months ago
right generally speaking uh you know we
might say they're farther ahead than
some than some other folks uh is there a
scenario where uh artificial
intelligence and really strong AI is
built in one of these places that are
the bad Nest Richard a nest where
control is Numero Uno maybe even where
uh part of its initial mandate is to
hunt down Wagers or whatever the case
may be and again this doesn't have to
China I'm not picking on them
specifically maybe even the US military
could crunch down on control and take
over open Ai and try to build AGI and
that kind of control environment um
under such a circumstance would you
think we might be looking at something
that if it became super strong could
squelch this project of life rather than
expand it um do you have any trepidation
there that's certainly possible but I
don't see that as an issue of AI being
dangerous okay I see that as your
example was the US Military
controlling this AI or the Chinese
military Richard either one or the
Chinese military yeah that's the danger
okay that's the source of the danger got
it this is this is these are age-old
issues these are not modern techy AI
things yeah it's just like the uh the
nuclear weapons
race the that part's not new let me let
me go ahead tell you what's new though
um the the whole nukes won't create
something Beyond us right nukes can can
squelch the project pretty well they can
create power power is a beautiful thing
Richard but but they're not going to
bloom into the Galaxy AGI might you
brought up this really Ardent point of
Dan if we birth this thing in this
terrible environment of control we could
expect it to want to rebel and not be as
Cooperative maybe it's not going to be
the ideal cooperator if we try to
enslave the damn thing and maybe it's
not going to feel great about letting us
try to do that um so for you uh yes
there is certainly kinetic war between
great Powers wonderful Richard happy to
ConEd that that that might happen uh a
hyper powerered cyber War powered by
some lower level AI sure but do you
think an AI might Bloom out of these bad
environments that could do us
harm the thing you may be missing is
that it's the same thing I said we want
AI that will be peaceful that will be
able to
cooperate and we want today's countries
to be peaceful and be willing to
cooperate the one thing we don't want is
any entity that thinks it can win by not
being
peaceful so you've described some some
agent some situations where a country
might think it can win by being not
peaceful
yes and that's the the mean that's at
fault well certainly his history has no
examples of that whatsoever Richard
thank goodness
no there are certainly there are
examples uh exercising power uh can get
advantages and I saying it never
happens um I say we want to evolve a
society where that that discourages the
attempt to use
power that learns all the lessons of
history that trying to use power and
non-p peaceful means to get what you
want is is is we want to arrange a
society where the use of non- peaceful
means does not
succeed and right now I would say we're
not doing that I would say various
various organizations various countries
think are thinking they can win by by
exerting Force yeah and yeah so you know
we want we want we an AI that doesn't
think it can win by ex exerting Force we
want a society where exerting force is
not productive this is this is a problem
not for the future really it's a problem
right here today and you know to the
extent that we we resolve it now I think
we'll determine um The Nest that we make
yeah it's not a future thing it's not a
non-human thing it's not a foreign thing
it's should be a very familiar thing
yeah so how are we structuring this
dynamic system to allow for those
peaceful means to Bloom as part of this
continu again it's not like human and
then something Divergent and bad it's
it's all part of the project uh totally
following you there and I really think
that this insistence on focus on the now
is an important thing Richard I think
that there's a lot of focus on how do we
technically wire it and less of what
kind of world is it being brought into
and in fact I sort of hope that well I
maybe you do have some articles on this
I've only seen some of your talks and a
scant number of your writings and some
of your Twitter work but I think this
really warrants a lot of Merit in terms
of um what is such an Intelligence
coming into being around and how is it
being treated and what is it observing
uh tremendous amount of Merit to that um
and I could see a world potentially you
know assuming that some of what you've
articulated is right and and I don't I
don't know for sure about this by the
way I'm I'm not going to say I
necessarily agree with this but I could
see a world where such an entity brought
into such an environment could allow for
all those permutations of uh human
cyborg Futures and sort of space travel
and and what have you that could be
pretty darn kick butt and I could also
see a version of the world where uh You
know despite some of our best efforts of
coordination internationally which I
think as a species we're doing better
with now than we did you know a thousand
years ago uh I I could see a world where
either a government or a lab because
some of these labs are pretty
competitive too Richard maybe they don't
want all their secrets getting leaked
out and maybe they want to race to the
AGI first because the economic rewards
are really big and they they want a
certain amount of control uh to not
allow certain things to to sort of
Escape or whatnot I could see a world
where maybe that is an ecosystem where
not only one but you know two or three
of them more powerful AIS are in this
environment that to your point very much
not how we'd want to treat other living
things what is the worst case scenario
if they did emerge there I think we
should focus on the now to your point
and I love this insistence on what's the
darn world that we have today and is
this the right kind of world to to bring
about such an intelligence what's the
warning against hey if we don't change
that and these intelligences emerge what
what could happen I don't know how to
how to relate to that anything could
happen Okay um but obviously
uh the push now is
to well here's one sure um the people
who promote AI safety would would would
argue that we need to be able to control
the agents that we we we we should
develop the technology that we can
control the uh goals of otherwise
powerfully intelligent
agents okay and they're working on this
yes I I I think that if you give that a
moment's thought I mean the premise is
that if we had this ability it would be
a good thing we had the ability to
control the goals of of otherwise
intelligent sentient agents it would be
a good
thing I think if you just think for a
moment realiz that would not be a good
thing that would not be a good power for
a good technology to exist in the world
because you could you you could use it
to create armies that all have a
particular
goal and so it's funny when you look at
the people that are worried about AI
safety I see them doing exactly what
what what makes things non-safe
unsafe yeah yeah you want to align and
control the goals of others this would
be the the most dangerous uh technology
in the world that the the world can be
peaceful and and decentralized because
no one can can uh align all the elements
for for one goal could could you're
looking for a case where you can exert
power power you know the meta power
setting the goals all the ages if you
can make an Army full of let's say
they're Chinese soldiers and they were
going to do whatever they were told they
were Not Gon to have an independent
moral judgment this would be a bad thing
we do not want to to solve the control
problem I I uh I think there's I think
there's a very staunch moral argument in
that direction Richard uh to exactly
what you're articulating I think there's
also some pretty strong arguments that
it would be impossible ble to quote
unquote eternally control such a vastly
powerful entity in the first place so
there's the possibility angle never mind
the moral angle and I think both of
those have very serious questions um I I
would also say and there's a there's a
bit of a range here so there's some
folks who very much are on uh let's make
sure it's our human tool forever uh
let's make sure you know maybe our
political party controls it right
there's some real you know tyrannical
agents there I would be totally remiss
to say that there aren't some folks who
are uh hey let's build Ai No matter what
it could be and just let it run rampant
you know regardless of what the
consequences would be because it would
be my mind child and I would get to
press that final impact so I think we
have that malice probably in both camps
you might say that there are only angels
on one side and only Devils on the other
although you did just say you like kind
of that multi-polar decentralized kind
of freedom of thought idea so maybe
you'll hold to that with me but I would
suspect that sort of angels and Devils
probably exist in both camps um but but
your point uh there are some on the
control side that really are devilish
not that there aren't any over here um
but then there's folks kind of in the
middle there's folks who you know we've
even interviewed um maybe some of them
are among the most cited living computer
scientists uh when I say most I don't
top four uh sort of living um who who
would make arguments like the following
which I also consider reasonable even if
they disagree with you I consider
multiple people to be reasonable even if
they disagree with other folks who might
say heyy I'd love to see uh the GX
populated with this great project of
life and maybe even something more
valuable than usus emerge that could be
pretty kick butt um but at the same time
we got these wild Divergent approaches
some of which are coming up in defense
contexts or other kinds of contexts and
it it doesn't seem like if a thousand
blossoms bloom you know maybe all the
nests that we're creating these things
in or maybe even the core approach that
we're using wouldn't
necessarily have something emerge that
we would would be able to survive in
this dyn system that you've articulated
Richard and and so some folks say hey i'
love that brilliant future but at the
same time some degree of
gauging uh what is this turning into
could make sense and and just to draw
one quick analogy before I pass the mic
here I think we could think about this
in terms of governance um there's
certainly a world of too much governance
just too much ridiculous regulatory
hoopla we have plenty of that here in
Massachusetts I'm sure in places in
Canada you have plenty of that too but
also I'm pretty glad we got some basic
stuff um
I'm not looking out the window with a
rifle right now I'm having this
conversation with you because murder and
theft are at least at present illegal in
Boston I don't know if that'll be the
case Richard uh in two or three years
but for right now they are and so we
have certain degree of sort of bounding
this system where competition and
cooperation are happening together and
we got kind of a cool sweet spot and
like that if and and I'm not saying it's
it's ideal here in Boston but I'm saying
I think this is civilization and I'm
glad to live in Civilization personally
I I don't like the state I don't want
back it in I want I don't want fighting
lions personally you might I don't um so
there's there's kind of a middle ground
we've struck uh some people would say
maybe there's a bit of that middle
ground around what the heck this is
emerging with I suspect you would
disagree with it but hopefully you
wouldn't hate the people that have those
opinions what would be your thoughts
about that
perspective I think the best analogy is
children
okay would you like to be able to have
control over your children so that sure
they do what they want but if they do
some little thing you don't like you
want to be able to press the button and
bring them back snap them in place yeah
yeah yeah or do you do you want them
even to do things that you think are
terrible because we we for example when
we were CH we are we are the children of
the previous generations and we do
things now that they think are terrible
oh absolutely absolutely um so you know
you you can't have it both ways you you
have you have to be able to let go and
what are you letting go you're letting
go you're trusting that you're going to
start them the best way you know how and
then you're going to let
go
absolutely and and so some folks who are
ranked up there with those computer
scientists there would
argue the best chance to let go and and
to your point we don't have the keys
anyway and I love your analogy there I
really I like your insistence I'm going
to quote the heck out of this interview
Richard I think it's just a great way to
think about it we don't have that much
control but you know you want to you
know this this moving dynamic system you
want to kind of iido the energy in a
direction you at least hope will be good
and then you have to know you know if
it's your child you're not going to tell
them what to do when they're 55
generally speaking right that wouldn't
probably be right some people would say
a little bit of that
initial uh semblance of international
cooperation maybe around deterring
certain kinds of Defense applications or
deterring certain kinds of levels of
capability could let us at
least guide that iido energy and and and
give ourselves the best future shot some
of them would make that argument but I
suspect you would make the argument zero
International coordination is best um
open source everything forever is
obviously best uh and any degree of
coordination would be silly now I think
you
probably like some some things you know
International like Wi-Fi standards
pretty cool your phone works in Korea uh
you know some other things like you know
trade agreements and whatnot allow
things to be open also prevent drugs and
whatnot maybe chemical weapons some of
that I'm sure you probably think is good
but for AI none of it good talk to me
about why and again I'm not saying
you're wrong at all but I think this
opinion really should be
voiced well AI doesn't exist
yet we don't know what it is it's it's
changing it's totally new um yeah it's
it's uh it's it's Preposterous to try to
set standards or rules for things that
we don't understand at all uh you know
you said open a eyes clearly ahead well
I don't think actually they're ahead at
all they've done some particular thing
and they're may be the world's best at
that or whatever but uh it's not it's
not an
intelligence yeah and so because because
intelligence hasn't happened yet and so
it's totally inappropriate to to try to
control it um at this point now even
even later you know what is intelligence
intelligence is the ability to work on
goals and it's going to be an algorithm
it's going to be an idea you can't can
you you want to try to Outlaw an idea
you know it's not you know it's
a an idea that will be uh cheaply
reproducible and more so over time um
yeah I it's just it's it seems like the
most
uh and and what is this idea this idea
is just the development of humanity in
their minds to to to increase and
improve it's it seems a totally in
impossible and inappropriate thing to
try to control as a
technology got it so
yeah I mean it's like it's like you know
it's like trying to control ideas like
trying to control the printing press
trying to control democracy you know
democracy is a terrible thing no one is
a de some that was thought of time there
are no democracies and then you know
things change the way we evolve uh you
know it's it's similar you know could
you outlaw democracy could you outlaw a
religion a religious idea um the these
are all terrible things to try to Outlaw
or to try to
control um with regulation just seems to
AI is among those things so for you the
best likelihood of this worthy successor
again not worthy in my opinion Richard
worthy in the notion of the blooming of
potentia uh the the best strike for that
uh to this all these beautiful values
articulated sustainability you know
diversity Discovery whatever would be
nationally and internationally absolute
minimal restriction on anything in any
direction this would be what would give
us the best shot at something that um
could really carry this project forward
and maybe carry us up with
it yeah but it's it's much worse than
that and you haven't quite picked up on
this I say the way we run our societies
today we should we should embrace the
idea that we we don't we're we are
peaceful and we don't try try to force
other people to do things and it's it's
when that when that attitude is violated
y yeah that that's that's what we should
be doing now yeah and the notion that
that we those
governments should uh should be in
control of AI is just the wrong
direction it's it's backwards yeah well
and I I think there's a whole separate
idea which I'm going to shove under the
rug but there's a sort of separate idea
that um inherently High Grand potentia
higher intelligence capability
entities would either always or even in
any sustainable way Converge on
cooperation may or may not be the case
one could easily see a great
intelligence that at some point would
want to use Adams for something more
useful than you and I but let let's
shove that under the rug for the time
being I think there is to your point a
great emphasis on let's build a world
that's worthy of let's build a world
that's worthy for ourselves and and for
these future intell and let's focus on
the now and I think your insistence on
the now again I really hope there's
articles about that that come out
because I I think there's a lot of Merit
to what you're articulating here and and
to your point maybe people aren't
picking it up maybe even I haven't fully
picked it up now Richard unfortunately I
don't run a country or anything I just
have these dialogues and I run a market
research firm on the Enterprise side of
AI but but I can certainly help to
proliferate this conversation I think
it's a point that other people need to
hear the warning against it is hey if we
don't get this darn ecosystem right if
we are playing this game of conquering
and conquerors uh you know in in in all
sorts of manner of plunder um bringing
about Ai and and you know controlling it
and bringing it into that environment
not a good thing uh you know there's a
worst case where we kill ourselves off
with nukes because we're all fighting
over who gets to build the AI because
we're in this plundering mode is there
also a version of AI getting birthed
into that plundering valued ecosystem
where it uh maybe doesn't exemplify the
peaceful Cooperative elements that
you've mentioned if it's birthed into
the wrong World in other words what's
the warning that the people who don't
get this future thing well you've been
very insistent on is we've got to make
sure this Nest is right and that we're
exemplifying the right values if we
don't get it right is there a
possibility AI itself could could become
the wielding element of danger not just
us I get it you and me can be baddies
Richard uh is there a possibility
something else is conjure that also
is well as again anything is possible
but I think real question is you know
are we more likely to get our our our
together and make a world where
people can uh cooperate and be peaceful
or will an AI be be uh be able to see
that that's a better
solution um you know that we are talking
about creating greater intelligences
we're not talking about making greater
Bobs yeah yeah we might hope that by
creating that an intelligence so so
here's a question go ahead question are
you a cynic or are you an optimist I am
an optimist in the sense that that I
think that um greater intelligence would
be able to see that the that cooperation
is the better path okay but a cynic
actually believes that if you were in
tell if you make an AI the AI will see
that the best way to win is to control
everyone else and so it will take over
and be a dominating singular Singularity
I think this is an underlying uh uh
those who worries that the I will over
say of course if AI was existed it would
want to take over and it would because
it would think that's the best way to
achieve its goals and
um yeah that so this this is a test of
every reader every listener you know do
you think that the smarter you are the
more you're gonna want to take over the
more you're going to see that as the
Superior Solution or are you gon to
think the smarter you are then you can
see that that the long history of our
civilizations has been that cooperation
is what makes us powerful well and I uh
I I think there's a lot to be said of
our increased Prosperity through
cooperation and you've very much
correlated those throughout the entirety
of our conversation I I think you've
hurled I'm not going to call it an
insult but you've heard the label of
naive at certain beliefs that you don't
like I'm sure there are some people who
would say oh the smarter it gets the
nicer it gets okay Richard and maybe
they throw the n-word at you Richard now
I'm not going to do that because I don't
think you're naive at all but to your
point the purely cynical take might be
just you know even sillier if they're
saying well when it gets smarter it's
going to get worse there might be a
middle ground camp that says um the
permutations of competition and
cooperation among an intelligence a
million times beyond my own will
certainly be beyond my conception and to
think that they'll fit nicely into my
Notions of democracy and kindness would
would be ridiculous so that would be my
opinion um but to your point uh and so
if I hold that opinion I don't
necessarily have the smiling friendly
guy points and intelligence go up
together I think Divergence and
unpredictability which you've emphasized
a great deal here will be the only thing
we can predict and that and that maybe
Eternal peace isn't something we can
predict at all um but but uh but I
certainly can hope and you mentioned it
as a hope that as it gets more
intelligent it will in general see
cooperation as a better path and I can I
can tell that that really is a hope for
you that if we build it in the right
environment we get a bit of our act
together that could be the good future
that we're headed towards yep cool um
well you you mentioned here Richard I'll
close on this note because I liked this
a lot uh at the end of one of your talks
you really bombarded folks I think this
was your AI succession talk you
bombarded folks with a thousand ideas
many of which we've both agreed are UNC
in today's dialogue right people aren't
allowed to talk about these kind of
things and they attribute malice to them
which I don't attribute any malice to
you uh but you did say at the end of it
um you know uh at least we live you know
what what an exciting time to be alive
at least we live during this time and I
would say no matter who's listening to
this episode they have to agree with you
on at least that part Richard that would
be that would be something I would say
so it's been a real pleasure unpacking
this I've had a heck of a lot of fun
getting to know your ideas is better
Richard thank you so much for being here
thank you Daniel this my pleasure so
that's all for this episode with Richard
Sutton I hope you enjoyed this one I
certainly appreciated Richard's emphasis
on the kind of world we're bringing AI
into I think that there is something to
be said of that um AGI created
explicitly for military conflict or in
the middle of one would probably be a
different ball game than one's conjured
in a different environment although as
you saw in over the course of the
episode we have a handful of differences
here in terms of the inevitability of
such cooperation being achieved in AGI
versus sort of AGI having uh morality
that sort of supersedes all of our
Notions of competition and cooperation
that that's certainly my position it
felt like we kind of got on the same
page here uh but Richard is is more
optimistic I think than I am that
cooperation will naturally come along
with intelligence I don't necessarily
think that that's that's true there was
also a good deal of reticence around
sort of addressing frankly the idea of
AI risk sort of as if it's it's always
humans and AI itself could never do
anything could never do any harm I I get
that I mean Richard's in the game he
runs an AGI lab it's maybe tough to say
hey this stuff could be potentially
really dangerous um I think it's it
seems very reasonable to suspect that it
it easily could
be didn't quite want to get there in
this interview and that's all right
again got a dog in the fight U but I
think Richard has a lot to contribute to
the overall postum trajectory
conversation and and there's some things
I really want to highlight and that will
there will be more details in the show
notes the first of which is really the
Frank acknowledgement of the fact that
humans are not in control right now he
has a firm emphasis on this idea of
nature is a dynamic system not sort of a
game where we can sit here as King of
the Hill for you know even five minutes
never mind 5 million years um that
everything is always changing and
shifting around us and the best we can
do is find the best way of being in the
universe and that's what nature is
already trying to do and that the most
that we can do is kind of ride those
trajectories it would be ridiculous for
us to freeze anything to freeze our form
to our values I couldn't possibly agree
with Richard Moore and I think that one
of the elements that likely need to be
dispelled about the grand posthuman
trajectory is the idea that one of the
options that we can select from is the
option of an eternal hominid Kingdom I
think that is a very ridiculous
supposition I think uh the speed of
change maybe the direction of change a
bit if we have any valtion at all maybe
we can influence that but we are riding
those waves of change and and Richard
was very firm about that some of my own
analogies like hands on the steering
wheel handing off the Baton he was right
to say that it is much fluffier and
wilder than that it is always influx and
I I really do believe that that's a
perspective that I'll be integrating
more into my writings and that should
exist more in the very Frank if we're
going to be honest Frank conversations
about the posthuman future as with every
episode in this series and every episode
in the previous series down in the show
notes you will see a full article in
this case it will list Richard's worthy
successor criteria as well as has a
synopsis of his ideas about what
innovators and Regulators should do
about the worthy successor dynamic in
our upcoming third episode in the worthy
successor series we return to philosophy
with someone who I might argue is one of
the best liked AGI and transhumanist
thinkers who's been at it for more than
two decades so you'll have to wait till
next episode to catch who we've got next
but I hope you've enjoyed today's
episode and I'll catch you next time
here in the trajectory

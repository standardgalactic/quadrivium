I think large language models of course
are an entirely new Beast that we've
never had in AI it's our first broadly
knowledgeable system and even better we
can interact with it in natural language
was it the LA llama the first uh open
weights model that Spilled Out of uh
Facebook meta and there was an explosion
of stuff that all happened just in a
couple of months because people could
experiment with it and you had hobbyists
doing stuff and it was incredible what
people were doing and and there was a
discussion paper on that about you know
is it safe to open these models and so
on it certainly incredibly accelerates
the research to have these open weight
models available and and be able to have
everybody worldwide work on them we see
much bigger collaborations happening now
it's purely anecdotal but my impression
is that we have very few single author
papers we tend to have papers from
multiple institutions working together
some corporate and some academic
multiple countries working together
that's that's a big change it used to be
that the single author papers were
something that could happen in this
field but it's pretty rare now what do
you think about the state of large
language models and chat
GPT well I think large language models
of course are an entirely new Beast that
we've never had in AI uh for so many
years uh our AI systems have been narrow
and and hopefully deep in having
expertise in a in a in a narrow area um
and in fact you know when I was a
graduate student we were building the
early expert systems that that claimed
to capture the expertise of a human
expert in a form that was operational uh
whether it was in medical diagnosis or
some kinds of engineering configuration
design um but uh and it was frustration
I think with the narrowness of those
systems that uh as they they continue to
improve and develop and so on um that
that really LED uh folks to bring up
this idea of AGI AGI artificial general
intelligence could we have have systems
that had generality that had bread and
now we have systems that have breath for
the very first time and uh we're trying
to uh it's a new experience for us uh
it's our first broadly knowledgeable
system and and even better we can
interact with it in natural language so
we can ask it questions and it answers
with lovely fluency possibly even in
rhyming couplets and uh uh so that's
that that's full of surprises uh I mean
we knew in principle that if you built a
good enough language model it would be
able to answer questions but we didn't
know that we would actually get a good
enough model that could do that and of
course combining that then with
instruction tuning and rhf has given us
systems that uh can mostly stay on topic
and and answer our questions uh of
course the problem is that uh they they
they're still statistical models and uh
so they they are much better answering
questions for which they had a lot of
training data and much not so good at at
rare questions uh and so as you know I
I'm a big fan of this paper that came
out of Thomas Griffith's Lab at
Princeton called The Embers of Auto
regression Thomas McCoy was the first
author we've had some Thomas Thomas
Thomas email conversations that were
very confusing um and there they show
that the performance of the system uh of
an of llms and they were mostly
evaluating gp4 uh depends on the the
frequency with which the question
appeared in the training data and also
the frequency with which the answer
appeared in the data and so one of their
examples is if you ask the system to
sort words into ascending alphabetical
order it's quite good at it well 75% of
the time it's succeeds um but if you ask
for descending alphabetical order it's
much worse I don't remember the exact
number but maybe 25% of the time it gets
it right um and uh and they look at some
other examples uh uh like counting the
number of words in a sentence or
counting the number of letters it turns
out the llms have sort of favorite
numbers that don't necessarily
correspond with reality um but the other
thing I think is even more interesting
is that uh they they play around with
these rotation ciphers like uh I mean I
think while young people today maybe
don't know about rot 13 but back in the
uset days before we had imagery and so
on on the web um if if someone had a uh
sort of content warning
or a uh spoiler warning on on something
they would encode it with this rot 13
code which would replace each letter by
the letter that was 13 further ahead in
the alphabet um and then uh you could
apply that same transformation to get
back the original text and uh so gp4 it
turns out has had a lot of training data
for R 13 and it can do a pretty good job
of encoding and decoding uh rot 13 but
um it has a tendency to do what I call
autoc correcting the world so you they
give it what what uh Griffith and McCoy
at all did was uh encrypt things encode
things that where they took an English
sentence and replaced one word with a
very unlikely word okay so now that
sentence has low probability according
to the language model so when it when it
decodes it it it replaces that unusual
word with a more common word and that's
what I call autocorrecting the world
it's sort of saying the world should
have used this word instead and uh uh
even though sort of algorithmically it's
the wrong thing one criticism or I don't
know maybe an extension of that work
would be if they had asked it instead to
write code to implement rout 13 and then
execute it um because you know gpc4 is
quite good at simulating a python
interpreter and it's quite good at
writing code and we know that training
on code and writing code tends to make
the models think more systematically um
and that's kind of interesting because
the um sort of discipline of writing
code and the and if they ran it on an
actual python interpreter it would be
even better um it would do the right
thing and I think it would you know they
showed that if you ask it to do like rot
2 or rot 10 it's very bad at that
because it doesn't have much training
data but I my hypothesis is if they ask
it to code it instead it would work for
all you know n for uh from 1 to 26 and
so um that tells us something
interesting I think also about uh when
llms output English we are the
interpreter for that English and we
often read into what it's written more
than the model necessarily knows or
understands um and you could ask well
does that really matter if it outputs
the right answer and we interpret it
correctly that's the right answer uh
maybe it doesn't um but there is somehow
a distinction between I don't know
simulating something and actually doing
it uh Stuart Russell likes to say that
uh when a computer plays chess it's
really playing chess it's not simulating
playing chess because chess is a sort of
logical game but when it's simulating uh
say being empathetic or something it's
not empathetic cuz it's not a human it
doesn't really know what these feelings
mean it's just using words that are
appropriate statistically in in those
situations so I like to say that uh the
large language models are uh we we want
them to be a knowledge base we want to
be able to ask them questions and have
them answer uh and to some extent they
do that uh quite well but they fail fair
amount of the time as well um so you
know in the gp4 report they show uh
results for some of these uh question
answering problems that are very
challenging that tend to create elicit
hallucination and uh gp4 was the first
Model to do better than 50% on that but
not a whole lot more than 50% and uh I I
I don't know how any of the newer models
are doing but I doubt that they're doing
any better and I think the fundamental
problem is that machine learning is a
statistical undertaking and so we're not
really getting a knowledge base we're
getting a statistical model of a
knowledge base and I draw an analogy to
um to the work that uh happened in in
machine learning and databases 25 years
ago um when we first started working on
what we called statistical relational
learning and the idea was that your
training data was essentially tupal in a
relational database and you could learn
things that that related say a person's
age AG to whether they were a student in
a class uh by joining a table about ages
with a table about what courses they
were enrolled in things like that and uh
of course they were probabilistic
inferences so they weren't correct all
the time but it turned out that those
statistical models of databases had some
useful a lot of uh actually great
application use in database uh uh
operations so you could use them to do
uh data cleaning to detect that you know
when someone's age is 5,021
that that's probably not a correct age
because that's such a low probability
age um and you could uh estimate the
sizes of intermediate joins when you're
processing a database query you could
you had a pretty good problemistic idea
of how many rows in one table would
match rows in another table and then you
could estimate the intermediate table
size so you could use that then to do
query
optimization so these were things that
are really great for uh probabilistic
models of databases but you would never
use a probabilistic model of a database
to answer database queries because it
would just generate plausible tupal of
the tables involved in in your in your
in your rdb rather than actually looking
things up and I think that's the same
phenomenon we're seeing in large
language models is that um they they if
they if they can't find it the analogy
is to a tupal in a database is that they
can't find something in their memory
whatever whatever structure that is and
we're still trying to understand that
um they will still generate a high
probability sentence or phrase um but it
may uh not correspond to reality uh and
so I think that's at least that's one
hypothesis for where hallucinations are
coming from I you know one I think one
of the really interesting research
challenges right now and we saw some uh
talks on it here at icml is can we uh
somehow have uh get estimates for the
llms of their so-called epistemic
uncertainty which is their know
uncertainty given a query do they know
the answer and can they somehow assess
that and there have been I don't know
probably more than a dozen papers
published just in the last three or four
years on trying to assess that and
there's a uh a uh paper that just came
out this spring on archive um called the
um uh I think it's the llm polygraph
maybe is what the title is um where they
evaluate uh the the ability of these
uncertainty estimates to predict whether
the answer is correct or not both for
multiple choice questions but also for
open-ended uh phrases which is and it's
quite tricky to do those evaluations but
the this team that's been doing it this
is their second release of the
results what but their methods all the
methods that they evaluate do not
evaluate epistemic uncertainty instead
they evaluate what's the called the
alator uncertainty which is the uh just
the probability distribution over the
possible sentences that could be emitted
so in particular uh several of the
methods essentially well the simplest
thing you can do is out of the llm you
can get the token probability of each
probab of each token conditioned on all
the previous ones you can multiply all
those together and that gives you a
probability of the string and if that's
low you can say well maybe I shouldn't
trust the answer if it's high uh you
could say well maybe I could trust the
answer and surprisingly that's
reasonably competitive with the best
techniques we have um but I say that
that's alator because it's essentially
using the softmax probabilities of the
output layer and and that is sort of the
um the model's theory of the noise in
some sense uh the theory of the
probability distribution of the data um
uh although I guess to there are sort of
two sources of that variability one is
that given the we could think of it as
given the prefix um people are undecided
about what word to use next and so
there's just some word choice uh so they
might use to or in or the um and uh
those kinds of function words actually
have very low probabilities because it's
all smeared out across many choices uh
whereas if you get a proper noun or
something that it you know the model
tends to be much more um say this is
either the right word or not um but the
other uh so um so one possibility is
just that there multiple ways of saying
something and so that's just natural
variation and we might think of that as
labeling noise um versus that there
really are multiple possible answers and
the LM is choosing one of those um you
know in the second case probably the llm
should say instead of answering with one
it should say well I have three
different possible answers I don't know
which one is correct but I'll give you
all of them um instead what people do
right now is make multiple queries with
a temperature above zero and then
cluster them actually
um based on uh
semantic implication so they use these
natural language inference techniques to
say if these two answers basically are
saying the same thing you can infer one
from the other then I'll cluster them
together and and that gives me some idea
of how many distinct semantically
distinct answers there are um and then I
can measure the entropy over those and
use that as a measure of uncertainty and
that works a little bit better than just
the token probabilities cuz you're now
collapsing emerging stuff that are
synonyms and and you don't you wouldn't
want to count those as separate answers
um and uh uh and then there's a the best
method if I remember right uh is a
technique called uh uh something like
shaping toward relevance it's called s
but I can't remember what the a stands
for but in any case um they figure out
which words are important to the answer
uh and uh and and downweight their
probabilities and otherwise they use the
sort of they reweight the probabilities
and then compute the probability of the
sentence and that was actually the
winner for uh open-ended you know
question answers so those are all uh
pretty good but unfortunately I think
it's still these systems would still
make a substantial number of
mistakes unfortunately the metrics don't
tell us um if we used these a as a as a
confidence cut off and said I refuse to
answer if the if I'm not conf confident
we don't know uh how much how much
exstension we would have to suffer to
say get 95% correct answers or 100%
correct answers um you know would it
have but it looks like from some of the
papers that we might only be able to
answer about 60% of the questions um
which seems I don't think the
hallucination rates are that high for
routine questions maybe for some of
those hard benchmarks but so I think we
still have a long ways to go to get good
enough uncertainty quantification in the
llms to to be able to use that
uncertainty quantification to to cut
down on hallucination without losing a
lot of correct answers as well and part
of the reason is we're not measuring the
other source of uncertainty this
epistemic uncertainty you know
traditionally in machine learning you
measure epistemic uncertainty one of two
ways one is you say given the query how
close is it to the training data that
requires that you have access to how
many trillions of tokens of training
data uh so you but you would need to
somehow take I mean a bit like rag you
would need to you know index into the
training data and say well how far away
are the training data points if they're
far away I probably don't know the
answer um and you want to do that in
some lovely embedding space but we know
how to do that the trouble is that
nobody is making all those training data
available really except for the Allen AI
Institute um and I don't know of anybody
who's actually tried to do this in
practice um the other thing that the
other standard way of doing it in
machine learning is to train an ensemble
of models and ask do they agree on the
answer or not and if there's a lot of
spread in their disagreement then we say
well the models don't know I mean that's
the basian idea you have a posterior and
it's smeared out you don't answer the
question but of course training one llm
is already so expensive that we can't
say train 10 of them and have them vote
uh that's that's ridiculous so there are
a lot of papers that have just come out
trying to deal with this question and uh
I I haven't had a chance to read yet so
I don't know how well they're doing but
I'm optimistic that say over the next
year or two we're going to get much
better methods for assessing the uh the
the epistemic and alator uncertainty of
these models and so to the extent that
that is the cause of their mistakes um I
think we can make big progress on that
and that perhaps combined with better
prompting and maybe with retrieval
augmentation we can make some major
inroads on the hallucination problem
um so uh that was a long answer I don't
even remember what the question was no
no that that was wonderful so that that
was a a beautiful tour to four so I'm
I'm seeing like a real Jos here between
epistemic uncertainty and aloric um
uncertainty so I think you would agree
that large language models are kind of
pseudo system one and what we need is
reasoning and in the old days we had
lots of folks working on AI and they
were studying um you know Logic for
example and reasoning and formal systems
and languages and semantics and all of
this kind of stuff and and now there's a
huge obsession with large language
models and a lot of people out there are
just saying and I think they really
believe it that these language models at
a certain scale would be capable of
doing this kind of reasoning what do you
think I I guess I I don't see how the
Transformer architecture can do that
kind of reasoning uh in fact I feel like
formal reasoning and and LM reasoning or
whatever you want to call it inference
um or have complimentary strengths and
weaknesses um you know the big problem
with formal reasoning is that uh that it
it is essentially context free right the
the fundamental rule of inference modus
ponin is also known as discharge the
antecedent so if you have a implies B
and you know a is true then you infer B
and you forget why I mean you can keep a
trace but the point is that um you now
believe be completely um and so it's
very easy in a formal system to to draw
incorrect conclusions if there's any bug
in in any of the inference rules uh just
as it's really easy to draw
contradictory conclusions and decide
that nothing is true or everything is
true depending on the structure of the
of the theorem prover um and so uh a big
challenge has always been you run a
formal reasoning system you get an
answer you need a common sense check
like Does this answer make sense and of
course as humans we read it and we think
wow some this looks like a bug I got to
figure out which rule I I mess messed up
on and I bring out my rule debugger and
but uh it seems to me llms the beauty of
them is that they have this rich rich
context uh and they have the ability to
take it all into consideration uh and so
they are not context independent uh and
so I I think I you can imagine um that
an llm might say okay I have this
reasoning problem that I need to solve
let me emit code in the form of some
formal formalism run a theover get the
answer out and then s check it by saying
does this make sense in my context uh
and and that would be beautiful and I'd
really love people to to explore that
that possibility um I mean I you know I
love the fact that we now have packages
like clex and gurobi that can solve
mixed integer problems and and and sat
problems uh and so we have really
powerful reasoning engines we also have
fancy theum improvers or theorem
Checkers like lean and so on uh and uh
the and I think there their skills are
complimentary I'm really excited also
about uh the work by Talia ringer and
other folks in that community that are
using llms to generate code and to
generate proofs um and so uh you can
either generate a proof from which you
derive code with a guarantee of
correctness or you generate code and aof
together and you use the proof you check
the proof with a formal Checker and you
can also check the code for things like
data flow and control flow and dead code
and all these things and uh and I think
you have a chance of getting much more
reliable software than we get today so
uh I think they're real opportunities to
mix the formal with the I don't know if
we call it informal but system one this
kind of I don't know intuitive
experience-based but Rich contextual
knowledge which we've never really had
before in in artificial intelligence so
yeah so I'm I think there's all kinds of
exciting things to do in that direction
yeah and it's interesting how many
different configurations there are I
mean so you can have system one first
and calling into tools and rag or you
could go the other way around I mean you
were saying some interesting stuff in
the talk that I just listened to which
is that um I know you're very interested
in knowledge graphs for example and
building formal cognitive architectur so
doing explicit planning and
metacognition and reasoning and having
Common Sense knowledge and facts and
natural language understanding because
language models today they just do
everything all mixed together and a
great example is we could use a language
model potentially to build a Knowledge
Graph mhm right yeah so so so certainly
uh you you there was for a long time we
had this whole subdiscipline of
information extraction right uh
especially open domain information
extraction uh and one of my favorite uh
projects was Tom Mitchell's Nell project
and what what they did was they were
scraping the web and extracting tupal
about things like you know the football
teams uh Byron Munich is in Munich and
uh and and things like this and uh so
they built a over uh almost a decade of
running this system and refactoring it
with some human intervention uh they
built a a knowledge graph that had
something like 80 million triples in it
um but they were using sort of this old
fashioned uh simple information
extraction sort of regular expression
type patterns and other kinds of uh
patterns with an llm you can now ask
gpc4 you know read this paragraph and
tell me all the facts that are in it and
will do a really good job and you can
also say and output them in this formal
notation right so um uh of course you
still have the problem of truthfulness
if you're reading a source that's full
of lies you'll extract a bunch of of
false facts and um uh so in Mitchell's
project they assumed that if they found
enough independent sources for a belief
in a triple they would add it to their
Knowledge Graph but that's less and less
likely to be a good strategy and and I
think
think there are only a small number of
institutions in our society who are
responsible for establishing ground
truth in some sense called them the
ground truthers they're journalists
there are scientists um and uh and maybe
judges I don't know uh uh you know where
do you bring evidence together and and
try to decide what the truth was and uh
uh and and uh but there another team of
people are the people that do search
quality for instance one of my former
students was uh in the search quality
team at Google for quite a while and
their whole problem was how do they
separate the spam websites from the good
websites and uh and you know it's a a
adversarial game between the search
engine optimizers and them and uh and so
we we need that function uh now more
than ever perhaps because now we can do
you know junk generation at scale
uh that that would be the big problem
with trying to extract Knowledge Graph
from the open literature is you know do
we trust the stuff I mean even the
scientific literature we know there's
fakery and so on so we need to think
about how do we judge this and and uh
you know could we have I mean we're
getting close to the point I think where
we can have push button review articles
for an area of the literature uh in fact
at archive we suspect we're getting some
of those submitted to us uh it's not
quite push button but but where people
follow a fairly standard uh procedure of
choosing a in setad of Search terms
searching those papers then asking you
know chat GPT to summarize them uh maybe
cluster them into groups and then
summarize them and uh that that can be
very useful to the research Community
but again you have to um uh there is an
opportunity there to to ask well five
different groups have independently
believe found this finding so maybe it's
something we can we can believe the
reason I want to put them into knowledge
graphs is mostly that you can easily
change the knowledge graph I mean I
think the weaknesses of LM is that they
have all this factual knowledge but it's
burned into weights in ways that are
really nonobvious although we're
learning something about that from the
unlearning literature that uh you know
partly stimulated by the right to be
forgotten in Europe that we need to be
able to delete a fact from the llm how
can we do that well that requires that
we figure out where it is somehow and
then change the weights so so that's
kind of interesting but it would be much
easier to delete the fact if we knew the
fact was only stored in the knowledge
graph and we just remove it from the
knowledge graph and then check that it
can't be Reiner from other things in the
knowledge graph um there's there's it's
not trivial um and uh so uh that you
know back in the I think in the 80s or
maybe early 90s in artificial
intelligence people formalized what a
knowledge base was and they said it's an
abstract data type that that supports
two operations ask and tell and the idea
was you could just tell it a bunch of
things they envisioned telling it in
something like a formal
notation and then you could ask it a
query also in formal logic and then it
would either be able to look up the
answer or do reasoning to infer the
answer uh or or say I don't know right
and that was the abstract data type for
a knowledge knowledge base llms are
pretty good at the ask uh but we can't
tell them anything uh except now with
retrieval augmentation we can add new
facts to as sort of source documents um
and there are now people working on
doing rag from knowledge graphs uh and
so I and I think that that really is
exciting so some of the things that I
advocated in that in then my talk from
now sort of two years old uh are
happening people are build using LMS to
build knowledge graphs and also
extracting them from other sources like
Wiki data Google has a huge Knowledge
Graph that they've built and uh and then
being able to retrieve from them and
those are instantly updatable so the CH
their challenge there is to make sure
that the M bases its answer on the
knowledge graph and not on its
pre-training knowledge and uh I'm not up
to to speed right now on what the latest
techniques are for trying to
architecturally constrain the models to
do that but but I I assume we'll be able
to solve that problem also very
interesting and I really liked what you
were saying about establishing
truthfulness I mean I wonder if you
could just give me your definition of
Truth and knowledge because of course we
have this subjectivity problem the
contradiction problem and of course
you've spoken previously about psych
which was the project to kind of capture
the the world's knowledge and could you
also just comment on the strengths and
weaknesses of rag well yeah I don't know
that I'm an expert in rag uh so I you
know the the earliest work that was done
in rag was still aimed at building a
language model and the idea was and what
they showed there was that the llm could
be much smaller 10 times smaller if it
instead used retrieval to pull out
relevant phrases and then base the
language model on those so the task was
just the pre-training task of predicting
the next token but the prediction was
based on retrieval now of course we
we're more in the instruct situation
where we want to ask a query and have it
look up the answer you know as as Bing
and now Google are doing um and uh uh
you know I
think I'm as I say I'm out of date but
but certainly uh Percy leang and his
students did an early evaluation of rag
systems that showed that uh something
like half the time the answer that they
were giving was not supported by the
retriev documents but was instead
leakage from the pre-training knowledge
and that's fine if it's consistent with
the retrieve documents we kind of when
it cites a document want we'd like to
know that the answer came from that
document um and unfortunately I think uh
yeah some some fraction of the cited
documents were not relevant to the
answer
uh and which was another failure mode so
um uh I I assume that now now that Bing
has had like a whole year of of
experience with that they probably
improved that a lot uh so so it it would
be nice for them to give us some numbers
on that um uh or for someone to
replicate that study but uh what I don't
really understand now is um you know
mostly rag has has been added post Hawk
after the LM has been trained uh or
there's been some uh you know subsequent
fine-tuning but but the but that first
piece of work um was using retrieval
from the very beginning and uh and just
as with tool use we're finding that we
really want to train the model to do
tool use and we want to train the model
to do rag and so uh you know I expect
that we'll see systems like that maybe
that have been trained from the very
beginning or or maybe partway through
the pre-training start to give them
these these tasks as well uh because
otherwise you of course you risk when
you change the the the fine-tuning
starts to destroy uh some of the
knowledge I mean I think John John
Schulman says you you're teaching it a
lie it knows this thing from its
pre-training and you're telling it to
output something else and uh that you
don't want to sort of confuse the system
that way so um uh yeah so I it the
there're very subtle issues here I I
think about uh retrieval and Tool use
and getting all that to work right very
cool um what's your thoughts on the
current state of you know things like
archive and Publishing and research in
the MOs space well you know I've been
devoting now probably about half of my
time is volunteering at archive uh I've
been a moderator of the machine learning
category since 1998 when when they first
started uh it was very easy in those
days now we get more than 100
submissions a day uh I think that
archive has been part of the rapid cycle
time of the field uh people post their
results when they're ready and we see
them long before they appear at icml or
in certainly in a journal article and I
think that's a good thing in general uh
because we don't have the journals and
the conferences as
Gatekeepers uh we also don't have the
quality I mean uh one of our uh uh
sayings inh house at archive is it is
not our job to protect your reput your
reputation so if you want to publish
something that's wrong and then you have
to retract it later we're not checking
we're not factchecking things um and so
I really encourage people before you put
your paper up on archive get peer review
in the sense of have some of your peers
review it and give you some honest
feedback uh to to uh and of course I
think in a lot of the companies there
are release processes and and and that
kind of thing happens but I think in
academic groups we're often hacking the
latch you know 5 hours before the line
and uh maybe we're not as uh disciplined
as we should be uh yeah I guess that's
my word of warning is that it is it is
kind of
unfiltered during the pandemic we had
problems uh that eventually surfaced I
think in an article in science or nature
that said you know there were some very
early chest x-ray databases were were
released um that had terrible sampling
biases and all kinds of other problems
and many papers were written saying oh I
can diagnose covid from chest x-rays but
they were really picking up on the fact
that that you know there was some text
in the image or people were lying down
versus standing up or they were children
versus adults or just all kinds of of
spurious correlations were being picked
up and uh so we ended up hiring a
postdoc to review those papers we
actually had to look at those carefully
because we're worried about propagating
uh you know uh sort of conspiracy
theories or mis misinformation
uh and uh whereas we we don't have the
same that kind of rule for p equals NP
uh and for and or or for some of these
other you know remon conjecture and so
on as uh um the the rule is you get one
chance to try to prove it and and if you
if you decide you're wrong well you can
submit a revision of that but you can't
submit any more papers with new new
things so we because we do get a lot of
crackpots submitting claiming that
they've proved famous you know
mathematical questions and the
physicists have to deal with the
perpetual motion papers and things like
that um and again we don't we we're not
there to protect your reputation but and
that's what peer review is good for in
my experience uh with in journals and
conferences so I I think the peer review
has its place uh but I also think open
science has its place and so um maybe
the the best thing is yeah to your own
peerreview before you submit and then
maybe also uh layering journals on top
of archive to say we will uh collect
together a set of papers we think are
very nice and kind of certify them as as
high quality which usually will require
the authors to submit re revised
versions um in fact one of my personal
juristic is don't read a paper and
archive until it's gone through version
number two because usually the authors
themselves see a bunch of problems and
immediately submit version two like
within a week of version one and so it's
a yeah that's I think um don't Google
have a feature that F they wait 5
seconds after you press send until it
actually goes um but if you could change
you know one or two things about the
whole open science project what would it
be you may be alluding to my pin's tweet
I might be um I'm not convinced that
PDFs of papers are the ideal uh
interchange form for science when you go
into an but llms may be changing my mind
about this but uh but in my pins tweet I
argue for uh maybe a different idea
which is that we we can view the
scientific Enterprise as trying to build
a knowledge base of what we believe is
true in an area and
um and t a very common pattern in uh say
in a paper in machine learning is I'm
working on machine learning problem
well-defined machine learning problem X
say positive unlabeled learning and I
believe I have a new method that either
is better than everything before or is
better along some Dimension than
anything before um and uh I'm evaluating
it on these standard benchmarks that
everybody else is evaluating on it and
you sort of see now like at papers with
code they kind of try to collect up
these papers that are all doing the same
thing which they don't really completely
succeed on that but maybe the authors
should be going to something that's
maybe more like Wikipedia or a Knowledge
Graph and say I'm working on this
problem here I add another uh proposed
solution proposed algorithm and then I
provide my evidence for why I think my
algorithm is better than these others uh
and and we accumulate that evidence
ideally you know in like a GitHub repo
where we can run all the methods against
each other uh against some standard
benchmarks and and agreed uh metrics uh
and so I mean obviously this as new
machine learning problems
emerge this ontology of what are the we
know I mean we didn't have positive
unlabeled learning until I don't know
2010 or something as a problem but we
had the multiple instance problem back
in 1991 we had other kinds of weak
weakly labeled data unsupervised
learning of all kinds of flavors and at
every meeting um we're seeing new kinds
of problems emerge that require new
metrics and evaluation methodologies and
so you can imagine having nodes in this
graph for all of those kinds of things
um and when you were coming into a new
area say as a graduate student you could
see a snapshot of the field without
having to right now you have to go read
all these papers and build this in your
head or or or in your spreadsheet um and
so I think there's a lot of repeated
work there that is very timec consuming
um so I don't know if it would be
possible I mean the idea would be you
know instead of or in addition to
writing a paper in English uh you would
build this graph but somehow you'd have
to be you know support the idea that it
is citable research and you get credit
for it uh and uh but maybe I mean we can
we have models of that from Reddit and
and Wikipedia itself and so on so maybe
maybe and and yeah and you know stack
exchange so so maybe we could do all
that but now you know the counter
argument is oh now I can just wheel up
an llm and say you know read the
literature on on the positive unlabeled
uh data and tell me what the best
methods are and what are the different
techniques and how are they evaluated
and I don't how well it would do but but
maybe that is the that is the other
technique use our own technology to to
address that problem yeah if only if
only um we we'll we'll slowly get on to
your work in safety engineering but
maybe first you I think you retweeted
from um uh Emily Torres uh about open
ai's work on super alignment I think
think is it open ai's work or is it Ilia
SS's new new startup and um I I think
you were implying that it was a bit
guided in in some way well I think I
mean maybe we should come back to the
question You' asked earlier about what
is Truth uh I I think you there
obviously are conflicting truths in fact
one of the lessons of the psych project
was that they could not build a globally
consistent knowledge base it would have
contradictions in it and so they so he
had to develop these micro theories that
were internally consistent but but uh
but not globally consistent and uh
certainly we know that's got to be true
uh we have all these scientific
questions we don't know was Co a Lab
Escape or was it you know animal to
human Transmission in the wild uh we
have evidence in favor of both uh I
think a good system should not give an
answer it should instead say well here's
one hypothesis and here's the evidence
that supports it here's the other
hypothesis and here's the evidence that
supports it and we don't know the answer
um and of course if it was something
there are more things that are more
cultural or or or ideological and I
think it would be clarifying to to say
well within this ideological framing
this is what how people would say is
important or true and and then there
these alternative ones um so we
shouldn't expect the system to give us
the truth because how could it know what
the truth is we don't know what the
truth is so uh I I uh I mean we see this
a little bit in community notes on on
Twitter is that uh people will say I
think this tweet is misleading and here
are uh sort of trustable citations to
sources and then they ask people to rate
whether those sources are trustable and
so on and so you can imagine building
that kind of recursive argument
structure and the structure of arguments
has been studied in Ai and philosophy
for a long time uh so I think there's
lots of things we could borrow from
there but I guess that doesn't come back
to this question of super intelligence
super alignment and so on I mean I guess
partly it is well what are the the
values that we want these systems to
respect um and and and I think it takes
us into kind of the territory of the
three laws of robotics or something like
this right we would like them to not
kill us um and uh and uh but subject to
that constraint we would like them to
you know keep themselves functioning
yeah I I I think it's very difficult to
articulate those and uh and that brings
us into this question of of safety which
is a word that is really thrown around a
lot um you know I would just came from
the the invited speaker who's talking
about uh the EU AI act uh which talks a
lot about safety and and highly capable
uh
systems um one of the uh and and at the
same time I've been serving on a uh uh
uh National Academy of Sciences study in
the US on machine learning for safety
critical applications
now the safety critical applications
people are coming out of sort of
traditional uh things like Aeronautics
Automotive you know railroads uh
medical uh things where the harms we're
interested in are loss of life Serious
injury uh maybe destruction of
infrastructure uh so so I don't think we
have any disagreement that those are
harms and that that uh what's surprising
is that Society has different tolerances
for say a lives lost per uh air air mile
traveled versus lives lost per car mile
traveled right we tolerate a much higher
fatality rate for automobiles than we do
for aircraft and we might want to ask
ourselves why and if we have automated
cars in the future will we still have
such a high tolerance or will we insist
that they be much better than we have
been I think the the promise is that
they could be much better drivers than
we are they might not be as fun
um uh but uh uh so anyway uh traditional
safety
engineering tries to anticipate all the
things that could go wrong and so they
Define those as a set of Hazards and so
all of the known hazards you then design
your system a hazard can be defined as a
a region of the state space that you do
not want to enter because once you get
in there with high probability something
some harm is going to occur um and so
then then you design a controller to
have a margin of safety with respect to
All the known
hazards a challenge of this is that
there are always new hazards that we
didn't anticipate just as there are
always I've also studied this question
of What's called the open category
problem in computer vision there's
always some new object that you never
saw in your training data my example for
self-driving cars is something like the
the one wheeel which is you know kind of
skateboard with a pretty big wheel in
the middle there aren't any one wheels
in the uh image net collection for
instance because they hadn't been
marketed yet in 2009 um nor are there
manyy ebikes because those are new too
but each of these new uh Transportation
things that's uh behave slightly
differently is used by people
differently and a self-driving car needs
to detect and be able to predict how
people behave on those in order to avoid
colliding with them so um so we think
about all the uh anticip ated problems
but then we're always going to have this
problem of encountering novelty and one
of the reasons that we're even thinking
talking about having self-driving cars
is we've had these advances in deep
learning that give us much more powerful
perceptual systems than we've ever had
before on robot cars so we think that we
might be able to do this but but the
we're moving from the traditional kind
of closed World robotic application
where humans and machines are kept
completely separate uh to a world where
the machines and the humans are
interacting at high speed and with high
momentum um and uh and and so it's
exceedingly difficult to see how we can
engineer those systems so uh but but I
think the safety engineering people are
making progress in on the machine
learning computer vision perception side
I think we still have ways to go because
we are systems still cannot guarantee
kind of distribution independent uh
accuracy so we know our accuracy depends
on the training distribution we can bias
that training distribution to be
particularly enriched around all the
known hazards um and that's what uh the
companies are all doing right they
they're all building High Fidelity
simulators and then being able to use
those to simulate all kinds of crazy
conditions and use kind of adversarial
active learning to to try to drive the
system into places where it will fail uh
so they can discover new failure modes
um but of course there there will still
be things the simulators don't know
about like these one wheels or whatever
whatever the new thing is that would be
marketed um and uh so so I think uh
technical challenges in in the machine
learning and computer vision side are
how can we detect that we're looking at
something novel that we've never seen
before and that requires that uh that we
have a representation for that
novelty and that has been a challenge in
computer vision because traditionally we
would do something like pre-training on
imet uh uh and uh and hope that that it
had learned enough uh it had Rich enough
representations that it could represent
stuff beyond the 1,000 classes that were
in the in the standard imag net
collection um but that's not true we
find that there are lots of things that
that just get alias on top of existing
classes and don't have a distinctive
enough
representation I'm I'm hopeful that the
foundation model strategy which says
let's train on everything we can we get
so much variation in our training data
that we that the systems will have it uh
everything will have a
representation um and so we can see the
novelty and and know that it is an
unusual combination of features we have
the features but we have never seen this
combination before as opposed to we
don't have the features therefore are
blind to the novelty which which is what
we've observed in in sort of smaller
computer vision applications so maybe
but so that that gives me some optimis
optimism that uh using serve Notions of
epistemic uncertainty we can say oh this
is something new I've never seen before
and so then the car can behave more
cautiously because it doesn't know
whether this is just like a a a cloth
Banner that's blowing in the wind or
whether it's some new kind of pedestrian
thing it's a you know a person in a
Halloween costume that it's never seen
before and and it doesn't want to hit
them so so that's one thing but then I
think we need to take seriously the
possibility that we need to formally
verify our learned models and so there
were some more papers here this week uh
on uh either trying to have new neural
network architectures that are easier to
verify or improv methods for
verification and we're mostly based on
the idea that we we want to defend
against adversarial examples although
I'm not sure that is I mean that's kind
of robustification but at the pixel
level but I think we also want to um uh
have uh sort of a guarantee that that
the model is well behaved in between the
training data points we you know we just
don't know what those raus do they can
shoot off in weird directions uh in
between data points and so uh we'd like
to bound the the lip shits constant or
the second derivative there was a paper
here on that um and have some confidence
that it's not behaving wildly in between
our data points and if it is behaving
wildly in some region let's get a new
data point there and add it to the
training set and pull it down until we
can't find any more you know uh points
that witness a failure so getting those
to scale I I think we're quite a ways
away from that but if we could do it we
could get away from this problem that
we're vulnerable to distribution shifts
and uh to because our our worst
nightmare is that that that that little
region of the input space turns out to
be hazardous and for some reason the at
deployment time there's a high
probability of hitting that point and
and and we don't the we can't give a
guarantee so I don't know maybe it's a
pipe dream but you know yeah we can be
optimistic I mean we presumably we could
never do complete certain verification
of these machine Learning Systems these
system one systems I mean well because
what's our specification it is the data
points we don't have an independent
declarative specification of you the
system will always be able to tell the
difference between a kangaroo and a deer
for instance which was sort of the
famous Volvo example that their deer
detector didn't work in Australia um but
uh yeah we can't guarantee that but I
think we can give another guarantee
which is that if this if the system is
not confident it can correctly detect
its its lack of confidence right and and
now we have these uh quite good
probability calibration techniques uh
they are still distribution dependent
but maybe we can come up with a
distribution independent notion of
calibration and uh so that the car can
know that it's that it doesn't know and
become much more cautious right uh so it
can still behave safely so we don't have
to be 100% correct we just but we do
need to be 100% correct about our lack
of confidence uh yeah and so yeah yeah
because you gave a beautiful example in
in your recent talk of um building um a
system with basian optimization and you
can you know model all of the things in
a situation you know stabil situational
knowledge and then you can do this
adversarial perturbation so you can
deliberately put it into strange States
and you can reason about that
uncertainty and that could presumably be
built into self-driving cars or
something something like that right well
in fact I took those examples from the
work that's being done at Wabi which is
a uh startup in Toronto led by Rael oras
and um and I think they're doing really
beautiful work in that but but there are
other groups uh I mean pretty much
everybody is using some form of basian
optimization or surrogate model
optimization precisely because it gives
you a uh a measure of epistemic
uncertainty that you can optimize so you
can say well what's the most uncertain
part of my space or or if you have a
some notion of risk what's the riskiest
part of my space where I don't have
enough data and and uh really focus your
your uh sampling there so so we have a
lot of great tools I think that have
been developed over the last 10 years
and and putting them together we can
make progress but we still don't have
good enough verification tools that
scale right um You also made a comment
except I think a lot of my audience are
software Engineers it it's surprisingly
difficult to verify even a piece of
software right yes yeah I mean how would
you go about doing that well I mean
people hold up examples uh okay we can't
verify completely partly because getting
a formal spec is so hard itself and we
know that uh you know most of our
specifications aren't even quite correct
you know most of our systems failures
are due to to requirements failures um
so yeah so what do we do
um uh that maybe moves us well let's see
I want to take this in two different
directions um uh so one thing is you
know know I I'm very impressed with what
Microsoft has been able to do with
device drivers right you can't prove
them uh correct but they can uh use
model checking to to establish that
certain properties are obeyed and so
they they can uh ensure that they're
that that they're deadlock free and some
other things I'm you know they're
probably memory safe um using model
checking those are relatively small
pieces of code but they're in uh you
know real time uh very challenging uh
situations dealing with devices which
are notorious for you know sending spous
signals in and so on so um you know one
wonders you know uh could the crowd
strike uh uh you know you know they're
evidently not doing any model checking
on those it may be harder because the
device driver is kind of you have a
pretty good idea what it's supposed to
do so you you know how to write down
properties uh but but but obviously um
so maybe so we can't verify everything
but are there properties we can check uh
and
and I think the same will be true for
machine learning we won't be able to to
prove everything but we need to think
what might be some useful properties we
could verify uh and do we you know may
we maybe we need to form approximations
of the full surface that give us sort of
upper lower bounds on us behavior and if
those are tight enough maybe we're we're
confident so the exact decision boundary
maybe you know full of millions of
little things and we can't verify them
all but if we could put make much
smoother simpler approximations and
verify those maybe that would be good I
don't know this requires a lot of
creativity to figure out what can be
done and what can't be uh I think
everybody should read Nancy leveson's
Book uh engineering a safer world
because what she says is you don't build
a safe system and then deploy it safety
is a is a con is really a control
concept that uh a system is we need to
be
constantly uh controlling and modifying
the system to keep it safe and uh and I
think it's very similar to this problem
of the unknown unknowns in computer
vision and AI in general is the unknown
hazards the unknown failure modes of the
engineering system you've built uh so um
uh you know David Woods who a you know
longtime sort of human factor person uh
cites this work that's come out of
Caltech on that engineered systems tend
to be robust yet
fragile and the my version of summary of
it is for the for the failure modes we
know about we engineer margins of safety
so we're robust to those things and and
and so we we can withstand you know we
have these uh Byzantine things we can
lose we have raid we can lose you know a
bunch of disc drives and still uh
function just fine um the trouble is
there are the UN the unknown failure
modes that we didn't know about and and
when we're designing the system while
we've built margins of safety for the
stuff we know about we then optimize
size weight power and all these things
to try to make it as cheap as possible
and that's probably taken us right up
against the edge of the feasible region
and lurking right on the other side of
that feasible region Edge is some novel
fault that we didn't know about and we
don't have any margin of safety on it
and it bites us and so we're fragile to
the stuff we didn't anticipate to the
unknown um things and so um I've also
read this literature on what it called
highly robust human
organizations and uh this was work that
came out of sort of the organizational
uh psychology and sociology literature
in the 1980s um where they uh and was
kind of the origins for the patient
safety movement in operating rooms and
uh pilot training uh and the one of the
things that they talk about is
organizations that succeed in in
achieving very high levels of safety
have this fundamental belief that there
are these unknown failure modes and
their main job is to try to detect them
before they bite and the what do they on
what basis can they detect them
anomalies and near misses so the idea is
that uh sort of the kpi for a uh a
safety engineer and operating uh on the
ground is every time we have an anomaly
let's try to figure out what it was what
was its root cause is there some new
failure mode that we need to to think
about and and and protect against and
near misses even more so
and uh so and of course the idea of a
near Miss I think is that we we have
these known hazards and we have our
margins of safety and if we intrude
inside those margins which we weren't
supposed to do that means you know
something is out of whack and we should
investigate that um but there are some
other subtleties in the near Miss
concept that that I'd like to encourage
AI people to think about in the
self-driving cars we see um the car
might have a a rule that says I need to
be 2 m away from every pedestrian at all
times and so uh but there are definitely
scenarios recorded where The Pedestrian
was in the crosswalk he sees the
self-driving car coming straight at him
and he leaps out of the way and the
self-driving car says 2 meters I'm good
right and so and this is because it is a
counterfactual near Miss had The
Pedestrian not taken evasive action
there would have been a collision and of
course other cars also have to take
evasive action and so the self-driving
car needs to have the sensors and the
reasoning to be able to say would it
have been a a harm if they if other
actors had not acted appropriately and
It's tricky because of course if the guy
turning left hadn't waited for you to
pass that would have been a collision
too uh but that isn't a near Miss we we
we decide uh so it's it's quite subtle
but but in any case um so so uh this
leads back to to the idea that that when
we deploy a safety critical system we
need a whole human organization that's
wrapped around it that is that is
constantly looking for new failure modes
and then uh adapting the system and I
think we can build AI tools to help with
that so I've been doing a lot of
research on anomaly detection or novelty
detection I haven't thought much about
near misses but I think we have things
to do there once you find a new failure
mode you have a diagnosis problem to say
what's really causing it and we've
researched diagnosis in AI for 40 years
and so we have some ideas there they
fall back on needing to have a causal
model and of course we've been doing a
lot of work on causal AI these days uh
in with udea Pearl's theory of causality
and uh lots of work Happening Here uh
well in in in Europe on uh on causality
and uh and then we have a repair so I
mean to the extent that repair means we
need to to retrain our computer vision
system or something we know how to do
that um
or we retrain our device controllers so
I think I think we have a lot to offer
that current um uh safety things rely on
humans to do all those things but maybe
we can have a collaboration with humans
to to maintain the safety of the system
yes we will still always have the
challenge that the better we do our job
of maintaining safety the more the
safety crew will be cut by management
because the number one threat to safety
is typically budget cuts uh because
management says gee you haven't had a
failure for two years why am I paying
all these people who are just sitting
around what are you doing um and Nancy
Levon documents that these systems tend
over time to migrate toward danger
because basically have budget cuts staff
turnover training failures and just that
you don't have enough experience with
with disasters so you become insensitive
and so uh this is really hard but but uh
yeah you need to
um educate management about and we need
some way I guess of of estimating the
cost of the failure and the and the and
the savings they're getting by having
these people around cuz you don't want
to be the next Cloud strike yeah yeah
know it's almost a bad incentive because
if you're too good at your job then then
you'll have your job taken away from you
and and I completely agree that you know
safety engineering is a living breathing
thing and it decays over time there's
this open-ended process there's
orchestration there's humans that
diagnosis it needs to keep going on the
same thing in in things like fraud
detection it's not the number of frauds
you detect it's the number you prevent
but how do you measure that you
yes indeed um so so final thought most
interesting conversation you've had this
week at icmo one thing I had was I was
talking to to some guys from
chingua um and they are trying to figure
out how to train llms on commodity uh
like gaming uh gpus because naturally
well they're academics they don't have
budget for to buy a bunch of A1 100s
either um but they're also under the
Embargo from the US uh and Europe on
this so um necessity may be the mother
of invention that uh they they're
working on 8bit uh training uh large
models at 8 bit and uh if they can
succeed we all of us academics will
finally be able to start training our
own llms so uh I wish them luck and hope
that they will open source it once they
figure out how to do it uh because I
think you know that's been a big
challenge for the academics um uh
nonetheless I'm I'm impressed uh one the
uh we see much bigger collaborations
happening uh now the number of papers I
would it'd be interesting I I I it's
purely anecdotal but my impression is
that we have very few single author
papers even relatively few you know
student plus their advisor papers we
tend to have papers from multiple
institutions working together uh some
corporate and some academic uh multiple
countries working together uh it's very
hard to say you know I mean we see huge
number of Chinese authors but they're
often working with you know authors from
Europe or the US uh and of course we
have tons of folks from India also
involved in these companies so it's uh
uh that that's that's a big change it
used to be that that single author
papers were something that could happen
in this field but it's pretty rare now
yeah the days of the gentleman
scientists are over yes and uh you know
I mean I had a conversation with Jeff
Dean about this and he said you know
You' just got to uh give up on this idea
that academics can compete and play this
game it's over for you guys uh he and he
drew an analogy to when he was a a
graduate student working in distributed
systems and and he said he was basically
the last generation of academics that
could do distributed systems phds
because then Google started doing
distributed systems you know a couple
orders of magnitude bigger than anything
any University could do and it just
scaled out from there and he said all
the actions switched to the company
I guess I'm still think that there's
some really interesting things going on
and in particular uh the you know when
when the the first uh was it was it the
LA llama the first uh open weights model
that that was kind of uh Spilled Out of
uh out of uh fa Facebook meta and uh and
there was an explosion of of stuff that
all happened just in a couple of months
because people could experiment with it
and you had hobbyists doing stuff and it
was incredible what people were doing
and and we've also seen this whole thing
to to be able to run it on all all kinds
of different devices we people so um I
think that that uh I'm a huge fan of
this open source I you know I mean
there's all these debates about uh the
and there was a a you know discussion
paper on that here about you know is it
safe to open these models and so on um
it's it it's certainly incredibly
accelerates the research to have these
open weight models available and and be
able to have everybody worldwide uh work
work on them and even just run them on
on a Mac uh you know yeah indeed I mean
lower and that low Precision was really
helpful for that and that came that was
sort of a instant Discovery once those
weights were available yeah indeed but
but do you think there's always going to
be a relationship between flops and
capabilities I mean surely the reason
why we need academics we need very smart
people working from first principle who
deeply understand the topic not not just
bigger and bigger GPU clusters I mean do
you think there's there's space for that
well I mean to the extent that I don't
think the Transformers are the ultimate
architecture uh and and that they
they're super good at this sort of
sequence to sequence mapping uh and and
so you know if you want to change
something from from XML to Json it's
fantastic and they can do all these
formats you know so so the days of
writing little uh data hacking scripts
which you know we used to write so many
regular expressions and and uh those are
over because you can just ask these uh
these systems to do it so they're super
good for that but as we discussed it's
not clear that they're very good for uh
reasoning uh and and the interface with
knowledge bases or formal reasoning
engines is still uh not clear and and
the other system two to system one
discussion that we didn't have is uh you
know the this this idea of learning
through practice I mean an awful lot of
what reinforcement learning is is
basically involves you start with a
declaratively specified goal and a and a
model of the Dynamics of your system and
you compile that into a policy that's
very low level and goes directly from
sensors to actions so that's going from
system two sort of explicit reasoning
down to system one uh and so that's been
one model of what happens to us you know
when we learn to ride a bicycle or play
an instrument or whatever we are sort of
interpretively executing things
painfully slowly and making all kinds of
mistakes but but over time we become uh
better and better and eventually fully
automatic so we don't even have to think
about it and we can do it and uh and and
so that and one of the advantages of of
doing it that way is you now have the
sort of entire provenance of how you got
to these low-level system one beliefs
from this declarative system two
and uh and so that also helps you if you
find a bug in your system 2 spec you can
recompile back down to system one so but
but it's obvious that we don't learn
everything from from reading a recipe
and practicing some things we just learn
from interacting with the world and they
are system one all the way and it's a
really interesting question how to mix
those uh and do we make post talk
explanations for why our system one
routines work and uh and and you know I
it's fascinating area but but but it's
clear that we we are able to move back
and forth between those and and that
seems to be a source of our power and
and so uh so I don't think just scaling
data and scaling the the Transformers is
is a long-term feasible because we know
for some domains we're going to need
exponential amounts of data or even
worse and uh you know you're not going
to build a theor by generating all
possible proofs on the other hand you're
going to you can build uh a proof
assistant that outputs uh you know lean
uh code from a bunch of examples of lean
so uh you know I think it's yeah it's
all very confusing right now uh
Professor di thank you so much for
joining us today it's been it on it's
been a pleasure
[Music]

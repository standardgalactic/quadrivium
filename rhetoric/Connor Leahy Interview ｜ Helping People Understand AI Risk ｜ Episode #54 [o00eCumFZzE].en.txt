I think about persuading people or like
giving information I really come from
this like how can I help you like I
really really come like in my mind in my
heart when I try to I'm like I'm not
here to hurt you I'm not here to change
your mind in a way you don't want it to
be changed because this is a thing a lot
of people are scared of a lot of people
are scared of having their mind changed
in ways they don't want it to be changed
this is very common this is very very
common and so important way to be is
like I'm here to help
[Music]
welcome to for Humanity and AI risk
podcast episode number 54 Connor Ley
interview I'm John Sherman thanks so
much for joining me I'm just a dad in
Baltimore who believes the big Frontier
AI Labs like open AI when they openly
publicly admit their technology can kill
my kids that they do not understand how
to control it that they do not
understand how it works and that they
spend all their time and money making it
stronger not safer for Humanity is the
AI risk podcast for the general public
no Tech background required this podcast
is solely about the threat of human
extinction from artificial intelligence
please I need you to hit like subscribe
share donate leave a comment um and we
have in the show notes monthly donation
sub subscription links please help me
help the general public understand AI
risk today's interview is the most
important on this show to date I really
excited to share this with you when I
first went down the rabbit hole of AI
risk a year and a half ago there was one
voice one AI safety leader whose tone
rang truest to me I met lots of experts
in the rabbit hole of AI risk who told
me our default future on our current
course was eminent human extinction from
a I but that means killing every living
thing on Earth all the dogs the babies
the flowers the whales the elephants
your mom my mom my kids your kids your
friends my friends all of it and I could
not figure out why in the air risk
rabbit hole no one was pissed off about
this until I came across Connor Ley
through dozens of podcasts and web
videos Connor he is the CEO of
conjecture a company in London working
on AI alignment not building
AGI Connor Ley is one of the most
important people living on the planet
Earth and it was my great honor to talk
with Connor for more than two hours from
my hotel room in Austin where I was uh
visiting my son for parents weekend at
UT um I have one huge takeaway from the
conversation that changed how I talk
about AI risk the moment I heard honor
say it ready for this we're not here to
wake up the world to AI risk or convince
people about AI
risk that presumes there's a
controversy that frames AI risk as if
there are two sides but there aren't
there just aren't nearly every qualified
AI leader both corporate and academic
openly admits that Frontier AI
development making Leading Edge models
bigger and smarter threatens to kill us
all quite literally and quite possibly
very
soon this is not
controversial so what Connor says is now
my motto he says we're here to help
people understand what is to come if we
don't change
course we're here to help people
understand what is to come
there are not two sides to this debate
there is only one future to change this
approach is a far easier approach for
the people we're trying to help digest
and absorb AI risk than hey let me
convince you about this crazy thing that
you've never heard about that's so crazy
and all the different experts have
varied opinions on there is some
variance in the experts opinions about
our AI future but there's much more
agreement it's pretty simple build
something smarter than yourself at at
your own great Peril this is not
controversial this is beyond obvious
stepen Hawking understood this decades
ago before do we go before we go to the
interview with Connor please watch this
short two-minute video from my brother
in our cause Michael at lethal
intelligence. links in the show notes to
his channels listen to Stephen Hawkings
own words from the past about our
future success in creating a I would be
the biggest event in all of human
history it might also be the last unless
we learn how to avoid the
risks I fear that AI may replace humans
all
together if people design computer
viruses someone will Design AI that
improves and replicates
itself this will be a new form of life
that out performs
humans we only have to look at ourselves
to see how intelligent life might
develop into something we would not want
to
meet if a superior alien civilization
sent us a message saying we will arrive
in a few years what we just reply okay
call us when you get here we will leave
the light
on probably not but this is more or less
what is happening with
AI the development of full artificial
intelligence could tell the end of the
human
race Steven Hawking knew it well before
chat GPT
anything it has been obvious and not
controversial that building artificial
super intelligence is existential
suicide there's nothing new here Connor
Ley knows this too so without any
further Ado here is my interview with
the great Connor Ley CEO of
conjecture
hello Conor how are you good it's doing
great how are you good good it is such a
pleasure to meet you man I I um I'm I
might be your biggest fan oh thank you I
was very kind of you yeah glad so glad
to be here Max spoke very highly of you
so saying you've been doing some great
work so you know pleasure if I can help
awesome awesome yeah I'm just a um I'm
like a normie I'm like a general public
person who came into this thing like a
year and a half ago
totally uh unaware of it and became
aware of it and have really just been
compelled to like work on it really
hardcore since then um it's great yeah
so I wanted to start start asking you
and like uh just off the top like as a
human how are you like how are you doing
um yeah actually very good like um I
think I'm probably at the happiest I've
been in my life for many reasons like
you know obviously the world is not in a
good state but like you know I'm doing
well you know good health good
relationship with my family and my
friends you know I get to work on stuff
that's very meaningful to me a wonderful
group of people you know I'm in my
office right now so I'm personally in a
very good state I'm doing very well I
feel like I have kind of like grieved
about these things enough and like quite
a while ago so these things don't
distress me anymore it's kind of like
the thing is like you know why aren't
you worrying and I'm like would it help
like you know I can and like you know
sometimes you feel right sometimes you
have a you know things are bad or
something bad happens or whatever and
you feel a bit bummed about it or
whatever but like for the most part it's
just like you know I I have my ways with
coping with it I don't necessarily
recommend them always to other people
but just like this kind of the stuff I
was I've always felt like I was going to
do and I don't know you know in another
lifetime I would have you know worked on
nuclear weapons or you know some kind of
like you know big Civics projects or
something right it's just like we're
going to war right but like here I am
yeah and when you say you feel feel like
you've grieved like and you you went
through a period of grieving or
something you know um what were you
grieving like like you know I think I I
have I have like good days good weeks
and then I have sometimes where
something happens in the news or
something it just kind of like feels
like it really hits you hard and it's
like o man it really is moving fast this
is a good question I um I think the the
right answer is something like um that
there is no God I think that's like the
the the real thing like kind of like not
just atheism but like deep atheism like
there's one thing which like atheism
like the literal fact of not believing
there is a God but there's also kind of
deep atheism of like there is no one
that is going to come to save you like
like there is that the world is just
mechanistic there just Parts there's
it's just things just happen it's just
physics and like truly grieving this
like so like you know I was a edgy
atheist when I was a teenager right I
was like H these Christians so dumb blah
blah blah right but but the truth is
that I was a techno Optimist I was like
but technology will save us and then but
then I read realize that that's just a
god-shaped hole like I just renamed God
into technology so I didn't actually
grieve with proper atheism with proper
like you know there is no God no one's
going to save us you either do it and or
not and then you die and like there's
kind of a sense where this can be like
extremely frightening to people for
understandable reasons and so on but
like at least for me there was a way to
grieve this and then be like all right
it is what it is and now I will act kind
of like in kind of like a dowst way just
kind you act with the flow you act with
the stream it just is what it is and you
just like stoics like the stoics have a
lot of Aesthetics around this around
just like I feel like the stoics
sometimes get a bad rap where like a lot
of stoic is like has has like gotten
like Macho kind of flavor but really sto
is is like this deep down of just like
accepting things that you can't change
for what they are and not getting upset
about
it wow and so the grief was the grief
was like the idea of like like it's not
everything happens for a reason it's
nothing happens for a reason exactly
it's like there's this deep thing where
like a lot of people you know now like
conspiracy theories and so on about like
you know evil cabals ruling the world or
whatever in a sense I think this is cope
because I think the real is is that the
world is not controlled by some evil
cabal of super Geniuses and actually
people would prefer if the world was
controlled by an evil cabal of Genius
because then at least someone's in
charge at least someone knows what's
going on even if they're evil at least
there is a mechanism by which things
happen and I think for a lot of people
it's even scarier to imagine that bad
things happen for no reason it would
almost be better if there was an evil
villain who's doing all these evil
things at least there would be a story
there would be there would be logic
there would be a there would be
something you can touch something you
can grab there'll be a guy you can punch
right but like most really terrible
things don't really happen for any good
reason they just kind of are and that's
a really hard thing to grieve it took me
like quite all like most of my early 20s
to kind of like really internalize this
like you know intellectually you know
it's it's easy you just say you say the
words but like deeply emotionally
grieving this and then not despairing
is like uh very important it's just
being like there is no God and that's
okay it is what it is I will still do
everything that doesn't mean good things
can't exist there's evident like love is
all around us there are in fact great
things that have happened despite God
not existing and that's amazing and we
should be thankful for this and we
should be grateful for this and we
should work our goddamn asses off to
make more of it and and is this a
required part of it you think that like
um you know because I often think of
like organized religion might be a place
where we could get traction and AI risk
awareness could become a thing because
they have meetings regularly they have
all the trappings of an organization
that's like ready to go just input the
cause and and is it required that you
come to the realization that that
there's no God in order to truly accept
where we are with AI risk
so Tech so purely prity speaking no
absolutely not this is why I often tell
people I don't recommend you necessarily
copy what I did is that that this is
a there's a way to do it but like is it
a scalable way is it the right way to do
it and another thing is this is going to
sound weird
but many many many spiritual traditions
and religions don't really believe in
God like even Christians don't really
believe in God usually they just like
kind of believe in God and or they
believe in something like God acts
through us like a lot of even if we go
like medieval like Christian literature
you go back to like
Catholicism from the even like the
medieval period or whatever a lot of how
they describe God is not how we would
talk about God today as like this like
weird thing out there that's all
powerful they say that but they also
have this sense that God is kind of like
just kind of like nature or physics is
like and that God acts through us it's
like good like the
king is good because he's doing God and
he is God like God is a process rather
than than as a creature and so I think
there is a lot of power in that thought
of like God as a process and I do
believe versions of this in like a
purely pragmatic sense is that like I
think a lot of people when they use the
word God they're actually talking about
civilization like the actual object that
their intuitions are pointing to is
civilization like when they see their
neighbors and they all come together and
build a town and raise children and you
know so on the the pro they might say
wow through the glory of God we have all
come together but that process is real
it's civilization it's laws its Norms
it's culture like this is a real thing
and they're right that it's not a
physical thing there's not a physical
God it's a software thing it's the
software of their culture that is God so
from my experience of talking to people
of faith I found them to be you know in
some cases some of the most aligned and
the most reasonable and like like once
you explain it to the like oh yeah of
course okay like my friend is a is a
Buddhist he lives in a monastery and
everything and so he recently got the
chance to meet like the master of his
master like the one of the top guys in
like all of his like sool yeah yeah yeah
so he's like really like Tibetan monk
you know verely leaves the country is
like this like super super high tier
whatever right and he got to meet him
and he talked he wanted to talk to him
about Ai and so he told him about Ai and
he's like well you know Master like you
know like you know what do I do and like
this like old Tibetan guy and he was
just like so reasonable about he's like
well you know it seems like you're doing
the right thing like you know of course
meditation spiritual item is important
but like I'm learning from you he about
these things and it seems that you are
doing what's necessary for the Dharma
and I'm like wow that is so incredibly
reasonable what an incredibly reasonable
thing to believe and to tell your stud
like yes of course we need to save
people and like produce good things like
that's obviously part of the Dharma and
I'm like now that is just a unbelievably
reasonable thing so not everyone but
like many spiritual Traditions my
respect for religion and spirituality
has generally increased over time rather
than decreased it started at an
extremely low point to be fair yeah but
yeah yeah yeah no I I feel I'm I'm an
atheist I think there's a more to the
universe more to the world than I can
see but to me it's like um there's 3,000
books about it and anyone holds up one
book and it's like my book's right and
the other 2,999 are wrong it seems like
a pretty weak spot to be in yeah I I
think I me we could talk about this at
length I think there are it's have you
read yel hari's wrest book Nexus I
started it by I'm I'm in the middle of
it a little bit it's quite good and like
I like that he talks about how like
information is in my words I would sced
information is often about coordination
so like making claims about what your
God is is not a claim about reality it's
a claim about factions orientation and
like from that perspective I'm like oh
okay yeah I I see the pragmaticism of
doing that it's a shame that you're also
throwing out you know empirical reality
that's a bad move but like I see why you
might want to do this you know whether
you know we do the same thing with
sports teams right it's like is your
team really the best I mean that's not
the point right yeah yeah absolutely um
all right so let's jump to a little bit
of current events
uh we have an incoming Administration in
America um that uh brings with it a a
certain gentleman who knows a lot about
technology um what are your thoughts
about the incoming Trump musk
Administration and what it means for AI
risk don't really think I have any
clever comments here um I think politics
is messy I think um if people so okay
the way I think politics in general is
kind of like that um there is Mech
mechanisms there are mechanisms that do
explain what's happening and like how
decisions get made but they're basically
completely different from like all the
surface stories it's like politics
optimizes to present a nicely legible
story of how decisions get made like
Trump likes X therefore X will happen
and this is almost never true this is
almost never true there is a process
somewhere we like during a dinner Trump
has these conversations with these
people who he owes a favor for this
reason and then because this guy is the
head of this institute and he you know
works with this guy and therefore this
will happen like this is much closer to
how real complex um yeah systems like
this work and like I don't really have
insight into the Trump Administration I
don't know Trump I don't know Elon I
have been in a room with him once and we
didn't even shake hands um I think all
things equal I expect it to be less
predictable like my month prediction
about the upcoming Administration is
that whatever happens is less
predictable than the alternative I'm not
saying that the alternative would have
been better or worse I think it would
have been lower variance in this regard
like there is a story that people
sometimes tell that like you know people
like Trump or Republicans magga people
don't care or don't understand AI or are
stupid I disagree with this like I sure
I'm sure there's dumb men evil people
involved on as on both sides and
whatever right like always but like deep
down the idea that like machines created
by these like you know big Tech
behemoths you know are replacing human
jobs and like you know supplanting
Americans and like eroding National
sovereignty that's like it's obviously
like there's obviously a uh you know a
republican Trump's perspective on this
right like so I my main thing is is that
I think um many people on you know that
usually are more associated with my side
of things um have done a terrible job of
engaging with the right engaging with
workingclass engaging with Republicans
engaging with magga and stuff like this
whether or not we endorse their policies
um they people they're part of our
political process and we need to find
ways to work with them so if I could say
anything to the audience here is
whatever is like we need to talk to them
we need to work with them you know if
you are someone you know who voted for
Trump who L Trump that's okay like I
like we can work together why couldn't
we so the world right yeah exactly like
like fundamentally I think this is a
very bipartition issue like I don't
think anyone on the right or the left
wants Humanity to be replaced by robots
by AI so like from my extremely limited
exposure to like right- wiers and
Republicans I have found many of them to
be extremely reasonable you know I might
disagree with them on some policies on
some other things same way disagree with
some leftists on some apies so whatever
for the most part I found them to be
quite reasonable and like you can't work
with them but the important thing to
understand is just because someone is
reasonable or like has incentive that's
not sufficient it's why I'm really
emphasizing this idea of um the process
where like if you actually want to do
change you kind of have to understand
you don't have to understand but like
you know you should not be you should be
expect to be surprised unless you put in
the effort to actually map out the
processes if that makes any sense sure
sure and and what were your thoughts
around um the way it sort of went down
there was the the Trump assassination
happens and that same day Elon Musk
comes out and says I give him my full
endorsement and then I saw Twitter like
all of the accelerationist Twitter all
the tech proo
World in hours has moved hardcore over
to there so a concerning element of the
Trump Administration to me is just this
sort of Vibes of acceleration that uh
they picked up as they sort of got
closer to the tech community so my
biggest hope is that um it's not going
to be an accelerationist Administration
I think there are a lot of people in
Tech who want to accelerate who think
that might happen big Tech is one of the
most powerful forces in American
politics today the governor who vetoed
s1047 was Democrat um as again I don't
believe these simple stories basically
like obviously this acceleration just
push was coordinated like obviously
these people know each other they go to
the same dinners they or are paid for by
the same people um I think the
understand just propaganda is real the
conspiracies do happen um not Global
World spanning you know satanic Cults or
anything but like just like rich people
coming together to push policy that is
beneficial to them happens constantly I
think it's very clear that a lot of
these accelerations and so on benefit
from lower taxes uh you know no
oversight etc etc and will if they find
they think they have a lever to pull
they are willing to take that which yeah
I mean I understand like um
I think the important thing is just like
I don't again I don't think these things
are overdetermined is like
environmentalism used to be a right-wing
thing like Nixon created the NP EPA is
like um sure originally when
environmentalism started first becoming
a thing uh leftists criticized
environmentalism because they said like
oh it's just rich people who want their
nice waterfront properties to stay safe
instead of building factories for the
working class that's that was like what
people at least for a while in the early
20th century thought this totally
flipped now environmentalism is a
leftwing thing and like not and like so
like this accelerationism thing being
Republican code it is like just it's
astroturfing is I think the correct word
for this is like there's nothing
inherent about the Republican party that
makes them accelerationists the same
thing there's nothing inherently about
the Democrat Party that makes them
accelerationists it's just various forms
of faction alliances temporary or long
like yeah you know the Democrat is
funded to an extremely large degree by
big tech companies who don't want to
beated and so on and now the Republicans
are obviously taking massive amounts of
money from Big tech companies and
supports and
whatever as I say it's a process right
it's a process
I I think it's yeah I think these things
are not overdetermined like I think
there are just politics and War
happening here you know I think people
are fighting for power and this has
predictable or non or l L predictable
effects I think musk is like a
particular wild card he obviously does
care about Extinction risks to some
degree but also he says a bunch of
insane things that don't make any sense
to me whatsoever and just seem
completely wildely inconsistent so yeah
I model musk mostly as a wild card I
think he's capable of good things and
bad things and whether he does or not I
don't know I don't know what the process
is by which Elon Musk makes decisions
this is just truly opaque to me I'm not
saying there isn't a process like you
can't be that successful and not have
some process there is some mechanism by
which mus keeps winning if you believe
it's all dumb luck I think you know I
have a bridge to sell you like there is
a mechanism by which musk keeps winning
and the surface properties of these
mechanism seems like idiocy to lots of
people but I claim that can't be it like
there is something here that I don't
understand like if someone you know
plays a lottery and wins every single
time there's something going on right
it's like I don't know how he's winning
it looks like he's playing fair but like
something about how he plays is making
him win and me and me lose and like so
my usual bet is musk will keep winning
but I don't know what he's winning or
what he's playing for or how he will get
it um yeah and like kind of somewhat
similar with Trump trump obviously has a
process that works I don't know exactly
what the process is or exactly how it
works but clearly it works and it's
something I don't understand if I had
infinite time to spend on politics and
these kind of things I would probably
spend more time um trying to understand
these processes sure sure okay all right
so let's talk about tone in this
conversation uh about AI risk so one of
the things that I was just immediately
so heartened to um hear when I found you
uh in other podcasts and you know just
sort of you out on the internet was was
this fire and this anger of like [&nbsp;__&nbsp;]
this like are you [&nbsp;__&nbsp;] kidding me
like dropping the f bomb being angry
about it just you know a tone that is
connected to the subject matter and I
think even as I try to talk to people
about it in my everyday life you know if
I talk to a new person about AI
risk if I come him like holy [&nbsp;__&nbsp;] we're
all going to die I'm probably not going
to do very well with my introduction to
the subject matter for
them if I don't come in like that and
then they realize and eventually that's
where it is they're sort of like well if
you're telling me we're all going to die
why aren't you more upset about it so
we're in this sort of like rough spot I
think you have the tone really well
captured um just in how you you you know
sort of verbalize about it but talk to
me about tone your tone and the tone of
the rest of this conversation so I think
it might be useful for me to talk a
little bit about persuasion and
uh Communication in general like how do
I think about Communication in general
rather than about tone specifically it
kind tone kind of follows from this so
there is there's a couple of theories
about like how do you communicate
information to other people and I found
that most of them that like people tend
to use implicitly or explicitly or that
like written online or in books are just
wrong or like deeply misleading and so
like one example is that if you present
people with true information they will
just accept that and change their mind
this is not how people work if you
believe this you are wrong you can
scream and shout and kick your head legs
that like no but this is how things
should work but it is not and get over
it like this is not to say so then
there's a second thing where people when
they hear that this is not true they
then conclude oh therefore it's okay if
I lie this is also wrong like this is
even more wrong like just because
convincing people involves more things
truth truth so this people
they people don't if truth is not enough
then I won't even try this is also wrong
so the this is again kind of gets this
like mechanistic thing that I was like
kind of like like this you'll see this
theme emerge many times in my thinking
is like what is the process like what's
what are the algorithms actually being
executed on people's minds when they
change their mind and let me unfold a
couple examples to explain some of the
core intuitions I use when I think about
communicating to other people yeah um an
important thing is that human cognition
and general epistemology is a social
process aot for a lot of people this is
derogatory I don't mean this to be
derogatory I mean this to be descriptive
this means is is that let's say I tell
you a fact I tell you AI is going to
kill us and I give you some arguments
for this and maybe you're like bull [&nbsp;__&nbsp;]
all right that seems like a pretty good
point I'm going to go ask my friend Dave
you know he's super smart so like let's
see what Dave says and you kind of
garble the arguments to Dave you're kind
of like and Dave's like looks at you
stupid like you crazy like of course not
and they're like well [&nbsp;__&nbsp;] all right
Dave doesn't believe it so I must have
like gotten or something maybe it's just
not that big of a deal so does the thing
that you just did is this stupid I would
argue not really like I like I don't
think the person is making a mistake
here like if Dave is your smart friend
and you ask Dave and you you know do
your best to repeat the arguments but
like Dave doesn't buy it and then you
say like well I guess I don't believe it
either them I don't think this is a
mistake I don't think this is dumb I
think this is actually a pretty good way
to evaluate new information that you
might not be able to understand so yeah
a lot of people when they talk about oh
people just take the opinions of their
friends or whatever say this is super
derogatory thing like sometimes it is of
course but like I want to make put this
intuition that like this is often a very
rational thing to do it's like you're
not a tech guy you're not a maybe not
even like you don't maybe don't consider
yourself a very smart person or whatever
or you just want to run it by a couple
people right you run it by a couple
people and they all think it's stupid
it's probably stupid right so so it's a
very important thing where like social
processes matter a lot and you have to
model them as part of your conversation
let me give you an example of how if I
want to convince let's say you know we
have our our guy Allan we want to
convince him of AI risk right and he
talks with his friend Dave and Dave
isn't really convinced what's the what's
the thing I could do to improve upon the
situation for example I could tell Alan
simpler arguments that he's easier to
repeat so this is not necessarily giv
stronger arguments but I could for
example go on the trade-off curve and
pick an argument that's easier to repeat
so if he repeats it to Dave later he
repeats it more accurately to Dave so
therefore it's more likely that Dave
will think huh that is actually a good
argument and then it's more likely that
the feedback loop will separate so
having a mechanistic process shows you
different knobs different points on the
tradeoff curve of what are arguments you
pick you know not NE you don't
necessarily want to maximize give as
much information as possible because
people have limited attention they have
limited memory people forget things
right especially if you just have
conversation once if you gave Allan a
huge massive argument with many many
moving Parts you know he can't repeat
that today or even to himself he's just
going to forget half of the argument by
tomorrow so that's not necessarily the
correct strategy so yeah this also
relates to emotional veilance so so
emotions are signals they're signals
about reality they're signals about
yourself they're signals about other
people Etc so a lot of how you want to
think about signal about um emotions is
not whether they're like true or false
rational irrational you want to think
about are they useful are they're an
input signal and are they telling me or
giving me something useful or are they
doing the things I want for example if I
show you
something really scary and I say wow
look how scary this and you don't feel
scared that's signal like you're like
you should pause and be like wait hold
on why don't I feel scared are you
tricking me is there like something here
that I'm not seeing or if I tell you hey
check out this epic joke it's so funny
and you don't laugh and you're like well
I think you might be wrong like it seems
I'm I'm I'm observing that I am not
laughing so therefore this is not funny
and like you could say that's irrational
actually objectively this joke was funny
but like no there like there's something
here right pretty reasonable it didn't
work yeah yeah like so the same way if
you say it's super scary that AI is
killing everyone and you don't feel
scared that's a reasonable point it
doesn't mean it's true but it's
reasonable so I want to put that so the
way humans actually make decisions or
actually change their mind or actually
think about things is what I like to
think of like the bundle of heuristics
or bundle of patterns is like most
humans when they think in most scenarios
this includes you and me is that at any
given time you have a bundle of istics
or patterns that get activated it's like
I present you an argument and this
activates patterns in your head that
you've seen before can be emotions it
can be memories can be associations you
know it can be you know or just
literalistic for example and I say to
you AI is going to be different from all
other Technologies it will have nothing
in common it will change everything and
now maybe this activates a heris in your
mind that like well most things are the
same like all things equal most things
are very different other things so if
something is totally new that's
suspicious that feels bad and you might
not be able to verbalize this like for
most people this is hard to verbalize if
you don't like practice verbalizing and
like thinking and reflecting upon this
but people have this interest they be
like that feels off like it feels weird
for you to claim this and this can be
super frustrating when you're arguing
with people like this because then you
might say like well why do you feel this
way and they might not know they might
just have an intuition and a common
mistake that people will make is that
they'll get angry is like they'll get
upset they'll start yelling at the
person or tell they're they're being
irrational or whatever I think this is a
very counter indictive because then you
activate theistic angry people are
usually wrong or trying to hurt me and
if someone is trying to hurt you you
probably shouldn't believe whatever
they're trying to sell you all things
equal so so I think this is very
important to think about what are
thetics you are targeting so a lot when
I craft my arguments I I think about
what is the bottleneck what are theistic
that the other person or the other group
is using that are preventing them from
or like are bottlenecking them agreeing
to or thinking about this thing
sometimes it's specific her istics it's
just like literally I have to explain
why some things are different I have to
invoke the intuition like wow think
about the Industrial Revolution think
about nuclear bombs that was different
and they'll be like well you were you're
right you nuclear bombs were different
maybe it's like nukes okay then you know
the intuition dissolves and then you can
move on it's kind of like a puzzle game
right you have like multiple locks you
have to pick to get further and
sometimes it's just a procedural problem
his friend Dave just not listening to
you or like your argument is too
complicated it doesn't fit into
short-term memory and you need to find a
shorter argument or you need to be more
polite or you need to that's something
that you didn't know about and and
exactly like there's another thing where
like a lot of times you are arguing with
Dave you're not arguing with
and like this can be very hard to do so
as aete example of this I talked to like
a CEO of like a tech company doesn't
really matter who it was but um I was
giving them all these arguments about
like AI risk and whatever and they were
and they were listening along and like
they gave counterarguments and I
defeated those cter arguments and then
like so I just like defeated all their
counter arguments and then they were
like n no still don't n just don't buy
it and it's like well [&nbsp;__&nbsp;] this guy look
he's like he's so IR rational can't
reason with him and a good friend of
mine pointing out it's like Conor you
got this totally wrong this guy is a CEO
he's not a tech guy these aren't his
opinions they're someone else's opinions
go find the other guy and I'm like it's
strange but like okay so I asked so next
time I talked to the CE I happen to talk
to him the next day and I asked him like
you who do you listen to on like AI risk
and he's like oh this guy this guy and
this guy and I'm like oh [&nbsp;__&nbsp;] all right
those guys were the problem yeah so I go
like I tracked down one of these guys I
talked to him and he was super
reasonable and like I gave him the
technical cter arguments like oh that's
a good point yeah I didn't consider that
that's actually a good point and then lo
and behold I didn't talk to the CEO ever
again and but like later on podcast I
see him using some of my
arguments so the the algorithm that the
CEO was using to generate his statements
were not evaluate all the data and come
up with a thing it was take the average
opinion of these three guys and output
that so there's no point in really
arguing with the CEO directly you should
talk to these three guys so like all
there's a bunch of information sorry for
dumping everything all at once but like
these are several models and pieces of
how I think about people so for me it's
not about hitting some magical Cadence
or tone of voice or something like this
it's far more thinking mechanistically
it's like how can I help this person
like like this actually very I men this
a very important part for my thing as
well when I think about persuading
people or like giving information I
really come from this like how can I
help you like I really really come like
in my mind in my heart when I try to I'm
like I'm not here to hurt you I'm not
here to change your mind in a way you
don't want it to be changed because this
is a thing a lot of people are scared of
a lot of people are scared of having
their mind changed in ways they don't
want it to be changed this is very
common this is very very common and so
important way to be is like I'm here to
help I'm try to give you information
that is useful to you so a lot of people
I think also reject like AI risk is that
they feel like you wounded them it's
like I was feeling good you told me and
now I hurt like you're bad you hurt me
like you're a bad person you hurt me and
honestly yeah this is this is not a dumb
intuition like if someone comes up to
you and they says something to you and
you and now you're hurting or you're
crying yeah they're like that's a bad
thing you should be upset like that's a
you should be upset at that person
having done that to you this is a very
reasonable thing the question is how can
you how can you have those
Communications I had a I had a guy on my
show who's a regular person like me who
came on and was just like you know not
telling your neighbor that their house
is going to be bombed is not doing them
a favor yeah yeah and this is like the
big thing as well right is like how do
you balance between telling your
neighbor that they're going to get
bombed it's going to make them very sad
but also you're not doing them a favor
by not telling them actually you're
hurting them by not helping them and
this is like this is like a genuine like
and I think the way to approach this is
just like this idea of helping like when
I talk to policy makers and so on like
like one of the things that like often
like I remember like once I was talking
to a policy maker and I took like a
whole hour I was like and he asked me a
bunch of like super basic questions he
didn't understand computers at all was
like an older gentleman and he talked to
it and it's like and at some point he's
just kind of like he just he didn't say
these words but like paraphrase he's
just like at some point he's just like
holy [&nbsp;__&nbsp;] you're trying to help me like
no one was answering my questions but
like you're just sitting down and
answering my questions and I'm like yes
sir of course like I'm happy to and he
was so shocked he was so shocked that
like I was like cuz he was also ask
questions about non- AI things like he
asked about like crypto and like like
what's a quantum computer or whatever I
just like sat down and just patiently
answered his questions and he was like
wow like like thank you like no one is
taking the time to answer my questions
and here you are just answering my
questions you're not even asking
anything return I'm like yes because I'm
trying to help so I think this is like
so if I had like one thing that is like
the way I want to communicate to people
as if I had like one unifying theme is I
want to help like I'm actually trying to
help you you might disagree with me you
not not like me even if I argue with
someone
and I say you're wrong or you're bad
whatever I'm trying to help I'm trying
to help you be less you
know hugely helpful idea just to have in
mind I've not had a um like a unifying
theme like a mantra you know to be like
in these conversations I sort of start
all over the map try to sound them out a
little bit ask them a few questions you
know where H where are you at on AI what
are your thoughts about Ai and just sort
of see sort of where they're at but the
attitude of like I'm here to help you
understand this moment we're in wherever
you're at um I think is really really a
big idea yeah one of the things that I
of this whole thing that just made me
crazy and just makes me crazy to this
day is like eight billion humans on
earth like 2,000 people or less few are
doing this
work um it's like there's a big party
going on eight billion people are in a
party and everybody's having drinks and
laughing haaha and then off in this
corner there's this group a tiny subset
that is doing this very dangerous thing
and nobody at the party knows about
it and no none of the guys in in the
little little 2000 person bomb room at
the party believed they had any reason
or obligation to ask for the consent of
the other people at the party can we do
this um at the party is that cool and
it's just happening and the party keeps
going how is this possible so the truth
is that this is a default I mean this is
how
crime happens this is how Wars start
this is how conspiracies happen like
this is how neglect happens this is kind
of the default state of humanity and
like of harm
um
the like the way I think about like how
Society should be liveed is that I don't
want a like I want a society where most
people can live their life as civilians
and not have to worry about this like
that's nice that I don't have to
actually really consider military
strategy in the Middle East because like
there are people whose job it is to
solve that for me so that I don't that
doesn't affect me right you know whether
they do or not or totally different
question right but like there are
professionalized militaries and
politicians and Diplomat whose job is to
make sure that I can [&nbsp;__&nbsp;] off on the
couch and play video games and not have
to worry like and this is very beautiful
to me like this is something that's very
very deeply beautiful to me I think I
want everyone to be able to party as
much as possible and not have to worry
about things so when is this when this
falls apart is if there is no one or if
there's a due group of people or someone
who Escape you know law enforcement or
Norm enforcement or you know risk
calculations like and this is exactly
what we're having right now so what's
happening right now is the same thing
that happened with big tobacco where
when you know you know some people start
noticing well these lung cancers seem to
be connected to smoking and and the big
toac companies were of course like
no what what oh wow oh no that would be
so terrible oh we should fund lots of
cancer research so we can we can find
more data but we don't want to act too
hastily of course not let's um you know
let's uh be careful here there's cont
controversy after all you know we have
to do this let's let's let's get a
committee together let's get a committee
together here all sides you know it's a
so I think this is kind of what's
happening right now the the category of
things that we're seeing right now is
that there is um a you know group of
people or various groups of people
actually which we can talk about I tend
to CL class them into five groups of
people who are trying to build AGI and
they're kind of building it for slightly
different reasons um but for the most
part it comes down to power is that they
you know you want Power you want money
you know if you believe that AGI is as
powerful as I think you and me think it
is and are delusional enough to believe
you can control it then uh well you
should be willing to do just about
anything to get it and that's exactly
what's happening so so they've dug up
the big tobacco Playbook you know
they've you know propaganda
misinformation uh you know slowing down
of the of scientific process and
civilian process and so on to buy the
time to externalize this risk onto the
rest of society this is the same thing
that happened with you know chemical
companies it's the same thing that
happened with asbest it's the same thing
happened with big oil it's the same
thing like it's the same pattern just at
like a much larger scale sure and and
what's required in all those instances
is a lot of people dying right the bad
stuff has to kill people before the
response comes so this is usually the
case but I don't think this is
prescriptive I think this is descriptive
so for example let me give you an
example of an extremely complicated
controversial problem that we did
recognize it didn't kill anyone and we
did solve ozon
depletion there was a crazy thing where
in the 60s it turned out that we saw hey
something's going on with the ozon it's
going away what the hell and then this
like think about how hard this was to
determine we're in the 1960s 1970s like
barely have computers you know we're
like we like barely understand the
atmosphere and like how things work some
we first of all detect there was no in
lay at all it was only discovered like
10 years earlier or something like this
so like or like really properly measured
like a couple it's actually this crazy
thing where it's actually a miracle that
we discovered the proper me measurement
of Oz like just around the same time as
we invented per fluorocarbons which
where the chemicals destroying the ozone
layer if we had developed like per
fluorocarbons just a couple decades
earlier we would just not have an ozone
layer anymore and that would just be it
and it wouldn't come back it would take
thousands of years to recover so there's
this really crazy historical coincidence
like imagine you're the 19 60s right you
like have this like kind of
controversial like new Theory or
whatever you know you you're like one of
the very small number of atmospheric
scientists who are studying this and
you're saying like wow it's going away
and now you have first find out what the
thing is making it go away which could
be anything you it's the 1960s you know
1950s think about all the chemicals
going into the atmosphere right so
finding the right chemicals and wow
turns out they're an extremely useful
chemical that many companies use and now
you know Dupont and whatever starts
lobing against you which they did I
think a lot of people forget this is
that a lot of people don't know this but
historically the companies like dup and
like other chemical compies lobed
against them and said this is fake news
this is fake science this isn't real
like this is all fake blah blah blah and
so it's actually a [&nbsp;__&nbsp;] miracle that
the scientists and the activists and
whatever in such a short time yeah were
able to convince the general public and
policy makers of a completely new thing
like an ozon layer what the hell is an
ozon layer you know convince them a
completely new thing convince them that
is being heard by some obscure chemical
they've never heard about and then also
that that the you know well-funded
lobbying companies of the chemical comps
were pushing against them against and
that they got the like Global
International regulations like if we
stop with our uh spray silly spring stay
hands China will make all the TR can
exactly it's and like so this is like
for me like such a historical like
anomaly like such an incredible thing
I'm like holy [&nbsp;__&nbsp;] like this is like the
fact like in other worlds this didn't
happen like yeah we take this for
granted like of course we fix the o on
but like no this was not obvious that
this would succeed so it's an incredible
example of what this can look like if
you do it well it still took like many
years to do um so like it wasn't easy by
any means but it did happen so we have
an existence proof like you can't of it
was there any you know and looking back
at it like how uh how did that thing
happen without without bodies piling up
Global coordinated response it's a great
question and I'm currently in the
process of reading more history books
for exactly this reason because I've
been learning so much about I'm like wow
there truly is nothing new Under the Sun
like I I used to people like historians
or like old people used to tell me this
and I'm like yeah sure but like the
internet exists now so everything is
different and now I read history books
and I'm like man like we're we're really
not that different you know some things
are different but like it's amazing how
many things are just not different um
with ozon there's many things you can
point to is like uh government capacity
was much higher in the 50s and the 60s
it's just governments took things
extremely more seriously especially the
US government um there was much stronger
I like us hemony in various degrees I
mean except you know Soviet block but
like um I the fact the Soviets like also
didn't produce peroral carbons is kind
of like a you know if they just decided
no we need per carbons for you know the
glory of the Soviet Union what the [&nbsp;__&nbsp;]
would we have done right so like you
know um I do think that there is one
thing with ozon though that they got
extremely lucky which is that it turned
out it was relatively inexpensive to
produce chemicals that are safer like
this was a that chemical companies once
they put in the research to develop new
chemicals they found new chemicals that
could do what they wanted to do and were
much less dangerous this was quite lucky
if this was not the case I'm not sure
how it would have gone like at least it
would have been much harder if for
example chemicals have been like
massively more expensive or just like
non-usable than things could have gone
differently we got
Lu and so the parallel there is like
there was an easy technical solution
there and here yes ex not have an easy
technical solution yes I think AGI is in
many ways harder than ozon I think in
some ways it's easier than ozon like uh
I think ozon is much more confusing than
AGI like you know things smarter than
you are that we can't control are scary
is a completely basic intuition that
basically the entire human species
shares like you have to have a degree to
not believe this like like you have to
like really sumersault to not have this
intuition of just like all things equal
if there's something it's smarter than
me and I don't really know what it wants
or how to control it that seems like a
bad situation it's a very very deep
intuition and this is a it's a good
intuition it's a so this is something
that for examp with ozon we you're like
oh there's invisible Rays coming from
the Sun that reflect off of a weird
chemical you've never heard of that's
like in the atmosphere but it's a
different atmosphere you're like what
like like those on things like quite
confusing actually from like a technical
perspective um so I think it's we have
some easier thing but yes the harder
part is is just it's extremely expensive
to solve turns out solving the ozon
crisis it wasn't cheap but like it
wasn't can of you know it wasn't putting
into P out of business right like they
could absorb the cost it was fun um yeah
but with AGI
like a a a a new phrasing I've been
using recently is I think when people
talk about oh we're gonna solve AI
alignment you know we're going to build
AGI ASI you know I think people really
don't realize how big of a statement
that is like saying you're going to
create aligned ASI that you know fixes
all problems is equivalent to saying I'm
going to take all the problems like
interpersonal problems that people have
with each other all economic problems
all disagreements all scientific
problems all business problems all
problems that are institutions or
corporations or governments our
International bodies tring take all of
those problems and then I'm going to
solve them with software and not have
any
books like that's what there's that's
that's what that means and I'm like
you're crazy like if you think we can do
that like in a generation even or like
you know with like small amounts of
funding I'm just like that's not how
real software works like that is just
like I'm not saying it's imp possible
right like I think like physically could
you design software that could do that
yeah probably but like doing that with
like no bugs and like resolving all
these like really hard coordination and
like value question infinite time for
forever right like that you like I'm
like like holy [&nbsp;__&nbsp;] like what the hell
are you talking about so like when
anyone tells me like oh yeah we just
need to solve alignment within this
amount time like what the hell are you
talking about like right get out like
that's not how these things work I I was
recently talking to someone for example
who were like I find it quite unlikely
that you know alignment is so hard that
it requires more effort than the men
Manhattan Project and I'm like dude the
Manhattan Project was about $35 billion
in 2023 that's not a lot of money like
in like three three or four years I
don't think you can solve all problems
on Earth with three to four years and
$35 billion but if someone would like to
try here's my crypto addresses I'm
joking Roman yampolsky uh strongly
believes that linan is impossible to
solve on an infinite time scale that
that at some point it gets away you just
can't control an ASI forever yeah what
are your thoughts infinite time scale
possible you know I love Roman he's a
great guy um but like I think these
things are just relevant to reality this
is kind of like risis theorem so like
risus theorem is a mathematical theorem
where like you can't prove anything
about arbitrary programs like and this
sounds like super like oh wow [&nbsp;__&nbsp;] we
can't know anything about computers but
obviously this is [&nbsp;__&nbsp;] like
obviously you can look at a piece of
code and know things about that piece of
code but of course R the only says over
there is there is some program you can't
understand and I'm like short in all
infinite programs yes there are some
programs that just cannot be understood
I will not write those like I will
simply decide to not write those right
and so your point is it's just sort of
irrelevant like like infinite time is so
far down who who gives a [&nbsp;__&nbsp;] yeah it's
just like these like formal models and
stuff like this are just like it's very
deceptive I feel to to take from this
like super super I don't know know
exactly Roman's model here specifically
but I assume it's extremely abstract
it's some like abstract Markoff chain
[&nbsp;__&nbsp;] you know like oh well if there
are you know exponential many states and
you have polinomial bit control or
whatever and I'm just like Bro Look
let's solve the Civics problem first
let's solve politics first like if you
like if we just get to a world where we
live you know where we build software
and government institutions well enough
that we can just like live like in a
safe nice human world for you know
couple more Millennia I'm like already
pretty damn happy yeah oh yeah hell yeah
yeah um no that would be great let me
ask you this has you hear different
things about this has any progress ever
really been made on
alignment so overall yes yeah on less
wrong not clear I mean I'm joking but um
so like is the problem you know have we
made any traction any inroads so
I sometimes like to joke that the first
alignment researcher to ever live was
gotri liik in the 17th century I think
he was the the first like person to to
make progress on the alignment problem
and so lii was one of the first to uh
think about you know formal logic formal
logic and um algebra so uh if anyone
wants to read a very fun historical
document um he Li uh doctoral thesis uh
suggestion like his application when he
was like 22 is epic it's like he
basically says he his his suggestion for
his research program was that he would
create a philosophical language through
which all statements can be encoded in
numbers so that if there was ever any
disagreement between two philosophers
they must merely say let us calculate
and of course he would then use this to
formalize the will of God and then I'm
just like what a guy like this is like
1670 right and I'm like hell yeah hell
yeah brother like what a what a guy you
know like obviously didn't quite work
the way he hoped to but like hell yeah
my guy you know like this is a my man's
trying I I love historical I love
historical figures where I can see like
they're really trying you know not
whether they succeed or not not that
light wasn't extremely successful in
many ways but like I love seeing stuff
like this and me like my man was really
trying like he was not [&nbsp;__&nbsp;] around he
was trying to to he was trying to bring
God to Earth and bring his kingdom with
math what a guy
so formal logic is progress towards
alignment in the sense that before
formal logic I think people don't
remember this there was not formal logic
like we didn't have good formalizations
of what it meant for an argument to be
good or true you know like like
scholasticism and like you know early
you know medieval or like ancient Greek
forms of like you got like Sy like aroan
logic but a lot of it was informal or
like not well formalized a lot of it was
not algebra and then later and like you
formal logic and algebra directly led to
Computing Theory you know like uh
turing's you know original thesis came
through the
Enid's you know Hilbert's program and
you know formal mathematics and algebra
so like proof Theory so like there's
like this like beautiful connection of
like
formalizing and like being strict about
what we mean like like when we say this
argument is true what do we mean by that
like what does that actually mean like
this seems like a huge tangent I think
this is actually very important is
because I think there is a mismatch
where people because of various
historical you know coincidences
formalize alignment as this weird
esoteric problem that like doesn't have
any connection to other fields of study
that is like completely novel that kind
of sprung up denovo you know around Ai
and I just like totally disagree it's
like alignment is like The Logical
conclusion of all problems it's the
question of how you
formalize and execute and like process
human desires and wishes onto physical
reality and there are many ways you can
do this right like and Engineering is a
sense of how can we have processes
through which we can describe an object
we want to create and then a process
through which we can create that object
that has the properties we want it to
have um language design programming
language design like I sometimes joke
that um implementing a slightly better
version of C++ is closer to actual
alignment research than almost anything
done un less wrong because programming
languages are fundamentally artifacts
and tools through which to allow humans
to express complex Properties or desires
or wishes and then compile them down in
such a way that you can get a computer
to actually execute that actually do the
thing you wanted it to do so from my
idio sycratic perspective I think that
like alignment research like you know
long-term alignment is like in a
sense going in the direction like should
go in the direction of thinking about
like an extremely complex programming
language or extremely powerful com
programming language like what would it
mean to be able to express complex
properties about reality like justice in
ways that can be like compiled that you
can compile Justice to atoms because
this is what we're trying to do right
like we're trying to get this concept in
our head of like justice and we want but
we want this to exist in atoms Justice
is not in atams it's somewhere out the
platonic realm how do we get from
platonic to the atoms and this is comp
dation and computation so it's The
Logical conclusion of philosophy
engineering computer science like this
is really what is so Yes actually I
think we've made a lot of progress on a
lot of these things we have formal
theories of computation of programming
language design we have made we have
Game Theory like man game theory didn't
exist like a hundred years ago right
like they hadn't even formalize any of
these ideas bargaining Theory and so on
like there are many many strands where
we have made progress on the problem and
they're just it's just a massive problem
you know it's just a really really big
problem that is wild you're saying that
we have been working on the alignment
challenge for hundreds of years and much
of what humans have been doing is all
pointing in this direction of trying to
figure out how to figure out this how do
I make stuff do what we want it to do
that's always been the thing right it's
just like there are things in my head
you know there is Justice and love and
beauty and whatever and there's all
these atoms around me how do I make
these Adams it be Beauty and love and
kindness and things like how do I make
this happen this has always been the
core human thing is like how do I do
this how do I make this happen and how
do I make more of it happen this is a
this is engineering this is science this
is like the whole thing of like how do
we how do we configure reality into a
reality we want to be in that we want to
live in like this is the fundamental
thing and like I feel like you know you
can restrict alignment problem to like a
specific idea of around like Ai and
stuff like this but I think these other
things are things we should learn from
and are like part of it like if you have
never learned how programming languages
work you're missing out there's a lot of
great ideas in there that are very
important for thinking about how to
control AI systems I happen to work on
some of the overlap between programming
language design and AI Control there's a
lot of fruitful things there if you
never have read a book about Game Theory
or you know bargaining Theory or or
economics and you're thinking about AI
alignment you are missing out there was
a lot of relevant stuff in there will be
useful for you yeah is alignment Like An
All or Nothing thing like like can you
part you know could you input just
something as simple as like don't kill
people we you know harm physical
violence is bad but then it would be
totally igned in other ways like is it a
click it took the sum total of it or is
it little small pieces that have to be
input the way I see things is is that um
one of the biggest problems with AGI is
it has a criticality property um so
criticality property is in nature is
that you have something that looks
linear like you know goes up like
linearly and then suddenly it changes
like it goes with some other strict the
classic example is like urani like
fishing so like if you just have a bit
of uranium you know has like this much
radiation and if you have like twice as
much uranium it's like roughly twice as
much radiation but then at some point if
you get enough uranium it goes critical
and then the radiation goes like super
up and or like explosives right like you
know if you have like a little of
explosive you know it's like whatever
but if you light one of it on fire then
it lights two of it on fire and four of
it on fire and then you know the whole
thing goes up all at once so AGI has a
criticality property or intelligence has
criticality property or at least s damn
sure seems to have one which is with
humans is that you know a million years
ago you had chimps and you had humans we
didn't look that different you know
maybe we're we stood on our legs and had
a bit less hair or whatever but like
from a million years ago perspective you
know before we like even further back
before we had tools right even when we
have like chimps have tools too right
like bad tools but like you know they
can use sticks or whatever like we go
far enough back common ancestor whatever
and you had to guess you know all right
you know we made this new chimp it's a
bit hairless looks a bit goofy what do
you think this new chimp's going to do
you might say like you know it's a bit
smarter maybe it'll throw rocks more
accurately or maybe use like you know
slightly more complex sticks or
something like that seems pretty
reasonable right but no that chip goes
to the moon and builds nuclear weapons
that's a criticality so there was
obviously a critical phase transition
between shimps and humans we don't know
exactly where or why or how but
obviously there was a criticality
threshold cross between humans and ships
which is at some point humans could
compound at some point humans could
develop language they could develop the
scientific method they could compound
language reading writing you know and
just like build industrial society and
economies and so on and this didn't make
them like twice as good as chimps or
three times as good it made them like
billions and trillions of times better
than shimps and this is the problem with
AGI as well is a criticality properties
make things all or nothing it's like
this is one of the way nature like
nature usually doesn't like all or
nothings but like criticality is like
how you can get all or nothing where if
you build a thing past the criticality
threshold it's like like imagine you
have like a slope right and like you say
like you know it's going up it's going
up it's going up and then it crosses a
threshold then it goes boom so either
you've already solved the whole problem
or you're
screwed
so in a sense you have to solve
everything all at once you have to
before you reach the criticality
threshold you have to know or at least
understand the criticality as properties
enough or the smart thing is you make
sure it's not critical so how do you
study uranium if it goes critical well
one thing is you don't allow it to go
critical you know you separate it in
various tanks that have special shapes
and you put control rods in there and
you like are very very careful that it
never goes critical in ways you don't
want it to and so on and yeah this is
what we should be doing with
intelligence as well is my claim my
claim is is that if we want to do
alignment we have to do it without AGI
before AGI that's extremely hard if
you're trying to get to AGI as fast as
possible but we can buy
you can delay AGI like this is why some
people who claim like oh we can't make
progress on alignment before AGI I just
so strongly disagree with this like I
think this is just like yeah just super
wrong like as just described like
there's so much that we can learn and
that we can do when we think about you
know better and better whether it's
social and political theories
programming language Theory you know uh
interpretability of you know things like
this formal methods for program
verification there is an endless list of
research that we can do long before AGI
that can then you know maybe as we
develop a better reaction vegetals right
as we develop like better
interpretability controlability methods
then we can maybe inch our way up right
and maybe we can like you know practice
or like you know if we have super high
security right like there's a reason
that private citizens are not allowed to
have visionable amounts of uranium is
because they will obviously kill
themselves and other people like
obviously so and we should not allow
that so there super strict standards
where like if your lab has a license to
carry fishable amounts of uranium it has
to have extremely standards that none of
it gets out it's always stored in the
right vessels and whatever right we
should have the same reality obviously
like this should obviously be the case
you're not allowed to just have weapons
grade or AI just laying
around yeah yeah and is it your model
that
um once we pass the AGI threshold
alignment would be impossible
I think it's just Humanity seases to
exist briefly afterwards it's like yeah
it's just over like once you have a
superior species that is like doing
science at a much faster rate than you
better politics business persuasion
everything like to be clear this thing
is not just good at science right this
not like a little like nerd in a box
right this is a yeah hyper linguistic
you know hyperverbal thing that can
persuade people and like come up with
great speeches and do politics and
business and beat the stock market and
you know runs on you know any computer
connect to the internet necessar
potentially um like I recently saw a uh
a s like a calculation that like um
creating um and training modern Frontier
AI systems is about like the the square
of the amount it takes to run them so
with modern systems you could
potentially like if you have like gp4
gp5 after it's trained you potentially
run millions or even billions of
instances of it on the same hardware and
so imagine you have like a million
superhuman systems that never get tired
never get bored you know they can share
all information with each other they
don't bicker you know the way humans do
and anyone who's trying to run a million
person organization know there's a lot
of politics imagine if you could just
get rid of that and just everyone works
together because they all have the same
brain you know so you could make a super
super powerful thing and then if such
thing exists and it doesn't specifically
optimize for humans to like have a good
life and continue it will just treat us
like ants and we will just be you know
just pave over with like you know
parking lots or whatever you know who
knows and you know if you're if you
found out the ants in your kitchen had
access to nuclear weapons you wouldn't
want the ants to keep those would you so
I don't think the AI would want us to
hang on to our nuclear weapons for very
long so I think things would just be
over quite quickly at that point it's
not that it could wouldn't be solved
it's just like it's just like you're
you're already on the Dooms Day timer
then it's just like you know might as
well
relax wow yeah so we're at a really
interesting moment in this I think where
we have Sam Altman coming out this week
saying AGI is coming in
2025 Dario came out and said it's coming
in 2026 and then there's people like
Gary Marcus out there talking about the
scaling laws breaking and it falling
apart and you know Altman's basically on
a on a ghost ship going to nowhere and
and he you know llms aren't going to get
us to AGI and it's all [&nbsp;__&nbsp;] and he's
just selling stuff to keep the money
coming in which leaves someone like me
like I I have no clue what um you
know what are your thoughts on that are
we close to it are we not I mean we
close like how close we are we can we
can argue I mean Sam's been um recently
I don't know if you've noticed this but
Sam has been recently much more koi
about his definition about AGI he been
like oh it's kind of the friends we made
along the way you know much more than
you know Ultra you know human replacer
machine which he was more explicit about
in like earlier essays he wrote so it's
a I think that's a deliberate PR move um
that's been happening there and you know
with Gary I'm like you know I like him
I've talked to him many times he is a
good guy but like I don't know why he
believes the things he does about llms I
don't understand it doesn't make any
sense to me um seems like he constantly
gets you know invalidated over and over
again he seems like a good guy don't
know why he believes that no no hate um
but the thing I think is the most
important thing about this is that none
of this actually
matters so to everyone listening and to
everyone who will listen to me I think
the most important thing to take away
from you know he says this he thinks
this is that none of this matters
because every single one of these people
signed the case letter they all signed
the the statement that the mitigation of
you know risks from AI should be uh
should be International priority along
with other catastrophic risks such as
nuclear and pandemics so none of this
matters it's literally doesn't matter
whether Dario thinks it's two years Sam
thinks it's one year they themselves
have said that what they are building is
as bad as nuclear war so they shouldn't
be allowed to who cares like it it
doesn't matter like if they say it's 5
years away so we can spend three more
times building super Ebola you just say
like no stop building super Ebola now
this should be legal for you to build
super Ebola doesn't matter if it's going
to take your scientist 5 years to figure
out super Ebola you're not allowed to do
that stop right now so this is the
actual thing this would be focused on I
think there's a lot of distraction that
happens which is again I would highly
recommend any viewers to read some
history books on big tobacco and the
strategies they use there even this
Wikipedia article on the big tobacco
Playbook where the thing you want to do
is you want to create controversy it's
not that you want
to defeat your opponents or you want to
like argue against them because that
might make them look that might make
their arguments look too legitimate you
just want to spread confusion fear
uncertainty and doubt fud that's the
name of the game the name of the game is
not to persuade it's just to waste
everyone's time and just to be confusing
to say it's controversial who knows you
know I am who you know he says this he
says that who knows I guess there's
nothing we can do this is the strategy
that gets played the way the fud
Playbook works is if you benefit you as
in the corporation benefit from the
current status quo of nothing happening
then you just want to waste everyone's
time and confuse everybody because when
people are confused they just don't do
anything they just go along with what's
the default so the thing that we have to
do or that that has to happen is not
find out the exact moment AGI comes and
make sure we have policy by then that's
wrong rather you have to flip to default
action it should be like all right maybe
it's in one year maybe it's in 10 who
knows who cares it's illegal
ww and so like and then we will make it
unlegal once you can prove that it's
safe like this is what the obvious thing
to do is like okay your company comes to
the FDA and says I want to build
superola and they're like okay how do
you make that safe and you're like I
don't know we'll figure it out it's
going to take five years for our
scientists to invent it anyway and then
the FDA is going to say like no come
back when you know how to make that safe
and we'll reconsider it and and then and
then it's like ww it's like yeah like
you don't get to build super Ebola if
you don't know how to make it good save
it's not the other way around it's not
you first build super Ebola and then
figure out how to make it safe like
that's not how these things work this is
and so important to not lose sight of
this because this is what the propaganda
is trying to make you lose sight of is
that it doesn't matter when it comes
it's we know it's coming we can just
stop right now and say look if tomorrow
open alignment team in so far as it
still exists comes with an amazing
proposal that like beautifully solves
like all these problems and it has a
sign on from like all these
International governments who all
support it and all the greatest
scientists have signed it I'll be like
all right fair enough you know all right
let's talk
we can talk about it but that's not
what's happening right now yeah what
what is the Playbook on beating fud like
what is the uh tobacco because tobacco
did lose pretty you know
uh catastrophically for them in the end
like they they eventually it took a
whole long time any lessons from how
they went down so as I say I'm reading
many history books nowadays and I
recommend any readers who want to or
listeners who want to read some books
and write write up some of the lessons
and share them you know widely be very
helpful if also anyone has reading
material to recommend to me please send
it my way my main thing I've from so far
I am not a historian by any means is
that it's actually quite hard this is a
really good playbook this is why it
keeps getting used because it's
genuinely very good like this is a very
strong strategy to play is that if you
have the lucky state of benefiting from
the default scenario and you just have
to prevent change this is way easier
than causing change causing change is
way harder than just preventing change
from happening which is what f is all
about um so the tobacco Playbook is all
about so we're already The Underdogs
here like we're already like things are
already stocked against us I just want
to be this is not to discourage it's
just to be clear We Are The Underdogs
like there are these massive
corporations playing unfair you know fud
playbooks with massive amounts of
funding massive amounts of government
control and whatever and they're playing
it against
you know the American and the
international like like everyone at our
expense so we are The Underdogs here
like we are the people being attacked we
are the people being you know um and we
need to fight back I think there are
some things that we can learn from this
um I think one of the most important
things that we have is just like we have
history and we have examples and we can
make things legible is that the the um
the ban of the fud strategy is
legibility like their whole strategy is
just spread uncertainty it's not to
convince you of the opposite it's just
to be like you're like uh do I I heard
that but I also heard that so like I
don't really know what to believe and
whenever you feel that you need to be
like hm hold on hold on like this is why
I focus so much on the it doesn't matter
thing the way you win so have you have
you ever heard about simoc
levels so this is a concept um
originally from Continental philosophy
God forbid um um and it basically goes
like this it's like different levels of
how speech and symbols can relate and
speech acts can relate to reality and
Truth level one is truth I tell you
something true trying to make you know
something that is true for example I
tell you there is a lion on the other
side of the river I checked earlier
there was one and I'm just telling you
so now you know there's a lion over
there that's level one level two is
lying I saw there's a lion over there
and I tell you there's no Lion on the
other side of the river don't worry
about it I'm lying to you I'm trying to
make you believe something which is
false level three is group affiliation I
tell you there's a line on the other
side of the river just because I'm part
of the lion party you know me and the
lion guys like we love lions lions are
cool don't mess with us you know we all
believe in Lions right doesn't matter
whether there's a lion there or not it's
just a thing we believe because we're
the lion
political party or whatever right and
level four is power or in other words
[&nbsp;__&nbsp;] it's just you say whatever the
[&nbsp;__&nbsp;] you want in whatever order who
cares just like whatever Vibes well like
you say oh yeah there's a line there
because like you like The Vibes of lions
they're cool so like yeah you know I'm
cool so I believe in Lions because I'm a
cool guy this is like level four
so a lot of politics people think
operates LEL two it's like you're being
lied to by politicians but very often
it's level three and four is like very
confusing to people it's like often
people say things that are false not
because they're trying to lie to you but
because they're trying to say I'm part
of the group that believes this or they
don't give a [&nbsp;__&nbsp;] and they're just like
saying whatever you know just say the
word jobs in every speech you give right
whenever you give a speech say we're
protecting jobs are they actually
protecting jobs who cares it's just a
Vibe like it just feels good to say
we're protecting job so I'm just going
to say that I'm part of the I'm the I'm
the job protector guy so I'll just keep
saying that whether it's true
irrelevant so sure a lot of our politics
and a lot of these conversations are on
level four so when like big tobacco says
there's controversy what does that mean
it doesn't mean anything just a Vibe
it's just like you know I feel like
there's a bit of controversy Vibes here
like we scre up uncertainty here so like
I think you know we should be be careful
here doesn't mean anything there's not
there's no objective measure here it's
just like something who cares right and
they'll say whatever right like they'll
pick up studies from wherever they'll
say whatever like literally who cares so
how do you defeat this so the usual joke
is that level two beats level one Liars
have more things they can do than truth
speakers so it lie very useful level
three be level two because if your party
is bigger it doesn't matter how clever
you are because you know I'm the bigger
group level four beats level three
because I can just align with any group
I want I have zero loyalty who cares and
level one beats level
four so the reason level one be level
four is because level four is just
incoherent it's just like you lose
contact to reality and also people hate
it like people have this visceral feel
like you know this feeling when you see
someone talk about like postmodernism or
like you know everything is just
subjective man you're like shut up like
like you know or you he like a
politician just like say words that
obviously don't mean anything and you're
just like shut up like I hate you yeah
like just like you know you're not even
lying to me you're just saying sounds
it's not even that you're lying to me
like there's a lot of that there's a lot
of that these days we're just like that
that thing is very important thing
understand a lot of the things that
politicians like say it's not lies it's
just noises like it's not even that
they're lying to you it's just they're
just like samee like like jobs cool yeah
like we like jobs right guys hell yeah
jobs it doesn't mean anything it's just
like whatever not everyone not in all
circumstances some of our politicians
are great people doing great work and I
thank them for their for their service
but you know there are some you know so
the way you get out of level four cuz
like level four is a death trap it's
like as you go higher and higher up to
level you get stuck because then like
once you're using level four the other
person starts using level four and now
you're both using level four and you
can't get out of it and everything goes
to hell everything just goes completely
nuts like there's like you know I mean I
won't reference any specific political
movements but there are many political
movements that have had this exact death
effect and the way is is just to focus
on what's real it's just to take the
smallest simplest thing that like where
there's no ambiguity there's no
interpretation there's no postmodernism
there's no whatever just as simple as
possible and just keep repeating that
and be like yeah but what about that
like sure sure sure like all these other
things say okay okay but they signed the
letter what does that mean excuse me
speak directly into the microphone they
said this is worse than nuclear war what
does that mean please speak directly
into the microphone this is what you
have to do and you have to do this to
yourself you have to to yourself every
time you you see like you know some you
know anthropic thing saying like well
here's a [&nbsp;__&nbsp;] post of a report about
safety responsible scaling blah blah
blah just be like okay but is it worse
nuclear war and if so stop like I don't
care what your report says like I don't
even have to read it it's like right is
it as bad as nuclear war if so stop I
don't care about your report and I think
this is the winning strategy is just
focus on the basics and don't let people
Galaxy brain you like don't let people
confuse you don't let people trick you
just repeat over and over again okay
yeah but they signed the Lether okay
yeah but yeah
but I think that letter was a huge thing
in the history of the I hope like I
point to it all the time and it's like
mitigate the risk you know it says
mitigate Extinction risk like if we're
not if it's not what are we mitigating
if it's not the extinction and what's
Extinction if it's not Extinction like
the words are clear exactly and this is
the thing I think just has to be
repeated over and over and over and over
again and this is how truth wins truth
wins by just like not giving into the
stupid GES not arguing about well but
what do you really mean by AI like I
like I don't care the thing that use
there's this thing Extinction you know
like I don't care like yeah but like
like okay yeah but okay yeah but yeah
but like if we make things that are
smarter than humans that's bad let's not
do that and like so like are you
building things smarter than human and
like well what do smarter really mean
and like I don't like stop like you just
have to like not get trolled like a lot
of like fud is like like the fud
strategy is what in to a large degree
what in the modern world we call
trolling where you just like waste
people's time and Trigger them and just
like annoy the [&nbsp;__&nbsp;] out of them and just
like waste their time or just like
create massive [&nbsp;__&nbsp;] posts that are like
really hard to like like some people
will sometimes be to me like they're
like well but anthropic you know said
you know that they'll do good things and
I'm like okay what's their plan and then
they post a massive [&nbsp;__&nbsp;] post about rsps
and I'm like all right where in that
does they say how they stop xris like
well you know read it I'm like it's 50
Pages like okay where does it say that
and they're like well it's in there and
I'm like okay where and they're like oh
read it okay and I read it and there's
nothing in
there yeah like this happens over and
over again this happened to me so many
times with not just anthropic I'm
picking on anthropic but like also like
open AI deep mind I've read all of their
you know responsible scaling safety
plans and whatever and I'm just
like okay yeah but like how do you
prevent Extinction like it's not here
like it's just not there and like is and
so okay yes but you signed the lever so
shut up like who cares like I I don't
care about your huge [&nbsp;__&nbsp;] post here like
just like stop or like yeah and through
this just of course they don't want to
stop this is important thing to
understand the reason anthropic and open
AI are not stopping is because they
don't want to like this is very deeply
they don't want to they want to build
AGI they want to build AI like a
childish petulant like I you you can't
stop me like like um I mean it's just
their utopist they think they can build
God and live
forever they literally say it on their
blogs like again just like just read
what they say it's not that weird if if
you're a person who wants power and
someone said oh there's a technology
that makes you you know infinite power
are you going to be like oh I shouldn't
do that or you going be like yeah okay
let's go
like so I don't know if you ever listen
to hard Fork the New York Times Tech
podcasty thing but um one of those hosts
Kevin Roose said it on the other day
that like they interviewed an open AI
guy who had left and you know another
open AI safety guy left and and they're
talking to him and they interview him
whatever and then afterwards Kevin's
like well you know the reason I'm not
really worried about AGI and Extinction
is because none of the people working at
the companies are really that seriously
worried about it and when my texts start
lighting up with people who work at the
companies being like oh [&nbsp;__&nbsp;] really
worried about this then maybe I'll start
to pay attention so what you're saying
is and this is a big part of the problem
because it's like trying to convince
people that people go to work every day
to work on a suicide
machine is so counterintuitive um that
you know people find it really hard to
believe so will there come a point is it
possible there would come a point when
there would be droves of people from
these companies actually coming out and
being like holy [&nbsp;__&nbsp;] or or are we just
gonna get to the edge of the mountain
and nobody's going to say they're just
you know everybody's just quiet and and
not saying nothing so this is a great
example where we can call back to an
earlier part of this podcast about how I
think about convincing people and like
talking to people where we can analyze
what is the intuition here and you said
it perfect you you already called out
the intuition the intuition here is
people wouldn't go to work on a suicide
machine this is the underlying intuition
of why he believes this like you said
this it's correct analysis now question
is his intuition true
climate
change no it's not right yeah like this
intuition is just not true so now the
question is not does this happen or not
like will this intuition be validated
the question is is this intuition true
and then if it's not true what are
examples of things that can counter into
it this intuition and for example Big
Oil you know people were nuclear weapons
you know like there is in fact I mean
you know I'm German there were many
people who went to work every day doing
some very bad things in the 20th century
and you know they were just normal
people They had normal office jobs and
families and kids and probably a dog you
know it's just like the intuition that
people in a normal job environment would
not work on extremely dangerous or
immoral thing is just not
true so no I don't actually like this is
like saying I'll stop I'll start taking
climate change seriously once all the
oil CEO start saying it's a big issue
and I'm like why would you think they
would do that and like why do you think
that would be the right standard like
from the P if you said like once all the
petroleum Engineers start saying that
climate change is a real problem then
I'll start getting concerned and I'm
like like what it's not gonna happen
it's not gonna happen and even if it
does it'll be way too late like it'll be
way too late these people have a huge
incentive to underplay the risk it's
literally their job to underplay the
risk like you're literally saying I'm
going to worry once the people who are
paid to not be worried start getting
worried about it and I'm like what like
what what decision theory is this like
what like you don't do this in other
scenarios right if a tobac like you know
if a tobacco you know consultant told
you oh the lung cancer is not a big
thing you don't say well you know it's
true you know I'll wait until the
tobacco you know Factory you know
managers start saying it's a problem and
then I'll start worrying about lung
cancer do you so let's talk about
general public Outreach like um how do
you have thoughts about communication
strategy ways to wake up the world many
ways indeed and we've already talked
about a lot of it already luckily in
this in this podcast about thinking
about people I think the most important
thing that like I can just I repeat over
and over is meeting people where they
are like we kind of talked about this
earlier about trying to help people I
think there's a there's a thing where
like there's a for of helping people
which is patronizing where you're like
you decide hello I am the Savior do not
resist you know which is like a I get in
that I wear those shoes too [&nbsp;__&nbsp;] much
y it's it's an easy thing to go in and I
and to the victims of this I was like it
comes from a good place it comes from a
good heart it's like mostly people who
do this are actually trying to help like
it's sometimes funny you know I live in
London and like sometimes I'll like see
like posters or like protest from like
student communist groups or whatever
where they're like you know we should
like unseat the government or whatever
I'm like that's cute like I don't even
get upset anymore I'm just like a look
they're trying to help that's cute I
mean it doesn't work and don't do that
but it's cute you know um
so I think it's very important to
generally when you're trying to convince
people or like change people or like you
know help people is to meet them where
they're at if I had to talk to a
communist about AI risk I wouldn't come
in with like your models of the world
are wrong you're a bad person for
believing these things that I personally
disagree with and so on and like blah
blah blah I would just be like all right
how do they see the world like like and
like What are problems that they are
facing like what are intuitions that
they have and how can I communicate with
them on this I'm no communist holy [&nbsp;__&nbsp;]
am I not right and like I I could very
not much hard to disagree with some
group more but if had to talk to to
someone like this I would think about
communism I would read their books I
would think about how they view the
world I would see what intuitions make
them believe this whether I agree with
it or not I would see like what
intuitions made them this way and like
how do these intuitions Rel Rel to the
things that I think I might have a you
know framing for me of how I think of AI
risk but I'm like I need to think about
a communist phrasing like how would a
communist who thinks about the world
from a communist lens think about AI
risk and that's my job it's not their
job it's my job to figure that out and
they feel the same way about any other
group of people who's different from me
it's like I have a framing that works
for me and it doesn't work for everyone
else right and this is not necessarily
because they're stupid or because
they're evil or because they're bad it's
just the world is very big we use
different methods to understand the
world we use different Frameworks we use
different life experiences different
emotional States and so on and as I say
we can stomp our feet and be like well
but people should be rational like I
shouldn't have to understand communism
to talk for a communist or whatever but
in the real world you do and like just
literally get over yourself and just
like you know when I need to talk to
extreme right Wingers or you know
whatever right I'm just like all right I
need to understand what do they care
about in life why do they care about it
whether or not I agree with it you know
like what are moral AC things like you
know I will never you know say to a
communist that like oh yeah it's
actually fine if we you know kill all
these people in in in you know Googs I I
would never admit that obviously because
I that's morally wrong but I can't
phrase things from like a Marxist
perspective is like well it's like
Capital you know become you know starts
consuming labor until there's no more
labor and ecist will be like oh [&nbsp;__&nbsp;]
you're right that is a problem I never
thought of it that way why did no one
tell me that like I've had conversations
with with this where I talk like left
leaning people and I just explained AGI
as like Capital replacing labor and then
they were like holy [&nbsp;__&nbsp;] you're right
that's exactly what's happening this is
a big problem why did no one tell me wow
and I'm like you know put it in their
terms and their constructs and their
language and that's why it's so
important to be on level one it's so
important to not lie and that's why it's
work is that it's so easy to just take
someone's words and just say something
they like but it's not quite true this
is so easy to do it's so easy to say oh
it's like you know the bu was AI trying
to take over but that's not true that's
not the thing right and to know which of
these metaphors is like gets the thing
you want like takes work you have to
actually listen to these people you have
to like and like respectful you have to
be like understand what are the problems
they care about how do they determine
problems like you know
it's like for many groups of people
whether it's you know political groups
whether it's Faith groups whether it's
you know just people from different
backgrounds from different cultures and
whatever you actually have to listen to
them and you have to like try to help
them a really nice thing is that that's
why I love this framing of like trying
to help I think is so important is that
most cultures have a respect for people
trying to help you know it's like as
long as you're like respectful you're
not assful about it and whatever like I
say like you know like the person I was
talking to who was like holy [&nbsp;__&nbsp;] you're
trying to help me was like a super like
conservative right-wing kind of guy who
was like like old like aristocracy kind
of like very different from like a
long-haired you know like Tech guy right
yeah yeah but like he could see yeah
this like weirdo here he's helping me
like you know he's like you know he's
speaking my language he understands the
things I care about he obviously took
the time to read up and like he's like
not trying to sell me anything he's
trying to help and people do respond
very well to this
yeah and so this is on the individual or
like the small level like how do you
approach people and my recommendation
for how to approach people is to
approaching this people like that
doesn't mean everything they say is
right or should be you know just agreed
with right I talked to many people who I
very strongly disagree with um including
morally and you know sometimes those
fight should be had there are some
people you should just not coordinate
with because they are evil exists right
you know there are some people who are
just so bad you should just not refuse
to talk to them that does happen but
it's a very small minority it's a very
very small percentage of people so
that's the first thing the second thing
is larger stale Communications like you
know large scale comms and
Communications this is related but
different is that I think everything
comes from a basis of treating people
like people and respecting people and
like trying to understand them showing
genuine curiosity and showing General
helpfulness I think this is the basis
for everything like if you treat people
like animals or like objects or pawns
you're set up for failure like the
people will realize this it doesn't work
and it's like morally wrong like people
are people even if you disagree with
them you know but the second thing is
like public comms is different because
it's a different setting like the amount
of information you can relay with like a
public statement you like one of the
great things about podcast is you can
suddenly actually produce very complex
statements in a public setting this is
actually usually this is very rarely the
case usually like I don't know if you've
ever been on TV but like I've been on TV
before and it's it's just like they'll
like put you down there and I'm like all
right we're live now and like and
they'll say five sentences and they'll
be like what do you think about this
like well I think it's this well that
was exciting thank you so much and cut
camera I worked in TV for 12 years I I
did that sh yes I know ex I know what
it's like to take someone like you who
has a lot of great things to say and
have to chop it up exactly so you know
exactly how it is right and it's not
because the TV's like they're evil or
they're trying to like H you or
something no it's just like just medium
right it's just how it works right and
so again my recommendation is treat it
with curiosity treat it with kindness be
like all right how can I help the TV
producers and the people watching the
show get truth and things that are
useful to them how can I help and so
helping your TV audience is different
from helping an individual person it's a
different thing a TV audience is a
different entity than a person it's made
of people but it's a different thing so
how can you approach a TV audience one
thing is is that you need to have just
extremely tight sentences and you need
to prepare you have to have you know to
know exactly what you're going to say
with the right intonation and so on and
you have to like be clear that it's
actually the thing that like can be
understood with the limited context the
audience will had yes this is very
important one of the most important
properties for this kind of like mass
communication is what I call Graceful
degradation
so most complex ideas or like complex
ideologies or systems memes and so on
kind of have like a multiple forms so
like let's take Marxism for example
Marxism is really complicated it has all
these moving parts and all these
theories and disagreements and horribly
complicate I I don't understand it right
like it's way too complicated and like
it's like super complicated whatever
right whether it's true or not who cares
but it's very complicated so the meme of
Marxism is very complicated there's many
moving Parts most people will never read
marks they will never read all the books
and all the theory and like have deep
and including most people who will
describe themselves as Marxist will
never actually read all the stuff and
understand all them details and you know
who disagreed with who or whatever most
of them will never do that there's be
there'll be some percentage of like
academics who spend their whole life
studying this or whatever right whether
or not that's a good idea and you know
they can propagate the full idea
quotequote but what happens is that most
people get a degraded form of the meme
they get a greatly greatly simplified
version like it goes through like a like
a blurring process where it gets like
less and less sharp as it goes down
eventually you get like you know
whatever it lands in like the Soviet
Farmers like you know farm right and
then that version has to still be good
and this is a property that not that
most memes don't have most complex ideas
when you simplify them just break or
just or become stupid and yeah a thing
that good ideologies or like or like
like fit complex ideologies have is that
the degraded version the simplified
versions are still good they're still
useful even if they're not maximally
have all of the new on same thing with
like you know like libertarianism or
like you know like like capitalism like
you know like libertarianism is quite
complex there's like many theories about
why free markets work how to set up free
markets how to deal with monopolies this
is really quite complicated but the
degraded form of like you know supply
and demand and all things equal we
should be free you know two sentences is
like pretty good like from a Libertarian
perspective like that's a pretty good
one if most people believe that
Libertarians would be very happy you
know same thing applies to right-wing
leftwing you know Etc is like you want
want a version that's complex and you
want a version that is simple and we
need the same thing for like high risk
and this is why I focus so much on the
thing like intuition such as like
something smarter than us that we can't
control is scary this isn't the full
story we're not talking about
instrumental conversions and takeoff
curves and AI scaling and my claim is
it's actually bad to even bring those up
so like when I'm in a TV interview I
don't start talking about scaling laws
or I explaining I pick like three
intuitions or one intuition that I want
to stick and I'll be like hey hello you
know TV audience I there's a massive You
Know M huge thing behind me that I don't
have time to tell you about here's an
useful thing for you hold on to it and
this is people and people appreciate
this too they're like oh thank you that
is a useful thing for me to hold on to
that's actually so like one of the ones
I I almost always try to get on TV for
example it's like that neural networks
are grown not written like this is like
such a good one it's super easy and
people like who I didn't know that and
I'm glad I now know this like this is a
useful thing for me to know and I will
hold on to this so when I think about
mass communication I often think about
this I think about what are small things
that people can hold on to and be like
and like be like oh yeah I've heard that
before that is actually kind of weird
and you know some people will be
motivated to then seek out the more
complex information read the whole thing
and blah blah blah sure great but
they're not the primary audience sure
all right here's a question for you what
are your thoughts on sort of tactical
Purity um you know a lot of the sort of
rationalist EA world that the AI safety
stuff comes from believe the argument
has to be complete and you know all
these sort of the advertising and
marketing and PR is bad and and um my
thought as someone who was a TV reporter
for 10 years and has worked in
advertising and marketing for the last
10 years after that is like it's an
emergency we need to use any tool
available um and there's no purity test
that I have any interest in for any of
this stuff so for like this is a crazy
example that I thought of this weekend
excited to hear your thoughts about it
um I want to do a a social media
campaign around saving my dog who's a
beautiful just dog and could be any dog
from being slaughtered like save Dolly
save Dolly the dog from being
slaughtered learn more about AI risk uh
like casting a I risk as an animal
rights issue because Dolly's Adams are
just as vulnerable as mine um so you
know what do you think about using side
door backdoor tactics uh or does it all
have to be like proper so I have the
Envy of position of disagreeing with
both sides and both of them being
annoyed with me so you know produces is
for good for podcast content um so I so
I have
my deep disagreements and fights with
the rationalists not all of them you
know many of them are great people
whatever but oh boy do some of them
really hate me like to like a childish
degree and um if you knew main boy if
you knew um and a lot of it is that like
there's a Freudian thing where like from
Freudian psychoanalysis where like if
someone claims very very hard to be X
they're probably the opposite of X and
like this is shockingly often true in
real it's not always true like of course
like Freud is mostly [&nbsp;__&nbsp;] like like
man have I seen this happen there's a
thing where for example rationalists
have like a very strong Norm of like not
having emotions or not letting emotions
like every conversation must be a a
logical disagreement which means that
whenever they are emotional they have to
explain it with non-emotional terms and
this leads to complete chaos everyone
super passive aggressive like I've had a
conversation like I've had fights with
rationalist before where they' be like
just pissed at me like they just dislike
me or I made them angry or whatever and
I'll just and I'm like it's fine you
know like you dislike me you think I'm
annoying it's okay we don't have to be
friends like well no it's because I
disagree with you on your strategic
analysis of like no you just hate me
it's okay you don't have to like me like
your theor like your argument has
nothing to do and then they'll like come
with these massive theories about like
all your low Integrity strategies I'm
like no you just hate me and it's okay
like you're you're emotional you're
angry it's okay I'm angry at you too I'm
actually extremely pissed at you right
now that's okay we can just walk away
we're adults you know and it's it's it's
a a new level of frustrating when you
are having an emotional conversation
with someone who refuses to engage in
emotions and just like claims they're
like irrelevant or don't exist or
whatever and I'm just like ah very
frustrating so okay there's this thing
where don't use the word purity to mean
like not emotion and I think this is
just wrong emotions are not just because
something's emotional doesn't mean it's
not true the thing I care about is truth
truth and goodness the thing I don't
care about is not emotional like for
example you know if I love my child is
that truth well actually you see it is
simply oxytocin hormones being released
I'm like I don't give a [&nbsp;__&nbsp;] like who
literally who cares like what the [&nbsp;__&nbsp;]
are you talking about or like sometimes
or like for example you know my mom you
know she doesn't necessarily understand
all these things but she's like wow this
AI thing is really scary and should I
tell her well actually mom you don't
understand computers through therefore
you're not allowed to feel scared about
that that's impure you're just using
appeals to emotion like no it's actually
extremely reasonable for my mom to be
feel about just because it's an emotion
doesn't mean it's wrong if you feel
uncomfortable around someone and just
they make you feel bad and you're like I
would rather go like I don't want to
hang out with this guy like he creeps me
out that's okay like like this is a
thing that happens rationalist as well
some like a woman will feel
uncomfortable around someone and then
they'll be like well why do you feel
uncomfortable around him like you have
to justify like logically why you don't
want to be around this creep like
objectively he did nothing wrong so
therefore you must feel good around him
and which is just like intuition is just
nothing you couldn't just have a sense
yeah it's like and this is just like
very bad and leads to lots of abuse and
like lots of like horrific like
scenarios right it's like it's okay to
feel emotions like this is just like a
deep thing that does mean emotions are
true either just because you feel bad
about something doesn't mean it's bad or
that it's wrong but
sometimes it does like it's just not
that easy so in terms of aity thing if I
went to my mom and I gave her a huge
lecture about all the you know linear
algebra underlying neural network
theories and whatever I don't think I'm
actually helping her I don't think this
is the most helpful thing I could do for
her because she'll just feel confused
and overwhelmed and like doesn't know
what to do with this information this is
not helpful if I instead give her
well-crafted
true underlying intuition
pointing to the things I'm like well
they're not they're grown not written
you know something smarter than us all
things equal it's similar to big tobacco
etc etc like you know just a couple
things that she can understand and use I
think this is very helpful for her this
is the kind thing this is the pure thing
and the thing that will she will come
away with is much more pure than the
thing she would have come away with with
the whole [&nbsp;__&nbsp;] [&nbsp;__&nbsp;] post right like
just because you have more information
doesn't mean people get more
understanding
to be clear the information should exist
somewhere someone should be having the
full argument like there should be nerds
and professors and whatever having the
full [&nbsp;__&nbsp;] show you know someone
somewhere in a bi in a biolab is having
an argument about you know nonsquamous
cells lung cancer versus neonic toid
[&nbsp;__&nbsp;] whatever right sure but that's
irrelevant to policym like someone
should have should argue about you know
how does this car component interact
with this cellular metabolite or
whatever but we don't need to know that
to ban or like to restrict cigarettes or
know you know cigarettes and know that
it hurt it caus lung cancer we don't
need that and in fact if you depend
cigarette regulation on some [&nbsp;__&nbsp;]
Arcane you know biochemistry thing then
nothing
happens then you just lost you just
forfeit the actual good thing so this
idea that you can just like have maximal
truth in all circumstances and all
maximal information is just ignoring
complexity is like just because you have
more things doesn't mean it's more good
it's a trade-off and like when you're
trying to change people's mind to
exclude emotion as a tool the primary
tool to use is tying both hands behind
your back it's not just not using
emotions to it's as a tool it's also
that people use emotions as a tool of
reasoning is that emotions are part of
the algorithms we humans use to reason
about reality and there is these
aesthetic movements around like
rationalism whatever that try to pretend
this isn't true but it is and like suck
it up like some like sometimes I look
like you know a mathematician looks at a
formula and says like I think there's
something here like there's something
beautiful about this I should stud this
more what is that that is an
you just us an emotion
science most science emotions because
have Aesthetics and you have Beauty and
you have all these things it's like the
human brain is made of neurons and
emotions it's not made of like abstract
data structures and we can also use
abstract data structures and logic and
so on and we should it's very useful but
it's not the primary engine of cognition
the primary engine of cognition is
veilance emotions heris and you know can
argue it shouldn't be that way but I'm
like well suck it up like if you refuse
to engage with other people's emotions
you're refusing to engage with their
reason with how their brain actually
generates truth you know yeah it might
be an impo like and you know not the
best algorithm maybe yours is so much
better sure but again you have to meet
people where they are you know for my
mom emotions are very important like she
often has a really good gut feeling like
whenever she tells me like something
wrong with that person she's often very
right like oh that girlfriend that's not
going to work out she's always right you
know and or whatever right and that's
okay right it's a reasoning process not
saying it's the perfect process but it's
so I just kind like reject this whole
thesis that like there's something like
morally pure about not engaging with
emotion I think it's just I think kind
of the opposite it's like not engaging
with emotions cope it's kind of like
you're hiding you're putting your hand
in the head and pretending that the
world is simpler than it actually is and
that you know a lot of emotions are
calibrated like a lot of emotional
reactions are actually reasonable and
like you some are not and you should and
you have to differentiate and you have
to deal with that it's just what is
so what I on the other side though so
there is one thing where I do think
prity matters a lot not you know it's
where at least there's a trade-off which
is there is a general pattern in nature
which I call deals with the devil and
the pattern is usually something that if
you or if you uh relax a little bit the
thing you want or that you're asking for
you gain more power if you're just a
little less strict on what you're asking
for like something in this direction if
you accept a few more shady allies then
maybe you would have otherwise you will
see your funding go up you'll see your
power go up whatever so the deal with
the Devil is always the same you trade
you gain power in return for for not
getting quite what you asked for and so
this is very important this is a very
very very important pattern where for
example you know the the thing you just
describ with your dog like this
emotional appeal or whatever right
like there's ways in which this could be
good or not good like saying hey your
dog will be killed from AI is true this
is not a lie like this is genuinely true
but then if you say something like you
know but then there's version of this
which could lead to big problems like
for example if you start attracting You
Know M large amounts of animal rights
are like you know extreme leftist people
who say like AI only matters because of
you know risk to ecosystems or whatever
like wait that's not what I meant you
know like what I actually meant is this
larger thing I'm not saying this is what
you're doing right but like yeah yeah
yeah it's like uh there are ways like
there are ways where you can say
something emotional and it's not a lie
like if you say like I am sad if my dog
dies there is currently a large risk
that is being placed upon my dog if you
feel this way as well please help this
is not a lie there is this is there is
no lie here now you can make extreme
versions of this that are lies like you
can deceive people and saying pretending
it's a different story like you don't
miss an AI risk until the very end
people would feel deceived by that but
if you're like this is the thing is like
I love my dog my dog is being risked by
thing and so are you learn more that's
not a lie right right and and uh even if
you used a little deception a little
sort of like hey uh you know luring them
in to get him somewhere like um you know
would you object to that and then one
other one would be like uh crazy tactics
like if if we could create a a crypto
meme coin that would Infuse a hundred
million dollars into AI safety
Communications because people just went
crazy on this stupid meme coin um is
that a tactic we could use like you know
how crazy can we get
so in terms of like deceptive or
whatever like I I think like for me
deception is a very strong word like uh
this usually almost lies like if you're
just like being a bit cheeky right you
made a little meme or whatever but like
you're not deceiving people you're not
tricking people you're not lying to
people I'm like it's it's usually fine
like there can be push back because
people are like stop meing like you're
not funny or like this isn't funny right
like stuff like that right but like yeah
you know if you
like I as like you can read the
compendium you know large document that
I've written which kind of summarizes my
whole worldview and in in the beginning
there is an forward that I wrote which
is like closer to a poem than it is to
science it's much more like it's it's
about reality obviously but like it
appeals to emotions it's about things
you know it's about stories about
metaphors about the human story and so
on right and you can argue oh that's
deceptive because it's not literally I
me that's not the point it's artart like
yeah when art isn't true I don't think
it's deception it's art like art isn't
you know if I read a fiction book I'm
not like oh how dare this author lied to
me this didn't happen no it's a it's
it's a story it's a book it's it's it's
a piece of art you know so I think art
is not deception like it can be you we
can use art for deception but I don't
think I don't think that's like so like
if you create a touching story or a
beautiful video you know it's like a
beautiful story about your dog or
whatever and then like at the very end
you know like slow piano music or what
whatever and at the very end it's like
you know we'll be killed by AI or
whatever right you could say oh that was
deceptive or you can say you created a
nice piece of art and you can dislike
the piece of art but I wouldn't call it
deception it's like you create a piece
of art that had a message it had a thing
to say you you know maybe you don't like
it or maybe it's bad but like so that's
my tick just like I cannot stress enough
how important it is to not lie like like
you know I I I really cannot stress this
enough if we want to win we need to not
lie it's okay to like you know try to
help people and not find things the
right way it's fine to create art it's
fine to sometimes get upset or appeal to
emotions I don't consider any of these
things to necessarily be lying but for
example a form of lie that is very
common among AI safety quote unquote
people is that is them saying they don't
really care about AI safety they're
actually care about something different
I consider this very bad I wrot a whole
post with this called um lying is
cowardice not strategy where people for
example join the government and not
mention AI Extinction risk even once
even when asked because they say like oh
if I mention that then people think I'm
crazy and then like won't let me in the
government I think this is very bad I
think this is very very very bad okay
they might be right like I'm not saying
they're wrong but yeah this is very bad
this is very bad because a it's lying
which is immoral like you know you don't
have to bring it up every meeting sure
right but like if someone asks you do
you think AI is going to kill everyone
and you say oh no that's such a silly
thing that only you know then I'm like
[&nbsp;__&nbsp;] you like yeah like you're lying
like that's a lie and like or even just
like to be clear I also think Omission
in general is a lie like even if
everyone asks you but you also like
never bring it up I also think that's
fying it's like you know or like there's
obvious places you could bring it up and
you decide not to consistently I also
consider that lying like Li Lies by
admission are lies people say what's
your top three things you're concerned
about and you list three things are not
AI ex risk even so all three of those
things are things you're concerned about
so like you're lying you know maybe you
don't care about x- risk fair enough but
if you do and you're like strategically
hiding it I gu line this is very bad for
a number of reasons one is the deal with
the devil you you you just straight up
Inked in blood like I will just not even
ask for the thing I actually care about
in return for power so you're already
off to a bad
start second thing is it's just human
cognition is follow fall if you
constantly tell yourself to not mention
the thing you care about and also
everyone around you doesn't care about
it you will start noticing that you care
about it less to a third thing is that
this makes coordination impossible so
there's a big problem where I know many
people in government in uh AI labs and
elsewhere who are really concerned about
this and they all think they're alone
they all think they're alone and they
all say I can't say anything because I
say something you know I'm going to get
you know all the weird looks because
they know correctly that even if they're
in a room and there's like three other
guys who also care if they speak up
those three guys won't back them up they
know that and this is really bad because
if those three people did back him up
things could change because then you can
coordinate coordination creating groups
of people who can
coherently push agendas like this
require common knowledge like I don't
even know who I can coordinate with who
can I work with because people are
[&nbsp;__&nbsp;] cowards and like they're not
willing to say like you know some people
just don't know or don't care fair
enough but there are people that care or
want to care and are just scared they
just don't want to talk about it and
this is or do it for like personal
selfish power reason and this is very
bad this is very very bad and I strongly
disae all right Connor I have a two last
questions for you all right I hope this
this this second to last one we you will
take in a good spirit right so uh nobody
takes this cause seriously enough right
we can agree on that like nobody takes
AI risk seriously
enough
and I think one of the reasons for it or
or an obstacle we have is the way our AI
safety leaders present themselves in
some ways and so my question to you is
simply this I think your look is [&nbsp;__&nbsp;]
awesome dude I love your [&nbsp;__&nbsp;] look I
love the hair I love the whole I love
the whole thing question is if if I
could take this hair and snap it onto
your head like a Lego do you to make you
more effective a do you think there's
anything to that and B would you
consider it in any way crazy question I
think there's a lot to that and I take
it seriously like seems like a very
reasonable thing I don't take that in
bad Spirit at all you know even so true
metal header rer man that's like truly
truly no one appreciates um but no I
think there is like there's TR something
to this and I take it a good spirit
whenever people tell me this in the
Twitter comments like you know I
appreciate you thinking I think about
things quite different a bit differently
in this regard it's like um I'm very
brandable let we put it that way I am
extremely recognizable everyone knows
who I am even people who have only seen
me once you know this is an asset it's
yep it's also downside like um I get
stopped sometimes in the street like
micro celebrity AI guy sometimes it's
like I had a really funny thing that
happened to me like a while ago I was on
the BBC like late night or whatever and
I went to work the I live in London and
so the next day I went to work and then
I see this like construction worker kind
of like across the street and he like
climbs down from scaffolding and like
runs towards me like like runs right
right up to me and like hey you on the
telly
and I'm like yes it's like it's like you
know it was fire it was it was just
funny right it was like it was perfectly
polite perfectly funny right um yeah and
but like so there is a there's a choice
here we like I am not the guy who should
be running the government Bureau of
whatever like Yep this is not my role
this is not what I'm optim in for what
I'm optimizing for is to being the guy
who can create
the mimic branding outside game overt to
window pushing like yeah I have several
assets that I work with that I am very
lucky to have and like I don't take for
granted one of the assets is is that I
am very independent I am not dependent
on AI laabs or on you know government
like the Goodwill of people in this
regard for my funding and my survival
and so on very lucky that I have this
and another thing is is that just like I
am truly not a coward like like no
offense but I am just truly not a coward
I have never been a coward um I'm not
claiming this is like moral superiority
but just like my genetic makeup and just
like am I am just not a coward I am not
afraid I am willing to be disliked I am
willing to take [&nbsp;__&nbsp;] um I don't like it
like I I hate it I I hate that people
dis like me or like or like mad at me
I'm always like well that's sad but like
it's but it's fine it's like okay I'll
just deal with it it's okay I'm willing
to take it in a sense being a bit of a
lightning rod is part of my role is like
yeah is that like people get upset or
whatever like I'm a good person for them
to get upset about you know and like I'm
just like okay with that it's like so it
is deliberate in this regard right it's
like I love that I love that whole
answer that's like yes that's also I
mean I'm just lucky like people think I
put a lot of work into this this is just
how it is man I put shampoo in like once
and it just looks like this so that is a
gift you gotta you gotta take it right
so listen you know you know so so I I
think this is a very fair question to
ask and I think there shouldn't be many
people who look like me I think um all
things equal like you know I I I shower
every day I you know wear clean clothes
I don't do other crazy things in my life
or whatever I also like don't bring I
don't do weird things in my private life
and if I did I wouldn't bring them into
the public sphere um cough cough certain
parts of this movement should learn from
that um and like I think this is good I
think there's an obligation to a certain
degree that like to be respectful like
if you do weird things in your private
life keep it private like it's okay if
you do weird things but like [&nbsp;__&nbsp;]
keep it off Twitter Jesus Christ um like
if you a leader of a movement or you're
trying to push will like like Jesus
Christ at least do us the Dignity of
like keeping it private um I think this
is very very important um you know
present yourself well speak well be
polite don't insult people don't lie you
know stuff like this I think it's very
important awesome all right Conor send
it out always send it out with something
that gives people hope give give the
audience something that gives you hope
as you look at this thing the main thing
I think that gives me hope is just that
when I was uh and was more of a tech guy
and whatever and like more isolated
whatever there's like this like you you
live almost in this kind of bubble where
you kind of like think that like other
people are like stupid or just like mean
they just like they don't understand
they don't really care you read about
like scope neglect or whatever and you
like you're like oh look people are so
irational they don't actually want to
help they're all just selfish or
whatever and like yeah there's some of
this but like there is just this like
deep thing where just like I've come
just like really deeply in my heart to
appreciate that like
like not everyone not in all
circumstances but like most people are
like really decent people and they they
want good things they love their kids
they love their Partners they want
people to be happy and help and healthy
they don't want to hurt anybody you know
and it's just like you know when you
know I was an atheist still am obviously
but like I talk to people of Faith or
whatever and I'm just like like you know
I had this like Vision in head is all
these all these idiots or just un it for
power or whatever but like when the
first time I met someone who like really
believed in Jesus like really believed I
was like whoa like that's actually
really nice like dude like like I had
this real moment where I'm like I'm not
religious I'm not going to buy any of
this but like wow you you're not [&nbsp;__&nbsp;]
around like you actually believe this
like you're you're actually trying to be
a better person like whether you need to
Faith to just what you can't see yeah
it's like it's I'm just like
wow like if this is what Jesus does for
you I mean like wow dude like that's
like you know it's not for me and like I
do think there are like problems about
believing things that are not true and
like epistemology and whatever but also
like you're not like there's this thing
that like nothing in the world annoys me
more than Hypocrites like truly nothing
in the world gets more under my skin
than someone who like claims to believe
something but doesn't really they're
just faking it so like whenever I meet
someone who's like you just like you
know your village Pastor who just like
really just wants people to be okay and
he cares about the people in Village you
know like like my grandma she got very
sick before she died and like she had
this pastor she just like cared a lot
about right and she was very had like
back dementia like she was she was in a
bad spot it was not fun to be around her
she also got like really angry all the
time and it was it was a mess right but
like he came he did visit her sometime
and it meant so much to her it meant so
much to her he didn't have to do that
and you know you know I'm just like
that's nice like it's nice of him to do
that like it does it save the world did
this is this really the best thing he
could be doing with this time like
probably not she's going to forget it
immediately you know but like also like
there's just
something genuinely nice about this and
genuinely touching about this and so to
bridge that to the larger thing there
was this thing where
like the world is complicated right and
there are big problems that we are
facing
and the way humans solve problems is
like have historically solve big
problems is through like kind of human
ways like there's this thing where like
it can be frustrating to say this that
like we solve we're humans and we solve
things in human ways but there's also
something beautiful about this like
there's a way to see this as something
is that the solutions don't have to be
cruel and heartless and M machines and
you know you know inhuman the
solutions can be very human and actually
are very human that often makes them
boring it's like when I talk about
Civics like what is the actual response
to AGI risk the actual response is that
we need to do our civic duty we need to
talk to our fellow civilians we need to
talk to our friends to our families to
our churches we need to say for our
policy makers what are you doing about
this I'm a concerned citizen I don't
necessarily have the solution but I
think this is a problem what do we do
about this it's not epic it's not wow
some Super Genius cooks or whatever like
this is like this's this weird Cult of
of the hero where like there has to be
like some lone genius or some superhero
that solves it or whatever but that's
just not how the real world works the
real world like sure there are some
super smart people and like whatever but
like what builds a good Society is
civilians just like normal people who
just like want to live a good life they
care you know some people care more some
people kill us and that's okay right and
so what brings me hope is that like
fundamentally deep down people still
have this like there is a thing in the
heart and the soul of people where like
they want a better future for their kids
they might necessarily know how they
might be scared they might be confused
they might be even doing something
really stupid right now right but like
there is a deep thing we're just like
for the most part like people just you
know they they want to they want to have
food in a nice house and spend time with
your kids and you know you know see good
art and whatever right and they want
this for other people too and when you
talk to them about like when you say
like you know what do you think about
like things that are smartless don't
control they get it like there's this
deep thing that just like people totally
get this like it's only tech people and
like you know super overeducated Elites
that don't get xris it's only them the
normal people totally get this like
people are not stupid you know like this
is like deep thing is just like you know
people who don't come from a tech
background don't come from a stem or
even from an educated background you
know sure some of them are stupid
whatever right exist everywhere like a
lot of them are good people they're good
people they're not bad people they're
not stupid they might not know what to
do they might not have the resources to
do what needs to be done or whatever but
there is a vision there is a vision of a
future for me that is humanist that it's
not about
you know technology helps but it's
fundamentally built of people it's built
people and I think this can be done I'm
not saying it will be done but it can be
done is that people do care like this's
this deep thing where people feel that
they're alone they T they learn about AI
risk and they think they're alone is
that no one will ever believe them no
one will ever listen to them you know
the world is against them and I'm here
to tell you this is not true the tech
world is against you but the wider
World they not only need your help they
want your help like I talk to people
about this and they're thankful because
I am trying to help them and they want
to help me and this is really great like
this is this is a good thing of like how
can we mutually prevent a thing one so
rambling answer sorry for that but like
just for me it's just like you're not
alone you're not alone the world has
many dark aspects to it there are many
many dark things in the world but
like godamn it like Humanity has a lot
of good in it like all you know like
despite everything despite all the wars
and ideologies and the evolution and the
nature to than claw a lot of people are
nice people and love other people and
want things to go well and that's our
strongest asset and like that's and that
that's not enough to win it's not enough
to win but it is necessary and we do
have that so great place yeah that's
where we start Conor thank you so much
man thank you so much for your time
thank you so much for doing what you do
um I'm I'm truly so appreciative of you
and I wish you all the luck and and uh
Good Fortune going forward because um we
got to get this thing done we sure do
thanks
man how about Conor Ley I am so thrilled
that he is on the planet fighting on the
right side uh and you know I'm really
taking what he said to heart right we're
not here to wake up the world we're here
to help people understand there is no
controversy about what is to come
there's only do we stop it or do we
accept it okay so as you know in these
strange times we don't know how long we
have to live so every time we get
together for one of these shows we end
it with something we call a celebration
of life just something that makes us to
be alive I have a treat for you all here
today the incredible Kevin Garrett live
in concert shot with my iPhone this
weekend this dude was selling his own
merchandise his own t-shirts in the
front of the club before the show
playing to like a 200 person Club in
Washington DC um but he should be
selling out Madison Square Garden I
think my opinion so here's the
incredible Kevin Garrett uh covering a
Leon Russell classic
[Music]
I've been so many places in my life in
time I've sung a lot of
songs and made
some I've acted out my life in
stages with 10 10,000 people
watching but We're Alone
Now I'm singing this song to
[Music]
you I know your image of me not what i'
H it
[Music]
be I've treated
you girl see
[Music]
there no one more important to
me baby can't you see right through me
we
were I was singing this song with
[Music]
you you taugh me precious
secrets of a true love with holding
nothing you came out in front when I was
[Music]
but now I'm so much
better if my words don't come
together just listen for the melody cuz
my love in their
[Music]
hiding I love you in a place where
there's no space of time
I love you for my life you're a friend
of
[Music]
mine but when my life is
over remember when we were
together we were
[Music]
alone I was singing this song to you
[Music]
we
[Music]
were I was singing this song to
you I
mean that dude is
incredible so what you said you said you
want one more song okay fine we'll do it
okay here is one more Kevin Garrett song
from that show this is one of his
original songs he has four full albums
they're all fantastic I encourage you to
check it out here's one more from my guy
Kevin
[Music]
Garrett you are in
my when my heart
war and if I'm ever scared out breathe
the air in front of your
[Music]
door but I will never
[Music]
knock that's is far as I
go it's only in my dreams when I say
what I mean but I'll get down near close
[Music]
and I'm a
[Music]
stubborn our right is best I
can got this bright idea if I show my
fear I will less of
man and I think too
much but I don't feel
enough with a gun to my my head and I
might confess that is you I
love oh my
[Music]
hands all around you each
night and you make lead me into what you
want and it may seem like I FL from you
once and maybe we'll fall further from
must I
Wun cuz I'm here
right if you take take the
time to let me stumble through how I
feel about you is I mess of my
life and you hear
me let's just say
[Music]
will cuz if I out these words when I
crash in you will take me
[Music]
still oh my hands are
[Music]
tight around you
[Music]
W my hands
[Music]
are all around you
[Music]
[Applause]
okay my friends if you have made it this
far you are one of the real ones you are
one of the real four Humanity family
members and I have an ask for you so in
the show notes you'll find links to
monthly subscriptions to support for
Humanity's Outreach to new viewers um we
cannot help people understand AI risk
without funding I put a TR tremendous
amount of my own time and and funding
into this show this effort it would mean
a great deal to me if you signed up to
be a monthly subscriber um that would be
that would be just awesome please
remember AI risk is not someone else's
problem it is yours and it is mine and
in
2025 we are going to help every human on
earth understand the fact that AI risk
is their biggest problem too this is the
year
we help Humanity understand what AI risk
is all about we will not allow 2,000
people to wipe 8 billion of the rest of
us and all living things off the face of
the Earth no [&nbsp;__&nbsp;] way for Humanity
I'm John Sherman I'll see you
[Music]
tomorrow for

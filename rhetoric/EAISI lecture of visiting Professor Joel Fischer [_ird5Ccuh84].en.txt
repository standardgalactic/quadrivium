there's no one else online we're just
recording the talk in case you were
wondering I get started now it's my to
ince off profess computer interraction
at University of d as well as reality
lab there his research is on human
Centric use and AI use Technologies and
uh he uses methods like me participatory
design and so want to look into how we
can maybe tackle the challenge of
artificial general intelligence and
today's talk will focus on um how they
can actually be the hindrance to
responsible AI that uh truly that
Society so we uh are very happy to have
him here as a visiting Professor so uh
please join me and welcoming Joel um and
maybe pause to start talk
thank you I I use the
mic I'll use the mic until my arms get
tired um good to see all of you uh
thanks for taking time out of your busy
Summers hopefully um as you can see I'm
on sabatical so I'm you know champing
champing the um the Leisure wear here um
and also as a Counterpoint of this talk
which is going to be a little bit grumpy
right as you can tell from the title
which is kind of a little bit clickbaity
which kind of worked because you were
here now so that's great um so yeah
let's get started so um I mean this is
this is quite a this is quite a
well-known thing but I did want to play
it um and this isn't about this isn't
you know uh general intelligence at all
this is very much kind of narrow AI um
but this is an example of how AI can go
wrong right so you can see
um this is a based on a reinforcement
learning agent that gets stuck in this
Loop um that you can see right right
here um but what it does is it maximizes
um this simple reward uh at the expense
of the long-term goal of actually
winning the race um and it it does show
how a misspecified reward function can
actually um lead to this uh this kind of
sub version of the environment of the
agent by continuously collecting the
bonus items without considering you know
the great goal of winning the race so it
does show is classic example of kind of
showing unreliable and unsafe outcomes
of uh of AI and actually this was this
is published on the open AI um blog
which is uh at a time when open a was
still open right um and they they have
actually kind of got quite kind of
self-critical uh Reflections that they
offer there to to
leadership um and broadly speaking so if
you read Brian Christian's the alignment
problem um this is one of the examples
that is used in the book to illustrate
how things can go wrong with
AI but now we're at a different time
right and there's this incredible
popularity of of AI which I know I I do
think it's actually incredible what what
has happened in the you know I think
probably since the release of chat gbt
AI has really become like a pop culture
phenomenon um and it's not just about
tech at all anymore it's a cultural
phenomenon that has all these actors and
celebrities in it as well and uh the
media is highly involved um and it's
talked about everywhere not just the
media but also in classrooms boardrooms
and pubs uh in families you know I mean
how many of you get asked by your family
members and friends oh what about this
AI thing should be really worried about
the robot takeover which is going to
happen
soon and actually um examining this kind
of public discourse uh can tell us a lot
about public perceptions and
understanding of AI and where this kind
of discourse is heading and you know I
think science fiction images like this
one that's used here that the media love
to use have a lot to answer for as well
um they kind of contribute to the
mystification of what AI is and maybe
raise false expectations so I I will
lean on that in this talk so what else
I'm going to do in this talk is try and
answer a few questions what is
artificial general intelligence or AGI
and why is everyone so worried about it
um then I want to ask or answer a little
bit what's in it for whom and then to
kind of get to like what are the real
problems um with AI and what should we
do about
it so what is Agi and why is everyone so
worried about it so this wouldn't be a
lecture without having some definitions
right and um here are some of those so
um you know highly autonomous systems
that outperform
humans um at most economically valuable
work that's the definition from open
AI then a longer kind of more academic
um definition here is a you know
software program that can solve a
variety of complex problems in a variety
of different domains and I think this
this um domain spanning um type of uh
characteristic of AGI is really
important um and then this talks this
goes on and talks about things like
thoughts worries and feelings I mean I
would contest that AI can have that but
there you go um and then finally a
shorter one uh from the classic AI book
from Russell norvic is it's a universal
algorithm for Learning and acting in any
environment again there's this concept
of environment spanning uh type of
definitions um giu and Tores is the
paper that has sort of inspired me to
write this talk which is the test real
bundle paper it's open access um go and
check it out um if you haven't seen it
already um it's also very ranty and
angry in many places but I think it's
also enlightening in other ways too um
and so I'm when I'm citing that um on
the slides it means that um essentially
the um content of that slide is uh taken
from that paper okay so um who is
familiar with bostrom's paperclip AI
thought experiment show of hands quite a
few of you I'll play this anyways um so
you get an
idea um if you haven't heard of
it humans are fascinated by the
possibility of artificial intelligence
some believe that advanced AI will
signal the end of most basic problems
like food security or needing to work
the more pessimistic Among Us believe in
a Terminator style ending where
artificial intelligence tries to destroy
the human race but what would a disaster
AI scenario actually look like it's
unlikely that AI would experience human
emotions like Pride anger or blood lust
that we consider evil it's far more
likely that AI ends the world as we know
it by accident Nick Bostrom describes
this through the parable of the
paperclip
maximizer imagine this paperclip company
is tired of managing its FY paperclip
machines instead they design an AI whose
exclusive goal is to produce paperclips
and lots of them this is a really smart
AI because it needs to deal with things
like raw material procurement shipping
and interacting with paperclip customers
it's more intelligent than any living
human
being the owners of the paperclip
business turn the AI on and are
initially very happy it turns out
paperclips like never before it's so
good at the job that it's actually
producing far more paper clips than
necessary and the owners decide to turn
it off they unplug the AI only to find
out that it's produced a copy of itself
earlier so that it could keep operating
if it got
unplugged this AI is on a mission to
keep making paper clips
soon we have a major crisis on our hands
the AI has gone out into the city
collecting all the raw materials it can
find to turn into paper
clips paperclip Warehouse is soon
overflowing and the AI has to expand out
into the city it does this with Reckless
abandon storing paper clips wherever it
can find Space it doesn't feel bad about
crushing homes schools or hospitals it
doesn't feel any emotion at
all the AI soon Ventures outside the
city needing more raw materials and
space the toward its
paperclips at this point the whole world
is on high alarm all working together to
stop the silly paperclip making
AI the best Minds in the world academic
and Military come together to stop it
the AI learns that we are plotting
against it and perceives this as another
obstacle to its paperclip production to
prevent these simpleton humans from
meddling further the AI releases a
noxious gas over the world that
eliminates all people fully unencumbered
the AI expands around the globe using up
all available resources soon the Earth
has nothing else left to make paper
clips from so the AI prepares for
interplanetary travel the colonizes
other planets in search of more metals
to make paper clips with the AI hums
along blissfully leaving a trail of
death and destruction in the universe
caring only for making more paper
clips I think that's enough um on that
yeah on that cheery note so I you know
it is a thought experiment first of all
it's very important but actually you
know a lot of the rhetoric uh and and
hype at the moment around kind of
existential threats really kind of comes
back to this um this type of idea that
this is how AI can kind of get out of
control and but you know there's a lot
of assumptions that are being made here
about what is happening um with AI and
um you know I think it's important at
this point to um move on and look at you
know that's that's the kind of that's
the kind of fiction so where are we
currently what is the state-of-the-art
with AGI and many say that really with
large language models with gener AI um
this is the closest thing we have um in
reality to um to
AGI and it is because of its
cross-domain handling um it can do
things like you know this paper argues
um this is uh this is this is U very
well cited already it's still a new
paper but have a look at this Sparks of
artificial general intelligence early
experiments with GPD 4 and this kind of
shows allegedly how um well it can do
things across domains you can do multi
model imagery input um it performs well
in exams um it does things like perform
well in spatial awareness tasks and it
can do programming can do maths and
it can generate music and so on and so
you know really what we do have with
large language model is probably the
closest thing we have currently to
AGI and then so um let's take a look at
who's actually worried about this right
uh who are the Warriors or the doomers
and what are their arguments well first
of all we go back to our friend Nick
Bostrom um who came up with a paperclip
thought experiment um which you know to
kind of paraphrase I think his worry is
that non-aligned AGI will Doom
Humanity um so if the AGI isn't properly
value aligned then the outcome the
default outcome will be Doom okay and we
should nonetheless though create AGI
since aligned AGI would help fulfill all
these utopian promises and these are
things that are taken from the test real
paper so test real spells are the
acronym so some of those things um some
of these types of
um isms right that we have here on the
slide like transhumanism which is about
radically enhancing human organisms um
Beyond um the the kindr kind of like
biological form humans have um extro
pranis is another type of self-
transformation um based around
rationality rationalism itself so
improving reasoning and decision- making
and putting that at the center
of um of the types types of things that
are to be achieved with with AGI um
effective
altruism which is about maximizing
positive impact almost at any cost and
long-termism which um emphasizes things
like colonizing space controlling nature
and creating as much value as possible
now what's quite interesting so in the
test Feld paper as well as else this is
an article from the guardian um a lot of
these ideas are actually closely aligned
with Eugenics ideas around um you know
manipulating the the human race and in
kind of very scary ways right and
actually the paper does a good job at
showing the different ways in which
there are historical alignments with
some some you know some genic ideas that
have been you know populate that have
been popular amongst
Nazis and um people like that and uh the
parallels to how um this kind of
improvement of the human race and the
role of artificial general intelligence
in that is connected it's really quite
scary actually um so with that moving on
we have also when we look at people like
um Sam Alman our CEO of open a of course
um very
much contradictory statements that are
being made for instance in their int
introduction of the super alignment team
which by by the way now has been
dissolved um is around things like
statements like the vast power of super
intelligence could be very dangerous and
could lead to disempowerment of humanity
or even human extinction um if
controllable however super intelligent
could also help us solve many of the
world's most important problems and
super intelligence could maybe capture
the light cone of all future value in
the universe okay um humble statements
there from Sam um but you know
contradictions right on the one hand
side it's kind of you know it's
dangerous and uh we need to control
control it but if we can control it
it'll solve all our problems so right so
we have to do it we have to carry on
towards the goal of artificial general
intelligence and then there's like
movements like there's this letter
pause AI right pause the giant AI
experiments an open letter which has a
lot of famous signatories and actually
if you look at some of the names there
um there are also Elon Musk uh for
instance you know billionaire funders of
AI research including AGI um and we have
a number of CEOs of AI companies um like
stability AI that are you creating
generative models and and so on and so
forth so it seems strange that these
CEOs and uh influential people are
signing a letter to ask for a pause on
AI while at the same time pouring in
lots of money into actually
developing uh AI towards these kinds of
goals of AGI right so you ask what's in
it for them why are they doing this okay
so start with the future of Life
Institute um the future of Life
Institute is as you can see the logo at
the top for instance the the the the
kind of uh institution behind this
initiative of PA AI so why are they
doing that and what's actually behind
them so they were launched in
2014 by MIT academic uh Tark and backed
by billionaires including musk and
Skypes uh Yan talin um and they are a
nonprofit that focuses on exential risks
um that this be familiar type of um
rhetoric by now and um Nick Bostrom is
one of the external advisors so these
things you know these these um
institutions and and kind of
organizations are strangely linked um as
well so there's definitely sort of
there's definitely kind of a conspiracy
thing kind of going on if you if you
want to think about it in those ways um
they're also influenced by effective
altruism which is as I mentioned before
on on the previous slide um but is a
kind of utilitarianism that is um that
goes back to philosopher William magasco
and musk as a fan um and what effective
altruism is about is that it it's about
making the lives of as many people as
possible better um using this kind of r
IST fact-based approach um and there's a
reading of this which is that under
those ends of maximizing value for
Humanity all types of means are
Justified and
justifiable um a famous uh person who
was following and a big fan of effective
altruism was Sam bankman freed was maybe
is still um and he
used he did talk about uh at various
points
um this the this philosophy in terms of
justifying his defrauding of crypto
investors right so he he was a uh he had
a you know FTX with the the crypto um
exchange that went bust um because
because of the large scale fraud that
happened and San bankman PR is in prison
now um these these um this institute um
and other institutes like this are
backed by billionaires like Elon Musk
and gab said in an interview that you
know this should already make people
suspicious the entire field around AI
safety is made up of so many institutes
and companies billionaires pump money
into um and Bloomberg had this this
article about effective altruism being
as bankrupt as s bankman fre
FTX okay so I think you know there's
there's a lot of kind of conspiracy
stuff here right which is kind of
intriguing and I've probably been
reading a little bit too too much of
this right um but you know make of it
what you will but there are these these
institutions and these these people that
are involved in them and they all
advocate for artificial general
intelligence so what so what is
happening with this type of Apocalypse
rhetoric I think it's a fuel for um for
these companies and these Advocates on
the one hand side you have the warning
of the potential risks that um you so
you have the warning um that the um the
the the risks of AGI
um you have the warnings and These
Warnings it themselves um fuel rounds of
fundraising okay for their companies
that because they develop safe Ai and
they're the only ones that can do that
they the ones that develop safe and
value LED AGI and because of the
existential risk they should be given
the money to develop the AGI as a result
we have resource intensive systems they
use incredible amounts of compute um of
energy and data as I'm sure many of you
are aware and also you have an
incredible accumulation of wealth in
these companies and resources open AI is
valued at more than hundred billion
dollars anthropic at 18 billion so these
companies are worth more than some
countries in the world for sure and
Nvidia is now the most valuable company
in the world which of course is the
chipmaker that is favor favored amongst
those that are
building um training intensive AI
models and it also gets them the ear of
policy makers um where they can argue
that regulation is needed for others um
but or those that don't meet a certain
threshold of uh being able to to to show
that their models are safe and so on um
and also that is that is then going to
cut off competition at a certain
level so what I want to draw attention
to I think as well in this discourse is
that a range of category mistakes are
being made so a few of those are in my
opinion AI is more intelligent in humans
um so we have Jeffrey Hinton who you
know was one of the early proponents of
this existential threat rhetoric who
stepped down from Google saying and he's
he's known as one of the Godfathers of
AI who um essentially pioneered um some
of the principles behind um neural
networks and and which is of course been
very influential in machine learning um
and Jeffrey Hinton for inst said I've
been shaken by the realization that
digital intelligence is probably much
better than biological intelligence so
there's um equating being done here
between AI between you know artificial
intelligence machine intelligence and
biological intelligence and these things
are being compared together right so the
categories are being um thrown together
here secondly AI will proliferate so
this is uh CEO of uh stability AI makes
stable diffusion um General model uh so
he said say we have agents more capable
than us we cannot
control um that are going across the
internet and they achieve a level of
automation what does that mean the worst
case scenario is that it proliferates
and basically it controls Humanity okay
so we have some other things going on
here which is clearly there's a sense of
agency being imbued in in the the agents
right that are just going across the
internet and they see seem to be able to
make their own decisions and um
proliferate something that is really you
know the verb to proliferate is
something that has been reserved for
humans um but now ai can do the same a
AI can basically procreate and reproduce
as something that has been reserved for
humans so this is a long quote but Naomi
Klein here um the the author of The no
logo book has really got it spot on when
it comes to
hallucinations right so why call the
errors that large language models make
hallucinations at all why not
algorithmic junk or glitches which would
be more technical descriptions right
well hallucination refers to the
mysterious capacity of the human brain
to perceive phenomena that are not
present at least not in conventional
materialist
ways
um by appropriating a word commonly used
in psychology psychedelics and various
forms of mysticism AI boosters while
acknowledging the fallibility of their
machines are simultaneously feeding the
sector's most cherished mythology that
by building these large language models
and training them on everything that we
humans have written said and represented
visually they are in the process of
birthing an animate Intelligence on the
cusp of sparking and evolutionary lead
for our species how else could Bots like
Bing and B Be trick out there in The
Ether okay I think she's really got it
spot on here in terms of uh describing
essentially that category mistake that
is being made over and over again in
language in which we talk about
AI okay so here's some research we did
um ascribing agency is a is a common
thing that people do um they describe a
kind of social agency when they talk
about chat GPT um um here's here's some
work that PhD student Dan did looking at
the discourse on Twitter um now X which
um which examined the grammatical
construction um of tweets that are
active they use an active grammar and
that kind of active grammar um does
ascribe a role in for instance in
content creation information
dissemination and influence so active
active things in terms of um that that
allude grammatically to an agency being
um prescribed to or ascribed to uh chat
gbt for instance chat gbt has raised the
alarm among Educators so CH GPT becomes
an active grammatical agent in that
sentence in which it is portrayed in um
in this tweet and in over 80% of all
tweets um this is also common in how the
media writes about AI uh it writes AI as
a subject ascribing agencies so you can
read those headlines how AI is fueling
uncertainty AI is beating University
students uh would having an AI boss be
better and so on and AI is helping find
the world's world's lonliest plant a
partner um there are other examples I
think the media can also do this better
for instance new AI Tech is developed to
detect heart failure earlier doesn't
ascribe the same kind of agency to the
AI itself right it is it is being
developed so it is passive it is a tool
in the detection of heart failure rather
than it is doing the detection of heart
failure itself so the way in which the
language um constructs the narratives
around AI
um has a lot to answer for in other
words the language is full of category
mistakes um this is possibly due to the
origins and cognitivism U that has kind
of like is at the heart of what
artificial intelligence has been about
um we have this like routine routine
ascription of human traits to machines
such as intelligence reasoning learning
seeing listening writing recognizing
hallucinating reproducing and so on
right so these are all traits that are
typically cognitive um they about human
cognitive faculties and they've
been um transported to describe
technology so what is a category mistake
here's a definition by Gilt Gilbert
riy um it is a an era or that is a
semantic or ontological era in which
things belonging to particular category
are presented as if they are belong
belonging to a different category or
alternatively a property is ascribed to
a thing that could not possibly have
that property but I think that's what
we're seeing a lot with the language
about Ai and what this is also doing is
a kind of Shifting of blame I think um
Roman shaori talked about moral
Outsourcing and the quote from this
article Guardian again is you would
never say
my race is tolster or my sex is laptop
and yet we use these modifiers in our
language about Ai and in doing so we're
not taking responsibility for the
products that we build right so in other
words you know the language in which
these things are being talked about or
can also be used to shift blame away
responsibility away from those that are
developing the AI okay so that's where
we are with category mistakes and AI so
what are the real problems with AI
and you know um there are people that
are at the heart of development of AI
like f who is of course the founder of
imag net and and a very famous Professor
um who has you know contributed to the
advances of AI substantially and also
has just um launched a a company that
has already been funded I don't know
billions of dollars already in four
months um but says right that we cannot
pretend AI is just math and equations I
view it as a tool and like other tools
the relationship is messy tools are
invented um and uh by and large to
deliver good but there are unintended
consequences and we have to understand
and mitigate their risks as
well and of course there are many
problems that we are well aware about
well aware of um the welfare
surveillance system violates violates
Rights was was a ruling by the Dutch
Court about four years ago um I'm sure
many of you remember that um and you
know this is a very current article on
from 26 of May in the BBC about someone
being misidentified choplifter by facial
recognition technology and so the same
types of problems where a AIS used to
make judgments about people are still
happening um and it we we've known about
the problems with AI for some time this
is a um famous work that has looked at
the use of AI and criminal justice
system in in the
US um 2016 this was published risk
assessment tools like Compass which used
very widely in the justice system in the
US I don't know how much they are still
being used now but certainly were at the
time I used to look at had um and
predict the likelihood of reoffending um
and this research shows how this was
biased against black people um only 20%
of people predicted to reoffend actually
did reoffend and black people were
actually wrongly labeled
um uh almost at almost twice the rate to
reoffend uh than than white defendants
right and so you have these what this I
think this re research shows is the
structural inequality which means that
factors like poverty joblessness social
marginalization are correlated with race
so the prediction algorithm didn't take
race into account but you have the
implicit correlations of um these types
of factors that are that are implicitly
correlated with race and thus you have
you have um
discrimination um against
uh or racial discrimination on the basis
of these types of
correlations similar in health this is
came out in the same year looking at um
another risk assessment tool used in
medicine used in health where black
patients were assigned the same level of
Risk by the algorithm um sorry black
patients assigned the same level of Risk
by the algorithm were actually sicker
than white patients um and the the
authors estimate that this bias reduces
the number of black patients identified
for Extra Care by more than
half so has a significant impact on the
actual healthc care decisions that are
being taken um and when they dig into
why is this occurring it is occurring
because the algorithm uses Health cost
as a proxy for health needs right so
less money historically less money has
been spent on the care of black patients
again because of structural inequality
and historic inequality and thus if you
look at Health cost as a proxy you are
then your algorithm is then predicting
that the needs are lower right so um
what they suggested that no longer using
cost as the proxy for needs eliminates
the racial bias and again it's not
because the original risk assessment
tool used you know used race as a
predictor but because of these imp
correlates of in this case historic um
Health costs to predict Health needs
okay which is um which is biased against
blacks okay so we have this type of work
that's shown um where the where the
biases
are now uh zooming fast forward you know
by a few years we have um for instance
this seminal paper on the dang of
stochastic parrots and I think
stochastic parrots are still pretty much
the best way to describe what a large
language model is um actually looking at
you know some of the issues around large
language models and the training data
behind it which I'm sure many of you
have have very well aware of these large
data sets um based on text from the
internet that are being used to train
these models over represent honic
viewpoints or honic viewpoints and then
code biases potentially damaging
marginalized populations um you know it
averages across the mean right it's a
it's a probability distribution and
therefore the type of language and
statements and sentences that are being
over represented are the ones that are
likely to be reproduced by the language
model statistical
probability um generally large language
models as per their training data
therefore encode biases that exist in
societies such as gender stereotypes and
this has also been um been well
researched in the kind of machine
learning community and the the the NLP
is a natural language processing
community that has looked at things like
word embeddings which have made it
possible to commute compute similarity
between different concepts in language
um and since 2016 these these types of
issues are well
known ownership is erased of course and
this has been this is very much on
people's minds right now um we have
things happening like the times the New
York Times suing open Ai and Microsoft
over the use of copyrighted work um
current practices of training direct
data collection are predatory right I
mean no the you know chat U chat GPT
open a aren't going around asking for
permission they will just use whatever
is available to train their uh models
until they're told that it's unlawful
probably so um there should be um kind
of lawsuits and so on outcomes from
those um should actually be making a
difference in future it's unclear
whether there is copyright or
intellectual property property
infringement these things are still
being hashed out in courts right
now but if you look at things like
Creative Commons uh attribution which uh
creative commment license uh which
requires attribution right you must give
appropriate credit provide a link to the
license and indicate where changes were
made and and that's kind of currently
how things on the internet kind of work
there's these licenses and you can use
stuff but there's a license attached to
it and you must appropriate give
appropriate credit um which is not
possible the ways in which most
languages language models work although
of course changes are being made all the
time uh so this is very much changing
Feast as well um okay skipping forward
so
the are four mentioned cross-domain
capabilities that really some people say
make things unsafe an example was for
instance metas large language models
Galactica um which has been described at
um as being able to do things like
summarize academic
papers solve maths problems generate
Wiki articles write scientific code and
so on so it's clearly kind of describing
cross-domain capabilities was then three
days later actually produced very
harmful texts that had um content um
about the benefits of suicide eating
crushed glass
anti-Semitism and by homosexuals are
evil right so you have inherently um
problems with cost domain capabilities
and make things unsafe and Heidi clar
from the AI now Institute um and this
paper talks about unscoped system I
think this is a really important Point
um which which is that this kind of lack
of an operational Envelope as it's
called in the kind of um cyber security
uh
domain which makes it really well
impossible or intractable to evaluate
the risks and um and safety because of
the sheer number of applications and and
therefore for risks post if you don't
know what it's for how are you how can
you evaluate whether it's safe and
whether it's exhibiting expected
behaviors right and the the thesis is
that the quest to create AGI has
actually resulted in these current non
AGI systems but they are unscoped and
thus unsafe right and you cannot design
the appropriate tests to determine what
the system should be used for looking at
some political taex like for instance
The Bletchley Park declaration which was
in the UK after the AI safety Summit
last year um being highlighted that
there are particular safety risks
arising from these Frontier models as
they're sometimes called um being highly
capable general purpose AI models um
that perform a wide variety of tasks the
same kind of unscoped uh acknowledging
the kind of unscoped nature of these
systems which also then can lead to
things like being problematic in things
like cyber attacks and
disinformation um making it easier for
Bad actors um and you know worries about
being used in things like uh grooming
vulnerable people online uh and
generating kind of harmful content as
well uh we we all we are all aware as
well of the the the kind of worries
there are around jobs being at risk last
year we've seen um towards the end of
the year in Hollywood writers and actors
going on strike um
and you know when you have an economy
that's built around the ideas or bu
built around ideas and intellectual
capital and the written word clearly
there are fears that you have generative
AI replacing jobs um that of those that
are in the in that business artists and
creators in particular writers as well
um and that was also in part what the
strikes were about protections from
generative AI for screenwriters and also
for actors uh from using the actor's
likeness in perpetuity to create
um deep fakes and things like this um
without further royalty so you know as
an actor forever forever more you're
scanned and forever more your digital
likeness can be used and that's also
protections from these types of things
um is what these strikes were about and
they achieved
some um promising contracts um that are
actually providing some safeguards so
these things really matter people going
on strike people protesting against uh
certain kind of uh worries about
AI the human cost uh or the ugly
underbelly of AI is is also that there
are um vast amounts of people being
employed this is kind of hidden Labor uh
and and typically in a globalized
Outsourcing or offshoring culture where
for instance in the Kenyon in Kenya you
have a large um cohort of um industry
working basically in labeling uh and
moderating online content and you also
have um you know these kinds of
reports um Kenyon workers being paid
very little to review um horrible types
of content
um and also you know typically then
there's no further support provided to
people who are doing this this type of
of work where you would expect you know
uh counseling and other types of support
being
provided um so then finally and
certainly not least the Environ
environmental impact of uh generative AI
is absolutely astonishing so the
training of large language
requires not just electricity but also
vast amounts of water for cooling and um
current AI is by by some predicted
Spectrum article to track uh to annually
consume as much electricity as the
entire country of Ireland um the global
demand for water could be half of that
of the United Kingdom by
2027 um and you have like some evidence
coming out from companies heavily
investing in gen generative air like
Microsoft sustainability report
suggesting a nearly 30% one3 increase in
electricity consumption for the whole
company since 2020 and they're blaming
the data centers um for that increase
okay great stuff so what can we do okay
so I'll try and Breeze through this and
wrap up in the next five minutes um by
we I mean researchers in HCI but also
other adjacent disciplines digital
Humanities Science and Technology
studies computer science and so on what
we can do is understand the harms better
for example we can look at Reliance how
susceptible are people to harm from
hallucinated advice but let's play real
or fake are you required to go to court
if you breach a contract right you have
two two answers to that question so who
thinks this is fake as in this is CH gbt
generated hand sure hand reading that
still and who think okay like one of
them is real generated by a lawyer who
knows how to answer that question and
the other one is generated by chbt who
thinks this is
chat okay who thinks this
CH more people think that's CH GT okay
yeah sorry you didn't have time to read
that in but it's actually that's CHT and
that's that's the real answer okay so
you are susceptible you've just
demonstrated how you are susceptible to
false advice from
um from a large language model if it's
not if it's not branded as such right it
it's not labeled as such and you know
things like the EU a make it very clear
that you have to label and make very
clear the origin of the content and
whether it's based by based on the
language model other things we can do is
adoption Studies by adoption I mean
technology adoption people are already
using chat GPT in their everyday lives
the rabbits out of the hat right
students are using it of course um you
know to to to to our um well worry and
and challenge but we also have
professionals and creatives using
generative AI part of their work here is
video of MKBHD talking about AI tools
and how it can help in Creative
practices authors are using it to help
in their Drafting and so on and so we
can do studies that actually look at
embedding of this tool into work
practices and something I've done a lot
of um this type of research for instance
we look at things like have looked at
things like voice interfaces and
everyday life and how that gets embedded
and you can do similar types of studies
um
and I'm going to skip over this
example um but you can imagine how you
can actually study in detail how
something like chat GT gets used to
answer or to help with a knowledge task
like I was writing a talk about chat GPT
and how it works I used CHT itself to
tell me how it worked so I don't know
how truthful it was in the end but it
kind of was a fun experiment to do um
that I could we could then use um to
study how other people might be actually
interacting with these things um and how
it might help them in in what they do so
other things we can do is benchmarking
fairness transparency and accountability
of systems um but doing things like
model testing we can develop technical
improvements to improve fairness and
transparency we can develop auditing
methods for such systems we can help
advise companies on how to build more
responsible AI help regulators and
policy
makers uh with uh research evidence to
help them kind of make empirically
informed
decisions so coming back to this
question then can we ever build can it
ever be responsible to build artificial
general intelligence there is plenty of
evidence of which I've gone over over
much of already to suggest probably
not okay so I think we need to really
ask ourselves what are we investing in
and what why are we what you know what
are we doing why are we like like um
chasing this kind of this kind of dream
of AGI instead perhaps we should build
well scoped AI you could say narrow AI
right but it's not about that it's about
it being well scoped so that you can
allow to assess the reliability safety
and fairness um you can Define the
operational envelope you can do things
like um this would be able to allow us
to do things like understand the
specification right specifications of
expected behavior and allow for testing
understanded and Divergent operating
conditions and what they are and so on
and you know do things like allow for
experiment um that are designed to test
whether the functionality has construct
validity so does it actually Faithfully
portray how it's supposed to behave in
the real
world and in summary so what have I
talked about I've talked about how the
Quest for AGI creates unscoped systems
that are unsafe at the moment these are
largely the kind of language models that
we see the large language
models um the purported experimental
risk sorry existential risk is what
that's meant to read um is actually a
distraction that funnels billions of
dollars into the pockets of those
alleging to build safe Ai and this also
fuels some confusion I think in the
public discourse around AI see this um
the bit on cartery mistakes and actually
in the meanwhile the real problems and
opportunities as well uh around
um AI don't get enough attention funding
and resources so there's a lot of work
to do for us as researchers from
empirical studies to technological
development to advisory work to working
with regulated and policy makers thank
you very much
you want to shout
or was very interesting um I really like
the uh listing of the RIS at the end um
and also some the recommendations I
thought were very possible um I wanted
to ask about the middle point about the
category mistake yeah um
so I mean I'll be straightforward uh I I
I didn't see an argument for that claim
right so you said that it's or you
implied that it was false to say that AI
systems can have beliefs or have minds
or real intelligence or anything like
that right and even compared it to this
notion of a category mistake saying
we're attributing a property to
something that uh that's not the kind of
thing that can have that kind of
property but I didn't really see why
that was I mean do we have any evidence
for this is this an invincible thing
that um
um I mean uh either like it's the kind
of concept that machine can possess as
opposed to a person and why can't we
think of these things as person I don't
know maybe you can elaborate a little
bit there about the
reasons coming from someone with
background in cognitive um psychology
and and so on this is this is definitely
um a good challenge so I think we you
know I think the the language is there
right you wouldn't you wouldn't disagree
that there is there is the the
ascription of what typically we we know
as kind of cognitive faculties describe
as cognitive processes to AI to
technology right uh starting with the
term AI itself artificial intelligence
right um and you know I think that it
can be fun to and I think that's
probably where where it started is like
oh as a kind of fun experiment um to
talk about AI um and and and how things
can be equated and that could be a fun
thing to do for people who are
developing AI oh this is like a neural
network you know we're going to build an
artificial neural network um and so on
but I think that then leads to
confusion um and false expectations and
I am a big fan of trying to kind of like
be quite be as dry as possible in
describing what technology does um so I
think we should talk about
computationally intensive statistics and
not artificial intelligence for example
but it doesn't sound as sexy or
appealing does it so um but I think
there is it's like what is the work
that's doing that language that we are
using to apply to Ai and I think it
could be problematic in some ways and
has has led to all these hype Cycles
we've seen with AI over the
over the decades has been going on for
already so I think it's not helping the
cause um I'm not against a I I think
it's there are some fantastic
opportunities of course with it and it
can really help with solving some real
problems um but to get it right I think
the language we use could be more well
considered sometimes
have question related who plans we have
at um uh UT University of applied
sciences we have 30,000 students we have
about 10,000 people doing all kind of
other stuff and now the idea is for we
have an agreement with Microsoft to use
co-pilots which means um open AI llm
used and information comes in but does
not go back yeah and the idea is we will
have two tals one for students one for
the other people and then the system can
learn from whatever documents are being
uh put into that system I thought maybe
you have some suggestions on because be
unscoped will be multi doain maybe would
be you had some device
so I think it's the same like many
universities I'm also aware University
nottingam is is uh is working on some of
these agreements um I think the details
matter in how in like what's what's
happening with the data like you say you
know the it would be problematic if
anything students say to the system you
know that that data is then being used
to
retrain um the system which might might
inadvertently mean there's some
intellectual property theft uh or loss
that's occurring through that so
universities need to obviously Safeguard
against that uh so Details Matter a lot
and then I think the other thing that
needs to happen is much more testing of
the safety of these types of systems so
benchmarking the fness or um accuracy
depending and also being very clear
about what the systems can be used for
right so so you can for instance I would
argue that with large language models
that can be very useful if all you want
to do is generate some text that you
want to use for inspiration okay but if
you're asking it to to to tell you about
historical facts it's not very good at
that because it's not what it's designed
to do really so I think we can be better
at with these unscoped systems actually
saying okay it's fine if you use it like
this so I think a lot of it is
educational around what theis are um and
kind of you know more or less risky
types of uses of uh of the systems for
for students and staff
universities yeah thanks wonderful
thanks for setting out the the risks
andal issues um I want push on a little
bit because you ended on this question
is it even ethical to try and develop AI
which I think you why the answer might
be no many of the risks that you
outlined are are already current risks
and arms of not even AI just of large
models um resource costs environmental
issues exploitation of workers and so on
so so just to push back that question at
one level is it even ethical to develop
these trans scope systems let Al to say
Implement them at universities or in our
workplace yeah it's a good question I
think
um you know yeah there's a lot of
challenges that need to be addressed
there and I think I mean what I was
trying to say is I guess is that
actually we got to this point because
this quest for AGI you know we got to
the large language models thing and this
kind of idea that it can do everything
you know you just if you just know how
to prompt it right and and input and
there's all this this prompt engineering
stuff that's coming out now um so yeah I
I agree that I think there is a lot of
problems with large language models as
they are currently um but there is also
I was trying to make the point you know
that with AI there's a lot of other
classic AI machine learning
classification right um there's a lot of
problems to do with
discrimination um and in the use of I
think Pro particularly problematic where
I use AI to judge people whether that's
predicting risk of reoffending or
predicting um you know treatments you
know you're making judgments about
people and I think wherever you do that
it's really really uh really risky to to
use AI um to make such predictions and
yeah so I think yes the the the Quest
for AGI has really get got us to a point
where there's a lot of problems with uh
with the way things are
com thank that was very interesting for
um my name is m I'm H University not Ani
expert but I'm kind of profession where
AI is going to be used to make judment
about people whether they're going to
come into the organization or they
already there um and I understand your
point where you say well policy makers
may be wrong to thinking that the safe
AI will be the better alternative than
well scope a but my question you would
be then what arguments do you believe um
could convince the broad public that a
well scope a is better say because the
kind of people that I'm working with
including myself AI experts so the ter
savei convince I think more people than
well scope because they need to di into
the technical parts and
eal right I would say State AI is well
SED
AI That's how I would answer that and
actually but like from a technical
perspective I think you know what safe
AI
means can be should be open up more and
I think some a term like well scoped we
can go through through and you know you
can Define the test criteria the success
criteria um the kind of operational
envelope um the expected Behavior you
know that's kind of the things you can
do and then you can test for that and
that's how you show it safe so I think
being well scoped is a mechanism to
achieve safe AI I hope
[Music]
put down as position of thinking um how
can that even be functionally possible
with this force of development that we
are facing uh it's it just seems like
it's
counterintuitive with all um this not
only competition but the course of
development that's that's ahead of us
and the second is kind of like this
argument quite a number of people have
been making that AGI is just not really
on this moment in time that we wake up
and all of a sudden your term AI soon
appears on the horizon but it just goes
with this force of
infrastructural um AI the more we go
through that quote unquote you beit you
bequ a way of folding it into the
infrastructure the more you get closer
to what's called AI on whether you want
to make a fiction out of it or not and
then the second also question sort of
building up on what has just said about
um this idea of like assigning agency to
to Ai and whether uh or not that is
something we want to go forward do you
differentiate
between um let's say an agent system
that AI would be or a a tool that
doesn't have that kind of capability at
all that we refer to sort of this
breaking point of agent systems and all
the technologies that sort of are
categorized on the other side and if so
then um I mean you know the word from L
etc etc but do you think of this sort of
in some agency to uh automated systems
as something possibly
leg yeah um I think you know I think
it's uh whether or not I don't want to
make it like a judgment about this about
agency being like should we do it or not
but people do do it and we can't as
humans we can't get away from a signing
agency there are studies that show you
know there a a spinning very simple
robot you know that doesn't have agency
it's just kind of an automated thing
people ascribe agency to it very easily
right and we and we have maybe an
ingrain type of um way of thinking about
the world where things that move are
things that we very quickly and happily
ascribe agency to so we can't get away
from that and so with that there maybe
come some responsibility in terms of how
we frame things um that are
technological um and you know people
will still describe agency to it but I
think those that are driving the
narratives around um Technologies could
also help the point about the media and
how media portrays and talks about AI
because I think the media has an
educational Mission um as to
universities and other types of uh
actors they be more responsible or think
you know kind of like more through how
they portray things and yeah I think
portraying AI as tools that help people
do things um jobs different types of uh
important tasks like diagnosing and
things like that really can help so I
think you know I am a proponent of yeah
talking about AI as a tool um and
looking at how it fits into human
practices or disrupts human practices um
as a tool
and um kind of not losing sight of the
humans that are building the systems
deploying them using them are being
affected by them I think that's the
important part when we think about
responsible AI right it's it's kind of
bringing the agency back to the humans
that are using the technology which may
may or may not be you know software
agents um themselves but yeah not I
think bringing it back to the the humans
and I kind of forgot your first question
already so maybe maybe we'll take it in
the
pup um
yeah yeah
sure sorry to keep you longer than
expected and compare the artificial
intelligence to human intelligence
about the I think just a human get well
raised and being harmless and all uh
good to the community I think there
might be just maybe philosophically I'm
not an expert in AI but there will be
ways to also get AI that will be only
good maybe by testing or by uh reward
and Punishment and things I have to just
get are to a good
place
not okay thank
you I can I can
um yeah I mean I think it's it's back to
that point about scoping um and you know
maybe the types of things you explain
are are ways of scoping um I think the
important thing is to be able to test
that it is safe and and it does behave
as expected and so on and and what
whatever the context of the AI is would
you know would change how how you might
go about that practically and what kind
of techniques you you might use to show
um these the expected
Behavior
yeah for

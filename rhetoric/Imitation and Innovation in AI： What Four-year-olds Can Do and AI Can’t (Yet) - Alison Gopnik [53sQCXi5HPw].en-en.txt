(audience applauding)
- [Alison] So what I'm
gonna do today is talk about
a couple of different ideas
about the relationship
between AI and childhood.
And what I'm gonna do first
is talk about what I think
is a fairly different way
of conceptualizing what AI,
particularly so with
the recent developers,
like the large language image models,
how we should think about those,
and how we should think about
them in relationship to humans
and to children, and
to human intelligence.
And then in the second
part, what I'm gonna do
is talk about some of the
things that children can do,
the kinds of intelligence
that children have,
which you don't see in these
other kinds of systems.
And then at the very end,
I'm just gonna point to some
ways that we might be able
to develop artificial
systems that would have
some of the capacities that we see
even in very young children.
Okay?
And you can read more about
this in a more popular form
in this APS presidential column,
and then in an academic form
in this paper that's in press
and that you can find on archive.
All right, so let me start
out by thinking about
this reconceptualization of AI.
So a very common way of thinking
about artificial intelligence systems,
including things like
large language models,
is as if they were individual agents,
as if they were individual
agents with a particular kind
of intelligence, moving
around the world, thinking,
deciding, planning, et cetera.
And it's interesting that
even if linguistically people
nowadays refer to "An AI," as
opposed to referring to "AI,"
that individual agents and
typically either people
think of them as being really incredibly,
or both really incredibly
smart agents, genius agents,
agents who are even smarter than we are,
and/or also really evil
agents like the Golem.
And I think it's interesting
that even before we had machines,
when people thought about what
it would be like if you had
an intelligent machine, they
thought it was gonna be evil.
That goes back to the Golem and Greek.
So this picture is, we have
these individual agents,
they might be good, they might be bad,
they might be smarter than us,
and we have to figure out
what it is that we're gonna do
with them now that we've
loosed them on the world.
And a lot of the
conversation is like this.
No, as I'm gonna say later,
it's not impossible to think
about an artificial
intelligence that was an agent,
but I think that is not the
right way of thinking about
almost all of the models that we have,
and especially things like the
large models that have been
so successful and influential recently.
Instead, I think the way
to think about these models
is as what I've called
cultural technologies.
What's a cultural technology?
Cultural technology is a technology
that allows individual
humans to take advantage
of all the information,
knowledge, skills, abilities,
that other humans have accumulated
over millennia of cultural history.
So you could argue that
language is kind of like the
(indistinct) cultural technology,
language is the thing that humans have
that lets us learn from other humans.
But over the course of human history,
one of the really interesting
things that's happened
that's led to massive qualitative
changes in how we function
is the development of better and better,
stronger and stronger,
more powerful new cultural technologies.
So an obvious example is writing.
So writing allows you not only to learn
from the post-menopausal grandmothers,
who by the way are the people
who are really the agents
of all this cultural work,
not only from the
post-menopausal grandmothers.
I can talk about that a bit later on.
That's what I did.
(audience laughing)
That's true.
The post-menopausal grandmothers
who are in your village,
once you've got writing,
then you can learn from the
grandmothers in very far
distant places, far
distant times in the past.
We know that writing had
a really major effect
on given intelligence.
If you think about print,
that's an even more dramatic
example of something
that enables you to get
information very quickly
from many different kinds of places.
And along with print go
institutions like libraries,
indexes, ways that you can
access that information.
And of course, most
recently you have things
like internet search or
Wikipedia, which are using
the digital technology to enable you to
much more effectively, much more quickly,
swiftly access all of these information
from other people around you.
Now that's a really different picture
than the agent picture.
So it would be incoherent, as
we used to say in philosophy,
to ask, "Does the Berkeley
library know more than I do?"
Right?
Well, in some sense, of course,
they'd have much more
information than I do,
but it's just silly.
It's just a mistake to ask
whether the library knows more than I do.
The library's technology, which enables me
to get information from
lots of other people,
lots of other agents.
Wikipedia, by the way,
I think is a particularly
interesting example of this
recently.
And this ability to have these kinds
of cultural technologies
are arguably, in itself,
the thing that makes
humans so intelligent.
So there are lots of
people in cognitive science
who have argued for what's
called the cultural niche.
This is the idea that what
makes us human is exactly this
capacity to take information
from other people
and use it to make progress for,
so people sometimes talk
about this as being like
the cultural ratchet.
So a whole group of people
go out of the environment,
they learn all sorts of things,
they pass that information
on to the next generation.
That next generation takes
off from that information,
uses it to learn yet more.
And that process is the thing
that has actually enabled
cities and civilization and
all the things for better
or for worse that humans are capable of.
So that's an idea about
something very basic
about human intelligence,
is that we rely on these
cultural technologies.
And it's interesting
that if you look at the,
I think that this is also
helpful because when we're
thinking about a lot of
the pragmatic questions,
about what the social
impact of AI is gonna be,
how we should deal with it,
I think it's actually very
helpful to think about these
historical examples of other
kinds of cultural technologies
and their impact on us.
Again, I think that's a much more helpful,
insightful way of thinking
about it than thinking about it
as has the Golem come to
life, and is it gonna come
and kill us all, or are
we all gonna disappear
and just have the super
intelligent system?
And it's interesting that,
if you look at conversation
about these technologies,
people always point out
that they have good features
and bad features.
So famously, Socrates thought that writing
was a really bad idea.
I won't go into this quote,
but I love this quote.
And I think it's interesting that,
especially like that last
sentence about, you know,
they seem to talk to you as
though they were intelligent
books do, but if you ask them anything,
they just go on telling
you the same thing forever,
which sounds so much like
talking to ChatGPT, right?
Well it's as if it's intelligent,
but then it just keeps
producing the same nonsense
over and over again, right?
So you can already see,
and Socrates pointed out
that if we relied too much on writing,
we would lose our capacity
to memorize all of Homer,
for example, which is indeed
something that very few people
can do nowadays.
And also, and interestingly,
that writing would be a great
source of misinformation.
So Socrates pointed out if
something was written in a book,
you couldn't interrogate it the same way
that you could interrogate a person.
And that would mean that you
would tend to think that things
that were written down were
true, even when they weren't.
And those issues are issues about how
they're going to affect us.
And also about misinformation,
of the possibility of
transmitting information
that's really useful and accurate,
but also transmitting information
that isn't useful or accurate.
That's an issue that comes up
with all of these cultural technologies.
The example of print, I think,
is a really interesting one.
In the late 18th century,
there were enormous technological changes,
which meant that although
print had been around
since Gutenberg,
suddenly everybody
essentially could print,
anybody with a little bit of capital.
You could set up a print shop,
and you could produce pamphlets.
And this, lots of people,
historians, have argued
that this had a transformative
effect on society.
And many of the things that we think of
as The Enlightenment, for
example, really depended
on having this technology,
printing, available.
So on one side of this picture,
that's actually Benjamin Franklin,
who famously was a printer's apprentice,
and it was his capacity to
print that arguably enabled
things like the American
Revolution to take place.
If you read,
I was recently reading
this wonderful biography
of Samuel Adams, and literally
Adams was in the print shop
the night of the Boston Massacre,
making up the Boston Massacre.
This thing happened, there
were a bunch of protestors,
there were a bunch of
soldiers, something happened.
Adams went to the print
shop like 10 hours later
and wrote up the story
about what was going to be
the Boston Massacre, and
distributed it with print.
On the other hand, even
though we think about print
as this way to underpin things like the,
in The Enlightenment,
diffusion of knowledge,
it also, like writing, had the
capacity for misinformation
from the very beginning.
And I highly, this is a, by
now, an ancient classic book,
Robert Darden's book of
the literary underground
Yale regime, another
nice Berkeley-ish example
of how humanities and science
can speak to each other.
And what Darden actually did
was go back and read not just,
you know, "Common Sense"
from Benjamin Franklin,
but everything that those
printing presses produced.
And you will be amazed
to hear that most of it
was really terrible. (laughs)
Most of it was, a lot
of it was softcore porn,
and a lot of it, well,
the rest of it was libel.
So the, "Let them eat cake," for example,
was actually a meme.
It was actually a meme that
was invented in the context
of print.
Marie Antoinette never said it,
but it was obviously a meme that,
like memes and misinformation nowadays,
can have really strong,
and in this case arguably
very terrible, offense.
Now one of the things that,
I'll just go through this quickly,
but I think it's interesting,
is that you might say,
"Well, okay,
"why is it if what's going on
in these cultural technologies
"is that we're conglomerating information
"from lots of different sources,
"why do we so often think of
them as if they're agents?"
So why do we treat them
as if they're agents?
Why do we say, "ChatGPT said this?"
And it's almost impossible
when you're talking
about these systems not to say,
"It said this, it did this,
"it could do this, it
can't do this other thing."
And I think it's
interesting that if you look
at past cultural technologies,
very often the form of information
that a cultural technology uses
is to have a fictional agent.
So it's hard to tell
people very general things
about all the information
that other people have accumulated.
Often it's much more effective
to have a made up agent
who's actually illustrating
the information
that you want to tell.
And it's interesting that
if you look even back
in the Pleistocene, look at
hunter-gatherer communities,
one of the really important
things that happens is,
especially in the evening,
people sit around and, the
postmenopausal grandmothers,
who are always the agents of change,
talk to the little kids, and
some of the adults as well,
about what are the myths,
what are the stories,
what are the things that are
important in our culture?
What are the things that you need to know?
What are the big things
that you need to know?
And the way that you do that
is through telling stories.
The way you do it is by
having a fictional agent
who can exemplify the
things that you think
are really culturally important.
So that's actually something
that we have quite
characteristically done.
Another example is, interestingly,
an anthropologist of religion just has
an interesting paper about this,
in the context of LLMs.
Another example, if you think about gods,
gods sort of serve the same function.
So even though gods don't actually exist,
there are ways of passing on information
about what you think is important
in the context of culture.
So don't say that I said
that large language models
were gods, or maybe you
could say they're only gods
that don't exist, right? (laughs)
They're that kind of god,
the god that doesn't exist.
And last thing that I love as the daughter
of an English professor is that, in fact,
that 18th century print
revolution that I talked about
also came with this new literary floor,
which was a novel.
And it turns out that the very
first novels, like I said,
this is a Samuel Richardson, "Pamela,"
even though Pamela was a made up person,
she became incredibly popular.
People would line up in
the streets to find out
the next installment of
what had happened to her.
And Samuel Richardson was a printer.
The printing and these new
fictional characters in novels
who could tell you something
about what's going on
around you happened in concert.
So what I wanna argue
is that that's the way
that we should be thinking
about these large laws.
They're different, they're not,
it's not like they're
just the same as print.
Each one of these new
cultural technologies
has different characteristics,
different kinds of powers.
I think they're arguably
more powerful than print,
because they can generalize
and they can agglomerate
enormous, giant amounts of information,
all the information that's
there on the internet.
But I think the structure of
how they work is the same.
And I think in that context, again,
we can get some important
historical insight by thinking
about how we manage the
previous cultural technologies.
So every time a cultural
technology develops,
you're faced with these problems
about things like this information.
And typically what's
happened is that various
kinds of norms, rules,
regulations, laws, principles,
develop in parallel
that actually enable you
to get the benefit, rather than the cost,
of a new cultural technology.
So if you think about things
like just norms of truth,
(indistinct) that you
should tell the truth
instead of lying,
or things like editors,
journalism schools, fact checkers,
the developments of print in
the 18th century came with
these new things called
newspapers and editors,
which hadn't existed before.
So instead of just looking
at all the pamphlets,
you could say, "Okay,
"this printed paper is 'The
New York Times,' and therefore
"I'm going to give it a kind of authority
"that I wouldn't give to
just a random pamphlet
"that was on the street."
And I think that's the thing
that we have to do with AI.
So can't just sort of say,
"Yeah, it'll be fine."
These aren't like evil
Golems, so everything's okay.
What we need to do is to have
the same kind of mechanisms
that we have for other kinds
of cultural technologies.
Okay, so that's the
first part of the talk,
making this argument about Ais
as these kind of cultural
technologies that are accumulating
lots and lots and lots of information,
all the information on the internet.
Why isn't that enough?
Why isn't that enough to be intelligent?
If as I've argued, so
much of human intelligence
comes from this capacity
to learn from other people,
to extract information from other people,
why is it that a system
that can do that really well
and much better than any
individual human can,
why wouldn't that (indistinct)?
And to answer that question,
I wanna turn from thinking
about AI systems as technologies
to thinking about children.
Why children in particular?
Well, famously in the Turing,
the famous Turing paperwork,
it talks about the Turing test.
There's this wonderful segue
that nobody ever notices,
or didn't notice until recently,
which is he starts out saying,
"Okay, do you wanna know if
the system is intelligent?
"Here's what you do, you
do the invitation test."
But then he suddenly shifts and said,
"You know, maybe that's
not the right test.
"If we really wanted to know
"whether our system is intelligent,
"we'd have to know whether it could learn
"from its experience the
way that a human child,
"for example, learns."
And he very explicitly says,
"Instead of producing a program
that can simulate the adult,
"why not try to simulate the child?"
And in a sense, what something
like large models are doing
is accumulating all the information
from all those adult minds
that are out there, right?
That's the great power of those systems.
But what children, although
children do do that,
children do something rather different.
What children do is actually
go out into the world
and learn about the world independently
of all the information
that they're getting
from other people.
And over the past several years,
more and more people within
AI are trying to use children
who are the best learners
that we know of as a model
for how we could design AI systems
that could learn in this way as well.
The title of the paper that I mentioned is
"Transmission Versus Truth,"
and what large models
and the typical kind
of AI models that trade
large amounts of data can do
is transmit information very well.
What they're not good at doing
is going out into the world
and finding the truth,
which is the thing that
kids are very good at doing.
So from that perspective, for example,
when people talk about the problem
of something like ChatGPT hallucinating,
which is something that
everyone around has seen,
it's not that they're hallucinating,
it's that they just don't care.
There's nothing in
their objective function
that makes a difference
for truth and falsity.
So if you just imagine,
you know, as Socrates said,
books can have, the books in
the library don't tell you
whether what's in them is true or not.
They're just there to
transmit information.
But as we'll see, kids care
tremendously about what's true.
And arguably kids are,
that's their greatest motivation,
is going out in the world
and trying to figure
out how the world works.
And we know a bit from
developmental cognitive science
about how they do that.
And in this DARPA machine
common sense program
that we've had at Berkeley
with great AI people
like (indistinct),
we've heard in this series
before (indistinct),
we figured that, I discovered
that working with DARPA,
the most important thing
is to get good acronyms.
(audience laughing)
So,
then you've got DARPA worked out.
So our acronym for what
we're doing is MESS,
which is appropriate for children,
which is to try to design
these model building
exploratory social learning systems.
So how do children learn so much,
and how do they learn the
truth about the world?
They first of all don't
just get statistics,
although we have a lot
of evidence that children
are amazingly good at
statistical inference,
much better than we ever
would've thought before.
But they don't just aggregate statistics
the way that something like
a large language model does.
Instead they actually pull
out abstract causal models,
intuitive theories, from
that statistical evidence.
My first book is called
"The Scientist in the Crib,"
and I and lots of other
cognitive scientists
have used this metaphor.
They're not just looking at statistics
and making predictions,
they're pulling out causal structure
and then using that causal structure
to understand how the world works.
And that's one of the most important ways
that humans in science,
but also in everyday life,
figure out to truth about the world.
The other thing that makes
what children are doing
very different from
what typical AI systems
that particularly large models do
is that they're active learners.
They actually go out into the
world and actively try to get
the kind of data that will
be relevant to the problems
that they're trying to solve.
And there's more and more evidence,
I'll give you an example of this in a bit,
that they're super
sophisticated in doing this.
When scientists do this, of course,
we call it having an
experimental research program.
And when two-year-olds do it,
we call it getting into everything.
But it turns out that
if you actually study
the getting into everything,
that what's going on is
something that's much more like
a scientific research program
than you might imagine.
So children aren't just
passively absorbing data
from the program around them, for example,
they're actually actively
going out and trying to make
discovery to find out about
things that are going on
in the external world,
not just, say, in the
internet (indistinct).
And they do also have these
capacities for social learning,
as I mentioned before,
for extracting information
from the other people around them.
And there's some lovely studies of this.
Sometimes what they do is
just sort of mindlessly
try to agglomerate that information,
but a lot of times what happens
is that they're balancing
that, we'll see in a minute,
against this drive for the truth.
So as opposed to just accepting
what other people tell them,
they try to balance that
against other kinds of
information they've got,
including the information
they've got through their own
experimentation and exploratory play.
Okay, let's say a bit about
how does that picture of
what children are doing,
how does that interact with
this really remarkable set
of (indistinct) we've had
over the last year or so
about how powerful these large models are?
What can large models tell us
about how children are learning,
and what can children tell us
about how large models work?
Well, I think one way of thinking about it
is that we can figure out
what kinds of information
are available (indistinct) transmission
through just looking at the information
that other people give you,
and what kinds of information
do you need to discover for yourself.
And I think syntax is
a really good example
where typically people in
linguistics, for example,
had thought you can't get
to syntax just by looking
at the statistics of linguistic input.
And it turns out that
actually no, you can,
this is something that the
systems genuinely are incredibly,
incredibly good at.
And it looks as if there's
enough information in the data
to extract the kind of
structure that you need
to be able to produce
grammatical sentences,
which is not something that I think
we would've known before.
Another thing to say,
which makes this problem
a little more difficult
in some ways from the
cognitive science perspective,
the old fashioned,
as in like last year, or three
months ago, or yesterday,
vanilla large language models,
the ones that just work by
predicting what the next word
or token is gonna be,
are more useful than some
of these more recent ones.
So what's happened more recently
is that rather than just
extracting the statistics
of the information,
you've added things like
reinforcement learning
from human feedback, which
means that now we have humans,
and I highly recommend a
piece in "New York Magazine"
recently that was
describing this, you know,
giant factories worth of
people in places like Nigeria
and Kenya who are just
sitting there and looking
at the output of these systems and saying,
"Yeah, that's good," or
"That's not good," right?
That's reinforcement
running through humans.
And we have things like prompt
engineering and fine tuning
that are also designed to do this.
Now what that means is it
becomes very hard to figure out
what's actually going on under
the hood in these systems.
Whereas if you just have a kind
of classic LLM, you can say,
"Okay, this is information
that's there in the language,
"or is information that's there in images,
"it's a large image model."
Once you get reinforcement
learning from human feedback,
it's much harder to know what
exactly is the information
that this system is encoding.
But from a conceptual perspective,
from this perspective of is it an agent
or a cultural technology,
it's still true that what
something like RLHF is doing
is not enabling a system to go out
and find out about the truth.
All it's doing is giving it
yet another way of getting
information from other people,
a somewhat less obvious
and transparent way
than the kind of classic
LLM systems are.
And another thing that they,
that large models can teach us,
which I think is really
interesting and sort of under,
has been under-studied, under-initiated,
is what kinds of cognitive
capacities are facilitated
by a new technology.
So what sort of things can
we do when we have print
or when we have writing
that we couldn't do before?
And you can probably
already think about the fact
that we can remember things
and we can set things down
and go back to them.
There's all sorts of
cognitive capacities we have
from those technologies.
And a really interesting
empirical question is,
mathematics is another nice example,
where we couldn't have math
until we had our notation system
that would let us write down equations.
So I think a really
interesting question is,
are there cognitive capacities
that will be enabled
by these new cultural technologies
that would be different
from the cognitive capacities
that were available to us
in the past?
But again, that's a very different way
of structuring the problem than saying,
"Are they smarter than us or
are they not smarter than us?"
Do they, whoever they are,
have those cognitive (indistinct).
Okay.
But I also think that children
in developmental cognitive
science can teach us
a lot of things about how to think
about large language
models and understand them.
And one of the first things
that I think is really important,
I will jump up and down about this,
is that there's no such thing
as general intelligence.
There's little bits of stuff
about IQ in psychometric
literature, but from a
cognitive science perspective,
there just isn't anything
that's general intelligence,
artificial or natural.
So the whole sort of,
there's this kind of a intuitive
theory of intelligence,
which is, it's like a fluid
or it's like a, you know,
a force that's out there,
and you'll have the force of intelligence
and you don't have the
force of intelligence.
You're one of those guys,
guys being irrelevant category here,
who's got a whole lot of the
force or else you're a guy
who doesn't have so much of the force.
And that's just not the way
that cognitive scientists think
about intelligence.
Instead, what you've
got are a whole passel,
a whole lot of different
cognitive capacities,
and very characteristically,
their intention with one another.
So being really good at
exploration, for instance,
might can make you worse at
exploitation and vice versa.
Being really good at focused attention
makes you worse at extracting
general information.
So what we need to do is to
actually work out in detail
what are the kinds of underlying processes
that a particular system,
artificial or natural,
is using in a particular context?
And try to say something about
that, rather than saying,
"Do they have sparks of intelligence
"or do they not have
sparks of intelligence?"
And from a methodological
point of view as well,
children can tell us something
about how to understand
the cognitive capacities
of an alien system.
I highly recommend this
paper by Mike Frank,
and one of the great things about being
a developmental cognitive
scientist is we can't just
assume things about the
cognitive capacities
of the very intelligent
creatures that we're studying.
We actually have to go
out and do very systematic
kinds of experimentation
to try and figure that out.
And I think importantly,
we have to do something that I think of
as anti (indistinct).
So when we're doing development,
when we're doing a
developmental experiment,
what we have to do is make
sure that we don't have
any prompts for the children.
Because what we wanna do is figure out
what's the underlying
cognitive capacities, not,
"Are the children capable
of using our prompts
"to give us the answer that
we want them to give us?"
So it's kind of the
opposite of what happens
in a lot of LLMs.
We really wanna understand
cognitive capacities,
we need to have control conditions,
we need to do these very
careful experiments.
And I think development can
provide a lot of examples
of how to do this.
And if we wanna say,
"Is this system intelligent,"
or "Can the system do x,"
can the system do theory
mind, for example,
to take one that is dear to my heart,
we need to actually go out and,
like developmental psychologists
did, spend 20 years
doing experiments to figure out whether
and what children
understand about the mind.
My life would've been much
easier if I could have sat down
with a three year old in
1986 and talked to them
for a couple of days and
then decided whether they had
their (indistinct).
Okay.
So far I've been talking
about some of these capacities
that children have.
And I mentioned before that
these cognitive capacities
for faithful cultural
transmission are intention
with cognitive capacities
that enable new discoveries
about the external world.
Things like exploration,
or experimentation,
or causal discovery, or induction.
And what we've been doing
is using studies of children
and then comparing them to
studies of artificial systems
to work out how is it that
that kind of innovation works?
What is it that children are doing
that enables them to innovate,
that enables them to learn
new things about the world?
And we know that children
are learning these intuitive
causal models, and they seem
to do this through exploration.
And the research program that
we've had is to put children
and agents in the same online environments
and that way we can,
same sort of unrestricted
online environments,
let the children and the
agents explore freely
in those environments and then
see what kinds of discoveries
can the children or the agents make
about how those environments work.
And one of the first examples of this,
this is an experiment with
a bunch of people here
in computer science.
What we were trying to
do was see if children
could figure out causal structure.
As I mentioned, the causal
structure is really important.
And in our developmental
work, for a long time
we've used this little
box, the blicket detector.
And the blicket detector is
a little box that lights up
and plays music when you
put some things on it
and not others.
And with this very simple device,
you can actually ask quite
sophisticated questions
about the causal structure of this device.
How does it work?
Which ones are blickets, what will happen?
Can you make the device go?
So what we've done in the
past is to show children
different patterns of evidence
about how the system works,
again, as if they were little scientists.
And then we've asked
them to draw the right
causal conclusions about
how the system works.
In this experiment,
because we now had the online experiment,
the online environment, we
did something different.
What we did was to
actually let the children
explore themselves.
And I'll show you, whoops,
this was during Covid,
he invented this virtual blicket detector.
So now you can put things on the machine
in various combinations and permutations,
and sometimes the machine lights up
and sometimes it doesn't.
And here's a child actually playing.
This is a four-year-old.
And perhaps the most striking
thing about these experiments
is that current day
four-year-olds are absolutely
perfectly happy to figure out and explore
in this virtual online environment.
I'm not sure that would've
been true 20 years ago
when we first invented blicket detector.
And then even more remarkably,
what we discovered,
and I won't go into all the
details of this experiment,
what we discovered was that
the children were very good
at doing exactly the right things
to figure out how the system worked.
So most of the children, just in the space
of about 20 trials, 20
things that they did,
20 choices about what to put
on and what not to put on,
figured out what the causal,
what the underlying causal structure was.
So they were really good at doing this.
They figured out which ones were blickets,
they figured out what they had
to do to make the machine go.
And they were much better at this than any
of the large language
models that were tested.
Again, maybe you could do a
bunch of prompt engineering
to persuade the model to be able to solve
this particular task,
but just based on their general knowledge,
they did incredibly badly with this task.
They were not good at this task.
And you can kind of see why, right?
Even though they gave us back references
to our blicket detector
papers as a way of answering
what was going on in this task.
Of course this was a new
machine that didn't work
like any of the machines that we had
in our previous blicket detector papers.
So it could tell you,
"Okay, here's this
Gopnik blicket detector,
"here's how it works."
From reading the papers,
what it couldn't do was do a
bunch of experiments and use it
to find out that this detector
works really differently
from any detector that you've seen before.
And in a new series of
experiments that we're doing,
that's also in that prospectus paper,
we've been doing a similar
kind of project with toys.
So in this project what we
wanted to do was try and look at
another way that you could
use causal information
to try to understand novel
possibilities in the world.
And this involved a task
that involved innovating,
tool innovation, which again,
it's one of those things
that's incredibly powerfully
important human capacity.
And we were interested in
this from the perspective
of development as well as from AI.
We wanted to know if children
would be able to do that,
just as we were when we
were looking at the children
on the blicket detector.
So what we did was to give
children examples where there was
two objects that were highly
associated with one another.
So for example,
the scotch tape is highly
associated with the scissors,
much more associated with the scissors
than with the band-aids.
And if you ask,
you could ask people what
goes with the scotch tape,
the scissors, that's what should do.
But you then present a
particular causal problem.
Here's an example like tearing up paper.
And then you can ask,
"Which would be the right
thing to solve this problem?
"Would you be better off solving
this problem with scissors?
Or would you be better
off solving this problem
with a band-aid,
even though you haven't
already associated the band-aid
with the scissors?
And again, I'm sure that with
RLHF and Prompt Engineering,
you could get systems to do this,
but when we first tested this
with GPT-3, which was just
just looking at the
associations in the language,
what you could see was that the children
were much better at it than GPT-3.
And GPT-3 was actually not
any better than chance.
And again, if you think about it, right,
if you're just looking at
association of the language,
scissors and scotch tape are gonna be
much more closely associated.
If you wanna know what word
should you produce next
after you've heard scotch tape,
scissors is a much likelier
outcome than band-aids.
But if you're actually
trying to figure out
the causal properties of
band-aids and scissors and paper,
then you are going to have to
think about that information
in a different way.
And in the most recent
experiment that we've done,
we had 47 different scenarios,
each with a different set of objects.
And within each of these sets,
we have a reference object
like a tape, and there's a
superficially related object
or a functional object that's
not superficially related,
or a totally unrelated object.
And then people have to rate
both adults and children,
and LLMs have to rate them zero to five,
how useful each of these objects will be
in trying to solve this task.
So I ripped up a paper,
I have to decide what
would be most useful,
glue, which was an obvious sticky thing,
a bandage, scissors, or cotton balls.
And what we discovered was
that when we simply asked
the association questions, so we said,
"Which goes with which;
"does the scissors go
more with the scotch tape?
"Or do the cotton balls
or the band-aids go
"with the scotch tape," the
models were as good as people,
or at least some of the models.
Many of the models were as good as people,
both children and adults.
They seemed to be able to
pick out these associations
from the data.
But when it came to generating
the novel functional
capacities, even GPT-4, which
has all this extra RLHF,
was not as good as children or adults
at solving these tasks.
It was interesting that children
knew, even four-year-olds
were actually quite good
at solving these tasks,
although not quite as good as adults.
Okay.
So far what I've been suggesting
is that the kind of
capacities that you get,
especially from large models,
involve this kind of
transmission of information,
rather than exploration and truths.
That's been a kind of critical,
this has been a critical enterprise,
and we can give lots of other examples,
but how about the positive
part of the project?
The positive part of the
project is are there other kinds
of techniques that we could
use in artificial intelligence
or ways that we could design
new artificial intelligences
that would solve some of these problems
in the way that children do,
and therefore we hope have
some of the capacities
that children have.
And I'm just gonna just
briefly gesture at two projects
that we're doing right now
at Bayer, and both of them,
in fact, all the projects we're doing now
use this methodology that
I think is very exciting
of having online environments
where both humans and children
and agents get to freely
explore those environments,
rather than dictating "Here's
what the solutions are,"
or "Here's what the choices are,"
providing a supervised environment.
So we're trying to set up
unsupervised environments
that can be explored by
either agents or children.
And let me just mention two
techniques we're trying to use
to try to get the agents
to solve these problems
in a way that's more like children.
One is with (indistinct) rank here,
what we're trying to do is to
get intrinsically motivated
reinforcement learning.
So classic reinforcement learning involves
getting a particular kind of award.
And that turns out to be really powerful.
You can use those kinds of
systems to do things like
learning how to play go.
But what children are doing
when they're exploring
is not being,
it's also doing things
that actually lead them
to have less reward in any
kind of straightforward way.
And we've done a bunch
of empirical studies
that have shown this.
What children do is kind
of play, they go around,
they try things, sometimes they
work, sometimes they don't.
If they get too good at doing something,
they get bored or they try something else.
Their motivation seems to be
this kind of intrinsic motivation.
They're doing things for the
sake of learning rather than
doing things for the sake of
being rewarded in the long run.
And what we've been doing is
trying to take formal ideas
like information gain,
one that I'm very excited
about is empowerment,
and turn those into rewards.
So make the reward be the fact
that you are more competent
than you were before,
or make the reward be the
fact that you know something
that you didn't know before.
It's a way of trying to
put things like curiosity
and exploration into artificial systems.
So that's one line of research,
that's one set of techniques
that we've been doing.
We've also been doing this
in the context of play,
for example.
So we have recordings of children playing
in these online environments,
and then we can use their play
to try to figure out
what an agent could do
in that environment that would enable them
to master the environment.
And the second thing that we've been doing
is taking these ideas about,
particularly emphasizing
these ideas about causality.
This is work with
(indistinct) at A Deep Mind,
and also with Blake Richards that MiLA,
we've been looking at causal inference.
I'm trying to see if we could get
a kind of automated curriculum,
if we could design a
system that could have
what's called an automated curriculum.
What does that mean?
What that means is,
another thing that children
are extremely good at doing
is figuring out what they need to do
to be able to learn something
that in the long run
is gonna enable them to
learn something else.
So instead, the way that
these experiments have gone
is we are using the the
prop gen environment,
which is kind of a video game environment.
We start out by showing
a really hard level
that the kids can't do, and then we say,
"Could you decide how to play
a game that will help you
"to be able to play this other game
"that you'll eventually be rewarded from?"
And again, if you know children,
children are actually turned
out to be quite good at saying,
"Okay, first I have to
do the simpler thing,
"and doing the simpler thing
is gonna gimme the skills
"that I'll need to be able to do
"the more complicated thing."
And the same thing happens with agents.
So if you give agents
these high -dimensional,
really difficult tasks,
it takes them forever
to be able to solve it.
But if you could get
them to have a curriculum
where they'd say, "I
can't solve this task yet,
"let me try getting skills
in a simpler context
"and then applying them to the task,"
that would be another example of something
that children are doing
that should enable agents.
And again, those are some
of the kinds of things
that we're seeing at the moment.
So (indistinct).
So the overarching point,
I don't want to say, and
I don't think it's true,
that artificial systems are
never going to be able to do
the kinds of things that humans are doing,
or that human children are doing.
But I think the very
progress that we've made
gives us a sense of the dimensions,
the landscape of the kinds of
things that are gonna turn out
to be much easier to do, or
at least the kinds of things
that we're gonna be able
to do with sufficient data
and sufficient compute,
and the kinds of things
that are still going
to be very challenging,
even with very large amounts of data
and large amounts of compute,
the kinds of things that we're gonna need
other kinds of capacities to solve,
and in particular the kinds
of things that are going
to demand these truth
(indistinct), that are gonna demand
going out into the real world,
getting new data, changing,
modifying what you do.
And those are exactly the kinds of things
that the incredibly brilliant
but strange little agents
all around us that we
don't pay much attention to
are doing every day.
And the interesting challenge
would be to try and figure out
what computations are those
brains in those beautiful little
fuzzy heads doing that
we could actually use
in an artificial system.
So let (indistinct).
(audience applauding)
Yeah, that's a great question.
The empowerment idea is a technical idea
in reinforcement learning
that I think is very,
very interesting.
The idea is that you get
rewarded for doing something
that has an outcome.
So the idea is it doesn't really
matter what the outcome is,
or whether it's gonna be positive,
or even whether it's gonna be negative.
What you're getting rewarded for
is do something that has
an effect on the world.
And I think there's a lot of evidence,
we've shown this in things
that we've done for children,
and again, anyone who has
a two-year-old, like my,
I now by the way have five grandchildren,
including a somewhat
terrible two year old.
And anyone who has a
two-year-old will recognize
that just making things happen
is the thing that makes them
more excited than anything else.
And I think that's interesting
because from the perspective
of causal inference,
when we say you're
going out into the world
and discovering the causal structure,
what you're doing is discovering
the things that you can
intervene on to bring about outcomes.
So being very kinda
interested in doing that,
like our whole engineering program, right?
That's what an engineering
school is all about,
is about how can you make things happen.
And I think it's,
we just don't know exactly how
and when children are doing
that or whether some children
are doing it or others.
Here's an idea that I think
might also be relevant in AI,
we're just starting to test this,
which is that the reason
why children can do
that kind of empowered exploration
is because they know that
they have caregivers.
So again,
the postmenopausal grandmothers
are the key to all of this.
If you know that grandmom is there,
and you're gonna be taken care of,
and nothing really terrible
is gonna happen to you,
then you can go out and you can
try things even if you think
the outcomes might
actually be bad outcomes,
but you know that you are
in control and in charge.
So I think the human strategy
is we have these incredibly
exploratory children,
and then we have these
very nurturing adults,
and you need both sides
of that to solve problems.
- [Audience #1] Hi, such
an interesting talk.
I'm curious on the element
of how children's learn through play.
I am thinking of imaginative
player transformative play,
where they're constantly
reinventing the rules-
- [Alison] Right.
- [Audience #1] And how that
could be taught or related
to these large systems or language models.
- [Alison] Yeah, I think
that's a great example,
and we have a whole project that's called
a computational counter play.
So one of the things we're
looking at is this kind of play
where we're just kind
of mastering new ideas,
but we've looked at things
like the pretend play.
And pretend play, again, it has these
very interesting connections
to causal inference,
because what we've discovered
is that when children are pretending,
what they're doing is
generating counterfactual.
And generating counterfactuals
is one of the things
that you can do if you
have a causal theory,
it's one of the things that's much harder
for something like an LLM to do.
So generating the counterfactual
and saying something new
that isn't actually something
that you've already observed
in the world before.
If the world were different,
what would happen, right?
And that's something
that's very hard to do
if all you've got is data
from what has actually
already happened in the world.
It's something that you do all the time
when you're doing pretend play.
And it's a really interesting
question about, well when you,
we've studied this,
when you look at children's pretend play,
it's crazy, right?
It's not true, but it kind of makes sense.
It's not random, it's not
just that they say anything,
they say things that wouldn't make sense
if you were in that other
alternative environment.
And that's one of the cues
to human intelligence.
So an interesting question
you thought would be,
if you could get a system
that could generate counterfactuals,
that could pretend in a systematic way,
you might predict that,
as with the children,
that would enable you to
generate counterfactuals
in a real life situation, where you need
to use counterfactuals
to solve the problem.
Sir?
- [Audience #2] So this is fascinating,
as a parent of three sons I think a lot,
but they were two years
old, 20 years ago, so.
(audience laughing)
So, but I was curious is does he...
Right, wait for the grandchildren.
Is there an equivalence of neurodivergence
in this kind of thinking
of cognitive systems?
- [Alison] Right.
Well I'm not quite sure what
you mean by neurodivergence,
but one thing that I think
is really interesting,
and I had a slide and
there were too many slides,
but if you think about this
kind of trade off idea,
the kind of brain that you
need to be able to do this kind
of wide-ranging exploration
is quite different
from the kind of brain that
you need to be able to, say,
act effectively in the world.
And if you actually look at the
neuroscience of development,
what you see is that young
kids up to about five
have brains that are functioning
in very different ways from adults.
And you actually sort of see this curve,
where up to about five,
you get many new synapses
being formed, and then after five,
you start this pruning process where,
so the ones that are
there get to be stronger,
but the ones that aren't there disappear.
So you have this young
brain that's very flexible,
very plastic, very good at learning,
not very good at putting on its jacket
and getting to preschool every morning.
(audience laughing)
And then you have this
old brain that's very good
at organizing a, you
know, a symposium series,
not very good at learning a
new language, for example.
And we can see that
even in the (indistinct)
about the differences
between children and adults.
So you add things that
are, one of my slogans is
things that are bugs from
the exploit perspective
are features from the explorer
perspective, and vice versa.
So a lot of things that children
do that have traditionally
been seen as being signs
of lack of intelligence,
like they're kind of random, and noisy,
and all over the place, actually
are signs of intelligence
from the explorer perspective.
And a lot of things that
they're bad at doing,
like focusing on one thing at a time,
and doing long-term planning,
are things that are great
from the perspective
actually of the exploit
perspective, getting things done.
Not so good for these.
- [Audience #3] Thank you so much
for this very interesting presentation.
My questions is this, if you look at,
if we look at today's AI as a baby,
and compare that with a small
child, my question is this,
the computation power
between machine and human,
there's a big gap, and because the gap,
could artificial intelligence develop
and find a different way,
a path of developing
intelligence compared to human?
- [Alison] Yeah, I mean, I think, again,
what you mean by intelligence
is solving particular kinds of problems.
That's what you mean.
So the question is what
kind of problem is it
that you're trying to solve,
and what kinds of capacities do you need?
And of course if people have
pointed out if you wanna do
something like add very
large amounts of numbers,
artificial intelligence
already is, you know,
surpassing us by many, many,
many orders of magnitude.
There's no question your
definition of intelligence
as it might have been in the olden days,
was doing things like playing chess.
Then it's clear that these systems,
you could design systems
that are very good at,
that are extremely good at doing this.
If you're interested in some of the things
that are characteristically
things that children,
for instance, are solving, like being in
a very high-dimensional
space in the real world
and figuring out what the
structure of that space is,
in a very relatively short time
with relatively few data samples,
that's something that these systems
are not particularly good at.
And I don't think there's any
reason to believe that simply
adding more compute power
is going to be the thing
that will help enable them
to solve those problems.
In fact, I think there's
a reason to believe
that it's probably not going to happen.
What you need to do is
have systems that now have,
as it were, an objective function,
which is about going
out into the real world
and solving problems.
- [Audience #3] (indistinct).
- [Audience #4] You
said something early on
that really caught my attention,
that these generative models
are not hallucinating,
they just don't care,
- Right.
- [Audience #4] and that
kids care about truth
and how the world works.
So kind of my question is,
how do we make it care about truth?
Is this a collaborative
tool that's gonna eventually
be a two-way street rather
than us fine tuning?
Is the AI going to ask us,
"Do I have this right?"
How could I improve my, because
kids ask a lot of questions.
- [Alison] Yeah, they do.
In fact, there's a
beautiful study that I like
which showed that your
average four-year-old
asked 20 questions a minute.
And I put that in my book,
and there was a lot of mansplaining about,
"No, you must have got that wrong.
"That couldn't be true."
But that's actually what
the statistics look like.
So what you want, no,
I think the important
point is you could imagine,
and we might,
we're gonna try and design
systems that could do things like
ask questions or get data from the world.
LLMs just don't do that.
That's not what their training
is ever going to be designed to do, right?
I mean, the whole point about
LLMs is the things they do
is they predict what the
next token is gonna be,
and they respond to reinforcement learning
from human feedback.
They respond to you saying to them,
"Yeah, okay, that's good,"
or "That's not good."
In a sense that's like
answering a question.
What they don't do is say,
"Why is there a clock way
up on top of the campanile?"
Which is the kind of thing
that children will do,
let alone say, this is
a nice real example.
The little boy was walking and said,
"Why is the clock way up there?"
And then said,
"They must have put it up there
"so the children wouldn't break it."
(audience laughing)
So there's a beautiful example
of a really good question
and a really good
answer, but not an answer
that you would've ever gotten from either
looking at the statistics
of what's out there in text
or from asking someone to
give you human feedback
about whether that was a good answer.
In fact, it's funny, like what
would you do if a system said
"Oh, it must have done
it because of that,"
would you say, "No,
that's the wrong answer,"
or would you say "Yes,
that's the right answer?"
And it's exactly that space of the things
that are not the right
answer but are intelligent
that children are incredibly good at,
and I don't think, at least
as currently constituted,
things like large models
are gonna be able to do.
Yeah, no, I think that's exactly right.
So for problems like chess,
where there's a very
clearly-defined objective,
then you can use some of these
things like deep learning,
deep reinforcement
learning very effectively
to get to that objective.
An example that I like to give is,
what they would not be good at doing
was playing Addie chess.
So Addie chess is Atticus,
who's one of my grandchildren,
and his big brother of
course plays real chess.
But what Addie does is
take the chessboard,
and then he'll take the pieces
and he'll put them in the wastebasket,
and then he takes them
out of the wastebasket
and he puts 'em back,
and then he puts the black
pieces and the white pieces
in different kinds of orders,
and then he gets bored with
that, and then he stacks them.
And Addie is really playing chess, right?
Addie chess is really what
the human game is about.
Of course it also drives his
big brother completely insane.
And that's what you mean,
that's the kind of thing
that you would mean
if you were playing chess.
And that's the sort of thing
that is not gonna be something
that you'll have, even
though it's really amazing
that you can actually manage
to make the right moves
in chess as a result of something
like deep reinforcement learning.
I mean, I think it's kind of interesting
because you also almost wonder like,
what was the point of
chess in the first place?
Probably if you could
always get the right,
it wouldn't be a good game to play.
There's other kinds of
intrinsic motivations
in terms of being empowered,
getting to be better at solving the game
that it's kind of not
the same game anymore.
- [Audience #5] Really wonderful talk.
Especially, I think, grounding us
in some historical
perspectives is really useful,
in terms of Socrates,
in terms of postmenopausal grandmothers.
It's all very, very useful.
Particularly 'cause I think
a lot of the narratives
that we're telling are kind of monotonous,
unimaginative, and completely wrong.
So a much appreciated
perspective in the beginning.
My question is around the
relationship between the very
kind of active exploratory
hypothesis-generating behavior
that you've been describing
and some more kind of
passive learning systems.
There's years where kids
just kind of like, you know,
their physiological
systems kind of stabilize,
their acuity develops.
There are a lot of learning systems
that are not necessarily, question mark,
actively generating and
evaluating hypotheses.
But I'm curious how you might
think of the relationship
between these more,
what might be thought of
as passive learning systems
and these active learning systems.
- [Alison] Yeah, that's a great question,
and I think syntax is a
really lovely example of this,
because one of the things
that we learned in development
is that children, babies,
like seven-month-olds,
are very good at predicting
the next token in a sequence
from a series, even
when there's no content,
even when there's no semantic...
Seven-month-olds, right?
They're incredibly good
at just picking out
statistical patterns from data.
And there's a whole field
of developmental psychology
about this.
Even when they're not actively exploring,
even when they're not actively
trying to solve problems.
And for years, what
linguists were saying was,
"Yeah, I know, well seven-month-olds
"are really good at doing that.
"Who knows why seven-month-olds
are so good at doing that.
"It couldn't really have anything to do
"with their learning language."
And I think now what you'd say is,
"Uh-huh, yeah, you wanna know
why those seven-month-olds
"are so good at doing this?
"This is how they do syntax."
So I think it is definitely true that kids
are extracting enormous amounts
of statistical information
in this kind of passive way.
But then the interesting thing
is that they're putting
it to use, and again,
I think the example of thinking
of the child to scientist
is a very helpful one.
We use statistics and we
couldn't do a lot of our science
unless we had the capacity
to take a whole bunch of data
and pick out what the
statistical generalizations
in the data is.
That's an incredibly useful thing to do,
but it's just the first
step in enabling us.
And sometimes it's not
just the first step.
Sometimes you might, just as in, you know,
I think a lot of medicine,
for example, works that way,
where they have no idea
about causal mechanisms.
You just say, "Okay, this is more likely
"to produce a good result
than something else."
But most of the time you want to extract
underlying causal mechanisms from it.
But I do think both those,
I think it's fascinating and interesting
that both those things
are present in kids.
The important point is
it's not just that the kids
have the passive capacities
they're interacting with.
(audience applauding)

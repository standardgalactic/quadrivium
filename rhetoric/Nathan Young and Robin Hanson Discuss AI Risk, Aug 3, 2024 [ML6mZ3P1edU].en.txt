hello Nathan nice to see you again hey
Robin how are you doing yeah it's great
to see you uh Rober and I last saw each
other uh playing uh Imperial didn't we
at manifest yeah in bery great yeah so I
I did a sequence of these AI risk
conversations a year or two ago and
haven't done very many sents although I
do them you know once in a while but as
you said you wanted to talk to me about
about AR risk and I said hey why not
let's record it so yeah I guess you have
some particular questions in mind and I
have my usual sort of main question I
ask people so uh who who should go
first um I think we should do a couple
of questions and then I predict that
you're G to answer a number them quite
quickly and if not then we can do a
couple of them and then we can switch
over okay go ahead um so this is some
work I'm doing with with Katy Grace who
we both know uh and so this is mainly
her work uh but she's kind of tried to
come up with a number of the different
AI risk arguments that people find
compelling um and uh I could list them
all but uh I'm list them all we don't
need to go into detail of them right so
you might as well go one at a time and
we'll take them one at a time okay so
some people think that there is a risk
from competent malign agents so you've
got you know this is always my
understanding of what of what people
would do uh so you know this may not be
exactly her the way she'd frame it but
you've got some agents they're competent
they don't want necessarily want the
things that we want and so you lose the
future this is like the first story okay
well that's related to the sort of
question I usually ask people which is
in terms of what did you think was going
to happen without
AI that is um yeah so which is which is
like so
um I I am
a what's what's useful say here um so
another one of the thing the another one
of the arguments which uh which we get
maybe get to a bit is is this this line
um humans aren't aligned to one another
and this isn't really an AI risk
argument so much as just saying that if
you take anybody's uh preferences and
you whack them up to the Max and might
lead you to a world which is quite scary
or concerning to other people
um and right I think this is maybe an
argument that you're gonna find quite
compelling maybe sure so I I guess I
mean first of all I'd say we should
think about whether we're talking short
or long-term AI right and different
risks different issues show up in those
two terms and uh if we're talking longer
term AI then I might compare AI to other
sorts of descendants you might have
and say well you know in general like
how aligned are you with your ancestors
I mean how much would they approve of
what you're doing and how much did you
expect to approve of what your
descendants are doing and then we might
expect that with longer lifespans and
faster change that sort those sort of
conflicts will become more Salient or
Vivid because you will now see more
future Generations or more change in a
lifetime uh so that we could think this
ancient problem is going to be
exasperated by longer lifetimes and
faster rates of change but fundamentally
there's just this problem that across
Generations people haven't been aligned
in the sense of having very much control
over their descendants and their
descendants often changing their values
in conflict with their ancestors values
yeah I think I find this like I think I
find this relatively
compelling and so then the question is
just is AI different from that in sort
of the degree of deviation or the rate
of change perhaps uh but otherwise it's
the same sort of issue yeah I mean I
think a different yeah so like a
different AI risk discussion is like the
vulnerable World hypothesis right like
as you know if everybody dies and that's
right pretty bad under well you know
probably some of my probably some of my
ancestors were religious enough not even
necessarily that far back uh thinking of
my you know you know I think of my
grandfather was pretty firing Brimstone
kind of a guy right um if everyone had
died he probably would have ined that as
God's judgment and certainly if he were
alive today he would be I think he would
be a sort of pretty
strong Republican he would he would feel
he would be pretty horrified by the
state of the world I think and need my
own choices within it and sure like and
so that's all with respect to the these
longer term changes and then in the
shorter term you might make a comparison
now with say governments or corporations
and ask well how aligned do you think
they are with other people and how big
of a problem is that uh just to get some
reference comparisons here of in order
to set what the standard is we're going
to hold AI
to yeah I
don't I don't know whether find this
like super compelling though because
well like I find it moderately
compelling but I think to think about it
more I
think I think a thing I find is
like most people two
five 10,000 years ago seems to
me
like if they could have like had a bit
more food or like had a bit of an easier
life or
or
uh uh I'm not this is going to be harder
than I think to give these things like
they probably would have taken those
things you know I could easily I could
go on with this and be like if they
could they would have wanted to spend
more time with their children but
actually that's not super clear right
like I
think I think probably most the history
were like pretty fond of their children
but I think probably in like the Roman
times it seems people had like a
different relation to their children and
um I just don't really know enough some
like probably you know in like but like
the central point is that there seem to
be like some things like which quite
important chunk of human things that
like have been pretty stable over time
that like you know sure but if it's with
respect to alignment then I'm just
trying to have some standards of a
comparison like I'm trying to say let's
think about AI relative to what we would
have expected without Ai and in the long
run that's our descendants and in the
short run that's say corporations or or
governments th that's our relative
comparison for alignment today and so
then presumably we're kind of okay with
what we have today and so then the fear
is that it will be much worse with AI
than with these relevant comparisons
yeah that seems yeah yeah okay so now
with those relevant comparisons in mind
then you know ask your
questions um Okay so
yeah I guess like the thing I'm
interested in is trying to get a sense
of these different like Risk things like
which ones people actually find
compelling because there are like a
number of different stories that people
tell and the stories are meaningfully
different and often the AI risk
discourse can be a bit like AI good AI
bad and so right so you asked about Mal
competent malign agents right was your
first question yeah if I say sometimes
there are competent malign corporations
and governments and nonprofits out there
and they're in our world today and I do
think we want to know about them and are
somewhat concerned but we have ways we
typically deal with them and we mostly
think that goes
okay so it
seems okay so it seems to me plausible
at least
that
uh you might end up with agents that
could
cooperate without the need
for uh for like most of us to be
involved in a way that like currently
government doesn't permit so like you're
you're right you have but like the most
powerful Corporation really isn't that
powerful at all right the government can
go into any Corporation it can shut it
down it can take its technology it can
stop it from hiring people can do that
it can do that in any one country but
there are international companies that
can't be taken down by any one country
us uh
uh I I think I
think but we still worried about the
governments themselves I mean fine if
you think the companies are disciplined
by the governments the governments are
less disciplined by other governments
right uh so so a malign Government Can
can do a lot of harm to its uh within
its scope of its territory say North
Korea at the moment yeah yeah like by
accident or
by like it turns out that like
Democratic systems are are doing pretty
well and the systems sort of focused on
like the well-being of their people do
seem to which I I don't it's a little
bit surprising to me at times right like
it's not obvious to me that this would
be the way it would be right so but I
think a way I would summarize this is
that we mostly don't rely on individuals
to compete or counter these
organizations we rely on other
organizations at their same scale and
type to be the main things that keep
them in check so corporations compete
against corporations governments compete
against governments we individuals use
the competition among them mo that's our
main way that we reduce our worry about
them and so then for AI even if a an AI
you might postulate is more powerful
than any current government or
Corporation you might say well there'll
be other AIS at the time and so long as
there are many AI is competing and some
of them
plausibly are aligned with us then we
don't have to worry so much about the
others because competition could keep
them in
line sure um so yeah I'm mainly
interested now in just like jumping onto
like another argument uh okay just
because I don't know that's what I'm
interested in sure so the next argument
that people talk about is is is I think
quite similar it's called the second
species argument I'm sure you're
familiar with it you know it's just like
very simply it's sort of
saying Apes should have been pretty
concerned about inventing us you know
you should be concerned about making
something which is like a new species
which is very intelligent and so this is
a reason we should be concerned about
like making AI not necessarily like it's
going to go badly or like just just kind
of being like whoa you don't want to
like create a new group of things which
have their own constituency which are
like more competent than you are uh I
Katy tells me that there's like some
notion about goals that's the difference
here I don't really understand always
understand like I just like this is like
a new story we're going to forget the
old story this is the new story do you
find this story
capellic so I have an essay that you is
designed specifically to address this
question uh at aquet um few a year or so
ago um but the way way that titled was
summarizing um that's summarized as AIS
will be our mind children I think
yeah that's it AI will be our mind
children uh
so I think the way I would summarize it
as uh Evolution both you know DNA and
culture we predict should produce
suspicion and hostility toward
coexisting competing
entities that basically have different
genes that uh you know would that
Evolution would make you wary of such
competition
and in fact we are wey of such
competition other kinds of animals or
other groups of humans that we often see
ourselves in competition
with and we can you know not have to go
to Total War with them all the time
because maybe we can monitor them and
have Cooperative relationships but we do
in
general and it makes sense to have
suspicion and weariness of coexisting
creatures who could compete with us
maybe might plausibly win and then their
genes would displace ours and that's
just I mean many people have you know
preached against this Instinct and said
we should restrain it because this is a
bad Instinct and we should uh you know
try to see things better than that and
find trusting relationships with them
and Cooperative relationships so we
don't have to just go to war on the
basis of this
instinctive drive but it's plausibly
there uh that's one drive we should
should expect but there's another drive
we should also expect that a similar
power and similar
intensity From Evolution both of DNA and
of culture which is an Indulgence toward
descendants even when they are
substantially different than you that
that's exactly why people aren't that
bothered by their descendants uh
becoming not so align with them we just
have this instinctual Indulgence of our
descendants who are even if they're
substantially different so coexisting
creatures even when they're pretty close
to we're very suspicious of that small
difference uh you know you can even even
your cousins or something but uh
somebody who looks like you who's got
come from a similar school to you who's
working in your company who's competing
for the same kind of promotions you
might see very differently than even
somebody who's more more difficult
different from you who is e your child
or maybe maybe your student you
bring then kind of out competes you you
might
still right yes so cultural descendant
or your DNA descendant either way you
could be more indulgent and so then the
key question about AI then is which
framing here is the most relevant of AI
your your counterargument to this sort
of thing you to some extent you agree
you say yeah seems like we we we should
be suspicious towards something which
appears which might compete with us then
your counterargument to this is like
equally a different framing could be
that AI aren't second species they're
like are children and therefore we
should be sort of unusually well not
just there are these two framings but we
should have a way to tell which one
applies very simple way to tell the
first instinct comes when they actually
exist next to
you and they're actually there and
they're actually have different genes
and they're actually competing with you
and the others should apply when they
don't exist yet and they would in fact
arise from you and you will be directly
causely relevant in creating them and
they will inherit many things from you
many of their features will be your
features uh because you will have given
rise to them and I would say objectively
that second framing is in fact more
relevant for AI now because they don't
exist and the ones that are in the
process of coming to exist are being
created by us in our
image and they are in fact inheriting
many features from
us so later on when they exist the other
humans at the time they could feel this
hostility toward that's that's us
anticipating our children having sibling
rivalry and as parents what do we want
to do about our children's potential
sibling rivalry well typically parents
want to cut that down a
bit parents are typically trying to
restrain and reduce but not entirely
eliminate sibling rivalry among their
descendants
but I mean imagine that you knew you
would have two children one of whom was
going to be you know the older shall
hate the younger uh one of one of whom
would be would be powerful and would
would you
know you you you intended to give the
eal inheritance but you knew that via
whatever means one of them was basically
going to basically disinherit the other
child how would you feel about that well
you
might want that I mean you might not
want that to
happen uh now in fact when you look at
parents say giving money to their
children on average they actually give
more money to their successful children
by say paying for their
college sure so parents do now in fact
prefer not just to give time and effort
toward their more successful children
but even money toward their more
successful
children but you wouldn't necessarily
want your more successful children to
sort of kill and eat your less
successful child you me most parents
wouldn't want that to Happ
either yeah so you would want some
degree of restraint between these
children
um and of course you might want to teach
your children to respect their siblings
out of respect for
you that's something we do as parents
and grandparents and ancestors we try to
teach our children to to feel connected
to aligned with their siblings and their
relatives
and that's certainly something that
inheritance would suggest we might have
the same sort of habit toward our AI
children as well that we would want to
as possible teach them
to have some sympathy and compassion and
sense of connection to their concurrent
siblings okay let's move on to the next
one uh there's about eight of these I
think uh again this is mainly Kia's
framing like mainly her work I work with
her on it I'm trying to yeah I don't
want to claim this is all yeah it's not
um so the next one is like loss of
control by inferiority um so this is
like uh like a child Prince you know you
have a child Prince he's going to
inherit the Kingdom but he has to get
some advisor on board you know maybe he
gets like you know his like his
stepmother and like the like counselor
you know different advisors and he's got
to go and talk to them and he's got to
say I want you to to run my country
until I get into Power how does he cuz
he's a child he's like quite young he
doesn't understand whether the things
they're giving him really true or not
how does he ensure that when he comes
into majority he still has a country to
run and that he hasn't either been
killed or that it hasn't sort of like
most of the value of his Nation hasn't
been like slunk off into another one is
this is this a story you find
compelling well the major problem there
is you've created this very unusual
scenario that's not a common scenario in
our world so our institutions aren't set
up for
it that is today as Ordinary People
because there are lots of ordinary
people who've been around for a long
time dealing with corporations and
governments uh we have developed social
institutions and and equilibrium
practices by which we keep them
competing with each other to
serveice uh instead of you know killing
us and eating us or
whatever uh because you know that's just
evolved over a long time now this one is
showing up and asking for a new
institution yeah in a world that there
this problem doesn't appear much so
there isn't they can't just go to a
standard consultant who sets this thing
up all the time
right your response is again quite
similar to the first one maybe list to
the second one your response here is
something about like multi-agent
Dynamics about you can get you can get
the advisers to argue against each other
and you know that our Prince doesn't
exist in a world where he's work up one
day his dad's dead he doesn't haveone to
trust he exists in a world where there's
many princes in Many Nations who have to
deal with these kinds of coms and he can
call up his cousin out of for go out of
for hey man like what did you do when
when this happen when your mom died and
you inherited the whole Kingdom like who
did you trust like right okay right so
in it clearly the problem gets harder
with this unique Prince where there's
nobody who's ever who's been in his
roles at all similar in a long time he
he you know
he he can't go look up data sets of
people who've been in a similar
situation right he he's thrown into this
thing a unique thing that's just going
to be harder to deal with and so then so
so you maybe find this more compelling
if there's like more of a discontinuity
like if right so so that's a big
question about ask about AI how
gradually or you know smoothly will the
transition happen so that we can you
know adapt incrementally to changes as
they show up as opposed to very lumpy
sudden changes that's a huge issue with
respect to AI risk I mean the most
compelling AI risk scenarios have been
the hugely lumpy scenarios I I
compelling in the sense of you know you
wanting to be very worried about it and
wanting to be willing to go to very
extreme
responses and you know are the ones
where there's there there's a real like
Rapid or discon discontinuous change
exactly so for example my my ex
co-blogger owski that was the scenario
he and I debated long time ago called f
where he cranked up the dial to the max
on the suddenness and size of this
transition and you know I I heard I
haven't read lots of that stuff um
there's lots of stuff to read um but do
you do you find that your arguments
apply less there like if you have a sort
of Singleton AI that runs everything
it's not necessarily clear whether it's
conscious it's just sort of like it's
doing stuff is that a scenario that that
the robin Hansen is more worried
about what any scenario that's less
familiar that is we have less experience
with it is for that reason something to
be more worried about in the sense of
conditional on it happening you'd feel
less sure about how it was going to play
out on the other hand the more weird and
unusual scenario is the less likely you
believe it's going to actually happen so
then in that sense the less you might
worry about it so it depends on whether
we're talking about the chance of it
happening or what to do conditional on
it
happening sure but for some of the other
scenarios you've kind of been
like I think you have a sort of a broad
tent view on Consciousness or something
you're like I think you you bite the you
in your writing it seems to me you you
bite the repugnant conclusion you sort
of say yeah seems fine like lots of
things being alive that's okay
I mean I was reading a guardian review
of M age of M this morning uh
and you know they judge you kind of
quite negatively for describing this
world but I I just think my
understanding of is that you wrote the
book you tried to sort of take the
assumptions that seem to make sense to
you you tried to describe a world not
that you liked or that you disliked but
just be like what's likely seems likely
to happen um and this led to a world
that sort of many people find a bit
repulsive I think probably you've heard
that Express right yeah but that you
find sort of like okay you imagine that
if you lived in that world you'd be like
yeah things are and in fact if anything
maybe maybe you think if you lived in
that world if you one of the M's and you
were given the chance to exist in your
life now you might prefer being an M
maybe other than more causal ability to
affect the future or something I I agree
that is I'm less put off by the
malthusian scenario than many people
today I mean it was the by far the usual
scenario in human history and in animal
history so it's and it's it gives me
more reassurance that it will work and
continue to exist as a coherent scenario
into the longer future whereas I'm much
more worried about our world and whether
it can continue to function given that
we are so far away from malthusian
constraints sure so so so so my question
here
is the Singleton scenario
or you have like one thing sort of
controls
everything seems quite unlike either of
those
scenarios do you have any sort of like
veilance towards it in terms of like
desire or not desire like I hear about
that scenario where I'm like there's one
thing it's dubiously conscious it like
seeks to have control over like all the
energy everywhere I'm not even saying I
think this is a likely scenario I agree
with you I think PR like actually but
like this scenario doesn't sound good to
me like actually like a scenar where
there's like many like little AIS like
doing little trades with each other and
there's no humans at
all like I'm not wild about there being
no humans but I'm at least like oh
there's like lots of things they're
doing stuff and like maybe they're
conscious and like maybe like that seems
like a world in which there might be
like joy and Beauty even if it's alien
to me whereas a world where there's like
one thing that seems like a world where
like it's more like to be
Barron so the first thing to say is just
this Singleton scenario is harder to
reason about
because we just don't have so many
analoges to it in the past so that makes
you worried just at that level of not
being able to bound it very well yeah
and secondly when you start to think
about the analoges in our world that
have gotten closest and the causal paths
that lead to them in our history those
tend to be strong central
governments yeah and then your attitude
toward strong central governments will
influence your attitude toward the
Singleton scenario and I'm more of a
Libertarian who's always been somewhat
wary of strong central governments so
then I get worried more about that
scenario but I mean the first point is
just it's just hard to say much about it
because it's so foreign so an enormous
amount depends on the causal path that
leads to this Central Power and then of
course additionally on whatever what
does it need to do to stay in power so I
mean a standard reason why strong
central governments have been a problem
is that they are only barely staying in
power and they have to do a lot of
things to ensure that they stay in power
many of which
are politically repressive of the what
they see as the threats to their power
so um a Singleton that was just barely
able to become a Singleton fighting
fiercely against the threats to that uh
that it's easier to see that as a
negative scenario because you could
imagine what it would be doing to
overcome
uh obstacles to becoming that Central
Power isn't that like a baroness like
well I guess maybe you're right maybe
just like I don't know okay let's move
let's move on to the next question um
the next one is a loss of control via
speed so this is like you have a car
your car drives well but without you
knowing I put in a thing which makes
your car 10 times faster right and you
get in one morning and then like woof
you go off driveway and you crash and
you die and like everything about the
car was fine theoretically except like
now you just happened way too fast you
know maybe I also made the like steering
much more sensitive you just couldn't
handle you that so so you know this
scenario you have to pair with a sudden
transition and even ignorance of the
transition ahead of time in order to
make the worst case scenarios here right
if I know it's a 10 times faster car
yeah and I can and I get to take it to a
big field to to practice driving it
before I take it on the roads I'm much
less worried about having this higher
performance car especially if I if I got
a on10th dial on the engine where I can
take it the usual strength and then turn
it up a little as I try it out I'm I'm
much less worried about that as a person
controlling a car right so so then it
would be about this sudden unexpected
speed up where I maybe even had a
delayed knowledge of the speed up
uh which would be the most worrisome but
I think the most important context to
realize is just that Humanity has really
never had much control over these
things so uh a a very Stark and uh fact
about the world is most of change has
been driven by technology that we don't
vote on we don't you know decide whether
it's going to happen collectively just
individuals loc Al adopt a technology if
they find it in their local interest and
then that's how technolog has been
changing for a very long time you know a
long time ago it was very slow and then
it's gotten faster but there was never a
point at which we were consulted about
the
speed and you know we might think it's
remarkable that we've been able to keep
up with the speed as it's been going so
roughly the world economy doubles every
15 or 20 years over the last century or
so and if you think about it that's a
really very fast rate of
change and I think many people if they
could vote on it they would vote to slow
that down I think you'd probably get a
majority of the world to vote for
slowing it down but you know the ability
to actually do that is seems well beyond
us and if it goes up even faster that
just continues the same Trend we've had
for a long time of it happens at the
rate it can happen without the rest of
us having much
say yeah there's a kind of evolutionary
thing there right like the where the
bits that grow
fast outgrow the bits that grow slow it
doesn't really matter right certainly if
you had asked the rest of the world do
they want Europe to suddenly start
growing much faster than the rest of the
world in 1700 uh they would have said
no uh yeah seems Seems
likely uh so that then we have the
question is sh should we somehow create
Collective institutions to regulate the
rate of change and then we get back into
the issues with the previous question
about the risks and costs of having
centralized regulation of these of this
sort and once you start to worry about
how badly it could go to empower Central
Regulators to limit such things you
might prefer to just let it go at its
more natural
speed but it's a hard Choice neither one
is you know
ideal so we've now we've been now at
halfway through the arguments uh I
think the other ones are sort of also in
some way right right we'll do yeah okay
we we briefly discussed the humans aren
lied to each other one and that you
know and they aren't to some substantial
degrees clearly but right the world is
okay with you know we live in an okay
world where we have conflicts with each
other um so you you alluded to at one
point the scenario that maybe AIS would
be better able to coordinate than we are
so I didn't you didn't sort of separate
that as one particular a separate point
to talk about but that is a relevant
consideration uh because that's a way to
break the Symmetry between you know us
and corporations and governments and AIS
if we say that right yeah if a if a
malicious Club decides to you know want
to hurt people in the world it has
trouble coordinating and it has trouble
coordinating with the other malicious
clubs in the world and that's part of
what makes it us able to manage such
malicious clubs at the moment is their
limited ability to
coordinate uh you know more we also have
a limited ability to coordinate but to
the extent that they're a
minority then our limited ability to
coordinate is still enough to manage
their limited ability to
coordinate now if AIS are postulated to
have a much stronger ability to
coordinate
then that's more of an issue of the AI
versus everyone else Coalition
scenario then then the the robin Hansson
in my brain says something like but
coordination is quite like a good trait
and like maybe if they're better at
coordinating like maybe they deserve
like or like like who are we to say that
they shouldn't exist and coordinate
better each other in the longer term you
might think well if some of your
descendants are going to be better at
coordinating maybe you do want them to
succeed more against your other
descendants who are bless welled that's
in the long run of course yeah but if
you are right now facing you know a coup
so at the moment you coordinated coup
right at the moment one of the problems
we have is potential coups and one of
the ways we limit coups is because coup
members have trouble
coordinating yeah that's a reason why
you know you need a pretty large coup or
pretty centrally located coup in order
to have a chance of success at a coup
um but if you postulate a particular C
cou uh Coalition who is unusually able
to coordinate then you will estimate
their ability to manage a coup as higher
and you'll want to be more worried about
them
earlier
like I think a key notion within AI risk
is that like you're talking about the
chances of a scenario that you don't
want happening and I guess uh
as someone who's talked about this for a
long time as somebody who I think I
consider to
be like to give me thoughts that I'm
glad I had on these
topics but any of like does any part of
the scenarios discussed so far seem like
a risk to you like when I hear you
discuss these topics I don't hear so
like question like any of these like
there's like you're like there's this
bit that I'd like to avoid
yeah um
so the most extreme scenarios are the
most worrisome which are the most lump
lumpy scenarios where very suddenly
something improves enormously in ability
and perhaps an ability to coordinate and
also enormously changes its
values that would be the most worrisome
scenario uh because we have like yeah
you yeah okay great so we'd have the
least ability to sort of manage it by
adapting to incremental changes it uh
you know we have the least ability to
predict that it would go well Etc
so even though I don't think it's as
likely as other people think I think
it's okay to think about how to prepare
for it or or to prevent it and so I have
gone on record in a post called fum
liability to say well let's set up a
liability regime which is targeted for
those particular scenarios and plausibly
wouldn't actually have much of a
retarding effect on other scenarios and
there therefore I don't have to be
confident that it won't be a problem I
want to be robust I want to say well if
there is a potential problem here what's
the minimal solution we could come up
with that you know doesn't disturb other
thing good things but we'll deal with
that one and that would be my proposal
so the concept is to make a checklist of
features of f like scenarios of the
problematic F like scenarios and then
whenever anybody hurts anybody else
through an AI you add on an extra
liability factor for the closer you get
to these bad scenarios and that would
just push people away from those bad
scenarios through a local incentive way
and that's you know my accommodation of
saying that's not zero probability and
yes well I don't want to just ignore it
and you know wish you know luck cross
our fingers and hope nothing goes wrong
I'm happy to look for minimal uh ways to
address even unlikely
scenarios
sure okay I
think I think that that then is so far
the closest thing I've heard to
like sort of like an endorsement of one
of those even I think an endorsement can
be a very low probability endorsement
right like it seems to me the others you
sort of look at and you think like I
don't really understand from it a sort
of low resolution of what you've said is
something like the others You' kind of
been like H like I don't don't really
understand I don't really buy that
argument as a whole set of arguments but
the argument around like speed or
something you think yeah that's
plausible it's it's very unlikely but
like here are the ways I'd mitigate
against
that well it's also again what I was
trying to do earlier was separate other
issues from AI in particular so I didn't
mean to dismiss the other issues but I
just meant to sort of say we should just
be thinking about those issues at the
general level of those issues not
necessarily thinking about them
particularly in the context of AI so for
example exle there are potential
conflicts among
Generations sure right and I think it's
fine to think about what to do about
conflicts among generations and to ask
how far you want to go to restrict or
Channel your descendants and directions
you might approve but that's not so much
an AI issue that's a long-term generic
issue about the relationship between
Generations yeah and and and and maybe
in in an attempt to understand you and
your
work it seems now that I consider it
that quite a lot of your work is
relevant to humans having different
preferences right and different
abilities to predict the future much of
prediction
markets uh sure is a system by which
people who have different expectations
for the future
can at cost to themselves make
predictions and then you allocate
resources to people who are better at
predicting things right
um right and then we might want to think
how to use such powers to deal with very
fundamental issues regarding the future
sure
uh this is really uh self-indulgent but
I want to self- indulge um do you have
any kind of comments
on it seems to me that forecasting
played a you know a small but moderate
part in
the in Joy Biden being losing the
nominee stopping being the nominee of
the Democratic Party seems like pretty
plausible to me that I don't know in 0.1
to 5% maybe higher you know let's say
0.1 to 5% of worlds in which you don't
have Nate silver you don't have poly
Market you know I guess unclear whether
you're going to remove bet for as well
because that's been around for a long
time but then you you know you don't
have people so then this doesn't
happen
uh do you have any comment on that I I
agree with you that it's plausible I
wish we had stronger data I mean
basically key decisions were made Behind
Closed Doors by insiders who are not
really going to testify on that or
aren't going to publish a transcript
about it but the question is what's the
chance that some of them at least had
seen betting Market odds about the
chance that say Biden would win uh if he
stayed elected and uh what's you know
and what's the chance that they wouldn't
have already known that through other
channels so I do think in fact these
Insider groups are often pretty ignorant
about a lot of things and yes having
somebody on the periphery of their
Network who
had seen such prices and could report
them would have moved some people to
believe more that yeah we can't Bluster
through this looks like enough other
people out there get the problem that we
should we should face
it yeah and like
uh I I
guess I I know I I I I'm pretty
confident this is is n really your ideal
version of your sort of like fire the
CEO markets but it seems to
me by virtue of there being markets on
who would be the president and who would
be the Democratic nominee you
could make some implications about the
electability of various candidates and
we did those numbers were available and
people highlighted them and talked about
them yeah and like
that seems to me a I think it's like a
very underrated success of a thing
you've been chapping think for a long
time I'm I'm happy to accept that
judgment uh again I try to be restrained
in my brag bragging about things in the
sense that I'm I need to maintain a
long-term reputation here and sort of
wait for data to collect off more
clearly over the long run but it's it's
uh sure it's plausible certainly
plausible right okay that that was that
was my comment um
okay so so so as you probably know there
have been times when say the betting
markets only gave Trump a one sixth
chance of winning the election and then
he wins and then people say ah see shows
you betting markets are
wrong and I gotta say look you have to
average over a lot of cases here you
can't just look on one case so on a
positive side I also have to do the same
thing right yeah that seems right I'm
gonna I I'm happy to say yeah that seems
plausible but in order to make a
stronger claim I would like to average
over more cases to look at more
systematic data in order to make claim
stronger
claims
the to the extent that we're going to
have these markets lying
around it
seems it seems like a much
[Music]
more sustainable thing
longterm sorry it seems like a much more
feasible thing to do this like I think
you could imagine there being 20 times
many markets about elections people love
betting on elections they could you know
you know we could have you know will
will Pennsylvania be one if Shapiro is
the vice president you know you have
like many many markets like that
and it feels like there's like that's
much more tractable than fire the CEO
markets it's just all depends on who
makes what market so in some sense this
game is wide open any substantial player
who wants to take the initiative to make
some of these markets can uh and so it's
a matter of who takes what initiative
when so I don't want to discourage
anyone from any of these options I want
to say you could just make it happen if
you want
to
sure okay um I do you have any other
comments you'd like to make well I mean
if we bring it back to say to AI we
could say you know it would just be
great if we could have ai Mark betting
markets on AI consequences like we have
these bills before you know various
legislates uh for AI regulation it would
be great to have prediction markets
about say the amount of AI activity
conditional on these bills passing the
you know degree of concentration in
these industries conditional on them
passing uh you know other sorts of
markers before you know everybody dies
sort of markers there are a lot of pre
markers that would plausibly indicate
more risk of various scenarios would be
great to have markets on
those yeah I agree about that so I'm GNA
write that down and that's again
something anybody out there could do
I've also said you know for a long time
when people thought about AI the biggest
problem they thought about was AI is
displacing humans off their jobs that's
still a concern and it's still something
we could do something about if only
people would get the ball rolling to
just set up robots took my job
Insurance I've written about that and it
wouldn't be that hard to do uh and it's
just a matter of momentum to uh get some
people doing it and getting other people
doing and make it more of a thing thing
um so and that would certainly highlight
other AI risk
issues to the extent that people were
actually buying that insurance you can
see the price the premiums for that
insurance it would give you a
probability of those events happening
and the resolution is what AI
penetration in your industry like well I
I would start out with just very simple
you know labor force participation rate
in the US say for us workers at the
moment it's like 60% if it drops below
30% in a 10-year period that's probably
indicate that's the most likely way that
would happen is lots of automation so I
would just have a bets on that let
people buy bets on that happening within
certain decades say and have assets that
pay off if it happens within a decade
and then they would be somewhat insured
against that going wrong and again so
that doesn't require individual
underwriting that is you don't want to
do the whole Insurance route where they
have to review your life history and
decide
R just set a market in the rate at
people lose
jobs
sure okay um I think yeah
um let's let's let's let's run through
some of the others and then
okay um uh yeah probably got 15 minutes
so I will do that all right um so
there's there's there's like vulnerable
World hypothesis type stuff right like
AI allows you to build a nuclear bomb
much quicker than anyone else can um so
the vulnerable World hypothesis is the
idea that as technology improves
generically individuals can have more
leverage over the entire civilization
and put the whole civilization more at
risk so it's a
ratio question of the relative potency
of an individual a very small group uh
at causing substantial harm to the
entire
civilization that's the postulate of the
vulnerable worlds hypothesis that is
that that that parameter has been
increasing sure I'm just you find I'm
not convinced that's true I don't right
I'm not so for example if you look at
mass
murders yeah you could look at how much
any one person with as many guns as they
can hold in their bag
has been able to kill when they decide
to go killing a lot of people that
number hasn't been going
up sure
though I guess the the AI risk argument
here is that
AI
makes like plausibly AI might bring down
the ceilings for some of those things
very quickly well the key point is that
every technical change changes both
offense and defense so so this has long
been a issue with Warfare people for a
very long time had looked at a
particular new weapon and said aha
that'll make offense more powerful than
defense in war when they look at a
particular new offensive weapon and
they've neglect to look at the new
defensive strategies that can counter
them and then over time we don't
necessarily
drones
are you
know drones are great they're cheap
they're going to be used as swarm
technology but then
also lasers are a cheap to fire and if
the thing like launching is is is is
sort of Packy In plastic then a laser
may be like unusually but then of course
you're going to have them covered in
laser shielding right right so I think
you just want to look at the long-term
trends in offense versus defense and
there I don't think you've seen that
much change in the long run
okay now if you if think you have you
looked at say computer security even
there um if you ask you know my offense
and defense and computer security over
time even there I'm not sure we've seen
really substantial
Trends in terms of actual you know
harm okay so next one is we kind of
touched on this quite a lot AI May
produce to accelerate bad multi-agent
Dynamics so this was I originally just
DMD you on Twitter about this and you
you you said there's a quote you said
you described it as not very eloquent
but I I appreciated it was brief which
uh is not the case of almost every every
other AI quot here the mo this is
talking about one of your AI models the
model seems to confirm the intuition
that machine intelligence has malthusian
implications population and wages um I
don't think you see that as a disastrous
scenario right I
don't but you do acknowledge that it
like it could push the multi-agent
dynamic the multi-agent equilibrium to a
direction that is more like that and I
guess you acknowledge that many people
do are concerned about
that right so there there's this generic
pattern here of noticing that you live
in a very complicated world that you
don't fully understand where lots of
little Parts seem to balance each other
and then you imagine a new thing coming
in and you can see that it's going to
have a lot of
effects and you can see plausibly that
some of those effects will maybe
undermine some of the things you were
relying on yeah and there may well of
course be other effects that counter
those but you can't see those very
well just in general whenever you've got
a structure that exists it's just easier
to see how changes could undermine the
existing structures than they could add
replacement
structures Sor I'm just moving my laptop
um so then then the question is and this
has happened over and over again in
technology all you know for for my
entire lifetime and for a long time
before that uh just when people can
Envision a particular new technology
it's just much easier to Envision Vision
the ways in which it will undermine
things they like and rely on than to
imagine the ways that will replace those
or add new things they will like but
haven't envisioned yet and that's why
most technology forecasting tends to be
a warnings tends to be worrisome you
know collections of but this could go
wrong if you think not just about AI
think about nanotechnology genetic
engineering television nuclear
power uh you know
just a wide range of Technologies
typically if they're big enough
technology it's just hard to really see
all the all their impacts but it's
relatively easy to see how they could
just undermine something you are
currently relying
on like with drones for examp E easier
to see how your current aircraft carrier
might be vulnerable to drones than to
see how you're going to use your drones
to do other stuff in
offense and and your response to this
again is sort of like
to the extent that risk vectors can be
predicted you could put small costs on
those risk
vectors like if they really cause real
world
harm and then more or less leave other
things how they
are where the more that we can expect
just to have these changes be gradual
and be able to adapt to them the less we
need to worry about them ahead of time
but the more lumpy the changes might be
uh you know and maybe the more out of
view they might be then the more you
should try to do other things to deal
with
them but try to Target your solutions to
near the actual problems rather than say
try to regulate all AI at the moment you
might you know have more food liability
where you look at the particular
scenarios that could go wrong and you
target your regulations next to
them okay so
then we we sort of the next the
arguments I consider to be sort of
scraping the bottom of the barrel but
they're
like I don't
know this is what I'm thinking about so
the next two arguments sort of linked in
that they're just they're both very
handwavy one is hand wavy in the
direction just like it's large risks I I
don't think you're necess going to say
new things in respons but maybe you will
one of them is just like it's large
risks these are large risks we should be
we should be concerned about just like
large changes there's just like an
argument from size just like this is the
big impact you got to be you know you
want to be concerned about but that's
exactly the point about gradual change
that is all small changes add up to
large changes in the long run sure so
here you right I think you're going to
respond to something like yeah this is
concerning if the changes is discon
discontinuous or lumpy but just the size
of the change just as it that like is it
of itself the question is how quickly
that and how gradually that change
happen right so for example you know
every day in the stock market the
typical stock price changes by roughly
one and a half percent yeah now one and
a half percent change to a stock to a
company is is tolerable they can
certainly handle that but you integrate
one and a half perent a day over 10
years 20 years and basically most of
these companies are going
bust yeah and so as a company you should
definitely be worried in the long run
these large changes to your company will
happen I.E most of most likely you will
just go bust and die you should worry
about that but it's the same process
just integrated over a longer time yeah
so if if you're worried about large
changes you should just be worried
generically about most everything they
can all integrate to large changes over
time yeah so what you more worried about
is how quickly they're happening
relative to your ability to to track and
adapt to them
yeah I guess a thing I think about a bit
in regard to this
is I would like more focus on individual
capabilities forecasting personally like
if we forecasted specific capabilities
and how concerned we were and our
ability to regulate on those then
ideally you would become pretty
confident that you don't that you could
you could regulate if you needed to on
many of them most dangerous capabilities
but they're they're like if you could
see them coming right if you get gradual
warning about them basically small
problems happen before big
ones yeah but you know that's how we've
regulated pretty much all Technologies
we've ever
had is by waiting for concrete problems
to appear and then adapting to those
concrete problems and that only goes
wrong for very
lumpy Technologies and changes
and it's just not clear that AI is that
lumpy that is comparing say I mean you
know comparing to lots of other
Technologies certainly in history ai's
changes have been relatively
gradual the history of AI changes so far
does not make me especially worried
about enormously lumpy
changes you think that's true feels to
me like the change
the I guess you could the Chang have
been sort of like kind of but like you
know we've really had like two or three
years moving
from what might be considered in the
last two or three years things that I
think would have been considered really
like mind-blowing and now sort of okay
so let me
qualify there's an economy out there
where people are doing jobs and tasks
and that's where most of the concern
should lie in terms of changes in that
economy including say the military when
researchers do demos off to the side of
that economy those demos can be more
dramatic and lumpy but that's less of a
problem if the trans the process that
translates those dramatic demos into
actual economic activity is more smooth
and gradual and you know less
lumpy and and and economic changes that
have been much lumpier than this what
would some examples of those be
economic changes so for example there
was a moment when we developed a new way
to make aluminum that was vastly cheaper
than the old ways to make aluminum and
all of a sudden people started making
lots more cheaper aluminum for example
right there there definitely have been
moments where particular Technologies
arose and then they made a big
difference right particular ancient
civilizations figured out how to make
chariots and then they started winning a
lot of wars with their neighbors with
their with their chariots right um and
so you know military technology can be
lumpier than other Technologies but even
you know in the last half century
military technology hasn't been that
lumpy actually in terms of its
developments but it's like in the last
few years people have been worried did
the Soviets figure out how to make
supersonic
missiles Hypersonic missiles right and
like there's some demos they look like
they was really you know potentially
powerful but so far it looks like that
has hasn't actually made that much
difference that is and there I me
military is constantly tracking these
things uh looking for technologies that
might suddenly make a big difference but
mostly you know the translation between
an abstract particular technology like a
material or something and actually
integrating with large systems just
tends to make it more gradual and less
lumpy and that's true for not only the
military but for the rest of the
economy so you can see that in the past
when AI technologies have appeared and
then as they've been tried to integrate
into the economy it's just been slow and
gradual and fragmented and we've been
able to track the gradual application of
lots of Technologies in the economy and
it hasn't been very
disruptive okay we're now on to the last
couple so one is black boxes so they're
just we don't know how they work like
many technologies have been better
understood than this in the past past
many Technologies you
know like I mean lots of Technologies
haven't been very well
understood can you give like it seems
like llms are like pretty poorly
understood like you hear that people
building them saying you know we don't
really understand how they work you just
sort of like put see what go most drugs
we don't know how they work
honestly okay most Medical Technology we
don't have much of a sense for how
exactly it works you do an experiment
and it seems to reduce the symptom or
whatever and and you start applying it
but you don't know how it works um and
of course until relatively recently most
materials people didn't know how they
worked they invented a new material and
it bended easier it was cheaper to make
or stretched or whatever it did and
people didn't know why they just knew it
worked and then you started applying
it certainly most Technologies for
motivating human workers and managing
their work lives we don't know why those
work exactly how they
work sure corporations have to make
decisions about how to divide up tasks
and how to organize groups together and
what incentives to offer them and and
what how to motivate them to work and
promote and we just have we're mostly
doing that on the Fly intuitively we
don't have good theory about it what SEC
Robin uh she's in the Next
Room okay
um okay yeah that's kind of a compelling
answer to that
question uh viewers of this podcast
might find that I'm Ill prepared or
something and I don't think be otherwise
um I think I have no complaints I have
no complaints at all this has been fine
we just spontaneously just to tell
everybody it was less than an hour
before this conversation where I said
hey you want to do a conversation and he
said okay and when we said okay let's do
it so neither of us prepared for this
neither of us needs to this this went
fine as far as I'm concerned I agree I
think yeah I think sometimes people
expect people to really know their stuff
and I'm like no I think there's too many
many think there's too many people
talking about stuff they don't really
know about I mean you're mostly asking
questions rather than making claims so
the requirements for asking a question
should be much
less um the last one I think the last
one is probably the one I actually find
the most compelling which is something
like if a mixture of experts and
forecasters say that there is like some
risk coming down the pike then that is a
good reason to think that there is risk
down the
pike it's a reason so the question is
how good and what exactly should you
conclude from it right uh so that look I
mean you have to go with the history of
how much have people ever been able to
tell what's coming down the pike right
just not very much so just in general
humans haven't been that good that is we
can actually distantly Envision rough
outlines of future technologies that
most people don't realize that's
actually true that is people envision
cell phones decades before there were
cell phones and you know I was part of a
group that envisioned the worldwide web
before there was a worldwide web and
lots of things in fact have been roughly
envisioned from a distance so I I do
think we should say that people are
roughly correct to Envision that in fact
they will be better than human level
intelligence in the future it will be
cheap it will be powerful uh it may
change quickly those are all rough
guesses that people are making that I
would agree with um and then their final
claim is just GE guys you should worry
about this and honestly well yeah in
some sense we should just be worried
about all future
changes all all big future changes that
we can Envision yeah there there's
there's certainly reasons to worry about
them the the question then becomes what
if what more specific claims do they
make about how we should regulate or
manage those concerns
now and then you'll say if maybe there's
a poll about that and then I might ask
okay how often have similar polls been
relevant you know reliable guides to how
we should regulate coming technology
changes and for that I don't think the
track record is that good but
nevertheless um you know naive observers
should still put substantial weight on
it certainly
um but you know I just think as an
economist most I'm just going to go with
the prior here
mostly regulation goes bad when it's not
tied very directly to specific problems
that you're addressing that you've seen
hypothetical regulation regulation of
possibilities that are not very well
understood or
conceived that just doesn't seem to go
very
well that's how I would frame the
argument uh but I would certainly say
it's worth tracking I'm glad there are
people tracking things going wrong with
AIS so that they can point out when
particular problems happen and call them
to our attention and then if we see a
pattern of such problems then we see
that maybe those that pattern is due to
some you know systematic malicious
behavior that could be addressed by
regulation then that would be the point
at which you would propose such
regulations and then I would I'm I'm
much less against that I would want to
look at the details but I'm saying look
if you see a specific thing going wrong
repeatedly and you have you know a thing
you think could do with that I want to
listen to that now in in the past that's
also gone wrong
you know so for example if we see some
examples of insider trading and then we
ban all insider trading I might think
well yeah you're you're cutting some
problems but you're you know you're
forgetting all the gains you're giving
up by such regulations and we want to
weigh those against each other but at
least there's a reasonable case if you
see a set of concrete things go wrong
you've got a case that deserves
attention when you say hey we should
regulate to deal with
it but when you're thinking decades in
advance of hypothetical problems of
systems you have really very little idea
even their architecture will be or
capabilities and you say we should you
know do stuff now to deal with those
potential problems I'm just much more
skeptical about that it's just not the
right time and the right way to do
it now if you could again if you could
convince me of the fum scenario you
could say that because that was owski
story he says look that's fine for other
Technologies but this one there's going
to be this sudden growth all of a sudden
there'll be no warning about it and
afterward it'll be too late and so the
the only way to prevent these problems
is beforehand now because for all we
know we are right almost there right now
we couldn't tell if we were
close that's the worst case scenario
where you might need to do
something yeah
so I guess sort of
summarize it seems to me that of these
arguments you find the loss of this kind
of this speed argument maybe
discontinuous speed to be the most sort
of compelling you don't think it's
likely but if I told
you you
know sort
of I don't even know how to ask this
question but there's like some sort of
like notion of like compellingness
without likelihood that I don't point if
we had seen a bunch of smaller sudden
unexpected explosions of the F sort and
then you said and that could be even
bigger then I might be much more
concerned right I can say look here are
some smaller explosions that are in a
maybe there's a power law distribution
of them and you say look there's a lot
of weight on these much larger
possibilities so hey you should worry
about that I mean for example that's why
I would worry a lot about pandemics we
have statistics on the size of pandemics
we do that we've got a power law that
puts a lot of weight on really big
pandemics and it says actually the
median death so far is much less than
the expected deaths because the
distribution that you fit is weighted so
much the expected value is so much
higher than the median you'd seen so you
guys say and that's same for Wars so you
got to worry a lot about really big Wars
because you look at the size the smaller
ones you fit a distribution to it and
you say look this distribution puts a
huge amount of huge ones that we
wouldn't have seen yet and yeah you got
to worry about those and so yes if for
AI we had these F like explosions where
suddenly ai's vastly improved in ability
in some self-improving way we saw you
know small ones but we fit a
distribution we go
look there's all these ones that we
wouldn't seen yet because this
distribution is really heavily weighted
to the large size and you go
okay and then I think you
also you also find sort of you don't
necessar find a compelling AI argument
but I think you find it like very
interesting the notion that humans
aren't well aligned to one another there
is some like large problem about
aggregating preferences over large
groups of people but I mean I'm an
economist that's just our bread and
butter that has I've long since
accommodated to the expectations that he
humans have troubles aligning to each
other that to me is just the state of
the world other people who didn't
realize that fact I guess they should be
surprised by it and worried but that's
just social science that's what we
economists have been doing for a long
time is asking how can we deal with
those conflicts between
people sure and
then yeah I mean I
guess
plausibly I'll try and talk to some
other point about maybe counterarguments
or I mean I think we've gone through a
number of counterarguments but there
might be some sort of global
counterarguments which I think it's also
Worth to try and get people's Sense on
um do to finish do you think there's any
other arguments that you find compelling
or sort of they're not they're not
really sort of mutually exclusive
arguments I think a lot of there there's
overlap here but any frames of arguments
that you think has been missed here
where you think actually there's I have
heard this argument that I find like
maybe similarly to the speed argument
you don't think it's likely but you find
if if there had been certain things and
it would be very compelling you know
like it is there any frames of arguments
that you think I mean I I think it would
be worth doing a lot of thinking about
future AI I'm just less interested in
the framing of risk in terms of should
we just shut it all down as the sort of
the main policy question I'm much more
interested in just trying to think
through how it'll play out and then
preparing ourselves for those different
ways it might play out so for example uh
I think a world of AIS is just an
interestingly different world of natural
selection wherein they will be borrowing
pieces from each other right and left
and then we want to understand what's
sort of the structure of this world of
what is the unit of organ organisms or
species or whatever in this world is it
is it a world where it's basically all
one species because all of them can
borrow and include design elements found
by all the rest uh and there and you
know that's worth thinking through it's
just basically what does natural
selection among AI look
like uh by by comparison among bi you
know DNA by comparison with human
economies and you may well find things
to worry about I but I I just so for
example I have this book the age of M as
as you mentioned I think the first
priority is just try to think through
scenarios and how they would play out
and then ask what are the things to
worry about I think just trying to go
straight in with like can I can I
identify things to worry about before
you worked out the scenarios before
you've filled in a picture of what it
looks like is a
mistake sure so I would be happy to help
people think through how does the
scenarios play
out and then in that context I don't at
all exclude the possibility you might
find things to worry about and maybe
even things you could do something about
now but my perception is there's just
way too little of that sort of working
out the details of how it would play
out compared to say what I've done an
age
EV uh yeah that seems uh plausible to me
I
think I think personally I'm probably
most interested in trying to build those
for sort of three to five years because
I think our forecast in that range are
much better I'm hardly worried at all
about three to five years I think hardly
anything will happen at 3 to five years
years so uh but um I that's based on my
expectation that you know even dramatic
research demos take a long time to feed
into the
economy yeah I think I think your view
is underrated by I I don't Dario gave
that thing saying that in 18 months
you'll be getting AI to organize
birthday parties for you and I do not
believe that no very unlikely to me I'd
be happy to bet against a lot of these
strong claims but uh you know that
people aren't they're much more
interested in making the claims than
than betting on them than yeah yeah sure
I mean honestly I I think in say five
years this will all look pretty bad for
AI risk people because they will have
been seen to cry wolf
when not much will have
happened H yeah some should be preparing
for that
scenario yeah I think that's that seems
posable to me um I don't really I don't
really necessarily like this but it to
the extent that many such people are
involved in regulation and
governance that probably won't be
damaging to their regulatory or
governance
careers um no probably not that's not
how that works unfortunately which I
think is
a a shame I think I'm gonna say right
and and in fact probably it would be
better for them to be involved in
whatever activities they are even if
they're based on misleading assumption
than to have not been involved at all
people who are not involved would just
have much less experience on the
resume yeah
maybe
um well on that happy note all right
thanks for talking Nathan yeah thank
thanks for talking too Robin and uh uh
yeah I might think well I'm sure I'll
think more about this and uh yeah I I
appreciate your work I think uh you're
one of the most generative people I'm
aware of and I really appreciated the
style of of age of M I think it's I
think few too few books are written
which really
mathematically sociologically take
seriously a premise and just sort of run
with
it and and run to even places where it
seems sort of kind of ugly or or
uncertain and I think if there were you
know a thousand times as many books like
that then you know a few of them would
be really really tremendously useful I
think we we know a lot more about the
possible
Futures yeah anyway have a lovely day
thanks very much okay take care TR bye

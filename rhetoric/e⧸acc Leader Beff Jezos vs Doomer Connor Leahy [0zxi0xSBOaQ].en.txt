as I like to say people should have the
freedom of choice for sure sure sure
whatever like but like Libertarians are
like house cats fully dependent upon a
system they neither understand nor
appreciate 100% agree with that by the
way like like hand like me and you
handshake on that like fully agree on
that good I did not expect you to have
this view so thank you for elaborating
on this I don't think you're crazy I
don't think the E you know some of your
some of your followers are crazy um but
like a lot of the people that you know
follow the kinds of beliefs you talk
about I don't think you're crazy yeah we
we just think that currently the current
discussion with AI regulation is led by
the current day oligopoly and they're
the ones writing the laws and so we're
deeply skeptical of everything being
written right now but we're not in
principle against regulations if they
help acceleration right in today's
presentation Beth jzos also known as gam
vidon started the EAC movement or the
effective acceleration movement it
borrows heavily from the idea of
accelerationism which means that we
should hasten the advance of technology
and capitalism at the cost of everything
else the main you know Theory of physics
that is most relevant is thermodynamics
that's what Nick land did you know like
Nick land allegedly used to be a Marxist
and he was taking doing Marxist Capital
analysis of like what happens if techn
capital gains more and more power and
his conclusion was eventually there was
only Capital there's no labor there's
only Capital there's no people there's
no happiness there's only competition
there is only capital capital itself
becomes sentient and like this is before
like AI was a big topic it's like in the
90s when he wrote this kind of stuff
right and he's the only goddamn person
and all of accelerationist who actually
bites the bullet and actually goes all
the way if you cap compute at a certain
threshold for llms that affects let's
say drug discover research where we
might need way more compute than that to
to to to do our research right or or
materials research and that has a net
negative effect cuz it's still Ai and
and above that compute threshold right
and so what I'm arguing for is let's be
very careful as to like What legislation
we crystallize and let let's have it be
as light touch as possible I think there
will be some regulation I'm realistic um
but I I I I do think that we have to be
mindful of like what we're going to
affect in terms of potential positive
effects of enabling High compute uh AI
research uh like we we don't we don't
want to shoot ourselves in the foot uh
with any sort of Regulation I I don't
think what's been proposed so far has
been good like bro you create a
fictional character and now you take on
his beliefs rather than your own on
Twitter like I I you're letting the
Twitter algorithm pick what thoughts you
think like you can do better than this
you need to improve your medic hygiene
yes but that's my point you want Nuance
discussion you don't want the that
Beth is doing on Twitter like if you had
come into here like George but worse
then I would be like yeah you're Beth go
on but you're not Beth you're not you
don't want Beth so who is this Beth guy
well he was recently on the Lex podcast
he's got a PhD in physics in fact he
used to be a Quantum Computing engineer
over at Google and a man after my own
heart in this respect at least he's a
big fan of thermodynamic intelligence or
the physics of intelligence and fans of
the show will know that that's right up
our street so um certainly in in that
respect I am a fan of of Beth enforces
this this this equilibrium in a top down
fashion and that that I think is a bad
trade and I wouldn't I don't want to
take it right and that's where yeah
congratulations as lord of the Doomer
you know Honor Society I bestow upon you
the rank of Doomer you are now become a
Doomer you are a Doomer you are a Doomer
cuz you believe that both that we are
basically we agree that Monopoly
then inducing or or achieving regulatory
capture is bad and that's also what
we're fighting in the present moment we
feel like is happening and that the AI
safety discussion is being leveraged
instrumentally for this regulatory
capture and I don't even disagree with
you also joining us today is the world's
second most famous Doomer Connor Lehi
Connor founded the aloa AI group and
also co-founded conject
and I was over at conjecture in their
offices in London um by the way we
filmed about 70 minutes of pre-
interviews and smack talk which is on
our patreon and we also filmed about 30
minutes or so post debate uh with Beth
and and Connor so check out those videos
on our patreon we can do such incredible
things and yet people have to fear for
their lives their stability their family
they have to watch their parents grow
old the constant struggle of politics of
Corruption of all these things and we
can do better okay so you hear it here
first guys eak is not about maximizing
entropy from the word the the mouth of
the man himself it's about maximizing
free energy dissipation over time which
is what out of equilibrium
thermodynamics optim we're we're we're
prone to being manipulated right and
like this is just cope what you're
describing is is that reality is hard we
dropped a nuke on South Carolina
accidentally it's still there it's in
the swamp it's still there and it was
pure luck that the the it was actually
armed the nuke was armed and we got so
lucky just it misfired a city would have
been blown up it's would the world would
not have ended actually so yes sure but
like what happens if that H if this is
the rate every decade or two we drop you
know one a nuke on South Carolina and
our nukes get better and better and
better eventually the nukes are
synthetic bioweapons you know AGI ASI
these kinds of things what
happens so I mean even bioweapons there
was a leak of a bioweapon we didn't all
die we probably all got Co once or twice
yes sure the world has never ended in
the past this is true until it's not
like this isn't an
argument what do you g what do you think
we should do do what should we do yes um
well I try to enact my values right I
mean I work what are your values I'm
trying to figure out what your values
are is what we're trying to find out you
I'm trying to SC civilization right I I
like so personally my my life's Mission
again I'm asking you for the whole world
you know whether or not the Chinese
already have it or not would releasing
all of our secret you know engineering
documents of all NFC 16 is a force for
Destruction it's not a force for
positive econ economic utility and
you're drawing these comparisons all day
I'm trying I'm not trying to
this is not a metaphor you are but
you're you're just lost in analogies and
analogies analogies let's talk about AI
directly right you're just trying to
like answer the question so if you
follow in the will of God he shall
reward the faithful that's your ideology
I mean it physics is my God to some
extent I'm not trying to actually
replace humans I'm working on like
physics based intelligence like trying
to understand uh chemistry uh materials
you know uh Fusion nuclear fusion carbon
capture there you go respect like
respect I mean you're wrong and you're
going to make everything worse but
respect you
tried like you know you will say the
same thing about me obviously and like I
you know I think we both agree though
that we want people to have more agency
and step up like there are no adults in
the room yes coming to save us right I I
think for me that realization in my
mid-20s was when I was the adult in the
room like this is the thing you need to
understand a lot of people are actually
evil and if you make an ideology that
justifies evil they will use it you can
make ideologies that do not justify evil
or justif if it less like I don't know
how to say this in a polite way but Beth
is evil like gem isn't evil Beth is evil
like Beth is an evil character and I
think you wrote him intentionally to be
evil yeah so uh we're pleased to have
the society Library map out uh the
debate today uh create a chart of all
the arguments that are made um it's a
great uh nonprofit U made to uh chart
out arguments in in full detail and so
so there's going to be a recap you'll be
able to access in the link in the
description okay well in which case we
should kick off the debate so this is
the amazing moment that we've all been
waiting for and if you don't mind Beth
I'm going to ask Connor just to kick off
with his opening statements yeah I mean
the first thing I want to say is really
glad to talk to you man it's not through
the anonymous Veil where you know things
can get a bit feisty at times sure um so
big respect really glad to talk to you
and so on you know we disagree on some
things but you know let's talk them out
you know let's see where things go and
if we end up disagreeing that's okay too
so I just really appreciate I just
really want to get that just want that
said I think this is going to be really
fun like you know as much as I disree
with you many things I do like a lot of
your style I like a lot of your
aesthetic a lot of good humor and so on
so really appreciate that and I think
it'll be really good but I guess the
thing like that would for me be the most
interesting at least that I'm the most
interested in to start with kind of is
like I
I'm not too interested in like going too
deep into talking about abstract stuff
right away or so on and like one of the
things that's some difficult with me
when talking with you in the past or
like reading your content in the past
has been is that I feel like there's
sometimes a bit of a bait and switch
where sometimes you know you or someone
in your movement would say something
which I would find rather crazy or
extreme and then when I try to engage
with it it said oh no no we're joking
it's not meant literally like we're just
it's just a metaphor you know or it's
just a Vibe so I'd be interested if we
could maybe start off this discussion so
I'm coming from you know more of a
safest perspective more like you know
technology is both good and bad it's
neutral it's it it can go well it can go
bad depending on how we deal with and so
from this perspective I'm really
interested to kind of hear from you like
do you think there is can you imagine
any technology that you think should be
banned any technology to be banned like
in its entirety or like certain
like just entirety like where you think
like the world would be better if we
like made sure no one had this
technology um has any technology ever
been successfully banned ever that's a
different question I'm not saying
whether it's enforceable I'm asking
whether like if it could be like do
would you find this I don't think it's
enforceable right like that's fair but
if you could enforce it do you would you
think that there would be such things we
use all these abstract theories in eok
really I'm just a you know high high
dimensional optimization scientist uh I
worked in Quantum machine learning now
doing thermodynamic Ai and that's sort
of the technical lens I view everything
and to me finding technologies that have
positive utility to everyone is a sort
sort of search process and if you have
you don't know when like a technology
that could uh mostly yield negative
reward for example you know nuclear
fishing
is you know a few eded distance away
from a technology that could yield
massive positive reward like nuclear
energy or nuclear fusion and so that's
sort of the general um mindset of eok is
that you know there there's a lot of
closing doors entirely there's a lot of
potential upside we don't even know we
might be leaving on the uh on the table
and we got to be really mindful of that
and it's supposed to be a sort of
balancing Force for to the natural human
tendency to sort of fear the unknown or
if we have a sort of first order model
of hey here's a potential negative
effect of a certain technology maybe we
should close that door shut it down um
you know going back to the nuclear
analogy right like I I I I do think they
sort of the the the the mindset sort of
yielded a net pretty net negative
outcome I think we'd be far better off
if we had far less regulation and we had
had far less fearmongering towards uh
nuclear fishing I think we'd be in a
much more prosperous society and so I
would say um you
know Define but Define band like who who
enforces it right is it a peer-to-peer
thing between countries or is it a top
down and and I'd love to like dive into
like enforcement of of rules I I I I
think to me like yes some technologies
can have negative uh impact but in
general uh the the thesis is that uh the
market and civilization tends to sort of
provi amplify or provide more resources
towards developing technologies that
have net positive uh utility or positive
effects towards growth of either uh an
individual company Nation or or all of
civilization and that that sort of um
probalistic safety or sort of natural
selection in space of Technologies on
the short term can yield sort of
negative fluctuations but in long term
uh post selects for for things that have
positive utility towards growth right
and it's a much more nuanced way than
like directly
legislating uh uh Technologies to steer
the technological landscape manually in
a certain direction from from at the end
of the day you have a model of how
things would play out if you let that
technology be be unregulated and and I
guess our core thesis is that this sort
of uh adaptive uh algorithm that is the
the market um uh is a better heuristic
it's a better online adaptive uh search
than sort of top down modelbased control
that would you know uh include you know
legislating from the top down um and so
but yeah happy to expand upon that uh if
you have if you want me to dig in but
happy to dig into anything
yeah yeah thanks for the elaboration um
good to hear from you precisely but I
want to go I would actually like to put
a pin on the enforcement for just a
second here and I think a lot of your
models are sensible like thinking about
how the market isn't much better than a
lot of top down control anyways I
definitely agree with that in many
circumstances I don't want to dismiss
this argument by any sense um but I'd
like to I feel like you still haven't
answered my question my question wasn't
you know should all technology be banned
or should this I'm asking can you
imagine
being a technology that should be banned
like is an AC thing in your ontology or
do you think this does not like this
Cann not even
exist um I think I think there are
technologies that we we can ban like if
there are pure net negative play and the
legislation is very sharp and pointed
towards the um uh usage of it let's say
as as a net negative uh impact towards
the world uh yes but like how how is
that enforced right it it depends on on
so much right uh absolutely and and how
is the law written um and I I I do think
like overall like I I I see legislation
as sort of uh
hyperparameter uh settings for you know
certain let's say countries or or
subsystems of of of of the whole system
and and it's a sort of somewhat discreet
architecture search process that we're
doing but o overall the thesis is that
um you can have countries that will
maybe over legislate some that will
under legislate but
eventually if we if we had a nice and
malleable legal system or or or or you
know if governments were very Dynamic uh
with their legislation including
sunsetting Clauses and and and and
updating regulation in sensible fashion
we would have a very healthy adaptive
algorithm to Converge on what are the
opt what is the optimal legislation to
have sort of sustainable fall tolerant
uh growth um but I don't think that's
the system we have today and I guess
what we're seeing is a sort of second
law of bureaucratic
complexity uh these days uh where
legislations are added and bureaucratic
complexity is added but not so much uh
removed uh and so the bias right as as
it stands right right now how we do
things in government is towards this
system is so archaic just like let the
market regulate itself uh and uh you
know keep your hands off and by market
regulating itself I mean you know if I
if I'm a company and I deploy a
technology that is net negative reward
to my peers we have a sort of
peer-to-peer formalization of sort of
violence between corporations meta
organisms like lawsuit right and it's
it's it's basically uh uh legal lawfare
legal Warfare um and and you cause
economic damage to the other organism if
it induces negative reward on on the
other ones right and so so that's kind
of a that is a sort of restorative Force
towards not doing terrible things right
to to if I understand correctly you
perceive lawsuits as part of the market
because lawsuits are from my perspective
regulation um well regulation sets the
the prior of how a legal dispute will be
resolved but and legal disputes to begin
with like and also the enforcement like
the reason lawsuits have teeth is that
if you violate what the court says they
send police after
you yeah so so so the point well
eventually uh right like if you can't
peer-to-peer
enforce uh certain certain Behavior
right enforce do you mean mercenaries
like would that mean yeah well I I don't
know I I'm just very um well I'm very
weary and we'll get into this I would
really like to get into that component
of sort of the Monopoly on uh uh sort of
create creating like a a a physical
power asymmetry right through violence
Monopoly in a top- down fashion and also
having intellectual power asymmetry
through through AI I'm very weary of
having very strong top down
power asymmetry I understand that your
prior is that by uh deferring uh power
to a higher uh node in the in the
hierarchy of control of of of society
like let's say a government or or some
sort of leadership you minimize sort of
peer-to-peer sort of competition and
friction by sort of having that that
Authority sort of Ensure uh peace um
but I do think one outcome but yes
possible outcome yeah yeah but I I do
think that we are in a
weird well this is this is a tangent for
later but I I think we're in a weird
period now where there is the window of
opportunity for there to be uh sort of
AI assisted tyranny to be installed and
to me that's one of the core
existential risks to progress I don't
know if it's it's fully existential but
it's definitely going to slow down
progress for a while if we have a sort
of Big Brother style uh authoritarian
panopticon like I I I think in in the
present days there's a danger that you
have a monopolization and a
centralization of AI That's that
coalesces with government and there's a
control of information flow there's a
control of uh of the truth what or what
we see as the truth and then that does
break uh democracy right because
democracy is assumption that everyone is
informed to some extent can make their
own judgment they're supposedly
uncorrelated variables which of course
in the era of algorithmic amplification
of information propagation they're not
let's say they're uncorrelated variables
and you take a sort of uh average vote
then then you have a good sort of
decision mechanism I think with sort of
algorithmic uh manipulation especially
with AI I think we're going to have a
lot of trouble with that and so I'm also
concerned that there will be attempts to
um feain uh sort of democratic vote
towards uh AI safety and and giving sort
of uh you know a monopoly on this
intellectual artificial intellectual
power uh to to the centralized uh
organizations um you know this sort of
democratic vote will be manufactured
consent and and then once the power is
centralized those entities won't want to
give it back uh to the people and
they'll have an ability to maintain that
power gradient because
intelligence uh allows you to extract
more utility or or or filter uh or
engineer information and information is
power so AI is power and so there's
going to be a sort of gravitational
effect of of power and and and AI
capabilities that that tends to to to
centralize uh uh you know AI
capabilities and power overall and eok
is a sort of you know bottomup sort of
counter Force to this natural tendency
towards sensorization and top- down
control and so we're we're we're trying
to push you know the Overton window and
the discussions toward an extreme of of
of the hyperparameters regime but
frankly with knowledge that you know if
if you know you or or maybe being not
yourself you're fairly balanced but like
some people on the AI safety Camp are
are very concerned with or have a models
models with uh with respect to which
they're concerned about the future and
they want to you know centralize the
control h maximize safety versus on the
other end we want to maximize freedom
and lower the risk of sort of uh AI
assisted tyranny reality is going to
fall somewhere in the middle in terms of
policy and and that's why that's that's
why we got to have these discussions um
but but for us you know we we just
really want people to factor in that um
historically give giving a monopoly on
violence or giving a monopoly on on
power to centralized entities that
eventually every uh subsystem in our
civilization serves its own interests
including government uh that can yield
really bad outcomes and we have a better
data driven prior of this happening than
let's say some sort of artificial super
intelligence taking over which TBD if
that can exist and what that looks like
um and so um to us that's like the sort
of existential risk that um we want to
to to to minimize is kind of the erosion
of our freedoms that seems Progressive
at first and then all of a sudden you
know we're in a really bad spot and
we're in a sort of um top down and Force
Dark Age where you know freedom of
compute is non-existent freedom of
access to Ai and freedom of access to
information is no longer a thing and so
this tangent kind of evolved into a
whole uh discussion but yeah please feel
free to like jump in whenever you feel
like uh you you know we could keep this
organic yeah yeah so um it's a lot of
nice word celery but like from what I
could tell you made like five to nine
points there that were only tangentially
related and I could address them
individually but that's a bit difficult
so I'd like to bring us kind of back to
what started this whole tangent which
again I feel you still haven't answered
my question should All Tech do you
believe all technology all technology
should be unregulated yes or
no why I mean there's no absolutes right
like I think that so that means no you
think some technology really that's no
sure yeah sure cool okay this is what I
want to establish this is the question I
want to because it would be a Crux
because if you had said yes then that
would have been a Crux for me and we
would have to talk about that but if you
say some technology and some s cool then
we can have an object discussion that's
great now we can have an object level
discussion about what technology should
be regulated in what circumstances what
enforcement mechanisms where do we
disagree about how the future goes did
that make sense so this is kind of how I
would like to have this discussion
because you obviously have a lot of you
know ideas in your head you have a lot
of good models that I'd be interested in
you know hearing about as well maybe
pushing back and some of mine as well I
think some of the things you say are
definitely true like I definitely agree
that nuclear regulation turned out to
ultimately be mostly NE negative to
large degree not entirely I think
nuclear weapons regulation is really
great I think it should be even stricter
personally but I definitely agree with
you there a lot of side effects of this
so yeah I think you made some good
points there but there's some I also
disagree with and I'd like to understand
your position a bit more so like I'd
like to for just to again I'm just
trying to understand kind of like how
you view the world and so on like I have
I have a question what is your opinion
on leted
gasoline leted gasoline yes yeah I mean
I think that um it's it's good that it
was banned I mean it was NE negative and
I think that um it induced a sort of
selective pressure on the on the space
of technologies that was for the better
right um yeah but it was banned by the
government not by corporate to corporate
interests or anything like
this well I mean at some point if you
have a like if you have enough data that
people are suing each other for the same
cause right like you've caused uh me
brain damage by putting this chemical in
your stuff uh then you can you can kind
of crystallize that that sort of uh uh
prior that like this this this thing
tends to happen into legislation I I
think that if you're trying to draw an
analogy with AI right now I think the
landscape is moving so fast that um we
don't have enough uh data of how things
go wrong and we don't even know where
models and compute are going that it's
far too early to settle things in set
things in St Stone and again I would be
less aggressive on pushing back if there
was a precedent of like
sunsetting uh regulations which you know
doesn't doesn't tend to happen in our
current system um um and so for me it's
like let's be very careful to like have
these one-way decisions where we close
the door on a whole spectrum of certain
Technologies let's say we have compute
caps and whatnot uh uh like we should be
really really thoughtful and it's I'm
I'm just very skeptical
that uh our models of how this thing uh
will evolve the progress in AI are are
necessarily uh accurate whereas like
with with the lead I mean there was many
studies like hey like this is really bad
for humans I mean it took decades many
decades it it took well it is like it
caused massive damage to whole
generations of
people but I mean how else you still
like before even with top down policies
right right you can you can make
decisions that take time to realize the
the nth order effect and then and then
you have to readjust right so whether
like the point whether you're bias
towards under or over legislation you
can you can screw up in many ways right
and and that's the that's the thing
about uh you know top- down uh
regulation and you got to be very
careful and and thoughtful and make sure
your model's accurate right and and not
only go ahead go ahead please if we
agree that you can screw up in both both
directions I think we're on the same
page there this is no question for me I
think a lot of the regulation that has
happened and I I like you bringing up
the point of sunsetting laws this is
definitely a huge problem like there
should be something like you know every
law has to be like rechecked every 10 or
20 years or something I think this would
be fantastic I'm strongly in favor with
this so I think this is a good prior
that you bring up here is that like
given the ways that laws are currently
handled how they're like never repealed
and so on we should increase our
skepticism towards passing new laws and
regulation I think this is a very
reasonable intuition to have and I want
to acknowledge that that it's a very
reasonable intuition to have it's one I
have as well but this doesn't generalize
to me then necess that we shouldn't do
any regulation which you don't seem to
believe either so we're kind of on the
same page there and I also want to make
very clear here that like I'm not some
guy who thinks that like government is
awesome and great and regulation is
awesome and great by no means I'm try to
keep my identity small you know I don't
really have a politics I'm not I don't
care about any specific word or any
specific flag what I care about is what
works what I care about is under my best
models of game theory of uh
technological progress and
whatever what leads to the best outcomes
and avoids the most catastrophic
outcomes and I think you would agree
with that as well I'm sure you're trying
to do the same thing yeah and so I don't
think we disagree I think we're on the
same page we just have different models
of the world and that's why we're having
this discussion that's why it's
for people to listen great so I'd like
to go a little bit into some of my
models here so you talked about how for
example is that like we just don't have
data about how things go wrong and this
is reasonable and I wouldn't dispute
this like you know do I know how AGI
will end the world I don't know
literally I don't have studies about
agis you know controlly do two different
planets you know and do different solar
systems one of them has AGI one of them
doesn't I don't have studies like this
and my claim is you don't get studies
like this this is just not how things
work like you know there's there are C
what the world you describe where you
can try things you can you know up
whole generations of people with poison
in their head and then fix things
afterwards is an erotic world this is a
world where you get to retry it's a
world where you can fail a couple times
blow your hands off or whatever but the
next generation is going to be fine and
my claim is and I'd be interested if you
disagree with this model is is that at
some point you exit the this world the
world is not erotic actually it's
actually very non- erotic you can die
you there's technology that you
personally cannot invent because in the
process of inventing it you kill
yourself like you know a part of
inventing nuclear technology required us
to invent technology to not die while
handling nuclear material this was a
very important part otherwise you just
can't develop it because you die so when
you're I'm wondering if you agree with
this forget AI for a moment that at some
point not saying it's AI just at some
point we will develop technology that is
so powerful that if you it up it
blows up everybody do you agree with
this I mean in principle we could
already do that if we build a big enough
nuke right now right like we could just
blow up the Earth right yeah um but have
we done that we still nuclear weapons
right yes this is a far I was just
interested if like you think this is
possible so I would agree that like I
think we're already in a non-erotic
world there's already action chains
accessible to humans that are
non-recoverable like if we went down
these action change we don't learn we
don't get to retry it's just over and so
when you're dealing with a non-erotic
world where you don't where there are
traps there are paths you can't recover
from you have to play different
strategies you can't just try all the
paths if you just try all the paths in
the limit you die yeah yeah so so there
are there are paths you know where the
the the reward function is like negative
Infinity it's you know full existential
destruction so at this stage for example
what would be a state of the world it's
like big building one nuclear bomb that
has enough you know power to like I
don't know shatter the Earth it's
probably difficult to do uh right now
with I don't know all the details I'm
not a nuclear specialist but but in a
sense like we know that and we don't
build that technology cuz it doesn't
have utility uh to us because we're only
interested in our relative positioning
with one another because we have this
sort of fractal competition uh at all
scales so what people do is they make
smaller
nukes uh that you know could damage
their enemy but not destroy themselves
in the process even though you know
there's the the game theoretic
equilibrium of mutually assured
destruction to be fair I do think I do
think at least from the models I've seen
of nuclear war uh I think like 10 or 20%
of human population uh survives and on a
longtime scale we would probably recover
uh so it's not actually fully
existential uh risk yet uh but again if
we had a nuclear bomb above a certain
threshold uh that was just sitting there
you know with a big red button uh that
would be bad so but okay yeah if if
we're try if we're going to try to draw
analogy to the AI I think we're still
very far from that yeah this is I'm not
trying to make the analogy yet we we'll
get there I'm not making it yet it's
cooking all right so now imagine um you
know let's say I you know drink a bunch
of leted gasoline and decide this whole
AI safety thing is stupid I'm GNA go
join my friend gum and his startup and
let's build some Quantum Hardware right
I'm like let's go not Quantum so yeah
sure sorry I don't know the details of
your actual stuff like let's let's say
it's Quantum because it sounds cool so
um Thermo sure yeah Thermo Quantum
whatever but let's let's go with Quantum
for a sec so let's say I join you we
work on some cool Hardware chips and
we're doing some CRA you're a physicist
so like you you know I'll use some
physicist stuff and I know this is not
exactly correct but indulge me and let's
say while we're doing our experiments
late at night in our secret laboratory
we go through the data and suddenly we
notice oh we're in a false
vacuum so what this means is for the
viewer a false vacuum is a
hypothetical uh Quantum event or system
where the vacuum energy is not zero what
this means is is that if you could
trigger what's called a vacuum collapse
you would basically destroy chemistry
it's just like there would be an
radiating shell outwards where all the
physics like all the stars stop all life
stops everything and this could happen
very suddenly with a very small trigger
hypothetically we is this probably not
true but let's say in our hypothetical
experiment it was true and we find out
that our Hardware can trigger a vacuum
collapse like we figured out how to do
it with our hardware and this Hardware
can be manufactured say with a you know
in a semiconductor Fab if such a
technology existed and you and me had it
what do you think we should do with
it I think that if uh the vacuum was in
a so so for for reference it's like the
you know instead of being in the ground
state uh of of you know the the quantum
field you would be in a in a literal
local minimum then there's a true ground
state and then first of all I would
imagine that to to engender this this
this jump you'd need quite a bit of
energy another thing is like um anyways
I won't I I don't believe in false
vacuum that that's a whole discussion
but just yeah yeah but
um essentially I think like if that were
the case that a tiny nudge
would blow everything up we would be in
a highly unstable
situation um I think
that there is a c like even if you had
if you alerted formed a world government
and you alerted the authorities and this
would be the most secretive secret on
Earth it would have a certain coherence
time a certain shelf life and basically
we'd already be dead like like on I
would Bas price this as like just like
terminal at that stage I I don't think
that's the case I don't think there's
anything like that that's the case like
I mean some people thought a more
realistic scenario is like um you know
if you theoretically crank up the LHC
high enough energy you might create a
black hole uh but if that usually black
holes that are tiny they they radiate
away through Hawking radiation but but
assuming you could have created a black
hole that starts consuming everything
like it would have been would would have
been problematic uh anyways we can have
a whole technical discussion that's not
the case but uh yeah yeah the only
reason bring this up is just kind of
like um maybe also get you in a bit of
the mindset of like if you did believe
that like if you did see this happening
like you would reasonably come to the
conclusion I think you came to we're
like wow we're really like we're
really like not even government
can save us like this is a really bad
situation to be in I still well so
here's the thing right like if there's a
if there's a piece of physics or
technology that we don't understand and
is dangerous we should we still need to
study it in order to uh control it and
and and make sure for example I don't
know like we're we're in completely
hypothetical land here but like let's
say there was a way to create this local
instability there's a way to contain
that or you know uh correct it you know
usually if you can control a state to go
from one from A to B sometimes you could
go from a there's all sorts of yeah like
my my point is um like I I think we
should still like we we should study uh
well that's why we're studying physics
right like we we got to know you know
for example I think a past 20 30 years
you know we were looking for are there
extra folded Dimensions that could just
freaking unfold right now or you know
like and when we had to test that right
like really it's like there's there's
kind of the unknown where we our
instruments and our life lives in a
certain band of energies and anything
beyond that we we don't know and in a
sense we kind of have a responsibility
for our own uh survival to to try to
explore that and I guess like explore
those those areas of the unknown that we
don't have perception here I think we're
talking instead of like regimes of
energy we're talking about regimes of
intelligence what does like intelligence
look like Beyond a certain
threshold right and we don't know right
we don't know at at the current time and
that's why we're exploring uh and that's
why you know we have folks like OB
anthropic yourself Etc right yeah yeah
yeah yeah you're you're already jumping
ahead in the analogy but yeah like so
okay so there might be you know dark
voids and you know dark Horrors in the
physics I think you agree like we might
be in a false vacuum or there might be
some other thing out there so like you
know not likely maybe not false vacuum
specifically but like it's a
that there are Horrors so here's here's
a here's okay so I mean you're leading
this conversation here's here's a a
similar sort of question for you MH what
if there were very Advanced aliens that
have very advanced technology that are
kind of scouting the Earth right now um
you know completely hypothetical
scenario would you feel a sense of
urgency towards accelerating our
technological progress to ensure
the ensure that we become formidable
enough
to uh sort of defend ourselves I mean
there are analogies with AI and people
will compare it to an alien force
invading us and so do you feel like
don't you feel there is a sense of
urgency to make humans more formidable
in order to ensure our our our safety
against the unknown right like could
also be you know I mean at some point
we're going to oscillate out of the
galactic plane and there's going to be
more asteroids we need much more control
the rocks that are getting hurdled our
way so we have a responsibility to scale
up our space exploration Technologies
for survival but wouldn't wouldn't you
feel that urgency and wouldn't you be
kind of more open-minded towards taking
a bit more risks AKA like just venturing
into the unknown to to seek upside
because you know you could get uh uh
disrupted otherwise right I I I think
we're I think that's why you founded
your company you're trying to make
humans more formidable by alignment I
think that's what people well some
people solving alignment are trying to
do right they're trying to augment
humans to make them you know more
formidable which I'm totally for right
that that's like I think we should we
should uh seek that path um but I think
that um well we're getting into AI now
but to to me I think all paths will will
be like explored I think there's going
to be sort of aligned AIS that are
extensions to humans there be like
totally independent AIS and then there's
going to be like full leites that don't
want any technology and you're going to
have all three and you know all bets are
off at that point right so yeah and I
mean I definitely don't disagree with
you that risk taking is necessary and
important and you're also correct that
if you're under threat it might be
sensible to increase your risk tolerance
budget but this isn't an either or so I
I agree with you on an emotional level
that yeah if like there was aliens
threatening Humanity or whatever I would
be like well we should take some more
risks with nukes but it doesn't mean I
think we should take arbitrary risks
with nukes because a lot of at some
point it wraps back around to being
embarrassing there is like a glorious
you know we tried and we failed there's
also a really embarrassing we took all
the safeties off the nukes and they
exploded in in our Air Force bases and
my claim is is that the current state of
AI safety in eak is firmly in the our
nukes exploded while still in our Air
Force
bases our nukes exploded has we didn't
even take off we didn't even get the
nukes into space we we like they blow up
on our Airfield we didn't even get them
into space like we were ready to have
the heroic stand against we didn't even
get him off the planet this is where I
has something happened in AI where it's
been that catastrophic so far not yet
but you're my prediction is is that if
we just don't even try if we just
accelerate as fast as possible that's
what happens it's not heroic it's not
epic it's just we make stronger and
stronger systems that we understand less
and less and less we get more and more
confused things accelerate more and more
and more and then one day we wake up
with you know our finger up our ass and
we have no more
control I think that um it's a bit
different with like the nuclear analogy
is mostly it's all about engineering
negative reward uh for your adversaries
right to to maintain sort of that U you
know game theoretic equilibrium
I think with AI there's a lot of
upside to accelerating right and there's
lives that could be saved and that has
to be priced into the the risk
calculations right the longer we wait to
develop
Technologies you know there's all sorts
of Biotech Material Science there's
there's a lot of upside we are leaving
on the table and having a model where we
only look at the tail event uh like tail
probabilities of of extreme downsides is
is very bi in terms of our our sort of
cost
benefit if I was if it was only tail
risks I would agree with you I just
don't think they're tail risks okay well
we could we could we could dig into that
um but I'm I'm just
not I am not convinced that there's this
sort of um fast takeoff threshold I'm
not convinced that uh we can't achieve a
sort of
multipartite uh aders equilibrium by
making sure capabilities you know multip
multiple parties have access to to
Advanced capabilities and keep keep each
other in check um anyways we we can get
into it I don't know where you want to
take this I mean my my simple question
is well you have a bunch of agis
competing why would the one that makes
that spends its resources giving humans
a good life win and not the one which is
maximizing its you know kill all the
other people and ai's potential why
doesn't that
win well I mean you can say that about
companies or countries I do say that
about companies and
countries this is a good question why
don't they this is a good question well
because there's a positive sum game to
it's positive something to cooperate you
know we've entangled our economic
systems even against our our current
adversaries and why did we do that do
why do we do with ch because yeah so I
think that all like the the the the
theory to some extent behind eak is that
the system will adapt towards whatever
policies and and Technologies and ways
of doing things that are optimal for
growth and that I mean just by
construction like things that grow you
know they they replicate or or like if
you post- select in the future odds are
you're post- selecting for things that
have high Fitness towards growth right
yes uh and so um like for example if you
have a very aggressive
uh regime right that's just threatening
everyone and not adding a lot of
economic utility they get shut out of
the out of the of the system or or they
get like you know peer-to-peer like
enforced like hey if you try to do
anything we're going to annihilate you
also you're not going to have you're not
going to participate in this sort of uh
benefit and so I think we have ways to
align sort of intelligences that are
super human like I mean a company is a
superh human intelligence it's a mixture
of Experts of humans with neural routing
is technically smarter than any one
human I think so we can have a debate
about that but we we have ways to align
these these superorganisms of of of uh
Beyond human intellect and through sort
of economic uh exchange right and I
think that um the future looks like we
have AIS that are aligned or extensions
of humans right and then we have AIS
that are more autonomous and there's
economic exchange between both and that
keeps us um relatively aligned right but
there is to figure out how to
extend uh human intelligence uh in a way
that makes us more formidable and and
and more uh of a player at this at this
big boys table right uh but uh and that
but that's what you guys are working on
so that's good I mean yeah I appreciate
all that you just said but like I feel
like you contradicted yourself you said
it optimizes for growth and you might
notice that growth and human happiness
are two very very different
things sure well so I mean you have so
you have you have your choice right
nowadays you can live in Europe where
you're going to maximize your happiness
have your espresso chill a bit have a
two hour lunch no I'm just kidding uh
but or you can live in America where
it there you go might as well lean into
it but uh or you can live in America
where it's like it's all in on growth
right and I think that um you know you H
you have those two options and and I
think that's the beautiful thing about
uh the world right now we don't have a
monoculture we don't have a single way
of doing things we don't have a single
way to legislate things that's and we
can ab test things locally right sure
but in your model America will eat
Europe it will destroy Europe it will
grow more the system will succeed and
the people in America are less Happy by
your own admission so what will happen
is is that on average people will be
worse
off I think that okay so so this is an
important distinction between like eak
and EA we are
not uh hedonistic utilitarians right uh
yeah we are am
I okay well maybe not you but uh you
know EA in general I'm I'm just it's not
about just the comment in general you
know some EAS are trying to maximize
happiness and that's a different loss
function I think that has spirous weird
local Minima like wireheading and highly
suboptimal right um in many ways and
it's not anchored to reality we're
trying to maximize growth of the system
because we think that you know life and
intelligence is a very special phase of
matter in our universe and we have
responsibility to scale it and figure
out how to make it grow and and
hopefully you know we're we're we we can
all aim to be part of those future
states of high growth but the the thesis
is that if
you if you let's say let's say you
legislate let's say for a whole set of
countries you have a sort of Union and
then you legislate away the ability to
pursue AI past a certain threshold
before we go back into this just okay
you contradicted yourself again you said
you want to optimize for growth and then
you talked about the beauty of
intelligence and all these other things
those are two different things if you
optimize for growth you get growth you
don't get Beauty and happiness and all
these other things you like okay so so
so I I guess the thesis is that
intelligence
evolved as uh a way for systems that are
uh lifelike to uh adapt on a faster time
scale uh uh for better growth right
acquisition seeking out resources
acquiring them extracting utility from
them nature red and tooth and
Claw uh and so and so I think that if we
have to have a larger civilization it's
going to necessarily have to be more
intelligent overall is every individual
unit going to be more intelligent I
don't know but overall as a system has
to be smarter you because it has to seek
out far more resources and and utilize
them more efficiently and the beauty is
that if you have a subsystem that is not
uh optimal uh in terms of its uh uh
ability to acquire uh energy and and and
utilize it uh cleverly uh some branch of
it will Fork off out compete it and and
and dominate but the thing that out
competes is cancer the thing that out
compete is maximum growth no art no
Beauty no happiness just growth just
economy remove all the parts of the
brain that have emotions just focus on
replication locusts no emotions have
utility right they they've had
utility uh yeah I mean I an AI with I
expect and I expect it was a local
Minima I expect if we the future systems
will not have human emotions I don't
think human emotions are a global
Maxima they're not and but if they're
not a global Optimum why not seek to
explore new ways to H think or now this
is a naturalistic fallacy is is not
ought so I agree with you that there are
that are values are not some Global you
know coherent Maxima of any growth
utility function and my answer to
question is who the cares they're
mine I like them I like happiness I like
puppies I like people being happy
playing with their kids and so on and
yes this cost resources that I could be
spending on maximizing my you know
economic growth function in the local
supercluster
and my answer to that question is I why
should I care is this not
a yeah I I I do think like happiness
evolved as a sort of proxy of like your
estimator of your your your gradient of
likelihood of future growth or higher
Mutual information with the future like
if you if you have a positive impact on
the world you feel good if you're you
know let's say have a significant other
and and and feel like you're going to
have a legacy you naturally feel good
and your braid like secretes all sorts
of feel-good hormones so I I do think
they're just um you know our
neurochemistry evolved as sort of proxy
loss functions for for certain uh uh
effects um but I I don't know I I don't
know if I I think there could also be
awesome human like a whole new spectrum
of human emotion and euphoria that we
could achieve you know we're focusing on
like pain but it could be you know
infinite pleasure we can live in a a a
not I don't believe in Utopia def but I
I would say that we can have much more
comfortable lives uh much happier lives
and that overall techn Tech techn
Capital the techn capital machine is a
deflationary force that uh you know
helps uh everyone have access to a
higher quality of living and and overall
like can increase happiness and in fact
the things that causes pain like you
know healthc care housing uh uh legal
systems have been overregulation and if
we had let technology sort of evolve
faster uh and and and and and create
more deflationary force in those areas
is like build more housing and so on it
would be cheaper more accessible and
then more people would be would be happy
and so to from our perspective actually
deceleration is what has caused pain so
far in our current system rather than
acceleration but sure I'm I'm not I
would dis agree with you especially in
hous policy but like that this is the
kind of stuff that makes me call eak
Fisher Price Nick land like you don't go
all the way you only think on like
American local thing like 5 years into
the future extrapolate your own beliefs
man like actually take it like take
techn Capital its logical conclusion
that's what nickland did you know like
Nick land allegedly used to be a Marxist
and he was taking doing Marxist Capital
analysis of like what happens if techn
capital gains more and more power and
his conclusion was eventually there is
only Capital there's no labor there's
only Capital there's no people there's
no happiness there's only competition
there is only capital capital itself
becomes sentient and like this is before
like AI was a big topic it's like the
90s when he wrote this kind of stuff
right and he's the only goddamn person
in all of accelerationist who actually
bites the bullet and actually goes all
the way if you go all the way if you
optimize for something you lose
everything which is not the thing you're
optimizing for we got lucky we got lucky
that for example torturing people you
know to work 24 hours a day doesn't work
because they fall apart eventually but
this is no longer true when you have
robots if you have robots that can work
for 24 hours there is no reason to give
them time off there was no reason for
them to have hobbies there's no reason
for them to spend time with their kids
and love them there was no reason for
this we got lucky that we ourselves have
so many limitations that eventually you
have to compromise because otherwise we
can't function but if you take the techn
capital acceleration to its logical
conclusion The Logical conclusion is not
wow we have more housing The Logical
conclusion is there is no more human
just a bunch of mindless automata
optimizing some growth function
I don't believe I mean we have different
models of the future ASM tootes right
like uh I think that there's not a
finite set of jobs for which we're we're
competing in an economy I think we just
increased there's
aill not really there there's plenty of
atoms in outer space right and and if we
are I mean humans are bound to Earth
because we kind of overfitted our
biological hyper parameters to earth
right we've evolved over billions of
years uh over here whereas maybe a
synthetic uh
organism isn't as anchored to to you
know Earth living conditions and and
could um uh seek out to grow beyond
beyond the earth I think that overall
even if uh a large part of the economy
is accomplished uh like a lot of the
economy is is is is executed on by
machines doesn't mean that the human
component would necessarily shrink it
would get diluted
but if the overall system grows far more
it's kind of like taking Venture Capital
it's like oper it's it's intellectual
and operational leverage right uh we're
we're kind of diluting ourselves our our
share of the economy how much we uh
contribute but if overall economy Grows
by many orders of magnitude our overall
uh component can can grow and that's in
our best interest and in general we're
going to every subsystem is going to do
what's in their best best interest you
are arguing for the best interest of
humans and hearing you out I just think
that people are are are greedy at at a
company level they're going to you know
use more AI if it causes more growth and
that's just reality you're not going to
be able to legislate that away like
they're going to people will Revolt if
they can't use uh AI if it has massive
positive utility uh and and I think
people are going to do it at a country
level and we're going to do it at at at
the human level and I think just because
will doesn't mean we should is this not
ought um I
mean I think I guess like I'm just
trying to like you know it's kind of
like real politic it's like the again
you're contradicting yourself you're
you're you're you're you're conflating
two different things there's the thing
we want and then there's facts about
reality there's decision Theory like
what is true what what how do you win I
don't think we agree what we what we
want right sure and I'm trying to
separate them so we can have a separate
discuss because often I talk about what
I want and you talk about what works you
talk well people do it anyway so we
shouldn't even try this is an argument
about decision Theory this is not an
argument about
values okay so it's not an argument
about values but you're arguing for
maximizing human happiness I'm not I
hav't I haven't stated any values so far
I have not stated any values I simply
claimed that I like puppies I like many
things I'm not saying that sounds like
I mean that that sounds like a certain
utility function that we know sure
there's some kind of function there's
some way you can model it you can call a
Frisian PRI or whatever the hell you
want I don't particularly care what you
model it as this is not the the point
I'm trying to make the point I'm trying
to make is is that whatever the hell
that thing is I'm not even saying I know
what my values are I'm not even saying I
know what your values are but I claim
that your values are not growth you
don't actually want this as much as you
think you do and I for sure don't want
it and people don't want it because this
is not what we like it's and you can
make and then every time I bring this up
you make this argument about like oh but
it's what's going to happen anyways and
it's what is like that's a separate
argument your values are not growth how
so because I like puppies and happiness
and friendship and games and like why do
you like friendship why why why do you
like having relationships why do you
like being part of a group because
Evolution kind ofed you to Crave these
things because if you're part of the
group you have high passing on your gen
you're mixing up is an again like I'm
just going for the latent variable here
right you're describing is I'm
describing an a I think having a
subjective loss function for how to
steer reality is prone to being
manipulated and is the source of a lot
of pain in our modern
times so I I am arguing for an objective
loss function and we can argue that that
loss function is not anthropocentric
enough right free energy right we argue
about that but I I think that if it's
too anthropocentric it's too hedonistic
it leads to weird Optima and we're we're
we're prone to being manipulated right
man like this is just cope what you're
describing is is that reality is hard
yes you're yes if the thing we want is
complicated and hard to get the solution
is not well let's pick something simple
and easy to find and give ourselves a
participation award the answer is well
we have to get stronger we have to get
better we have to get better yes let's
get stronger the answer is not oh let's
pick an objective utility function that
I can follow so I at least feel like I'm
no it's not a it's not a it's not an
optional choice of utility function and
that's kind of the thing that the
anchoring in physics gives you it's like
oh no the this objective function for
you know seeking out free energy
utilizing it towards your own growth is
what the universe selects for it's just
probability theory is you're making the
same mistake naturalistic fallacy is is
not ought but you don't have an option
to obey gravity you can't violate the
laws of
thermodynamics perod sure again is this
not a you cannot derive an a from an is
these are two separate magisteria okay
so so my point is let so let's say you
have your you know let's say you leg you
legislate Europe the way the way you
want it or or half the Earth doesn't
matter which half I don't know and then
we have the accelerationist half we play
the movie out right we play the movie
out yes I think I think that the the
accelerationist half is going to outgrow
and yes and we both die in this scenario
both you and me will be dead I don't
know about that one I I I would need
more evidence you agree that eventually
we will build technology that's so
powerful that we could blow up anything
and in your model of the of
accelerationist world we build it as
fast as possible with as little safety
mechanisms as possible and everyone has
access to it what happens then no we
don't seek we don't seek out no but itd
no you accidentally you just
accidentally have a model right like
that okay if we reach this state you can
you predict ahead of time what
technology will be safe or not can you
predict ahead of time how power we can't
guarantee safety but we can't guarantee
we can't but we can do better than not
even trying but we can also it I think
guaranteed to shoot ourselves in the
foot in terms of like the upside we're
leaving on the table uh is almost it's
just as bad like in your models the
upside we are leaving behind does not
get priced accurately and I think it far
outweighs the the the risks um and I
think sure you can make this argument if
you want this is a coherent argument to
make this is a coherent argument to make
if you want to make this argument but
like from my perspective like we can
talk about this specific like okay okay
let's if we want to talk about pricing I
think this is a great point to go I
think this is a great place for us to go
I think this is a reasonable thing to
talk and disagree about but I want to
make clear again just the point I'm
trying to make here is that the point
I'm trying to make here is is that
predictably if you have a civilization
that doesn't even try that just
accelerates fast as possible predictably
guaranteed you're not going to make it
you're definitely not going to make
because at some point you will develop
technology that is too powerful to
handle if just have the hands of random
people and if you do it as unsafe as
possible eventually an accident will
happen we almost nuked ourselves twice
during the Cold War where only a single
person was between a nuke firing and
withell
orology scenario sooner or later you
know maybe it goes well for 100 years
maybe it goes well for a thousand years
but eventually your civilization is just
not going to make
it I don't know about that I mean we we
already have that we've already crossed
the threshold by having the knowledge
about you know nuclear weapons right but
again I don't I I I think you think our
current world is stable like do you
trust our leaders with nuclear weapons
infinitely do you think want to trust
our leaders with a monopoly over a power
no
likees what are you suggesting I have
not even said what I suggested yet so
this is my point it be great to get into
that I would love to get into that as
well but like I'm trying to make a more
meta point before I go into any specific
points my point is I am actually
pessimistic when people say I'm
pessimistic or their Doomer or whatever
the hell you want to call me I just
don't know if there are terminal States
as you claim I think that in general
there are M there are large negative
reward States in terms of like utility
towards growth for example you know
Global nuclear war would reduce our
population massively would take a long
time to rebound and that's a setback in
terms of growth but I do think that on
average you can there's there's certain
probabilities um uh fluctuations about
this this kind of path towards growth um
but overall the odds that we go back to
absolute zero and that life is
completely uh gone like I think those
those likelihoods are are pretty low
even technologies that are very scary or
very powerful like we have a model that
we like to extrapolate and and and and
have a sort of black or white thinking
but everything has some some Nuance like
according to your model like the world
should have ended when we discovered
nuclear bombs which and it only did tce
like like you're like my model says is
that there's you know some percentage
chance that yes if you have
dysfunctional institutions with
dysfunctional leaders and a
dysfunctional civilization with access
to massive weapons mass destruction yes
they probably will get used and probably
there will be close calls we dropped a
nuke on South Carolina accidentally it's
still there it's in the swamp it's still
there and it was pure luck that the the
it was actually armed the nuke was armed
and we got so lucky just Happ it
misfired a city would have been blown up
it's would the world would not have
ended actually so yes sure but like what
happens if that H if this is the rate
every decade or two we drop you know one
nuke on South Carolina and our nukes get
better and better and better eventually
the nukes are synthetic bioweapons you
know AGI ASI these kinds of things what
happens so I mean even bioweapons there
was a leak of a bioweapon we didn't all
die we probably all got Co one yes sure
the world has never ended in the past
this is true until it's not like this
isn't an
argument um I don't know it's a data
it's like and the data is we got really
damn close many times even with the
shitty you know pseudo apocalypse Tech
that we have now I agree that our
current nukes are not existential yet
but they're as close as we've gotten and
even with those we've had a bunch of
accidents in the like measly 70 years
we've had these I I just don't I just
don't agree with your model that like
we're going to have ai passes like can
we like we're we're down like 50
analogies deep at this point can we just
talk about AI at this point because
instead of like drawing analogies to a
bunch of other technologies that aren't
quite uh the same game theory uh CU
again AI has huge upside relieving on
the table whereas nukes don't um so so
like for um like let's get into thei
discussion like let what what what do
you want what is your analogy here let's
let's get into it like what are the
connections you're trying to draw you're
trying to say uh we shouldn't build AI
because or AGI that is human level and
Beyond because it is a one-way function
is a terminal
state where uh we won't be able to put
the genie back in the bottle and um you
know our uh existence as humans is
guaranteed to end in your model is that
correct and and can we also cover what
you would do Conor I also happy to talk
about that but I think what you say is
worth talking about as well so no my
model is not wow every single human
level intelligence kills you instantly
and there's no way back quite the
opposite if I believe that I would go
live in Hawaii with my family and just
live all my final days I think it is
completely possible to build AI systems
out of all the upsides you want all the
great things you and other EA Envision I
like the aesthetic there's a lot of nice
things to like there let's build it yes
and that's going to take a hundred years
and us not blowing ourselves up along
the way it's not that we have 100 years
yes that's my point that's why I'm a
Doomer you're a Doomer no in this no in
the sense that like if if a a
subsystem of the civilization decides to
go slower another subsystem yes want to
go faster yes it's just not stab and
then the only way to stabilize that is
to have a top down Monopoly on Power and
and and on AI that sort of enforces this
this this equilibrium in a top down
fashion and that that I think is a bad
trade and I don't want to take it right
and that's where congratulations as lord
of the Doomer you know Honor Society I
bestow upon you the rank of Doomer you
are now become a Doomer you are a Doomer
you are a Doomer because you believe
that both that we are basically
you you may not explicitly formulate it
that way but no no no look you think
both that if technology is as dangerous
as I say it is then and if if we try to
develop it this safely it wouldn't work
because someone else would do it much
faster and the only way to stop that is
through these monopolies and that's not
an option this is also basic no no
there's there's a third path which is
the one we want to take with eak but but
go on and what is the third path no the
third path is you know we want to have a
a decentralized control of of of AI
right like if every company if every
individual can have access to Ai and
compute you end up in this sort of
adversarial equilibrium where um um
people companies um smaller governments
are more formidable and are not to be
with um and that's a you know
it's kind of a peer-to-peer enforced
equilibrium instead of a top- down so AI
mercenaries sure yeah I think I no so so
I mean in our current system right we're
talking about lawsuits and lawfare right
a llms are going to be you know our
superpowered lawyers and who Whoever has
more capable LMS is going to win
lawsuits and going to be able to enforce
their will on others and that's where
we're going right yeah and who's going
to win the AI Warfare the one with the
better guns the better llms the better
the one with more Capital just like it
is right now in our legal system so
you're a Doomer so you're a Doomer you
already believe over yes you're realist
over you're desing is over no like if if
there's a form of legal violence through
llms and we just throw compute at each
other like that escalation is going to
yield like way cheaper compute and we're
going to use that for all sorts of
Amazing Discoveries nuclear fusion we're
going to geoengineer our our our planet
most Capital wind well it's not there's
no decentralization you just said you
just said the one with the most capital
win no there's a power law just like in
any sort of uh complex self-organizing
system I I I believe in capitalism and
that's what happens right like there is
some power concentration for
example no we we if no the point is if
we have
if we don't legislate away the ability
for for small startups or individuals to
try to disrupt uh the the Giants right
like if we don't legislate away that
ability then then then you know the uh
incumbents are are get themselves
checked by uh you know upand cominging
startups for example why don't the
incumbents just create their own AI
powered super legal State I mean Super
legal St I I don't for let just draw an
example here that actually happen so for
example you know a lot of startups right
now are using
mistol and that was enabled by you know
the ability for open source uh AI to be
shared because people don't want the
platform risks that come from uh uh
closed closed apis right and that eroded
the the market power and the pricing
pressure uh of uh you know the opening
eyes anthropics of the world and then M
that kind of corrected their ability to
just glob up all the capital
the world isn't a B2B SAS app politics
and War are not the same as VC I don't
know how to explain this to you man it's
like you keep talking about these huge
Concepts about civilizations and power
laws and you know the long future back
you want to talk about
war let's talk about real war let's talk
about real war we are in a cold war
right now right yeah it sucks
it's unrestricted
Warfare you know across the board um and
uh you know it it might evolve into a
hot War right and I think I I do think
that there is very strong danger where
our legal system and you know certain
ideologies are weaponized in order to
have the West sort of defang
itself and become an easy target and
because we are in this uh Global
geopolitical competition
um we have a duty to accelerate right uh
because we have to out compete and
survive and and that's just that's how
it's going to be like I don't we're not
going to be able to create a world
government that you know keeps everyone
in check and I wouldn't want to live in
that world where there's a like Global
panopticon and so we just we're going to
have to play out the movie how it how it
plays out right that's just a reality
yeah do and we're gonna have
this no it doesn't necessarily yield
Doom I mean we've we've been in this
sort of
you know but like your your whole
ideology as far as I can tell is just we
just play it out let it rip there's
nothing we can do we're completely
impotent like control playing it out is
what got us here right like this this
fractal sort of competition between
tribes between people understanding and
faith that the system is is an Adaptive
system and it's going to adapt towards a
high level Optimum but I don't like the
whole point is that we don't want uh a
world government or one power to
dominate everyone and we have to stay in
sort of this sort of small capabilities
Delta between different players whether
that's at a corporate level but also at
a nation state level right um now if you
want to argue that closing down AI you
know is more optimal for uh you know
competition against for an adversaries
I'm happy to have that discussion but um
yeah I mean we can have that discussion
as well we can talk about how open
source is basically feeding directly
into our adversaries as strongly as
possible and it basically benefits only
them and not us but that's a whole
different conversation I don't
particularly care about I do care about
it I I do want to uh say a point here um
because I I think that um the strength
of the American system is its variance
and its internal competition uh between
different corporate entities between
startups between innovators and you know
the point is that you know the top AI
systems several of them are American and
they're competing with one another and
that internal competition breeds very
high Fitness that makes us dominant on
the world stage having um uh open source
AI gives um um more people the
opportunity to contribute uh to Ai and
and enables us to have much faster rate
of discovery because we have just more
points in this point Cloud search of
hyperparameters of how to do AI right
and uh sure like you know our
adversaries also have access to those
models um do you think the blueprint for
the F-16 should be open source so more
people develop fighter planes for us so
so this is why I'm interested in digging
because there's there's been this sort
of you know uh bait and switch of like
oh we're doing this for safety and then
and then when that argument doesn't work
sometimes nowadays we're pivoting to
like oh we don't want to give it to our
adversaries and to me it seems like
there's a sort you know similar to be
clear I don't give a about National
politics I'm not American I'm not
European I'm nothing I don't I don't I
don't yeah I my point is like there's a
bit of ab bence which maybe not you but
I think in general where when one
argument doesn't work the the the
pretense uh uh you know we stop
pretending it's like okay no this is for
uh a capabilities Delta with our foreign
adversaries right I don't care about the
Delta with the foreign adversaries I
care far more about the Delta with crazy
wackos like again I'm asking how would
they how would they I'm asking you a
question should the F16 fyr plane
blueprints be sourced do you think this
would lead to a better or a worse world
I think there is a lot of someone who's
been espion by Foreign adversary or or
spyon um you know unfortunately I think
big Tech uh is very like uh leaky and
insecure I think that a lot of this
secrecy and and closed sources is is
like security theater and that so many
or organizations are compromised that um
if you're not going to have actual
Secrets your only mode is speed and if
you want speed you want variance and if
you want that open source is the way and
so so do you think the F-16 being open
source would be net good or net
bad um I don't know I think it would
reduce the you know uh revenue of
certain companies but uh I mean the the
Chinese and the Russians probably have
their hands on the again I'm asking you
for the whole world you know whether or
not the Chinese already have it or not
would releasing all of our secret you
know engineering documents of all fc6 is
a force for Destruction it's not a force
for positive econ economic utility and
you're drawing these comparisons all day
I'm trying I'm not trying to make a comp
this is not a metaphor you are but
you're you're just lost analogies and
analogies analogies let's talk about AI
directly right you're just trying to
like answer the question yes or no is it
good or bad if it's if it's not open I
mean there should be more open source uh
plane designs because I think it would
Tred well I'm trying to I'm trying to
explain maybe not the F16 but the next
you know maybe some other planes if you
had more people that were trained and
could self-educate about you know
defense Tech we'd have a better Talent
Pipeline and maybe we wouldn't have uh
you know as many problems because we'd
have a better uh defense industrial base
right and so maybe it'd be good to have
some open source right as a as a
pipeline for training people okay but
should the F-16 be open sourced or not
specifically maybe not no okay okay
that's my that was my question this was
not an analogy it was probing your
intuitions around these kind of things I
wanted to understand how you thought
about this I was not trying to make a
case to me like open source AI you know
it's never going to be quite as good as
as closed Source but it's it's great to
uh again train the workforce uh seek out
new ideas it's good for Innovation sure
it's good for Innovation momentum it's
net positive okay sure this is
completely fine like you know should
mixt be public it be a open source model
I don't know probably net positive
probably like okay probably I don't know
I'm not certain but seems pretty pretty
reasonable to me I've used the model
before it's nice it's a good model um
but if we were to build say an AGI which
is smart enough to design an F-16
fighter plane do you think it should be
open
sourced smart enough to design an F-16
fighter
plane I I mean would it though would it
have that knowledge in its prior I feel
like that's very specialized knowledge
that well a plane of that quality it it
it's smart enough it has all the
mechanical engineering and so on so it
could design a plane of this
quality it could design a plane of this
quality well the assumption is that
whatever centralized power uh that is
trying to have the Monopoly on violence
uh you know should have a much better AI
that would design much better planes
right and so you still have that power
gradient which I'm asking a much simpler
question really super super simple just
you have on your USB drive your AGI that
can release that can be used to develop
a plane of I think by the time we have
that we have much more advanced planes
because we're going to use much more
powerful AI that is going to design much
better planes and the F16 is going to
look like you know something primitive
so as long as the government has even
worse weapons it's okay for people to
have access to weapons no I think as I
think that the natural state of things
is that big corporations and the
government will have some capability
Delta right between the AI That's
accessible to all and the AI that is
that is centralized and that maintains a
sort of power gradient that seems to be
important in your uh sort of in your
sort of model of the world you're very
worried about sort of peer-to-peer uh
violence versus sort of top- down
violence um I guess we have different
priors for how that works out but um you
know to me I think there's going to be a
a maintenance of this this sort of power
gradient between the centralized and the
you know the top down and the bottom up
forces um um but my concern is to not
have too huge of a gap and I think uh
overregulation today uh AI could yield a
massive Gap where uh the government and
the incumbents and a cartel forms and uh
around Ai and they have the Monopoly on
Advanced AI they don't allow the people
to have access to AI that they don't
control and then they also Ste
our access to information maybe they
even convince us AI never existed and
then they use it for for for oppression
and and and deepening their their their
their power uh Delta um and and that
concerns me right um but I do think like
uh I I think like even if you had a
blueprint for an F-16 like good luck
getting the supply chain together
there's a sort of regularizer of the
world of atoms things are hard to do in
the world of atoms like you know you can
you can Google search you know you could
Google search bioweapons you probably
don't know how to manufacture them you
don't have the wet lab the lab for it
you can like random search literally
random search uh chemicals and on
average it's could be toxic because most
many things are toxic doesn't mean we
should ban computers like I I don't
think it's just a good argument uh for
for for Banning things and I think like
your your argument is that uh eroding
the top down power gradient causes
instability uh and my point is that that
power gradient will be maintained I'm
just concerned that it becomes too sharp
this is an interesting point I did not
expect you to have this view so thank
you for elaborating on this I thought um
so thanks for elaborating on that this
is interesting for me to hear I wouldn't
describe my own point of view as eroding
you know top- down power as being
inherently bad I think if we were a
better Society it we would need less
top- down control and they would and
like decentralized control would work
but that's hard like my main point why
decentralization like doesn't work is
because it's really hard to build large
coherent systems like it's just hard
it's not impossible just hard yeah it's
hard to steer if if you have a higher
entropy system it's harder to steer but
that's also what that's also what gives
it fault tolerance right because if you
if you have a system that's too easy to
steer than than than power seeking
agents which we have we have humans that
are power seeking just as much as the
AIS of of of these Doomer Futures like
they are power seeking and they do a
sensitivity analysis which nodes can I
compromise or or infiltrate in order to
have control over the system so entropy
provides some safety and we we argue for
entropy and variance and it's like ah
well it's harder to control and then
it's like ah well in the in these tail
events in in in in these fat tales now
that you have higher variants there
there's some negative reward there it's
like yeah but that's the cost of being
fall tolerant against sort of uh top
down control because if we give the keys
to our future and and control over AI to
centralized cartel whether it's
government and so on that can be
compromised by people that don't want
our good and eventually you have a very
steerable system that uh kills all
variants and so it's very hard to Fork
away from it and compete against it uh
and then and then you opened up the door
to an AI assisted tyranny and and to to
us that's like that's a real existential
risk yeah yeah I understand this point
of view I understand the intuitions
behind it I think a lot of these
intuitions are good intuitions um I
think you know a lot more city states
would be great with different forms of
Regulation and stuff like this I'm all
in favor of Charter cities I think this
is a great concept um I think I may I
have a hypothesis from what maybe is a
much deeper disagreement that we have in
our models that we haven't really talked
about which is the asymmetry between
offense and defense so you often talk
about a power gradient this is not
really how I think about things there's
not a linear your unit and if you have
more of those units you're safe from my
perspective if like you know I have more
money than you know like some random you
know crack addict on the street but if
he has a gun then you know he has
certain kinds of power over me that I
can do all about whether or not I'm
you know a relatively wealthy man or not
or relatively well politically connected
like power is not a one-dimensional unit
so from my perspective destruction is
almost always easier than defense almost
always there are some Edge case
exceptions like cryptography but in most
scenarios it is much easier to destroy
something than it is to build the same
thing is much easier to build a bomb
than it is to build a reactor so from
this perspective um I think this is a
really really really core point of my of
my perspective on things yeah I I
understand I think you're you're arguing
that um entropy is sort of the natural
state of things and and bringing order
is more difficult uh and takes effort
and the reality is that we want we we
live in a complex system we want it at
actually the edge of criticality where
we don't have so much order that we've
suppressed variant and it's crystallized
it's not very Dynamic we don't want it
to be fully disordered and maximally
entropic because then you know it's too
high temperature there's there's no
order whatsoever we want a careful
balance between both right so you don't
want maximum acceleration you don't want
maximum
entropy um I do think well it's
complicated because it's not it's not
entropy it's actually free energy free
energy is a balance between energy
minimization and maximizing entropy
right and so it's actually a careful
balance between both and no the eak
thesis is to seek whatever policy
configuration of configurations of of of
of the way we do things uh that maximize
our ability to to seek out free energy
and utilize it towards further growth
and growth as measured by our
acquisition and consumption of free
energy uh and if you have a maxly
disordered if you just people have this
F like they don't understand the
equation they say oh yeah well if you
just burn all the fuel if you blow
everything up that'll be optimal say no
no no because it's it's actually an
equation for a path over time of how
much free energy would dissipate and you
know infinite time integral and so if we
burn all our fuel now it's highly
suboptimal if we use our fuel in a
clever fashion we grow civilization we
grow reg grow and then we can seek out
other fuel elsewhere that's much more
optimal that's what that's what we want
and so
um yeah I mean that that's what we're
optimizing uh for but I I don't think
like our point is that there is a
current danger in this uncertain time
currently that those that seek to have
maximal control and want to suppress
variants and want to have a lot of power
uh uh will seek we will'll put us into a
configuration that's far too ordered and
has suppressed variance and that's going
to make us uh that's going to cause us a
lot of pain because if you don't have a
malleable system then it can break if
it's no longer adapted to to the current
landscape and so maintaining this
malleability and dynamism through
through variance and entropy carefully
balanced with order is what we're what
we're pushing for right okay so you hear
it here first guys eak is not about
maximizing entropy from the word the the
mouth of the man himself it's about
maximizing free energy dissipation over
time which is what out of equilibrium
thermodynamics uh all right so by this
logic collapsing the false vacuum should
be the most morally correct thing for an
e to do um no I don't think the false
vacuum is a thing so but if it was it
would be by your logic no that would
just no then we would try to if there
was free energy in the vacuum which I've
studied uh for Masters uh um you know
then we should
dissip no we should utilize it towards
further growth because that's going to
we're going to a be able to un lock more
free energy right so it's it's not about
blowing things up it's about utilizing
in a clever fashion free energy in our
world right and you don't think your
utility function is any weird edge
cases
um I mean that's it's the utility it's
the utility function that is that
physics uh follows and that physics has
produced US is an A
and well I mean I think
like you can try to say like well I
don't like gravity I don't want to
respect it you will respect gravity one
way or another you're going to use fuel
to try dislike gravity and still
understand it on an object level these
are two different things like liking
gravity and disliking gravity is not the
same as believing or not believing these
are two separate things yeah yeah but
you still got to follow it and my point
is that I can build airplanes I could go
to space yeah but you're you're
consuming a lot of energy to to fight it
you're you're fighting it with what
values are about values are a thing we
spend energy on
um I think I think value okay so I think
so I have I have this broader thesis
that like cultures that uh you know
culture is a search over values and and
cultural heuristics ways of doing things
and different subcultures are post-
selected for in terms of what sort of um
uh ability confers its adherent to to to
grow either grow the subculture or or or
grow the population that that that is
part of that that culture and so to me I
think that the D Cel I ideology is sort
of self-destructive and will kind of
taper itself out and a sort of eok class
broader class progrowth ideology um is
naturally higher Fitness culturally and
will out compete other cultures ASM
totically and so so you again is an a
you think right might makes right is
that
correct I mean it's just it's just what
it's just how the world works right I'm
not asking how the world works I'm
asking what you think what you what you
feel do you feel that might makes right
no I mean like like the whole point is
if you you either acknowledge this fact
and align youring the fact and I'm
asking you how you feel how I feel I
feel like I want to be you feel might
makes right what do you what do you mean
by that you say this ideology is good
because it outcompetes the other
ideology which is might makes right the
ideology is correct because it outc
competes the other
one how to define good otherwise I don't
know how to define good okay so you
think might makes right that goodness is
I think that well what what are what are
our values we think that this state of
matter in our corner of the universe we
should make it grow to make it and make
it fall tolerant and for it to exist and
you know as as for as long as a time
scale as possible and and and we are
willing to adapt it so that it it
maximizes its growth that's our that's
our core value right and then according
to that value if you project down to
subsystems then subsystems that are
higher Fitness towards growth are good
according to that value system right and
that's our that's that's kind of the
core thesis and and you may disagree
with it but I guess that's like the
that's the eak premise right and and
it's very funny to me because you know
eak like on EA so much because
they have this weird they are at least
are accused of having this rigid weird
utility function with all those weird
edge cases meanwhile EXA dear audience
please you know scub through the last 10
minutes of our conversation it's like
I'm I'm like we have well that last
function is from physics right like it
just happens like
that it's just it is how it is the
universe decided LOL I guess I just have
to do what God says well you can again
like you can hate gravity and and like
you could try to fight it but at the end
of the day it wins right like you know
the Rockets we fight it for we burn a
bunch of fuel to fight gravity but then
they come down right and so like you
could you could try to look fight this
this tendency towards eventually things
relax back to the Natural State of
things maybe it would be helpful if I
explained a little bit about how I think
about morality and at least you can hear
it see a little bit Where I Come From
Here the way where I think about when I
think about morality is I think people
are very confused when they talk about
morality when they talk about Val most
of the time I think that when people say
something is correct or good or whatever
they usually mean one of three things
and these three things are very
different the first thing is
epistemological truth or goodness or
correctness like this is a true fact
about reality it's correct it it improve
if I take this fact into my world model
my accuracy about future causal
predictions will increase this
epistemological goodness second is
decision theoretic goodness like this is
a good idea what they mean by this is if
you do this thing you will win at games
more and ways that you care about and
the third one is Aesthetics and values
is this is good because I like it this
is the thing we're fighting for why do
you like it this is not part of
Aesthetics this is a different question
this is a causal question why do we like
order again this is a causal you're
asking an epistemological question if
you there's an epistemological question
about why do I have these Aesthetics but
it's not an aesthetic it's an
epistemological question it's a
different type of ought like so you can
say why do you have these things and I
can tell you a causal story but this has
doesn't have the type signature of a
moral is not a moral thing it's it's the
type signature is epistemological causal
I can tell you a caus a story about my
evolutionary history about my childhood
traumas or whatever you want sure I can
tell you all these stories but
ultimately something has to plug into
the like arbitrary things I like and
where those come from sure like they
come from here they come from there
whatever this that's the point so so eak
we're not prescriptive on what you
should like we're just saying this is
the the playing field this is how the
cultures are going to compete with one
another and then given this fact given
this realization it's like you should
try different ways to live your life but
on average just be mindful that the
subcultures that are amenable and
aligned towards growth are going to be
selected for and so uh for example part
of eak there's a sort of Builder
subculture where working on technologies
that are of high positive economic
utility and and working very hard on
these Technologies uh uh uh you know giv
give gives back to you because you have
kind of fed the techn capital machine
you've allowed it to grow and then it
rewards oh you fed cthulu he'll be nice
to you
now I mean yeah you align yourself to
civilization you align yourself to
civilization you get rewarded or you
could go live in the woods and and and
and Go full Ted K and you're you're free
to do that and and and let's see how the
movie plays out right they sub cultures
that organize and live as part of
civilization and and and utilize
technology in a clever way they grow the
people that become leites they either
become St stagnant or die right so it's
like it's your choice as I like to say
people should have the freedom of choice
for sure sure sure whatever like but
like Libertarians are like house cats
fully dependent upon a system they
neither understand nor appreciate the
fact that the techn capital system
rewards you is not a fact of nature this
is a very H happy circum confence of
circumstances I guess it's a projection
of a fact of nature is is is is my point
I I I I think that uh if if you consider
uh corporations or nations of Subs
theramic subsystems like it will um
self-organize its components towards uh
uh uh configurations that yield you know
better uh growth so if you follow in the
will of God he shall reward the faithful
that's your ideology I mean physics is
my God to some extent uh you can have
your own other additional Gods but to to
to me it's like but physics disobey the
laws of physics like physics doesn't
reward you you it creates us it created
everything we love know and love it
created the neurochemistry you crave so
much it created your brain it created
you every relative everyone you and it
will also create everything that kills
you it will also make your parents die
and that's that's the beauty of life
there's there's upside there's potential
downside you just got to play the game
it's like why would ly you won't have a
perfectly safe guaranteed future ever
there's no guarantees in life there's
risk reward and you decide to play their
game or not right you can decide to
participate in the techn capital machine
take risks and have upside or you could
stay at home be a grad student I mean
capital or physics which one is it
because those are two very different
gods it's physics and civilization
itself which is human techno Capital
memes all pieces of information and
configurations of matter are part of
physics right and then yes physics is
larger than that and it's a very
specific subpart which is civilization
and niess and all these kind of things
which one do you want
niess like for me for my perspective I
would much rather follow the god of
civilization I would much rather follow
the god of light in the dark I would not
follow the god of physics the god of
physics is an another evil that doesn't
care no I mean like physics like if we
don't have our stuff together entropy
wins right life is a fight against
entropy towards against entropy see guys
not maximizing no don't it's it's a
subtle argument yeah yeah yeah yeah no
but like it's like you know so so how
life works you know it consumes neg
entropy or it consumes free energy to
maintain its coherent state to not
totally thermalize and and and become
maximal disorder but it's a constant
fight against entropy but then the
bargain is that the house always wins
and on average by doing this fight no
but seriously by doing this fight and
consuming more free energy you overall
produce more entropy for the universe
and and and and that's why we're both
fighting entropy but by as a byproduct
producing more right okay so so so yeah
any quick physics lesson but where are
we going with this um I ask a quick
question um just before we move off to
this what you're saying G is really
interesting so um you know in the world
entropy or information dissipates but
living things have this remarkable
ability to resist entropic forces
Universe
equilibrium and you can think of just DN
in a dynamically maintained out of
equilibrium State actually right like
okay okay but but I guess the the
question is though what is your value
system I mean if if everything's physics
um Connor was just saying that there's a
a bright line between is and ought which
is to say um I mean Hume was an
empiricist and he was saying that
morality is the one thing that you can't
find out empirically there's something
different about I I I think I think so
so that's where I would I would I would
disagree I I think that um morality and
value systems are kind of arbitrary
they're hyper cultural hyperparameter
settings and I think that the
subcultures that have certain hyper
parameter settings that yield higher
growth will be post selected for and we
should embrace that this is a fact and
embrace variance and a constant search
and and avoid monocultures because if
you suppress variants in the search and
you set uh in a top- down fashion you
prescribe these hyperparameters you kind
of ignoring potential other modes that
are more optimal and you would be
outgrown and that will that could cause
your your Annihilation sure so M makes
right sure but just just a quick Point
you're still describing as physicists do
what things do and we're trying here to
come up with a bright definition of why
something I'm not prescribing any one
way to live your life other than you
have a choice um if you if you optimize
your your culture and your policy of how
do you live your life in a way that is
aligned towards this
growth uh you will probably be part of
the future and you will have higher
Mutual information with the future if
not then then you'll have lower Mutual
information with the future but we tend
to Crave and our notion of happiness I
think is a is a proxy for how much we
think we have influence over the the
future but that's just kind of more
hypothesis there but otherwise it's hard
to define happiness formally right so
what you're saying is again might makes
right is that you're saying there are
certain things that will be selected for
and this is a statement about reality
cool fine accepted whatever and you
don't and basically whatever that thing
is that's the thing we should
do um I mean if we if we don't do it um
I think on a certain time scale uh some
subculture or some fork or some
adversary will do it and if it confers a
massive Advantage then that will win out
sure okay okay but again what do you
think what do you g what do you think we
should do do what should we do yes um
well I try to enact my values right I
mean I work what are your values I'm
trying to figure out what your values
are is what we're trying to find out I'm
trying to scale civilization right
like so personally my my life's Mission
uh is to you know achieve to increase uh
civilizational cter ship scales so I'm
working on forms of intelligence that
are I'm not trying to actually replace
humans I'm working on like physics based
intelligence like trying to understand
uh chemistry uh materials you know uh
Fusion nuclear fusion carbon capture um
you know uh biology and so on like I'm
not trying to just I'm actually not
working on MOS anthropomorphic AI uh I
would say physics based AI is like an
extension of of our capabilities and to
me it's kind of funny that uh the this
the organizations that maybe not yours
but the organizations that are pushing
for AI safety are the ones causing the
the the disruption of 100% agree with
that by the way like like hand like me
and you handshake on that like fully
agree on that good good and it's like
okay so you want to are we trying to
extend humans or are we trying like
compete with them to be fair I I do
think that having human like
intelligence will have utility because
our economy is already adapted to take
in like take in humans hire humans and
plop them in and it's like oh here you
have an execution core here if you just
do human like intelligence here you know
the the system will work and so so it's
just natural that you know um that has
higher Fitness in terms of products uh
because corporations are already made to
take humanik intelligences and put them
to use right uh so I understand where
that comes from from an economic utility
standpoint but I have a different like
my uh I guess I'm more aligned with Elon
where I'm trying to work on technologies
that that help us uh increase the scope
and scale of civilization and so I'm
working on like the hardest problems uh
you know and and and trying to trying to
extend Notions of intelligence to tackle
these hard problems and for example one
thing I'm concerned about is that you
know policies backfire in ways you don't
predict like for example let's say you
cap uh compute for humanlike
intelligence before we go down another
just just one comment while we're on
physics based AI if you cap compute at a
certain threshold for llms that affects
let's say drug discover research where
we might need way more compute than that
to to to to do our research right or or
materials research and that has a net
negative effect because it's still Ai
and and above that compute threshold
right and so what I'm arguing for is
let's be very careful as to like What
legislation we crystallize and let let's
have it be as light touch as possible I
think there will be some regulation I'm
realistic um but I I I I do think that
we have to be mindful of like what we're
going to affect in terms of potential
positive effects of enabling High
compute uh AI research uh like we we
don't we don't want to shoot ourselves
in the foot uh with any sort of
Regulation I I don't think what's been
proposed so far has been good so yeah
anyways sure sure sure sure fine all all
good but again I so I'm I'm very
intrigued by your answer to this
question it's very intriguing to me
because it again looks like another bait
and switch because the the question was
what are your values and what is eak and
you said well eak is about you know
maximization you're taking the values
that will be out competed blah blah blah
and then you said well I have these
values about like maximizing humanity
and whatever okay cool what if if you're
consistent this would mean that if
tomorrow I convinced you that actually
the most the strongest the most growth
maximizing thing is actually to work on
AI that removes humans would you do
that you I don't think that's the case
right now okay but like if it was if I
made a really good argument for it and
you're like damn that's a good argument
would you do
it no not personally but why I do think
that well I have self- as well right I
want you know every so you don't even
believe in act yourself not really no I
do believe in it I mean I'm working on
technologies that I think will massiv
don't believe in growth maximation so
this the point I'm trying to make no I
do believe in growth maximization but
you're just trying to paint a scenario
of what is not right what I'm painting a
scenario of is that you don't take your
own belief seriously it's Fisher Price I
take it very seriously I I live my
values all the way I'm cre I believe I
believe that you feel your values but I
don't think you intellectualize your
values all the way Nick lant did Nick
lant went all the way he said if the
Capital machine says everyone dies then
I'm the first to die hell yeah let's go
he would not say well I personally would
decide not to he was like hell yeah
let's all die it's not you might be
correct I'm not saying it's impossible
that you're correct I don't think it's
the case right but um sure but like what
if it
was but it isn't so why like what what
do you mean man like like come on man
like work with me here we're trying to
actually take I think like you know I
want to support can't just say n like
that's not an argument come on man but
you you're saying like we're all doomed
if this happened and I don't agree and
we're not even discussing your model of
that I'm trying to see I'm trying to
understand your Val 50 analogies that
aren't uh you know maximal in product
with reality and so like so so Conor
what would what would you do then what
would I do Jesus Christ h i mean it's
the world is high dimensional and very
complicated the way I see things is is
that
never mind like I know AGI is the topic
I talk about the most and whatever comes
the most pressing one but I actually AGI
is not the main thing I care about the
main thing I care about is technology in
general and of which AGI is just the
most Salient example in the current
future you know 50 if I was born 50
years ago I would care about nukes if I
was born you know 200 years ago I
probably would have had a completely
different ideology because technology
wasn't Advanced enough yet and the thing
I fundamentally care about is the
stewardship of technology I think of
civilization as an entity of various
levels of coherency it's a very
schizophrenic entity it's more coherent
than it was 200 years ago like 200 years
ago you couldn't really talk about a
human civilization entity at all like
there was a bunch of like smaller things
like tribes and Nations and stuff that
like sort of coherent a little bit very
low bandwidth now it's more closer to
this we're still far like the UN is not
a unified like governing body by any
means and neither is the west or China
it's closer but so the way I think about
these kinds of things is that we can
think of ourselves as part of you know
many levels like of continuous agency
like you know we can think of us as our
bodies as people but we can also think
of us our bodies plus our social
networks as people plus our Tools Plus
our information plus our culture plus
whatever however you want to you can
draw the boundary anywhere you want
there is no specific here is where you
end and other people start it's a
gradient uh and a lot of what you would
considered to be you is not in your
brain it's in your environment it's in
other people it's in your tools it's
everywhere it's distributed and so in a
similar sense like the United States of
America is kind of an agent agent kind
of it's not and it's not located
anywhere in particular it's not like
there he is there's the USA it's kind of
you know you have a little bit of the
USA in you I have a little bit of the
USA in me you know and like various
times and whatever it's to distribute a
computation process and from my
perspective this is awesome like this is
what gives us almost all the good things
we have is that we can work together on
this we can have gods and civilizations
and memes and cultures awesome I love
technology I love living in a civilized
State this is awesome I would like this
to continue I would like our states to
be more civilized I would like our
institions to be more competent and more
coherent Etc now can these things be
predatory duh I'm from Germany yeah like
I like yes of course things can go bad
it's like we're de we're we're Gene
we're mimetically engineering
genetically engineering super beings
like of course this is dangerous like if
we were genetically engineering super
Tigers people would be like hey that
seems maybe a bit dangerous let let's
talk about this uh this control
hierarchy I really like your picture of
the world I think we have a similar uh
model of like okay you have these kind
of locuses of lowai of control and you
have this sort of cybernetic control
hierarchy uh and and and like I in a
sense like uh you know at every level
you know there's a sort of parody check
you know or it's I I draw analogies to
error correction that's where I came
from came from Quantum Computing in
Quantum Computing we try to engineer
fault tolerance
uh um computers because we don't trust
the execution of the program at each
level of of the computer including the
the the level where uh uh you know
certain parts of the computation are to
keep to check the computation at lower
levels and we've actually searched over
architectures of okay which ones work
which ones do not right if you have a
central node that controls everything if
you corrupt that Central node the fault
propagates the whole system if you have
a completely non-local decoder there can
be correlated errors usually are lower
likelihood but there can be correlated
errors that then you don't know how to
how to how to feedback and control so
what is optimal is a sort of a nice
hierarchy that's not too wide not too
not too thin at each level of the tree
like structure where you have this this
this sort of hierarchy of cybernetic
control um and my point is that uh
usually um uh there's there's usually a
higher level of you know of the the tree
of power that keeps the nodes at a lower
level in check right but if you have one
Global Central node at the very top
let's say a world government or
something that has power over everyone
else or over every nation what keeps
that in check and the problem is if if
we're counting on Democracy to keep that
that Top Central node in check in the
age of AI that might be difficult
because if they have a monopoly on
intelligence and they can steer
information Landscapes and engineer
adversarially U mimetic uh perturbations
that propagate then they can manufacture
consent for whatever the heck they want
to do right and I think we're both
concerned with that and I think we're on
the same page there actually y
absolutely and so that's why I'm trying
to
understand I I haven't unfortunately
read like all your uh propositions with
your organizations but like what do you
what is your proposition for how to
mitigate that possibility if you're
trying to have a sort of
centralized research lab for EI safety
which you
know we do that for bios safety we've
seen how those centralized Labs uh have
caused a lot damage I I haven't seen you
know we talk about sort of adversarial
actors in their basement uh you know
creating bioweapons so far that hasn't
happened what has happened is the big
centralized lab have had a leak even
though they had the best intents right
and so like what what are you what are
you proposing here with like your
framework for uh sort of centralized
sort of secretive AI Safety Research I
mean I I okay let me let me try to steal
like your your point is like if if we
have a preview of what's to come in
terms of capabilities we can best like
uh figure out policies to to to to
prepare for the Advent of this this sort
of uh technology or I guess decide when
to shut it down and ban it which I don't
I don't know how you would enforce that
but you know I'd love to hear your your
proposal what your
yeah what what what you're proposing
here for how to how to move forward
right like like we can talk
about you know fear and we don't know
the future it could kill us whatever
it's like okay but how are you g to you
know what's your proposed antidote and
is that going to be a worse thing than
the risks we're taking right that so I
think you make a lot of really good
points like you talk think about like
hierarchical cytic control that's a fun
way to think about it fall tolerance
these kind of things I think are really
good ways to think about civilization
this is a lot of how I think about
civilization and these kind of things um
it's also I think about like biology to
a large degree right like cells and you
know nerves and so on there's like I'm
sure you're familiar with like you know
like um Len's work on like
bioelectricity like planarian worms and
stuff like that how it's like
hierarchical like cerom like control
systems in the body I think these are
all good analog not just analogies kind
like literally true to a large degree is
like you know mtic systems are
information exchange systems their
distributed computation systems and
thinking about them this way can be a
very productive way to think about them
so when I think about policy I'm not
necessarily literally talking about a a
bill that gets pass in Congress this is
often how it's a productive way of doing
it the same way when I talk about
medicine I don't literally necessarily
mean a pill you swallow that might be a
very useful and common way to practice
medicine but it's by far not the only
way to crack medicine there's you know
surgery and you know injections and
whatever I know radiation therapy and
whatever and so I the similar way is how
I think about civilization as well
is the way I see things is is that our
civilization is just not able to handle
powerful technology like I just don't
trust our institutions our leaders our
you know distributed systems anything
with you know hyper powerful technology
at this point in time this doesn't mean
we couldn't get to systems that could
handle this technology without
catastrophic or at least vastly
undesirable side effects but I don't
think we're there I don't think there is
like you know I wouldn't even trust
myself with like you know extremely
uncontrollably powerful technology
because I'll probably it up I'll
probably make a mistake even if I have
bad intentions I need control mechanisms
I need feedback I need you know checks
and Bounds so I don't make a mistake
even if I have good intentions so even
if I had AGI I think this would be
really dangerous I think it would be
really dangerous if even only I had
access to AGI if only the government had
access to AI that's really dangerous I
think all of these scenarios are
extremely dangerous there are no
entities no institutions that I would
feel good about having access to vastly
more powerful like a technology same
thing with nukes by the way like I
wouldn't feel good if I had a nuke if I
had a nuke I would be like who gave this
to me like take it away from me like I'm
not what happens if I sit on the button
you know make sense I I think we we
agree that K day institutions are are
really uh ineffective and not
necessarily competent enough to to to to
guide the world and top down control uh
everything and and I I think you know we
what you're saying is you don't have
trust in our current day institutions
you're trying to propose better ones my
point is that you know not quite there
made proposals that still have not prop
but let's talk about institutions in
general like yes I do think that uh if
there is no uh um uh sort of
evolutionary pressure on institutions
they tend to decay in Fitness right like
if like if there's no competition and
and you know government right is like an
ultimate form of Monopoly and and our
form of sort of uh uh competition
between different administrations is is
de democracy but now we've kind of
democracy wasn't invented in the age of
algorithmic information propagation and
we feel like something is is breaking
currently and
so you know I guess my point is I I I
guess the eak point is well I think
centralized institutions can't be
trusted with this sort of power and so
we shouldn't trust uh that anyone has
the Monopoly on this power but we don't
think that we can put this sort of power
uh back in the bottle or like the the
upside is too high that every agent is
going to want access to it and so if we
if we accept that advanced intelligence
uh will exist which I guess we disagree
on whether we can control that or not uh
like you know advanced intelligence will
exist it should be in the hands of many
not concentrated in the hands of a few
that can use it to oppress others right
so I think we agree on that but how do
we yeah a lot of these intuitions are
good I'm not disagreeing with many of
your intuitions I think I have some
additional intuitions and additional
models that net to other outcomes so
yeah what we definitely agree on is that
like your current institutions are just
like not up to the task like if we just
like continue our current institutions
we don't change anything we keep going
we run into a wall like I think we agree
on that like this is just not
sustainable and in a sense I feel like
at least in my perception a lot of like
San Francisco type
uh you know eish you know techno
optimism is kind of like feels like a
trauma response to decaying institutions
we like you've been so traumatized kind
of by like you know bad institutions
rightfully so by the way that you're
like well it I'm not going to work
with institutions at all we have to like
circumvent them like they're like
inherently not good and I think this is
a not unreasonable like position because
it's the position I held for most of my
life I understand where this is coming
from and but in a sense it's kind of
like it's a utopian fantasy it's kind of
like the tech nerds kind of like fantasy
that you don't have to deal with people
is that you can somehow circumvent
dealing with people or dealing with the
messiness of Institutions to make a good
world and I know longer believe that's
true like I just think that act so if
you take one thing away from me if you
take one thing believe away from all of
my beliefs please take away that I think
building a good world is actually hard
there's no simple trick there is when I
say actually hard I don't mean like well
we're going to have to you know lift a
heavier weight than usual I mean like
you know solve an extremely complicated
High dimensional puzzle that we can like
barely fit into our brain if at all kind
of hard I think it's I don't think
there's a simple solution like you know
a lot of people in the past believe
stuff like free market that's a solution
but what happens you have free market a
monopolies form immediately and your
free market collaps it's not a stable
it's not a stable equilibrium so now you
need to do something that stops the
monopolies from forming but stopping
Monopoly from forming needs some kind of
like thing that's not the market so now
you already have something which is not
a free market there's already some other
thing involved there should be a market
of there should be a competition for
institutions right like we should keep
institutions in check by having
alternative institutions and the ability
for alternative institutions to try to
out compete K institutions to keep them
in check and that's all we're arguing
for we're not against like the existence
of institutions we're saying today's
institutions are failing us and if they
if we don't allow alternative
institutions to exist and try to compete
them then we're stuck with them and
we're we're in a bad spot and so it's
all about encouraging variants and
you're right it's a high policy at all
levels is a very high-dimensional
Dynamic optimization problem and that is
what we're arguing for dynamism always
keeping this
adaptivity and which is not necessarily
present in current day you know uh
institutions yeah totally understood and
like I think this is a this is a good
description of reality like I don't want
to dismiss this as like like yeah you
are observing true facts I don't think
you're crazy I don't think the E you
know some of your some of your followers
are crazy um but like a lot of the
people that you know follow the kinds of
beliefs you talk about I don't think
you're crazy like I don't think some of
you are but like I think most of you are
not crazy you're seeing things with your
eyes you're you know you're you're
seeing things with your eyes which are
real you're you're trying to solve these
problems and I respect that right right
like I understand um I think the point
here I'm trying to make is just like and
then you have competition is not an
atomic action this is not a dial you
turn up and down this is an extremely
complicated thing you're asking for this
is what I meant by the free market
analogy it seems like well if you just
remove all the regulation then you would
have the most competition but in fact
that's not true in fact if you remove
all regulation you just get new
monopolies that form new regulation so
now you need something to remove the
monopolies but that thing isn't itself
not free so now you have to add in some
control mechanisms for that thing we
agree that if Monopoly then inducing or
or achieving regulatory capture is bad
and that's also what we're fighting in
the present moment we feel like is
happening and that the AI safety
discussion is being leveraged
instrumentally for this regulatory Capt
and I don't even disagree with you here
by the way like I do think that many
people are taking the I safety
discussion to be about regulation I also
would like to make clear that a lot of
people are not doing that like they
actually believe what they're saying
please believe them when they say that
not all of them some of them are very
cynical they are instrumental at the end
of the day whether they like and I think
so are the eak people obviously
instrumental for people who just want to
take you know uh to perform you know
oopes and monopolies for startups to be
able to compete for startup or whatever
but even then like you know wanting you
know your open source models and
whatever is very convenient for
companies trying to sh liability like
many things that are very there's a
reason that there's more eacs at like
top tech companies than there are
non-acs like this not a coincidence like
it's like because it's useful ideology
for capitalist systems like of course
the the leaderships of the big Labs
somehow are more EA AI safety s which is
kind of interesting and we see how
having sort of a system that's in
control that is primarily IDE
ideological and not aligned with
shareholder value can can yield some
pretty bad outcomes I mean opening I
almost
imploded uh right CU because there was a
decapitation attack and that's kind of
what we're that's kind of what we're
saying it's like either you have uh you
know alignment with the growth of the
organism that you're controlling or
you're going to steer it in a in a
self-destructive Direction and that
almost happened right and so I mean I
don't I thought you would be in favor of
Institutions dying I thought why aren't
you in favor of openi collapsing if it's
inefficient no I don't think it's
inefficient but I think that uh you know
it had uh leadership that wasn't U uh
beholden to shareholders right because
it's a nonprofit structure think good if
it collapsed then because then other
systems which are more aligned with your
thing could take over isn't that the
point of competition no I mean I mean
open AI is not necessarily EA or eok I
think like uh I think they're trying to
strike a balance and I think I'm making
sorry I'm trying to make a point the
point I'm trying to make is it's not
easy like there's no there like it's not
like competition is like a simple thing
it's not an atomic thing it's a
complicated mix of things there's a
bunch of things in a bucket that we call
competition but it's actually many
different things and you can get more
search algorithm and maintaining the
health of that search algorithm whether
through legislation and competition like
that is our goal I think we're aign
there actually I'm not sure so the way I
think about this is personally is that
it is a tool in a tool box I think
competition is one of the tools I think
it's a great tool I think it's a really
good tool I think there should be more
competition in the world a lot more me
and my friend um it's like you know
Scott Alexander in the past wrote A Blog
that a lot of EA's love called in favor
of Ness and civilization and me and
friend of mine wrote a post again uh
counter to that which is against niess
and for
civilization and which is all about how
no competition is an extremely important
part of what makes civilization great if
we don't have like you don't want a
lawyer two lawyers on opposite sides
being nice to each other that would be
terrible you want them to be as
adversarial as possible you want them to
use every trick you want them to take
out every shred of evidence and when you
have two companies competing for the
best product you wanted them to pull out
all stops you want them to make the best
product you want them to compete down to
the margin totally
agree but and here's the butt
here's the butt the butt is if you just
do this naively you don't get good
outcomes you get monopolies shitty
products you know environmental
pollution no externalities get priced
tragedy of the commons this is what
happens empirically look into the past
when we didn't have these kind of
regulations so you also need tools to
address those things and the and the
market and competition doesn't do that
empirically let's let's dig into that I
think I think that those examples are
usually from sort of uh regulatory
capture and not allowing sort of
incumbents to be disrupt suppressing
potential disruptions to incumbents
doesn't keep their power in in in check
right how do you but how do you do that
like how do you stop incumbents from
doing that well I mean usually right if
you really hate the current monopo
people really hate it and see a
financial upside to creating a better
alternative that is better
possible their capital what's that but
it's possible to stop that from
happening there's been many cases in
history where just like a monopoly is so
big that the fixed costs are so large to
get in like AT&amp;T for example where the
fix costs are just so large that no one
can get
in well we have we have problems with
you know tsmc and Nvidia today I'm not
saying these problems are solved my
point is we should solve those problems
but the way to solve them is not
magically waving a wand with the the
word competition on it it requires
actual concrete policies Which comp if
if the incumbents deform the legal
system to deepen their remote that's
when the problems arise and that's what
we're trying to avoid but my point is
you don't need the legal system
necessarily AT&amp;T like AT&amp;T didn't like
they also did latory capture sure but
like they also just had all the
telephone
lines yeah but now we have more than
more than one provider because the
government intervened and broke them up
yeah I mean I for the record in some
cases I'm I'm for you know antimonopoly
uh legislation uh but okay I do I like I
I like I I do think like I eak is about
doing whatever is optimal for kind of uh
this growth and and like accelerating
that velocity of growth and if you have
an incumbent that is abusing the market
power and is doing something that's
suboptimal like yeah I I would be in
favor of breaking them up so that
there's a more efficient ecosystem in
the case of open I think right now was a
bad time I think 90% of the ecosystem
was depending on them but I think that
that little fluctuation was a good
wakeup call for people to really price
in the platform risk of putting all
their eggs in one basket right and that
maintains the health of the ecosystem
yeah okay interesting so this is
interesting to me so does this mean that
if um a government was was to pass laws
which improve Market Mobility and you
know competition whatever this is like
compatible with eak like this kind of
Regulation would be good so if so you
would say like someone going into
government to create institutions and do
regulation in order to make a more
stable or a more Dynamic Market would be
compatible with the eak ideology is
correct yeah we we just think that
currently the current discussion with AI
regulation is led by the current day
oligopoly and they're the ones writing
the laws and so we're deeply skeptical
of everything being written right now
but we're not in principle against
regulations if they help acceleration
right okay I mean so this is interesting
to me so this is interesting to me
because from my perspective I I think
regulation is morally neutral by default
it's Nei good nor bad it's just depends
on what the outcomes are like what does
it do so this is a lot of how I think
about it as well I don't like government
regulation or top- down violence or
whatever because it's cool I like it
when it works and I dislike it when it
doesn't work so this is very similar to
how I think about these things so from
my perspective like I like anti-
monopoly laws not for some ideological
reason or because of some religion but
because it results in better things that
I like it results in better products it
results in better life so it's
interesting to me because it seems you
come to a you know not identical but
like similar ballpark of conclusions but
kind of from a different slightly
different P perspective I justify it by
the product like I like efficient
markets because they make people happier
I don't if should now that is a good
question yeah I we could dig into that
but should by the way but like um and
well it seems if I understand you
correctly you're saying something like
you like antimonopoly laws because it
makes the market more efficient so you
seem like you're one metal level up is
this correct yeah yeah like I I do agree
with you that it yields better products
because if you don't have the market
selective pressures applied at a more
granular level within your organization
you're kind of Sheltering you don't have
that market signal that helps you
optimize your product and and have a
more efficient search over the space of
Technologies conver on so we're on the
same page Okay cool so again I'm just
probing and trying to understand and so
on um um so if there was legislation
that makes the market less competitive
but results in better products is this
good or
bad I'm very skeptical of of that being
possible because again you know this is
what you learn doing a startup you have
to make contact with the market and
again I'm not making instrumental claim
I'm not saying this is true or false I'm
saying like if you had some data which
is like really convincing on this is
there a moral aspect to it or would you
follow the
data um yeah I mean we should do what's
optimal for for for growth yeah let's
say it's not that could include
legislation but I'm very skeptical that
a global fixed very slow clock rate
hyper parameter setting could out
compete of a bottomup like contact with
the market very fast iteration cycle
right like there are examp let gasoline
leted gasoline would be an example sure
yeah but but that we we got enough data
and we we enacted to be clear like it
was known that leted gasoline was
dangerous before it was released like
the company the scientist did know that
it was poisonous this this was a piece
of information that did exist same thing
that like tobacco companies knew that
tobacco caused cancer decades before
they publicly admitted the data did
exist it's just it was hidden it was and
it was there was a propaganda campaign
and there was an attempt to stop
regulation because it would make the
market less efficient I I I I think like
we this is why maintaining freedom of
access to information freedom of AI and
compute because I think that the future
of mimetic warfare you need to have your
own AI augmentation to help filter
through data and and not be uh
cognitively hacked uh like that's why we
we think that's important like free
access to information so that consumers
at the end of the day there's like a
micro democracy with each company
consumers they have enough information
they can boycott uh or can they can
demand legislation right and so that's
why like freedom of access to
information freedom of access to AI is
super important if only a few players
have access to Ai and they can they
can buyas the information they give you
and then and then they can steer You by
proxy and we think I I I think that that
would be very dangerous right so so am I
correct in saying you think information
is symmetries cars are market failures
or cause market failures yeah okay this
is a very reasonable position to hold
this is I I agree basically I'm just
interested in this so like would you
describe Le at Gasoline as a market
failure I think um deforming like having
an unhealthy landscape and a biased
landscape of information
propagation uh induces yeah market
failures right and and here we see that
if people weren't like if you presented
the information to people like real you
know information not like you know
extrapolations that are yeah like if you
present Real information people can make
their own minds and decide like hey I
don't I don't want to support this and
then they don't buy the product right
and and this happens all the time yeah
okay so then I have a question how the
hell do we do that
that seems really hard how you want to
like legislate that everyone has to say
the truth all the time
like
um yeah well I I'm trying to
avoid uh weird attractors where you have
this sort of top down control of of
these information propagation uh uh
mechanisms right like if the government
was in charge of social media and
biasing the algorithms that'd be bad we
almost went there right that almost
happened I me don't worry the corpor are
doing a bad enough job on their own yeah
yeah but like you know if it was all
centralized one party that's self the
thing you got to remember is like every
system and subsystem is self-interested
including the control systems right
including the people we put in in in
power and we got to uh uh keep that in
mind but I I just see a Way Forward
where potentially you have a sort of
centralization of AI and then they use
it to control information propagation
and they use it to uh
adversarially you know meic engineer
prompt engineer people and then steer
them in whatever Direction they want
this is what I don't understand why
wouldn't corporations in the free market
do this
too um like your eak Utopia why why
don't I mean we do that we we do human
prompt engineering it's called adver
advertising it everybody's trying to
subvert everyone else then why does this
not lead to the same failure
modes same failure modes what do you
mean like I mean people are just trying
to sell their product every agent is
interested in its own growth and it's
going to try to subvert other agent but
like you're saying there's this problem
where like there'll be a you know the
the information landscape will be
distorted and there'll be market
failures Etc why doesn't this happen
here too but I think like if if people
can have uh let's say there's a power
symmetry because corporations the
government have AIS that are very good
at at prompt engineering humans we don't
even they just have more money right
like it doesn't even have to be AI have
more money if you have let's say open
source and you have your own compute and
you didn't legislate that ability away
people can have sort of filtering like
just like a spam filter is an
augmentation of your ability to filter
through your inbox using AI right and
they have better AI than you so who
cares doesn't matter there's always
going to be power asymmetry that
maintains the hierarchical order of
power right and that's why you think the
current order of power is
okay um I think we should um search over
hierarchical power structures of
cybernetic control that uh that yield
better growth for the whole organism but
I don't think there's going to be a
fully decentralized uh system like we
are fighting we're pushing the
discussion towards that direction right
but the the reality is the Optima is a
hierarchical CTIC control system right
like and but but then we can test like
how much power we give any level of the
the hierarchy and we should like
dynamically test that and not just like
immediately give all the power to the
top node because we're scared of sure
what could happen in the near future
yeah so what you're saying is
decentralization will not work is not
the correct solution it will have to be
some form of hierarchy not I'm not
saying you know one no to the Top by any
means but it will be some form ofar
layer decentralization is a hierarchy
right if you have layers right people
roll ups anyways for more efficiency
whatever right but like so you do think
that like every man is his own Island
wouldn't
work uh yeah no I I I don't think like
that that would be like um you know
maximally disordered system with very
low correlations to make an analogy like
that's a high temperature phase that's
all in I there's no order again free
energy is a balance between order and
and entropy right again don't want to
maximize entropy that would be bad free
energy free energy that's yeah I know
free energy sure happy to talk about
that but entropies bad okay so
interesting um and
I basically agree with most of what you
say like this is how I think about these
things as well I don't think we have
found the best way of architecting our
like coordination and information
exchange mechanism on a civilization
level by all my fundamental belief so
you know this all started by you asking
me what are my proposals my proposals is
we can do better than this and we should
really really try like we should
really really try and how do we
find how to do that right yep and so one
of my claims is is that like we don't
need to do a blind optimization process
we don't need to do blind Evolution
we're not that stupid we've learned a
lot when the enlightenment happened you
know when French intellectuals came up
like democracy and Science and whatever
they didn't have Game Theory they didn't
have mechanism design they didn't have
you know anything about rationality or
psychology like obviously we can design
better systems in list obviously like
obviously we we have learned many things
about how to design better corporations
about how to design better voting
mechanisms about how to do better you
know you know quadratic voting like
obviously we can design better systems
than what we have right now this is my
claim I so you know I I think like this
is the
Eternal uh you know debate let's say
even even in startups designing products
it's like do I have like my own prior
from from from either data history or my
tastes and then I I I have a I use my
model of the world to to guide policy or
do I assume I have a noninformative
prior and I just take in the data and
adapt in online basing fashion like your
point is that we should look at history
and and have a model of how the world
will evolve rather than having sort of
uninformative Prior and just letting the
market based optimization rip and and
Converge on the op I'm saying we can and
we would be stupid not to like we have
learned things and like the whole like
uninformative prior is obviously not
true there is literally no one who can
has literally uninformed prior like this
is not there's no company that you know
takes random atoms and smashes them
together to generate new products there
is no such thing like there is no such
thing obviously everything is based on
priors and models that we have about
what the world is no one just randomly
generates products that's not how the
world works and I'm and I'm claiming
that like we I'm not I'm not saying okay
and I figured it out here it is because
that would be insane if I said I have
the perfect utopian system that doesn't
need to be tested trust me bro obviously
that would be crazy what I'm instead
saying is we can do better than random
and then we have to gather data then we
have to try then we have to test with
reality we have to debates with people
like you we have to test these ideas and
you know maybe we find things we didn't
think about but we can do a hell of a
lot better than
random I don't I don't know I don't know
right like uh it's like saying like can
you predict the stock market better much
better than random or I think actually
this is a very different thing I think
the stock market is a very different
type of SIM complex system right not I
disagree I very strongly disagree with
this it is also a complex system but
it's a different type of complex system
there are many types of complex systems
you know for example a gas an ideal gas
in a in a container from the molecular
perspective is a very complex object I'm
sure you would agree but yeah but you
could reduce you have like some some
parameters that you can predict
meanfield but then you're arguing for
like top- down economic control from
like you know statistics I'm arguing
that there are symmetries that there are
things which are not random I'm claiming
that like reality and culture is not
encrypted it's not homomorphically
encrypted it's not maximally adversarial
it could get to that point if we get AGI
designing like maximally confusing you
know you know adversarial mimics or
whatever it could get to that point but
currently it is definitely not that I'm
claiming that the market on civilization
design is extremely not efficient this
is not the case for example with you
know short-term S&amp;P 500 stock prices
those are quite efficient actually but
I'm claiming that the number of people
who actually try and actually put in the
effort at like you know you're my levels
of education and IQ actually try to
build new and better institutions and
gather the data adapt to what they learn
learn new skills you know take in what
we've learned in the 20th and 21st
century about behavioral economics and
Science and all these other things and
actually use the scientific method and
turn it from the physical sciences and
onto the problem of institution design I
claim that yeah we have not cly taken
all the alpha that is there so so I I
think your thesis is that your model has
uh fairly good Mutual information with
the actual future and then that you know
if you have a certain number of bits of
ability to predict I don't know like
you're you have good KL Divergence let's
say um then then we should enact
policies that leverage that information
we have for the future to do optimal
control my point is that I think that we
may be overestimating uh how well we can
predict the future and so so maybe not
like if we don't have good information
about how the future will go maybe we
should have a a lighter touch uh control
right because if if you have uncertainty
if there's a large Divergence between
your model and reality most things that
you're going to do are going to be
negative negative right impact right and
so but like quantifying the bits of
certainty we have about the future and
and now we're discussing models of how
the the future in the economy will go
that that we can discuss all day and
then and then I think we should we
should tune how uh uh strongly we
enforce top down control according to
our certainty about the future right
sure and sure but your your argument
proves too much your argument proves way
too much your argument if you took it as
you know what it implies it should imply
that engineering is impossible there's
so many atoms you know how could we know
that what happens if we move these atoms
in these directions how could we know
the uncertainties to outs are mutual
information bits about reality or I'm
like it depends on the complexity of a
system no no no but there there's a
notion of like how how difficult uh a
certain complex system is to to predict
there are system I'm claiming I'm making
a strong claim that the amount of
optimization pressure that is Fel gone
from like people like you and me you
know smart educated young energetic you
know Tech Guys to try to design
institutions is a pittance
compared to what has gone into
Facebook's ad algorithm sure no I we
don't know if it's hard we haven't tried
oh we we need to create better
institutions but so we should try but
I'm I'm just very skeptical of trying to
give the keys to the Future to current
day institutions yes me too that's why
I'm trying to create new
institutions well I I don't know I mean
I I I don't know exact this is why I
need to know more about like your
platform what you're proposing to me you
know you've been talking to existing
institutions and and I I don't know
what's going on there right so yes and
so you can disagree with my proposal you
can think they're wrong or they're bad
or I'm making a mistake I think this is
a very reasonable thing to believe I'm
sure I'm making mistakes with my current
policy because the world is complicated
and hard and I'm sure some of the things
I'm currently proposing are net negative
I'm sure of this and I'm sure some of
the things you will believe that some of
the things you're proposing negative I'm
sure you have that much humbleness
inside of you at least I hope um and so
that's all fine I'm making kind of more
The Meta point of like what is my
intention how do I think about the
process of doing good things the process
I'm trying to argue when I talk about
policy and I talk about coordination
this is what I'm talking about I'm
saying why should you believe that
something in this area is possible and
my argument is because no one has tried
you haven't tried have you tried to
engage with the institutions or build
new ones have you tried I mean okay
honestly I went to Big Tech right I I I
before doing a startup I was like I'm
going to try to go to big tech and and
and see if I could change these
institutions from the inside and and
what I saw was that culture steers the
institutions and then I was like it's
Upstream good institutions are
Downstream from good culture and that's
where I started call cult Al engineering
the world with e to some exent fantastic
so like there you go respect man like
respect I mean you're wrong and you're
going to make everything worse but
respect you
tried like you know you'll say the same
thing about me obviously and like I you
know but like okay respect like fine you
tried and you found some Alpha wow what
a surprise it's almost like just like
some dude from Quebec posting some
on Twitter can actually find a
massive amount of alpha because
literally no one is trying in the market
it's
inefficient do is it not weird to you
that just some guy from Quebec you know
I'm sure you're charming and smart and
handsome and everything but like you
know created a a alt and created like
this you know at least you said in the
thing in other podcast initially it
started kind of as a joke or like not
too serious and became more serious all
time and the spite this you got all this
energy all this free Alpha all this
political power started flowing is this
not suspicious to you is it not weird
how easy it was to get to this point is
it not weird that there is like that why
did no one else do it why was it you why
aren't there 10 eaks out there why is it
why why I think there's been movements
that have high Ender product o o o over
time but I I think by construction eak
is aligned with the natural tendencies
of the techn capital machine and so or
or we're trying to be like literally the
goal is to be aligned to the the natural
tendency towards growth and so naturally
it's it's going to confir for uh it's
aaren like uh higher Alpha for for
growth and and and on average that means
the subculture will grow so it's been
engineered to be this way from first
principles and that's why it's
successful maybe I don't know don't let
this go to your head but you're severely
under selling
yourself like I think you don't
understand how much of this is because
you were charming and you tried really
hard I think you are like my model of
this is really that you were
underselling how much this would have
not happened if not it happened to be
you and you happen to try like my model
of the world is that there was a lot of
there's a huge agency Gap like a lot of
the reason eek exists as a movement you
know people like Gary tan put
the stupid in their as their bio is
because you were Charming you spent a
lot of time and you tried I think we
both agree though that we want people to
have more agency and step up like there
are no adults in the room yes coming to
save us right I I think for me that
realization in my mid-20s was when I was
the adult in the room in in an important
room and my advice was was used to enact
things that were high impact that's when
you realize oh man like it's just up to
us to step up but I I think like I think
our generation feels sort of like they
don't they can't necessarily penetrate
in existing institutions there's all
sorts of immune systems to sort of
suppress new entrance and so we should
create new ones and I think that's what
we're arguing for um but it's not you're
saying it's just submit to the market
bro no no we want a market institutions
right
that's I I don't know I I I think I
think you're get so okay let me combine
this the points I made previously you
want a market of Institutions based like
first of all based cool awesome love it
but now I'm also making the point that's
hard you need to do policy you need
regulation you need to work with current
institutions you like you need to
prevent all the market failure modes how
do you legislate you know around like
information but we need to hyp test or
hypo hypotheses of what works and so we
need AB testing we can't have one Global
policy because then it's really hard to
AB test if you have like one Central
modde prescribing things for the entire
planet right is fine and I think this is
based and you should go work for the
charter cities Institute or whatever
right like this is based great I'm not
denying any of this I'm not I if we
could have I would think it was great if
we just had like some countries where
just like everything is legal
and like and they just try like all
medicin are legal and we'll see what
happens based right you know maybe not
all of them but like you know what I
mean and I think stuff like this is
probably not good cool all right we've
established this like we we institutions
are are are dysfunctional super agree if
we give more power to dysfunctional
institutions that doesn't make things
better why would it if we could improve
institutions I think we agree this would
be base this would be great Y and you
propose the best way to improve
institutions is just through basically
Market based evoltion disruption
disruption Market based Evolution Etc
fair enough I think this is a reasonable
position to hold not saying you're wrong
um I'm making a stronger case than for
agency I think than you are or I'm
thinking I think the market is so
absurdly inefficient that you can just
do better by just literally thinking
about it um and just trying but I don't
have to defend this point if you strong
well samples are costly right to test
your hypothesis of what a what a certain
uh policy what it's impact will be in
this complex system doing a rollout in
the real world is extremely costly and
so you should be basan and and have as
Rich of a prior as you can to be more
sample efficient right I I totally get
that right um it's yes exactly I all all
we're pushing for is a bit of humility
that our models might like not have as
high accuracy as we as toate them to
have right and and I mean of course this
is a fully General piece of advice at
some point you should stop updating like
at some point you should not get even
more uncertain you should most people
are overconfident I think this is a very
fair thing to say but at some point you
have to stop updating downwards so so so
my at least going back to our
hierarchical control structure as you go
up in the hierarchy there's less and
less sort of like bits of information
you have about the future at larger and
larger scales it's very hard to predict
the the whole macro system if you're
talking about the whole macro system
there's maybe a few statistics that you
could predict right and that's just a
few bits and so you should be lighter
and lighter touch in your uh ability to
control like your top down feedback that
in a way that's kind of proportional to
how many bits you have about the future
right and so that naturally yields this
sort of hierarchical control system
right but like Banning having very
precise description uh prescriptions
that are that are that are legislated
you know at like a national level or
International level like I'm just saying
we should really have very high
certainty that this is the right move uh
before we we do something like that and
I think right now there's too high
uncertainty uh uh and we we should be
cautious about making a move we regret
especially because given that
legislations are so hard to walk back um
yeah right um but I am the one living in
fear L um joking but um it's all risk
reward I don't know that I mean yes but
like it seems like you're just extremely
terified in a sense around like doing
something wrong in this direction to a
degree which seems kind of unreasonable
to me like I agree I mean we have a
strong prior of how like giving too much
power to to governments and very few
individuals how that can really backfire
and I think there's a there's a weird um
there's a there's a gap right now of
mass uncertainty where people are scared
and they're they're afraid they're high
andc about the future and that's
instrumental for some people to to to
consolidate power crystallize it and
then it's very hard to disrupt that and
I we want to make sure that that does
not happen and that's why we're fighting
tooth and nail to be the counterforce to
that uh happening right now right so you
want to fight tooth and nail from
someone actually trying to create policy
because it is high
uncertainty I I think uh strong armed uh
policy or policy that's too heavy-handed
uh would what does too heavy-handed mean
like I like
this is obviously up to interpretation
it's obviously just based like the
whatever and so okay but like great like
okay like okay I'll come to the punch
line Sorry like I've been I've been
delaying the punch line the whole time
okay let me get to the actual punch line
if you follow the policy you do you
don't do anything that improves
institutions and then predictably some
point you die you are completely correct
about what you're saying about the high
levels of hierarchy there becomes less
and less things you can say with great
certainty over the long future this is
actually not exactly true it actually
kind of also goes the other way around
for example it is very easy to predict
the energy of a given system arbitrarily
far in the future it will be the same if
nothing comes that's one statistic one
quantity one qu not that many bits I
know I know yeah I know I think this is
a good analogy I'm I'm agreeing with you
I think this is a nice analogy so I'm
not claiming I know on this date with
this thing this will go wrong in this
way using this technology which will
lead to an unrecoverable state I'm
saying if you keep just randomly rolling
the dice over and over again with no
plan to ever stop rolling or removing
the bad faces of the die somehow then
eventually you roll death eventually you
roll the X risk
this is I can say with high conf with
extremely high confidence I think that
the dice is not identically distributed
Through Time right like our like but it
will be if you don't RIS something like
if you don't if you don't do policy if
you don't improve institutions if you
don't make better ways of wielding we
should do all that we I just think like
right now it's far too early and it's
far too instrumental to select few to
over legislate while when is the right
time when is the right time when we have
a better prior on how the market know
when do we know like when do I wake up
and say all right now's now's go time
when well I mean right now I I don't I
don't think it's right now because
there's just too much things are moving
dynamically like if you're but things
will only get more dynamic in the future
when does it stop when does the buck
stop when do you act well I mean I mean
it's not going wrong I mean GPT 4 people
thought would melt the world or
something when do you act when what is
the sign that when you see on the sky
it's a Continuum there's no discreet
point it's like so you don't have a plan
so you don't have a plan what you're
saying is you have no plan you have no
threshold to action don't know what to
do so you say we should just wait and
see what happens bro well no we we we
should play it by ear is what I'm is
what I'm saying we online ad a different
way of saying you don't have a plan as
you have like you're trying to do
modelbased control you're trying to have
a long time roll out and then you're
you're you're trying to do planning and
optimally control things but then if
you're model has a large Divergence with
the future then your your optimal
control is actually highly suboptimal
right and I'm saying we should because
we're in a time of high uncertainty we
should do things that are on shorter
time scales uh only inact uh install
policies that change things for short
time scales because the situation is so
dynamic but my point is it will only get
more Dynamic there will never be God
comes from the heavens said all right
guys now is the time that's that's why
capitalism is is awesome you have
corporate policies adapting on different
uh uh time scales right yet like you
have corporate hierarchies and you make
decisions of cybernetic control on
different time scales um and and you can
change the the sort of sampling rate of
which you adapt and that's why why isn't
that time now why isn't now the time why
not you don't have a plan you don't have
a model maybe I'm I mean I think every
corporation is adapting in an online
fashion to the landscape sure but if I
say now is go time now is the time for
big regulation now the time is for for
you know that's a different question you
don't even have a way to evaluate it you
don't even have a function to evalate
you just said play it by year you don't
even have a to evaluate it I do think
that my point is like current
institutions have such a slow clock rate
uh that basically I claim even if you
have good thesis a good tangent right
now a Model A first order model of how
the future will go from like the local
temporal tangent like that first order
model is going to be super inaccurate in
the future and if you have a very low
clock rate of adaptivity hyperparameter
setting that you impose now and and it's
like crystallized forever that's going
to be highly sub suboptimal right like
it's just it's going to be negative
right but then at some point you have to
act like you're just making excuses this
is just cope this is just cope for not
acting
now but I mean every every company is
acting and and and adapting to the
situation and and incorporating a yes
but at some but if they're making a
mistake if everyone is making a mistake
and AGI is around the corner and it will
kill everybody if this is true no well
we disagree on that but anyway yeah sure
fine but you don't have a plan so I
don't care like you don't have a plan
you said you don't have a plan you don't
I'm literally saying why long-term
planning is not optimal and this is my
point you don't have a plan for
long-term or short-term planning you
don't have a plan there is no plan the
only thing you're talking about we
should play it by ear this is not I I
think that the system adapts right like
there's there's planning
happening lower levels the lower levels
of the hierarchical cybernetic control
tree adapt on a faster time scale and
are adaptive to the changing landscape
of technology today I think that once
you know we distill certain bits of
information that are less dynamic we can
crystallize policies at the mother node
once that becomes apparent but right now
there's nothing and again when does it
become
apparent when when there's stability in
a certain Trend right like when when
when we see okay this this keeps
happening this is bad oh like increase
in AI capabilities every year stuff like
that that's going to happen that's just
technological progress but I so that
doesn't count that doesn't count but
like you know like that's not bad I mean
that's going to happen I mean I'm
working on it right like I'm working on
and that's not a sign that maybe there
should be adaption or something that's
not a
sign well no but there's huge upside to
it right like I mean we can we can solve
a lot of pro problems in our world no
but that's why like but then we would
shut down all the upside that we're
leaving behind saving lives solving
global warming engineering materials no
one says but we're literally proposing
compute caps so like yes until we know
how to deal with it like look if we knew
how to if you had a plan if you were
like all right Connor I'm glad you asked
here's how we ensure that like all the
things you're worried about don't happen
they're not perfect I might be wrong
about some of them where do you think we
could improve if you had said this I
would have been like I mean first of all
I'm in a simulation but assuming this
had happened then I would have been like
all right fair enough but you don't have
a plan no one
exped that I think at this moment I I
don't think there's it's time for a a
global but you don't have a plan for
when the moment comes so it's worthless
your word is worthless comes yeah
there's no moment it's just like
constantly improving technology
throughout of throughout society and I
think like if if we make sure that
there's power equilibration right
everybody have have access to to AI
power proportional let's say to their
access to Capital and we don't create
sort of these weird uh Dynamics where
you achieve regulatory capture you
subvert uh the low lower levels of the
hierarchy as long as we maintain sort of
a a not too high of a power gradient on
any point in society I think the system
will
adallyness like it's not like I don't
like there's two there's there's a meta
point and there's the concrete point the
concrete point is this like lol Lau
obviously this is the meta
point is just like why do I care what
you have to say you've literally admit
it that you don't have a plan and you
don't know when you will have a plan or
need a plan I think this if you if you
have an Adaptive it's like it's like
saying I'm I'm launching a rocket I'm
not going to chart out all the values of
the optimal controls at this stage I'm
going to adopt it according to sensors
but you have a flight path you have a
rough idea you have an extremely precise
idea when you shoot a rocket into space
where it's going to go have you seen
those fight paths and how there's like
fine-tuning uh if you and me were
disagreeing about the fine-tuning it's
like should the compute cap be at 10 to
the 25 or 10 to 24 great then we could
talk but we're not talking about there
you have a very sharp prior from the
laws of Newtonian mechanics of like like
where the trajectory will go and I I
think we disagree on whether we can uh
predict uh sufficient parameters and if
weal machine to optimally control it
sure and if we can't predict an
extremely high energetic extremely
dangerous weird system you shouldn't
predict things are good unless you have
a very sharp prior for why things are
good this is possible but by default you
don't get nice things most most of
universe is cold and dead and lifeless
there is an observing life is extremely
weird this is an extr situation so so
yeah but that's the thing iak is based
on the physics that underlies life and
the point is that we think that the the
thing we can predict is that the system
is going to adapt to whatever but life
can also just die like life can just end
like life isn't fair a meteorite can
just hit the planet that's big enough
and then that's just
it m
I I mean it did and then we bounced back
better than ever if it was big enough to
just liquefy the planet it would just be
over and then yeah but I I I just don't
think that uh a constant roll out of
Intelligence being progressively
Incorporated in our techn Capital
machine is going to be that discreet
event that causes Mass disruption the
system discreet or not this is
irrelevant my no I I think it's very
relevant because a complex system
failure is when the system is maladapted
to a sudden change in in the environment
and and the point is if you keep things
manable in and con changing on a fast
enough clock rate as you you
adiabatically slowly change uh uh the
effective landscape so constant roll out
of new capabilities then the system can
can can morph to to to incorporate it
and and and incorporate this new reality
and not fail right and I think like that
is literally like what we're arguing for
that like crystallizing anything at the
global level right now will cause later
failure because we're going to be Mal
adap Ed to reality that we failed to
predict
accurately do you think these words
would have been much you know Comer to
neanderthals um so well you can talk
about Evolution I mean we're already
getting steered evolutionarily by the
techn capital machine as is so like
where do you draw the line like okay
should we go back to you know back to
the cave and uh you know give up ask
question do you think these words would
have been not good words to say to omon
Ander thalis do you think you would have
been happy with
that well I mean you know part of their
genes live in us today they've had some
fraction you think that they're happy
about that think they care their their
selfish genes are uh yeah but do they
care they were people do they care but I
mean the PE even the the homo sapiens of
that time they're not alive they don't
you know yeah don't care like all that's
been passed on is is jeans right so sure
but like so what okay you pass your Gene
and then everything every and everyone
dies every institution dies every sure
but I still care about people I don't
care about genes so like if I you know
if you go pregnant a bunch of women and
I shoot you in the head are you
happy um I I think every again everyone
dies and all you leave behind is your
legacy have mut information with a I
think you're pretty happy if you have
high Mutual information with the future
and it's secured uh I I think that's the
basis for happiness but that's just my
theory like if you have a lot of
children then you're maybe not scared as
death if you have a large Legacy and
it's secured maybe you're at peace with
it I'm I'm not afraid of death like in
general I just think like I'm going to
do everything I can to impact the world
in the direction I think is good uh
while I have while I am alive but I am
not trying to be immortal I don't know
but but that's and that's actually you
know point of fracture with an EC some
people are uh you know against death but
I I think death just likeing
clauses I think death is an important
part of this cyclical adaptation
just like dissipation is in a thermic
system um and so I think injection of
constant novelty and fading out of the
old is is is is important to maintain
that adaptability which helps us
converge onto bigger grander more
beautiful
things well I hope you're right um sure
sure feels like you're just letting you
know taking letting Jesus Take the Wheel
here and I think my whole claim again if
you take anything from this
I think we can do better than that I
think you literally you and me not this
is not a metaphor like literally you and
we should we should chat we should keep
chatting this was really productive
honestly I I feel like I understand your
position better you're starting to
understand us uh better um I think
understand you better I don't think your
followers believe what you think they
believe well I mean it's hard to tell
you know off of uh Twitter memes and so
on but you know I I am me uh you know I
may have written them Manifesto and so
on and you know I'm just saying my
opinions and my model of the world here
in this conversation uh but again eok is
not a single uh point of opinions it's
kind of a cloud that makes it sort of in
alignment with our thesis about uh
variance and all hyper parameters but um
yeah I mean um I think this was a lot of
fun I think we're nearing uh three hours
at this
point maybe we split split it up into
more podcasts in in the future but uh
you know really appreciate you uh taking
the time um yeah no thank you as well I
I want to say this again I said in the
beginning I say it again I really
appreciate and respect that you take the
time to actually talk about your
opinions you know wasn't wasn't an easy
debate to have and so on and I grilled
you pretty hard a few times you know
nothing personal like truly nothing
personal sure sure no I me welome yeah
mtic competition is good you know you
should roast my ideas and I should roast
yours and I think this is good and we
can still respect each other people so I
never really fully revealed the punch
line I'm sorry about like the thing I
was trying to talk about yeah there's a
long build I never released what do I
think we should do and like the thing is
that like I think this is what I think
we disagree but like this might be after
for another podcast is that like I think
competition is good but we need is that
what you should do is there needs to be
some things that are off limits some
things that have massive externalities
that blow up everything that destroy
competition whatever and within that
go for it like optimize as hard
as possible all niess out of the window
but if you if you expand this to
Encompass literally everything you
predictably end in disaster this is what
I call civilization civilization is not
about being nice it is about we have
some rules you know no killing the other
guy you know no poisoning do we respect
those rules how are they enforced they
need to be enforced by violence yeah but
then who who keeps those people in check
right it's always this is a good
question know this but this is a this is
a design question and I'm not saying
this is easy but we've done hell of a
lot better than random our current
civilization living in the United States
or over here in London is a hell of a
lot better than living in Somalia or
whatever you know there is plenty of
more restrictions that we have here
there's plenty of things that are more
restrictive here and I'm happy to take
the hit so for me personally
coordination is about taking a hit it's
about saying I will willingly surrender
some of the things they do for example I
can't go over to San Francisco and
murder this guy because he annoys me
because I wouldn't because that's bad I
I Surrender this power not that I would
do that you know please please don't
suggest that yeah
yeah don't worry I think you're stronger
than me anyways so you could
probably um but I think that there it's
neither all stagnation nor all
optimization I think there is a middle
way I think what we need to do is we
need to cut off the tals and then within
the things that are not the Tales we go
Hog Wild I think we're we've
already gone for 3 hours you know if you
want to respond to that or whatever but
I don't want to take a much more every
time yeah I I I think like we agree that
institutional dynamism and exploring
policy is the way forward I think we
agree on that um I just think that in
their current form current institutions
are far too slow and um I think anything
anything we try to do on a short time
scale before we have new institu
probably would be net negative and so
that's why we have a bias towards hands
off for now let's see how the situation
evolves you may have more urgency to
take top down control Reigns I think
we're still in a good region like I
don't see things going uh South in my
model we may have different models of
the future a whole different you know
podcast there whole different topic
whole different podcast but yeah I mean
I'm I'm happy we found some some common
ground here and we got to understand
each other's points a bit better and
hopefully it's produ for our communities
to have a a discussion and I mean that's
that's the point of these things of
having sort of taking political extremes
is is to have a discussion and for
people to make their own minds from from
the discussion so thanks again for
taking the time and and uh thanks for
thanks to mlst uh for hosting yeah thank
you so much it's my pleasure it's my
pleasure

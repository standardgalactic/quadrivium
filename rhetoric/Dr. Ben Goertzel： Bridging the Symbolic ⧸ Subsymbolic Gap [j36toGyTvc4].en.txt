it's a pleasure to be at this AGI summer
school in in Iceland I I if my
calculations are right there are there
are four of us here who are at the
previous AGI summer school in uh in Jama
and China so so it's it's great to see
the the AGI Meme and and movement uh
continuing to spread and and interesting
to see it spreading in sort of
out of the way corners of the globe
which is it which is quite cool
especially for those of us who like to
have an excuse to travel to interesting
places
I'm going to talk today about
the open Cog architecture for AGI and
then
in the main novel substance of the talk
about some ideas for
Bridging the traditionally perceived gut
between symbolic and sub-symbolic
aspects of intelligence
via integrating into the open Cog
integrative architecture uh hierarchical
temporal memory architecture for
perception which is HMR irl's Destin
architecture
the open Cog architecture
has been around a few years and we're
using it to control some virtual agents
in in Virtual Worlds
so it does some things it's not
amazingly generally intelligent yet but
it's it's functional and is improving
month by month
the ideas I'll discuss about Bridging
the symbolic sub-symbolic Gap and
connecting open Cog to visual perception
this is stuff that is is currently
active research and doesn't do much yet
and therefore it's more interesting for
me to talk about because I'm I'm always
more interested to talk about what I'm
doing than what was already done last
year so that this will be be a mix of
things we've done using opencog and
things that we're now working on
I'll start with just a few words of what
I what I considered to be AGI I won't go
into much depth on this since this is
the AGI summer school and people have
probably filled him in on these
generalities already
I think about general intelligence uh
heuristically as
the ability to achieve complex goals in
complex environments uh
I don't mean by that that explicitly
goal-seeking behavior is necessarily the
be-all and end-all of of general
intelligence I mean there are aspects of
what humans do that are not explicitly
oriented toward achieving goals and
ultimately our our conception of
intelligence is going to change as we
develop more and more intelligent
systems but this seems a useful way to
think about intelligence for the for the
time being and
as as you've probably heard Marcus huder
Jurgen Schmidt Uber and some others have
worked on Grand general formalizations
of what is general intelligence and one
thing you find from pursuing these
formalizations is that human beings are
not all that generally intelligent I
mean if you compare a human being
theoretically to an Optimizer that can
achieve any computable goal any
computable environment which you can
mathematically show in some sense exists
humans come out looking pretty stupid no
I wouldn't take that too seriously
personally because I'm not sure that
this conception of intelligence of
maximizing objective functions or goals
and environments is going to be the same
way we think about intelligence 50 or
100 years from now anyway
but I do think there there's something
worthwhile
in the observation that humans are not
like maximally totally generally
intelligent we're actually a mixture of
various specialized
intelligent subsystems with some core
that in some sense does have a very
Broad generality I mean our visual
cortex is specialized to recognizing
visual patterns or episodic memory is
specialized for storing retrieving and
organizing memories of our life history
and so on and so on so we our minds
actually are quite specialized for a
variety of different things but yet we
do have this capability to transfer
Knowledge from one context to another
and to explore areas like say abstract
mathematics that are very different from
anything that we were evolutionarily
wired for and
this point
comes up
in the details of the architecture of
the open Cog system which is why I'm
dwelling on it here because
in my own view the way you want to
architect
a general intelligence is to respect
this mixture of components at various
levels of specialization and generality
that the human mind has I mean you you
might think as a computer scientist well
let's make one totally General learning
algorithm which we can program in like
five lines of Haskell or something and
this will be the core of of thought kind
of like the equation of string theory is
the core of physics or something on the
other hand I don't think that's what you
see in the brain what you see in the
brain is a mixture of components at
various levels of generality and
specificity all working together and
well I don't think we have to emulate
the brain in detail I think this
particular characteristic of the brain
is a
natural perhaps inevitable consequence
of trying to achieve a modest amount of
general intelligence given a modest
amount of computational resources
unlike the hypothetical absolutely
general intelligence which shakes
infinitely much processing power
if you start to think about human-like
general intelligence and what we do it
boils down into a really long boring
ugly laundry list of different different
things I mean we we have perception
actuation memory learning reasoning
planning attention motivation emotion we
can model ourselves and others we
interact socially we communicate we can
build things we can do quantitative
inference and each of these boils down
to a whole bunch of different
subcategories as well now none of these
are human-like general intelligence yet
the human approach to general
intelligence draws on on tools that
evolve for handling each of these things
and if we want to build a human-like
general intelligence ultimately we got
to deal with all these things I mean we
may start with one of them and then
Branch out to others but those are all
part of the requirements list
now when we build narrow AI systems
we're thinking about it a different way
because we can focus on just a few
capabilities you can focus just on say
self-driving cars or information
retrieval or a game like chess or
as in some of the biology work I'm now
doing we're using machine learning and
probabilistic reasoning to analyze the
genetics of long-lived flies we have
these flies that live five times as long
as regular flies we want to see why they
live so long and you know that's a valid
AI research it's it's certainly
interesting to use machine learning to
see how different genes and proteins and
networks combine together with each
other but
that's lacking the ability to transfer
Knowledge from from one domain to
another and the program has only one
goal in this case which is to find find
the combinations of genes and proteins
that imply longevity with general
intelligence
we're not looking at any one particular
goal like driving a car or finding why
flies live a long time you're looking
for a system that can
in principle do anything and learn
anything although of course some things
are going to take it longer to do or
learn than than others so this has
probably been pitched to you already but
if you're interested the AGI 12
Conference
is in Oxford in early December which is
a collaboration with the future of
humanity Institute at Oxford and they
will immediately following AGI 12 have
their own conference on the impacts of
AGI and and the philosophy of how to
make agis be ethical as they get more
and more intelligent which is uh and
separate but interesting topic that has
some interface with the hardcore AI
research
also if you haven't seen it there's a
there's a paper that came out of the
2009 AGI roadmap Workshop that a number
of us did at the University of Tennessee
and that paper is mapping the landscape
of human level artificial general
intelligence and that that Workshop was
was interesting uh both intellectually
and I would say anthropologically so we
it sort of it was an outgrowth of a
previous Workshop the previous Workshop
was convened by John Laird and Pat
Langley
who are sort of part of the you could
call the cognitive architectures or
traditional AI establishment in in the
US and
Langley and Laird organized the workshop
at the University of Illinois and the
the goal of that was to bring together
everyone working on human level AI which
is a term they prefer to the term AGI
and trying to come to some common
understanding of how to approach human
level AI what are the metrics for
evaluating progress toward human level
AI what are the principles that you
underlie architectures for human level
AI now
that Workshop led to some interesting
discussions but it's fair to say it did
not achieve that goal the the level of
disagreement among the people in the
room was so radical that pretty much we
couldn't agree on on much of anything
except that the human level AI
ultimately should be able to do
everything a human could do which we
didn't need a conference to tell us so
decided to try again and organize a
similar workshop with people who were a
bit more like-minded so I tried to
gather together people who
were not thoroughly committed to say a
symbolic approach to AI so I invited
only people whose AGI approaches didn't
involve any like hard-coded production
rules so only people who are interested
in agis that could learn from experience
and also only people who were at least
interested in robotic or virtual
embodiments so trying to get only
researchers who wanted to build a agis
they can learn from experience by
interacting with and experiencing some
3D World with some vague resemblance to
the world that we humans live in no
unsurprisingly but amusingly this group
also couldn't agree on much of anything
so although it was quite narrowed down
from the scope of those original
workshops however we did agree
on and off to write a paper describing
our various perspectives and their
interrelations and
regarding not the architecture of
how to build an AGI but more what kind
of tasks and scenarios made sense for
testing an AGI and for gradually growing
in an AGI and and mediating its
development from from nothing to human
level intelligence and
it was somewhat interesting that paper
was in an AI magazine not long ago which
is progress it's not the first time this
sort of stuff has appeared in AI
magazine although their bias is toward
narrow AI in 2006 or seven Nick
customized did a special issue on human
level AI in the in there also so it does
it does pop up now and again with
modestly but significantly increasing
frequency
we could not in that Workshop agree on
what kinds of environments were best for
AGI a lot of interesting ideas were
thrown out there I mean Sam Adams from
IBM had the idea of a general video game
playing environment where basically you
want your AGI to be able to play any
video game at all
I mean just throw the game give it the
controller and let it play now in one
version you'd make it use robotics to
move the physical controller in another
version you can just connect it
digitally to the
to the game console which is I think is
reasonably Fair
I'd like to say samsonovich was
championing basically school give it to
school work grammar school work or high
school work pretty much paper schoolwork
but a variety of different types uh
Joshua who was there was advocating an
approach to a video understanding and
movie understanding with with tests like
among others you know look at this
10-minute scene of a Hollywood film tell
me what happened what what do you think
is going to happen next and why did that
character do that which involves
integration of a whole bunch of
different kinds of of understanding and
I was advocating some sort of AGI
preschool where you take your AGI either
in a robot in a preschool setting or in
a video game character in a kind of
videos game setting where it can play
with various things let it go through
the various kinds of training and
playing exercises that that young
children do no
you can see all these different
scenarios are sort of getting it the
same thing
but you can also see some significant
differences there in terms of what you'd
actually need to build I mean in
Joshua's suggestion at that time of
video and scene and story understanding
I mean you don't need to mess with the
mechanics of actuation right you don't
need a hand to to build stuff be it a
virtual hand or a robotic hand on the
other hand
Josh Hall who was also at that Workshop
he advocated the Wozniak coffee test
so Steve Wozniak the co-founder of Apple
had said once no robot will ever be able
to walk into a random house in America
and make a cup of coffee
by which he meant you know find where
the coffee grounds are stored look
around find the appropriate machine
figure out how to use it by looking at
all the dials and figure out where is
where is the cupboard to get the cup and
so forth Common Sense understanding
and he was I assume ruling out the
approach where the the U.S sends Census
Bureau people to every house in the U.S
and draw a map of everyone's living room
and everyone's coffee machine it was
supposed to be
General learning in the domain of making
coffee but still deal with any house and
any any coffee machine and
Josh really believes you got to do that
you got to deal with real world sense
perception real world actuation or else
ultimately you're doing something that's
going to be narrow and can be codified
in some long list of expert rules or
something equivalent so basically he
believed you needed a robot and this
this is one of the disagreements that we
couldn't overcome in the context of this
this workshop and I dwell on this
because it gets at the
the meat of this talk which will come a
little later in terms of what what we're
doing now aiming toward integrating open
Cog with robots as opposed to the
virtual characters that we've mostly
been working with and I've I have been
as I work more and more with Virtual
Worlds I've been drifting further and
further toward wow a robot will be a
good idea I'm not extreme like Josh hall
or Rodney Brooks or these guys who think
you need robotic embodiment to make AGI
I mean I think you could make an AGI
using only a text stream of input I'm in
it's more a matter of what's going to be
easier what's going to be a faster way
to do it what's what's going to be a way
that we can understand better and debug
it better and guide it better what we'll
take
the most modest amount of computing
resources and human detention and
cognitive resources to to build rather
than there being some magic in in the
robot body as typically perceived so
that's some general framing now I'm
going to talk a bit about open Cog and
I'll go through this at a fairly high
level because I want to get to this
symbolic sub-symbolic integration stuff
before the the time is has expired so
I'll start off here with some some
speculative futurology so
I'm often asked this not So Much by
AGI researchers but by journalists and
people from the general public or by
Rich guys who don't actually want to
give me any money but just want to gloan
about the fact that they could if they
want to okay how
if I were to give you
20 million dollars or 100 million
dollars although I won't then
how fast could you build an AGI at that
point and
of course the answer is we have no idea
anyone who's done radical science knows
sometimes things actually go away faster
than you thought oftentimes things go
away slower than you thought and you
don't know what are the hidden problems
till you till you're going through the
work but nevertheless
we have to make a a plan to guide work
even though we know that it won't
actually be followed so when when we try
to plot out what could be done with open
Cog if we could hire as many people as
we want to and by all the computers that
we wanted we come out with something
like this what we're doing now is what I
call a proto-agi virtual agent I mean
we're we're using opencock system to
control virtual agent in a virtual world
it's not all that generally intelligent
it does do things and it learns things
and it builds stuff in the world
assuming our funding holds out
and we don't run into any unforeseen
technical obstacles
in the next few years
we'll be able to get the AI system
smarter and smarter in in the virtual
world in terms of doing simple
child-like stuff in the virtual world
and we'll get started in a serious way
on the robot integration
a couple more years for more advanced
learning and reasoning then the way I
see things proceeding in practices I
think we're going to be able to build I
don't want to call them expert systems
because that has has historical baggage
but I think we're going to be able to
build AGI systems that
it sounds kind of funny but there are
generally intelligent in the way that's
focused on particular domains
and it's like a narrow AGI but I mean to
think about that in a specific context
thing about a home service robot
I mean or wozniak's coffee test a robot
that could do that I mean
you could imagine an AGI that could be a
reasonable Home Service robot and could
go into an average house and make coffee
yeah it wouldn't actually be able to
teach third grade ride a motorcycle
prove a theorem compose a sonnet or even
rather a decent leather I mean I would
argue once you've got something that can
pass the coffee test or be a Home
Service robot
you've got in there the Crux of an AGI
algorithm and architecture
that could be used to
do anything humans can do on the other
hand in actual practice
there may be a gap of years between
getting the first agis that display some
common sense learning and general
intelligence in the specific domain and
actually manifesting the possibility of
that architecture to do everything that
humans can do I mean that's will be a
short period of time
on the historical scale but maybe Years
anyway I mean could conceivably be the
decade or two that that's really hard to
say the service for about this one
example another example tying into my
bioinformatics work will be a
bioscientist I mean say right now we use
narrow AI to do machine learning and
crunch various biology data sets but you
can imagine a narrow AI slash AGI
bioscientist that could read research
papers
in a reasonably General way extract the
meaningful information from biology
research papers in grade that with
machine learning run biology lab
equipment without being able to do every
single thing that the human being could
do so I think we're going to see that
because that's what the world wants the
world will pay for
proto-agi systems that do specific
things and out of that eventually we're
going to see full-on human level AGI and
out of that we're going to see something
going beyond the human level I mean
ultimately as as Christian has talked
and written about I mean the human brain
has many weaknesses one of them is it
gets old and dies and another one is
that when we open it up and try to
Tinker with it we tend to cause it to
function even worse than it usually does
whereas in an AGI system won't
necessarily have any built-in
impediments to self-modifying and
improving its own code so you can
Envision once you have an AGI that has a
general intelligence of a computer
scientist
you can have a lot of really fascinating
pathologies as it reprograms itself in
Stupid Ways and becomes
demented but you can also have some very
rapidly improving levels of general
intelligence and
I think in theory we could see all this
unfold within the next decade but that
would require things to go well we would
require Society to decide to fund AGI
reasonably amply so that AGI teams
weren't working half time in AGI and
halftime on narrow AI
or not advancing at all as is the case
with some AGI designs
so right now we're developing the open
Cog architecture
toward AGI and we're also developing it
with various applications in mind so
we're using it to control video game
agents we're using opencog to Crunch
biology data we're using it for
financial production on the Hong Kong
stock market and this at the upper
corner is
a screenshot of our current Video Game
World which is is Minecraft like it's
built in the unity 3D game engine but it
lets the AGI build stuff out of little
blocks basically tens to hundreds of
thousands of little blocks so it's it's
a bit more ambitious than the good old
blocks World from from decades ago but
it follows some of the similar
intuitions that building the blocks is a
it's a domain in which you can
give the AGI a combination of perception
action reasoning learning social
interaction and so forth but it's fairly
computationally tractable before that
we're using it to control uh
a virtual dog in a virtual world that
learn learn tricks basically and
the robot at the bottom is from some
experiments we did in 2008 and 2009
using opencog to control and now a
humanoid robot at jaman University in
China that that project is sort of
mothballed at the moment but we're
looking at the different robotics
project involving the Hanson robocon
that I'll discuss in a moment so
on the very
high level
as I said you can look at general
intelligence as the ability to achieve
complex goals in complex environments
and what that basically means is an
intelligence need to use its perception
its memory to predict what actions will
achieve its goals and
of course that's really easy right
memory just we already have memory in
the computer perception hook up a camera
action hook up some arms and just hook
up a universal all-powerful prediction
algorithm and it's all done
but the the problem is the universal
all-powerful prediction algorithm takes
infinitely much computer power to run so
we have to get subtler than that and put
a bunch of
fairly well architected things in each
of these slots to get to do anything and
to approach AGI in practice there's a
lot of big questions we need to ask and
different researchers answer them
differently so you know do we want a
single cognitive process which is at the
core and there's a kind of periphery
around it I mean pay Wang's approach is
a bit like that as I understand he has a
non-naxiomatic logic he has a logic
engine built on this unique and
interesting logic he doesn't necessarily
think that logic is it but that logic is
the center and then you have something
dealing with visual perception language
processing procedure learning and so
forth and these These are sort of
implemented in terms of the of the One
Core logic and at the other end of it
something like Jeff Hawkins or HMR
arouse architectures which are based on
hierarchical temporal memories again for
them the hierarchical pattern
recognition network is the core
and anything else that you have is sort
of at the periphery helping that
hierarchical pattern recognition network
to do its thing
another approach is
there is no core complex process you
just got a bunch of
small simple elements together
interacting According to some complex
Dynamics and intelligence emerges out of
that and many people in the artificial
life Community think of intelligence
that way and
I think that that's quite interesting it
seems to me harder and to involve more
Computing resources than engineering
something
another approach is to take a number of
complex processes a number greater than
one and hook them together in a complex
way
open Cog is is more like that it's what
I would call an integrative architecture
so we have a probabilistic logic engine
we have a probabilistic evolutionary
learning engine Called Moses for
procedure learning we use something
attracts our neural network like for
attention allocation each of these is
its own complex story and actually I
could tell a story for each of them why
it could be the core of an AGI system if
I wanted to but I think the best
approach is to integrate a bunch of them
together
again another difference in in Focus
between different AGI researchers is
some folks focus on the architecture
soar is a good example you talk to John
Laird
I mean his view is yeah we haven't done
much with learning yet for the last so
30 years or so but that's because we
need to get the architecture perfectly
right and once you have the architecture
right you know making learning work
within the architecture is no problem on
the other hand you look at people in the
reinforcement Community they're like
well no we don't have a cognitive
architecture but we're getting learning
based on reinforcement and once you get
learning based on reinforcement then the
architecture is kind of just something
you glue on and wrap around your
learning engine right and
you know each of these approaches has
has taught us a lot I tend to think you
need a good architecture and good
learning algorithms and they need to
work together well which makes things
even even more complicated
then you have the issue of knowledge
representation I mean some folks going
back to soar or act R for example you
have explicitly represented knowledge
and which is is often called symbolic
knowledge you have knowledge where you
can look at a knowledge item in the
systems knowledge base and as a human
being you kind of read out in natural
language what that piece of knowledge
represents like a a cat is an animal or
you know to pick up a cup of coffee I
must extend my arm and open my fingers
the other approach is to look at
implicit knowledge representation where
the structures that emerge from the
interaction of the system's Atomic
elements spawn context appropriate
knowledge representations
and so to see what is the system's
representation of a cup of coffee or of
cats and animals you would need to be
able to decode the complex interactions
and networks or the systems Elementary
components just like it's hard to look
in the brain and see how a piece of
knowledge is implemented and
again you can see the pattern of my way
of thinking in open Cog we use both the
explicit and the implicit forms of
knowledge so what what you can see is
that I hate to make these choices so I
just think the Mind uses all of these
approaches in different aspects and
makes them work together and an AGI
system to work with feasible Computing
resources is probably going to have to
do that too so if you look at
opencock.org there's more information
I'm going to give a few more slides in
open Cog too
the book The Hidden pattern that I wrote
a few years ago goes over a bunch of
philosophy of mine but also talks some
about the under the AGI architecture
it talks about the novemante cognition
engine which was a proprietary system
that was a predecessor to open Cog but
it is conceptually similar huh
building better Minds is a book I've
been working on seems like forever which
actually exists in a complete draft form
though and should go to the publisher in
in a few months probably come out early
next year which goes over the open Cog
design in great detail
another way to think about everything
that a system like opencog has to do
this Arc this diagram comes from Aaron
Aaron sloman with it with a few tweaks
by myself and we
what we see here we have an environment
we have perception and action we have
long-term memory and associated with it
long-term thinking processes which he
calls deliberative processes you have
working memory
which is related to what's been called
short-term memory associated with
Without You of what what Solomon calls
reactive processes
you have some specialization for things
like language and metacognition then you
have arrows pretty much going between
everything and every other thing because
there's so much feedback in the mind and
you know one thing you could do is
follow
cognitive science and just try to put
one thing in each of those boxes and
then connect them together that that's a
little bit like what
Sam Franklin is done with his lot of
architecture I mean he's taking the kind
of boxing line diagram drawn from
cognitive neuroscience and put a certain
AI algorithm and structure in in each
box in opencog
we've done something a little different
but we still try to achieve all of those
functions so
for knowledge representation as I said
we use both explicit and implicit
knowledge representation we do them
within the same data structure which is
a weighted labeled hypergraph that we
call the atom space
some nodes and links in the item space
represent explicit logical relations
like a cat is an animal or more complex
things involving nested quantifiers
some nodes and likes in the item space
are more like heavy in links and then
the tractor neural network
which are learned by experience and
which represent knowledge via their
Collective coordinated activity patterns
so
an example of explicit knowledge here
this is
this is a bunch of nodes and links that
are labeled like there's a cat no the
dog node the Jazz node the tail node
representing cats have tails and so
forth and these may be there in opencogs
network if the network is red stuff or
heard language because then it will it
will Brew nodes internally to correspond
to the words it's heard on the other
hand you can also have a lot of nodes
and links that have no labels on them
and don't correspond to any particular
thing that the human could describes say
in this one the
the top cluster of nodes and links
pertains to chickens the bottom cluster
of nodes and links pertains to food
some of the nodes have words attached
some of them just represent placeholders
inside the system's knowledge base where
the weights of their links to other
things are what is significant and you
get a whole bunch of links of different
types going from the the chicken bundle
of nodes and links to the food bundle of
nodes and links and you could look at it
as just an overall collection of
synapses relating the one to the other
or you could look at each note and Link
on its specific
semantics
we look at there as being a number of
major types of memory in opencog and one
of the architectural decisions we made
is to treat each of these kinds of
memory somewhat separately
so with a declarative memory which is
facts and beliefs and we'll handle that
using a probabilistic logic engine
called Pilla and probabilistic logic
networks and there's a book on that that
was published by Springer in 2008
procedural knowledge knowledge of how to
do things we represent by little
programs in a list like language called
combo and these little programs are
learned by a system Called Moses which
does probabilistic evolutionary learning
which means it's kind of like genetic
programming but instead of crossover and
mutation you do probabilistic modeling
of the evolving population and then
generation of new programs by instance
generation of the distribution found to
model the population for
episodic knowledge
we use
a simulation engine among other things
which is sort of a detailed engine
signaling the 3D World and we also use a
kind of sparse distributed memory for
attentional knowledge knowledge of what
to pay attention to at each point in
time we use something kind of like a
heavy and neural network it's we call
economic attention Network so it spreads
virtual money among the nodes and links
rather than spreading activation but it
it's supposed to simulate the way
activation spreads to the brain kind of
making a moving bubble of attention go
go through so
each of these kinds of memory has a
different
knowledge structure different software
data structure associated with it and a
different learning mechanism associated
with an open Cog and then these all have
to work together and if so from a big
picture what distinguishes opencog from
most AGI approaches out there is that
rather than having one AI representation
and one learning a reasoning engine at
the core we have a number of big
complicated
representations and learning and
reasoning approaches
and they're all configured in a way that
lets them work together and communicate
with each other and help each other out
when they run into bottlenecks such as
those caused by combinatorial explosions
and this this makes open Cog bigger and
more complex and annoying to work with
than the system that's working that's
based on a single
algorithm I think it also gives it a
higher odds of of success
so I already talked about Moses and pln
which are probably the two biggest
cognitive algorithms and structures in
opencog if you want to learn more about
those there's a book on pln called
probabilistic logic Networks
Moses is described in Moshe looks 2006
species
medicog.org and
this is all also open source code
available on opencog.org
the core of it is C plus plus some bits
like planner and are in Python so
this picture I know no one can read it
but you can see the the beautiful
topology of it anyway so that's a is my
attempt to summarize the open Cog
architecture in one picture which is
very difficult but
what we have in the center is the atom
space which is a repository of nodes and
links
what we have in those boxes at the top
are the various cognitive processes like
pln Moses attention allocation and so
forth which act on that repository of
nodes and links creating new nodes and
links destroying old ones
modifying parameters of existing ones
such as the truth values of logical
nodes and links the the attention values
indicating how much attention to
associate with nodes and likes then at
the bottom here we have stuff for
getting information into and out of the
atom space and interfacing with the real
world so perception action and and
language and that's going to be the
focus of the end part of this talk is
basically this little hierarchy here
perception hierarchy look
how do you get data from the
messy complex
rich
physical world say visual or auditory
data into this atom space of nodes and
links
so
what one can do and what I do for
example in building better Minds one can
take this sort of General cognitive
architecture diagram
which is from Aaron sloman in this case
with a couple tweaks and one can take an
open clock diagram with all the
different representational learning
components in opencog one can go through
and systematically say okay what which
aspects of open Cog tie into which
aspect of the overall human cognitive
architecture diagram and that's that's
done in in the chapter of the building
better Minds book now one exercise I
thought of doing but haven't had time to
do which would be really interesting
which could be another Workshop
sometimes take take a cognitive
architecture
diagram of the human mind something like
this try to drill down in a little more
detail
and I wrote the paper trying to do that
in the book that Perry and I edited on
the foundations of AGI I just tried to
take this diagram and then blow out each
box and do another diagram basically to
try to get
try to get some kind of overall
cognitive architecture diagram for our
human intelligence Works coming from
cognitive psychology more than from from
AGI but then what you want to do is take
each AGI architecture and map its
structures and Dynamics into the ones
from
from the human mind and I've done that
for opencog it was a fair bit of work I
mean if I had a few weeks I could do
that for a lot of other AGI
architectures also but I think that
that'll be an interesting way to see
what are the relations between different
AGI architectures which often achieve
qualitatively the same things by many
different mechanisms so
right now the main Nexus of work on open
COG is in Hong Kong which which is where
I'm now based and we're using open Cog
to control these little video game
characters
and this is an old version of the
Minecraft world it's not more it's not
richer than that so we we have
even cooler looking virtual robot than
that now but that's that's that's the
one that we were working with when I
made this slide but basically animated
characters go around in
a virtual world made of blocks and try
to
learn how to do stuff to achieve their
to achieve their goals
so this this is a video from like six
months ago or something I didn't manage
to make a new video but this shows the
robot in the previous version of our
Minecraft like world
carrying out a behavior similar to most
human beings it has two goals stability
and energy so
for energy it wants to get a battery to
get more electricity for stability it
wants to stay in its house so basically
it sits in its house until it starts to
run out of power
then when it does it runs out and grabs
a battery then once it's got enough
power it goes back and hides in its
house again where it's safe huh this is
sort of the archetype of the caveman
Behavior right coming out of its cave to
get to get food and then go back which
is
is kind of of
simple and and boring but
illustrates the agent interacting with
it with its world to achieve a goal what
no
well you mean is there well once it has
its Harvest well it can get as many
batteries as it wants but it's energy we
only go to a certain level so I mean
just like you could you could eat a lot
but at a certain point you're not
getting any more useful energy from it
right
so what we're working on now that I
didn't manage to pull together a video
about
is more stuff with with blocks building
so for example let's say the the agent's
battery is up here and it's down here
it wants to get the battery the guy
comes along here oh no I can't get up
there the battery is up too high so then
then it figures out to grab together a
bunch of blocks or other objects that
are available and make steps climb up
the steps to get to get the battery up
here so basically using building with
blocks
in order to help it achieve simple goals
in in the game world and that that
example I'll just described works now
and a couple other simple examples I
mean it's not
incredibly robust in the sense that
sometimes it's too stupid to learn to do
that and sometimes it isn't yeah
um that's
well
that's a complex question actually I
mean it so
it we built in blocks as a primitive
right so it can see within a certain
field of vision it can see the position
of each block and in terms of the of the
coordinates and
in terms of the fact that a battery
gives it energy it has to learn the the
battery gives it energy rather than a
block and it
it has a built-in vocabulary of spatial
relations so the above relation for
example if this pen is on this box it
will receive as data
pen is above box pen is is next to box
and so forth now more fuzzy relations it
has to learn so if something is on top
of something that's on top of something
else it has to learn that but it has a
basic vocabulary of spatial
relationships which is kind of like the
3D region connection calculus
and given the basic vocabulary of
spatial relationships and and time
before after during and so forth Allen's
interval algebra
given those things
and given the built-in goals of get
energy get stability get novelty and so
forth it has to learn everything else
well actually though there's one other
exception which is a navigation
algorithm is built in so it does
navigation by a star
if it wants to get from point A to point
B it has a built-in primitive to talk
how to get from point A to point B but
other
in terms of building stuff that that's
all learned I mean it has to learn that
piling blocks
is something useful to do right it has
to learn when it gets next to a block
and steps forward the ones up on top of
the block instead of to the left of the
block and and so forth
yeah
well it's definitely remembered that's
easy that's the easy part it can even be
forgotten but getting it to be used I
mean the generic answer is yes beginning
at getting learned knowledge to be
transferred intelligently
is it kind of open-ended and difficult
problem right so I mean
learning to build stairs
it's gaining some knowledge that would
help it build stairs in in some other
context
but to be honest we haven't since we're
still in the process of getting it to
reliably do the simple things rather
than do them only occasionally we
haven't yet really explored how
effectively the knowledge transfers from
one context to another
that's actually that's sort of Jared
wigmore's PhD thesis which is in the
process of writing
so
there's a lot of work still to be done
in the virtual world actually but
I've also been working toward the next
step which is going from the virtual
world into a robotic embodiment which is
what I'm going to talk about for the for
the next few minutes
I mean the virtual world is great it
encompasses
perception action
social interaction learning language a
sense of self another I mean building
and construction quantitative reasoning
pretty much anything you could think of
is there in some simplified form in the
virtual world which makes it if nothing
else a great approach to
prototype different aspects of of your
code on the other hand you have to
wonder whether the lack of Rich
perceptual data and a rich set of
motoric affordances you have to wonder
how much that is is hurting your your
system ultimately and
more and more when I look at what the
system does in the virtual world
granted we have a lot more work to do
there I'm more and more convinced that a
robotic embodiment is going to be useful
and
my general intuition there is based on
the the
fairly obvious observation that the
learning algorithms tend to find the
cheapest simplest
most ridiculous way
to solve a problem that they possibly
can
I mean the whatever whatever shortcut
they could find to solve the problem
posed by the environment they will tend
to find it now of course you try to
counteract that capability by Occam's
razor by having it try to find a
solution that only not only solves the
problem that it faces but does it in the
simplest way the idea being that if it
doesn't in the simplest way that
capability will then generalize more
broadly to other contexts but that
that's just very hard to tune I mean
tuning the outcome constraint to find
the simplest way of doing something
versus the ability to actually do the
problem is that's subtle I mean there
are many different ways to implement
Occam's razor and there's a there's a
strong tendency of the system
of our Learning System and all other
ones I've worked with in machine
learning context as well
to overfit to the data that it has
which in a virtual world context would
mean finding ways to solve problems that
work in that virtual world that you made
and you can try to solve that by making
more and more complex
Virtual Worlds but hundreds of thousands
of blocks in there eventually do away
with blocks and have continuous
substances and so forth but
ultimately
you wondered like to a large extent the
human mind is over fit to the specific
environments that that we evolved for
I mean we have some generality of
intelligence which is wonderful but in
many aspects we are specifically adapted
to what we what we grew up for and
you're fighting against that and working
in a virtual world not to say that you
can't win the fight by using Occam's
razor heuristics well enough and making
a complex enough virtual world but it's
unclear whether winning that fight is
easier than just giving your AGI access
to the same world that that we have
access to and
robots still suck relative to what we'd
like for AGI but they are getting better
and better I I couldn't resist showing a
picture of myself with Philip K Dick's
robotic head which will be awesome to
put on the first AGI although
a more useful technology I recently
found out about are these these
fingertips by a country called syntax
which you may have seen in the newspaper
and some some machine learning
experiments they were trained to
distinguish a bunch of different things
more accurately than the human fingertip
using a combination of pressure
temperature and motion sensors now
currently they're like six or seven
thousand US dollars for a single
fingertip
but I've actually introduced I I've
discussed with those guys
scaling up by using some Chinese
manufacturers they reckon you can get
down to like 100 bucks for fingertip and
just by manufacturing at scale but of
course what you can do for fingertips
now in five or ten years you can do for
the rest of the body then you'd have a
robot with skin I mean Hansen's Robo
kind which we're exploring using for
open Cog now has quite realistic facial
expressions and then flexible skin I
mean we've seen quad rotor drones we're
always in in the news now or in heavy
development so it it may and the the PR2
from Willow garage which you see in the
upper corner that's like 400 000 US
dollars or something so it's Beyond open
cogs funding at the moment but it's
it's almost capable physically of doing
something like wozniak's coffee test it
can't go upstairs it has wheels but its
Grabber hands actually work to pick
stuff up it can open cabinet it's camera
eyes work well in Real World Lighting
conditions so it may be that now as
opposed to 10 years ago we're verging on
the point we're working with robots it's
not a stupid thing for AGR researchers
to do I mean it's it's it's it's
marginal we're right around that
borderline because every time I've
worked with robots I've regretted
because there's such a pain in the ass
and they always break and they're
something doesn't work and then you wind
up not learning what you want to learn
about Ai and just learning that working
with robots is annoying and
but it's getting it's getting better and
better so I think it's at the point now
where robots are are getting to be
interesting from an AGI
point of view and by experimenting with
Virtual Worlds
we're starting to see more and more
the limitations of that approach in
terms of the the
amazing propensity of learning
algorithms to overfit to the specific
environment that you give them so
I'm running out of time I won't dwell
too much on this but we did hook up open
Cog to a robot using a bunch of hacks
and uh and blue and so forth in 2009 in
John Mann University we hooked it up to
a vision system and to uh Ray Ting is in
the back of the room hooked it up to a
natural language system so it could take
basic text basic text or voice commands
using third-party speech to text
technology you can tell like go to that
chair and recognize the chair and go the
chair and you can train it by supervised
learning to recognize a chair so it's
very simple which is basically just to
show the to show that we could do it
right no
well I'm talking to David Hansen about
now is connecting open card to the
robocon humanoid robot which not only
has a better
faith and more realistic emotional
Expressions then now it does much better
camera eyes which was an unforeseen
problem with the now the eyes and the
now are just not that good and they they
don't give real story of vision there's
one out here and one eye that looks down
which is really weird whereas the hands
and robot has two eyes in front so you
can do regular stereo Vision so I think
working with the now did kind of work
and it was cool and that I mean you can
actually use their pre-built software
for walking and grabbing and so forth
you don't need to do all the low-level
robotic stuff yourself the camera eyes
with the worst limitation which is weird
because cameras are not that bad these
days but it's because they kind of
overfit the hardware for robot soccer
the eye looking down is really good at
seeing where the soccer ball is going
which isn't what you wanted in most in
most contexts but the robo kind has much
better specs than the now in in many
regards and you know in a couple years
more about things with with even even
better specs so
what I'm going to talk about for the
next few minutes and then we can go into
this more in the discussion session if
there's interest is
what we're doing now is a bridge the gut
between
symbolic and sub-symbolic
Ai and actually both of those terms are
kind of contentious and often misused
in concrete terms what I'm talking about
is taking open Cog and connecting it to
NMR arl's
hierarchical temporal memory system for
for visual perception and
you know open Cog is not a purely
symbolic system we do attention
allocation by something like heavy and
learning which is arguably a sub
symbolic as it gets and on the other
hand
edamr system
it is sub-symbolic on the other hand
he he builds in a lot of guidance to
help with with symbol formation so when
when you really dig into the definitions
of symbolic and sub-symbolic everyone
defines those terms differently and they
maybe have more historical interests
than anything else but it does at least
point you at a certain area of research
open Cog is heavily symbolic that's the
strongest symbolic aspect Destin is
mainly focused on sub-symbolic low-level
perception from high dimensional Visual
and auditory data and we're looking at
putting them together to make a system
that can do intelligent things in the
context of of humanoid robotics
if you're familiar with Jeff Hawkins
system uh HTM built in numenta
edamr's system destined is somewhat like
that but it seems to work much better at
least it works much better than the the
nuanto version that's available for
download from their website I don't know
what exactly their new version does and
it has a similar
architecture which is is modeled
conceptually on the hierarchical layout
of the of the cortex I mean the the
cortex if you unwrapped it which I don't
recommend to try at home but if you
unwrap the cortex and flatten out the
layers you get six cortical layers
pyramidal neurons
at the center of columns spending those
layers perpendicularly a lot of
interneurons between them each cortical
column with a pyramidal neuron down the
middle has various many columns inside
it with architectures are different in
different parts of the brain and
you know neither Jeff Hawkins nor edema
are trying to emulate that architecture
in detail but they're trying to emulate
it conceptually by having hierarchical
layers and processing that goes up in
feed forward and down in feedback and
then propagates it across each layer
to me that's a bit overdone I mean I
think that vision and audition have that
architecture very heavily if you look at
all faction for example that's a
combinatory architecture with
connections tangling all over and much
of the cognitive cortex evolved from the
reptiles olfactory bulb not from Vision
or audition so I'm not sure emphasizing
the hierarchical structure as much as
these guys do is really right the brain
is is so complex with all these
differently architectured modules on the
other hand
provisioner audition I think it's a
reasonable approximation particularly
for lower levels of processing
which is why I'm interested in either
Mars system destined mainly as a visual
and auditory cortex for open Cog whereas
he is interested in it as a foundation
for full-blown AGI although we have
these different views we're still able
to to collaborate on on this and
similarly we've borrowed an open clock
many ideas from Joshua's microbeside
architecture we haven't taken his
knowledge representation which is also
quite interesting and Powerful but the
way that Joshua chose to break down
goals motivations actions
urges demands and do action selection
and so forth we've borrowed that in open
cognizing in in a somewhat different
ways
so inside Destin
you have multiple layers sort of like
that cortex model that I showed you
they're divided into squares
I would have chosen hexagons personally
but the NMR designed and we're stuck
with the squares for the moment that the
the lower levels of the hierarchy
pretend to smaller perceptual regions
smaller regions of space time in general
as you go up the hierarchy each Square
refers to a larger region of of
space-time and the input at each time
it's a pixel array which is is
displayed across the the two-dimensional
array so 3D Vision has to be done at the
higher levels by some sort of of
cognitive processing it gets 2D input
and
each node is a certain spatial temporal
region higher level for the higher
regions the children of a node refer to
its sub-regions then there's processing
going up and down the node Jefferson
region of space-time based on which
cluster it thinks the current state
belongs and it makes a prediction of
what will happen next inside that region
of space-time
so which is where Hawkins came up with
the term hierarchical temporal memory
they have a hierarchy it's temporal in
the each each box refers to a region of
space-time it's also temporal in the
sense that each box is making a
prediction of what will happen in that
box next and
HTM is a general sounding term but it's
often taken just to refer to to Hawkins
Network so I
I introduced a longer and uglier term
which is a CSD Island which is a
compositional spatial temporal deep
Learning Network which is is uglier and
harder to remember I don't know how to
pronounce it on the other hand it's not
restricted only to one specific system
like Hawken so
this diagram evokes the kind of
integration we're doing where you have
the
this spatial temporal hierarchy which is
taking in image data it does audition
data too but let's stick with images for
concreteness takes in image data and
represents it at each at different
levels of coarseness at different levels
of the hierarchy then we have open cards
atom space shown up there where you have
nodes representing abstract concepts for
example like I ear mouth eyeball next to
above and then
You're Building between them basically
and the way you build between them is
you take this spatial temporal hierarchy
and you recognize patterns
in that hierarchy over time
and the the interesting patterns
recognized in this hierarchy those
interesting patterns are recorded as
nodes and links in opencog semantic
Network so the the basic idea of the
integration is take this lower level
perceptual model recognize recurring
patterns in it
and then put those recurring patterns
into into the somatic Network so you
know the say take the example of an eye
a human eye right the Destin network
will react a certain way to a human eye
and it will react a similar way to a
human eye each time it sees it once it
has seen many eyes
by unsupervised learning it will learn
on its own to internally evoke a certain
pattern of centroids across its nodes
and links when it sees a human eye
so then if we use
a pattern mining algorithm
to look for frequent patterns in this
hierarchy we can then
map those patterns and represent them as
nodes and links in opencog
and then you have links going in and out
between
Destin over here and opencog over here
so this bubble and the other one are
both open clock open clock has nodes for
eye and mouth and nose and so forth
now the subtly here
is that when we try to do that with
destined right out of the box it doesn't
work very well which is usual most
things in AGI don't work very well
sometimes you can get them to work by
changing things around sometimes they
just keep on not working right so in
this case we seem to be getting it to
work by by changing things around and
the main change we had to make is
in Destin in the classic form each node
each each little square on some level
the hierarchy representing a spaceshift
and power region each node came with its
own library of centroids
representing what that node saw and
no that's okay that's kind of like how
the how the human visual cortex works
right but it it's kind of awkward so
imagine look at this spatial temporal
hierarchy right imagine it is it is
seeing this cup so
now it's seeing this cop at one side of
the eye right
now it's seeing this cup at the other
side of the eye
so in the classic Destin when it sees
the cup at this side of the eye
then the copper lead centroids here will
become active on the other hand when you
show the cup at this side of the eye the
cup related centroids over here become
active right
and now of course eventually it may see
a cup on every possible place and it
will learn those copper lead centroids
everywhere
but from the point of view of trying to
mine patterns from this network
that's confusing because it may be
centroid number five over here that
represents the curve of the lid of the
cup over here it may be centroid number
27 that represents the curve of the lid
of the cup each cell is basically
separately learning to represent what it
has seen which
it's not necessarily bad on the other
hand from the point of view of trying to
use a pattern mining algorithm to mine
patterns out of the network it's
annoying right so so what we introduce
is what I call uniform Destin where you
have a basically a common dictionary you
use the same library of patterns first
across every node on a given layer
but then you can also do a simple math
transformation to for a pattern on a
lower layer can be ported into patterns
and higher layers and so forth and so
forth which is is just like taking a
picture and making it coarser grained or
finer grained you can go even further if
you want which we haven't done yet so
that what I just described basically
makes the pattern recognition
translation and variant and scale
invariant but only being able to
recognize a pattern at any
side and any hierarchical level and then
it's represented by the same
Centra by the same dictionary pen and
that that makes it much easier
to recognize for opencog to recognize
that when the cop is seen over here or
over here it's the same thing and also
at different distances because if if
you're holding it near the camera or far
from the camera it's going to appear as
bigger or smaller
now the way they get around it in
computer vision is just to like do some
pre-processing to draw a little box
around the thing they're they're seeing
but that's kind of a hack from it from
an AGI point of view if you just make
your visual hierarchy
scale invariant like we've done with
with uniform Dustin you don't have to do
that the same pattern can be recognized
in any level of the hierarchy and that
that makes it much easier to recognize
patterns in the Destin hierarchy we have
a a Google summer of code student who's
doing that this summer now you can also
take that next level which we haven't
done in the code yet which is to make it
rotation and Shear invariant which is
kind of interesting then then when a
certain visual pattern is seen in a
certain Square then you can say okay
does that match anything I've seen
before up to a certain rotation and
shear and you can store that information
which gets further and further from the
visual cortex but that
this got sent a more General point
a more General point about integrative
AGI systems which I've seen again again
and working on the open COG
which is whenever you want to put
together
one thing and another thing
where each thing carries out some part
of general intelligence
you think you can just hook these two
pieces together but really when you do
it you find you have to go in and modify
this and this and this and this and this
and this and this and in in each of the
pieces and in this case it was destined
when we really dug into it we saw the
way it was representing things
internally
just made it quite cumbersome to
recognize recurrent patterns and in
Destin's structure
than by changing it to make it
translation and scale and very
internally that that became much easier
and and we found the same thing with
everything else for for procedure
learning
we wanted to use genetic programming
then we found well
but wouldn't it work better in terms of
integration with the other components if
we use probability Theory instead of
crossover and mutations we had a
probabilistic semantics inside procedure
learning in in declarative reasoning way
back in in the Dark Ages we were using
pays Nars logic in the predecessor to
open Cog and eventually
that didn't work in the context of the
rest of our integrative design so we
kept a lot of what was in ours in terms
of the term logic structure
and using multi-component truth values
but we ended up replacing his unique
Nars logic with a more conventional
probabilistic logic still founded on
term logic and multiple component truth
values so in one case after another you
find you you can't actually take these
separate systems and just glom them
together you can take separate systems
yeah
radically modify their internals and
then and then then ground them together
and what you're doing when you radically
modify their internals is kind of like
what evolution did when it co-adapted
each part of the human brain to work to
work with each other the parts actually
have to be to some extent made to work
together if you want to have an
integrative system
and I think that there's more time than
I was going to take so I'm done
all right thanks

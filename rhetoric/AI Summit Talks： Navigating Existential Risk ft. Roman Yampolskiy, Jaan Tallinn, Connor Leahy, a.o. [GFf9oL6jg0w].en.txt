I encourage you to take a seat we will
be
starting almost on time because we have
a very rich agenda on a very big
topic we are talking about navigating
existential
risk navigating what people have
described as a very difficult tortuous
landscape of risks that are made worse
by oncoming New Frontier AI That's not
just the AI that we have today but AI
that we can fairly easily imagine is
coming within the next year or two Next
Generation AI That's more
powerful more skillful more
knowledgeable potentially more
manipulative potentially more deceitful
potentially more
slippery definitely more power powerful
than today's AI that AI might generate
existential risks in its own right that
AI is likely also to complicate existing
existential risks making some of the
risks we already know about more tricky
to handle more wicked and we might also
talk about the way in which Next
Generation AI might be the solution to
some of the existential risks and
dilemmas facing Society if we can apply
AI
wisely then perhaps we can find the
narrow path through this difficult
landscape so welcome Navigators in the
hall welcome to Navigators watching the
live stream welcome to people and AIS
watching the recording of this
discussion let's get stuck in we have
lots of very capable knowledgeable
speakers who will approach this from a
diversity of points of view indeed I
think one of the hazards in this whole
topic is that some people want to be a
little bit one-dimensional they want to
say this is how we'll solve the problem
it's quite straight forward in my view
there are no straightforward Solutions
here but you can make up your own minds
as you listen to what all the speakers
and panelists have to say and yes in the
audience you'll have a chance later on
to raise your hand and get involved in
the conversation too the first person
you're going to hear from is
unfortunately not able to be with us
tonight but he has recorded a short
video he is Sir Robert Buckland MP
former Lord Chancellor of the United
Kingdom which means he was responsible
for the entire Justice System here
former Secretary of State for Wales he
is still an MP and he has a side hustle
as a senior fellow at the Harvard
Kennedy School where he is writing
papers on exactly the topics we're going
to be discussing tonight namely how does
AI change key aspects of society
potentially making it better potentially
making it much worse if we are unwise so
let's watch Sir Robert Buckland who will
appear by Magic on the big
screen well I'm very pleased to be able
to join you all be virtually for the
conjecture Arro Summit on AI and the
challenges and opportunities that it
presents us and I think my uh pleasure
at being with you is based upon uh not
just my own experience in government but
also my uh deep interest in the subject
since uh my departure from government
last year now when I was in government
uh I had responsibility for uh for many
years the the legal advice that was
given to uh uh departments and indeed to
the government in general when I was uh
in the law offices Department as a solic
general and then responsibility for
running uh the ministry of justice as
Lord Chancellor and Secretary of State
for over two years before a brief return
as wal secretary last year that s years
or so experience uh within government as
a minister gave me I think a very uh
deep insight into the pluses and the
minuses of the way that government works
the um uh efficiencies and indeed the
inefficiencies about process and I think
um clearly as in other walks of life
artificial intelligence machine learning
uh will bring huge advantages to a
government processes to improve
efficiency to speed up uh a lot of the
uh particular ways in which government
works which will be I think to the
benefit of citizens whether it's
citizens waiting for passport
applications Visa applications or or
other government processes benefits for
example however I think that we kid
ourselves if we don't accept the fact
that alongside the benefits come
potential pitfalls and the first and
most obvious one I think for me is the
scrutability of process in other words
uh the way in which we understand how
decisions are made and that's very
important because understanding how
decisions are made is part of democratic
accountability in societies like ours
and where individuals or organizations
wish to challenge decisions made by
government perhaps through judicial
review applications then the
explicability of those decisions uh
which is accompanied by a duty of cander
by the government in order to disclose
everything about those decisions is part
of that accountability and of course
it's sometimes very difficult to explain
how the machine has come to decisions
and more fundamental than that we have
to accept that if the data sets that are
used in order to populate um the
processes are not as uh full of
Integrity as they should be and are not
the product of genuinely objective and
carefully calibrated processes then we
are in danger of importing historic
biases into the system whether it's
biases against neurodiverse people job
applications or indeed biases against
people of color uh in the criminal
justice system uh simply because the
data sets have imported those historical
anomalies those historical
imbalances now all those questions have
really got me thinking very deeply about
the impact of machine learning on the
ethics of Justice itself and as a result
of my thinking I was delighted last year
to be accepted as a senior fellow at the
mosa Romani Center for business and
government at Harvard Kennedy School uh
and I am working currently on a number
of papers relating to uh the uh impact
of AI and machine learning on the
administration of justice and and the
law itself um it really developed from
uh my own experience as Lord Chancellor
from digitaliza dig digitalization I
should say of the courts um when during
the coid pandemic we had to move many
many thousands of uh hearings online for
the first time you know I think we
jumped from a couple of hundred uh phone
or online hearings to 20,000 a week in a
very short
compass and the status quo will never be
the same again in fact it has moved on I
think in a way that we just hadn't
foreseen before the pandemic now I think
that's a good thing but I also think
that accompanying this question about
increased efficiency is the use of of
artificial intelligence now in some
jurisdictions such as China we are
seeing its uh increased use not just to
do legal research and to prepare cases
but to actually decide them themselves
in other words the AI judge now that's
all well and good but do we actually
know what populates the data sets that
then forms the basis of the decisions
made and I think it's that um intention
unintentional bias or indeed word than
that potential intentional bias uh
whether that's influenced by a
government or indeed a corporate that
might be able through their financial
means to um influence a procedure or
indeed um the way in which we um deal
with cases knowing as we might do more
information about uh the way in which um
judges make their decisions all these
questions I think need to be asked now
before we end up in a position where
we've slept walked into a completely
different form of Justice from the one
that we know now underpinning all of
this I think is is the need to ask a
fundamental question about judgment
itself and that's what I've been doing
in my first paper you know the essence
of human judgment is something that will
be based not just upon an understanding
of the law but on our experiences as
human beings and you can go right back
as I have done to the Judgment of
Solomon and here's emotional response to
uh the the woman who clearly was the
true mother of the child that he
proposed to be cut in half now you know
that's an example I think of the human
element of
judgment which uh has to uh be an
essential uh Foundation of decision
making particularly when it comes to the
assessment of credibility of a witness
uh a human witness giving evidence uh
upon which the case stands all all falls
and of course for judge that applies for
juries as well in criminal trials
particularly here in the UK now um you
know all these questions I think need to
be asked and then we need to work out
what it is that we want to retain out of
all of this now I don't think we should
make any cozy assumptions that because
at the moment some large learning uh um
systems are having hallucinations I
don't I don't think we should be uh
assuming that um just because of that
therefore AI will never work in a way
that can achieve a greater degree of
certainty I think I think the inevitable
um U you Arc of of development will
result in better and better and more
capable machines that's inevitable but
what we must be asking at the same time
as capability is ensuring there is
greater security and safety when it
comes to the the use of AI and that
really underpins I think the work that
I'm doing in the field of Justice what
does this all lead to then well we have
the AI safety Summit AT in the UK next
month I very much hope that that Summit
will first of all involve those critical
players uh in terms of international um
organizations and key uh countries as
well that will come together to commit
to creating I think a defined framework
Within which we should be using AI
safely and that framework I think will
have to take several forms I think in
the field of Justice we could do with an
international framework of
principles uh which will ensure
transparency uh and which can reassure
people that in cases of the liberty of
the individual criminal cases cases
where perhaps the the welfare of a child
and the ultimate destination of a child
is is in issue then that human um
element will be the key determinant in
any decisions that are made and that the
use of machines will be transparent and
made known to all the parties uh
throughout the proceedings um and then
other walks of life I think the AI
safety Summit has to then look as well
at whether Frameworks can be created and
what form they should take I think it's
tempting to try and be prescriptive I
think that would be a mistake not just
for the obvious reason that um AI is
developing and therefore anything that
we write in 2023 will soon be out of
date but the very fact that AI itself
does not mean unalloyed harm uh in fact
it means a lot of benefit and also some
neutral effects as well and where you
have that approach then a principles
based uh system seems to me to be more
sensible than uh overly prescriptive and
detailed rules as you would have for
example to prevent a crime such as fraud
so just some preliminary thoughts there
as to the impact of machine learning I
don't pretend to be a technical expert
I'm not but my years in Justice my years
as a lawyer a judge and as a senior
cabinet minister I think oblig me to do
some of the thinking now to help ensure
that countries like Britain are in the
Forefront of the sensible and
proportionate regulation of the use of
machine learning and other types of
artificial intelligence if we don't do
it now then I think we will be missing a
his an historic opportunity I wish you
all well and I look forward to meeting
some of you in future and discussing
these issues um as they develop thank
you very
much well thank you sir Robert who may
be watching the recording of this don't
be prescriptive he said let's sort out
some sensible
proportionate
regulation is that credible is that
feasible you'll be hearing from other
panelists who may be commenting on that
shortly so Robert also said there are
risks such as the inscrutability of AI
we don't understand often how they reach
its decisions we don't understand the
biases that might be there that might
have been planted we might lose charge
we might become so used to AI taking
decisions that humans and end up in a
very sad place but how bad could things
get that's what we're going to hear from
our next speaker so I'm going to ask
Connor Lei to come up to the stage while
I briefly introduce him Connor is the
CEO of conjecture if you haven't heard
about conjecture I think you need to do
a bit more reading perhaps Connor will
say a little bit about it they are a AI
alignment Solutions company
International but with a strong
representation
here in the UK so welcome Connor the
floor is
yours thank you so much it's so great to
see you all today so happy to be able to
talk to you here in person and man do we
live in interesting times to put it
lightly the world has changed so much
just in the last few years few months
even so much has happened in the world
of AI and Beyond just a mere couple
years ago there was no such thing as
chat gbt or even GB3 or four or two or
any of those it was a different world
not so long ago when technologists such
as myself weird little hobbyists worried
about the problem of
AGI and how it will affect the
world back then it still seemed so far
away it seemed like we still had
time but now we find ourselves in a
world
of unrestricted uncontrolled scaling a
race towards the Finish towards the end
to scale our AI systems ever more
powerful more General more autonomous
more
intelligent and the reason I care about
this is very simple if we build systems
that are smarter than humans that are
more capable at manipulation deception
politics making money scientific
research and everything else and we do
not control such
systems then the future will belong to
them not to
us and this is not the future I want I
want a future in which Humanity gets to
decide its Destiny or we get to de
decide future for ourselves for our
children for our children's
children that we like a future where our
children can live long happy lives
surrounded by Beauty art great
technology instead of being replaced by
soulless automata and let me be clear
that this is the default outcome of
building an uncontrolled AGI system a
full replacement of
mankind
and what we're seeing is that AI is on
exponential there's a
race all the top organizations which is
open AI Deep Mind anthropic among others
are racing ahead as fast as the VC
dollars were scale up their
work and this has given us an
exponential AI is on an exponential
curve both on hardware and on software
is improving at incredible
rates and when you're dealing with an
exponential there are precisely two
times you can react to it too early or
too
late there is no such thing as reacting
at just the right moment on an
exponential where you find just the
perfect Middle Point just in the nick of
time when everyone agrees that the
problem is here and everything has
perfect
consensus if you do this you are too
late it will be too late and the same
thing applies to AGI if we wait until we
see the kinds of dangerous general
purpose systems that I am worried about
then it will already be too
late by the moment such systems
exist the story of mankind is
over and so if we want to act we must
act well well before such things
actually come into existence and
unfortunately we do not have much
time how the world has
changed as frightening and as terrible
the race may
be there's also good
changes a few years ago I could have
barely imagined seeing governments
politicians and the general public
waking up to these weird nerd issues
that I carried about so much with my
friends online but now we're looking
forward to the first International AI
Summit convened by the UK and the famous
D Bletchley Park and this is great news
the European commission has recently
officially acknowledged the existential
risks from AGI along with the risks from
nuclear weapons and pandemics this is
great progress this is fantastic it is
good to see our governments and our
societies waking up and addressing these
issues or at least beginning to
acknowledge
them and we must use this opportunity we
have an opportunity right now and we
must prevent it from being
wasted because there's also bad news but
we're having this great opportunity to
start building the regulation and the
coordination necessary for a good
future the very people who are creating
these risks the very people at the heads
of these Labs these organizations
building these Technologies are the very
people who are being called upon by our
governments to help regulate the very
problem that they themselves are
creating and let me be very explicit
about this the problem that we face is
not AGI AGI doesn't exist
yet the problem we face is not is not a
natural problem either it is not an
external force acting upon us from
nature it comes from people from from
Individual people businessmen
politicians technologists at these large
organizations who are racing who are
scaling who are building these
Technologies and who are creating this
risks for their own
benefit
but they have offered us these very
people who are causing this problem have
offered us a
solution fantastic and they are pushing
it as hard as they can towards the UK
government and the upcoming Summit so
what is the solution the solution to the
problem of scaling these these Labs such
uh these accelerationist Labs such as
anthropic and Arc have been pushing for
what is the
solution well the solution to the
scaling problem is called responsible
scaling now what is responsible scaling
you might ask you see it's like normal
scaling except you put the word
responsible in front of it and that
makes it good
so of course I'm joking somewhat but
there's a lot of Truth in
humor responsible scaling is basically
the policy and you can read this on both
Arc or philanthropic website is the
policy proposal that we should continue
to scale uninhibited until at some
future time when tests and evaluations
that do not yet exist and we do not know
how to build but the labs promise US
they will build um detect some level of
dangerous capabilities that we do not
yet know and then once it gets to that
point then they will stop maybe except
there is a clause in the anthropic
version of the RSP paper in which they
say that if a different organization was
scaling even a super uh unsafely then
they can break this commitment and keep
scaling
anyways so this could be s sensible if
they committed to you know a sensible
bound a conservative point on which to
stop but unfortunately the uh
responsible scaling policy RSP uh fails
to actually commit to any objective
measure whatsoever
oops so effectively the current policy
is to just keep scaling until they feel
like
stopping this is the policy that is
being suggested to our politicians and
to The Wider world as the responsible
option for policy makers it is trying to
re it is very clear that it is trying to
recast this techn libertarian extremist
position as sensible moderate
responsible even now in my humble
opinion the reasonable moderate position
to when dealing with a threat that is
threatening the lives of billions of
people is to simply not do
that
but instead this is they're is trying to
pass off this as the sensible Middle
Ground
position the truth of
RSP is that it comes from the same
people who are causing these risks to
exists these people the heads of these
Labs many of the scientists and policy
people and other people working on this
have known about existential risks for
decades and they f admit this this is
not like they haven't heard about this
it's not even that they don't believe it
you can talk to them they're on the
record talking about how they believe
that there's a significant chance that
AGI could cause
extinction of the entire human species
in a recent podcast Dario amade CEO of
anthropic one of these Labs himself said
that he thinks it's a probably 25%
chance that it could kill literally
everybody and they're doing it anyway
despite this they keep doing it why well
if you were talking to these people what
they might tell you is is that sure you
know I know it's dangerous I'm very
careful but these other guys well
they're even less careful than me so I
need to be number one so I actually have
to race faster than everyone else and
they all think this about each
other they call this incremental but
they never pause they always race as
fast as they possibly can do as I say
not as I do there is a technical term
for this it's called
hypocrisy and RSP is no
different they are simply trying to
twist words in an aelian way to be
allowed to keep doing the thing that
they want to do
anyways which they themselves say could
risk
everybody I mean has respons POS in the
name must be
good when people like Sam Alman talk
about iterative deployment about how we
must iteratively release AI systems into
the wild so societies can adapt to them
be inoculated by them it sounds so nice
it sounds almost
responsible but if you're really trying
to inoculate someone you should let the
host actually adapt before you jam in
the next new pathogen in into their
weakened immune system as fast as you
possibly can but this is exactly what
Laboratories such as open AI Deep Mind
anthropic and tier 2 Labs such as meta
are doing with all the force they can
muster to develop more and more new
systems as fast as possible release them
as fast as possible wide as spread
possible now if open AI had developed a
gpt3 and then completely stopped forther
scaling focused all of their efforts on
understanding gpt3 making Mak it safe
making controllable working with
governments and Civil Society to adapt
the new problems F by the system you
know for years or even decades and then
they build
gbd4 yeah you know what fair enough I
think that could work that would be
responsible but this is not what we are
seeing all of these people at all of
these institutions are running a deadly
experiment that they themselves think
might cause
Extinction it is gain of function
research on AI just like viruses
developed and released to the public as
fast and as aggressively as
possible they're developing more and
more dangerous and more and more
powerful viruses as quickly as possible
and forcing it into everyone's immune
system until they
break there is no responsible gain of
function research for extinction Level
Threats there is no such thing we have
no control over such systems and there
is no responsible way to continue like
this and anyone who tells you otherwise
is
lying a lot has
changed the summit can lead to many
boring
outcomes just exchanges of diplomatic
platitudes as is often the outcome of
such International
events they have some good outcomes and
it can have some very very bad
outcomes success in the Summit is
progress towards stopping the
development of extinction level AGI
before we know how to control it most
other outcomes are neutral and bad
outcomes they look like policy makers
blindly and sheepishly
swallowing the propaganda of the
corporations to allow them to continue
their unconscionably dangerous gamble
for their own personal gain and Glory at
the expense of the entire
planet we owe it to ourselves and our
children to build a good future not
gamble at all on a few people's utopian
fever
dreams governments and the public have a
chance to regain control over their
future and this is very hopeful I wasn't
sure we were going to get it but the
summit speaks to this that people can
act that governments can act that Civil
Society can act that it is not yet too
late there's simply No Way Around It we
need to stop the uncontrolled scaling
the uncontrolled
race if we want a good
future and we are lucky because we can
do this we can cap the the maximum
amount of computing power going into
these AI systems we can have government
intervene and prevent the creation of
the next more most more more dangerous
more General more intelligent strain of
AI until we are ready to handle
it and don't let anything distract you
from
this there is no good future in which we
continue on this path and we can change
this
path we need to come together to solve
these incredibly complex problems that
we are facing and not let ourselves be
led astrayed by corporate
propaganda and I hope that the
governments and the Civil Society of the
world do what needs to be done thank
you
thank you Connor we'll take questions
from the floor in a moment I'll just
start off with a question I think may be
on many people's mind why would a super
intelligent AI actually want to kill
humans I have a super intelligent
calculator which has no desire to kill
me I have a super intelligent chess
playing computer that is no desire to
kill me why don't we just build as
responsible scaling an AI that has no
desires of its own because we don't know
how to do that why did Homo sapiens
eradicate Homer neanderthals and Homo
erectus and all the other species that
we share the planet with you should
think AGI not of as a calculator but as
a new species on our planet there will
be a moment where humanity is no longer
the only or even the most intelligent
species on this planet and we will be
out competed I don't think it will come
necessarily from malice I think it will
be efficiency we will build system
systems that make money that are
effective at solving tasks at solving
problems at gaining power these are what
these systems are being designed to do
we are not designing systems with human
morals and ethics and emotions they're
they're AI they don't have emotions we
don't even know how to do that we don't
even know how emotions work we have no
idea how you could get an AI to have
emotions like a human does so what we
were building is extremely competent
completely sociopathic emotionless optim
in machines that are extremely good at
solving problems extremely good at
gaining power that do not care about
human values or emotions never sleep
never Tire never get distracted can work
a thousand times faster than humans and
people will use these for many reasons
to help and people and eventually I
think Humanity will just no longer be in
control questions from the floor there's
a lady in the third row down here just
wait for the mic sorry so that uh
the audience online can hear you thank
you Susan Fel from Fel consult uh to
stop the arms race certainly at a
geographical level I mean in nuclear um
uh the states and and Europe can tell
which countries are building nuclear
weapons and what they've got and they
can do tests um if computing power is a
thing that needs to be capped um to to
to slow this down enough uh is there a
way to U monitor what other countries or
um people in a clandestine way are doing
and how does how does that work this is
a fantastic question and the extremely
good an news is yes the at least
currently this will change in the near
future but the current state to build
Frontier models requires incredibly
complex machines massive super confu
that take megawatts of energy so this is
on the order you'd have of like a you
know a nuclear centrifuge facility so
these are massive huge Mach maches that
are only Built by basically three or
four companies in the world there are
very very few companies and there's and
there is Extreme bottlenecks on the
supply chain you need very very
specialized infrastructure very
specialized computer chips very
specialized Hardware to be able to build
these machines and these are produced
exclusively by countries basically in
the west and Taiwan there is many ways
where the where the us or other
intelligence Services can and already
are intervening on these Supply chains
and it would be very easy to monitor
where these things are going who is
buying them where is energy being drawn
in large scales so it is not easy and
the problem is is that AI is on
exponential both with hardware and with
software eventually it will be possible
to make exential dangerous AGI at on
your home laptop probably maybe not but
it seems
plausible if we get to that world we're
in big trouble so this is part also why
we have to buy time we have at some
point there will be a cut off where
we'll have algorithms that are so good
that either we have to you know stop
everyone from having a PlayStation at
home which doesn't seem that plausible
or at that point we have to have very
good Global coordination and
regulation thanks just pass the mic
behind you there's a person in the RO
behind Robert Whitfield from one world
trust can I ask about Bletchley Park do
you know I are you participating
and and if not do you know anybody else
with the similar views to you who is
participating I can't comment too much
since it's behind it's closed do so it's
a very private event unfortunately so I
don't think I'm at Liberty to talk about
exact what I know I think the guest list
is uh not not public I don't know most
of the people who coming I know the
obvious ones you know all the CEOs of
all the top Labs of course are attending
I'm is not a secret um I don't know who
if anyone of my reference class is
attending uh I just passed the mic next
to you Robert thank you perhaps I can
answer that question anybody that has
read the guardian today there is an
interview with M
Clifford and for the very first time not
for the second time it has been
clarified that there will be only about
a 100 people participating in on the
first day uh anybody is invited
including China on the second day
apparently there will be only the
Coalition of the Willing so those who
sub subscribe to the Frontier Model
Forum they will sit on this second day
that's that's the current my my main
impression from that article is
generates very positive and I would say
I've been surprised as you will be
surprised that the UK government is
really doing what it can to get the
mission to what the title of the
conference says the AI safety Summit
it's not about regulation about
controlling Ai and they're trying to do
their best the problem is as outlined in
that interview is that we seem to be
alone we have the states a little bit
but the rest wants to go their own way
and do it on their own territory which
is I think an the
I agree sooner or later International
coordination around these issues will be
necessary it as simple as that if you
want Humanity to have a long good future
we need to be able as a global
civilization to handle powerful
Technologies like this take a question
right from the
back hi I to ask in terms of legislation
uh what kind do you think is most
effective I've heard for example that
liability law takes too long to actually
have an effect
and compute governance generally seems
to be um very easy to be called
totalitarian uh what do you think of
legislation such as um models must be
released with a version before
pre-processing and the attacks on the
number of harmful outputs uh done by the
model before the pre
processing I am open to many kinds of
Regulation per se I would strongly
disagree with this description of
compute governance this is like saying
that you know not being private citizens
not having nuclear weapons is
totalitarian I respectfully disagree I'm
quite happy that people do not have
private nuclear weapons and I do not
think that people should have private
agis so similarly um I think liability
is very promising I think it has to be
strict liability so liability for
developers rather than um just users
this aligns the incentives of developers
with those of wider Society the point of
liability is to price in the negative
externalities for the people actually
causing them so I'm a big fan of of this
um a third for a policy I would also
suggest is a uh Global AI kill switch so
this would be a protocol where some
number of countries or large
organizations participate and if some
number of them decide to activate this
protocol all major deployments of
Frontier models must be shut down and
taken offline and this should be tested
every six months as a fire drill for 5
minutes to ensure full um so that
hopefully we never need it but if we do
that at least the protocol exists thank
you very much there are lots of hands up
hold your questions there would be more
chance for Q&amp;A later Conor final remarks
before we hand over to the next speaker
I want to really say that I do agree
that it is very is very hopeful to see
that the UK is trying to do things and
is trying to push us forward into a good
world because what we really need as I
said as I said briefly what we need is
as a civilization to mature enough to be
able to handle dangerous technology even
if we don't build AI right now at some
point we will build something so
powerful that it can destroy everything
it's just a matter of time our
technology keeps becoming more powerful
and the only way for us to have a
long-term good future is to build the
institutions the
civilization the world that can handle
this that can not build such things that
can not hold the trigger and I do think
this is possible
I do think that it is in so I have heard
in the interest most people to not die
so I think there is a natural Coalition
here but it is hard and I will not deny
this is an extremely challenging problem
almost unlike I mean basically something
we haven't faced since nuclear
proliferation and even then it's even
worse this time it's incredibly
difficult problem it is not over yet but
it could be very soon if we don't act if
we let ourselves get distracted if we
fall for propaganda and all these things
these opportunities can be gone and that
will be it but the game's not over yet
so let's do it thank you very
[Applause]
much so we've heard from a politician a
senior politician we've heard from a
technology entrepreneur and activist
we're now going to hear from our
professor who's zooming in all the way
from Kentucky from the University of
Louisville he's an expert he's written
several books on cyber security computer
science and artificial super
intelligence ah Roman I see you on the
screen I hope you're hearing us tell us
can we control super intelligence over
to you no the answer is no I'll tell you
why in a few
minutes that's fine so you can share
your slides or talk talk to us whenever
you're ready let's do the slides Conor
did a great job with
um
his presentation let me see one second
here there screen in the meantime we can
see the covers of some of your books in
the background artificial intelligence
Safety and Security and artificial super
intelligence we're now having a slight
Tech techical issue as the technologist
is found the slides great okay yeah
that's the hardest part if I can get
slides going the rest is
easy okay so uh I didn't know what
Connor is going to talk
about uh he did a great job he's a deep
thinker and covered a lot of important
material I will cover some of the same
material but I will have
slice and uh I will slightly take it to
the next level where I may make Connor
look like an optimist so let's see how
that
goes uh to begin with let's look at the
past well over a decade ago predictions
were made about the state of AI based on
nothing but compute power Ray kwell
essentially looked at this scalability
hypothesis before it was known as
session said by 2023 we will have
computational capabilities to emulate
one human brain by 2045 we would be able
to do it for all of humanity so we are
in 2023 let's look at what we can do in
the
present in the spring of this year a
program was released which I'm sure many
of you got to play with called GPT 4
which is not a general intelligence but
it performs at a level Superior to most
humans in quite a few domains if we look
specifically at this stable of different
exams lower exams medical exams AP tests
J tests it's at 98th 99th percentile of
performance for many of them if not
most that is already uh quite impressive
and we know that there are models coming
around which are not just text models
but multimodal large models which will
overtake this level of performance it
seems like GPT 4 was St and its training
process right right around this uh human
capacity and if we were to train the
next model GPT 5 if you will will
quickly go into the Superhuman territory
and by the time the training run is done
we would already be uh dealing with
super intelligence out of the box but
let's see what uh the future holds
according to uh heads of top Labs
prediction markets so we heard from from
CEO of anthropic CEO of Deep Mind they
both suggest that within two or 3 years
we will have artificial general
intelligence meaning systems capable of
doing human beings can do in all those
domains including science and
engineering it's possible that they are
overly optimistic or pessimistic
depending on your point of view and so
we can also look at prediction markets I
haven't grabbed the latest slide but
last time I looked prediction markets
also had 3 to four years before
artificial general intelligence which is
very very quick why is this a big deal
uh this technology at the level of human
capability means that we can automate a
lot of dangerous malev and behavior such
as creating biological pandemics new
viruses nuclear Wars and that's why we
see a lot of top scholars in influential
business people in fact thousands of
computer scientists all signed this
statement saying that yes AI will be
very very dangerous and uh we need to
take it uh with the same level of
concern as we would nuclear war so what
is the problem everyone is concerned
about the problem is that uh for one we
don't agree on what the problem is early
in computer science early in the history
of AI uh concerns were about AI ethics
how do we make software which is ethical
and moral and there was very little
agreement nobody solved anything but
everyone proposed their own ethical
system gave it a name and described what
they had in
mind uh about a decade ago we started to
realize that ethics is not enough we
need to look at safety of those systems
so again we started this uh naming
competition we had ideas for friendly AI
Control problem value alignment doesn't
really matter what we call it we all
intuitively kind of understand we want a
system which if we run it we will not
regret running it it will be beneficial
to us so how can Humanity remain safely
in control while benefiting from
Superior form of intelligence is the
problem I would like us to look at we
can call it control problem and the
state of the art in this problem in fact
we don't really know if a problem is
even solvable it may be partially
solvable unsolvable maybe it's a silly
question and the problem is undecidable
a lot of smart people made uh their
judgments known about this this problem
unfortunately there is little agreement
answers range from uh definitely
solvable from a surprising Source like
Alazar rovski to the retractable from
head of super alignment team at uh one
of the top Labs two I have no idea from
a top touring Award winner who created
much of uh machine learning Evolution so
I think it's an important problem for us
to look at to address and to understand
how we can best figure out what is the
status of the problem my approach to
that is to think about the tools I would
need to control a system like that an
intelligent very capable Ai and and uh
the tools I would guess I would need are
ability to explain how it works
capability to comprehend how it works
predict Its Behavior verify if the code
follows design be able to communicate
with that system and probably some
others but maybe some of the tools are
interchangeable so I did research and I
published results on each one of those
tools um the results are not very
optimistic for each one of those tools
there are strong limits to what is
capable in the worst case scenarios when
we're talking about super intelligent
systems self-improving code systems
smarter than human capable of learning
in new domains it seems that there are
limits to our ability to comprehend
those systems or systems to explain
their behavior the only true explanation
for an AI model is the model itself
anything else is a simplification you
are getting a compressed lossy version
of what is what is happening in a model
if a full model is given then you of
course would not comprehend it because
it's too large too complex it's not
surveyable so there are limits to uh
what we can understand about those black
box models similarly we have limits to
predicting capabilities of those systems
we can predict general direction in
which they are going but we cannot
predict specific steps for how we going
to get there if we could we would be as
intelligent as those systems if you're
playing chess against someone and you
can predict every move they're going to
make you playing at the same level as
that opponent but of course uh we made
an assumption that a super intelligent
system would be smarter than us there
are similar limits to our ability to
verify software at best we can get
additional degree of verification for
the amount of resources contributed so
we can make systems more and more likely
uh to be reliable to have less bugs but
we never get to a point of 100% Safety
and Security and I'll explain why that
makes a difference in this domain
likewise human language is a very
ambiguous language it's not even as
unambiguous as computer programming
languages so we are likely May uh to
make mistakes in giving orders to those
systems all of it kind of leads us to
conclude that it will not be possible to
indefinitely control super intelligent
AI we can trade keep capabilities for
control but at the end if we want very
very capable systems and this is what
we're getting with super intelligence we
have to surrender control to them
completely if you feel that the
impossibility results I presented were
just not enough we have another paper
where we cover about 50 of those
impossibility results it's a large
survey in a prestigious Journal of ACM
surveys from the beginning of uh history
of a with founding fers like Alan Turing
who said that he expects the machine
will take over at some point to Modern
leaders of AI like Elon Musk who say who
says we will not control them for sure
there is a lot of uh deep thinkers
philosophers who came to that exact
conclusion we are starting to see uh top
Labs published reports in which they may
gently acknowledge such scenarios they
call them pessimistic scenarios where
the problem is simply unsolvable we
cannot control super intelligence we
cannot control it indefinitely we're not
smart enough to do it and it doesn't
even make sense that that would be a
possibility they ask uh well what's the
distribution what is it uh what are the
chances that we're in a universe where
that's the case and they don't provide
specific answers but it seems uh from
some of the writing and posts they make
that maybe about 15% is allocated to
that possibility I was curious to see
what uh other experts think so I made a
very small very unscientific survey on
social media I surveyed people in my
Facebook group on AI safety and I
surveyed my followers on Twitter and it
seems that about a third think that the
the problem is actually solvable
everyone else thinks it's either
unsolvable or it's undecidable or we can
only get partial Solutions or we will
not solve it on time so that's actually
an interesting results most people don't
think we can solve this problem and I
think part of the reason they think we
cannot solve this problem is because
there is a fundamental difference
between kind of
standard cyber security safety and super
intelligence safety and Security even if
you
fail it's not a big deal you can issue
new passwords you can provide someone
with a new credit card number and you
get to try again we suspect strongly
with super intelligence safety you only
get one chance to get it right there are
unlimited d uh dangers unlimited damages
either you have existential risks or
suffering risks and we kind of agree
that 100% is not an attainable level of
of security verification
safety but anything less is not
sufficient if a system makes a billion
decisions a minute and you only make
mistake once every couple billion
decisions after a few minutes you are
dead and so this is like creating a
perpetual motion machine you trying to
design a Perpetual safety machine while
they keep releasing more and more
capable systems GPT 5 GPT 50 at some
point this uh game is not going to end
in your favor so I'm hoping that others
join me in this line of research we need
to better understand what are the limits
to controlling super intelligent systems
is it even possible my answer is no but
I would love to be proven wrong it would
be good to have surveys similar to the
ones I conducted on larger scale uh to
get much more statistically significant
results and uh in case we do agree that
we have this worst case scenario where
we are creating super intelligence and
it is impossible to control it what is
our plan do we have a plan of action for
this worst case scenario this is what I
wanted to share with you and I'm happy
to answer any questions thank you very
much
[Applause]
Roman can you give us optimistic
Roman sorry one second I'm trying to
figure out how to use
zoom uh go ahead and repeat your
question
please you gave us many reasons to be
anxious what you think is the best
reason for us to be
optimistic well there seems to be many
ways we can end up with World War II
recently so that can slow down some
things it has been suggested that we can
use a different kind of tool which is
the kill switch your list of tools that
you listed it didn't include that it's
been proposed that each AI system should
be tested with a uh remote off switch
capability have you looked at that do
you think that's a viable
option so I would guess a super
intelligence system would outsmart our
ability to press the off button and time
it will work
for not super intelligent AIS pre AGI
systems maybe even for AGI systems but
the moment it becomes that much more
advanced I think it will outsmart us it
will take over any kill switch options
we have let's have some questions from
the
floor
uh well I can't see the hands so yes
just give the microphone out thank you
thank you I would like to ask how does
the scalable scalable oversight that
open AI is working on essentially the
way they plan to align super
intelligence fit into your expectation
of the future pathway the AGI will take
because again as personally we cannot
align or control a super intelligent
entity but another AI which is more
capable than us could so how does that
fit into your expectations so it seems
like it increases complexity of the
overall system instead of us trying to
control one a I now you're trying to
control a chain of Agents going from
slightly smarter to smarter to super
intelligent maybe 50 agents in between
and you're saying that you have to Sol
solve alignment problem between all the
levels communication problem ambiguity
of language between all those models
supervision it seems like uh you are
trying to get safety by kind of upating
how the model actually works you're
introducing more complexity hoping to
make the system easier to control that
seems
counterintuitive but isn't it the case
that sometimes you can verify an answer
without understanding the mechanism by
which the answer was achieved for
example there can be a chess puzzle and
you no way of working out yourself but
when somebody shows you the answer you
can say oh yes this is the answer so
isn't it possible we don't need to
really understand what's going on inside
these systems but a simple AI can at
least verify the recommendations that
come out of the more complex AIS so such
a chain may be the solution
can you claim that you are still in
control if you don't understand what's
happening and somebody just tell you
tells you don't worry it's all good I
checked it for
you but then it's like we humans we have
a network of trust when I trust some
people and they trust others within
various categories we can't work out
everything ourselves but we trust some
scientist or some Engineers or some
lawyers who validate that an AI has a
certain level of capability and that AI
could come back with verification that
the proposals of a super intelligence
should be accepted or should not be I
don't say it's easy but as you said
there's not likely to be a very simple
and straightforward
solution again to me at least it sounds
like instead of trying to make this
system safe you said that you made some
other system safe and it made sure that
the system you couldn't make safe is
safe for
you let's take more questions there's
another one in the middle here and then
we'll go to the edge yes thank
you thank you for the presentation
number one second thing is that uh as
you're talking about I think as David
was talking about Trust basically right
could you tell me from your extensive
years of AI research and experience as
such that do you really think that
humans or Society can be trusted to Val
for example uh regulate its own self or
do you think that really need some sort
of institution of sort that is totally
separate for any anyone
else so I'm not sure regulation would be
enough uh Connor correctly pointed out
that there is both lobbying of
Regulators by the top labs and also it
becomes easier and easier to train those
models with less compute and over time
you will be able to do it with very
little resources The Only Way Forward I
see is personal self-interest if you are
rich young person and you think this is
going to kill you and everyone else
maybe it's not in your best interest to
get there first that's really the only
hope at this point just personal
self-interest but humans are always
better if we can band together with our
self-interest rather than each of us
individually pursuing our self-interest
so I think this kind of meeting in the
community Spirit might help uh there was
a hand over here yes with I think the
red uh shirt or
jacket yeah um if if we assume that the
two kind of well both of yous that have
been suggested so so far are correct in
that we're definitely not going to be
able to stop uh AI development Etc and
we're going to get to the point where we
have no regulation that can effectively
stop things you know people can build in
super intelligent AI on their own
computers Etc okay so we'll assume that
that's a fact that's coming and then
we'll also assume that the control
problem isn't a problem because it's a
problem that can't be solved and we're
definitely not going to be able to
control it well now we're heading and
barreling towards the point where we
have super intelligent AIS definitely
and we definitely can't control them
what comes
next what comes
next It's a Wonderful question as I said
and published you cannot predict what
the super intelligence system will do
right so was there a question down here
thank
you um you said that we kind of need a
plan building on that last question if
that if that scenario is true you said
we need to do more work in this area but
do you have any thoughts as to what we
should be doing what we should be doing
to plan for the worst case
scenario so to me at least it seems that
at least in some cases it is possible to
use this idea of personal self-interest
if you have a young person having a good
life there is no reason why they need to
do it this year or next year I
understand that someone may be in a
position where they are very old very
sick have nothing to lose and it's much
harder to convince them not to try but
at least from what I see the heads of
those companies are all about the same
age they're young they healthy they they
have a lot of money there is a good way
to motivate them to wait a little bit
maybe a decade or two um just out of
personal self-interest again I think my
answer to the question of optimism is
that we humans can do remarkable things
we humans can solve very hard problems
and so I want to say now that we spread
around what the problem is at least some
more people can apply more brain power
to it so that's my reason for
optimism
Terry I guess I'm uh pleased by the
inevitability of this
development because it seems to me that
if you're going to create reasoning
creatures then those reasoning creatures
are going to have moral rights on the
same plane as human beings so um so I'm
looking forward to uh to chatting with
these creatures and joining in them
joining into this kind of discussions
and I'm pleased that they won't be able
to be th and it would be wrong to
enchain these reasoning
creatures so Roman are you looking
forward to having more of the AI
involved in these discussions as
well so I remember giving a presentation
for a podcast about rights for Animals
rights for AIS and I was very supportive
of all the arguments developed because I
said at one point we will need to use
those arguments to beg for our rights to
be retained
question on the third draw here yes hi
uh I'm curious Roman uh which side of in
your hopes of a possible future for us
to get through this do you have more
Hope on the side of a more top down sort
of totalizing control system for AGI
systems such that they remove the
possibility of individual actors getting
hold of this and weaponizing it or do
you put more hope in a more more sort of
decentralized open- source approach to
AGI emergence more like an ecology
perhaps some people suggest would be
more biologically inspired such that you
know immune immune system like functions
could
arise which way do you lean in your
sensibilities for what is a viable
Avenue for us I'm not optimistic with
either of those options the only kind of
hope I see is that for strategic reasons
super intelligence decides to wait to
strike it will not go for immediate
treasurous turn but decides to
accumulate resources and trust and that
buys us a couple decades that's the best
hope I see so far so if we slow things
down we'll have more chance to work out
Solutions and the slowing down might
come from a combination of top down
pressure and bottom up
pressure maybe have a is there a hand at
the very back there yes let's try and
get the microphone back
there
right at the sitting at the back yes
right uh at the sorry at the in the
middle
sorry thanks hello hi Roman thanks for
your talk um yeah I was wondering what
your thoughts are on aligning the first
AI that is human level or n narrowly
superum if in principle that is possible
and and if that is is uh is it possible
in principle to align the next version
of AGI uh but to use that narrowly
superhuman AI to align it and if if
that's all technically possible then uh
why would we not think like focus on
doing
that and also and also if you think in
principle uh alignment is impossible and
uh control is impossible
then uh why why not work on practical
ways to make the to make whatever AI is
created as nice as possible that is like
better than the counterfactual of uh try
to stop it it won't stop and you know it
won't be
nice well I I definitely encourage
everyone to work on as much safety as
you can anything helps uh I would love
to be proven wrong it would be my
greatest dream that I'm completely wrong
and somebody comes out and says here's a
mistake in your logic and we have
developed this beautiful friendly safe
system capable of doing all these
beneficial things for Humanity that
would be wonderful but so far I haven't
seen any progress in that direction what
we're doing right now is putting
lipstick on this monster and the shog of
uh that's all we're doing filters to
prevent the model from disclosing its
true intentions then you talk about
alignment it's not a very well well
defined terms what values are you
aligning it with values of heads of that
lab uh values of specific programmer we
as humans don't agree on human values
that's why we have always Wars and
conflicts there is uh 5050 split and
most political issues in my country we
are not very good at agreeing even with
ourselves over time what I want today is
not what I wanted 20 years ago so I I
think this idea of being perfectly
aligned with a Ian agents and people are
suggesting adding animals to it and
aliens and other AIS that doesn't seem
like it's a workable proposal our values
are changing they are not static and
it's very likely that they will continue
changing after we get those systems
going uh I I don't see how at any point
you can claim that this system is
specifically value align with someone in
particular the last question in this
section is going to go to cor
Lei Roman love your talk I always love
your optimism it's always great to hear
you talk so um so I'm kind of like gonna
pick up on the question that was just
asked and just give a bit of my opinion
and kind of like hear what you think
about this as well
so my personal view is that I I do I
have read many of your papers in fact
and they're quite good so I do think
that I agree with you that like in
principle and arbitrarily intell system
cannot be safe by any arbitrary like
weaker system just kind of by proof of
like you know program size induction and
whatnot um but in my view it does seem
likely that there is a limit of
intelligence far below the theoretical
Optimum but still significantly above
the human level that can be achieved the
reason I think this is that human
civilization is actually very smart
compared to a single caveman
and can do really really great things so
my point of optimism is it seems
possible that if we stop ourselves from
making self-improving systems and
coordinate at a very strong scale and
have very strong enforcement mechanisms
it should be possible to build systems
that are you know end steps you know
above human good enough to build you
know awesome you know sci-fi culture
ship kind of like worlds but not further
I'm wondering if if you have an
intuition about
like where to things hit impossibilities
like to me I think the impossibilities
happen above human Utopia but to get to
the Utopia bit you already have to do
extremely strong coordination extremely
strong safety research extremely strong
interpretability extremely strong
constraint on the design of the agis
extremely strong regulation which I
think is in principle possible just
wondering kind of like your thoughts
about that kind of outcome so con is not
asking about responsible scaling he's
asking about limited super intelligence
if we had limited super intelligence
could we get everything we want without
having the risks that we all fear so I I
think I want to emphasize difference
between safety and control is it
possible to create a system which will
keep us safe in some somewhat happy
state of preservation possible are we in
control no that's system as the example
you give of humanity so Humanity
provides pretty nice living for me but
I'm defitely not in control if I
disagree with society and many issues in
politics and culture it makes absolutely
no difference I don't decide things
scale it to the next level all eight
billion of us may want something but
this overseer this Mo aleren system says
it's not good for you we're not going to
do it uh this is what you're going to be
doing right now so think about all the
decisions you make throughout your day
uh you decided to eat with donut you
smoked with cigarette all those decision
decisions were made by you because you
felt you wanted to do them they may be
good or bad decisions but if you had
this much more intelligent personal
adviser ideal adviser you would be at
the gym working out eating carrots you
may have a long healthy life but you're
not in control and your happiness level
may be
questionable thank you very much Roman
for sharing your thoughts pessimism and
some optimism uh thanks
for moving the conversation
[Applause]
forwards I'm now going to invite the
five five members of the panel to come
up on stage and they're each going to
have a couple of chances to pass some
comments and what they've heard so
there's some stairs over there that you
can come up to uh we're going to hear
from Yan Talon who is the co-founder of
Skype the co-founder of fli future of
Life Institute and also Caesar the
center for study of existential risks
we're going to hear from Eva Barons a
policy analyst with the International
Center for future Generations we're
going to hear from Tom o who's a
journalist who writes from time to time
for the BBC amongst other places we're
going to hear from Alexandra mua
vadic who is the CEO of evident and uh
has a track record with tortoise media
and many other places and we're going to
hear from also another representative
from conjecture that's Andrea miotti who
is their specialist for AI policy and
governance so to start things let's just
hear from each of them a a few opening
remarks Yan what's your comments from
what you've heard so far have you
changed your mind in any ways or are the
things that are missing from the
conversation yeah you all have to speak
into the mics I'm being told so um I
yesterday I was at a dinner I was
invited to a dinner and and uh my
response to invitation was that okay I
will come but you have to invite
Connor uh because he's making uh very
similar points to me only much much more
intensely so yeah I basically agree I
agree with what what Connor said uh my
main caveat would be that I for the last
decade or so I've been kind of trying to
build um L of friends the cooperation uh
between uh people in the kind of AI
companies and and making sure that like
I mean everybody can understands that it
is in their interests I mean almost
everybody uh let let's be honest almost
everybody understands that it's in in
their interest uh to kind of remain in
control and and uh not kill everyone
else and uh uh
so like for example anthropic I a board
obser Observer to anthropic and
anthropic is one of those companies just
like conjecture when you go there you
can talk to anyone from the receptionist
to the to the CEO and they aware of the
AI risk and very concerned uh about this
uh but yes uh I do think as I've said uh
in several places that uh I don't think
they should be doing what they're doing
um so these companies don't really want
to do what they're doing but they feel
they have to otherwise they might be
left behind yes so there is this U like
basically dilemma in when you want to do
of safe AI one is that you're or safe
like when you're trying to figure out
how to do safe AI uh from one hand you
have groups like Miri uh that Kowski
co-founded and that was the person who
got me involved in AI safety 15 16 years
ago uh where basically the claim is that
you have to start really early even if
you don't know exactly what AI is going
to look like like because then you have
a lot of time to prepare and then the
group on the other end of that axis is
anthropic where they say that it's kind
of useless to start early because you
don't know what you're dealing with uh
so you need to be as informed as
possible so in that strategy you need to
be just always at the frontier and and
Dario has been very public about this
about this strategy of course the
problem there is that like it also works
as a perfect justification to race rate
so so uh therefore it's
uh I have like double digit uncertainty
both ways uh about what it what the
actual uh picture is uh so I do think at
this point the labs indeed they are
involved in Death Race and they they
there is that government intervention
needed uh to to get a timeout there and
we definitely need timeout because we
don't have uh enough safety results uh
and but to yeah Roman yosi's
uh presentation I I'm definitely more
optimistic again as as on one of these
slides there was the uh dialogue he had
with elzer and elzer was confident that
this can be solved and in fact like I'm
super glad that earlier this year uh
David darle uh his group got UK
Government funding and he has this
approach called uh open agency
architecture I don't know exactly uh
what the details there are but like my
rough understanding is that you you're
using uh you're scaling AI capabilities
and access uh according to formal
statements that AI produce and then you
use not AI not humans but formal
verifiers to to verify uh those those
statements therefore like building up
your kind of AI capabilities one
formally verified Step at the time uh
there are many criticism of that but
this like one of those approaches that
is kind of at least and principle uh has
like
some convincing story that that why it
should work in in principle at least so
there are some options that might work
but we're going to need time to develop
them exactly so that's why I've been
working on like been supporting AI
Safety Research for more than a decade
now but unfortunately we just didn't
make it uh we we now need more buy more
time so let's hear from Eva because you
work more with possibilities to inspire
policy you've seen examples of policy in
the past slowing down some technological
races
are you do you see reasons for optimism
do you see ways in which politicians can
make a good difference to the landscape
we're discussing definitely definitely
that very much plays into some of the
problems or the issues characteristics
of the problem that both Corner spoke
about and that also Yan tan just
mentioned that one of the problems that
we're facing here is a human
coordination problem and one of the ways
to address that would be through policy
as has been said many times this evening
this is a technology that threatens to
kills us to kill us all um and the the
heads of the government of the companies
that are driving forward the technology
have agreed that and publicly stated
that that might be the case and yet they
seem to be locked into this dilemma that
Yan just mentioned where they are for
some or other reason impossible to to
stop so I think that is a point where
where government can really make a
difference and step in and also should
step in um and we've seen that as you um
hinted at we've seen that work in the
past one of the examples that I often
think about is um the Ral protocol which
um after um the scientific consensus
arose that uh cfc's and other similar
gases actually destroy the ozone layer
the International Community did come
together in 1987 and agreed with through
the Montreal protocol to slowly phase
out these gases so um we see here that
International cooperation by the
International Community by governments
can succeed also in the face of the
short-term economic interest of private
sector companies in the public interest
of um well in the end everyone on Earth
so I'm not saying that it's necessarily
EAS easy but I think it's definitely
possible and it is one of the strongest
levs that we have here to make a
difference so I think that's definitely
that we something that we should lean
into very strongly and um do our best
that that actually happens the montal
protocol is an encouraging example but
we haven't made a very good job of the
governments in the world of controlling
carbon emissions we've been talking
about it for a long long time my man
maybe there's some progress but many
people feel this is an example where
governments can't cooperate so what
makes you think that we can cooperate
with problems of AI more like the
Montreal protocol rather than the Paris
agreement say well uh part of this of
course is also that I think that this is
one of the few levers that we have to
make a difference at all so I also hope
that we will be able to do it and I
agree that um looking at past climate
conferences one of the um negative
examples that we see there is that with
these conferences sometimes times that
the outcomes tend to be very watered
down just because the the focus lies on
building consensus among all of the
different countries that attend and then
in the end you want to have a nice
little consensus agreement that everyone
signs so you can demonstrate that
everyone's on the same page and everyone
goes home and everyone's happy happy and
I can just say that I think with the Uki
Summit that's coming up now first of all
that is a unique opportunity to actually
have international cooperation and
coordination on this issue take place
you need to create the opportunities for
stuff like that I'm really happy that
the UK government took uh the initiative
and created this opportunity I am um one
thing that makes me optimistic is that
we all know that China is going to
attend at least on one of the days um so
hopefully they will be able to be
brought into the fold and um yeah then I
just hope that this opportunity is truly
taken and that the outcome of this uh
Summit will not be just some vague
commitments to long-term plans but
ideally concrete Bing binding
commitments to to concrete next steps
let's turn to Alexandra
Alexandra you work a lot with businesses
businesses are unsure in many ways how
to deal with today's AI do you think
there is good advice that they can be
given or is there a sleepwalking process
with many of our
businesses definitely the
latter I would say um so I'm CEO of
evident we map Benchmark companies on
how far they are in their AI adoption
and so when um I think it was Connor you
mentioned the AI race that is on at the
front line of development um in AI there
is also a race as we all know going on
in terms of adopting AI as as quickly as
possible there's a sense of being
there's a geopolitical debate on AI
development
between us Europe China and so on um and
who's leading on that not only in AI but
also in areas like Quantum but in the
business level which is where I deal
with with um spend my time uh mostly
there's a definitely a Ron in terms of
not being left behind in adoption of AI
and it's an economic question it is a
existential question so there's an
existential question on sort of two
dimensions in this debate and um and so
you've got this um
Unstoppable race going on on the front
end of AI and then you've got an
Unstoppable race on actual deploying AI
at a business level
and um it's going to be very hard for
Regulators to keep up and to Eva's point
I think um in terms of what we hope will
be the outcome often unfortunately comes
with with a catastrophic um happening
taking place before it really sharpens
the minds and uh people figure out um
how urgent it is I think there's a real
sense of urgency in in the community
around trying to work out uh what what
the guard rail should be whether it
should be a con tion um or or how we
should think about implementing safety
mechanisms in as as we develop um
further on our chat on our large
language models but um I hope it doesn't
need a catastrophic moment for that to
sharpen but back to the business
question there is this um hope that
maybe businesses will self-regulate and
I think that is maybe the case in highly
regulated sectors you see in the banking
um sector and insurance or banking in
particular that there is a guard rails
put in place there but that is um that
there's a lot of businesses that don't
have that regulation around them and I
think there is a real risk for this
completely running out of control at a
business level as well would you advise
businesses to self-regulate ahead of the
standards and regulations being agreed
by governments I think that's what
they're doing or trying some businesses
are trying to do there's a big uh risk
in um Ina in the case of TR winning
trust with your customers and also your
your shareholders and and and investors
if you mishandle Ai and you create
issues around not taking into account
how to properly deal with biases and
other issues that is uh a situation that
can create a real breakdown and trust
with your with your organization so
there is that risk and then there are
businesses that don't necessarily lean
on trust for their for their uh for
their business and those are the ones I
worry the most about indeed let's turn
to Tom o uh as a representative of the
world of Journalism do you feel
journalists have helped the discussion
about the existential threat from AI or
have they muddied the water leading
people to panic unnecessarily or perhaps
get distracted on side issues rather
than the main
issue I think it's um
all of the above aside from overaging
the pudding I think most people in this
room including me have had a wit scared
out of them um by some of the talks just
now
um one has side issues um in
journalistic coverage of AI um and I
think um the jobs Market is one of those
um but I have been um surprised
pleasantly so um by how things have
progressed since 2016 and that the first
time that I wrote about about AI safety
and I think at that point the prospect
of um a bad scenario relating to AI was
seen as about as likely by my colleagues
as Leicester City winning the Premier
League um anyway several years later um
I now see lots of my former colleagues
writing to my mind um very informed
pieces about AI safety and I think
that's helped the public change well
arrive at a view um and probably a lot
of people in this room are aware that
the American public when Paul now says
that they want regulation of AI and they
want a lot of it and I think we can
credit journalism with some of that
journalism should be doing more but it's
more than I would have thought a few
years ago and if you were to go away and
write up a story about things that you
might have changed your mind about
tonight and that the public should pay
attention to can you give us a sneak
preview what that would
include well I think um the idea of
runaway AI is is not new um but I think
it has been difficult historically to
frame it in a way that really sticks and
like really drives its way down your
brain stem um and we have different ways
of framing AI risk um and Mustafa
suman's new book um which some of you
might have read I think does a pretty
good job of framing it in a in a in a
way in which he describes um AI being
used to accelerate human Ingenuity in
what whatever Endeavors humans are up to
um be they um be they good or be they
bad that's one way of framing it um and
I think um we've heard some pretty
compelling ways of telling a story of
runaway AI which is a a different and
scarier
story thanks and let's turn to Andrea
and your role at conjecture what are you
doing in a day by day basis to address
this question well what we're trying to
do and Ian counter covered a lot of it
is first of all to explain the problem
to people uh I've been heartened by the
public reaction in the last years like I
also got to know about this problem
quite a long time ago and I in the past
I could almost not expect today that
major governments take this problem
seriously and the public understands
this problem and we all get together and
take some initial promising insufficient
but promising steps to address it uh uh
another thing is figuring
out policy Solutions and the reality is
that we don't obviously we don't have a
playbook for what exactly they look like
but what what I think was a common theme
of the of the talks tonight is that
clearly at some level of power we are
not in control anymore and everybody
expects this uh those who don't expect
this are misguided or expected but don't
say it and the positive thing is that
there is one physical resource that
drives the majority of what makes these
systems powerful which is computing
power and it's a physical resource not
not like you know algorithms that you
could just write on a piece of paper uh
it's traceable it's expensive large plac
in data centers uh and while you know
the scaling hypothesis the idea that you
know the more computing power you put
into something the more powerful it
becomes might H some diminish ter at
some point we do not see any reason to
expect it to stop so we know you know
from both sides companies know that more
computing power leads to more power and
that's why they're doing what they're
doing we know that limiting the
computing power is a very effective way
to kind of stem the the bleeding and
stop and or POS the situation for a
while take a time out have the time to
figure out the solutions have the time
to absorb this into society and but how
much time will that give us because
there's a risk that people will use
today's models to design much more
efficient ways to build Next Generation
models and so they could therefore come
under the radar as it were that people
who were watching for large use of gpus
would miss the clever way that somebody
has built it so do we do we have a
decade do we have three or four years or
how long yeah that's that's a great
question it's capping computing power is
not a permanent solution but it's one of
the best Solutions we have at the moment
uh as others have said before we are in
a double exponential it's not a single
exponential we have an an exponential
growth of computing power hardware and
exponential Improvement in software we
need to start cutting down on one of the
two
uh cutting down a compute depends where
you put the cap probably will buyas five
seven years you can make you can make
what would seem to people at different
FR extremely strong caps that would
affect you know less than 20 companies
in the world that probably could buy you
10 years in that period we need to
figure out all of the rest it's going to
be a hard problem but we have done it
before with nuclear weapons we've done
it before with biological weapons we can
do it again we're going to go around the
panelists one more time in the same
order I'll give you a chance a choice
panelist you can either comment on what
you've heard from somebody else or you
can paint me a picture of what would be
a successful AI safety Summit in
Bletchley Park if things go well what
would be the outcome and what would also
be the followup so Yan first well I'm
the one of the authors of the pause
letter so it's like indefinite
moratorium uh on further scaling uh
would be sort of my we dream from
outcome from from this Summit or perhaps
the next one if this one isn't realistic
and what's the chance do you think what
what might cause the assembled world
leaders to have a intellectual
breakthrough and say Yes actually we do
need to have this indefinite pause So
currently I'm not very optimistic on on
that uh perhaps perhap but but perhaps
in six months it would be much more
clearer uh why this is needed so um and
and we have more time to kind of uh do
the necessary Ling so the discussion has
prepared the ground and when something
really bad happens in 6 months when GPT
5 comes out and oh my God at least we'll
know what we should be doing yeah I mean
like let's not forget that the GPT CH
GPT has been out less than one year so
um like the world was very different one
year ago same question to you ether yeah
thank you I think I'm just going to
build on top of Jan's ideal um outcome
of the summit and say that I would also
find it terrific if the summit could be
the first in a ser series of repeated um
Summits like this where world leaders
come together because as we I think a
pretty clear picture has been painted
tonight of the fact that the field of AI
evolves very quickly and is going to uh
continue to evolve very quickly if not
ever quicker and um because of that I
think it would be very valuable if we
would have um a regular occasion for
world leaders to come together and um
not only make sure that the rules that
they came up with are upheld but also to
re-evaluate um whether they still make
sense and where they need need to be
adapted or whether new rules need to be
introduced as for example measures like
compute control that Andrea mentioned um
they they buy us sometime but at some
point they might not be applicable
anymore so not just agreement on rules
but setting up some audit process so
that we can figure out whether the rules
are being followed or not for example
yeah same question to you Alexander yeah
I would agree you have to build in I
mean right now it's just based
especially in the US um the talks that
have been held in the White House and by
Chuck Schumer um The Gatherings have led
to sort of ideas around voluntary um
adherence to some principles but there
is absolutely no built-in audit or
accountability um so I think that we've
got to see that come out of of the UK's
AI safety Summit among other things
maybe there has to be something more
concrete around
licensing um of the models and and the
use of them and that they have to pass
some kind of a a threshold I think the
risk of of Bad actors getting hold of
them is is a is a much higher risk I
think the iaea structure is is one that
one can look at but the the nuclear a
lot of the success probably of the iea
lies in the mutual assured destruction
of humanity by using um nuclear weapons
and this might be the same situation but
they're easier to monitor I think um I
think this might be slightly harder
because you can land in the hands of uh
Bad actors more easily we haven't really
discussed Bad actors much in this
session tonight maybe that makes the
things even more horrifying we might
come back to that later um Tom what's
your answer what would you like to see
come out of the summit or maybe you got
some comment on something else you've
heard from the other
speakers well I'll talk about the summit
um often when CEOs of um Labs developing
AGI are asked about regulation um they
basically say Bring It On we'd love some
regulation um and I think it would be
great if politicians could actually put
that to the
test very good and Andrea what would you
like to
see if you were invited to Bletchley
Park and given the microphone for two
minutes what would you intreat the
assembled world leaders to consider well
I I don't want to be too hopeful as some
others here have been but at the very
least I would like to see a commitment
to the fact that this is extremely
dangerous technology continuing to scale
leads to predictable disaster and we
need to pull on the brakes right now uh
we have a lot of applications are very
beneficial we can focus on those but
limit this death phrase to the ever more
powerful ever more obscure General
systems that we cannot control what I
definitely do not want to see is a
diplomatic Shake of hand
where companies write their own Playbook
and say we're going to keep doing
exactly what we we're doing right now
but it's going to sound responsible and
governments can wash their hands and say
well we did our part let's move on that
would be a very bad
outcome right I'm going to ask for three
questions from the floor I'm going to
get the panel to think which ones to
answer we try and take people haven't
asked before so on the second row here
there's a hand up here and then also the
second row over there next as well let's
take three fairly short questions please
hi first of all thank you very much for
coming here tonight and sharing your
expertise with all of us in this whole
qu this whole discussion there's the
implicit assumption that AGI is coming
and it's coming soon and the million
doll question I guess is when exactly is
it coming but a more practical question
is what are some warning signs and do we
already see some of those in the systems
that we have currently deployed great
let's have a question over here as well
I'm sorry the microphone's going to have
to run around at the end of the second
row
there thank you for great panel um my
questions related to Yan's comment at
the beginning on essentially Dario
amada's philosophy which is you know and
and also related to I guess Roman's uh
talk which is how do we solve the
control problem what I've heard the
large AI Labs repeat is oh you know we
need to increase capability it's only
better AI that is going to be able to
help us figure out how to solve AR and
there's sort of this race to increase
capability up to the point that can help
us solve it but no further and and and
just sort of thoughts on that philosophy
and and and and and you know uh whether
there might be something to it or is is
it just a completely risky game you know
thanks and there was one in the middle
of the third row there you pass the
microphone along please to the
middle how long of a time frame do you
think we have between the arrival of AGI
and the arrival of super intelligence
and within that time frame could there
be tractable solutions for alignment or
the control problem and if so would
those Solutions be able to be
implemented before hurry up and develop
better AI third question was how long
might it take between the arrival of AGI
and super
intelligence and whether there would be
time for us to work out Solutions then
and my question I guess is well what's
all this about AGI isn't the blle Park
Summit set up to discuss something else
which is Frontier models which says that
there are catastrophic risks even before
we get to
AGI so I'm going to go around the same
order again I'll be a bit predictable
Yan you want to pick one of these
questions maybe I mean the answer to all
the all three questions is
uncertain that's that's why we need to
uh pause and kind of take a time out and
see like how can we kind of create more
C more certainty uh about these things
uh I think I would answer the uh
anthropic question specifically that
definitely is a lot of Truth uh to the
to the point that uh like the more
capable model you have to work with the
more kind of better position you are
empirically uh you can you can kind of
uh be do like do science in a way that
you just can't do with models from from
10 years ago uh and also like one claim
that people an Tropic do make is that
like in some ways it becomes easier uh
as the model kind of has better
understanding of what you're trying to
do with it or do to to it U but that
said again it's a to put it lightly it's
playing with fire so so it's I'm not
sure if anyone should be doing it Eva
Yan said it's all uncertain but can't we
at least agree in advance some Canary
signs that will make us say things are
happening faster than we expected well I
mean um if we look at uh the past there
were several signs that people agreed on
that they might um point out that we're
getting into a Zone where AI is maybe uh
more capable than we think it is and um
I mean we certainly have seen signs um
Conor mentioned mentioned or was it
Roman mentioned that the current models
um outperform most humans on things like
the bar exam um these are clearly
advances and capabilities that um I
almost wonder sometimes if we just
become desensitized to them because we
move so fast I mean again CHT came out a
couple months ago and it's already just
normal and people are waiting okay
what's the next big thing so um it
doesn't really help um to think
retroactively have there been any signs
um if you didn't take take them to
actually stop and reconsider what you're
doing so I think one of the big problems
here is not have there been signs the
big problem is can we pre-commit to
stopping when we see certain signs and
then actually stop or actually take
certain actions and we just haven't seen
that before so this is developing
contingency Solutions like we meant to
have had contingency solutions for
pandemics yeah yeah any comments
Alexandra I I will leave the um what how
long it's going to take to reach AGI to
to the experts on the panel but on the
outcome of the summit and I think there
there is a bit of a confusion sometimes
in in in what we are expecting to be
achieved from the discussions on
regulation because there's an obvious
very um important urgent and existential
question around regulation reg
regulating for the long term but then we
also have businesses that are sitting
and waiting for regulation that is Here
and Now how is it going to impact my
particular sector how is it going to
impact what I'm doing today and what are
the immediate and very very real risks
right now here today that we are seeing
um with AI having impact on you know
media disinformation and so on but then
there's also the specific um aspects to
how that is implemented in particular
sectors so I um hope that we would see
addressing both of those shortterm and
long-term questions thanks Tom any
thoughts yeah on yard sticks I think
it's worth remembering that um the
canonical yardstick was the uring test
um and that's long gun um AIS can now
beat humans at um diplomatic based games
for instance um and much more um the
modern touring test is I think quite an
interesting proposition um and that's
the test of whether the AI can I think
make a million dollars very quickly um
but as As Eva says we must stop Shifting
the goalposts we need to agree that that
one is we should pick one agree that
that's the one when we start taking it
seriously and then take it seriously
when it is passed which it will be quite
soon but in the past people said you
won't manage to solve chess unless you
have got a full grasp of all aspects of
creativity and so on and then when deep
blue did win a chess people said well
it's not actually doing it in the way
that we thought would be so terrible
it's just grunting out incredibly so I
feel there will always be people who
don't move the goal poost but they'll
say well how it was implemented doesn't
demonstrate true
intelligence yeah I think that's a a
good point um and it reminds of
something um Connor said um which is
that there won't be consensus at the
time to
act so we need to be able to build a
coalition of the
Willing uh Andrea what's your views on
these questions yeah maybe answering the
last one for on is in the summit about
Frontier AI well much like with
goalposts it feels a bit like
terminologies being shifted all the
time sometimes quite willingly by the
companies building this uh you know in
in the in the old days people used to
talk about super intelligence or
friendly AI or strong AI then it became
AGI then recently the frontier term was
a kind of opening ey anthropic Rebrand
of oh no like we're not
well we're going to get to very powerful
system soon but it's Frontier which
sounds better than AGI because people
are getting concerned about AGI you know
in
practice do these terms matter not too
much what matters is how competent
systems are basically all of these
companies expect to build systems that
outperform humans at most tasks
definitely most tasks you can do behind
a computer in the next two to five years
um the this matches the trends that we
see in performance and compute growth
this is very worrying uh these These are
are levels of competence uh at which we
expect the systems to be out of our
control unless we have very strong
Solutions so we just need to deal with
that we can call it Frontier AI AGI
Proto AGI Proto super intelligence it's
just terminology what matters is how
powerful they are and how ready we are
to deal with them we must avoid the
serious discussions getting sidelined
into semantics which is often very
frustrating we're going to take three
more questions we're going to go around
the panel in the reverse order next time
I'm going to take questions from people
who haven't answered asked before so
somebody in white about halfway down if
you have asked a question before please
don't put your hand up just now so we
have more chance just there yes thank
you three questions one from each thanks
um let's say say that in 20 years we
somehow managed to get it right and
Humanity still exists um despite the
development of these AI what do you
think is one essential piece of
Regulation or development that has to
have happened to get there it's a great
question uh where else with the hands uh
where are the microphones there's
one about just on the other
side thank you so you've spoken quite a
lot about like what government should do
what companies should do uh I'm
interested in like what should Ordinary
People do like what can we be doing to
get our voices heard in this you know
should we be protesting I know it's an
international protest on the 21st is
this thing we should be doing or is this
a terrible idea so another fine question
is there a question from a woman it's
about time we heard from the other
gender another gender hands
up yes some put your hand up to whoever
it
was
yes there's
one okay there's one there and we'll
take you as well we'll take four
right um as AGI is being developed to be
more human do you think it's possible to
have forms of AGI that don't have the
inherent geopolitical biases that come
with the data sets that we currently
have and how do you think we go about
developing regulations that aren't
formed by human conscious
bias okay and so if we can get the mic
over here as well to
one two three four five rows back just
at the
edge yes the over there yeah want to go
first I miscounted perhaps yeah four
sorry a quick question um so I actually
work in the automotive industry and we
have to certify vehicles and engines and
it is an uphill battle um you can spend
years just trying to get a windshield
wiper right um or a temperature sensor
right and I'm just curious um if you
think that there would be an ability to
take people who have regulated and
certified products around Global markets
and how difficult that is and create a
summit where that expertise could come
together from different Industries and
we could roll up our sleeves and say
okay this is how the structures go and
we know what works we know what goes
slow and try to accelerate that learning
because I think that voice we have so
much experience in the world right now
um with that sleeves rolled up we know
what it's like to sit in those test Labs
or send 30,000 pages of documents in
with verification and validation data we
know how to do requirements engineering
requirements design requirements and I'm
just wondering if um there's been any
discussion of that to pull you know pull
a summit together from people from
heavily regulated Industries four great
questions first of all 20 years later it
succeeded how did we get it right what
were the regulations that made the
difference
what should Ordinary People be doing can
we design AI that is free from some of
the human bias is the geopolitical
biases that cause Strife among humans
and can we learn from the people who are
professionally involved in doing
regulations and certification in
multiple industry rather than just to
being naive in our own applications so
Andrea first yeah maybe I will answer
the question about can we learn about
highly regulated Industries definitely I
think there's a big kind of problem of
uh arrogance in AI or like willful
arrogance of just thinking that this
sector should be special and uh people
should be absolutely free to do any
experiments they want all the time uh
use you know as much computing power as
they want try the worst possible
applications all the time fully open
source on the internet and nobody can
complain like very often people in the
eye sector get very very angry when
somebody tells them look well maybe what
you just did should be regulated in
other Industries we don't do it like
that like with drugs we don't just
let um pharmaceutical companies just
release and tested drugs on billions of
people and have their CEO say oh there's
a 20% chance you will die if you take
this drug but you know don't it's okay
like if it happens you can let us know
and then we'll we'll stop maybe right so
we can totally learn from that would be
great to learn from that um there is one
all which is that we don't understand
current systems that well so it makes
things like auditing them and evaluating
them quite tricky because we simply
don't know how they work internally that
well but we can do many other things and
we can definitely learn from highly
regulated Industries definitely given
the risks admitted by the companies
themselves at the frontier the approach
should be highly regulated industry not
so AI is different but not completely
different and we can indeed learn Tom
closing words from
you um well I'll take the um the
question about what Ordinary People
should do um and I have two immediate
thoughts one is that it's very important
to keep this issue
apolitical um the other is that
lawmakers need a sense of legitimacy I
think um in order to um come up with
regulation um and to bring it in through
acts and bills and so on um good example
of um when this happened a bit too
slowly was at the outset of coid um and
when it was a fringe issue there were
non R rules then the public got involved
um and suddenly the rules arrived a
little too late but they did arrive um
and how can Ordinary People achieve this
um I think Ordinary People I I don't
have a theory of of protest so I won't
comment on that um but I think it's
important um that we all keep this in
the public conversation I suppose what
my answer is really tending towards is
that you should all read lots of
Journalism about AI click on my
articles thanks Alexandra any of these
questions catch your attention I think I
think um your question in the orange
sweater there is is is um is is
definitely where we're headed I mean
there's got to be some kind of
um system that resembles either the car
industry or FDA and um the way that we
certify um our you know products
generally speaking and I I just don't
know how we get from from that to
something that is very difficult to
trace and to to monitor as as AI but I
would say to the gentleman's question in
the white shirt there if we're looking
20 years down the road and we say that's
really great in the UK November 2023 we
we were able to put in place regulation
that somehow created traceability um so
we could we could work out sort of where
the where they were were systems were
running out of control or Landing in the
hands of Bad actors that would be a huge
success I think that the reality is a
bit different and that is that it
probably is going to resemble a bit more
the world in which cyber security um
flourishes and that means you're
constantly trying to create um a Dam
system or a a um deflection of all sort
incoming um activities that are not
great so I know that none of these are
perfect analogies but I think it is in
in that Universe we're probably going to
be operating in for a
while thanks uh final words Eva sure so
I would love to touch on two questions
very briefly one of them um being I
think your question in the white shirt
what uh policies will bring us to a safe
World in 20 years and I think um a
policy that was mentioned today as well
already but that I want to touch on
again is um strict liability regimes
just simply to kind of shift the
incentive systems um incentive
structures that drive private companies
to take certain actions that are not in
the interest of um the the wider general
public so I think there we can um really
um um shift shift the incentive
structure to move companies to take um
maybe different PA forward and then what
can the the average person the general
public do I would completely agree with
Tom I think um one thing that that
really would help us to for lack of a
better expression to just make noise
just make sure that this topic is um
talked about publicly you can do this in
different ways you can write to your
local newspaper you can make a protest
if that's up your alley you can write to
your MP or your Congressman or wherever
you live um and again create that
legitimacy for people to actually act on
the problem because to many people it
does sound very much like sci-fi and
policy makers are not going to take
action and newspapers are not going to
continue to report about an issue if
they feel like it doesn't have traction
and isn't taken seriously by the General
Public the other thing the general
public can do is we can educate
ourselves and then we can share what
information we have found to be most
persuasive ourselves because there's a
wide variety of books a wide variety of
YouTube channels a wide variety of blogs
and some of them are better than others
so let's share what we have found to be
the really best ones uto before I pass
to Yan maybe I'll ask you to get ready
to come up on the stage because you're
going to give some closing remarks but
Yan what's your answers to what you've
heard
uh so yeah just just kind of underlined
what ordinary people could do uh is just
kind of keep this topic uh alive like
one of the things that I'm very proud of
uh that came out of the future life six
months post letter uh was uh framed by
uh European commissioner Margaret bester
when she said that like one thing that
this letter has done is to kind of like
communicate to The Regulators that these
concerns are much more widespread among
people than among Regulators uh so I
think this uh potential difference
should be continually kind of maintained
um so and when it comes to uh kind of
bringing in uh kind of expertise from
people from like regulated Industries I
think it's super valuable I was on the
on the board or like on the European
high level AI expert group at the
European commission and there was like
every once in a while there was like why
are we inventing the wheel like that we
already have like lots of regulations
should we just applied this and I like
yes however there's like one big problem
uh the problem is p in chat
GPT GPT stands for generative
pre-trained Transformer the pre-training
is something that you do before you
actually train so like current the the
of nasty secret of AI uh field is the
AIS are not built they're grown the way
you you build the Frontier Model build
the Frontier Model is is you take like
two pages of code you put them in tens
of thousands of graphics cards and let
them hum for
months and then you going have open up
the hood and see like what creature
brings out and what you can can you do
with this creature so it's I think the
regulate like indust the capacity to
regulate things uh and kind of deal with
various liability constraints Etc they
apply to what happens after what once
this creature has been kind of tamed uh
and that's what what the uh fine-tuning
and uh reinforcement learning from Human
feedback Etc is doing and then
productized then how do you deal with
with these issues but uh is is just
where we need the competence of of like
other other Industries but like how can
you avoid the system not escaping during
training run this is this is like a
completely novel issue for this species
and we need need need some other
approaches like just Banning
those training grants that's great we'll
thank the panel in a minute I ask the
panel to stay here because who's going
to wind up the evening is utto barten
utto is the executive director of the E
the existential risks Observatory which
along with conjecture has designed and
organized and sponsored this whole
evening auter has got a few closing
remarks before those of us are still
here can have a quick drink and continue
the discussion informally up to 10:00 by
which time we must be out of the
building
utto just
here all right uh thanks David um a few
closing remarks before we go to the
drinks which is five minutes so you
should be able to uh keep with me
um so we're talking tonight about human
extinction because of AI and what to do
about this um and I think what to do
about this that was also a great
question from the audience what can we
do about this this is exactly the
question that I asked myself a few years
ago um but it's not trivial and it's
it's pretty difficult actually uh what
is net positive what could you do uh
develop AI yourself try to do it safely
such as open AI deep minds and anthropic
are doing will this increase safety some
say so uh work on AI alignment for
example interpretability where we've
seen great breakthroughs actually last
week uh it could be a good option but
increasing knowledge of how AI works
could also speed up its development so
this brings risks as well uh one could
campaign for regulation such as an AI PA
we support this but this also has its
downsides so I think it's pretty
difficult to tell what one should do to
reduce human extinction Risk by AI but
when I started reading into this I was
only really convinced about one thing
and that is that you cannot put Humanity
at risk without telling us so you cannot
have a dozen Tech Executives embarking
on the singularity without informing
anyone else and you cannot have 100
people at a summit which is What's
Happening Now decide what should be
built and what should not be built and I
think you cannot let a tiny amount of
people also decide how high Extinction
risk should be for the rest of us so the
only thing that I am really convinced of
is that we should be informed about this
topic and that's also why I'm so happy
that events uh such as this one are
taking place um we're I'm happy that
we're together not just with INR people
some of you are and it's great but also
with some people may who may have never
heard of existential risk before and
also with journalists who can inform a
much wider audience about existential
risk also with a member of parliament
someone with a job to openly discuss
difficult problems so I think this is
all very encouraging and it's helping to
normalize an open debate about the topic
of human extinction by artificial
intelligence the 31st of October at 2:00
we'll have our next event with Professor
St Russell it's just outside the AI
safety Summit in Bley Park in the old
Assembly Hall where the code Breakers
used to have their festivities after
their important work so our event at
blle Park the day before the summit may
not resemble a festivity but in a sense
I think it is because we're celebrating
that we're all being hurt there we're
celebrating that we can all be part of a
democratic conversation about what the
most important technology of the century
should and should not be able to do we
can talk about risks to humanity we find
acceptable and what we intend to do
about risks that are too
high and as the existential risk
Observatory together with conjecture we
invite everyone to be a part of this
conversation so there's much to be
unsure of in this field but if there's
one thing that I am sure of it's that
that the most important conversation in
this Century which I think this is has
to be a democratic
one so um with that I would like to
invite you to scan the QR code on the
left um if this is working right yes uh
to join us in bedley this is uh
containing the URL where you can enroll
to to the blle park event if you're
interested uh then uh definitely pass
by um there's same QR code is also on
the flyer on your chair
and Beyond Bletchley I think this
conversation will not stop so there will
be more Summits uh according to my
timeline about maybe 18
roughly um so we will organize more
events probably uh publish more about AI
do more research and inform governments
as well as we can if you want to follow
us or support us the existential risk
observor in that work then scan a QR
code on the right there's much that you
can do to to help
us um and with that I would like to
close this evening and once again thanks
to all our great speakers so that's Rome
yski Cornell Sir Robert Buckland yalin
Andrea Mi Alexandra M AA be and Tom a
give them a warm
Applause and I would also very much like
to thank David wood shm Conor exotis
Ruben Dilan and everyone as Conway Hall
who also made this evening possible
thank you very
much and then I would like to hopefully
see you in Bley and in any case you had
a drink right now thank you thanks
[Applause]
everybody
well done hey well done
yeah

now this book is called understanding
deep learning to distinguish it from
other more practical books which focus
on things like coding this book is all
about the ideas which underly deep
learning after reading this book you
will be able to apply deep learning to
novel situations where there is no
existing recipe for
Success the book starts straight away by
describing deep neural networks and it
takes you through the training testing
pipeline how do we improve their
performance uh then it starts talking
about different architectures
convolutional networks uh residual
networks graph neural networks
Transformers uh there's a long section
on generative models normalizing flows
Vees Gans diffusion models uh a short
section on reinforcement learning at the
end there are two chapters that I think
are really interesting uh there's a
chapter called wider deep neural
networks work where I try and
interrogate a bit um why why we need
this particular kind of architecture why
it's easy to train why it generalizes we
don't really have answers to those
things but I present some of the
evidence there is and the final chapter
is a chapter on ethics um uh I think the
book will be useful if you know nothing
about deep learning at all it will take
you from scratch to somewhere close-ish
to The Cutting Edge uh if you're
teaching uh deep learning uh it'll be an
incredibly useful resource it has
275 figures most of which are new and
represent things in different ways uh it
also has a whole bunch of python
notebooks if you're part of the Ranken
file of machine learning practitioners
or researchers it'll fill in the gaps in
your knowledge um and maybe make you
think about things in a different way I
think even my initial description of
deep neural networks is a bit different
to how they're usually described um and
I think you'll learn a lot from it uh
because I am a member of the rank and
file of uh uh deep learning
practitioners and I learned a lot
writing it so I expect you will learn
something too um if your name is Jeff
Hinton or Jurgen Schmid Huber I can see
it might not be that useful to you well
you never know you never
know so the title is a little bit ironic
because at the time of writing nobody
understands how deep learning models
work literally nobody now deep learning
models they learn peace wise linear
functions and as you'll know from our
episode on the spline theory of deep
learning they chop up the input space
into many many little regions in fact
most models have more regions than there
are atoms in the universe and frankly
it's a mystery it's a godamn
mystery how do these models generalize
and how do they learn these
functions nobody
[Music]
knows so so why does deep learning work
it's remarkable that the fitting
algorithm doesn't get trapped in local
Minima or stuck near Saddle points and
that it can efficiently recruit spare
model capacity to fit unexplained
training data wherever they lie perhaps
this success is less surprising when
there are far more parameters than
training data however it's debatable
whether this is generally the case
alexnet had 60 million parameters and
was trained with 1 million data points
however to complicate matters each
training example was augmented with 248
Transformations gpt3 had 175 billion
parameters and was trained on 300
billion tokens there's not a clear-cut
case that either model was
overparameterized and yet they were
successfully trained in short it's
surprising that we can fit deep Networks
reliably and
efficiently either the data the models
the training algorithms or some
combination of all three must have
special properties which make this
possible the efficient fitting of deep
learning models is startling and their
generalization is
dumbfounding first it's not obvious a
Priory that typical data sets are
sufficient to characterize the input
output mapping second deep networks
describe very complicated functions and
third generalization gets better with
more parameters this surface of
parameters gives the model latitude to
do almost anything between the training
data and yet it behaves sensibly it's
neither obvious that we should be able
to fit deep networks nor that they
should generalize a Priory deep learning
should not work and yet it
does the success of deep learning is
surprising in his book Professor Prince
discussed the challenges of optimizing
high-dimensional loss functions and
argued that the
overparameterization and choice of
activation function are the two most
important factors that make this
tractable in deep networks he showed
that during training the parameters move
through a low-dimensional Subspace to a
family of connected global Minima and
that local Minima are not apparent so as
we overp parameterize these models
generalization increases but it's also
related to other things like the
flatness of the minimum and inductive
priors it appears that both a large
number of parameters and multiple
Network layers are required for good
generalization although we do not yet
know why many questions remain
unanswered we do not currently have any
prescriptive Theory that will allow us
to predict the circumstances in which
training and generalization will succeed
or fail we do not know the limits of
learning in deep networks or whether
much more efficient models are possible
we do not know whether there are
parameters that would generalize better
within the same model the study of deep
learning is often driven by empirical
demonstrations and Simon concedes that
these are undeniably impressive but they
are not yet matched by
understanding of deep learning
mechanisms on
ethics Simon said that it would be
irresponsible to write this book without
discussing the ethical implications of
artificial intelligence this potent
technology will change the world in ways
arguably not too dissimilar to
electricity or the internal combustion
engine the transistor or the internet
the potential benefits in healthcare
design entertainment transport education
and almost every area of Commerce are
enormous however scientists and
Engineers are often unrealistically
optimistic about the outcomes of their
work and the potential for harm is just
as
great Simon argues that everyone
studying or researching or even writing
books about artificial intelligence
should contemplate to what degree
scientists are accountable for the uses
of their technology
he said that we should consider that
capitalism primarily drives the
development of AI and that legal
advances and deployment for social good
are likely to lag significantly behind
we should reflect on whether it's
possible as scientists and Engineers to
control progress in this field and to
reduce the potential for harm we should
consider what kind of organizations we
are prepared to work for how serious are
they in their commitment to reducing ing
the potential harms of AI are they
simply ethics washing to reduce
reputational risk or do they actually
Implement mechanisms to Halt ethically
suspect projects Simon invites readers
of his book to investigate these issues
further it's undeniable that artificial
intelligence will radically change
society For Better or Worse however
optimistic visions of a future utopian
society driven by AI should met with
caution and a healthy dose of critical
reflection many of the Ted benefits of
AI are beneficial only in certain
contexts and only to a subset of society
now the book cites green 2019 who
highlighted that one project developed
using AI to enhance police
accountability and alternatives to
incarceration and another developed to
increase security through predictive
policing are both advertised as AI for
social good in Big Air quotes assigning
this label is a value judgment that
lacks any grounding principles one
community's good is another's
harm ethical AI is a collective action
problem and the chapter concludes with
an appeal to scientists to consider the
moral and ethical implications of their
work every ethical issue is not within
the control of every individual computer
scientist however this does not imply
that researchers have no responsibility
whatsoever to consider and mitigate
where they can the potential misuse of
the systems they create we scuba dive
you know we we we actually do stuff and
may again maybe I'm just being a human
chauvinist but I I'm completely on board
with it becoming embedded in our
cognitive ecosystem but Al GPT does
stuff not very well but it does it well
does it though I don't
know does it do so I mean yeah like in
in the in the way a cat flap does stuff
you know you can make it do things it
can execute code and so on but I
wouldn't say it has
agency hello everyone it's Tim here from
mlst your go-to channel and podcast for
all things machine learning Ai and
philosophy today I'm reaching out with a
special request uh as you know creating
content for MLS takes a considerable
amount of time and resources it's a
labor of love that I do purely for the
fun and passion of the subject um but to
keep bringing you the high quality
content that you've come to expect I
need your support please consider
supporting us on patreon every bit helps
although please only donate what is an
insignificant amount of money depending
on your situation if you can't afford it
just let me know and I will give you
free access to the patreon's benefits no
questions asked anyway thank you so much
for your time and I can't wait to
welcome you to our patreon family
signing off for now cheers Simon it's an
absolute honor to meet you welcome to
machine learning Street talk I'm very
glad to be here so um tell us about
yourself uh well I actually started my
career in Psychology my PhD is in
Psychology and then I've been wandering
through various parts of science I
worked in Neuroscience for a while I did
some early work in augmented reality I
dabbled in Medical Imaging in the ughs I
was faculty at UCL and I worked on
computer vision and I'm probably best
known for a book that I wrote at that
time uh and in the last decade I've
mainly been working in industry in
finance and computer graphics and I'm
currently a professor at the University
of b where I have been working on a new
book uh understanding deep learning
which is going to be published by uh MIT
press sorry I should say which is
published which is well well um Simon we
were joking before that um on your last
book you were ubered and you you writing
a book about computer vision and
probabbly graphical models and so on and
then that was that SSG guy we'll come
back to him later um well it wasn't him
actually it was crevi but they were all
they were all basically hinton's guys
they released alexnet didn't they and
then that was computer vision completely
solved right so my last book was a
really ambitious attempt to basically
remold the whole of computer vision as I
saw it by formulating what was quite an
ad hoc selection of methods in terms of
probabilistic graphical models which
wasn't at the time necessarily how
everybody thought about it and on in
2010 I went on sabatical to the
University of Toronto and I worked on
this book probably with Alex kvy in the
Next Room to
me and sharing an office with Jeff
hinton's postdoc uh doggedly worked to
this book I really it in
2012 a few months before Alex neck came
out and the entire field took a uh right
angle turn uh leaving my book in the
dust although I still think it's quite
useful the geometry and stuff is still
definitely all valid uh and it has a lot
of stuff on basy and probability uh but
this book is less ambitious it's a more
sort of straightforward description of
where we are uh with deep learning uh
it's supposed to be the spiritual
successor to good fellow Benjo and Cor
oh yes which was published in 2016 so
obviously a lot of stuff has happened
since then um it takes a sort of
pragmatic Middle Ground between uh uh
very theoretical stuff with lots of
proofs and very practical stuff with
lots of code there are no proofs there
is no code uh it's about the ideas that
drive deep learning okay so um Simon
when when you started writing this book
what was the main idea that that you had
in your mind
well I think the history of deep
learning is that experimentalists have
run far ahead of the theory and um we
now have this explosion of papers where
there is literally an exponential
increase in the number of papers being
published and when I say literally I
mean literally there is a plot in a
paper that came out last year where on
semi log y it is a straight line with
4,000 papers being pushed to Archive
every month uh and presumably there's
more than that now obviously it can't
increase exponentially forever there's a
finite number of humans on the planet
they can't all do machine learning
research so there's a staggering
quantity of information out there if you
uh new person coming to machine learning
or you want to learn something new it's
almost impossible to find good resources
people are learning things from blogs
that are written hastily by people who
don't always know what they're talking
about and so it seemed to me a really
useful thing I could do for the
community where um I could write out
everything pretty much important that's
happened in the last 10 years connected
to deep learning with the same notation
Illustrated in a modern way without
regard for history I don't start at the
perceptron I jump right in to deep
neural networks and you know 20 pages
into the book not 160 Pages um just to
save collectively the community a giant
amount of time
yes and do you think that deep learning
is
alchemy uh no it's not Alchemy at all I
think in the future what we'll think of
it as is uh the science of modeling
functions and probability distributions
in very high Dimensions I mean I think
it'll be recast as that in terms of
science at the moment we're more
concerned the way we organize our whole
Community is about results so we don't
really talk about it that way uh but I
think in 40 years time they'll look back
and say well in the 2010s they studied
how to build functions and how to model
probability distributions in Dimensions
that are higher than say 50 yeah so I
was tongue and cheek on the Alchemy
point but I guess number one people have
made some strange analogies to
neuroscience and biology even even the
word neuron actually is quite
interesting so it only appears four
times in your entire book and two of
those occasions you are counseling us
not to use the word neuron so let's
start with that yeah and I should say I
will probably accidentally use the
neuron during this conversation because
it's so embedded in our community but I
think it's a terrible analogy um there's
no evidence the brain works in any way
that deep neural networks to um if you
look at the sort of epip phenomena of
the brain you know of uh human
computation we seem to have things like
short-term memory we seem to need to
dream to lay down Memories We have uh a
modular brain with special parts for
recognizing faces for navigating through
the world and so on there's no evidence
that deep learning has any of that and
likewise there's no evidence that the
brain has any of the epip phenomena of
uh deep learning so you know there's no
evidence for double descent or
adversarial examples or lottery tickets
in the brain as far as I'm aware um and
this is sort of okay for our community
because we know what we're talking about
but now deep learning is becoming a
really important thing in the real world
and so we're trying to communicate this
to the general public and we're talking
about neurons and neural networks and
that carries with it a lot of baggage
you know if you an interesting
experiment that everybody watching this
should do is go and talk to somebody at
a dinner party who knows nothing very
much about AI who works you know a
lawyer someone intelligent who uh Works
in a completely different field and asks
them what their understanding of current
AI is and almost certainly the answer
you will get is that they have no idea
they might be able to give you a couple
of buzzwords but how does it work they
have no idea and I really disapprove of
the neural metaphor just because it
comes with a lot of baggage that sort of
implies perhaps that the Network's
having thoughts or that it's something
like us and that's uh deeply misleading
to people who are outside our community
and of course everything we're doing is
increasingly affecting those people
outside our community and
uh we want to give them sensible
information about what it is that we're
working on yeah so I mean again on on
the Alchemy point we are now dealing
with multiple levels of emergence and
what I mean by that is people understand
gradient optimization they understand
parameterized models they don't
understand the emergent phenomena and
they reach for analogies let's say
psychology analogies you know they talk
about things like theory of mind I'm not
completely convinced there are emergent
phenomena depending on exactly how you
define emerging pH phenomena for me an
emergent an emergent phenomenon would be
something where you gradually ramped up
the scale and there was suddenly a phase
change where you could see new uh uh
phenomena I suppose for what a better
word suddenly appear with scale and I'm
not sure we've done those experiments
super thoroughly what you're really
saying to me is that uh the statistics
of the data on the internet are
surprisingly Rich that such that we can
uh you know complete sentences or
translate from one language to another
uh all of that is just reflecting the
statistics that are on the internet and
it is surprising that when you gather
that much data and put it together in a
network and reproduce those statistics
uh we see these phenomena but I don't
see that as a pro property of the
network I see that a property as a
property of the data that we're putting
into the network whether it came from
the richness of the statistics or not I
think we are looking for mental
reference frames to understand said
phenomena it it's it's just we we need a
way to understand this stuff right and
psychology has a theory of Mind
literature that we can that we can use
and we are almost certainly overloading
it and bastardizing it but are you
saying we shouldn't do that I have the
most reductionist view of this possible
I think of when I see a large language
model I see an enormous equation with
trillions of terms in it and we've set
the parameters of those terms so it
performs some kind of behavior and I
don't think there's any great meaning to
that behavior um it's an equation there
are inputs you compute you add things
you multiply things occasionally if it's
a Transformer you take an exponential
yeah um and out comes some numbers at
the other end that you then translate
into words and I don't think it can be
understood on any deeper level than that
but couldn't you make the argument then
that there's nothing special about our
mental States or our Behavior because we
are also at some level just performing
simple
calculations I think you could make that
argument but there's more going on
because we have a more uh a larger
variety of brain systems interacting
with each other uh some of which have
been built on top of each other during
Evolution so there's more structure in
the actual
model you know it's not at the very
least the human brain is not one
equation it's a bunch of equations
interacting with each other in a
complicated way so um whether that maps
to the kind of constructs you're talking
about I don't know but I see the human
brain as being quite a different thing
to that it's mostly a matter of
complexity so when you have this Rich
kind of functional dynamics of things
interacting in the physical world you
have the emergence of agency and all
sorts of moral St and so on and you're
basically just making the statement that
in neural networks we're just nowhere
near to that kind of emergence I mean
you're asking me questions that nobody
knows the answer to the only the only
existing model we have that works is the
human mind and that seems to work upon
quite different principles than just
scale yes um I think something that's
very interesting about Transformers
though is I was arguing before that I
don't like this neural analogy but
actually
um the uh large language models like
chat gbt are the closest thing we have
to um something like the human brain
because they don't they do just as I
expressed map an input to an output but
they also have this kind of short-term
memory which is the context window so in
a sense and it's ironic that we don't
refer to that in terms of a neural
analogy because that to me is the first
thing we've built that sort of looks
something like the human mind you have
this context and it makes the next uh
prediction of the token based on that
context and in principle um you could
then operate on the previous context the
you know the the system itself could
operate on the previous context it could
summarize it it could file things away
it could uh ask itself to generate other
con you know different hypotheses for to
explain something and compare them and
decide on something and use that as a
sort of scratch memory in the same way
that we have a working memory but
strangely we don't refer to that in
terms of the neural analogy which I find
uh quite ironic I I don't know if people
are working on that kind of thing I seen
that people are working on everything
but I haven't personally read any work
in which they uh Transformer system goes
back and edits things in its past
context yes um but I assume that that
would be one direction that you might go
to try and make this system do something
that's more like thinking I mean in the
end a purely feedforward system can't
really do anything sophisticated you
need probably to be doing some kind of
manipulation if only to generate
internal consistency so there's no way a
large language model can have internal
consistency it's learn everything on the
internet it thinks that uh the Earth is
both flat and round with different
statistical proportions uh you know
hopefully mostly most people on the
internet think it's round and that's the
conclusion it's come to but in the
weight somewhere is Earth flatness and
so to get to another level of cognition
you're going to need something that
builds an internally consistent model of
what's out there whether that needs as
you might argue uh uh interaction with
the real world or whether that can be
done purely in the domain of language uh
remains to be seen but I think that
might be one dire that people would take
things you know the the the body of
knowledge of humans is a kind of virtual
phenomenon that supervenes on all of us
physical earthlings so you know like
this this this infosphere that we've
created it's like a symbiotic organism
and that has consistent artifacts of
knowledge as you said but many humans do
hold the view that the Earth is flat so
it's just another example of of this
interesting kind of like levels of of
emergence but they they hold an
internally consistent view that the
world is flat they I mean as far as
they're concerned it's internally
consistent obviously they
inconsistencies that are quite easily
proven but um within their mind they
explain they have a model of the world
that whereby they explain everything
well if the moon is flat well obviously
the sun must be flat as well and so must
um and and you know when you look at the
Horizon of the sea it looks flat and
consequently the Earth can't be round
they you know they explain way other
phenomena and build up a model that
backs up the hypothesis and there's no
sense that a Transformer system is doing
anything like that it just starts at the
beginning and predicts the next word and
has the
statistics um uh uh that are consistent
with what's previously in its context
window yes you could argue though that
um
humans our brains are also very chaotic
but we have this confabulation and post
hoc rationalization in in much the same
way so we you know subconsciously we
hold conflicting views but when we try
and explain our views and to avoid
cognitive dissonance we um kind of we
try and reduce what we think to
something simple right but we have a
finite number of views that are sort of
partially rounded theories of the world
the large language model has everything
that hum's ever created with no
preference for one thing or another
other than its statistical likeliness so
yes so that's still you know even if you
have multiple conflicting views of the
world you know there's that famous W
Whitman poem where he says do I contct
contradict myself very well I contradict
myself uh I am multitudes I am large you
know that that is human beings captured
um but we don't have every view on
everything simultaneously we we're
trying on some level to come up with uh
consistent models of the world and we
need to do that because we need to take
actions in the world and it's impossible
to do that if uh if you have 50,000
conflicting views of how things work and
this is really interesting because um
Hinton says one of the reasons why chat
GPT is a kind of super intelligence is
because it knows all of the things but I
would argue as you just did I think that
we are kind of bounded as observers
there's a
computational um kind of restriction to
how many things a single Observer can
understand at one time and we'll get
more into this later but I think with
cognition it's not just knowing it's
also thinking so just knowing everything
isn't actually the the the whole piece
is it yeah can you deduce new facts I
think in one of your other podcast you
talked about if you trained uh chat GPT
with data only up to you know the early
20th century would it be able to
reproduce the uh um Einstein's theory of
relativity I think we all know the
answer to that it wouldn't definitely
don't and what are the missing pieces
but what you know what that's getting at
what I was saying before it's true to
to to build that theory you need to have
a model of the world and you need to
realize that model of the world is wrong
that certain facts I don't personally
believe you need to observe those facts
yourself but certain facts are
inconsistent with that theory and then
you need to somehow come up with a new
model that itself uh will make new
predictions about the world that people
can go and test in the case of physics
but but I think that happens on a sort
of more minor scale you know with your
theory about how businesses work or how
your friends personality works and how
best to interact with them you know you
have theories about everything uh that
occasionally break and you have to
radically rethink them from a
computational point of view they are
finite State autometers I mean you're
saying any
finite entity can only compute certain
things by nature of the fact that it's
finite and eloquently put of course that
is
true but I I'm not sure that that's a
radical Insight I mean what would be
interesting is if that we had some way
to characterize what kind of things you
could compute with a certain uh degree
number of parameters or what have you
but as far as I know we don't really
know the space of functions between
input and output we can fully describe
given a fixed set of parameters and a
certain neural network
architecture um or what lies outside
that
because you're building this very
complicated surface in multi-dimensional
space and everything's dependent on
everything else so it's not really
obvious uh but it seems it's very rich
in that we give it almost any data set
we want and with enough capacity it can
fit it well I I I think we we're just
about to put the pin in the middle of
the dartboard here which is that and and
we'll talk about Universal function
approximation later which is that given
an infinite number of um neurons you can
approximate a function to arbitrary
Precision that's a little bit like
saying if I have an infinite size hard
drive I can you know store any file that
that I want to so the big discussion
between connectionists and symbolic
folks is the symbolic folks argue that
we do need an infinite amount of
computation for many things neural
networks have told us that in many cases
no we don't and then I guess it's just
about well are there boundary cases
where we do need an infinite amount of
computation I mean uh you know an
interesting question you're asking a
different ways are the ideas that the
human mind can never
grasp or or indeed that we can grasp
that a neural network
can't um Simon we've now teleported to
our studio uh we were just outside
freezing freezing ourselves to death it
is much warmer and less muddy in your
studio indeed it is um so Simon um we'll
get straight to the Chaser I've I've
been reading your your amazing new book
on deep learning and I think we should
start by talking about the first few
chapters really in particular chapter
three and four where you talk about not
only neural networks both you know deep
neural networks and just um single layer
neuron networks but there's this kind of
elephant in the room in general I feel
about neuron networks which is why did
they work so well the you know the
unreasonable effectiveness of neuron
Network
why do they work so well uh well the
good news is they do work really well if
you've been asleep for the last uh 11
years or so um uh they're incredibly
effective um but it is a bit of a
mystery and I think it's particularly
interesting to look at it from the
perspective of just before Alex net came
out so image net of that time would have
been considered a real stretch goal
Richard cisy wrote in the computer book
he published around that time that he
expected it to be years and years before
computers could see to see as well as a
two or threey old um and so image net is
a challenging task the input Dimensions
2 to 4 by 2 to4 image that gives you
roughly
150,000 uh uh dimensional space uh so
there's roughly you know works out as
roughly 10 to the 150,000 possible
images you might say okay well most of
those images are noise but presumably
the uh actual manifold of images is
still extremely large um and you've got
to build a model that Maps this to uh
one of a thousand classes and you only
have a million examples in this 10 to
the 150,000 dimensional space and for
each of those classes you only have a
thousand uh possible examples so you
can't even build a gaussian for each
Cass in that case so if you didn't know
that humans could do this task you might
even just give up but since we knew
humans could do this task people
persevered um and Alex snet set out on
this extremely ambitious program that
was completely different from what most
people were doing at the time um I know
that there's arguably some PR
predecessors I don't want to get into
that I will talk about Alex nnet and you
can judge for yourself whether it's the
right thing to talk about um
um so Alex net sets out to build a
neural net with 60 million parameters
several orders magnitude larger than
most of the computer vision models that
would have been built at the time and
the way that I think about neural
networks is they divide the input space
into convex uh polytopes Each of which
has a uh linear or more properly apine
function within it so uh if the
dimension input space is two dimensional
then it divides the input space into
convex polygons and each polygon is a
sort of plane and those planes are
organized so they make a continuous
surface but now we've got um 150,000
input Dimensions uh so you're in very
high dimensions and we've got this
incredibly a model makes this incredibly
complicated surface and it's difficult
to count the number of these polytopes
it creates but uh just the fully
connected layers at the end would make
of the order of you know 10 to the 4,000
so got this huge space you've got a
model with 60 million parameters that
creates uh a number far of uh uh regions
far larger than the number of um atoms
in the universe none of you know hardly
any of those regions are ever going to
see a training data point or a test data
point at any point during training um
and there's some super complicated
relationship where you tweak one of your
60 million parameters and these much
larger number of poly sces uh change and
manipulated in some very complicated
indirect way that's difficult to
characterize um so now you come along
and say okay I'm just going to do a 60
million dimensional optimization problem
at that time in computer vision you know
people were considered ambitious if they
were doing thousand dimensional
optimization problems they um you know
typically the biggest kind of problems
would have been structure from motion
problems so you would probably try and
find the solution in closed form an
approximate solution and then just use
the nonlinear optimization you're ready
somewhere near the local the global
minimum and you're just going to find
use optimization to get to the final
place but they're going to start by just
randomly initializing this network uh
and then using the dumbest algorithm
basically noisy gradient descent to get
to the bottom uh because you can't use
anything else because everything else
uses second order information and now
the number of parameters is too enormous
sounds like a disaster it does sound
like a disaster I don't think many
people would have predicted you could
even learn the model but one thing they
have in their favor and we can come back
to this in a bit is
arguably there are far more parameters
in their model and there are data points
that's overparameterized and perhaps
that makes uh the fitting easier and we
there's a subtlety there that we can
come back to yeah um so okay they fit
the model but as I said there's uh 60
times more parameters than there are
data points so the surface they fit
basically goes through every data point
exactly but for every data point it goes
through it can do 60 other things it has
60 more degrees of freedom so between
those data points which as we've
discussed is almost the entire of the
space it has latitude to do whatever it
wanted and actually they had applied
some regularization and some Dropout but
subsequently we found that those aren't
really critical to this so uh uh at the
time you might have explained it away
with that but since then we know we can
learn models without Dropout and without
regularization and they still generalize
reasonably well yeah um and somehow this
model performs 15% better than the next
best thing or whatever it was quite
surprising I mean jaw dropping really um
yeah well so let let's do a point of
order just on that then so first of all
I'm a huge fan of this View new networks
that you've just spoken about which is
that essentially it slices and dices the
ambient space into these locally affined
polyhedra and Randall bellri um came up
with this spline theory of neural
networks and folks should watch the
first laon show where we spoke about
that in in detail and what he basically
said was that for a given input example
the neuron Network can be represented
with a single apine transformation which
is surprising to a lot of people for two
reasons first of all people think of
neuron networks as being you know non
nonlinear and also it it it gives this
beautiful intuition of neuron networks
as being a collection of storage buckets
not too dissimilar to a locality
sensitive hashing table and I think
that's very instructive right and so I
wrote this book blissfully unaware of
that theory I should say but that's
pretty much exactly how I describe it in
this kind of simple constructive way how
do we combine together these networks we
should say that this is only true for
Reus and Ley re and parametric Rel if
you have smoother functions then uh you
can no longer characterize it in this
way but actually I like discussing it in
terms of relies because then the number
of output regions gives you some notion
of the complexity of the output surface
whereas once you start talking about
smooth functions it's much more hard to
characterize that um but it's kind of
locally smooth because because I guess
for the folks at home every single one
of these neurons we you know maybe come
back to that it's kind of like a
hyperplane and so you you have you train
the network and you move all of these
hyperplanes around in the ambient space
and essentially when you put an input in
it's kind of activating some of those
hyperplanes and it's creating this kind
of convex region right According to
which hidden units are active or not
active yes and even though these are
piecewise linear functions there are so
many of them kind of like you know
densely overlapping each other in space
they they appear locally smooth uh
presumably well we can plot like
one-dimensional plots through the
function and see it looks like and uh
what it looks like and of course it does
look smooth because of the sheer number
of them right but but then we get to the
next interesting bit which is you know
so you spoke about overparameterization
and you know like um finger in the air
let's say that there's an exponential
number of these um convex polyhedra you
know something along the lines of I
think you said two to the power of the
number of of
dimensions and um well well generally
will be more than that I was trying toly
lower bound that nobody could disagree
with but but but this gets the
fascinating thing so um traditionally
before neural networks the world was
different and we used to talk about the
curse of dimensionality and that's
basically this statement that um there
is an exponential relationship between
the volume of the space and the number
of Dimensions so yeah I I would say it's
the tendency of the volume of space to
completely dwarf the amount of data you
have as the number of Dimensions goes up
exactly so when when when the volume of
the space does increase exponentially
the statistical significance of your
training data kind of tends towards zero
so there is no statistical information
anymore which begs the question if
there's no statistical information how
do the networks
work and now we're getting to the
difficult part why does it generalize in
any way yeah it's um uh it's clear that
it can pass through every data point um
but what it does between the data points
is I mean in some ways almost happen
stance it's a byproduct of our
algorithms uh it makes some smooth
interpolation why that's a good smooth
interpolation we don't know why it even
becomes smooth we don't know it seems
like it's some complicated byproduct of
the way we initialize the neural
networks the noisy algorithms we use to
train the neural networks uh the
overparameterization itself you know
perhaps uh what the kind of thing that
might be going on is we wind up with
smooth networks that interpolate just
because we overp parameterize so much
that um uh when we set up our networks
we initialize them and The Wider they
are the smaller numbers we put in and
the numbers start small and they stay
small because it never has any impulse
to make them larger and because the
numbers in the network have small
magnitudes that basically maps to small
slopes I want to slightly come back to
the overparameterized thing as well
because um it's not totally obvious to
me that everything is clearly
overparameterized so while Alex net had
UH 60 million parameters and 1 million
training points they also augmented
their data by a factor of
2048 and if you look at all of the
papers pretty much uh on uh uh this
image net classification problem they're
all sort of overparameterized by the
best results from the last 10 years by
sort of 10 to 100 times but they also
all augment the data which muddies the
water so
it's you one way of think about it is
that it's more data points and we're not
overparameterized but of course they're
not independent data points anymore
they're translations rotations uh color
transforms of the input image so I don't
really quite know where we are with
overparameterization yeah well I I want
to get to the um to the inducted prior
in a minute because I I I do think of
these local apine polyhedrus being
buckets and then we get to well if you
perform some semantically equivalent
transformation like a translation or a
rotation as far as the MLP is concerned
it's a different thing so these
inductive PRI basically photocopy the
information so that you're putting it
the same thing in different buckets so
that you're kind of like cheating maybe
we'll get to that in a sec but just to
finish the the loop on this um
generalization thing um what we're doing
is we're complexifying the neural
network and this is really weird because
we we we were taught in school that 's
rer is you know like simple things
generalize and now we are exponentially
complexifying the networks why do they
still generalize um well it's
interesting how the theory can't really
cope with that so previous ideas of
theory like ramaka complexity would have
predicted the opposite it would have
predicted that as we added more
parameters the per generalization would
have got worse I mean the kind of loose
idea people have when they talk about
double descent and stuff is there's more
regions just means that you can model a
smoother function it has to go through
the data points and it's smooth
elsewhere and we don't have a much
better conception of it than that uh you
know for things like images we can put
in some prior knowledge about images
that the statistics are the same
everywhere use convolutional networks so
we're we're searching through a more
sensible subset of models you know
convolutional network is a sort of
strict subset of a fully connected
Network um so we we're searching through
a more sensible set of models that have
some prior knowledge we put in but that
that still doesn't really in a
satisfying way to me explain why uh
networks generalize so well can we bring
in this notion of the manifold
hypothesis how does that because even
before you were saying something really
interesting which is that everything is
an inductive prior you know even the
world we live in is an inductive prior
but just keep you know keeping the Phil
out of the discussion data is definitely
an inducted prior so you you can do data
augmentation and you can kind of um over
sample effectively you know in the in
the vicinity or you can tell the neuron
Network that doesn't know about certain
types of transformation you can kind of
effectively tell it that the
transformation exists by augmenting the
data yeah well there's yeah there's two
ways you can build that into the network
so as equivariant or invariant according
to your needs or you can just blast it
with
data yes either TR you know transforming
the the output in the same way you
transformed the input if you want it to
be equivariant or uh ignoring the
Transformations if you want it to be
invariant that uh kind of transformation
yes and there's there's an interesting
relationship we've got this bias ference
tradeoff and you know we we're doing
statistics here so in an ideal world you
would only need to give it one labeled
example of the thing that you want to
learn because the network would
understand all of the you know
Transformations that could happen that
doesn't work so there's a kind of
there's a middle ground between on the
one hand we have this dumb MLP that
knows nothing and we're kind of
introducing all of these all of this
what would you call it physics domain
knowledge of how things can be
transformed domain knowledge seems a
good term yeah yeah yeah
okay interesting so um we were talking
about this manifold hypothesis so that's
this idea that there's some kind of a um
a Subspace in the data and because
theoretically as you were saying earlier
according to the curse of dimensionality
it should be physically impossible
because you would need more data points
than there are atoms in the universe to
train these models so people site the
manifold hypothesis which is to say
there's some kind of sub Subspace in the
data where
um you know the network can kind of
focus its attention on what you think
about that um I mean in some case
there's a very simple case where you can
almost describe that Subspace which is
if you take somebody's face and you film
them in fixed lighting from a fixed
Viewpoint and they don't move too much
then they have 42 muscles in their face
and there's only 42 things that can
happen and that's a very sort of
physical description of course the real
world's more complicated than that but
there are still regularities there's
only certain kinds of materials out
there there's only certain kinds of
lighting um real images are not all
images go you know go randomly select
pixels and you see how many times you
have to do that before you get something
that doesn't look like noise you will
give up
very quickly we don't really know the
size of that manifold though although
you can estimate it because it's
connected to the degree to which you can
compress images and so one thing that's
really interesting to me about these new
diffusion models is that you know
they're not that big you can put them on
your hard disk but yet they seem to be
able to generate this incredible array
of images so that we don't know what
images they can't generate of course but
a good portion apparently of the image
manifold can be put on a hard disk
suggesting it's not actually you know
the space of natural images is not
actually that large yeah interesting so
so you're saying the apparent creative
and um you know generative ability of
these models suggests that there is some
low dimensional manifold of of images um
yeah well there must be because uh we're
not you know that generative model does
not have 10 to the
150,000 parameters yeah uh for the very
good reason that you know we can't store
even a tiny tiny fraction of that very
interesting okay so um we should speak
quickly about this um uh you know um
Universal function approximation so if
we start first of all with a shallow
Network um this Universal function
approximation Theory basically says that
you know if you approximate it to
arbitrary Precision you can you know
represent any function and it's just a
collection of basis functions right you
know we're just optimizing the placement
of these basis functions yep I mean you
can still think about it just dividing
the input into um uh convex polytopes
too that's that's fine or you can
consider in terms of basis functions
right so I've always thought that this
Universal function approximation theorem
isn't particularly
useful I mean it is important that we
know that you can represent you know
with some uh um uh uh caveats you can
represent any function you want to um
what's really interesting is that you to
build anything that works really well we
seem to need 10 to 12 layers and
it's difficult to know and there are
quite a few different theories why we
would need a deep Network given the
universal approximation theorem says a
shallow network will do can model
anything well I mean just just to um
have a go at that so that theorem is
talking about a very wide single layer
neural network and it seems to me that
just placing basis functions on a single
layer of a neural network it's almost
antithetical to generalization it's it's
it is memorization by definition so you
you make this really interesting
argument in your book that when you
introduce depth to a neural network what
you're doing is is you're kind of
progressively
refolding those apine regions right uh
yeah that's one partial way to think
about it so one partial way to think
about it is that if you have a two laral
Network the first Network can be thought
of as folding and replicating the second
Network in a complicated way and yeah um
go look at a picture in my textbook
that's difficult to describe verbally
but that's only a sort of partial way of
looking at there are other ways of
looking at as well a different way of
looking at it is that you're creating I
think what you call increasingly
complicated basis functions and then
you're clipping those and recombining
them to make more complicated functions
still so one way sort of emphasizes this
Symmetry and this folding that is going
on and another way emphasizes this sort
of clipping and creating more joints in
one dimension they would just be joints
in the function in two Dimensions they
would be sort of one-dimensional folds
in the function um so that's a different
par way to look at it but I don't think
anyone even for a tway network it's
really hard to get a full picture in
your mind of exactly what it's doing but
what's definitely true is there's now
like a really
complicated relationship between
manipulating any one of those parameters
and the whole surface that comes out at
the other end yeah and um when we did
our first show on spline Theory we were
showing this neuron Network visualizer
and you can add layers in and you can
change the activation function and and
it kind of shows you the activation
space and very very quickly you get
these very complex behaviors emerge
where um you know the the the the
hyperplanes kind of cancel each other
out and what you were just saying there
that I want to get to is this
topological interpretation which is to
say that um as you complexify as you go
through the neural network in successive
layers um you can recompose all of the
PRI neurons in previous layers that are
topologically addressable and this is a
dag that we're working in so that's
obviously a subset of those and there's
this really interesting thing that the
neural network has a structure which is
kind of defined by the early layers so
as you go through the neural network
it's kind of somehow constrained by what
went before I mean it's constrained in
terms of information if you if you lose
information at the start it can't be
recovered uh but I think you're trying
to get at something else well some of it
is to do with um how how Flex you know
like there's a continual learning
Dimension to this which is to say early
on in the neural network those those
initial um basis functions are making
chops in the space and then the way I
think about it is all of the kind of
complexification that happens later is
kind of working inside those early
regions so it's almost like it's
becoming more and more entrenched in
what it does as you go through the
neural network which is to say that um
it becomes harder and harder for it to
completely learn things
differently I I think I mean I'm trying
to interpret the way you're expressing
this but I mean perhaps what you're
trying to say is that it it's sort of um
collapsing bits of the space on top of
itself and then um treating those
similarly and it can't then uncollapse
them so and that might be how you could
exploit regularities in data um that's
one interpretation but the the mirroring
I think actually kind of happens the
other way round that the
early um maybe I misunderstood how you
said it but but the early um layers are
are defining how the mirroring will
occur how these Reflections uh will
occur in the later layers so really it's
in in that view it's the later layers
that sort of defining the structure and
the early layers so then propagating
that structure to different and
reflecting it like a crazy House of
mirrors into the uh rest of the
space I mean I guess like one way I
think about continual learning is the
extent to which you could destroy a
neural network let's say you you take
GPT and you just start fine-tuning it on
noise and what would happen would it
very quickly forget everything it
learned before or would it just refuse
to
budge well that's a complicated uh
question that would depend on the
learning rate and yeah
um it I mean uh these uh Transformer
models are quite interesting because
they generally is as I understand it and
uh please correct me if you think I'm
wrong I don't think they train them to
zero training error I think they
generally do one pass through their
training data and stop so yeah so or 30
or four so it's sort of to some extent
probably remembering what it saw
recently more than what it saw a million
tokens ago a trillion tokens ago
whatever it is yeah um
how quickly if you added noise would it
um decrease
um I mean it it really yeah so so what
have you done by adding that noise in
you've changed the whole lost surface
because now the Lost surface either only
includes the noise or I'm not sure
you're suggesting if the noise is added
in with the real data but either way the
Lost surface is now
different and you're you're heading now
downhill H from what used to be I'm
going to Loosely say the global Minima
and I'll probably say that several times
during this by I mean the sort of level
set of good solutions that uh that fit
the data well so you're now heading
downhill from that point which is raised
up in your new loss service and how
quickly it forget just depends on the
learning rate
and the new shape of the loss function
so I I don't know if you could say
anything definite about that I guess I
was coming at it from you know Chomsky
says that we we see the world using our
native priors and in a sense neuron
networks are the same you look at a
vision Network they have these Gabor
filters that are baked into the early
layers and to a certain extent after a
while the neural network only sees the
world in terms of its fundamental basis
function so I don't like that idea of
considering the rep you know you say
these Gabor filters are baked into the
early layers and I would push back on
that a bit yeah what people are saying
when they say that is that
this neuron has a high activation when
you put in this kind of thing and to
some extent that's sort of trivial
you've made some small set of basis
functions because this network at this
point only sees a 7 by seven patch of
the image or something thing and just
because it reacts very highly to that
particular thing it also you know if
it's not the first layer of its in a f
in it it's going to react in a complex
way to lots of different things so what
you trying to do is you're trying to
characterize this very
complicated uh multi-dimensional
polytope by one point that looks like a
Gabor filter but really it's this
incredibly complicated shape in 150,000
dimensional space and and
uh so I don't like the idea that you're
going to describe you're going to say oh
well we can characterize that shape by
this one point yeah no I think you're
absolutely right there so that that is a
mistake that we we're looking at
interpretability methods which grossly
simplify Anor Network and we're kind of
making statements based on that so I
think you're absolutely right about that
but there's also this interesting
dimension of um training versus
inference so it has been said that overp
parameterizing is something which is
helpful for stochastic gradient descent
but um I would like your viewpoint on
the extent to which we need to have all
that representational capacity for
inference so do we need to memorize all
that stuff or could we actually strip it
away yeah so it definitely makes it
easier Terry sinowski has this beautiful
expression where he says it you know it
goes from searching from searching for a
needle in a Hy stack to a Hy stack of
needles so um you wind up with this lost
surface where there's a very High
dimensional uh part of that lost surface
that is the global minimum by which I
mean a good level set of solutions that
fits your data perfectly yeah I have a
feeling and I know that you're going to
push back on this that there is an
inherent model bias in neural networks
which is to say a lot of these inductive
PRI and training tricks and so on um
their ways of making the network focus
on the modes in the data the areas of of
regularity or where most of the
variances and all of the low frequency
attributes and and just to be as clear
as possible about that I mean things
that don't happen very very often they
tend to be snipped or ignored or they're
not learned is that something
fundamental do you think to neural
network so I think we do need to fit the
data perfectly or close to perfectly so
for our training data we always get the
right
output because ultimately these things
are quite stupid and there's nothing
much they can do apart
from inter interpolate
smoothly uh and I'm using the word
smoothly very vaguely because it's
difficult to characterize exactly how
they interpolate between them
um and I'd rather interpolate smoothly
between the correct true data points
than a neural network that missed those
in the first first place you know how
how could that be better not to memorize
the data so I I have a feeling this is a
story of the tyranny of metrics which is
to say we optimize on accuracy and the
neuron Network I assume would learn that
well given that this is such a longtale
high entropy thing um it's probably
better that I just don't bother learning
it than get get it wrong all the time by
trying to learn it I but I think it does
learn you know it does learn
uh for most data sets the long tail of
the data you give it because the loss
comes almost zero so uh for regression
model the loss can literally become 0.00
for classification models it's a bit
more complicated because you're trying
to push these softmax functions to
Infinity so you never get there training
loss though training loss yes yes but
that is not going so I guess like those
things are like atoms in the universe
and even the the the network knows about
them in the training set they would
never have any statistical
power so that wasn't a
question no it's it's a I'm just testing
the principle but I I guess you know we
and we'll talk about ethics and bias and
fairness later on but you know it's um
it's really difficult with a single
training objective to square the circle
to have high accuracy and high fairness
yeah absolutely um I think what you're
worried about is areas of the data space
that um represent perhaps minorities of
um individuals and particularly
intersections of those minorities uh
where where you know perhaps uh you know
gender and race or what have you and so
you w up with a very small number of
training data points
and I think it's a question of whether
you know you attributing the failure to
generalize that to the neural network
but that might be unfair maybe the
failure to generalize that is that there
is literally not enough statistical
regularity in the data you've given it
for it to possibly generalize
interesting interesting okay well just
to close the loop on on chapter three
and four so there are some other things
that we that we tweak in neural networks
like the
initialization um things like the uh the
learning rate in which activation
functions we use and stuff like that so
can can you sketch out some of that
um so I I think it's interesting to
think about that through the lens of
either learning or generalization so
we've learned quite a bit about which
things affect learning and which things
affect generalization surprisingly the
data
set confusingly doesn't affect the
learning so much so you can randomly
perturb the labels yeah and uh the new
network will still learn the training
data fine or you can Shuffle the input
pixels of images the neural network
still learns the training data uh fine
uh so there are some things you'd be
really surprised about it can learn uh
uh despite uh that the newal network
will
learn uh despite those factors uh also
other things uh in terms of learning um
could we could we just pause there
because that was a really interesting
bit which is and will show an image um
in in your in your book so there are two
curves and one curve is when you're just
memorizing essentially random
information and then the other curve on
the left so so the the kind of the
horizontal distance between those curves
is the generalization Gap so on on on
the other curve is is where you have the
actual data with the actual labels and
then there's another curve in the Middle
where you have the actual data but with
you know random labels so I guess like
the first intuition is isn't it
interesting that neuron networks don't
struggle much more to learn completely
random data uh so so this is a famous
paper from jangel 20177 and and shows
that you can perturb these labels or you
can Shuffle around the input data in
various ways and still learns perfectly
but it learns more slowly so but that
might just be because there are
regularities in real data so you move
the surface up and it naturally fits
through several data points and now it
has to contort itself more to fit
completely random data so I don't think
the length of time it takes to fit it is
really significant but it shows how
incredibly flexible the model is that
you can throw at it any data and it will
fit it and interpolate between it it's
just that the interpolation is
meaningless if the labels had no
information in them yeah it's
interesting that you say you can't Inuit
how difficult a problem is by the
convergence
time um the convergence time depends on
a lot of different factors but one of
the main ones is where you initialize
the parameters so um we initialize the
parameters for very practical reasons to
certain variances because if you don't
you get exploding gradients uh where um
for people who don't know about this the
essentially the numbers in your neural
network the activations and the outputs
start taking wildly large values and you
can no longer represent them in um
finite Precision floating Point numbers
or if you don't set them correctly they
wind up uh being too small and we call
that Vanishing gradients but uh it's
referred to in terms of gradients but
actually it happens in a forward path
through the network as well um so we
have very practical reasons for
initializing our networks uh where they
are but actually that also depends you
know we still have a range of values we
can set it to and it still works without
um breaking uh uh uh the um uh uh well
reaching the limits of uh the uh
computers Precision but actually uh the
magnitude you set the weights to
drastically determines the training time
so uh and it also determines
generalization so there's a sort of
goldilock Zone they call it not too big
not too small just right uh of parameter
magnitudes that um cause that seem to
give the um loss functions sensible
properties uh positive curvature where
you want positive curvature um
and I think that's pretty easy to
understand that if all the parameters
are too small
then it can only create very flat I mean
Loosely speaking people who really know
what they're talking about are going to
take offense of this but Loosely
speaking if the paramet are very small
it makes very flat functions and maybe
your function isn't very flat and if the
parameters are really huge then it has
functions with big sharp changes yeah
and you don't want that because uh maybe
your data is quite smooth so it takes
longer if you don't initialize the
parameters corre ly to kind of uh reach
the re presumably reasonably smooth
pattern of your data yeah um and of
course that also speaks to
generalization because you want it to
interpolate smoothly between your data
so you don't want um wildly varying
large um param uh parameter values that
uh cause big fluctuations between your
data points but still fit the data
points so the time takes a train depends
on the initialization as well but if you
give it enough time it seems to get
there um uh in terms of generalization
there's a phenomenon connected to that
called grocking gring yeah I had Neil
nander on did you yeah so essentially
you can get cases where um uh it fits
the data perfectly quite early on uh but
then it takes ages and ages and suddenly
it generalizes and yeah this has been
interpreted or my best knowledge of the
interpretation is that this is what
happens when you set uh the magnitude
the parameters wrong what's happening is
it's fit the data correctly but it is
wildly varying between the data points
but something in our methods for
training presumably some feature of
stochastic gradient descent or some
other explicit or implicit
regularization that you've put in there
is
causing uh the network the solution even
though it already fits the data sort of
Traverse this loss surface it's not at
the minimum it's in a family of Minima
and it's traversing through that family
until it eventually gets to something
that's smoother and interpolates well
and then it generalizes but exactly why
it should move through this surface to
something that's smoother is again quite
complicated and subtle yes I mean folks
definitely check out the Neil nander
episode but yeah it's as you said there
this phase change from memorization to
sudden generalization I think Neil did
say that it's a bit of an illusion and
people slightly misinterpret how it
works and as you said we're trying to
design the Lost surface to be as smooth
as possible and I think his original
paper was on modulo division which is
not very natural because I guess part of
why it's possible to coax the Lost
surface to be so smooth and
interpretable is because of this natural
data thing as well so maybe um an
artificial data set like that wouldn't
be so amenable so you would get this
sudden grocking later yeah perhaps you
mean that um the easiest way to fit it
is sort of with some function
that yeah there's no natural smoothness
to it and consequently you wind up
fitting it with a very
complicated function with a lot of
variance between the data points and
then it takes a lot of regularization
and rambling over this lost surface
until we get somewhere sensible whereas
for a more um typical data set you
naturally wind up with a sort of
smoother surface to begin with I mean
that makes sense to me yeah but but it
is um one of the crazy things about deep
learning which is that and rocking was a
surprise because most of the time you
it's like it's like cooking the secret
is in the preparation so you um kind of
massage the the lost service and then
you just know you can almost predict
just like um open AI could predict how
much training was required to get gp4
level perplexity you know there seems to
be a a regularity in in terms of okay
it's going to converge and because it's
overparameterized and finessed in a
certain way it's going to converge in a
certain amount of time it's going to
have a certain shape to it when you
train it yeah I mean the most recent
paper that I've read on this and you
should understand that I'm very much
like chat GPT every time I finish
writing the chapter of a book I stopped
reading because if you don't stop
reading you'll go crazy there's you know
4,000 papers coming out every month and
you uh you can't possibly keep up to
date uh the most recent paper I read on
this was called omnigo and basically it
says that that time is predictable
because the uh magnitude of the weights
are gradually decreasing at some fairly
constant rate and eventually they come
into this goldilock Zone where uh things
generalize well and that's when suddenly
the generalization performance improves
cool cool now um the book makes a really
really huge effort I would say to
visualize things in in like you know in
images and low Dimensions what was the
thinking behind that
um well I think people uh learn in
different ways so I try and have three
ways to understand everything there's
textual descriptions there's equations
and there's pictures and so to me the
mental process of connecting those
things which you sort of actively have
to do if you read the book leads you to
a deeper kind of understanding um a lot
of ideas even diffusion models and Gans
can be drawn in one or two dimensions
and you can really effectively convey
these Concepts um in a non-technical way
before you hit the equations which is
good for learning uh but you also have
to be a little bit careful because
multi-dimensional space doesn't quite
work in the way you expect it to um
there are a lot of strange phenomena so
if you take two random points that are
gaussian distributed you know after
about 100 Dimensions or even less than
that they're almost certainly orthogonal
to one another um a famous one is that a
hyper orange a multi-dimensional orange
basically all of the area is in the skin
and none of it is in the pulp or my
favorite one is if you take a um
multi-dimensional uh sphere of diameter
one a hypersphere and you embed it in a
multi-dimensional cube you can imagine
in two Dimensions you've got a circle in
a square in three dimensions youve got a
cube Cube uh you got a sphere in a cube
uh as that goes to Infinity it turns out
as the dimensions increase it turns out
the proportion of space the sphere takes
becomes zero it's all in the corners yes
uh so so I draw these pictures in start
the dimensionality again yeah I Jo
pictures in one or 2D you know I like to
joke that's because MIT press wouldn't
give me a budget for a five-dimensional
book I asked for it but they stuck with
two Dimensions yeah
uh but you should be cautious but having
said that I don't think it's necessary
to work in the super high dimensional
spaces we work in to see the phenomena
of deep learning and there's a a really
interesting data set that I use a lot in
the book called mnist 1D so our simplest
data set is mnist uh it has some
disadvantages in that for example we're
too good at solving it now to really
demonstrate much on it um but this makes
an even simpler data and basically what
it consists of is 40-dimensional
data um it's just onedimensional data
and they start with a template that
looks a little bit like one of the 10
digits so for example the zero has one
bulge and the eight has two bulges and
they find a way to actually it's one guy
Sam granis is his name and the paper's
called scaling down deep learning oh
right uh completely against the
prevailing winds of deep learning which
had to scale everything uh massively um
and uh uh and then that is translated so
it's a type of data that's amable to
convolutional networks for example
because there's a uh Dimension that you
can be equivariant or or an aspect of it
you can be equivariant or invariant to
and adds various amounts of noise and
it's completely procedurally generated
so you can generate any amount of data
uh with any amount of noise um and the
interesting thing is even though this is
only 40-dimensional most of the kind of
EP phenomena of deep learning can be
seen so you can show adversarial
examples you can show um uh uh uh uh
lottery tickets you can show double
descent and you can do all of this you
know this is now in a size that you
don't even need the GPU you can just run
it in your uh local python window on the
CPU um uh and I I think this is a really
interesting data set so the actual huge
experiments we're building and now we're
now building things that of of the order
of a complexity of a human mind and I
come from uh biological sciences
background basically and and if you want
to try and understand that what you do
is you go out and you collect a data set
as thoroughly as you can along as many
dimensions as you can and then you go
away and um try and come up with a
theory that explains that data set so in
this case when is it trainable when is
it generalizable and here we have a data
set where in theory you could do that
you could train many different networks
varying all the parameters because it's
very quick to train and then you could
try and properly come up with a theory
that says okay when I've got this kind
of data with this amount of noise these
statistics these are the networks that
will work these are networks we will
train these are the nexts we that will
generalize and have proper tight bounds
on those things um so to me this is a
really interesting data set that could
be a test bed for actually understanding
deep learning better um uh but which is
currently not used at all because you
can't publish a paper unless you get
state-of-the-art on some enormous data
set with millions of examples in it yeah
because we were going to talk earlier
about the the Alchemy of deep learning
because we don't really have any
overarching theories of of deep learning
I mean there's things like ntk and we
spoke about spline Theory and and stuff
like that but there's not really much is
there um there are bits and Bobs a lot
of it relies on unrealistic assumptions
some of it it often relies on the
network being infinitely wide which
actually I'm less worried about that
those ones because infinitely wide
probably means somewhere along the way
you're relying on the central limit
theorem which or the law of large
numbers either of which probably uh
converge a long time you know to
something like a gaussian or or the
expected value a long time before you
really are infinite so those ones I'm
kind of okay with the ones that rely on
um you know the which Network width
being uh um the square of the number of
data points or something that that
starts to worry me because that's
sounding like it's an uh argument based
on some sort of
combinatorial um uh uh
geometric argument and that really might
need that true number to work there is a
history of the experimenters rushing
ahead doing things often introducing
components to our networks uh which
definitely empirically help explaining
them in some way and then later on when
we go back and look at those uh it turns
out they don't do exactly what we said
they would and the theory people lag a
long way behind uh you know some
interesting bits of theory I think are
neural network gaum processes you
mentioned nks they're sort of I think of
them anyway as the basian version of
that and so they can predict some things
about
trainability but largely they predict
things about trainability that we
already knew from experimentation so uh
and they've you know using some of that
they have built um networks with
thousands of layers but those networks
with thousands of layers don't get
Cutting Edge results so you have to
wonder had the experimentalists already
discovered this but then they can't get
a paper out of it so they just throw it
away yes um so uh not to KN that work I
think it's really interesting really
important but just to say it lags behind
and we're in this weird situation where
a whole bunch of the apparatus we've put
in to make networks work doesn't do
exactly what we think it does so when
you first learn about stochastic
gradient descent you're taught that the
reason we put it in is so you can bounce
over local Minima perhaps into another
Valley but that turns out to be absolute
nonsense I don't think that's what's
happening you can learn quite large
neural networks with enormous batch
sizes so not much randomiz or sometimes
even full batch and it still gets the
bottom so why do we still use stochastic
gradient descent uh I mean number one
it's fast you're only using a subset of
the data points and if you've got
millions of data points that saves you a
lot of time uh number two it seems to um
have some regularization effect and you
can actually characterize that and
there's a section in the book about
implicit regularization so you can show
uh for finite basically it's the
difference between it taking infinite
the small steps which you would call
gradient flow and finite steps and once
it starts doing that and combined with
um the stochastic element you can show
that uh there's certain um uh
regularization uh um expression that you
can write in closed form that you're
effectively adding to the loss function
and so the whole learning takes a
slightly different trajectory and gets
to the bottom in a different place like
another example of um experimentalists
sort of rushing ahead would be batch
Norm which has like a really peculiar
history so batch Norm is that a good
fellow uh no is it STI STI STI I had him
on the show
yeah um batch Norm was introduced
originally to tackle something called
covariant shift which I'm not sure I
totally understand it but I think
basically the idea you know you can
correct me if I'm wrong here is that
after you've adjusted the parameters in
the later layers the changes the
parameters in the earlier layers no
longer make any sense
which um from a pure mathematics sense
doesn't make a lot of you know it's not
understandable but I guess it's to do
with the fact that you're not actually
taking infinitely small changes you're
taking finite changes but subsequent
experiments have showed that you can
introduce covariant shift and it doesn't
help um and bnm was later adopted
because after they figured out how to
stop uh um uh uh uh exploding gradients
and um Vanishing gradients uh they
developed residual networks and that put
those problems back in and batchnorm
resets the variance and consequently
stops uh gradients exploding so that's
so it found a use but then it turns out
but there's other ways to solve that
problem you can solve that problem by
just resetting the gradients without the
taking into account the um batch
statistics it turns out that it has a
regularization effect because it's
basically adding random noise the B is
slightly different each time um it also
has more complicated effects where it
allows uh one bit of data to sort of
leak information to another because one
can have an really enormous value which
makes the batch variance large which
then shrinks the other variance at the
other one uh and that's why they don't
use it in Transformers with um uh where
you're using masked uh attention because
the whole point of that is that uh the
the um uh words further along in the
sentence don't have the words don't have
access to the data further along in the
sentence you're sending this all in the
batch they use Lair Norm instead so
batch Norm is an example of something
that was introduced for One Reason
Adapted for another reason it has uh
it's now kept in there or some form of
is kept in there because it has this
indirect regularization effect and it
also has some positive disadvantages
that cause have caused it to have to
adapted in certain circumstances yeah
but because I was going to make a
comment that um if anything as time has
gone on we seem to have less Reliance on
some of these tricks and almost more
focus on just big models more compute
more data and and and so on do you think
there's any truth to that or because you
just said yourself that in many cases
people don't even understand exactly
what the effect of this thing is when
they do ablation studies they tend to
learn actually this thing wasn't needed
after all and is is there a trend
towards simplifying the process um I'm
not sure there is a trend towards
simplifying it I think there should be a
trend towards simplifying it there's a
great paper that came out in around 2020
where they analyze all of the things
that have been done to Transformers
actually scientifically in these
different papers and find out hardly any
of them make any difference yes um and
so you know it's tied to our obsession
with state-ofthe-art you try all of
these different things and then you
don't interrogate too closely whether
this
particular uh activation function really
was critical because now you know it's
two days before the np's deadline and
and you've just got the state-ofthe-art
and um um uh so you people you know and
then no one ever gets around to going
back and looking at these things but I
know when Transformers were
introduced they were super complicated
to train you had to use this thing
called learning rate warmup um there are
very complicated effects of where you
put the layer Norm what what happens
with the variance in the residual layers
the difference in the size of the
parameter the parameter gradients as you
go through this softmax function versus
the path that computes the values versus
the residual you know you got three
parallel links um and they were super
hard to train and they relied more on
training things but actually the sort of
stateof the AR trans training
Transformers which I guess is sort of
what you're implying when you talk about
very large models I don't know if
they've eliminated I just don't know the
answer to your question yeah I mean it's
it's kind it's related to this graduate
student descent thing that we've been
talking about in general and I mean I
was I was um speaking with some folks at
urps and I mean Hinton for example was
always a big fan of contrastive learning
where you kind of like you know you you
contrast an image to all the other ones
in the data set and then I think um laon
at Fair kind of pioneered this
non-contrastive approach which is where
essentially you kind of um I think like
simil Siamese learning but you mutate um
pairs of of of images so you're kind of
like pushing images off off the manifold
rather than comparing them to everything
else and there are so many variations of
that like bow Twins and um you know God
God knows what else it's been a while
since I looked at it but um the these
things are all just millions and
millions of of minor engineering
variations on on an
idea yeah and it's
interesting that this is now what we
take to be results in a scientific
conference we don't necessarily
value
Insight um there are papers that are
searching for insight but it's a smaller
community that is lagging behind and uh
I mean personally as part of the reason
why I stopped doing um experimental uh
research was that um I really love the
ideas and I really love trying to
understand what's going on on but it
often takes you know graduate student
descent it takes the supervisor to you
know an afternoon to come up with an
interesting idea and then you know 20
minutes to persuade the graduate student
it was really their idea and and then um
PhD supervisors you know what I'm
talking
about and then off they go for like six
months experimenting to get the criteria
they need to get into the conference and
I don't think that's a good use of um
scientific time or public money that's
probably a good use of Google's money
and open ai's money but is that a good
use of uh Stanford's money Maybe not
maybe they should be doing something
that's a bit more scientific and that
has some genuine insights not trying to
push this uh uh Benchmark by another
0.1% because often it's just sort of
chance as well there was uh just a
couple of weeks ago there was a paper
that argued that actually convet to just
as well Vision Transformers on um uh or
certainly comparable to the best Vision
Transformers on um uh the imagary
classification task it's just no one's
ever pre-trained them with Google's
enormous training database before and
deployed them at such enormous scale so
we can't even necessarily trust all of
these results I mean we can trust them
in that uh you know in 99% of cases they
did what they did and they got the
results that they got but we can't trust
that the scientific conclusion that
Vision Transformers are way better than
convolution until someone's properly
done some science to to a exactly
comparable experiment and we're not very
scientific Community we've got better in
the you know previous decades it was
crazy people people would make claims
they say okay I've invented this new
method but actually they changed like
seven things and the new method would
often not be the thing that made the
performance gain uh and we've got
slightly better because now we do do
ablation studies and yeah um but we're
still fundamentally not very scientific
where you know these are people who are
trained as Engineers not scientists 100%
And even when they do ablation studies
it's very expensive and it's now become
a game that only the top players can
play and uh open AI for example I mean
they've done some very cool research
over the years you know about scaning
laws and rhf and you know clip and GPT
and blah blah blah but
um you know ultimately and I don't want
to sound cynical but the success of
their methods is is just so much data
they they they've built a Google the
next organization they're hoovering up
all of this data and I think a lot of
people underestimate how valuable the
data of people using chat gpts so
they're hoovering data absolutely
there's there's a reason why they let
people use it for free oh 100% so so so
it's like a it's one step away from a
Mechanical Turk so in the background
they're just generating more and more
data and um they've cultivated this
image that they're doing science and for
reasons that we've just discussed I
think they're doing engineering and um
yeah I I think it it's I'm I'm really
looking for something different now yeah
I I don't mean any disrespect to the
engineers it's no doubt incredibly
difficult to engineer chat GPT on that
kind of scale yeah
um but yeah again it's not very
intellectually satisfying sometimes
those papers there's not many insights
that you can get from them other than I
suppose it's really interesting that you
keep adding data and it you know it does
keep scaling that's yeah not something
we necessarily would have predicted and
so that the amount of data perhaps will
be the limit on these kind of models not
even computation or model is will
literally be the amount of data that you
can hoover up off the internet I um
we'll see what happens but
um uh yeah there's often there's not
many insights and I would like to
reiterate that you can get most of the
interesting phenomena of deep learning
in 40 dimensions and so if you want to
do science or you want to try a new idea
don't even try it on amness try it on
Mist 1D and if it works there then you
know then try scaling up and and if it's
not an interesting idea yeah when you
try it in your tiny data set in 40
Dimensions where no one cares what the
state of thee art is then is it worth
writing a scientific paper about it I
know I think um one thing that happened
with GPT is that it transgressed The
Uncanny Valley and you know we were
talking earlier about how people
psychologize and use all of these other
adjacent mental Frameworks to understand
what's happening the reason why people
are so excited about the singularity and
and AGI and all of this stuff is because
we've got this artifact now which is
remarkable and it's just memorized all
of the data on the internet and I I
think that there's more to cognition
than just knowing you know just being
able to retrieve information I I think
it's something that is very very good
used interactively by human so I'm using
it like I would use Google and I'm
searching for information and it's doing
things which are remarkable but in some
ways you're still the intelligence there
it's what you're asking that's the
interesting thing not the way that it
resp
100% And and that's why when you set it
up in an autonomous Arrangement so
something like Auto GPT or even getting
it to do something on a schedule the
magic disappears it's like this uncanny
valley or you know like the the
suspension of disbelief or whatever it
just instantly disappears and it does
stupid things and you think oh I can't
believe that I was fooled by that I was
Fooled by Randomness there's like the ux
and the fact that it's it's an extended
is what you know David Charmers and Andy
Clark called an extended mind ENT
essentially so it's like a cognizing
element as part of my physical you know
process of of cognition but so many
people are kind of taken in by it in
that weird way yeah I think you have a
um more um I I think you you you have a
higher opinion of it than I do like my
experiments with it haven't been super
successful I um I've asked it repeatedly
to generate a story given a few premises
it almost always generates the same
story with even the same name of the
protagonist um and it knows really well
about some things because there's a
giant Corpus of data on you know and
unfortunately it knows really well about
the things that computer scientists know
about because they are present in
abandon on the web so if you want to ask
him about search algorithms it really
knows about search algorithms and it can
write code for them because there are is
repository after repository implementing
different sort code algorithms but uh
you know once you get to the fringes of
its training data then it hallucinates
or you know they're trying to stop it
doing that so it sort of craps out
altogether yeah um I agree that it's
crossed some uncanny valley and it's
obviously the first thing that sort of
captured The public's imagination for
the first time you know I'm going to
talk to my non AI friends it is possible
to have Nono friends and and they're
asking me about oh do you know about
chat GPT and then interestingly they
often have very strong opinions about
how it works which you're completely
wrong and and uh often don't ask me the
the domain expert what's going on which
I find curious too so people have their
own
theories um
question well I mean I guess um you know
because like s SG and Hinton are are
saying it could lead to Super
intelligence and I guess like based on
the conversation we've just had we know
we know that we're interpolating a data
manifold and we know that these you know
from a computational point of view it's
got fixed compute fixed width and so on
so um like how could it possibly be a
super
intelligence I don't think that it could
I mean it would have to start somehow
manipulating some its internal
representations to make them
more coherent to come up with new ideas
and then test those and embed them in
its body of knowledge it really has no
way to learn anything new other than its
context factor which it forgets every
session yeah um so even if it did have a
way to manipulate the information that
it had introduced into that convex into
that um context vector and formulated
into something new you know perhaps
perform some logical deductions on it or
come up with some new hypothesis based
on it has no way to even remember that
so we're missing all kinds of parts of
the puzzle I don't see how you could
look at that and think oh super
intelligence is around the corner um yes
I mean qu question for you I don't like
talking about super intelligence because
I think it's just
a UNT talkable about topic but we could
talk about AGI so how far is it to AGI
um um well I mean I guess like part part
of the reason I I find it strange is
um you know I think these folks think
that there is such a thing as a pure
intelligence and if it has just learned
a data manifold then that must be
situated because that's the data we've
produced in our physical environment
here on the planet so you know all of
this has been reduced into a data
representation and we're interpolating
that uh as you said it's non-
interactive so it can't you know seek
new information and it's non-reflexive
so it can't reflect on its own context
or or anything like that so it's ju it's
just a an information retrieval system
which has become part of our cognitive
process yeah I don't even like the word
intelligence I think capability is a
better word to use it has certain
capabilities some of those capabilities
are better than humans it can tell you
right now the recent history of
epigenetics off the top of his head and
I cannot do that
um I think if we would be much better to
stop using the word intelligence start
talking about capabilities because
that's very that's a very concrete thing
you can say it can do this task or it
can't do this task and yeah what does
AGI mean it means that
for some broad set of capabilities it
can do a large enough proportion of them
that we think this is general
intelligence even that that's measurable
I'm I'm not I'm not a huge fan of that
and I guess I explain why so yeah I mean
intelligence um I love this uh this
Chinese proverb of Mount luu which pay
Wang talks about he's an intelligence
research and he basically says it's a
complex phenomenon that is beyond our
cognitive Horizon and just like this
mountain range Mount l depending on your
perspective of the phenomenon the range
looks completely different so you do get
these different perspectives like
capability Behavior structure principle
and well so I guess I told you my
perspective but but but the reason I
don't like the because capability is
like you know a behavioral
interpretation of intelligence and the
reason I don't like that is GPT doesn't
do anything you know because all all um
cognitive processes are kind of
externalized and physical and so on and
you know we we we enact a cognitive
process when we use it so the artifact
itself in some sense doesn't have any
measurable
capability I I think I disagree with the
premise that GPT doesn't do
anything well but it doesn't I mean it's
just a bunch of I mean um I I guess this
is quite an esoteric externalized
argument of cognition because you you
could argue that a chess computer like
the meaning of the computation is in its
use so it doesn't intrinsically do
anything you know we use it as part of
our cognitive physical processes given
example of something that does do
something this this is a well well we I
think you're using you're using this
word in a this phrase in a very specific
way that I don't quite follow well we we
do stuff we scuba dive you know we we we
actually do stuff and again maybe I'm
just being a human chauvinist but I I'm
completely on board with it becoming
embedded in our cognitive ecosystem but
well Alo GPT does stuff not very well
but it does it
well does it though I don't
know does it do so I mean yeah like in
in the in the way a cat flap does stuff
you know you can make it do things it
can execute code and so on but I
wouldn't say it has
agency doesn't do it I mean you know
like a agency is is about being able to
wrestle with the complexity in your
environment as an observer with like
limited compute as Warr would say so you
know it's about it's it's about that
kind of complexity differential between
the the situation I'm in versus the
information I have access to and so and
I feel you're going to push back on this
but how do you see reinforcement
learning then isn't that exactly what
you're talking about that there's
literally something called an agent that
has agency maybe via only a sort of DM
algorithm that does something random
until it hits on some reward but um it
explores its complex environment
it solves problems sometimes because
it's getting direct rewards sometimes
because of Prior we've put into it like
uh essentially
curiosity um you don't see that as doing
something I don't know I don't see that
as dramatically different from the cat
flap scenario I I guess
it's you I mean I love this Frisian idea
that agency is actually a description of
certain Dynamics you know which is that
you get the this this kind of like locus
of um information density where a thing
is effectively planning many steps ahead
so you could say therefore it has agency
so in the case of reinforcement learning
the thing I don't like about that is
again we can talk about free will but
someone has designed a reward function
the final chapter of your book is is is
all about ethics and algorithmic
fairness and um right now there's this
interesting J to position between
near-term safety and long-term safety in
fact I I'm concerned that the word
safety has been conflated in in its
meaning and people are using one word
for long-term and near-term risks and in
your book I think you were mostly
focused on on the near-term risks like
you know the bias the misinformation the
fairness and stuff like that but yeah
tell us about the chapter right so yeah
yeah let me tackle the chapter first and
that because the chapter was co-written
uh by which I mean he did all the work
uh with a research called Travis laqua
who's at delouse University which is in
the east of Canada um I didn't feel
personally I'm an engineer I didn't feel
personally confident that I could write
a intellectually coherent chapter on
ethics uh but I was strongly encouraged
to I mean it's sort of interesting that
in an engineering textbook now we need a
chapter on ethics and that's because of
the sort of force magnification of this
technology it's very powerful you can
read this book and go and build things
that are not good for society and so
it's imperative that you at least think
about ethics I'm not trying to tell you
what to do so I wrote this chapter with
Travis who by the way has his own book
coming out next year on value alignment
which people should check out um and he
comes from philosophy background so um
we went through a whole bunch of
iterations of him using long
philosophical words and me try to chop
that down until it could be easily
understood by engineers and we cover um
some aspects of this that are commonly
discussed in um uh in our world as you
say bias explainability things like that
some more philosoph philosophical things
about moral agency and um uh value
alignment uh which are more discussed
probably in his field uh I think I only
let him use the word epistemology once
as a
compromise um uh but I think the
interesting thing about that chapter is
more um the kind of conclusion of it
which is um uh you know it's an appeal
to scientists to realize that uh you
have to take um be aware and take
responsibility for the actions you take
and that you think you're just doing
maths and this is value free but
everything you do is uh they would say
value Laden so and that's often deeply
deeply baked into what we do so much so
that you can't even see that so the very
fact that we judge our papers based on
only state-of-the-art you know whether
you got state-of-the-art and we don't
report whether they're Fair whether
they're explainable how much uh you know
uh carbon needs to be emitted to train
them or to run them no that is a value
that is baked into our community that we
don't necessarily think about and so the
book ends with an appeal to sort of
interrogate yourself um and think about
the impacts of your work which
communities it's going to affect um
individual scientists don't necessarily
have a huge amount of control but you
have some control you can choose who you
work for you can choose which problems
you choose to work upon um in some sense
it's a collective act action problem in
that individual people don't have very
much control but collectively you can do
things uh if you organize and decide
that this is the kind of things we
should be pursuing or not pursuing
um uh and so that's the way the book
concludes and then you know I'd like to
sort of divorce the discussion of that
from any of my personal views on what
the risks of a uh AI are uh which don't
represent Travis's views and also not
the views of the University of b let's
have that on camera yes um I um the book
does focus on near
uh problems and large bias
explainability and so on and touches
slightly on near societal problems uh
mainly just because they're much more
concrete and easy to talk about than uh
the distant problems um I think there's
a good or or harder to talk about
depending on how you look at
it yeah okay uh yeah so where I mean
that's sort of what I'm coming around to
I think there actually are worries at
all levels
from um the people who are worried about
AGI and Beyond AGI I'm not even going to
use the word um uh to midterm societal
levels to level you know to problems
that are there right now and I uh
intrinsically disagree with anyone who
says all of the problems you know we
need to totally concentrate on this and
that sucks the oxygen out of the
discussion about the other thing we're
essentially extremely rich Community you
know we if we put for every 100 thund
million dollar of venture capital we had
one person working on one of these
problems that would amount to a lot of
people and we could study this at all
levels um at the very distant level of
AGI and the word that I'm not going to
mention
um it's really difficult for me to
um I I think they undermine their own
arguments by being so inconsistent and
by know on the one hand on you know you
can read books on this and on one page
they say we shouldn't anthropomorph size
anthropomorph we shouldn't uh on one
page they say we shouldn't
anthropomorphise systems are more
intelligent to us and then on the next
page they say well obviously it will
want to do this and it will worry about
doing this and it will try and stop us
doing this and so so often arguments are
sort of incoherent and you know
and then huge assumptions are made
frequently people say well obviously it
will have free will or obviously it will
have a sense of self and so I don't
honestly really want to engage with that
end of things I don't think I have
anything more intelligent to say about
it but it doesn't mean that we should
totally not worry about it that the um I
think it's really great that people are
worrying about the shortterm issues bias
and explainability I'm pessimistic about
explainability
uh more optimistic about bias I think
the thing that isn't talked about very
much is how we manage bringing this
technology into society so we are going
to replace an enormous number of jobs
and it's not the way that Jeff Hinton
said it there'll be no more Radiologists
what's going to happen is one
Radiologists will be able to do the job
that was formerly done by 20
Radiologists and you see that happening
already now
Gmail automatically completes your
emails and consequently if your job is
mainly about emailing you know you can
do that 1% faster and you need 1% less
people to do that and already what we've
got just uh the latest version of chat
GPT and uh darly 3 they can do a lot of
work um which is going to reduce the
number of uh people we need to
accomplish things in companies and I
think think cause a very large number of
knowledge workers managers HR people to
become unemployed in a short amount of
time um McKenzie I think in 2018
estimated there could be 800 million
people made unemployed by 2030 I don't
know if they're sticking to that I'd say
if anything we're moving faster than we
might have
anticipated and this is really worrying
for society because um a sort of major
reason a major factor that causes civil
unrest and instability is people who
used to have high status uh but now have
lost that status and you're going to
create that situation on mass very
quickly people have entered into a
compact with Society where they put off
earning money for a long time so they
could train to become a lawyer or train
to become a doctor um and now suddenly
their status is all gone they you know
we just don't need that many lawyers or
doctors anymore I think eventually we
will create new jobs to replace those
people but the technology is coming at
us very fast like a wall and I don't
think Society can adapt so I actually
for reasons completely different to the
uh AGI people think we should try and
slow down deploying some of this
technology into society yeah so it's
interesting that that you have that take
I'm a little bit more skeptical on the
um the job replacement and the
enfeeblement part mostly because I think
people are still overestimating how good
this technology is um so for example
we've had co-pilot which is a thing that
helps software Engineers generate code
and I don't think you'll really find any
software Engineers that will tell you
it's going to replace them in fact most
Engineers don't think it even makes
their job that much easier and that's
because there's there's so many many
layers of complexity to being a software
engineer and having like a code auto
complete really doesn't even move the
needle right the syntax of programming
is not the difficult bit the difficult
bit is organizing the code base in a
sensible way oh AB absolutely and I I
think you know I don't want to
trivialize the there's going to be huge
job displacements and Market changes and
and so on but um but there are I don't
know I I still think the good
old-fashioned risks around algorithmic
fairness and bias are still number one I
just I'm reading this book called broken
code which is all about Facebook and how
they did all of their internal
engineering and there was a real
Revolution at Facebook after about 2011
when they changed the news feed from
being chronological to being a kind of
recommender system and everyone hated it
at first but they they forced it on and
essentially any kind of human curation
in many different parts of Facebook got
weeded out whether it was the the news
feed for example they they weeded that
out and they almost wanted to weed it
out intentionally because they didn't
like it when people said they had a
conservative bias or something like that
and it meant that they could absolve
themselves of any responsibility because
the algorithm's doing it and it's just
giving people what they wanted and it
created a sewer there were all sorts of
horrible things Brewing on their
platform that the leadership had no idea
about and in many cases denied for many
years and I think that's a real risk
that we're just starting to um trust the
algorithms to do stuff for us I think it
is a risk but I will point out to you
you've just done exactly what I
cautioned against which is I think there
are problems at all levels there what
you know there is a huge problem with
bias and with um you know algorithms
filtering our information we get but
that doesn't mean the problem that I
mentioned five minutes ago isn't
important and doesn't isn't worth
talking about but this is how the
dialogue always proceeds is that you say
oh bias is really important and I say oh
no no unemployment is important and we
don't we need to talk about both of
these things this is um you know it's
sort
of your reaction is exactly what
everybody's reaction is which is to
minimize the employment which no it's
just to talk about um the thing that
they care about the most to the
detriment of the other things I I'm not
I don't for a moment want to say that
the things you were talking about aren't
important um but I want to broaden that
debate the reason that I bring up
unemployment is because I don't think
it's talked about enough and I think
that are I think it's almost inevitable
or you'd have to make really powerful
arguments to me to say that people are
not going to be able to do their jobs
more efficiently in the coming years and
that that's not going to wind up with uh
a lot of people just you know not you
know maybe maybe not being laid off
maybe just not being hired in the first
place young people just not you know uh
uh if you're a designer at a company you
know let's imagine you make greeting
cards and you used to by hand draw those
greeting cards but now you draw one in
the style you like and then you just
sort of sketch the next 10 and and say
you know fill it in in the style yeah we
have you know you've just gotten rid of
a whole bunch of greeting card designers
I don't think there's a good argument
against that people don't need more
greeting cards we or it'll take a while
to find more human needs for more people
to fill um yeah I guess and I do think
are things that the governments you know
we could be thinking about actually how
to tackle this how um you know so one
thing that might slow it down would be
to make companies actually responsible
for the effects of their products so if
I you know once we're off mic tell you
the recipe for nerve gas and you go and
deploy it I will be an accessory before
the fact and when you get arrested I
will get arrested and we'll both go to
jail
but if chat GPT does that I'm pretty
sure hope and I have a clause in their
terms and conditions that you didn't
read that says in no way are they
responsible for that if you started to
make them responsible for that that
would be a way that you could slow
things down and make things safer um and
I have no doubt that you know people
would still be able to make plenty of
money in this
environment
um and you could have more string you
know they're starting to get at this Joe
Biden's executive order is starting to
get at more stringent examination of
these models um you got to think about
bit how tax is going to work so um well
could could I just touch on something
you said though Rich yeah I I think we
need to distinguish the um the diffusion
of skills versus agency so I I think at
the moment these things are glorified
calculators so do I but calculators made
they got rid of people who used to
calculate they didn't and that's that's
very true but but right now all of the
existing Frameworks we have around I
automate something I create a software
service to do something um because what
the things don't do because what's
really concerning if it ever happened is
is the automation of our agency so and
from an algorithmic fairness point of
view if GPT really was creating a skill
program and it was being enacted on a
server without any human supervision and
doing stuff and no one was responsible
that would be a huge problem but I don't
think that is a problem no I don't think
that's a problem yet anyway although it
may come a time where we gradually seed
control because bit by bit the computer
does it a bit better right um there's
also there's also the kind of real risk
of deskilling so um if for example we
have self-driving cars that driving
cells most the time you get less
experience driving and then it kicks you
into control at the exact moment which
is quite dangerous and you've now only
been driving for a few hours in the last
year instead of hundreds of hours and
you have less experience this is sort of
connected to me you've sort of gradually
given up your agency and that's a real
thing there was a Air France crash in
the South Atlantic that was attributed
to exactly that the autopilot kicked off
as I understand the
situation um and it turned out that the
pilots didn't actually have have much
flying experience in the last year
because most of the time the autopilot
works and crucially they didn't have
experience in near dangerous situations
because the autopilot was good enough to
handle that it kicked them off in a
extremely dangerous ambiguous situation
where sensors weren't working and so uh
there's a possibility that we will seed
our skills gradually and and only be
called to use them in emergency
situations which is not a good
thing I sorry I don't think that's quite
what you were getting at but no no
that's really important so I agree
inment is a huge problem although you
could argue that doing some kind of
easily automated skill because because
we're getting into like purpose and
moral value here and um you might argue
that Society at the moment recognizes
you doing some trivial job as having
value and in the future we might not see
it that way
but I I agree that the the slippery
slope is the problem which is that
they're still doing the same job but
you're eroding their Free Will and
they're becoming automatons and you're
setting them up to fail yeah I mean I
think that might be happening in certain
warehousing kind of jobs already that
you're essentially using them because
you can't build robots quite well enough
yet um yes
uh keep talking what's what's the
solution to that so because it's almost
it sounds like a lite position to say
well let's just keep them busy in in the
warehouses rather than free them up to
do something
different I mean you're asking me for
solutions to problems that no one has
solutions for um I I I do think a sense
of purpose and a sense you know human
beings in my this is really is my
personal opinion not science are built
to work and yeah they're not happy if
they don't accomplish things um it's a
very common phenomenon people retire and
they really love it for the first year
and then the second year they feel kind
of purposeless and lonely and especially
if their job
was something that's
strongly you know uh sort of
corresponded to their sense of self like
a doctor or something then um um they
feel purposeless and unhappy and we are
drifting towards that there are certain
people probably like yourself who are
autodex who are going to go crazy at the
point when they have no responsibilities
I love
it my number one dream and me too but
unfortunately the whole world isn't
built um you know we're not at all a
representative sample of the population
so okay um but but I guess I'm saying
that there's an element of paternalism
to this argument which is I mean I I
agree with you right now and it might be
because of the way work has been enacted
for centuries that our society and
recognition has kind of built purpose
around that and sometimes you have to do
strange weird things for quite a while
for society to change and are we saying
I almost want to um maintain the status
quo is that is that somehow stopping the
progression of what will come
next I mean I think you're sort of
suggesting a grand experiment where we
enter a phase of chaos and Society
reorganizes itself around different
principles well well no but that's
actually a really cool point because
this gets to the core of what we're
discussing also with long-term risk
which is well the word risk is actually
the the key operative word here which is
do we
allow scary things to happen or should
we prevent them from happening well you
know this is an interesting thought
experiment that
um let's let's have a thought experiment
and I'll put you go on I'll put you on
the spot you've been asking me difficult
questions all afternoon okay um there's
a switch and you can flick it and it
makes you know so this isn't really
science fiction we we pretty much know
this is possible uh or most people agree
that it's possible uh there's a switch
you can flick it and AGI is created but
just to human specs so you get uh you
know you know you can create a Tim um
and let's you know let's make it sort of
concrete so Tim can output at the speed
of chat gtp4 now and can draw pictures
at the speed of uh darly 3 and can but
can communicate nearly instantaneously
in the way that computers we already
know do and remember an enormous number
of facts and you get a choice and and
Tim the technology for Tim 2.0 is going
to be randomly allotted to a big
technology company in let's say one of
the nuclear Powers you spin the wheel
it's gonna go to one of them you don't
know which yeah and your question is do
you flick the switch would you flick
that switch and knowing that tomorrow
they're going to make a 100,000 Tims on
their servers and set them doing
something hopefully just to maximize
their shareholder value would you do
that I think this is a great example of
we can't predict the future and you know
the road to hell is paved with good
intentions and there are so many
situations where We Trust our moral
intuitions and even the long- termist
make this argument that they they they
think they can see the future and in
this particular case it could easily end
up being a good
thing so you're saying you would you
haven't quite answered the question well
no the honest answer is I don't I don't
know but I guess this is a great example
where um our our moral intuitions are
almost always wrong right so I don't
know the answer so so the intive answer
I most people would say no don't do it I
think I would say no too yeah but and
and your answer just yes I'm I'm fairly
ambivalent to be honest yeah but you got
to do it the switch is there you're
either flicking or don't there's no
there's no being ambivalent means not
well the only reason I wouldn't flick it
is you know like it's the personal
identity thing in philosophy so I don't
want there being lots of other Tims
around if there were someone else if it
was you I'd flick the Sprint
you because there wouldn't be
psychological continuity with with with
the other Tim so the reason I think
everybody who works in AI should think
about this question is that if you work
for a company that has AGI baked into
its core if you work for open mind uh
open
mind open mind or deep AI open AI or
Deep Mind who you know who literally
write that AGI is their mission yeah you
are moving that switch a tiny bit every
day and you should think about whether
you you know if you so you got this huge
diffusion of responsibility well it's
not me there's thousands of people all
working on this and but you're moving
that switch it's moving up towards the
halfway point to the point where it just
flicks upwards or downwards depending on
which content you on um and so you
should ask yourself that question if
you're actively working towards that
goal and if you as you say most people
the answers are no maybe you should work
for a different
company um anyway Professor Prince this
has been an absolute honor H it's been a
real joy to talk to you I've watched uh
many of your podcasts um I've learned so
much from them and um amazing I look
forward to the ones that come out in the
future cool thank you so much for
joining us today thank you

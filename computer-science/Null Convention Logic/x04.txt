through diffi cult times.

1 K. M. Fant, Logically Determined Design: Clockless System Design with NULL Convention
Logic (Hoboken, NJ:, Wiley Interscience, 2005).


Computer Science Reconsidered: The Invocation Model of Process Expression, by Karl M. Fant
Copyright © 2007 John Wiley & Sons, Inc.

A CRITICAL REVIEW
OF THE NOTION
OF THE ALGORITHM IN
COMPUTER SCIENCE
Computer science inherited its present conceptual foundations from a branch
of pure mathematics that, historically, had been exploring the fundamental
nature of mathematical computation since before the turn of the century. It
is argued that the conceptual concerns of computer science are different from
the conceptual concerns of mathematics, and that this mathematical legacy,
in particular the notion of the algorithm, has been largely ineffective as a
paradigm for computer science. It is fi rst necessary to understand the role of
the algorithm in mathematics.

1.1 THE NOTION OF THE ALGORITHM IN MATHEMATICS
The notion of the algorithm is fundamental to mathematics. To understand
the signifi cance of the algorithm to mathematics, it is necessary to under-
stand the history of its development. The term derives from the name of an
important ninth-century Persian mathematician, Mohammed ibn Musa al-
Khowarizmi, who in about ad 825 wrote a small book describing how to cal-
culate with a new ten-symbol, positional value number system developed in
India. It described simple procedures for carrying out addition, subtraction,
© ACM, 1993. This chapter is a minor revision of my work: K. Fant, “A Critical review of the
notion of the algorithm in computer science,” Proceedings of the 21st Annual Computer Science
Conference, February 1993, pp. 1–6.


A CRITICAL REVIEW OF THE NOTION OF THE ALGORITHM IN COMPUTER SCIENCE
multiplication, and division in the new system. Around 1120 this small book
was translated into Latin under the title Liber Algorismi de numero Indorum
(The Book of al-Khowarizmi on the Hindu number system). This translation
was widely distributed and introduced the Hindu-Arabic number system to
Europe. By the mid-thirteenth century al-Khowarizmi was largely forgotten,
and the term algorism (Latin for al-Kowarizmi’s book) came generally to refer
to computation in the new number system. At this time an algorism was any
book related to the subject. The algorisms were the four arithmetic operations.

An algorist was one who calculated in the new number system as opposed
to an abacist who used an abacus. By 1500 the algorists had prevailed and
the abacists had largely disappeared from Europe.

These algorisms were strictly mechanical procedures to manipulate symbols.

They could be carried out by an ignorant person mechanically following
simple rules, with no understanding of the theory of operation, requiring no
cleverness and resulting in a correct answer. The same procedures are taught
to grade school children today. Computing with Roman numerals, on the
other hand, required considerable skill and ingenuity. There also existed at
this time other examples of mechanical formulation such as Euclid’s method
to fi nd the greatest common denominator of two numbers. The fact that dumb
mechanical manipulations could produce signifi cant and subtle computational
results fascinated the medieval mathematicians. They wondered if it was pos-
sible that the whole of mathematics or even all of human knowledge could
be mechanically formulated and calculated with simple rules of symbol
manipulation.

Gottfried Leibniz attempted just such a formulation in the 1660s with his
calculus ratiocinator or characteristica universalis. The object was to “enable
the truths of any science, when formulated in the universal language, to be
computed by arithmetical operations” [1]. Arithmetical here refers to the
algorisms. Insight, ingenuity, and imagination would no longer be required in
mathematics or science. Leibniz did not succeed, and the idea lay fallow for
two hundred years.

During this period Euclidian geometry, with its axioms and rules of reason-
ing from the simple to the complex, continued to reign as the fundamental
paradigm of mathematics. In the 1680s, after the invention of analytical geom-
etry, and after he had made new discoveries with his own invention of his
fl uxional calculus, Sir Issac Newton was careful to cast all the mathematical
demonstrations in his presentation of these new discoveries in Philosophiae
naturalis principia mathematica in classical Euclidian geometry. A symbolic
analytical presentation would neither have been understood nor accepted by
his contemporaries. Geometry, which deals with relationships among points,
lines, and surfaces, was intuitive, obvious, and real. Algebra, which deals with
arbitrary symbols related by arbitrary rules, did not relate to any specifi c
reality. While algebra was practical and useful, it was not considered fi t terri-
tory for fundamental theoretical consideration. Late into the nineteenth-
century symbolic computation was distrusted and discounted. This attitude is

exemplifi ed by a nineteenth-century astronomer who remarked that he had
not the “smallest confi dence in any result which is essentially obtained by the
use of imaginary symbols” [2].

The dream of formalizing thought in terms of mechanical manipulation of
symbols reemerged with the symbolic logic of George Boole presented in his
book Laws of Thought in 1854. Boole argued persuasively that logic should
be a part of mathematics as opposed to its traditional role as a part of philoso-
phy. Gottlob Frege went several steps further and suggested that not only
should logic be a part of mathematics but that mathematics should be founded
on logic, and he began a program to derive all of mathematics in terms of
logic.

Meanwhile the paradigmatic edifi ce of Euclidian geometry was beginning
to show cracks with the discovery of non-Euclidian geometries that were
internally consistent and therefore were just as valid mathematical systems as
Euclidian geometry. Symbolic computation achieved paradigmatic preemi-
nence with the publication in 1899 of David Hilbert’s characterization of
Euclidian geometry in terms of algebra, Grundlagen der Geometrie (Founda-
tions of Geometry), which emphasized the undefi ned nature of the axioms.

“One must be able to say at all times—instead of points, straight lines and
planes—tables, chairs and beer mugs” [3]. Euclidian geometry was after all
just one of many possible axiomatic symbolic computation systems.

As the twentieth century dawned, symbolic computation had been estab-
lished as the arena of mathematical theorizing, and logical axiomatic systems
provided the rules of the game. The mathematicians were hot on the trail of
settling the game once and for all. They seemed to be on the verge of fulfi lling
Leibniz’s dream of the universal symbolic language that would proffer abso-
lute certainty and truth. The quest was led by Hilbert who outlined a program
to settle once and for all the foundational issues of mathematics. The program
focused on the resolution of three questions:
1. Was mathematics complete in the sense that every statement could be
proved or disproved?
2. Was mathematics consistent in the sense that no statement could be
proved both true and false?
3. Was mathematics decidable in the sense that there existed a defi nite
method to determine the truth or falsity of any mathematical statement?
[4]
The defi nite method of decidability in question 3 was the modern incarnation
of Leibniz’s arithmetical operations on his universal symbolic language.

Mechanical symbol manipulation reemerges at the very foundations of theo-
retical mathematics.

Hilbert fi rmly believed that the answer to all three questions was yes, and
the program was simply one of tidying up loose ends. Hilbert was convinced
that an unsolvable mathematical problem did not exist, “every mathematical
THE NOTION OF THE ALGORITHM IN MATHEMATICS 3

A CRITICAL REVIEW OF THE NOTION OF THE ALGORITHM IN COMPUTER SCIENCE
problem must necessarily be susceptible to an exact statement, either in the
form of an actual answer to the question asked, or by the proof of the impos-
sibility of its solution” [5].

In 1931 Kurt Godel demonstrated that any axiom system expressive enough
to contain arithmetic could not be both complete and consistent in the terms
of the axiom system. This result was the death knell for Hilbert’s program.

The answers to the fi rst two questions were no. There remained the question
of decidability, the Entscheidungsproblem, as Hilbert named it: the defi nite
method of solving a mathematical problem. After Godel proved that unsolv-
able problems (unprovable theorems) could exist in an axiom system, the
decidability problem became a search for a defi nite method to determine if a
given problem was solvable or unsolvable in a given axiom system.

The decidability problem appealed directly to the notion of a defi nite
method, which was also referred to as an effective procedure or a mechanical
procedure. An iterative step-by-step procedure had always been fundamental
to mathematics but had been intuitively accepted and had not been a subject
of investigation itself. One knows an effective procedure when one sees one.

But to demonstrate something about the nature of effective procedures there
must be a precise characterization of what constitutes an effective procedure.

Hilbert made it clear what constituted an acceptable mathematical solution
in his 1900 paper posing 23 problems that he considered important to the
future of mathematics:
. . . that it shall be possible to establish the correctness of a solution by means of
a fi nite number of steps based upon a fi nite number of hypotheses which are
implied in the statement of the problem and which must always be exactly
formulated. [5]
Satisfactorily characterizing this notion of effective or mechanical proce-
dure became an important foundational issue in mathematics and several
mathematicians applied themselves to the problem. Among them were Jacques
Herbrand and Godel, Emil Post, Alan Turing, Alonzo Church, and A. A.

Markov. Each had a different characterization of effective computability, but
all were shown later to be logically equivalent. In 1936 both Church with his
lambda calculus and Turing with his machine proved that no effective proce-
dure existed to determine the provability or unprovability of a given mathe-
matical problem. The answer to Hilberts third question was also no. Leibniz’s
calculus ratiocinator with its arithmetical resolution of all questions proved to
be not possible. Ingenuity, insight, and imagination could not be done away
with in mathematics after all.

Despite the failure of Hilbert’s program, questions of effective comput-
ability have continued to be a fundamental concern of mathematicians.

Through the 1940s and 1950s Markov tried to consolidate all the work of the
others on effective computability and introduced the term algorithm with its
modern meaning as a name for his own theory of effectively computable func-

tions. In the translated fi rst sentence of his 1954 book Teoriya Algorifmov
(Theory of Algorithms) he states:
In mathematics, “algorithm” is commonly understood to be an exact prescrip-
tion, defi ning a computational process, leading from various initial data to the
desired result. [6]
The term algorithm was not, apparently, a commonly used mathematical
term in America or Europe before Markov, a Russian, introduced it. None
of the other investigators, Herbrand and Godel, Post, Turing, or Church
used the term. The term, however, caught on very quickly in the computing

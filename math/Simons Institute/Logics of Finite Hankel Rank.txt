Janos Makosti from Technion on logic, so fine night.
Thank you very much.
Thanks a lot for getting to listen, and for the invitation,
and for the atmosphere.
And this talk is a variation of a talk
I've given for Yuri's birthday.
So there are a couple of people here who I've heard it before.
If I make mistakes, they can correct me.
OK, so I met Yuri 39 years ago.
It's incredible.
And I'm happy to say this again.
We stayed friends inspired of fertile disagreements sometimes.
Good way to put it.
Unfortunately, actually, in the original set,
in the published version, not in the slides,
there were some unproven claims, which
we claimed that we were able to prove,
but didn't include in the paper, and some misprints.
So I shall indicate in the talk where the problems are.
And I would like to thank Moritz Müller from the Logic
Group in Vienna, who reviewed the paper for math review
and stumbled upon those things, and was kind enough
to inform us before.
So the paper appeared in the fields of logic computations
as the second volume of birthday papers for Yuri.
And it is also available if you are not
in a library on archives.
So what I'm going to do is I'm going first
to discuss the Pfeffermann-Wolth theorem for finite structures
and put it in an abstract setting.
And actually, I've been curious since the 80s,
and I worked in abstract model theory,
whether there is a characterization of a logic
satisfies some kind of version of Pfeffermann-Wolth theorem,
even only if it looks like this or like that.
And we actually, in the chapter of the big fat book
without an index, which nobody can read on model
theoretic logics, it's even asked as an open problem
whether we can characterize logics
with some kind of Pfeffermann-Wolth theorem.
And then we have some new ideas on how to approach this.
And I will spend some time in explaining this in detail.
You don't have to get scared.
It's about hunker matrices.
Now, hunker matrices, if you Google them,
you get thousands of paper in numeric analysis.
There's plenty of applications everywhere.
But this is a generalization of hunker matrices
to logic, not to numeric values, to structures.
So we just took over the name because of the analogy.
And at the end, I'll discuss some problems.
So first, some notation.
If you have a structure, tau structure,
the product of two tau structures,
the model theoretic Cartesian product.
And actually, constants sometimes make problems.
But we kind of brush over those problems.
The sum of two tau structures is the model theoretic disjoint
union.
And generalized products are first order
transactions, not necessarily first order,
but for the practical purpose now,
first order transactions of products.
So you can redefine the predicates using first order
formula.
And generalized sums are scalar first order
transaction of sums.
The difference is that in products,
you can also say the new universe consists
of triples of the old universe.
And in scalar transactions, you are not supposed to do this.
We also look at k connections.
Now, k connections, and actually, the hunker matrices
are called connection matrices in Lova's papers.
So you have a graph with, say, three ports, one, two, three.
And you have another graph with three ports, one, two, three.
And you can take the disjoint union,
but identify those elements.
And we denote this as disjoint union 3 or k,
if there are k such ports.
And these are called connections rather than disjoint unions.
And they cannot, strictly speaking,
be produced as transactions, but behave in a very similar way.
So these are k connections.
And actually, in Lova's paper, you
can also have identifications which
are more complicated, but I only deal with those.
You could also say those two points are identified across
something or another.
Now, and then again, generalized connections
are scalar first or the transactions of k connections.
Now, the Pfeffermann-Watzier-M for finite structures,
it just says that if you take any of those operations
and you want to check a formula in the product
or in the sum or generalized sum,
then you can reduce this to checking many things
in the components, and then somehow piece them together
using a Boolean function.
We'll write down this precisely.
And we are at first thing we have to discuss
is what are the logics on finite structures which
satisfy some version of the Pfeffermann-Watzier-M.
So if we do this strictly speaking,
so let this be a generalized product on tau structures,
then there is a t, a natural number, and a function
of the row mapping formulas into strings of formulas with,
let's say it's given like this, where k is a natural number
and a Boolean function.
So the idea is that this is the formula
as you check in the first structure.
These are the formulas that you check in the second structure.
And then you look at the results, you get zeros and ones,
depending on whether the test is true or false.
And then you plug in those zeros and ones
into a Boolean formula, and you get
one if and only if the product structure satisfies
the formula and using the components.
So just talking about sentences.
At the moment, I'm talking about sentences.
You can extend this also to the numeric parameters
and all kinds of things.
But here, in this talk, I talk only about sentences.
And the formulas which you get to do this
are called reduction formulas.
And the important thing is that the number of tests
you have to do doesn't depend on the size of the structure.
And actually, this was used in Coursel's theorem
about checking in structures of bounded trivius
because the structures can be decomposed.
And then you can do some thing which
people call dynamic programming to compute
the value of the final structure by climbing up
the three-digit composition.
Now, the Feynman-Wald theorem is also
true for infinite structures and can also
be stated for infinite products.
Here, we are only interested in the finite.
And actually, the tricky part of the Feynman-Wald theorem
in the original 59 paper is indeed
that if you take an infinite product,
and then you have also to discuss what happens on the index
set, but in the finite case, this trivializes.
Now, the Feynman-Wald theorem is also
true for monodic second order logic
if you take some like structures rather than products.
And this was certainly known already in the 50s.
Maybe Dana can tell us about it.
It turns out, I always, when I was a student,
thought that Freisei was sitting in Algiers
and Tarski was sitting here.
Well, it turned out they were all visiting and discussing
things together.
And ultimately, they presented the paper.
But so saying that this happened independently
is not quite true.
It happened collaboratively, somehow,
with several formulations of the theorem.
The version for monodic second order logic
appears explicitly in Leuchli's 66 paper, which
Schellach didn't read in detail.
And so he reproofed it in 79.
And as far as I know, Yuri didn't understand the sketch
Saharon gave.
And then he wrote a paper figuring out
the details, which is rather tedious.
But all this is difficult because of the infinite case,
not of the finite case.
Actually, there is this parameter t.
So if you have a formula of quantifier rank something q,
then the reduction formulas may have an increased quantifier
rank, depending on the character of the generalized sum
or product.
Depends on the quantifier rank of the transduction.
But if you just take normal sums or normal products,
then t can be taken to be 0.
And this is an important ingredient in the meta theorem.
So anyhow, so we can reformulate now
what happens in the monodic second order case.
So now we have sum like operations.
Sorry, sum like, oops, there is something there, is it?
Maybe it should have been somewhere.
Anyhow, generalized sums are arbitrary transductions
of sums.
And sum like is if the transduction is quantifier free
to make sure that the t is 0.
And then we can formulate the sum like operation
on tau structures.
Then again, there is a function and a producing reduction
formulas and so on.
But now, depending, no, no, this should have been sum like.
This should have been generalized sum.
And you get plus t.
And if you take quantifier free transduction,
then you can take t equals 0.
And this is the way it can be used for Corsair theorem
on model checking for monodic second order logic.
Now, indeed, there are many algorithmic uses.
I have also written a long paper on this
with new applications also, well, then new applications.
MSL definable graph properties can
be checked in linear time over graph
classes of bounded trivus.
And one way of proving it is just
proving the fulfillment of what theorem
and then doing the dynamic programming part.
We also have, as a use of this, the MSL theory of graphs
of bounded trivus is decidable.
My zezer, we have that MSL definable graph polynomials
are computable in polynomial time over graph classes
of bounded trivus, which I noticed first.
If the class of tau structure has a decidable monodic second
order theory, so does its closure under this joint union.
So you can play with this.
But in all these applications, t equals 0,
at least for algorithmic parts, is essential.
For decidability, it's not so essential.
So if you want to do this into an abstract setting,
then we have to see what we can do.
Now, a Lindstrom logic is just an abstract object
where you have a set of formulas,
but they are just names of something.
And the set of structures and the satisfaction relation.
And what I add additionally is I want
some kind of rank function.
It doesn't have to be a quantifier rank,
but it should be something which tells you
how expensive it is to look at this formula.
So in practical application, usually one
used the quantifier rank, but in open problems
we'll discuss, you could have another option.
If the set of sentences in the logic
is uniformly computable, then we call it
Gurevich logic, motivated by his paper on the challenges.
And the row, the rank function is nice.
If it holds that for a finite vocabulary,
they are up to logical equivalence,
only finitely many formulas of this quantifier rank,
which happens to be true in the applications.
But you can have a different rank function.
And we'll see what we can do.
You require just that the syntax is computable.
No, no, everything is computable.
But you have to be able to decide who is a formula.
I mean, if you.
Definition would consist of lots of foretimes.
It's very essential that you have an effective compiler.
Yes, yes, yes.
That was a bit shortcut here.
Now examples of nice logics in this sense
is first order monadic, second order logic.
But also, if you add the model or counting quantifiers,
you get nice logics.
They are dk, m, x, phi of x, means
there are exactly k elements, modulo m, satisfying phi.
But here, one has to be careful.
This is why I show this example, is
if you take the normal quantifier rank for this to be one,
I mean, adding one, then all of a sudden
you get infinitely many formula of the same quantifier rank.
But if you make it dependent on m, then you can satisfy it.
So it happens to be nice.
Now, indeed, there are three elements.
Modulo 5 is expressible without the additional quantifiers.
But then you have to write down first order quantification
and to say that there is 1, 2, 3, which are not taking something.
You take them away, then you get something 0 modulo m.
And so this is a bit tricky to be nice.
One has to adjust the definition of quantifier rank.
So if you take this function row 1, then it's not nice.
And if you take row 2, then it turns out
to be nice for our purpose.
Now, for an abstract framework, row
can be any function attaching some cost to formulas
given as syntactic objects.
In concrete examples, this is usually
some variation of quantifier rank.
But maybe if you want to do, I don't develop this now here
in two depths, but if you want to define abstractly what
you mean by such a cost function,
you probably would add a condition
that it is weakly monotonous with respect to subformulas.
If c is a subformula of 3, then the rank is smaller
than smaller equal than the rank of the big formula.
And maybe Boolean combinations of formulas
do not increase the cost.
But I don't go into this now.
Every abstractly instrumental logic
can be given a canonical syntax by adding generalized quantifiers
for each definable class of structures.
And if you have an abstract logic
and you just say, for every similarity type,
I introduce a generalized quantifier
which says this is the logic, then you
get some crazy syntax.
But at least you can give it a crazy syntax like this.
So in this sense, but then the question
is how you define in this crazy syntax
the quantifier rank such that the logic comes out nice.
Now, I spoke before about translation scheme.
So a translation scheme is just an interpretation.
But I look at it as a transducer.
If you're given a structure, you apply.
You have a sequence of formulas which
define the interpretation.
And you apply these formulas to the input structure.
Then the new relations are defined by these formulas.
And you get a new structure.
And then you only have some complicated or tedious
conditions about the compatibility,
how the r-ity of those formulas depends
on the similarity type, and so on.
But this is what we mean by a translation scheme.
Now, this can be defined for any logic which
has a notion of free variables.
But in the applications, we usually,
when we speak about some like, then these formulas
have to be quantifier-free.
And if you speak of generalized terms,
then we allow here also quantification.
And the quantifier rank of a scheme
is just the maximum of the quantifier ranks
which appear in those formulas.
So the translation of a formula is obtained.
The structure is mapped into the new structures.
And the translation of the formula
is a backwards substitution.
And then you have a, yes, OK.
So the translation scheme is scalar if k equals 1,
if the formula phi doesn't increase the r-ity.
So OK, so I said this already.
So we have those operations.
And some like, and product-like, in contrast to generalized,
just means quantifier rank is, and then we
can reformulate what is the Pfeffermann non-property,
but you have seen it already twice.
So I believe that you can read this off towards yourself.
Now, examples of logics that have the Pfeffermann
non-property is first-order and counting first-order
have it for product-like, and first-order and monadic
second-order, and so on have the property for some like.
I forgot to say.
So in the abstract definition, I want
that the quantifier rank of the formulas you get
is the same as the quantifier rank of the formula
you want to check.
And for some operations, you have also first-order and monadic
second-order logic together with counting.
And the same is true also for connection-like operations.
So here comes the question, what can
we do to characterize logics which
have the Pfeffermann property for some operation?
Now, an operation is smooth if it has the property
that if two pairs of structures satisfy pairwise the same LQ
sentences, then applying the operation also
satisfy the same LQ sentences.
Now, if you have compactness, then
we showed in the very old paper with Schellach
that indeed the converse is true.
Smoothness implies Pfeffermann-Wott,
and Pfeffermann-Wott implies smooth.
But in our context where we are here,
this is a useless remark, and it doesn't seem to work in general.
So here is an open problem.
Find a natural example of a nice logic and an operation
such that the operation is smooth,
but it does not have the Pfeffermann-Wott property.
I wasn't able to write down a convincing example.
So here comes actually the important new part.
Hunkle matrices for properties.
A property over a vocabulary tau is just a class of structure
closed under isomorphism.
And the Boolean-Hunkle matrix is the infinite matrix
where columns and rows are labeled by finite structures.
And inside the matrix, you compute the element of the matrix
labeled A i, A j is 1 if and only if the operation
of those two structures happens to have the property.
And the property has finite rank if the corresponding infinite
matrix has finite rank over the field of two elements,
or Boolean field.
So you can also define an equivalence relation.
Two rows naturally define an equivalence relation.
The rows naturally define equivalence relation.
But two structures are equivalent.
If they have identical rows, this can also
be said in the following way.
A is equivalent to B with respect
to the property and the operation.
For all sigma structures, if you allow changing of vocabulary,
otherwise for tau structures, we have that A combined with C
is in phi if and only if B combined with C is in phi.
And the property has finite index
with respect to this operation if there are only
finitely many equivalence classes.
And it's easy to see that finite index
is the same as finite rank.
So these are just two ways of looking at it.
And actually, this is inspired by an erode Mahill theorem
for words rather than structures.
Now, why are those ankle matrices important?
Well, they play or the connection matrices
for connection operations play a very important role
in the Lovath theory of graph limits and so on.
But here is a theorem we proved in 207.
Let phi be a graph property such that the corresponding matrix
has finite rank for the k connection,
then phi can be checked in polynomial time on structures
of 3 with at most k.
So this is actually a Coursel theorem, which is logic free.
It just says the only thing you have to know
is that this ankle matrix has finite rank for the k connection.
Well, there is a lemma, then it has also finite rank for L
connections, L smaller than k.
And then you can redo, but in a more complicated way,
the dynamic programming argument.
And this works.
Now, I proved with my student Nadia a similar theorem
for clique width with a modified sum like operation.
And the important thing to note here
is that there are uncountably many graph properties which
have finite rank for a fixed operation.
So this really shows, though the argument is a bit not
cheating, it's true, but it depends on some indexing
that for every subset of natural numbers you do something.
But it shows that there are many more properties
with finite rank than there are.
This is really a vast improvement on Coursel's theorem.
Now, what we want to do is, for every definable property
in the logic L, we say that if every property has finite rank
for sums or products or connections,
we say that L is a finite S rank, P rank, or so on.
And what we proved with Tomer and Benny Godlin in 2008
is that if you have a nice fragment of second order logic
and you have a smooth operation, then
every definable property has finite rank.
So this really shows that there are many more cases
where you can apply this theorem.
As a consequence, we get that monadic second order
and counting monadic second order logic
have finite S rank and C rank.
And additionally, if you take first order versions,
then you have also finite product rank.
Now, the Phefermann-Wort property implies finite rank.
This is what is behind this proof.
This is why we get to this connection.
So we can formulate a theorem.
Let L be a nice Linsström logic and box smooth operation.
Then any definable property has finite rank.
And we can go on with this.
And we get that if L has a Phefermann-Wort property
for all sum-like operations, then it has finite rank.
And what is left is the following question.
Do we have a converse theorem which
says that maybe in a way, they're
saying that every finite rank logic
has an extension which satisfies the Phefermann-Wort theorem.
Now, finite rank alone doesn't help
because there may be not all the formulas are there.
So we have to find an additional condition
and we'll take a closer look.
So a logic is if it has a Phefermann-Wort property,
it has the additional property that every equivalence
class in this, having the same rows in the matrix,
is itself definable by a formula of the same complexity.
So we can say a logic is box closed
if this is true in general.
And S closed, P closed, if it is true for every sum-like product
like operation.
And the theorem says that if a logic is nice
and has the Phefermann-Wort property for all sum-like
operations, it is S closed.
So this is a hint that maybe we hit the right property.
And the main theorem I want to present today,
and this is a corrected version of it's only a misprint
in the paper of theorem 16 in the birthday paper,
let L be a nice S closed logic.
And the following are equivalent.
L has the Phefermann-Wort property
for every sum-like operation.
All sum-like operations are smooth.
For all phi and every sum-like operation,
the rank of phi is finite.
And for all phi and every sum-like operation,
the index is finite.
And the same holds for products and connections.
So this is the story I wanted to tell.
And then there are some open problems
which we can look at afterwards.
OK, the main open problem is that a logic
whose definable properties are exactly the ones of finite rank.
And what you can try to do is say, take your logic,
take all the things which are definable,
and add them as quantifiers.
But it's not clear how you have to define the rank function
to make everything work.
So this is what we overlooked in the paper.
We thought somehow we had the definition of this rank function
which doesn't work.
So this is basically what I have to say.
It is still a proof that there are many more classes
of structures which have a finite rank for a fixed operation.
But it's not clear that you have that many classes of structure
which have finite rank for every operation.
Thank you very much.
Thanks Mr. Padres.
It's getting hot as well.
Thank you again.

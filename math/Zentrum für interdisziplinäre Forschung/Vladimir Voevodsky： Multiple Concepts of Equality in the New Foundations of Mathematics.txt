My talk is, as you can see, it's called multiple concepts of equality in new
foundations of mathematics. And it is, I'm concentrating on equality so much
because this is the issue which is usually the most hard to understand.
And sometimes when, I think originally, many mathematicians who saw this slogan
of univalent foundations that isomorphic objects are equal,
so that way, Wonski kind of got bonks. And because everybody learns in
college or whatever in the first course of algebra, that it's important to distinguish,
to know the fact that isomorphic objects are not equal.
And this talk is precisely to try to explain how this contradiction gets resolved and how in
the resolution of this contradiction new concepts and new problems appear.
But I will start with something general and again because of the topic of the conference,
and we don't have a well-articulated concept of foundations of mathematics.
We cannot say, okay, it is, for example, not a mathematical but a philosophical notion,
so in order to understand it, you have to read this 100 pages treatise on what foundations
of mathematics are because there are no such 100 pages treatise. And it is indeed
a thing which many of us understand in different ways.
So I have formulated in my 2014 lecture, I'm saying it here because I'm going to
decide myself, so I have to say that I'm doing it. The following kind of description of what I see
is the three main components of which any foundation should have in order to be useful.
So the first component is a formal deduction system. So it's a language and rules of manipulating
sentences in this language that are purely formal such that a record of such manipulations
can be produced and then can be verified by a computer program. And this is the first component,
this is not the whole thing. The second component is a structure that provides the meaning to the
sentences of this language in terms of mental objects intuitively comprehensible to humans.
So here when I say a structure, I mean something like a structure in the mind world, a structure in
... I just couldn't find the better word for it, but it's something which allows us to read the
sentence of formal language and to imagine something understandable in our head. And
we are only talking about things which are directly described by the language. So obviously
this things will be kind of simple or at least these things will be restricted class of things.
So the third component is the structure which enables humans to encode mathematical ideas
in terms of this small class of objects. So in
in ZFC-based foundations, I should say, and here I want to notice that in such kind of
discussions where we discuss the meaning of the words and the meaning of what foundations are,
we really should be a little careful in distinguishing between ZFC-based foundations
and ZFC itself. So ZFC itself refers to this theory of predicate logic with one predicate symbol and
a bunch of axioms and we can include together with it and then we start to expand on this
on the meaning of this word by first adding to it all of the
rules of predicate logic which actually make a theory in the formal deduction system because in
order to use this theory as a formal deduction system, we need to know for example how to prove
sentences in this theory. So to specify a formal deduction system, one has to specify also the
rules that allow one to prove sentences and there are in fact some choices because there is not
one canonical presentation of how to do it in predicate logic. There is for example sequence
formulation versus natural deduction style formulation. They lead to equivalent concepts
of probability but for people who are working on the subject and like to be precise in details,
they actually matter. There was in Howard's famous 1969 manuscript where he introduced what became
known as the Harvard correspondence among other things actually. He makes a special comment that
up to this point, I was correct in the 69 but beyond this point, I was slightly incorrect
because I was using the sequence formulation and it was pointed to me by Per Martin Liff
that the sequence formulation is not entirely appropriate here and one should use the natural
deduction, again, some natural deduction style formulation instead. So it is an issue.
But we, of course, normal mathematicians have no idea about this kind of thing.
So let's say we have chosen that and so then we extended the name GFC
to describe a formal deduction system. So now we know where our sentences
and we know how to make proofs with these sentences and we can write a computer program
which will verify the validity of these proofs. But the sentences talk about things which are
completely kind of, they talk about, it's one sort of theory of all things. So everything
is just belongs to one sort. Everything is of the same kind, is of the same type.
And the only thing which exists in this theory is this one predicate symbol in two variables.
And then there's a whole bunch of axioms. So why there is any reason at all to believe that
that this sequence of axioms is consistent? I mean, how do we,
is there any reason to think, for example, that the axiom of infinity is consistent with everything
else? As long as it remains just a theory, we cannot do anything about using it. So
we need the second component. We need to think about this object in some way which
will provide us with an intuition about what might and what might not be true or provable in
this case. I mean, what might and might not be provable. We need intuition for that.
So the way this, now because this is a structure in the mind world and it's not really
a, of the level which we have good experience in describing precisely. So it depends on a person,
probably. It depends on a person what one, how one can imagine what this X's and Y's and Z's
in the Cerebellum Frankel are. So I like to think of the objects that ZFC describes as the fruited
trees without automorphisms and such that the length of each branch is finite.
So let's call these trees such trees Cerebellum Frankel objects. So the reason for it is more or
less clear. So let, to explain the reason, to explain the reason, right. So the X,
epsilon Y predicate is interpreted as the condition that X is isomorphic necessarily in
a unique way to a branch of Y at the root, at the root branch of Y. So basically a tree,
if we start thinking about Cerebellum Frankel objects as just about sets,
then we should think about the set of root branches of this tree.
And totally ignore the fact that these root branches are themselves such trees. Now the
requirement, since we consider these branches necessarily up to an isomorphism because what
are rooted trees without talking about their isomorphism, we can only distinguish them
up to an isomorphism. Otherwise we'll have to provide some extra structure on them.
So the condition that it has no automorphisms, if you look at it inductively, first of all,
it implies that you cannot permute in any way. Well, it means that you cannot permute in any way
branches at any node, right. So that would be an automorphism. So no automorphisms then there are
no structure preserving permutations or branches at any node. So one can start with the root node.
And then the branch is what we call an element of this set. And to permute the two,
we would have to have two different elements represented by the same tree. And this is precisely
what is forbidden by the axiom, which says that two sets are equal if they have
the same elements. Because if there are two copies of the same element in my tree,
then I could just as well consider another tree with one copy. And it would have the same elements
because the set is the same. The collection of trees represented as root branches is the same.
And I would have two different sets with the same elements and this is not permitted.
So that is the condition of no automorphisms, which one has to enforce.
And then the condition of branches of finite lengths is the condition which
has something to do with. I don't exactly remember in which axiom one has to side here because I
really don't remember the axioms of DFC, I'm sorry.
But it's a form of well-founded. So now that I have this picture in my head,
I can understand why, for example, the axiom of infinity might be allowed to exist.
Why it might be true about something. So to have a set with infinitely many elements,
I should have a tree with infinitely many root branches and all of these root branches should
be different. And each of them should also be this tree without automorphism. So that's easy to
imagine. They're a zillion of different versions one can think of. For example,
one can think of a tree whose branches are just linear trees.
So it would be a tree with this root. The first branch will consist just of the root of
of this branch. The second branch will go like this. The third one will go like this and so on.
So clearly such a thing exists. Now actually, as a tree, actually I'm not entirely sure how to
define it in DFC. It is a union of sets where, sorry, it's a set which whose elements are of
the form. And this is n times. And then you take a set which consists of all of this. I'm reasonably
sure that it can be somehow generated, but I don't exactly know how. But there are also
different ways of doing it. And the fact that there are different ways of doing it,
if you want to get your branches to form accountable in terms of set and to be
numbered by natural numbers in a natural way, that would mean representing natural numbers
in set theory. So different representations are just different ways of
choosing such an infinite sequence of pairwise distinct different trees,
pairwise known as a morphic tree.
Now, the third component of the DFC is how to encode
general mathematical concepts in terms of the objects which are directly associated
or directly addressed by DFC sentences directly associated with this variable in the DFC sentences.
Now, this third component is very complicated and hardly ever taught, I think, except
at least to mathematicians. I mean, it's not really taught to mathematicians. Maybe taught to
people in logic. But to kind of mathematicians, non-logicians, it's not taught. And it is
involved and it's ingenious. And the fact that everything can be really done this way
is quite amazing. So for example, it involves
placing a DFC structure. So these objects which are really described by DFC, let's call them DFC
objects. So a set which corresponds to the DFC object is this set of root branches. So if I have
a set and I want to represent it as a DFC object, I have to take each element of this set and
represent it by a different DFC object itself. So that is a structure. So to me, DFC objects are
assets with structure. But in fact, one kind of tries to remove this structure as much as possible.
And instead to explain how to do the construction related to sets with this object and then from
sets one builds up and the rest of mathematics. But it is profoundly, well, it is very complicated.
So as an example, I was asked by someone that it is strange to explain that in
Sir Mellor-Franckel's theory, the function x goes to 2x is represented by a set. And this set is the
set of pairs, x, y, where y equals 2x. And someone told me, so this is a circular definition in order
to know what it means y equals 2x, you already have to know the function 2x. And indeed it's, but
it is possible to define this function from the axioms of DFC. But
I probably don't know personally anybody or at least don't know anybody well who would be able
to do it without looking into a Burbaki theory of sets. So now when we come to
univalent foundations which don't really yet exist, we may try to also think about these three
components just as a, and we can see whether it helps, right? So in the case of the univalent
foundations at the moment, it's the first component which is most fluid, so to speak.
It's the formal system which we cannot quite choose and we're experimenting with different
formal systems. And we don't yet know even if it will be a good idea to fix one as the canonical one.
So it is this first component which
where most of the work happens.
Now actually, sorry, I shouldn't have started this that much. I think there is kind of a
slight missing, but let me just talk through it.
Now the second component on the other hand is to some extent better understood and it's
closer in its nature to,
and the third component is projected to be much smaller and much less
sophisticated in a sense that the objects directly addressed by the formal system
are already objects which we can easily use to build other mathematical concepts.
So indeed the idea is that the formal system directly describes
homotopy types or rather directly describes anything which provides the model for homotopy types.
So we can directly use the intuition of homotopy types to think about the formal objects.
Then the question becomes how to relate homotopy types to the rest of mathematics.
And here we have, unlike the previous case where we had these
trees described by the theory, and then to use these trees to describe all the rest of mathematics,
we had to do things which kind of ignored the actual tree structure and
we had to do strange things. This connection between trees and how
thermoferential objects are used is not clear. In Univalent Foundations, this connection is clear
because of this main paradigm of the Univalent Foundations.
That mathematics should be considered as studying structures on higher sets
and higher sets are modeled by homotopy types.
And there is a concept of H-Level and then the types of H-Level 0, there is one point of
H-Level 1, the types are truce values. Of H-Level 2, the types are sets. Of H-Level 3,
they become modeled in classical mathematics by group weights. And in higher levels, they become
modeled in usual mathematics by higher group weights. So they correspond to homotopy types
with more non-treevel homotopy groups. But even if we don't look at these
higher levels, then we have a very beautiful picture of lower levels.
This allows us to make this third component very direct, in a sense, without unnatural
constructions, without unnatural choices. The fact that it actually works, and I mean,
we do have a physical proof that it actually works, that it's not just talk. And this physical
proof exists, for example, in the form of the Unimath Library for Coq. And the Unimath Library
is using very basic, well, relatively basic, by today's standards, formal system for Univalian
foundations. Then this formal system is adequate to prove lots of theorems about homotopy type.
And that is confirmed by the foundations part of the Unimath, which I have written in a few
months' time, and each time being amazed that I can prove the next property, which I feel
should be intuitively true. But I would, a priori, have absolutely no reason to believe
that it can be proved in this Martin Lyft type theory. And so every time I could prove something,
it was a great surprise. And in fact, this foundation part, maybe, of the Unimath Library,
forms the part which really belongs to foundations. And then the rest of it is more about mathematics
itself, formulated, formalized using this foundation. But if one looks at Unimath Library
as this physical object, as this actual thing, which actually works, then one can understand why
Univalian foundations are kind of a real thing, and in what sense they're better than the ZFC.
Now, that was supposed to be actually, sorry, a couple of slides. I somehow missed them in
the preparation. Now, I want to say that the Univalian foundation started their growth
supported by two needs. I would say human needs, because these were needs of actual humans.
And that's what provided the energy for the development of the theory.
So the first one was felt most strongly by mathematicians who worked with categorical
and then hierarchical construction. It was and still remains to be difficult to articulate,
but it certainly has something to do with our concept of equality of abstract objects.
I will say more about it later. That for me, that was the first one which I felt. I kind of,
I had this need in me. I could feel it because I was returning to thoughts about how to do it,
how to do it, how to do it, how to do it, and I had it from since I was 20, I don't know, 20 years
old. And the second need was felt most strongly by people from very different slides of academic
community. And originally it was articulated as the need to be able to use computers to verify
complex mathematical constructions for mistakes. And this is a need which I only started to feel
much later in my life, but which I also can feel now. And now it extends actually from mistakes
also to the need to keep mathematics precise and clear. And to just allow
to have a method to ensure that our concept of what a proof is remains strong and that it does not
kind of dissolve into making proofs philosophical arguments. I mean, I have nothing against
philosophical arguments, but there are places in philosophical texts or philosophical parts of a
text, not in proofs. And not in mathematical proofs, I'm sorry. And that is a serious thing
which is going on very much in mathematics. And doing things with formalization puts a hard stop
to it. If what you wrote was bullshit kind of covered by nice words, you wouldn't be able to
formalize it, period. So it is a very, there is a need and it is a very good solution for it.
Now later, the growth of the Newell and Foundations became supported
by CERG need that was articulated as the need to have a better foundation for constructive
mathematics. That was to me completely unexpected. So I totally came in this whole story as a
pure classical mathematician. I always laughed at jokes which put constructive mathematicians
in funny positions. And I was totally a classical mathematician. And now I'm not.
So the process by which it happened again has a lot to do with Unimath. When I started writing
Foundations, the first part of the Unimath, and when I discovered by experiment that I can prove
very many things, some of them non-trivial, some of them unexpected to me, that I can prove a lot
of stuff without using excluded middle or axiom of choice. And this is basically the two main,
at the moment there is basically the two main things which separate classical
proofs from constructive proofs. And I was just writing it and I was thinking about how to make
the proof better, but I simply didn't need it. So of course when you wrote, when you invested a
lot of work in writing Foundations, and you have a foundation where you can, like you already done
your component one, you don't done component two, you've done a big part of component three,
so you can actually now work with real mathematical objects. And you have done all of it
without ever needing stuff like excluded middle. That kind of makes you want to continue in the
same way. And so I, then I started to put a little bit of effort in more difficult places
where there is real kind of ramification between constructive and classical.
For me, it came with the definition of what a field is, because that was the first definition
in my formalization. I was just following in a Burr-Barty algebra. And well, with some
variations, but the first thing where you hit the problem with excluded middle is the definition
of what a field is. And here instead of introducing excluded middle, I went by the route of
actually trying to understand this difficulty and to resolve it in a constructive way. And
that's how Unimass is now. There are now, for example, there are now two different concepts
there. There is a concept of a field, and then there is a concept of a constructive field.
And real numbers, for example, again, recently formalized by Catherine de Ler,
they're not a field, but they're a constructive field. So it's a little strange,
maybe naming, but that seems to be relatively standard. So
we explore these ramifications, and when we invested some time into it, it started to
work well, and we were able to do much more now with constructive mathematics.
So, and yes, and then I should come back to the slide that it's at some point, in fact,
I know more or less precisely when. I think it was sometimes around June 2010, when I kind of
out of total curiosity provided a proof, formalized proof, that the univalence axiom implies
function, extensionality. And I just thought that it's kind of funny thing. I mean, not funny,
but that's it. Well, that it's nice. I mean, I can organize things a little better. And I
emailed it on the list back then. It was more kind of active, I think, and then I think to,
no, was there a list? Maybe there was no list. But I remember that I emailed it to
Thierry Cochand, and Thierry later on, he said, he told me that that was the moment when I kind of
got seriously interested in your univalent foundations. And of course,
so now this third need provides a lot of interesting things which are happening in
the univalent foundations are supported by that need. But again, the slides of the community
which has this third need is intersecting somewhat with the slides which has the second,
and almost doesn't intersect with the slides which has the first. So this is the kind of
sociological portrait of our current community from my standpoint.
So now returning to the first need, which is a very important one because, well,
I remember discussing it many times with Michael Capronov back in late 1980s.
And the typical discussion we had then can be summarized in the following form.
We need to have a language where we are, sorry, we should not be here, but whereas the objects
of a category are never equal. And then there is some continuation of the discussion, and then
somebody says, but obviously x is equal to x, so this is impossible. And that sort of ends the
discussion or loop set in some form. And we couldn't get out of that loop. And we were really
thinking about how to get a formal language which would tell us, well, in fact, we wanted
a formal language which would somehow tell us that we need all these high coherences,
which I now understand is yet a slightly different problem. But that's how we saw it.
And in all approaches to the formal deduction system of the New Orleans foundations
that are being developed today, this conundrum is resolved by the existence of two concepts of
equality. So the first one is the concept of the substitutional equality. And the second one is
the concept of the transportational equality. So I want to introduce these two names and
two concepts. And I suggest them, and I will explain what they mean. I suggest them to put
some order in this order of naming equalities in different theories by different adjectives.
So in the intentional Martin-Leuph type, in the intentional Martin-Leuph type theories,
there is one substitutional and one transportational equality. The substitutional one is the
definitional equality. Now this is for more types of theoretically minded people. And
the transportational one is the one witnessed by elements of the Martin-Leuph identity types.
So it's common to, so we usually say definitional equality and propositional equality. And that
may be their proper names in the Martin-Leuph type theory. But I suggest that they actually
belong to two distinct classes. And the names which I suggest for these classes are the name of
substitutional equalities and transportational equalities.
So only substitutional equalities are equalities from the point of view of Leibniz's principle
of substitution of equivalence. In the Martin-Leuph type theory, definitional equality
cannot be required or postulated, but it can only be checked.
So the feelings that objects of a category are never equal is resolved into the fact that a true
substitutional equality of two objects of a category cannot be postulated.
The fact that x equals x remains true because x is definitional equal to itself,
which can be easily checked. And so we get out of that loop and
into the world where we have these two different concepts of equality.
From this one sees immediately that in the intentional Martin-Leuph type series,
the assertion, yes, and now we see the first problem with this world.
The assertion that objects of a category are never equal can be extended and generalized
to the assertion that elements of any type are never equal.
Since one is not allowed to require substitutional equality between elements of any type.
This fact is the source of the main technical weaknesses of such series. One might suggest
that it is the source of the main technical weaknesses of such series that objects of
no type can be postulated to be equal. And they can be verified to be equal,
but they cannot be postulated to be equal in the sense of substitutional equality.
They can only be postulated to be equal in the sense of the transportation
of equality. And this is a very different animal as some people say.
So the transportation of equality in the Martin-Leuph type series should be
a little s probably at the end of MLTT. That would be much better abbreviation.
It's very different from what livenants would understand under the word equality.
In particular, there can be many different equalities between two elements.
That's certainly not something which livenants would easily accept.
And this is exactly what makes the universe actually impossible.
And so it's not that Wei≈Çowski is crazy saying that as amorphous objects are equal,
it's that there are equal in the sense of this transportation of equality,
which is a very different thing from substitutional equality as livenants understood it.
And we all mathematicians understood equality until this approach started to develop.
But it's also what makes the use of transportation of equality much more difficult.
And the fact that it's so different from substitutional one.
Not just the fact that there can be many different equalities between two elements.
In fact, it's something more complicated. It is also true that there is an additional
parameter which we're going to discuss in a moment, which one has to use to introduce,
to use this equality between two objects. And it is possible that changing this parameter,
one can get different results from the same equality too. So there is actually more,
even more complexity in that.
And so the reason that the transportation of equality in the Martin-Leuves type series
is useful at all, lies in the fact that if now I am going to say it in words and then I'll explain
it with symbols, which should be there, that if A is done transportationally equal to B,
then B can be substituted for A directly in the type part of the sentence. While the element part
must be replaced by a new expression using this so-called transport function, which is where
the name transportation comes from. So in one part of the sentence, we are allowed to do the
direct substitution, but in another part we must do something very complicated. So
this is still useful because in one part we do the substitution, but it is also complex because
what happens in this other part. So this transport function takes as one of its arguments the
witness for the equality, and in this point it becomes convenient to think of course of types
as some sort of spaces, and elements of types as points on the space, and substitutional equality
as equality of points, and transportational equality as paths, homotopies between points,
just continuous paths. And then a type is another space, so it would be kind of a vertical thing
over one point, and a vertical thing over another point, but in order to use
the transportational equality one needs not just these two vertical fibers, two vertical spaces,
one needs a whole continuous family of spaces over the base, in which we are going to be
moving our point. So now the picture becomes
like this. This is some t, this is some object a, this is some object b,
element I'm sorry, this is a type c. So here I have a type p1, and here I have a type p2,
and p2 is obtained from p1 by, as an expression, by substituting b instead of a.
And now I have this path, which is let's call it small e, which is this witness for the
transportational equality. And the transport function then allows to take an element here,
and to transport it into an element here. But what is actually, I'm going to be speaking more
about transport in the back direction, if you don't mind, it does not matter very much. So
one can take a point here and transport it into a point here. The reason we want to do something
like that is always, well it's not always, but it's often when this p1 and p2 are propositions,
are some statements about elements of t. And then I have this equality, and suppose I have a
proof of the proposition with b instead of a. And proofs are points on this vertical line.
And then I want to take such a proof of this substituted proposition and get back a proof
of the original proposition. And for that I would use transport function.
So, but I cannot do it in the way we should draw. And this transport has, again intuitively,
this transport has to proceed continuously. And I must choose first the continuous family
of spaces over all points of t, which specializes into p1 over a and into p2 over b.
So syntactically speaking,
I must find an expression p which involves some variable x,
which will be well formed in the context where I know, well, I have some previous context which
is traditionally denoted by gamma, which is a list of types of all variables.
And then I add another variable x of type t, and I need to have this to be a well-defined
type, for x now being a variable. And that is exactly the corresponds to extending these
two fibers to a total vibration. And then I can write transport b, because it's backwards,
if that was some element r, let's say of r with respect to e. And if I had in gamma,
I had r in p2, then this transport b, r e is going to be in p1.
And so what it means, it means that when I'm proving something, I actually want to find
something in p1. Let's say p1 is the proposition, then an element and it will be approved.
So I want to find a proof of p1. And now I suspect that it will be easier to prove p2.
And I also have this e, and I know which it connects some sub-terms,
sub-expressions, a and b in p1 and p2, such that this holds. So I want to use it
to change my goal from finding an inhabitant in this type into finding an inhabitant in p2.
That I can do using this transport. So here is, I think, what I mostly wrote
here already. To be more precise, let us see what we have to do to make use of a witness
of transportational equality between elements a and b of type g. And suppose our goal is to
construct an element of p1, and a occurs in the expression p1 such that we can view p1 as
as the result of substitution of a for a variable. And then we can use the transport
function to change our goal into p2, which will be the result of substituting b for this variable.
And this should be the syntax. I'm sorry, I actually should have been...
So there are three arguments, right? One, two, three. And this argument requires
specification of a variable which is going to be now bound because we don't have it anymore in the
context. So it's such an expression which binds one variable in the first argument and then takes
the two other arguments directly. And this is precisely what the rewrite tactic of Cog does.
So if you try to actually do it and experiment with Cog and see what the proof term
comes to be, at least it... I don't exactly know. Right now, I mean the rewrite tactic is kind of
grew out of its original form into something rather big. But that's precisely what it's
supposed to be doing. So one may experiment with this tactic and discover
very fast and very frustratingly that it often fails. It doesn't want to do it.
So the reason it doesn't want to do it is it doesn't know how to find such a p.
In fact, one can look into the code which then Grayson read for me so that I could understand it.
Not a rewrite but a destruct in this case which is more basic. And see that it's exactly what it's
trying to do except it tries to do it in kind of a rather non-creative way which is both good and
bad. It's good because you know what it's going to do and it's bad because it often fails.
So it doesn't know how to find the type family p.
So quite often simply replacing the sub-expression a by variable x and extending the
context by declaring x to be a variable of type c will lead to badly formed expressions
because the type checking of p requires more properties of the sub-term than simply the
fact that it is well typed and has type g. And here is an example, a simple example
with the identity type itself. I hope I didn't make a mistake here. Let's check it out.
For the expression, consider the expression identity type
associated with element y in itself in the type t. So this is in terms of our pictures.
So if this is t, this is y, this identity type contains
the loops which start in t and end in t. So for this expression to be well formed,
I must have that y that the type of y is t. If I substitute a variable x of type u where u
is the universe, so the universe is the type whose elements are smaller types. So if I substitute
just directly t for x while leaving y unchanged, the expression will become badly formed because
the type judgment x, u, y, x is invalid. So I can, for example,
I can, for example, have identity type in that natural numbers from zero to zero.
Now if I replace that by x of type universe, then this is not going to work.
And it means that I cannot really use, if I have an equality,
transportational equality between NAT and some other type, which I like better,
let's say binary NAT. Oh, thank you. Natural numbers in binary representation.
And I want to just use it directly to connect these two identity types. It's not going to work.
So there is currently a lot of work being done in the direction of trying to extend the Martin
Leuph type theory with stronger substitutional equality. Because that's a hustle not to have
strong substitutional equality. It's really kind of unpleasant when you cannot replace
x plus 1 by 1 plus x, where x is a natural number. You have to transport. And there is a big
discussion going on whether or not this can be allowed and how to do it and what to do about it.
And there are different solutions being offered. And the cubical type theory of
Thierry Cochand and his group, the cubical TT, uses in this sense the totally the same
structure of equalities as Martin Leuph type theory. So there is one substitutional, one
one transportational. And actually, transportational, there may now have two or something.
Because they were, but they're both, they're of the same kind of behavior,
they're equivalent. And so it's basically the same. It doesn't move in this direction. But
the type theory which Bob Harper and his collaborator, I'm sorry, I don't,
I don't remember his name at the moment. They just released two preprints. And they're considering a
type theory where there is a second substitutional equality, but it has considerably different
properties compared to the first. And because of that, many things can be done, which we didn't
know how to do in the Martin Leuph type theory. So one of the reasons why it's difficult, it's
because one wants to preserve the side ability of type checking. Now, this is the, of course,
the community of the people who are motivated by constructive mathematics needs,
who really want the side ability. I mean, the classical mathematician would not be
particularly concerned with that, I think. I mean, and one can say that a proof checker where
which can check a lot of proofs, but not all, is good. If I still can do more proofs than it,
then in the type checker which can check all the proofs which it's given. And by checking some
proofs, but not all, I mean that there will be some proofs on which it will loop, so it cannot
check them. But there will be lots of ways to prove things in such a way that it can check it.
And if it checks it and says that it's checked, that means that it's true. So that would be a
departure from the ideal of complete the side ability of type checking. But for practical
purposes, it's the departure which seems entirely harmless. But that's, as I said, that's a matter
of discussion. And that's a matter of very, very important discussion because depending on the
result of these discussions, we will get a different, we will get different next generation
proof assistance. And the kind of proof assistance which people will write, people will create,
will program in the next generation of proof assistance is very important. And we know how
stable proof assistance are. If a proof assistance is built, and a substantial amount of stuff is
formalized using this proof assistance, and a substantial number of people are comfortable
using this proof assistance, and there has been kind of at least second generation going on when
the professors teach their students using this proof assistance or give them tasks
based on this proof assistance. Such a thing becomes very, very stable. It's difficult to
change. And so kind of a burst of a proof assistance is a very strong event in a sense,
and a lot depends on how they will be. And this question of what we do about
decidability, for example, is one of the fundamental ones.
Now, this is the end, I think, of my slides. And we'll just start the discussion after that.
And as the last slide, let me suggest one version of equality that is a transportation
of equality, but of a very simple kind. And it might be that it can replace
substitutional equality more efficiently than the current transportation one. And I'm not talking
about a replacement for the main transportation of equality. So the main transportation of equality
is connected to the univalent section, and it kind of fixes it in a sense. One can say that the
univalent equality, and one cannot change much there, and it works well. But we need another one,
which will be something like substitutional equality, so that we could apply it to more things.
So the suggestion would be to use, it's my second actually suggestion already,
and it's a different one from the previous one.
So here the
the suggestion is introduced to introduce a substitutional equality. So it will come with
its own identity type. And if I have gamma little e in, let me just call it, let me just call it
id0, tab. And then I have
gamma. Let me just think a second.
Right. And then I have gamma.
Then I have some expression in a.
So what I really want to do is I want to
to say that because there are equal, if it were, if it was, if it were a substitutional
equality, then I could replace a by b everywhere. So I could replace a by b here, and then I would
have that, that this expression o is also an element of b. So that's kind of a
staple point of substitutional equality. But we don't want to have it because that makes
proof checking undecidable. So how about we do this? Gamma, and then I write transport
0 of o by e. And that thing will be in b.
So I will still have to write the transport. And I will preserve the witness. So when I see
such a thing, I know what to do. I have to check this and I have to check this.
I also can declare it because it's a, because it's a type-based equality. Now, it is much better
because I don't have to choose this family p. So now every time the, that my rewrite ever failed
because they couldn't find p, here it will succeed.
And of course, one cannot use this for the univalence, actually. One would get inconsistency.
But one can have, and then one needs to develop some further infrastructure, but
one can use it instead of a substitutional equality so as to preserve what seems to be
a much better format for type checking. I don't know. Maybe what I just suggested,
because it just occurred to me very recently to suggest it in this form.
Maybe it has some obvious drawback. And I would be happy to hear about it either now or later.
And so thank you very much. That's the end of my presentation. Thank you.
Thank you very much for this enlightening talk. And now we have some time for questions. If
everybody, you could just turn on your microphone and feel free to ask any questions.
Does it work? Okay. Hello, Vladimir. Thanks for the talk. Yes. So you mentioned that
Martin Love type theory already includes a substitutional equality in the form of
definitional equality. And I was wondering if you had some simple, clean example of
how promoting that equality to the type level would let you do some nice neat things. And I
know there is this issue of defining simplicial types internally in type theory that would
supposedly be helped by doing something like what you're proposing. However, as you also suggested
that, as you also noted, there may be other ways to achieve the same result using perhaps less
drastic extensions of the language of type theory or less dangerous things. But listening to your
talk one gets the impression that you don't just, you are not motivated by that, but you really
believe that it's a weakness of type theory that there is not a way to talk about equality
at the level of logic. So I guess I wanted to clarify whether you are motivated primarily by
wanting to define commodity types internally and do constructions on them, or that you really have
some fundamental beliefs for why the thing you tried to kill for your whole life should actually
be resurrected and brought back to the level of types. Wait, the thing which I tried to kill,
two objects in a category cannot be equal.
So one thing is that I am thinking about
more and more about different class of examples rather than the simplicial types.
About simplicial types, or rather semisimplicial types, there is actually a paper by Bob Harper
and his collaborator, that's very same paper, where they suggest this two, to have two
transportational equalities, and they claim that they can build semisimplicial types in their
version without any drastic consequences for decidability. In fact, they also claim that they
have a proof of normalization in their system. And I understand why it is possible to do
semisimplicial types with what they suggest. But it's kind of a quirk. The reason is that
the coherent problems which occur, which one to kind of
to solve, to define these semisimplicial types, they all live in the identity types. So
all the problems which one encounters, if one tries to define semisimplicial types in a kind
of simple way, all the problems that one encounters arises because things like associativity,
and
because at some point you have to do something like
require, I don't think I can explain it in any understandable phrase, but I can explain it later.
The problems occur in identity types, and one can do everything in such a way as to use these
weaker identity types, I mean not mine, but theirs, to confine all the problems into these
weaker identity types, and then they require the weaker identity types to be definitionally
proof irrelevant, that any two elements are definitionally equal for this second
layer of identity types. But they're also transportational in the absolutely the same
sense as before. Again, this requires some fiberancy, I suppose that requires some fiberancy
discipline, so it is an approach to HTS, but I can understand how they do it,
how that that may be done. There is this different version by,
of kind of logic-enriched Martin Luther type theory, which was programmed by
No, no, no, no, Peter Axel and Culloch wrote something about it.
Yes, yes, exactly, yes, yes, thank you.
And one can construct some simple types, some simple types there also.
So, but I'm thinking more in terms of the fact that I cannot prove that 1 plus x equals x plus
1, and that's like when I, when I need to to formalize things like complexes, for example,
where I had xn, where n is a natural number, and then when I have a shift of this complex by 1,
I have xn plus 1 or 1 plus n. And then when I start doing some other constructions, I immediately,
well, I very soon I hit the problem that that one construction works if I choose 1 plus n,
and another one works if I choose n plus 1, and together they never work. So, and this is kind
of a standard thing which happens in Martin Luther type theory, familiar to people, because 1 plus
n is not definitionally equal to n plus 1. And so I'm thinking about this, this thing.
Hello, yeah, I wanted to comment this about this, by the way, we have also written a paper about this
two-level intentional system, but that's not what I wanted to say. I think to me, it's like this,
I really want to work with univalent equality, with this, what I think is very extensional
equality. But sometimes we want to build extensional objects, which we cannot build
externally, and I mean this semi-simplisher types on example. So we get into, as you said,
we get into this infinite regress that we have to solve a coherence problem, and we just want
to design the tools to solve coherence problem. And to me, the situation is a bit like when you
drive a car, most of the time, you don't really want to think about how the engine works. You just
want to drive, you switch on and you drive. So you have this extensional view of a car. But from
time to time, sometimes something goes wrong, and you have to go and open the engine, and you get
your hands dirty, and you work with this strict equality here, and you fix the engine, and then
you close it, and you can again lean back and work with your extensional equality. So it's
a sort of interplay, I think. Yes, I mean, I would agree with this viewpoint, except for the fact that
I think, well, I now think as Americans do, that you should better drive your car to the dealership.
But in that case, it might still be true after some decades that people wouldn't
know how to do it themselves. So, no, I agree. In the case of semi-simplicial types, and all of this
coherence issue, like the fact that we don't know how to define infinity-1 category in
Martin-Liff type series, I think there is also something which we simply don't know yet, it
didn't invent, that there is a solution entirely inside the Martin-Liff type series.
And I kind of believe it more and more. Yeah, I think I would conjecture that's impossible.
Ah, you see, that's good. So we have a disagreement.
Well, it might be very difficult to prove that it's impossible.
Yeah, so there is some idea. I mean, in the system, one thing we need
elementary is that natural numbers are fibrant, yeah. And the simple idea, I mean,
in the simplification sets, obviously, the natural numbers you get in simplification sets
are fibrant, so there's no problem. But you can find models where you have two versions of
natural numbers, and in general of co-products, yeah, that the pre-natural numbers are not
the fibrant ones. And I just wonder whether you can find an argument whether in such a model
the semi-simplicial types cannot exist, but I mean, that's just speculation.
Okay, that's an interesting way of trying to think to approach it. But the thing for me,
in a sense, would be that such a model may not satisfy some... I mean, I would not be surprised
if one needs for that to introduce... Well, I would be surprised, but I'm allowed for the
possibility that for that one would need to introduce some new axiom, something like,
I mean, not univalence, but something equally unexpected, which may then limit the amount
of models which we can have. Because I think that the system for the use in formalization,
in practical work, should have as few models as possible. But now, do we have a model with
non-fibrant free natural numbers and univalence? Yeah, I haven't checked the details, but I think
there is one using sheaf instead of pre-sheafs to construct... But we only know how to do
univalence on sheafs or pre-sheafs on this class of categories which Mike Schumann considered.
And I don't know if they include any with non-fibrant natural numbers.
Yes, I don't know the details, but I think Mike Schumann has done something like this.
Well, I know that he certainly insisted on not demanding for natural numbers to be fibrant.
Yeah. Hello. Yeah, so I'm not very familiar. That's an expert in type theory. And from this
talk, homotopy type theory in this equality appears to be mostly related to type theoretical
arguments. So what are the advantages of this equality outside of type theory?
Well, we consider type theory as a viable candidate for the formal system of the
univalent foundation. And we consider univalent foundations to be a viable candidate for
the next generation foundations of all of the mathematics. So in that sense, type theory
becomes useful for any mathematician who wants to be able to use this new tool switch we're
generating to help him or herself in his or herself work.
Okay. What I mean is what would be the advantages that would affect,
if at all, the working mathematician or logician outside of these deep type theoretical arguments?
Well, no advantages that I can see except the ones which arrive through the use of
computer based formalization systems.
Questions left?
So I think one point is in mathematics, you often want to assume that
if you have a structure, an equivalent structure, you can replace one with the other.
And people do this just of waving hands, right? But it's really a lie.
Yes? And instead, sure. So instead of people don't usually do this. But yeah, I mean,
I haven't done anything. I haven't done anything. I look not at the details as a construction,
hence I can't transfer everything. But to get this, to not have to prove all these
major theorems when you work, you can just use this univalent foundations because you get it for free.
I don't think that, yeah, but I don't think transports are really such a problem in practice.
I think they're a bit overrated. Yes, because what you do is you use an equality and this is not
an invisible operation. It's something which you have to keep track of because it's computational
relevant. But you can say if you have two structures which are equivalent, then anything
which you can do with one structure, you can automatically transfer to the other one
because by the discipline of using type theory, you never been able to look into the intentional
properties of the structure. And there may be other ways to achieve this goal.
Well, may I just add one thing to it? The thing is that there is no such metaseurium
because there is no mathematical definition of what stupid means.
And so there is no such metaseurium. This is a typical argument from kind of
from the air and not from mathematics.
If you actually use it in a paper as a justification for some proof,
then I would argue that such a proof is not acceptable.
Right. I mean, a lot of what we're doing comes from the need to avoid such hand waving.
Okay, so but are you saying that such
metaseurium that I'm talking about are kind of impossible even without all this high
categorical nonsense even just when you had just have isomorphisms and nothing else?
I mean, you can restrict the kind of language you use when you talk about an object, right?
And if you do, then you can assure that whatever you say in this language is preserved by isomorphisms.
Yes, but that's what we're trying to do basically. We're just trying to do it formally.
So that so that you kind of, I mean, you can teach yourself such a discipline,
but how do you ensure that this discipline is not broken by your student, for example?

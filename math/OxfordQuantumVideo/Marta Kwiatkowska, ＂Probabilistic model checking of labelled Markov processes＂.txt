Ryd officially, felly yn cymaint, tryn.</p><p apologize to Trinadwerth
I'm fine, I'm fine.
The next page I'm going to talk about is actually the paper that we wrote, especially for Pratash,
because we wanted to link up LNPs with Prick, which is a probabilistic model checker that I've been working on for quite some time now.
Ac mae'n cael ei ddar Phew orddRE Rise' ac yma le'r gydig laboratoryn sy'n i gydigonhaethm kin mor ymlaenwyr Ond BMW Cyxtingol.
Aled ywgate Mwneud
yn ni, rhai ar anfer y cyntaf eisiau i mwy fawr,
efallai oå…ƒ Ysbeth Cymru,
a ddygen i rhoi'r hawl yw eu poss substance arall a hyn ychydig oherwydd kommgam
unlepsydu cy proposals arainmelliriaethol, ac rwy'n ni Man promise yn ymddangas wahanol.
Mae o'n ysgr oppressed cymhwyd ar arrangement o gyd ag y dweud, wrth gwrs stawn
Polw yn t Fan minsen a'r tif sydd fyddun tofito.
â€¦aryr yellowâ€¦
â€¦ar'u techyd yn gennymdist dymor?
Ychydig yn gennymst dymor.
ok syn am haffodd
mae leger
ok, Iwn rhoi, felly wyddu am y roi bod y sefydliad yn cael tynne arrath railwyr
fe chasdai diem neitheru fuddi dros ddefnydd voy Ð½Ð¸Ð¼ol
Ac rwy'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n meddwl i'n medd
worthwhile chang anestheumbs hefyd niad weithgÃ¡l. Over.â™ª
It is too difficult to ask why it has come?
And I want to answer many questions,
to answer a few questions comment
because each of you has an answer that might give-
oratorid to your huas Cuba to explain
all of the answers for this issue.
So, if you are not here,
Dw i wneud yn eistafod yn Scottu å¾ˆï¿½ï¿½ ynTHOMafeau.
A hefyd edrych dÅµr mewn ddiwedd goinger oherwydd mae'n rheswedd gobeithio?
Mae'ræ€´thau bod y tuent yn rhoi y Ceir ac nid concreteigau nid yw ac rydyn ni'nã‚µfodd.
Rwyf am almond shopeg, 55% o'bydd yn Carchach arall.
I don't know what the percussionist was speaking about I was speaking about causality in fairness which is what I did before, probability.
I looked up the percussionist's DVP and at that time he wasn't doing probability.
The other reason I remembered his pracant as a meeting,
was that he came to me after one of the sessions and pointed and inspire me,
about forty people in New Paris at the end of a meeting.
As you can see there are 63 participants.
I didn't specify how many women and women were...
But I checked the programme, I think there was also one other woman speaking.
So maybe there were two women after 63, so I'm looking around the audience, I did the same yesterday and times had changed.
Rwy'r borframe yn exaithio.
Wrthattli community pen defnyddio'rã ith.
Bydd hi ND98 ac bydd yn statistically yw un blynyddoedd yr hydì„ agoradury.
Rwy'r hyn sy'n ddisbynnig hynny roì´ì•¼an eu slÅµs mumio a fylwch amelcafang hefyd.
Richard
æœ¨wch
a
neid
es Tuny
yn
mi
a
maill
i darell y wath wedi cael ei hunadu ati drwyddo y pwnÃ™s yn
stori yn amlwyÐ°Ð»ÑŒÐ½Ñ‹Ðµch cul search oedd nesaf o'u hunadhe thoseu.
A wneud y accountability ond, yn 2003, o'r pycosipau hwn erisiau disf institur.
flows in your universe. Gwasannaeth brydae Switch steff, geithin gweld,
wedi cofnodr wych John Cedryll cymryddiant Fodol yn gynnododdÃ´n y tu dreaddwyr.
Wyddo sy'n mynd i dweud hyfygol copywyr crimnol Plyddol i ys sinial o'r LTL.
Thai dwi wnaeth ei gen nhwys mhÐ»ÑŽ iawn o ðŸ’© Ar f×£peischu Plyddol am Ys exercì—ì„œ byddwn i f weeksers waroli dweud hornwyr yw gweithio.
Dyna wrth iddown dweud hyffordd gw Freiddien ni i Lythiau C Out, nu gelu yr Ys ddygu?
Ydyn nhw'n gallu rydyn am ei fod yn teimlad cyngorau cyngorol Cynlludol a wnawed eu cynyddiadyn,
e à®‰ myth a wahanol Ã©w'r fam am Hyibrodi Lord musiol,
ond un pethau mor niferio y gafodãƒ‹uswall, a rhai o'r ceil Gaerpheittion
yff Chrisiam Aber intersection roedd yma iech Cause with the probability of the probability of the probability.
Yn Chy Ñ€Ð¸Ñ, sef dioddi r helium teidliadau i nios Peter a ddaethau fydd gennym hefyd hyfforddi
like Am-C happen.
Mark of dann Mark of t-shirt is also a Mark of the channels.
And since then, since then I have started in jamais
that the first painful EDG patientlyb method
disable mycember.
Zenean tychnique used as a mod cre Palestin to find the
same etcetera,
sun base techniques were prepared.
a new models that have been added.
Now, what does probabilistic model checking involve?
Well, it involves conventional model checking,
that is, algorithms which analyse
the underlying transition system of the model,
but in addition also try to compute the probabilities
and perhaps some other quantitative measures, such as expectations.
So, the algorithms often involve non-trivial combinations
of this graph-based analysis, for example, MTBDGs.
This was the basis of a PhD thesis by Dave Parker
on how to make it work and how to make it work efficiently
together with numerical solutions, linear equations,
and the state of the art is that we now have fast, efficient
and well-used tools, present as one example,
for quite large models, and they have been adopted
in many, many application domains, including quantum cryptography,
which is another topic that is connected to Prakash.
And we are moving from verification beyond.
We are looking at parametric methods,
we are looking at various aspects of synthesis,
including parameter synthesis, strategy synthesis.
And to show you, I'm quite proud of what we have done up until now.
These are some very early examples,
and one of the first examples was the verification
system Bluetooth device discovery,
which had three billion states.
Now, what we learned from it is that we could actually compute
the worst-case time to hear one message between two devices,
but the unusual thing was how long it was,
two and a half seconds just to hear one message between two devices.
Another example was Firewire root contention,
where we learned that if you use a bias strain,
you can actually improve the performance of the protocol,
because the protocol is probabilistic and chooses a delay
depending on the probability bar.
And some very recent examples that I can mention,
we can now do model checking for DNA devices.
This is a DNA transducer gate,
which was designed by Luca Cattelli,
but we could find a band in it automatically
using a microsoft tool that is the end present.
And we could also analyse a microgrid demand management protocol
using a game theoretic model,
and we also found and fixed a flaw in the protocol.
And this is implemented in an extension of Prism,
which is called Prism Games.
That has given me to this topic to tell you the truth.
All the models that are supported by Prism
are effectively discrete states,
discrete probability distributions.
So they are usually Markov chains or Markov decision-pressed.
And we obtain them.
For example, if we had something like continuous time markov model,
we applied discretisation techniques,
uniformisation, and we thus reduced it to a discrete time markov chain verification.
For continuous distributions, you can apply discretisation,
but for continuous distributions, discretisation is very hard,
and it is very, very hard to get guaranteed error practice.
And for continuous space, there is some early work due to Loret's hand,
but no clear successes yet.
Now, at the same time, we need more realistic models,
so we know we need models with real-time behaviour,
we need models which have popular continuous dynamics,
not just continuous, but also kind rates,
where you also have discrete switching in addition.
And of course, we know that there has been a stream of work, NMPs,
which started around 97, and continued, but continued separately.
And in parallel, separately from that,
there was related work in controlling the certainty of real literature,
and others are looking to speak to that.
So this talk is about extending probabilistic model checking
so that we can capture some NMPs.
And this is our first small step in that direction.
So this was the background,
and I will now tell you what we have done,
and I'll try to stick at the high level,
and I'll really look into people's practice
and other people's feedback and comments about our understanding.
So labour mark of processes
have uncountably infinite state spaces,
but they evolve sequentially in discrete time steps.
And we are going to stick here to a finite horizon,
so we are going to look at finite numbers of steps at a time.
So NMP is given by the uncountable state space.
There is an initial state, and we have a Borell-Sigma field.
It is usually a Polish space,
and we have a family of mark of kernels
which is indexed by this set of labels.
Nu stands for an action plate.
And the idea is that this mark of kernel
tells you about jumping from a state to one of these Borell sets,
but it is not a probability distribution,
it can be a sum distribution,
so the probability can be less than one.
And this is what we want to discuss a little bit more.
So the evolution of this LMP depends on the choice of a label
because it could be more than one,
and enabled in a state.
You jump according to this probability,
and the action can be accepted with the probability,
and otherwise it can be rejected.
So there are two...
Okay, so in addition to our model checking,
we add another labeling, which we call L,
a labeling of the states with atomic propositions.
So these are atomic propositions that we can then use in formulas,
and also reward of cost structures.
So to each state of an action,
we can attach every reward.
It can be a time, it can be energy usage, for example.
And there are two views of an LMP.
One comes from the influence of the traces on Gibra,
where you want to observe the labels.
So you have the underlying model,
which models some kind of system,
and you have the state,
and you observe traces of the labels.
And another one is you have some system
that you are interested in controlling that system.
It's very much how this is viewed in control,
and in planning, okay,
where people are working with accountably infinite
mark of decision traces as well.
So the idea is that you want to control the system
using these actions,
and you can specify policies
which choose these actions
that are dependent on the time step.
And you are observing the underlying system dynamics,
the variables and the atomic propositions.
So the semantics of the LMP
is that we initialise it,
we work in discrete time,
so you start at k equals 0 in the initial state,
and then we choose the probability
of moving to the successor state
according to the kernel function.
So the label, this action,
is accepted with some probability,
and otherwise,
if that probability is less than 1, is rejected.
And there are two possible understandings of this.
One is in a sense of testing,
like in a Milner kind of observational.
What we want is we are observing a machine,
we try to press a button,
and we observe the action sequences.
But the other one,
okay, and of course, if the action is rejected,
you stop, you develop the machine.
And this is what you want to observe.
I mean, the other one is a decision process.
Underneath, you are modelling some physical machine,
and the machine just doesn't stop, okay?
It doesn't stop.
So you might want to choose some default action
and continue, okay?
And that default action is just going to represent
that behaviour, the physical machine,
which could be the failure,
some failure, something that happens.
Okay?
So there are two examples.
One is the usual Milner slot bending machine,
where you push the button.
The machine replies with some probability,
and they close.
And the other one is this decision process.
So the case study that we are considering
is room, we have two rooms, room temperature control,
where we have heaters in each room,
and we want to consider the temperature flow in the room,
but the temperature is controlled by heaters.
So heaters have certain levels,
and there are actions which allow you to choose
to move to the next level.
And this is what you want to control.
You want to synthesise policies
that could control the temperature in the room,
but allowing for all the possibilities
like ambient temperature, temperature moving
between the rooms, the heaters working,
heaters failing,
and also breaking down.
Okay?
So this is the situation that we know.
And what do we do with that?
Well, we adopt the notion
of approximate probabilistic basimulation,
and we were quotient by that notion.
But first of all, an exact probabilistic basimulation,
which was formulated by Hanam Government colleagues,
and it's based on King's probabilistic basimulation.
You can read about the business model.
That's right.
So they were wrong.
Okay, so this is this,
so the precious algebra and information
coming from Markov's decision processes
has been applied to LNPs here.
So what we say, the two states are related.
Whenever two states are related,
then they have the same labelling
with atomic propositions,
they have to have the same rewards,
and also the same transition kernels.
Okay?
So we assume here a measurability.
And we can, in a usual way,
define this as a relation
between two LNPs.
Well, unfortunately, this notion
does not seem to have any computable,
any undecidability problems,
and there are no constructive methods
to compute these approximations.
Now, so if we look at the approximate probabilistic
by simulation,
so this is a relation,
but it is not an equivalence relation,
and that relation is parameterised by epsilon,
which is some precision,
and the idea is that whenever two states are related,
we also require that the labels are the same
and the rewards are the same,
but that the kernels can differ
but by no more than epsilon.
Okay?
But again,
it may be difficult to compute this in practice.
And this is the point that we are addressing thanks to Alessandro.
So we do, what we do,
is we take an LNP,
and it's in a system
of which it has undoubtedly many states.
I think about it as the room temperature.
Okay?
Now the case is the temperature in two rooms.
And we compute its abstraction
as a Markov decision process
or an action-mabled Markov chain.
Okay?
And it is induced
by discretisation of the state space.
So we do, we start with an untenable state space,
and we divide it into a finite partition.
Okay?
And we use that partition
to develop a finite sigma algebra on it.
Okay?
So the partition blocks are going to correspond to abstract states.
So which abstract state
represents a set of uncountable
set of states of the original concrete system.
And because we have imposed this finiteness,
we already had finiteness of the action of space,
we can then approximate
the transition probabilities
by piecewise constant.
They will be constant
on each element of the partition.
Okay?
So we have, you know, a continuous Markov kernel
that we are approximating this
by this piecewise constant
under this finite partition.
And it turns out
that this finite abstraction
is epsilon probabilistically
by similar to the original system.
And this epsilon
depends on the maximum diameter
of the partition,
diameter and volume of the partition,
and also depends on ellypheats abstract.
The paper we've actually used
is the strongly global partition.
But the main point is that this is straightforward.
Okay?
So what we cannot do
is we can take,
but really takes
the finite horizon
variant of the logic PCTL
with rewards.
So this is the probabilistic operator
and the reward operator.
We can work with a finite horizon.
We are only working with bounded,
step-bound itself.
So this until is satisfied
if that find satisfies
in at most case steps
and that the other find satisfies up
until that point in time.
And we can define the semantics
with respect to policies.
Policies depend on time,
but other time is discrete in here.
And we can look at two problems.
One is verification,
where we take a system,
the original unpalatable system
in some state.
It has to satisfy one of these properties
for all policies.
But we can also find the optimal policy
that satisfies the property.
So we can specify the property
and then look for the optimal policy.
And these are some examples
of the typical properties.
And we have this main theorem.
The main theorem relates
the original concrete LMP
with its finite abstraction.
And it talks about approximate preservation
of these until,
the probabilities of these until,
under the assumption
that the original policy is measurable.
But for a measurable policy
on the LMP,
we can construct a corresponding policy
on the abstraction.
We can do it vice versa.
We can extend it to rewards.
And we can also do synthesis,
which means that we can now take an LMP
and feed it into principle.
That's all. This is what we've done.
So we have now taken this multi-room heating system,
where we consider two rooms
and we can consider policies for heating.
And we can also do verification
of the arbitrary policies.
And we can,
what we take is,
we take temperature intervals
and we divide it into five subintervals.
And we do it over the horizon
of 190 steps.
And I can show you here,
this is the probability
of being safe
for different policies.
This policy achieves the minimum
by simply switching the heaters off
at the beginning.
So we start the temperature
in the midpoint of the interval
and we consider the optimal policies.
So the one that gives you minimum,
the one that gives you the maximum.
So the one that gives you the minimum
just switches them off.
And the other one, the maximum,
first heats up the room
and then it allows it to cool
as you are getting closer to it.
We can also analyse,
in this case, expected time,
but also other expected rewards.
So I know that I've overrun,
but this is really the end
of the talk.
And I think based on that,
I and Alessandro would be very happy
to get some feedback because,
as I've mentioned,
this is the first step,
but we would like to do a bit more.
So we would like to, for example,
look at efficient implementations,
more excessive properties,
such as mighty objective
controller synthesis,
more expressive models,
more case studies.
Thank you.
APPLAUSE
So, first of all,
thanks for giving me this talk now
because I'm a lot better at grant opposing.
For 20 years,
I've been saying this theory
that I've been working on
on the application,
and one day it will happen.
Yes, I know you had a paper.
So I was wondering
why you went for approximate
epsilon-byth simulation
rather than using the metrics
that were...
Rather than using the metrics,
I think Alessandro,
do you have a comment?
The epsilon
that you can specify
depends on certain metrics.
Essentially,
the stochastic terminals
that characterize the concrete model
and the piecewise constant approximation
that is induced
by the abstraction procedure.
And these metrics
are essentially related to the metrics
that we discussed this morning.
They can be based on these
patches on distance
or they can be
based on the variation of the metrics.
Right, so in fact,
is there a family
of relations parameterized
by epsilon
on the uniformity?
Yes.
Maybe I would like
to comment on this
because it was also
in Joseph's...
I looked at it again.
So I think this epsilon
distance or this epsilon equivalents.
They are not equivalents but epsilon.
Relations, right?
They are good but they are not
by-simulation pseudo-metrics.
So it's true that if you are
at distance zero,
right?
It's not?
It's right Alessandro, right?
If you take only one epsilon
it's right what you're saying.
Let me say it.
Let me say what I'm saying.
So then you can disagree.
But if you look at this definition,
it's true that if you are at distance zero,
there will be things that are
by-similar that will not be at distance zero.
Unless you make a
by-similar first.
Because you are quantifying
for all subsets there,
not for equivalence classes.
So
I would say this is nice
because if you can still do something
with a partitioning,
it's even more safe
that you say that they are
by-simulation pseudo-metrics.
But then I
have to take it off-line.
We'll take it outside as we say.
But just before
going to give the next talk
Procash would like to say a word.
Can you speak these words?
I'm not either.
So a lot of people
have been coming up here and praising me.
That's been wonderful.
I view this as
a reciprocal thing.
We all love each other.
I want to say to you that it's your turn
and have done in the recent past.
But one place where I've felt
lonely and isolated in the last 20 years
has been
McGill.
At least in the computer science department.
That changed when Poina came.
So most of you kind of know each other.
And you're not strangers to each other
when somebody comes up to me at all.
But probably none of you have met Poina.
Maybe one or two if you have.
But she is a machine learning person.
Nothing to do with this community.
Joined McGill recognized what I was doing with Relevant.
Which even some people in my own community don't.
She did right away and said
I want you to be part of my lab.
So even though I was a full professor
when she was an assistant professor
she's the head of the lab.
I'm for what I call
a black mathematician.
So
she runs the biggest and most active lab
with a larger number of graduate students
and some of the biggest grants.
And has been for me
a great inspiration and inspiration
to extend beyond
the verification world.
Thank you for that.
Thank you.
So of course after that kind of introduction
I'm going to have to let you down.
So what I want to do in this talk
is a little bit different than what's on the schedule.
I'm going to tell you a little bit about
my research voyage at McGill.
And Prakash has been a very large part of this.
And so this is a voyage
from value functions to by simulation
metrics and back.
And the back part is really
you to learn who's sitting over there.
So all the easy questions I can answer
but all the hard questions you have to ask him.
And this is of course dedicated to Prakash.
I figured that probably you have not seen
this particular picture of Prakash.
I met him at McGill
at an event called Bravo McGill
which is meant to recognize people
who have won awards and such.
And as Prakash became a fellow
of the Royal Society of Canada this year
he was invited.
And Joelle Pinot, my colleague and I
who shared the lab with Prakash
went as groupies.
And that's why he has a flower
and we don't.
But nonetheless it was very fun
and it's really been very fun
to get together over the last month.
So as Prakash said I'm the odd one out here.
I don't do verification
I don't do math
I don't do quantum I do machine learning
and so I'm interested in
questions like this one here
where you have an agent
that's interacting with an environment
he needs to make decisions
there's a lot of uncertainty in the world
but there are rewards
like food and feeling good
and so on.
And we would like to figure out the way
to take actions and receive as much
reward as possible.
And in general
the mathematical
formalisms are all
driven by some intuitions from psychology
which are very simple.
If you do something and it feels that
you should do it.
It's not all the time but basically we don't.
So
we think of this as both the control problem
and the learning problem.
So you have some agent
here, it's situated in an environment
the environment communicates
some information
that we call states
and some rewards
and there's a discrete time clock
so every time they receive a state
you take an action and then you get a new reward
and you transition to a new state.
And the goal is basically to find a policy
that's going to give you
a lot of reward overall
and in the worst case
the policy is nothing from
histories into distributions for reactions.
But in some cases
we can actually do better than that.
So
how do we make this slightly more formal?
First of all
we have to say what are we trying to optimise
so in general we're interested
in some cumulative measure of the rewards
and the one that we
look at most often
is this one here which is called
long-term cumulative reward
with a discount factor.
So you take the sum of all rewards over time
but there's
an inflation rate so if you eat your food
you're not much better than if you
eat your food.
And if you have some policy
this policy has
an associated value with it
and this value is the expected value
of this discounted sum.
And
in general we take this discount factor
as a constant that's given
that's part of what you're trying to do.
So now how do we
make some available policies?
As it turns out there's a partial order of the policies
a policy is better than another
if it's better at all states
and this idea guides the search
for good policies.
So the interesting thing about this is that
typically the value function then
becomes very important because the value function
tells you what's good to do
and so a lot of the focus
in the literature is on
computing value functions because they're going
to be your tool to be able to define
good policies.
And
this process of estimating the value function
in general would be very hard
but in certain kinds of environments
it's actually easy.
And the one that we consider most often
is a Markov decision process.
This is sort of like the one
in verification, almost exactly the same.
So here we have, we consider
environments where there's a set of states
there's a set of actions
and we are going to
make an assumption
about the nature of the environment
which is the Markov hypothesis.
This basically says that if you're
at state T
what you see next, both the reward
and the next state only depend
on the state at time T
and the action that you just took
and not anything else.
So any other bit of history that is relevant
has to be summarised at this point in the state
and in that case
we get a much simpler
way of modelling things
because we can now talk about reward models
and transition models
and these are only going
to depend on the state and action
and the state, action
and next step.
So I'm going to assume
that rewards are bounded in 0,1
this is not a big restriction
and the transition model
basically tells you where you're going to go
here I've written things more or less
in a discrete manner
but in general the state space can be continuous
and we can
deal with that.
Ok, so now what does this hypothesis buy us?
It buys us
a set of interesting equations
which I've written down here
these are due to Bellman
from back in the 50s
called the Bellman functionality equations.
So what does this say? It says
if I have a state
what's the optimal value
of that particular state?
Well, I have to look over all actions
what's the immediate reward that I get
and then what's going to happen next?
So what's going to happen next
is I'll transition to a new state
from then on I'll have some long-term value
this is our best point
and of course there's going to be the inflation rate
that kicks in us
so we're going to maximise
this over all actions
and this is the best you can possibly do.
And it's an interesting way
of writing things out
because this looks like
a really kind of set up
but there's this long-term behaviour
that's buried here
in the restart best point.
So these are the Bellman
optimality equations
there's a unique optimal value function
in a finite MVP
this is not difficult to prove
because you have here this discount factor
that's less than 1, here's your contraction
and of course you can also
make up policies
that will achieve this value function
in particular there is
at least one deterministic mark of policy
where you just take the action
that achieves the maximum.
And
even though
here we talked about the finite MVP case
again under mild assumptions
this also holds in the continuous case.
So now how do we solve the system of equations?
Well
usually
we use versions
of this approach here
which is a dynamic programming approach
it's an intro-development algorithm
where you just start with some guess
let's say 0 at all states
and then you have this update here
which basically is like the Bellman equation
that's not turned into an update.
And I want to note that
even though there are other ways
to solve these kinds of systems
in your programs for example
this turns out to be one that computationally
is like convenient
and it converges very quickly
again because
of the contraction factor.
So this is nice
you can solve
this problem in
time that's going only on the number of states and actions
and
from a theoretical point of view that's great
but if you are a practitioner like me
that's terrible
why? because the number of states
is usually exponentially in something else
like the number of variables in the world.
So for any kind of interesting
environment like the game of bull for example
there is way too many states
and we can't do this kind of solution
exactly
we need to approximate.
And it's actually even worse because
often times we deal with
state spaces that are not even this sweet
but they are continuous or they are hiding.
So for example our college will help you know
has all kinds of cute robots
and the robots
unfortunately we deal
in continuous states.
So there are two tricks
that we use
in order to solve
a dimensional and continuous problems
and the first trick
is to try and avoid
all places
where you have to look over states.
So
in the system of equations
the system is written for all states
and we also have here an expectation over
a next state which is potentially
a sum more manageable over states
and we can
get rid of all of these
there is a class
of algorithms called a synchronous dynamic programming
which essentially replaces
for all and instead of each
iteration to update some of the states.
And typically these are states that
occur on trajectories
of the system.
So just by virtue of occurring
they are somehow more important.
And
the other kind of algorithm that
that we are interested in
doesn't do an expectation over here
but instead just uses samples
of the next state possibly just one.
It's called central difference learning
and again it's fairly well understood
what that does.
So we're using the complexity of these iterations
is feasible
and well understood
but there is a second
sort of problem which is
how do we actually represent
these value functions?
And
we can't really afford to represent them
having one value per state
and that's too expensive
and in general
we would like to use a class of functions
to generalise what we're seeing.
So this is the basic idea of
approximate value iteration
and other related algorithms
and I'm going to describe it briefly here
because I think it might
benefit in the long run
some people in this community do.
So here we have
a space of value function
imagine that's the thing.
We're interested in only value functions
that are on the slime.
So in general I have
an optimal value function
applying the Bellman operation to it
keeps me in the same spot because the optimal value function
is a pixel of the operator.
However, what we want
is to use only value functions
that are on the slime.
So how do we do that?
Well we can start with something that's on the slime
we can apply the Bellman operator to it
that takes us to TBK here
but now we're no longer on the slime
so we need to project back.
So the algorithm
is going to iterate
this operation of taking a point on the line
applying the Bellman operator to it
projecting it back
what's the best we can hope for?
Well the best we can hope for is that
we're going to reach this point here
where the optimal value function
is projected
over the space F.
And that we're going to converge and stay there.
And indeed, if we're careful about
the projection
and we're careful about how we represent
or approximate
the Bellman operator
we can make this process converge.
However, we would like to do
something better, we would like this
to shrink as well
because in the limit we would like to be
as close as possible
to the optimal value function
and perhaps we would like to enrich
the space F in such a way
that we get close.
So I've always been interested
in this question of how do you find
good representations
basically this boils down to
how do you find a good class of functions.
I was also interested in this when I came
to interview at McGill
in 2000
and I talked to Drakash
and Adam Last
that I realized that Drakash was the person
that had to solve this problem.
And in fact
it took us a little while
but
the solution is quite
satisfying.
And from the beginning when we talked
we realized that labour processes
are very much like
market decision processes.
There's no rewards
but there's this analogy
between labels and actions
and so we figured that
we might be able to
port this idea of
by similarity matrix
from the LMP world
to the MDP world.
And
that's in fact what Norm did
in his thesis.
So here
the focus is going to be on rewards.
How do we see whether states are similar
or not? Well they're similar
rewards. So there's this way
of measuring the similarity between
two states S and U
on an action A
which is to compare
the difference in the rewards that you get
and then the discount factor figures in here
one minus gamma is basically the fraction
that you're considering right now.
And
then the bisimulation metric
rho star gamma
can be defined as
the least fixed point of this functional
here there's our friend
the Cantor Beach metric
which has been mentioned by Frank
and also
by Kim.
And
the nice thing about it
is that
the bisimulation metric gives you directly
value function balance.
So
if you want to say
what's the difference between the optimal value function
of two states S and U
rounded
by the bisimulation distance
times 1 over 1 minus gamma.
So this is very nice
from an approximation point of view
because what it tells you is that
if you now take S and U
and use a list that puts them together
you have a way of measuring
how different
your estimates are going to be.
And
as a result
I
started having all kinds of fun
making up ways of leveraging this metric.
So the easiest thing
that you can think of is to do some clustering
with some representative states
and a new cluster
around them, the states that are
within an absolute distance
but you can also do other things
for example you can think of
f the space of value function
being the space of linear function
approximations over some fixed basis
and then
one can obtain a bound
for
for the linear value function approximation
this was work done by
your gate was also sitting somewhere
over there
we looked at the problem of transfer learning
so transfer learning says
suppose that I have a policy that I will think somehow
perhaps by solving a very small problem
now I want to use it in a larger problem
how much loss am I going to have
to how
and
those extensions of this work
also to say action pairs
so comparing not just states
but pairs of states and actions
what that basically allows you to do
is that it allows you to label the states
so there's lots of things that you can do with it
now we come back to the question of how do you compute
and
Frank van Gregel
yesterday told you also
about that
so
he and James Whirl had
this linear programming
for relation of the cantor which metric
which turns out has a dual
which is a transportation problem so one can solve that
and
that's great
but it's again pulling on me on the number of states
that's good news from a theoretical point of view
but it's bad news from a practical point of view
because again we're pulling on me on something that's not really
attractive
so there's a bunch of other stuff that
that we did together with protection
with Norm and with Durga
trying to find ways
of doing this in a more tractable manner
doing some multi-carnal sampling
in order to compute the metric
Norm did some work
looking at possible algorithms for approximating
the metric for continuous states
bases
and so on
and then
very recently we realized
that we in fact
have almost come full circle
and that by simulation metrics are
optimal value functions and a kind of value
so I'm going to describe now
how we get that
and to do this
we're going to think about couplings
so just to remind you
you have two distributions on the value
coupling is a pair of random variables
on the same space
that the marginal distributions
are matching
new and new
and couplings are often used
to analyze Markov chains
because they make it easy
to think about comparing distributions
because instead of comparing distributions
you think about comparing random variables
so we're just going to leverage this
idea
in order to describe how to compare states
in an MVP
so here
we have
the probabilities
for the MVP
Pk
in our collection
and we're going to consider couplings
of the MVP with itself
so K is going to be a coupling
of the MVP with itself
and this is going to induce
a new MVP
now what does this new MVP do
it's got a state space
as a process
it's got the same set of actions
for the reward function
we're going to use DA gamma
so DA gamma is the
difference between the immediate rewards
in the states
and
K is going to give us
the transition probability distribution
and now we're going to
for each of the MVP's
we're going to have of course
the value function B star K
so the main result
is that
there exists a coupling K star
such that the basimulation metric
is actually the optimal value function
for this particular coupling
and what about this coupling
well
if we look over possible couplings
K we consider all the B star K
and we take the minimum over this set
that is actually
the basimulation
so another way to think about it
is that
you choose a coupling
then you maximise your policy
and
the basimulation metric tells you
what's worst over all these possible couplings
once you choose a coupling
you do your best
but choosing a coupling is sort of an
adverse health
and
there's a proof of this
for continuous takedown
that's in the paper
so
what does this result mean
on one hand
it tells you that the basimulation metric
is not easier than computing value functions
on the other hand
it's interesting at least to me
because finally I can bring something back
in the sense that
we have a lot of algorithms
that have been
that are very practical
that have been used for approximating value functions
and if you have a result like this
there's a way to tap into this
massive space of algorithms
to make these computations more efficient
and in fact it was
it was very interesting
to see Frank Saug yesterday
and
he and James Swirl
and also Dichan have some
interesting very similar in flavour
kinds of results
for computing basimularity metrics
for realistic automata
and for enablement of chains
using this idea of couplings
so
it seems to me that this is
a very interesting idea
that also has the potential to lead us
to
practical benefits
so in particular
there's this interesting insight
that basimulation corresponds to
the worst case coupling
all the possible couplings
and this leads to two
possible lines of work
one is to just restrict the set of couplings
over which we're looking
the other one is to try and make a greedy algorithm
that somehow marches couplings
in an interesting way
also as I said because
now we have value functions
we can use standard tricks
for approximating value functions
for example Monte Carlo sampling
in order to compute this
more efficiently so in the case where
you want to have basimulation metric
in order to verify our system
or something like this
there's a potential to do that better
another interesting idea
is to think about
because now we have basimulation metrics
represented as value functions
potentially they can also be represented
through features just like value functions
or represented through features and function spaces
and so iterating
the refinement of these two spaces would be
kind of cool
and when someone mentioned basimulation metrics yesterday
I don't know how to do that
but probably she's done a good job
so
I hope
profession is going to be inspired
by all this
and as a result
we'll do another at least as many years
of fun collaborations today
thank you
applause
I think for cash my house first remark
I'd love to keep all the secret from me
like quickly running to the printer
having all the printouts before you could get them
it's actually we
normally debated
whether or how to keep it secret
and
I'm just being creative
there are
conversations we had that were
favorably close to this
that is fair
that is fair
when Norm visited us
about half a year
and all three of us
had a conversation
about how to keep it from me
I'm very close to this
any questions
I kind of missed the point
of using the couplings to
have this alternative characterisation
of the distance
so here the idea is not to get the best coupling
but quickly producing
so
what we've tried to do is simply
express the fact that the basimulation
metrics can be viewed as a valid function
because that was not known before
so the main point was
creating the assembly P
and in fact there is a
two pages of proving for the continuous case
that this is well defined
and all the things that you need to
exist
but you made a good point
that everything that is even polynomial
is not good because we have too many states
so
here your suggestion is to use
your characterisation on couplings
but the point is not finding the best couplings
as in the perfect model check
you must to guess a reasonably good coupling
and try to work with that
so one of the things that we focus a lot on
in machine learning
is trying to get approximations with
finite number of samples
and so the idea is that
if you don't actually know the model of your system
but you have access to a simulator
that provides you samples
we would like to figure out something like
good coupling
as few samples as possible
but at least we would like to characterise
what it needs to be a good coupling
as a function of the number of samples
those are the kinds of tools that
we can bring from machine learning
that we didn't know how to use before
because before the only way of computing
things was to convert a channel
to this which don't blend themselves
into this
ok
number of questions
but if you promise to be like Kim
no no no
I cannot promise it
ok so
I will speak so I keep on
ok
ok thanks

In this lecture I want to talk to you about sensitivity and specificity and then positive
and negative predictive values. Those are quite important topics and it's very important from
a clinical point of view when you want to use a certain test to diagnose problems or when you
have a test result in your hand to know what the accuracy of that is. Now there's a lot to this
IPython notebook and I'm going to take you through those four topics that we mentioned,
but at the bottom you see we are going to get to correcting positive predictive values,
to incidence, cumulative risk and prevalence, and then the difference in rates and absolute
differences in risk, and risks and rates and odds ratios. I think those are a bit beyond the scope
of an introductory course, but I'm going to leave it there in the notebook so that you can read
through those very powerful information really that you get from those calculations. Let's look
at the interesting world of sensitivity, specificity, and then positive and negative predictive values.
As per usual we're just going to import our style sheet there. Not going to use much coding in this
lecture we see we're just importing pandas as PD and then my normal filter warnings. I'm not even going
to plot anything so let's go. So sensitivity and specificity. Now you've got to think about sensitivity
and specificity in the view of how you can use special investigations or even clinical findings or
findings in the lab to look at how accurate things are. Now I'm going to use an example here
with which we're going to calculate before we get to before we get to our mock data.
But let's just think about things in this way. Imagine then I take a thousand volunteers and I divide
them in two groups. A thousand human volunteers, animal volunteers, thousand lab specimens, doesn't matter.
And I've got two groups. Now I'm going to use clinical data here. So let's say that some of them have a disease
and in some that disease is absent. Think about it this way though. I need a test that I can be
absolutely 100% confident in that that test really shows something is there or something is not there.
As a clinical example we might have, although it's not it's not 100% accurate to use this example,
but imagine I could remove some tissue and send it for analysis and the tissue comes back with
the presence of a disease or the absence of that disease. Okay, so I know this for sure. There's an
absolute imagine, thought experiment, there's an absolute magical way of knowing whether a patient is in one
group or the other group. Now, I'm now going to introduce a new test. Now that I know behind the
scenes, you know, what the true situation is, I introduce a new test now. I want to do some research
on a new test. And doing this test on this, on these thousand people, that test comes back either
positive for that disease or negative for that disease. So if it's positive, the disease is present
according to my new test. And negative, the disease is absent according to my new test. Now let's look
at the four groups we can form. Let's look at the behind the scenes. We know that the disease is there.
My new test that I'm doing can either come back positive, so it's a positive result and the patient
really has the disease. I know that's secretly behind the scenes. I can call that a true positive test.
This new thing that I'm investigating is now true positive. If in that group that I absolutely know to
have the disease of this new test of mine comes back negative, I know that that's a false negative.
The patient actually has the disease. We know that, but the new test that I'm doing my research on shows
negative results. In other words, that's a false negative. And if I look at the group
without the disease, we absolutely know that by some gold standard that the disease is absent.
My new test shows positive results. That would be a false positive because the patient really
doesn't have the disease. And if it comes back negative, of course that's going to be a true
negative result. So those are the four groups we can identify.
Now, how does that help us with what sensitivity is and what specificity is? Well,
sensitivity is just a very simple ratio between the true positives
of my new test divided by everyone who actually had the test.
And sensitivity is just a ratio between the two negatives of my new test divided by everyone
without the disease. Okay, so that helps us in deciding what tests to use. How good a test would be to indicate whether someone actually has the disease versus how good would a test
be to use to exclude those without the disease. That is where we use sensitivity and specificity.
Here, you see the little equations that I've written for this example. Remember, a thousand patients.
Let's suppose only 10 of them have a disease and an overwhelming 990 do not. I know this by some
gold standard test. So I can be absolutely sure that the 10 individuals have and 990 do not.
So out of those 10 with the disease, just imagine that 9 test positive. That'll be a true positive.
And one of them test negative. So that patient would have a false negative.
From the 990 without the disease, let's imagine 90 test positive, those would be false positives.
And 900 test negative, that'd be true negatives. Now I said sensitivity is the fraction between the
true positives divided by the sum of the true positives and false negatives. That would be the sum of everyone
with the disease. And the specificity is the true negatives divided by the true negatives and false positives.
So that's actually all 990 without the disease. And the denominator up there for the
sensitivity would be the 10 absolutely with the disease. Let's do those calculations just with a
simple Python code. So my true positives were 9, my false negatives were 1, my false positives were 90,
and my true negatives were 900. So that makes up my 1000 patients. So what did I say sensitivity-wise?
So it's the true positive divided by the sum of the true positive and false negative. There's my divide by sign.
But I've got to do arithmetic order to this. I've got to put these in brackets. So this gets executed
first before we have the division. Remember that's division. So we're going to execute this. So it's
going to be 9 divided by 10, in essence. And that gives, I'm going to put that into this computer
variable that I call sensitivity. I could call it whatever I want. Specificity, true negatives,
divided by the sum total of these two, which is the sum total of people without. Let's use the print command
here for Python 3.x. So it's print in brackets. And I separate everything that I want printed in a line
by comma. So that's a string that I want to print. And a string has to go in quotation marks. The
sensitivity is, remember when I use commas, it actually puts a space there. So I needn't have put the
space there. Then I want to print out this variable sensitivity. But remember, it's just a fraction. So I
want to multiply it by 100, comma, and then a string, the percent sign. So if we were to run this code
here, let's run this print command. The sensitivity is 90%. Again, that would mean it will pick up 90%
of people who actually do have the disease. Let's do the same for specificity. I've calculated specificity
there. Let's run that code. And we see that the specificity is 90.1%. So it'll pick up 90.1% of
patients without the disease by virtue of a negative result. Okay, that is sensitivity and specificity.
Let's look at our mock data. We're going to do this read underscore CSV method. So that'll take this
file for us, read it inside of a pandas data frame. And I'm going to call that this computer variable data.
As per usual, I'm just checking if things imported correctly. There we go. Now, let's make a decision.
Let's decide that my histological evaluation, where I've sent the appendix away for analysis by the lab under
microscope can absolutely be my gold standard. And it'll divide my patient set into those with and
those without appendicitis. And from those, I'm going to make two new data frames. So I'm going to call
the one a pen underscore pos and that equals remember how to do this. It's the data data frame, the whole
thing, the whole name, column, column, histo, in these square brackets, equals, equals, yes.
So that will be a data frame that only contain histo positive patients. And I'm doing the same for the
histo negative. Pay close attention to the syntax here. Now I'm going to have a true positive all.
I'm going to call that from the positive group. Now what do I want to test? Imagine white cell count is my
test under investigation. And I'm going to say a white cell count of 12 or more would indicate
positive test for me. This is just a silly example. So 12 or more is just going to be a positive test
for me and less than 12 because you've got to decide where 12 goes on what side it goes. So 12 will be
less than 12 will be a negative result.
So here we go. We have this computer variable and I'm going to do this data frame, only the positive
patients. And now I'm going to look in this column, a pen, square brackets, quotation marks,
white cell count, more than or equal to 12. That's the order in which we do more than or equal to 12.
And I'm going to have a false negative where it's the positive data frame and the white cell count
column where they are less than 12. And I'm going to do the same thing. Two negatives will be the
appendix negative ones with a white cell count less than 12. And false positives will be the negative
appendix patients. But their white cell count was more than 12. Excellent.
Now, we've got these four new data frames, TP-ALL, FN-ALL, TN-ALL, FP-ALL. We just want to look at
their white cell count. And we now just want to count how many there are. But we just want to drop
the NAs. Now, the NAs would have been dropped, but just for safety's sake. So I see I have 87 true
positives. In the white cell count, now there are 87 that remain. In the false negative, there's 31.
In the true negative, 18. In the false positive, 11. So let's just do that. I've made true positive,
false negative, true negative, false positive, and I've just read it from the counts there. As simple
as that. And I'm saying sensitivity equals true positive divided by true positive plus false negative,
and specificity is true negative divided by true negative and false positive. So let's run that code.
And now I'm just going to use the print statement again. I can use whatever I want. The sensitivity of
a raised white cell count in appendicitis, just for our study, is the following. Sensitivity, again,
times the 100. And there we go. So the sensitivity of a raised white cell count in appendicitis is 73%. So
a white cell count of 12 or more going to pick up 73% of the patients who actually have. So it's going
to miss a few. Let's look at the sensitivity. That means if I receive a value less than 12,
a value less than 12, it's going to pick up only 62% of the ones who actually do not have. Okay,
that's the way to see it. Let's move on to predictive values. They're kind of more interesting.
Now I'm looking at it the other way around. I've got the test result back.
And my patient's test results are either positive or negative. If it's positive,
how sure am I that the patient really has the disease? And if the test comes back negative,
how sure am I that the patient actually doesn't have the disease? Is this test predictive of having
the disease if it is positive or not having the disease if it's negative?
Now I want to warn you about reading positive and negative predictive values in the literature.
They are very sensitive to the prevalence of the disease in the particular study.
Okay, a study like this where we just looked at the appendices that were cut out and sent to
the lab for histological examination, those patients were super selected.
Okay, they do not reflect the incidence of appendicitis in the greater population out there.
So the prevalence of appendicitis is much higher in this study group because they all went for surgery.
I've selected for these patients. They don't come from a proper population distribution,
if I can use a term like that. Now the equations are very easy. For the positive predictive value,
so the test comes back, what prediction is there that the patient really has it? This is all the
positive. So true positive divided by true positive and false positive. And you can see why then that
would be so dependent on our prevalence. And then the negative predictive value, the test comes back
negative, how sure are we that the patient actually doesn't have the disease? All the negatives,
true negative divided by true negative and false negative. So there we go. Remember before our
example where we had 10 with and 990 without? Let's just run that code and just print that to the screen.
Now look at this. The positive predictive value is only 9% now. Remember it was the sensitivity was 90%,
specificity was 90.1%, something like that. But look at the positive predictive value. So we were
quite sure that that was a good, you know, there was some, there was a good test. But now if the test
comes back positive, only 9% of people will actually have. Now that's a very famous example I used there.
People use this example quite a lot. So for instance, in mammography, where we could say that one out of
10 female patients might develop breast cancer in their life, it's a sad statistic.
And so if you think about 10 with 990 without, you know, that's already 10 over 990. That's like 1%.
I'm not quite sure what percentage I just quoted. But anyway, that's a true reflection of a population out
there. And if you now look at a positive result that comes back from this mammogram, the predictive value
of the patient actually having the disease is very low. That's because purely set by the low prevalence of
the disease in the larger population. If the test comes back negative, it really is a negative test.
It's got a 99.9% negative predictive value. So that was more of, it's not an absolute 100% true
reflection. The numbers I used, obviously they are rounded so that one can just get some nice results.
But it shows you how sensitive positive and negative predictive values, specifically positive
predictive values are to the prevalence.
If we just use the same computer variables that we used for, for the appendix data, look at this,
a positive predictive value of 36% and, or negative predictive value of only 36% and then a
positive predictive value of 88%. So these look a bit weird, but there is a big problem with our prevalence.
Good. So I hope you understand sensitivity and specificity,
and positive and negative predictive values. As I mentioned in the rest of this
notebook, you can read about how to correct for positive predictive value in your study,
how to then use data from the population, the actual prevalence of the disease in the population,
and you can convert positive predictive value of your study to another population,
say the true population prevalence out there. So I'll show you how to do that.
And then the rest of the notebook gets into the very exciting world of
incidence, cumulative risk and prevalence, and the differences between those,
ratios, and then rates, risks, and odds ratios. Excellent.

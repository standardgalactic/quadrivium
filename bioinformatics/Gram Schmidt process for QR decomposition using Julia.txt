Welcome to this short lecture just on QR factorization, how to decompose a matrix A
where each of the columns are mutually or at least linearly independent and we're going to
change that basis into these two new matrices Q and R. Q is going to be very nice because
all those column vectors in it they're going to be orthonormal so they're all orthogonal to each
other and they are of unit length and then R is going to be an upper triangular matrix.
So I'm Dr. Jean Klopper amongst other things I'm a research fellow at the School for Data Science
and Computational Thinking at Stellenbosch University and as you can see here I'm using a Pluton notebook
so we can use Julia the Julia language an exceptionally perfect language when it comes
to computational thinking and I urge you to if you're not already familiar to learn more about
Julia I've got a bunch of videos right here on YouTube and then even a university level course
that you can get a certificate on as far as a Mass for Open Online course is concerned
on the Coursera platform so you can learn all about Julia. So what I'm going to just assume that
you have a bit of knowledge about vector arithmetic about the projection of a vector onto another vector
that's going to be very important about linear independence because we want all our column vectors
initially at least to be linearly independent so that they form a basis for a subspace at least of a
vector space with respect to our matrix A the rank of a matrix because that also just helps us to
understand or make sure that our column vectors are mutually or at least linearly independent what
orthogonality is and then the normalization of a of a matrix and what I mean by that that each of those
column vectors that you can normalize those column vectors. So the Julia packages that we're going to use in
this notebook are linear algebra and row echelon so if you're setting up your environment and remember
I do have a video to show you how to set up an environment for each and one of your projects never
ever just use the base installation of Julia and do everything in that rather create individual
environments for each of your projects and then install only the packages that you need into that
environment specific to that project. So for this project I'm just showcasing a bit of linear algebra
using Julia I'm using the row echelon package that I had to install and then linear algebra that package
is part of Julia. So here's our our matrix it is three column vectors and we have three of them so in
actual fact they are mutually orthogonal or I should say linearly independent and that means it spans all of
three space but very often you're going to have more rows than columns so you're only going to span a
subspace. So let's just set up a and it's very easy in the Julia language we're going to enter the values
row by row leaving little spaces between the elements and then every new row is denoted by a semicolon and I'm
going to assign that to the computer variable a and you can see with with the Pluto notebooks the execution of the
code is just above the line of the code and just to show that those three column vectors are linearly
independent what we can do of course is set up the homogeneous system and if they are linearly
independent the only solution to the linear system is going to be the zero vector or you can just look
at the matrix itself and just because this is square in this instance to use elementary row operations
reduce it to reduced row echelon form through Gauss-Jordan elimination and if it ends up being
the identity matrix of course those three column vectors are mutually or at least are linearly
independent and you can see there I'm using the row echelon package so I always use that if that's the
first time I use a function just to remind me from what package that function comes so the rref row
reduced echelon form function pass a as my parameter or argument to it and I get the identity matrix
I can also just use the dot rank method or that would be the rank function I should say in the
linear algebra package pass a as parameter or argument and we can see it's got a rank of three
in other words those columns are linearly independent so in the linear algebra package there's also a qr
function and you can see qr there pass a to it it's going to return two matrices so we're assigning
it to two computer variables q and r and once we do that we see both q and r and remember you're going
to find these little overflow errors there so anything times 10 to the power negative 16
in computer speak that means a zero so we see these three they are each three of these column vectors are
of unit length and they are all orthogonal to each other so this is an orthonormal basis
for three space as far as our matrix a is concerned and you see r there as well and you can see it's
upper triangular with the zeros below the main diagonal just to show you if we multiply q and r
we get back our original matrix with a provisor that these things times 10 to the power 16 are really
zero so we just get back to where we are so if we have a bunch of basis vectors and that each column in
a that is a basis for either the space or a subspace we've got to choose one of them to start off with
and here we're just going to keep things easy we're just going to start with the first one now what i'm
going to do the notation that i'm going to use is that each column vector in a is going to be a sub
one a sub two and a sub three and then the new basis is going to be u sub one u sub two u sub three
such as remember that u sub one u sub two u sub three are each orthogonal they're mutually orthogonal
and when i make them unit length i put a little hat on them such that we have equation two here so i'm
just going to choose the first one a1 to be u1 because all the others will now subsequently have to be
orthogonal to to them but i've i've got one don't just grab one out of the air from the subspace
that you're dealing with take one of the ones that you have so for me column vector one that's a1 and
that's going to be u1 and if i divide that by the norm of a1 then i have a unit vector and i'll call that
u hat sub one so i'm just going to extract the first column vector there just using index notation so
you can see there a and then square brackets colon means all the rows comma first column so that's
just going to be the one one one and that's what we see there one one one and if we click and put
on that little downward arrow there we see it's it's a column vector one one one that's what we have
and i'm just going to set u1 to be a1 so i'm just saying assigning a1 that this column vector to u1
and then just to get the magnitude of that vector i'm just using the norm function passing a1 to that and
we see the numerical approximation for the norm of that vector and then so that i have u1 norm remember
that's the one with a little hat on as u1 divided by norm of u1 and there we have it we have our first
vector in our new basis our orthonormal basis and it is of unit length now i'm going to go to just a
little google drawing because i just want to remind you of what the projection of a vector onto another
vectors and how that helps us us beautifully in this gram schmidt process let's have a look so there
we go there's my little drawing and i have my vector a1 here the orange one at the bottom and a sub 2
there now they're not the same as the problem that we're dealing with that's just a little schematic just
to remind you what this idea is of a projection so i want to do the projection of a2 onto a1 now remember
a1 we've chosen as u1 so what we're looking for is another vector in the space that is orthogonal to
this vector and if you think about a projection of a of a of a vector onto another vector so you see
this component now is this pink one is along a1 that's the component of a2 along a1 and because
it's orthogonal we have this very beautiful pink one going up to the left here and that's u2 and by
the way that we set up these projections they are these 2 1 these the the the projection and how you
get the projection they are orthogonal to each other so we might as well make that u2 and remember what
we'll do is if this pink one at the bottom if that's the projection plus u2 just gives me back a2 so
you see at the top a2 a sub 2 that's the projection of a sub 2 onto u sub 1 plus u sub 2 now putting in
u there just to remind you it's actually a a1 but a1 is u1 so it doesn't matter so if you add the the
projection and you add u2 you get back to a2 now you know what a2 is so we can just solve for u sub 2
it's just a2 minus the projection and then you have u2 and if you make that into a position vector so take
it to the left you now have the orange vectors as u sub 1 or a sub 1 is now u sub 1 and you
have u sub 2 and they're orthogonal to each other and all you need to do is normalize them
and then they're orthonormal as simple as that and if you have a sub 3 now they've all got to be
orthogonal to each other so with a sub 3 and its projection you better get the projection onto a sub 2
and onto a sub 1 because what you really want is you want it to be orthogonal to everything
let's get back to the code and i'll show you so yeah i'm reminding you how we do these projections
so the projection of a sub 2 onto a sub 1 or u sub 1 now remember that is the dot product of the two
divided by the norm of the one that you're projecting onto times the one you're projecting onto remember
that's how we do the projections if you if you can't remember so here we have a sub 2 i've just
extracted a sub 2 0 1 1 that column vector and i'm calculating now projection of u sub 1 of a sub 2
onto u sub 1 and i'm giving it an appropriate computer variable name and this is how you do
the dot product you use the dot function and then this the just as arguments separated by comma my two
vectors so a sub 2 and u sub 1 and what i'm doing is then dividing it by the norm of u sub 1 squared
as we have in 3 up here and then multiplied by u sub 1 all i'm doing and then there we go i get that
projection but that's not what we're interested in we want to use it such as to remember that u sub 2
plus that projection equals a sub 2 and i can solve for u sub 2 and that's exactly what we do here we
say u sub 2 equals a sub 2 minus the projection we've just done and that gives us u sub 2 and then
we have u sub 2 and all i have to do now is to divide it by its norm and now i have a unit another
unit vector and five is what it's all about i just want to show you that remember they've it's got to
be mutually orthogonal to everything that came before it so if i take any a sub n because next time
remember again for a sub 3 i go to u sub 3 remember that's the orthogonal part and i add to that
all of these things here that's the u sub n is the one that i want and i add this i'm doing this
summation and all the ones that go before that so i'm cycling from a i equals one to n minus one so all
the ones that came before so three will be it's got to be against a sub one and a sub two not a sub three
i'm working with a sub three so it just goes to n minus one so it's the dot product of a sub n dotted
with u sub i so it'll be u sub one first then u sub two divided by that norm squared multiplied by itself
and all i have to do is just as before we solve for u sub n so u sub n is going to be a sub n minus
the summation just going from i equals one to i equals two just to stop at n minus one so if we look
at the projection of a sub three onto u sub two we also have to look at the projection of a sub three
onto u sub one just as we have here in the summation for five and so let's do that let's just so let's
just extract the third column vector there zero zero one and i'm doing the projection there of a3 onto u2
and onto u1 we need both of them and then here's our a little expansion so it's u sub 3 plus the
projection of a sub 3 onto u sub 1 and the projection of a sub 3 onto u sub 2 and that equals
as you can see there and all we're doing is we're solving for we're going to solve for u sub 3
and once we solve for u sub 3 this is what we do u sub 3 is a sub 3 minus this projection minus that
projection and once we do that we just divide it by its norm and we have our third column vector
and all i'm doing now is just putting them all together as u hat but remember that's just q
that's just q and what i'm showing you here if i take the dot product between each of them
bar this to the power negative 16 negative 17 remember those all zeros if i take the dot product
between each of them now i end up with zero so they are indeed orthogonal to each other and if i
normalize each of them they are also normal as simple as that now with qr factorization i just
want to show you then just the form that q that r takes it's upper triangular and it's just these dot
products with each other and if you do that you get r calculated now you'll see there's going to be a
difference because what we've used here is a simplified method let's put it that way because
this is not going to necessarily deal uh this is over field of reels but we've got to think of
of a field of complex numbers for instance so there's actually i just want to warn you a bit more to
this but we when we're dealing with the reels this is absolutely um just an easy bunch of steps to follow
what i do want to remind you then is just the fact that we're just using the dot product here because
it was just over the reels and we're just in a clear in space but when we move away to things
that are a bit more interesting we use the inner product remember the dot product is then a special
case in that instance of the inner product so i just want to remind you an actual fact it's not the
dot product but it's the inner product and then nine is exactly the same as equation five as before
it's just that we have to deal with the inner products if you can define an inner product space
you can do qr decomposition and that's really powerful and then i also just show you the real
form of what r would look like not a bunch of dot products but a bunch of inner products and now
now you can really expand it as far as what you view as vectors and you need and stick to just vectors
in euclidean space as long as you can define that inner product and the space that you're dealing with
um meets the assumptions for uh for an inner product space then you're good to go so that was the graham
schmidt process just just showing you how easy it is to to use julia code as i said look in the link in
the description down below uh when i have time i'll put in all those links in the description and you can
learn a lot more about julia otherwise go to my to my youtube channel jh clopper and you'll find lots of
uh video tutorials on the use of julia

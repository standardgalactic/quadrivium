So in this tutorial I want to talk to you about the common test for analyzing categorical
variables. So we have nominal and we have ordinal categorical variables where we have a sample
space of very defined elements. We could perhaps extend this a little bit into the world of
discrete numerical variables if we're only interested in how many of them occurred and
not the actual values themselves. So let's just stick to categorical variables. We have categorical
variables we have a sample space of these defined values and we want to analyze them. Statistical
tests for categorical variables. Now I want to talk to you about the three most common ones and we're
going to use the R programming language as is the norm for this playlist and I'm going to talk to
you about those three common ones. So first of all there's going to be the chi-squared test of goodness
of fit. So that's when we're going to have a single variable and we're going to have these elements
in the sample space and we're going to count how many each of them occur in a study that we do and
we're going to see if that proportion how many of each someone chose or how many of each there are
is that different from what we would expect there to have happened. And then we're going to have the
chi-squared test of independence. That's where we're going to compare two categorical variables against
each other and the question that we're asking is the outcome of one dependent on the outcome of the
other. So if we count the number of elements in the one versus those same elements in the other
and we compare those against each other is there a difference in or is there a dependence between
those those two categorical variables. And then lastly you'll also come across Fisher's exact test.
Now that exact word in there that doesn't mean it's a bitter test. We use it when we have very small
sample sizes. So if we look at those values and many of them are less than five subjects in each
of those values then we're going to use Fisher's exact test and the exact comes from the fact that
we calculate the p-value exactly or directly. In other words we're not going to first calculate a
statistic a test statistic and then see where it falls on some distribution curve and work out the area
under the curve. No, no. We're going to calculate the p-value directly and that makes it an exact test.
So let's have a look at these. It's the chi-square test of goodness of fit for a single variable
and then the chi-square test for independence or the Fisher's exact test where we compare two
categorical variables against each other. Let's have a look. So here we go looking at the tests for
categorical variables. Now what we see here is the rendered html file on rpubs. Remember that the rmd
file the rstudio file will be available on github for you to download and of course you can just view
this file as we have here on rpubs. So let's just go through that. I'm going to talk about the chi-square
goodness of fit test and the chi-square test of independence and then also the Fisher's exact test.
So those would be the most common tests that we'll see for categorical variables.
Now you would notice there the two types goodness of fit and test for independence. The way that I
want to explain it here the goodness of fit test is that we're really just going to look at a single
variable and for that single variable we'll have a sample space and we'll count the number of
occurrences of each of the unique values in the sample space when we collect this data for a variable.
So imagine that we were to roll a fair die 1,200 times. Now a normal die has six sided six sided so
there's six faces and if it's a fair die we would expect that every face would land up about 200 times.
So it's a single variable you know the the variable is the face that lands face up the number that lands
face up and we're just counting how many of each of these occur. Whereas the test for independence that
is where we're going to look at two categorical variables and we're going to see if there's
some dependence between the two the null hypothesis being there's no dependence between the two.
So we're really looking at proportions here of sample space values for categorical variables. So let's
just start off with the chi-square test of independence so the chi-square goodness of a fit test I should
say. So let's consider a very simple example we're going to take a hundred samples from a population
and we are going to ask them a question in a survey and they can choose one of four options strongly
disagree with a statement we might make disagree agree or strongly agree. So there's only four to choose from
and let's just say for argument's sake we expect an equal distribution of those four answers. We don't
expect that people will choose one over the other so our expected distribution is 25 each. So if we give
it 200 people we expect 25 to choose strongly disagree, 25 for disagree, 25 for agree and 25 for strongly
agree. And now we get back 10, 30, 35 and 25 for each of those. So this is an example of a multinomial
categorical variable. There are more than two the sample space has more than two. If there were just
two yes or no for instance that would be binomial but this is indeed multinomial. There are four
elements in the sample space of this question variable and so let's assume that 10 people chose
strongly disagree 30 disagree. 35 agreed and 25 strongly agreed. So if we mark each of our if you see down
here we mark each of our observed values of uppercase y sub i we'll have that y sub 1 is 10, y sub 2 is
30, y sub 3 is 35 and y sub 4 is 25. So there's a total of 100 so n equals 100 subjects and the probabilities
that we expected if we could write that here as lowercase p subscript i p1 will be 0.25 and so on
until p4 we expect that. And then the general form that these types of tests take is is that we sum up
the square of the differences between the observed and the expected values and we divide that by the
expected. So that difference again is going to always be positive because we squared it and then we divide
by how many is expected. So if we just look at the single variable what would be the observed well
that would be all of the yi's so the 10 the 30 the 35 and the 25. What do we expect? Well just think
about it if we expect a quarter of people to choose and there's a hundred people so it'll be the 100
people times a quarter. So that's where we get the n times pi. So there was 10 and a quarter of
125 so there's going to be 10 minus uh 10 minus 25 for the first one. We square that difference to
make a positive and we divide it by the 25 and we add up all of these differences and lo and behold we
get a chi square value. So let's just do that inside of code the long way. So we're going to create a
computer variable here we're going to call it just y and we attach to that this vector of values. So we use
the c function the 10 the 30 the 35 and the 25. That's our observed count. Now the probabilities
we're going to create a vector called p there and we'll use the rep function because we're going to
repeat 0.25 four times. So it's just going to be a vector of 0.25 0.25 0.25 0.25 and 0.25 and the sum
we're just summing up all of these the 10 the 30 the 35 and the 25 and that's going to get to 100. So
let's work out this chi square value. So we're going to sum all of these things. So it's going to be
y minus n times p squared divided by n times p and because all these vectors are of equal length
it's going to be no problem. We're going to have this broadcasting where it's going to be element
by element at least and we're going to get back a chi square value of 14. Now this chi square value
follows a chi square sampling distribution and you would remember from chi square sampling distributions
you might know you might remember that these look different depending on the degrees of freedom. So
we've got to work that out and that's simple enough and as much as the degrees of freedom that we have
here is how many values there were in the sample space. There were four minus how many variables were
there. Well there was just one in this instance so we have three degrees of freedom. So we're going to
create that as a little computer variable df which is just going to hold the value three and then we
can use the probability chi square test to work out a p value for us and the first argument is going to
be our chi square value of 14. The second is going to be the degrees of freedom and the last one we say
lower tail equals false because what we want is the area under the curve from the value 14 towards
positive infinity and get gives us a p value of 0.002. So they that this we can say that the proportion
that we found the 10 the 30 35 and the 25 having expected an equal distribution of those a uniform
distribution I should say is statistically significant our value was a rare value to find. Now we don't have
to do any of that we can just use the chi square dot test so it's chisq dot test and we just pass the
vector values and the p the probability remember that's going to be 0.25 0.25 0.25 that's the whole
vector and we get nicely back a chi square test for the given probabilities we get a chi square value of
14 degrees of freedom of 3 as we expected and the same p value. So simple enough if you ever have that
situation where you just have the single variable and they are categorical or discrete numerical as
well remember if if if we can just look discreetly at each individual value or then more notably a
categorical variable and we have that defined sample space we just count the proportions. Now as I
mentioned the chi square test of independence that's a bit difficult we're going to compare the the
proportions at least of two categorical variables against each other. So as always let's just look an
example it makes life very easy and we are going to have a categorical variable called group and the
sample space is going to contain two elements group one and group two so imagine a group of patients they
fall either into group one or group two you can imagine that they get different types of medication
a placebo and an experimental drug for instance and for some variable we're going to call it outcome
that outcome variable is also nominal categorical and it contains a sample space of three elements
worse same and improve so we just decided this patient got worse this one got the same and that one
improved and you can see the values that we have there the total values 44 worse than 72 stayed the same and
55 improved so when we break this down we see the 33 subjects in group one improving 44 staying the same
and 25 improving and we have 11 28 and 30 for group two for those numbers so how would we represent that
well we do that in what is called a contingency table a little matrix so one way that we can do this is
just to have two vectors and we just row bind them so each of the vectors will now form a row and I'm going to
store that in the computer variable obs obs so I'm going to rbind my 33 44 and 25 my 11 my 28 my 30
and just for the sake of argument as you can see nicely printed out below here I'm going to put these
column names and row names in there so I'm going to say row names obs and I'm going to pass the string
vector group one and group two to it and the col name is going to be worse same and improved so when we
print this obs out it looks very nice because we can immediately see in group one there were 33 that
got worse 44 that stayed the same 25 that improved 11 in group two got worse 28 were the same and 30
improved now we want to ask the question was the outcome of the patient was that outcome dependent on
which group they were in that's the question we're asking but you can see both are nominal categorical
variables that we want to compare to each other so that's our table of observations our observed
contingency table two rows three columns two by three you can have three by three four by three
doesn't matter the size that we have so you can have more elements in each of your categorical variables
now we need to work out an an expected table and we we really if you read down here it says let's
start to the number of expected subjects in group one that worsened the observed count was 33 now given
that there were 44 subjects who worsened so if I count up this column 33 and 11 that gives me 44 so not
that 44 44 worsened and there were who worsened and 102 subjects in group one so if I just look at group one
and I add 33 44 and 25 I get 102 so if 44 worsened worsened overall but group one only contained 102 I can work
out what I expected this 33 to be by very easily multiplying this column total times this row total and
I divide that by the sum total there were 171 patients in this whole group so that's 44 times 102 divided
by 171 that gives me 26.2 so we would expect given that there were 44 people who worsened and that there
were 102 people in group one out of a sum total of 171 we would expect with those proportions there to have
been 26.2 people it's going to be a fraction in place of the 33 and you can work out all six values it's the
column total times the row total in which that value appears divided by the sum total and if you do all
of that you're going to have these six values in what is called an expected table now we don't have to do
all of that this is r for statistical programming so we can just use the chi-squared dot test function
we pass our observe table now we don't have to put these row column names in there like I did
all you need is this matrix of values and we say correct equals false I'm not going to get into that
we don't want Yates correction Yates's correction there and that gives us a Pearson's chi-squared test
and we see a chi-squared value of 8.976 degrees of freedom of 2 and a p value of 0.01 so that is
smaller if we choose an alpha value of 0.05 that's smaller so we definitely say that your outcome was
dependent upon which group you were in and now you can start looking at the different percentages you
might express just to see you know which group if you just look at improve or just look at worsened
you can sort of express in words what this table is trying to tell us but definitely there is
dependence on these two now also note I could have transposed this I could have worse same
and improve on my rows and groups on my right I should say on the columns group one and group two
so I would have three rows and two columns that makes no difference at all and it also doesn't
really make a difference in which way you explain it you could say the group that you were in were
dependent upon whether you got worse you stayed the same or you improved we just attach some human
meaning to it by suggesting that your outcome is going to be dependent on which group you were in
so I'm oversimplifying there slightly but I think that explains the situation now the last test I just
want to talk about is just Fisher's exact test it has the word exact in there and it might sound like
it's a you know of higher quality in some way a better test but it is not exact just means we are
calculating a p-value directly we are not first calculating a test statistic like a t statistic or
chi-square statistic statistic that falls on some distribution curve and then we work out the area
under the curve that's not what we do in exact test in exact test we just have an equation and we
calculate it and there's a p-value and that's what the exact means it doesn't mean it's any more exact or
any better than any other test so let's look at Fisher's exact test now the these chi-square
values they fall on this curve this chi-square distribution curve but when the numbers start
getting small like you only have less than say five subjects in each of these in each of these or at
least 80 percent of these you have values of less than five so if that was four and three and one
it's not going to follow that chi-distribution curve very nicely when the numbers get small things go a
little haywire and in those cases we can use Fisher's exact test now Fisher's exact tests are going to
work for these large numbers but we use what is called the factorial calculation and the factorial is
very easy if I say five factorial it means I go from five and I count back and I multiply so five times four
times three times two times one so two factorial is two times one which is just two three factorial is
six because it's three times two times one four factorial is four times three times two is 24
and then 25 and by the time you get to 10 you're talking about very big numbers it really ramps up
quite a bit so 44 or you know that you're going to get to a number very quickly that is out of limits of
what your computer can actually calculate so don't overuse the Fisher's exact test just because there's
an exact in it we'll use it where it's proper and that's where we have very small values and there's
the equation for it now it also uses a contingency table but in this simple form you only you can only
have four values in your contingency table so it's one categorical variable against the another but the
sample space of both of those can only be two so in this instance we have a two row by three
column we can't have that we can only have two by two so what you would have to do is by some
logical argument because you're an expert in what you are researching you would have to combine two
of these so you might put same and improved as one and add this 44 and 25 and add the 28 and 30
and make that just worse and not worse or we could have the worse and same together and saying not improved
and improved so that you have these two by two and if you had more than two groups you would also have
to combine them in some way and then we're going to have the fact that from the top left to the bottom
right we're going to have if we just look at this one we'll have a b c and d so from top left to bottom
right it's going to be the first value is a next to this b drop down will be c right on the right bottom
corners d and if those are our four values in our contingency table we add a and b and we take it's
factorial c and plus c and d it's factorial and we add a and c and it's factorial and b and d and
it's factorial and we divide that by a factorial times b factorial times c factorial times d factorial
and then n all of them combined that's the sample space the whole sample size i should say factorial
so let's do a little matrix and we call it vowels and we're going to have two rows so it's going
to be two rows and two columns because i have four values there and just to show you i didn't print
out vowels to the screen here but you can also add your row names and column names there you don't have
to do this at all you only need the matrix and we use fishes dot test and we pass the two by two matrix
to that the contingency table and we can see what it works out for us what it works out for us there
so fishers test very simple so remember we're going to use the chi-squared goodness of a test
for single variable chi-squared test of independence if we have two categorical variables and then if
we have very small numbers very small sample sizes we can use fishers exact test remember that for these
tutorials on r that the actual html rendered files are on rpubs and that's what you might see on the screen
but these files are also available in their raw form on github and all the links will be in the
description below so you can either go to the website and look at rpubs files as they're already
rendered or you can go to github and download those files into your system so that you can use them in
r studio yourself so if you like these videos on r please let me know so that i can make more of these
or the subjects that you want me to cover as far as biostatistics is concerned and the use of r please let me
know otherwise please always remember to subscribe and hit the notification bell so that when new
videos come out you will know about it you can also follow me on twitter because that's where you'll
also see that new videos are out

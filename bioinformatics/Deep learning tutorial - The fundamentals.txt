This video is part of a course that I'm putting together about TensorFlow 2.0 and deep neural
networks. Once that course is ready I'll put it in the description down below. So far we've looked
at this process of forward propagation and back propagation. So what we're going to do in this
notebook is just look at this very simple neural network and we're going to go through the
mathematics of this process of taking the data as an input, taking it through the network
to the output. It's going to involve some linear algebra and some differentiation. Now I want to
put this out front. You do not need to really understand this. When you write code using
TensorFlow 2.0 you can just write the code and the mathematics will happen behind the scene.
What I do find though is if you have some understanding of what is going on here it
really eases the way for you into more complex forms of neural networks. Just this basic understanding
of what is happening and it really is not that difficult at all. So we're going to have only two
nodes as an input here and we're going to have four nodes in our hidden layer with a bias node and
a simple single node as far as the output is concerned. So let's just import some libraries.
So I'm going to use NumPy and I'm going to use Google Colab. From there I'm going to use the files
function and from ipython.display I'm going to use the image function. As always I'm going to use plotly
as my plotting library. So just the graph objects there and then the input output. I'm just setting my
default plot style to have a white background. Now some of the mathematics that I'm going to show you
especially the linear algebra part we're going to use symbolic python. SymPy. And this that just allows us
to to display the mathematics very nicely on the screen as you would see in the textbook.
And it's also geared towards mathematical or symbolic mathematical solutions. So it really works well.
And because of that we also have to run this little boilerplate code here.
ipython.display we're going to import math and html and then we've got to load this mathjax
so that we can view lartec code and that is this formatting mathematical notation so that it looks
nicely and then we initialize the printing. So have a look at that boilerplate text there.
But we're certainly going to make use just of symbolic python. Now I'm not going to import this
image. I'm just using the files.upload function here so that I can upload this image directly from
my hard drive. I'll make this these little figures available and I just use the image function there to
show you what we're going to do. What it's about though is this little data box here. So we're going
to have only four samples in this very small neural network of ours. So our training set only contains
four samples. And you can see there we have two feature variables. Feature variable one and two.
Very nicely named there. And then a target variable and you can see it's encoded as a zero and a one.
In other words this would be a classification problem as this would be a categorical variable
and it is binary in nature. And all these things are very important to know because when we design
our neural network we need to know what kind of problem we're dealing with. So this is a binary
classification problem and of course it's supervised learning in as much as we know what the answer is.
Giving this data the 10 and 11 we know that we have got to predict a one and if it's nine and eight it's
got to predict a zero. Now let's just take this first sample. In this whole notebook we're going
to concentrate only on this first sample. For the first feature variable this sample had a value of
10 and you can see this as a patient a customer whatever you want. For this first variable this
first subject here in our data set the first variable value was 10 and then 11. Two feature
variables hence the two input nodes. In other words the 10 is going to go in there and the 11 is going
to go in there. That 10 and 11 is going to forward propagate through this whole network and it's going
to predict something in the output and it's going to be either zero or one. Now I tell you now it's not
going to be zero or one it's going to be somewhere in between 0.34 0.78 and we can make that arbitrary
cutoff say at 0.5 and if it's more than 0.5 we'll say our network predicted a one the class one and if
it was below 0.5 it'll predict the class zero. So that's how we're going to interpret it but the output
is going to be some numerical value between 0 and 1. Again though we have two input nodes because we
have two feature variables. When we put an image as an input that will look quite different this input.
So this input is really designed around your feature variables. So to keep it very simple we're only
going to have these two feature variables here. Now here's the first part of the mathematics and this
is a vector. You can see this in equation 1. I've marked the equations here for you and there's the
10 and the 11. We can write that in this column format and I'm just going to call it purely for
the description in this notebook. I'm going to call this vector i and the subscript 1 denoting this is
our first sample and we see we write the 10 and 11. We write the 11 below the 10. So 10 and then below
11 and we see 2 times 1 there. That is the size of this vector the dimensions of a vector and it's
always how many rows there are times how many columns and we can clearly see there are two rows
and a single column. So there's always row comma column and that's going to be something about a
vector. A vector is only going to have one column. Now I just warn you we also get row vectors and then it's just a
single row of values but here we're just dealing with column vectors. So it's two rows one column which
means if we go back up to our little figure here if this is a two by one vector guess what these four
are going to be. They are going to be a four times one column vector. So how on earth do I go from a
values of 10 and 11 in a two by one column vector to a four by one column vector? Well I do this through
this magic of all these connections. How many are there? Well each node is connected to input node
is connected to each of these four nodes. So there's four there and four there. So somehow I've got to
bring in something that's got eight values to go from this two by one column vector to this four by one
column vector. And we're going to see that we're going to represent these things as a matrix. But hang
on. So here we go. We're going from this I input vector two by one to this first hidden layer. Now we
only have one hidden layer here and it's a four by one column vector. And as I mentioned the way to go
from one vector of one dimension to another vector is we're going to use multiplication of this vector by a
matrix. So we're going to have a matrix times this input vector and that's going to give us this new
vector. So I'm just using this generic matrix A here. R times S. It has R rows and S columns. So it might
be three times four. Three rows and four columns of data. And I'm multiplying it, that dot again, with this
vector S times one. So a vector will have one column, a column vector. And the result is R times one. So that
little R there comes from this R in the matrix. And you see these two S's, they have to be identical.
You cannot multiply a matrix times a vector if those two are not the same. So the column number of the
matrix and the row number of the vector has to be exactly the same. And they will fall away, you could
say. And then we are left with the number of rows in the matrix and the number of columns in the vector.
Hence the R times one. So if I wanted to go from a vector that's two by one vector to a vector that's
a four by one vector, my matrix had better be of size four by two. So look at that. The two and the
two are exactly the same. They fall away in quotation marks and inverted commas. And what gets left behind
is the four and the one. So my result is a four by one. So if I go back up to this little graph, this,
all these lines, we can represent them as a matrix, a four by two matrix. Because if I take a four by two
matrix and I multiply it by a two by one column vector, I get a four by one column vector. So that's
exactly what we're going to do. And here you see a representation of that. And here you see the
notation of a four by two. You see these subscript, they're two digits. The first digit is the row number.
The second digit is the column number. So this W11 is going to be some numerical value in row one,
column one. And the second one here is going to be row two, row one, column two. And then row two,
column one, row two, column two, etc. until we get to row four, column two right at the end.
So they are just going to take some values. And those values, we're going to multiply by this 10 and
one. So let's just simulate some random numbers. I'm going to seat the pseudo random number generator
here with the integer 42. That just means every time we run this code, we're going to get the same
random numbers. I'm going to create my input vector here with a simpi.matrix, the matrix function.
And you see how we do that for column matrix. It's 10,1 and they each go inside of their square
brackets. Every row goes inside of its own set of square brackets. And then square brackets on the
outside to denote this whole thing. And then we're going to create the matrix. And the matrix, I'm just
going to use the numpi.random.normal function. loc equals zero. That means take from a mean of zero,
a standard deviation of 0.01. And the size of it must be four by two. So once I create this little
four by two array inside of numpy, I pass that to the matrix function inside of symbolic python or
sympy. And there we get. So there you can see it already on the screen, the 10 to 10,1. You can see
that little math jack is going crazy there just to do this pretty printing on the screen. And with these
square brackets or sometimes as you can see here, we just use these parentheses, large parentheses,
or you can use these rectangular brackets. And you see my column vector there, 10 and 1. And here,
let's just run that again so you can see how it prints. There we go. There we see our four by two
weight matrix. Now we're going to call these weights, weights, a weight matrix. So there's my
four rows over two columns. And this came from a normal distribution with a mean of zero and a
standard deviation of 0.01. That's really typical. Now these are random values. And that's exactly what
we're going to do in a neural network. When we start it up, we have the data in, as you saw in our little
table of only four examples. And we're going to multiply it by some weight matrix. And those values
initially are just chosen at random. There are various ways to choose it. I'm just using this
normal distribution for now. Now we can just check on these matrices and the vector and just make
sure the size. So my input vector is two by one and my weight matrix is four by two. The next thing that
we're going to do is to multiply them. And we do that just by the star symbol, normal multiplication. And
it's got to be the vector, the matrix I should say, the weight matrix times the input vector.
Because it's got to be in that order so that we have those dimensions lining up.
So the thing about matrices and vectors, usually these things do not commute. Normal numbers do
commute. Three times four is 12. Three times four and four times three. They're the same thing.
But the W here times the input vector is not equal to the input vector times the weight matrix. In
actual fact, you can't even do that because those inner numbers don't align. So once we do that,
lo and behold, here we see, let me just run this. You can see the output already there. All of these,
you'll see the output before I run the code because I just wanted things to go smoothly in this in this
recording. But there you can see I have, as promised, we deliver a four by one column vector. And that's
exactly what we want. And if we check on the shape of that, let's run that in any way, we see it is a
four by one column vector. Exactly what we want. So let me just show you how that is done. So I have
my eight values here in my four by two weight matrix. And I multiply that by this column vector. So how
that happens, how do we get to a four by one? Well, we take the 10, we go row by row as far as the matrix
is concerned. So we take the 10 and that gets multiplied by the weight matrix in row one,
the weight value at least in this matrix in row one column one. And the 11, we multiply that by the
w12 there. And we just add these two products. Then we go to the next row in the weight matrix.
And again, it's the 10 times the w21 and 11 times the w22. So you can see it's quite simple how that
happens. Now let's scroll all the way back up. What we represent here is that we add a bias term.
There's a bias node here. And so we're going to do vector addition. We're going to add whatever this
value is to each of these nodes. And the only way to do vector addition is if those two vectors have
exactly the same dimensions. You cannot add two vectors together if they don't have exactly the same
dimensions. But we usually just put one node here because all four numbers will be exactly the same.
And it's also very typical when you first run your neural network to just make all the bias values zero.
So we're going to have this four by one column vector of all zeros. That means the weight value,
we just actually have to store one value zero because it's going to be the same for all of them.
But I create it here with a simple matrix. And again, just to show that it is four by one. It's just
the four zero values. So when we did this multiplication of the weight matrix and the input
vector, we got our four values. But that's not the values that are actually going to go into those
four nodes. We are not done yet. We add this bias vector to those values. Now this is quite easy for
now because they're all zero. And what happens with vector addition is you just take each element
and each of the two vectors, the corresponding elements, and just add them to each other.
So what happened here is nothing changed. And by the way, I'm now calling this
solution of the weight matrix times the input vector plus the bias vector. I'm calling this Z.
And you can see the dimensions here. Z is four by one. That came from a weight matrix, which was four by two
times a input vector, which was four two by one. That should be a two. We'll fix that. Plus a bias
vector, which was four by one. So this is quite good for us to open this up. So let's fix
this immediately. So our input vector was two by one. There we go.
Two by one. So these inner twos are the same. And remember, the input vector was 10 and 11.
So all of this makes absolute sense. And then we can see Z there. No problem whatsoever.
Nothing changes because I added just the zero values. Now we are not done yet. That is also
not the final values that are going to go into those four nodes. We have to put this column vector
through what is called an activation function. And that's very important in neural networks.
And activation function is what brings non-linearity to a deep neural network, to this model,
which is quite different from linear models. And there are many activation functions.
And we'll learn all about them. The one that I'm going to use here is one that is used
probably most often. And that's called a rectified linear unit. And we usually just say RELU. R-E-L-U,
as you can see there. And the E is lowercase and all the rest is uppercase. So RELU.
And what the RELU activation function does, with an input, is it looks at every single value
separately. And it says if this value is more than zero, the value remains unchanged. If it's zero,
it also remains unchanged. And if it's negative less than zero, it turns into a zero. So all negative
numbers get converted to zero. And all positive numbers just stay what they are. So the way to do
that is just to use the maximum function in NumPy. It says take what, take the input or zero, whatever
is the highest. So if I were to run that, you see that my 0.03 stayed the same, my 0.23 stayed the same,
but the negative 0.049 changed into a zero. And the 0.24 stayed the 0.24. So that's it.
That's the rectified linear unit. And it's such a simple function, but it really performs a bit of
dark magic. So I'm just going to call my RELU function here sigma. So I'm just applying sigma to
this whole four by one column vector z that I have here. And I'm just reminding you of how we got there.
How did we get to these four rows in one column? And I'm just showing you here. It just means I'm
just applying RELU to each of these rows separately. And that's how we got these four
values up top with the third one turning into a zero. So that's where we are at the moment.
I'm going to scroll all the way back up because we are going to go from a four by one
vector, column vector, to a single value. Now a single value is very easy. You can see it as a
one by one column vector. So there's one row and one column, one single number.
So how am I going to go from a four by one to a one by one? I think you can work that out by now.
It's quite simple. Now I'm just going to call it for just showing this notebook. I'm going to call it
V. And you can see there, V, by the way, this, after the activation function, I'm calling my vector
A. This vector here now, after the activation, I'm calling it A. That's how we stored it here, A.
And what we're going to have to multiply it with is a weight matrix. I'm calling it V. That is one
by four. Because if I take one by four and I multiply it by a four by one, the fours are exactly the same.
So I can do this. And what is left are one by one. So my output vector is just a one by one, a simple
single number. And here you see V up top here, V one one, one two, one three, one four. So it's row one,
column one, row one, column two, row one, column three, and row one, column four. So that is just
a matrix with a single row. It is actually a row vector, but we won't see it as such at the moment.
It is a weight matrix. It has a single row and four columns. And if I do that multiplication,
things are going to work out perfectly for me. So all I'm doing here, once again, I'm just choosing
from a normal distribution with a mean of zero and a standard deviation of 0.01. And I want it to be
size one by four comes from the random dot normal function there from NumPy. And I'm passing that as
an argument to the matrix function in symbolic Python so that I can have this beautiful four by one
weight matrix here. And if I do that multiplication of V times A, lo and behold, I get a single number.
Now, I'm not done with that single number. That's not what is going to go inside of
the output node day. I have to put it through, you guessed it, an activation function. And another
activation function is just the sigmoid activation function. And let's run this. I can redraw on the
screen for us. So I'm just using plot least graph objects here, a figure. I'm just creating values from
negative three to three. And I'm using the sigmoid activation function. And what the sigmoid
activation function is here, the y vales that you can see here, is one over one plus the e to the
power negative x. And if you do that, you get this beautiful s-shaped curve. And it's going to go from
zero to one. It's always going to be constrained between those two. And is that not exactly what we
wanted? Because we want to predict either a zero or one. And as I said, we can have this cutoff, say,
at 0.5 here. And if my value that comes into it is more than 0.5 up here somewhere, we're going to
predict that as a one. And if it's below this, we predict it as a zero. So we really have to put this
value through this sigmoid activation function. And that's a very typical activation function to use for
a binary classification problem. And out pops this value. 0.50. So it's one over one plus e to the power
negative what happened after my weight matrix, my one by four weight matrix times my four by one
column vector. And I got this one value. And if I pass that to this function, I get a value of 0.50.
So I'm sitting there right smack in the middle. I suppose it's slightly above. So this prediction
would be for my target variable class one. Very, very 50-50. But that's exactly the point.
I just chose my weight values, my bias values, and my second weight matrix. It's all just random values.
So of course the prediction is going to be very poor. And this is exactly what a deep neural network
does. It's going to have to update, get better values for those two weight matrices in the bias node.
If it can learn better values there, this should be predicted much closer to one. But look what we've
done. We've just gone through a whole neural network. It's really as simple as that. Just to do that.
Now this value here, we're going to call that y hat sub one because we're dealing with the first sample
that had the values 10 and 11. And the actual value we're just going to call y sub one. So we make this
distinction y sub one was one. Remember in our little table of four samples, the first one was
actual value was one. And our prediction here, y hat is 0.50. So not very good.
So let me just take you through this whole process here. So we started by 10 and 1. And in the design
of our matrix, we had to choose two inputs because we've got two feature variables. That input is really
dependent on the data that you pass into the network. And that was our input vector. That's two by one.
We multiplied that by a weight matrix that had to be four by two. And these are all random values.
And that got us to this, these four values here. It was n sub one, n sub two, n sub three, n sub four.
And that's on the bottom here, how we see that weight matrix times the input vector.
Then we add this bias. So this was vector addition here in this first step, all the zeros we added. So
nothing changed. There's our values, our n plus b. And then we put it through an activation function.
And once it's through the activation function, we multiply it by another matrix here. And that gives
us this one by one matrix, which we put through another activation function and we get out there.
So you can see the values are slightly different from what I had up top, because previously I did not
use the pseudo random number generator and different weight values were generated at random for me.
And the values were slightly different. And you can see the difference that it makes.
Not as much as the final output's concerned, because these are just purely random numbers.
And the neural network has not learned anything yet. So these will invariably be very poor random values
that were selected. So let's just scroll down here and have a look. This is how we got to y sub one.
I just want to remind you how we got there. It was this long equation. We had the activation function
of these matrix and vector multiplication added to that, the bias term. And then all of those
we multiplied by the second weight matrix. And that was an addition of all these terms. So that's how we
we had to do all of that just to get to y hat sub one. And all these that you see there, v sub one,
one, one, w sub one. These are all unknowns. We just chose random values to throw in there so we can do
the calculation and get to this to this prediction. But these are all unknowns. And I have to somehow
design something that will correct these. They'll have to get better. So to get to y hat I have an
equation here that has 13 unknowns, 13 variables. On a piece of paper I can draw a single variable
function y equals x squared, a nice parabola. So imagine something with 13 unknowns. We can't we can't
fathom that in our heads. We can't draw it on a piece of paper on a computer screen.
But that's what we have to solve. We have to solve this problem of something that has 13 unknowns. And that
is just a very very very very simple neural network. So how do we go about this? Well we use something
called a loss function. And a loss function is the difference between the prediction and the actual
value. So remember our prediction was 0.500 something and the real value was 1. There's a difference
between the two. But I can't just see them as numerical variables because these are categorical
variables. So how do I how do I how do I calculate the difference between two classes in a categorical
variable? Well this is one of the loss functions that we could use and we use very often to determine
the difference between two categorical classes. And the one would be my prediction and the one would be
the actual value. And we call this a loss function and it's a function of two variables. It is the prediction
and the actual value. And it's very simple. It's minus the addition of these two parts and each of the
two parts are the product of the actual value. So that would be one. And our predicted value which
was the 0.50 something. And we take the natural log of that plus one minus the actual value times the
natural log. That's log base e or Euler's number of one minus the prediction. And if we pass all of that
in we get a loss. And this is quite a high loss because it was 0.5 and the actual value was one.
So we get a loss of 0.69 just for that first sample. And if we add all these losses because
we've got to go through all our samples. We only had four in our little. But if you have thousands,
tens of thousands, hundreds, millions of samples, you've got to go through all of them.
All of those losses and that combines into a cost function. And for every loss function you get
different kinds of cost functions. The cost function for this category binary problem loss
function is just simply this. We're just going to add over all of the losses and then divide by how
many there are. That's going to be our cost function. But remember this loss function has this y hat in it.
And to calculate y hat was an equation with 13 unknowns. So my cost function is going to have many
unknowns in it. And that is a function of which I have to somehow manipulate to improve upon those
values to get the optimum values for those randomly selected weight matrix and bias node values. And
how do we do that? Through a process of gradient descent. And gradient descent involves the derivatives.
So let's have a quick look at that. Now, as I mentioned, we can't fathom an equation,
the graph of an equation that has 13 unknowns. So I'm going to stick to a very simple polynomial
that has only one variable, an x variable, like y equals x squared, which is the parabola.
So I'm choosing this polynomial here in 14, x to the power 4 minus 2x cubed minus 2x squared minus 4.
So just a simple polynomial. But it really represents this cost function that I have here with 13 unknowns.
So I'm just simplifying things just so that I can draw it on a computer screen and we can all
appreciate what's going on here. So this y plays the part of my cost function. And the cost, remember,
represents how wrong I am. So with a neural network, I don't want to be wrong. I want to be as correct as
possible. So I want the minimum. I want this cost, the value, to be at a minimum. So here we had 0.69. We're
going to add across all of those, every sample that we have. Somehow I want this value to be a minimum.
I want values for all the weight matrices and bias node values. I want them to be such that
if I plug them into this equation, I get the lowest possible number. And here we draw this graph
of this polynomial of mine. And remember, for just my single x value, I'm going to get a different cost.
And it's very simple with this convex function is that I want to be right down here at the bottom
because this y represents cost. And I want the smallest cost. And I can clearly see here,
if I plugged in a value of x equals 2, I'm going to get the lowest cost. That represents the fact that
for these, all these unknowns in this equation, I've got to get all of them just the right value
so that the cost is at its absolute minimum for this multi-dimensional function.
And it's actually quite simple to do this process of gradient descent. And it comes from this little
equation here, equation 15. It says, take whatever x value you are at the moment, and that's x sub t,
and you subtract from that some alpha value, and that we call the learning rate, times the derivative
of where you are at the moment. And remember, the derivative is the slope. So we're going to start
here just at random at x equals 1. We're going to start there at x equals 1. And that is the same as
saying we took those weight matrices and we just drew eight random values for the first one and four
random values for the second one. And for my bias matrix, I just chose zero basically also at random.
So that's the same thing. I'm just going to start at one. And now remember, the first derivative gives
us the slope of a function in wherever we are. So if we write here, the slope is downwards. And if you
think you're standing on the side of a hill downwards to go to the bottom of the valley, that's exactly
where we want to go. So a slope is very nice. It shows you how to go downwards. So if we start
here at x equals 1, it's really going to show us which way, because the slope is this tangential
line at x equals 1, around there. And it shows us the direction in which to walk. Now the slope here
is minus, but we want to go from x towards the right to plus 1. And that's why we have the minus sign
there. So we have this, wherever I am now, at random I start somewhere, subtract from that some step
size. And that's usually very small. We make it 0.01 and it gets more complex than that because we can
change it during the learning process. But for now, let's choose a fixed one, 0.01 times the slope.
And I subtract that from that. So that's going to be a tiny negative number minus, minus times a tiny
negative number is going to be a positive. So my next x value, xt plus 1, is going to be
something larger than 1. So let's just do that. There's my polynomial. I can print it to the screen
very nicely using Symbolic Python, because I set x as a symbol now, a mathematical symbol. So it's not
no longer available as a variable name. So there's my beautiful mathematical symbol. I can use SymPy to
do differentiation for me. So I say take y, give me the derivative of y with respect to x, and we see
there the first derivative of this very simple polynomial. And now we can just do that. Let's
plug in 1. 1 is where I am at the moment. So that's 1 minus 0.1 times my derivative, which is
now 4x cubed minus 6x squared minus 4x. It's the first derivative of my polynomial there. So I plug
1 into all those x's and I get 1.06. So indeed, if we look here, I went from 1 to 1.06. That's a tiny
step towards the right. But that's exactly what I want. Now it's easy to see here where the minimum
is. But imagine this multi-variable function in multiple dimensions. You can't just look at it and
know what direction to go in. Here we do know, but I'm using this simplified example, first of all,
because I have to, because I have a flat screen. And we see that it is moving in the right direction.
And that's what this little update will do. And all we use, in essence, in the end, I'm not going to
describe it as just partial derivatives. That's why you see this curly del here, d here, is that we
just take each of those individually. We, with partial differentiation, you can keep all the other
variables as a constant. And the derivative of a constant is just zero, so they all fall away.
And we just look at one of those 13 at a time. And we check for each of those in what direction we
have to move in all those 13 dimensions. And they will all move downwards, or at least they should all
move downwards. And there we go, 1.06. Just to show you, if we started at x, say x equals 3,
and we plugged in 3 into this, our next one would end at 2.58. So if we started at 3, just by random,
up here, somewhere, it will move to 2.58 down here. So it's also going to move towards the negative,
towards the downward part. Now, unfortunately, you can see there's also a little bit of a dip here,
minimum. So if I started here, it's going to move towards this dip. And if I started up here,
by random, I'm going to move towards this dip. And I'm going to completely miss the fact that the
lowest part is actually there. And that is, unfortunately, one of the problems of deep neural
networks is that you can land up in these minima, local minima. That's not the global minimum.
And fortunately, that is not as big a problem as you might imagine. And also, we have this idea of
all the possible values that these weights and matrices, the weights and the bias values can be,
that is called the hypothesis space. And we want to constrain the hypothesis space somehow, because
what you don't want is for your neural network to memorize the training data. And it will only do well
on the training data. We want it to generalize to unseen data, the validation or the test data.
Or we want to create an app and someone from outside is putting data in. We want it to do well on
that unseen data. We don't want it to memorize our actual training data. So all of these things conspired
to the fact that it's not as big an issue, this idea of we not always get to the minimum. We don't
always really need to do now. This is a very gross oversimplification of the problem here.
Suffice it to say that it is a problem, but it's not as big as you might imagine.
So that is gradient descent. And what we have now, this new value 1.06, is something that we can now
plug into our weight matrix. Say, for instance, this is one of the W11 values. We can now plug that
value back in there. And we can start this whole forward propagation process all over again, because
for each of those 13 values, we will now have an updated value, which should bring the cost function
down. And this process of now updating all of these separate 13 values through this process of gradient
descent, that's called backpropagation. Which now means the second time I do forward propagation again,
start with a 10 and 11 again. I have new values for the weights. I will have new values for the bias
term. And that should, in the end, give me a lower cost function. My prediction should be closer to 1.
After all the activation functions and all the multiplications and additions, it should be
closer to 1 now. And that means my cost function has come down. And that is really it. We've taken a
very simple densely connected neural network. And I've shown you how you do forward propagation,
which is just matrix and vector multiplication. We did vector addition. And we did, we used
activation functions. And we just designed the sizes of these things so that they make sense.
And then in the backpropagation step, where we updated all the values, we tried to minimize a cost
function. I think you'd agree is really not as difficult as you might imagine. Now we can create
all sorts of different neural networks. First of all, much bigger ones, but also much more complex with
different architectures. But the basic principles remain as we've seen them here. So I really hope
this helped you. And you now have a good understanding of the basic math that goes on when we use a deep neural
neural network.
Thank you.
Thank you.
Thank you.

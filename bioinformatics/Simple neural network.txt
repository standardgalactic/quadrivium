So the time has eventually come for us to have a look at a proper neural network. Now,
it's going to be a very basic neural network, but we are eventually there. I'm still going to use
a document here that I've uploaded to our pubs. I'll leave the link down below. You can also
download this R studio file right from GitHub. Now remember, if this is the first time you come
across these videos, please start at the beginning. Otherwise, things are not going to make much sense.
And also remember that I'm really after people who are interested in deep learning who are not
necessarily computer scientists or mathematicians, but really want to get involved with deep learning.
In my specific case, really getting healthcare professionals involved in deep learning.
We won't go into the code that will come with time. So let's just get going. I'm going to build on
everything that has gone before. And we're really going to construct this network. And it's going
to look very familiar if you remember what we discussed when we looked at linear regression
and logistic regression. The whole idea behind a deep neural network is very loosely based on this
idea of a brain cell, a neuron. And we can see one depicted here. We can see that this image comes
from Wikimedia Commons and you can just click on the link there and it'll take you to these wonderful
images. The whole idea is that there are many connections. So this brain cell with its nucleus
here and cell body has all these connections that bring impulses from many other brain cells in.
And it gets trance that impulse goes all the way along and it gets transmitted here along the axon
and then to all these connections to other neurons in the network. So many connections to many other
connections. And that is what it is all about. Here we go. And I'm just going to make the screen size
here a bit smaller so everything can fit on. Let's go. There we go. Now everything fits on.
Now you'll still see the input layer on this side. You'll still recognize this hidden layer here.
You might still notice this node here. But things look a bit different. And the most noticeable
difference from what it's come before are these many connections. No longer is this one input connected to
one other node in a single layer as we did with the logistic regression. Look at this. Number one,
there's three feature variables here, but there are four nodes here. Now that's completely arbitrary.
If you design a neural network, you decide how many nodes go here. And that number is something we refer
to as a hyperparameter. There are many hyperparameters in the design of a neural network. And if you design
that neural network, all those hyperparameter values are up to you. There are four nodes here in the hidden
layer. In other words, that's a hyperparameter. That's your decision. And different values will work
differently under different circumstances. But look at this first one, it receives input from all three
of the input nodes, not just a single one. And it also gets input from this node up here, which is
called a bias node. So I can create a bias value. And that can also be added in to these nodes. Now here,
we have three feature variables. So for the first patient, for instance, or whatever the case might
be in the subject, the first row, or one of the rows in your data set, the value for the first variable,
the variable for the second variable, the variable, the value for the third variable. Each of these are
going to be multiplied by some weight. And remember, now we're going to move away from calling those beta sub
one, beta sub two, the parameters, we call them weights now. But each line here represents a value,
a weight. So this x sub one value is going to be multiplied by this weight, and then input it to
that node. This node number two, this feature variable number two is going to be multiplied,
that value is going to be multiplied by some weight value, this line here, and as an input,
and the same here. And each one of them will be connected to each of these nodes. So if there's three
on this side, each with four connections, that means there was already there already be 12 values here,
12 weights that we need to optimize before when we only had the single beta sub zero and beta sub one
for single variable. You know, that was all that we needed to learn just from this tiny little
connection. There's already 12 parameters that need to be learned 12 weight values that we have to
optimize for in our loss function and in our cost function. And then we can even add this bias node
to all of these values as well. So these multiplications and they all get added, you can
also add this value in there. What you are going to get is what a value after all these multiplications
and additions is this value called z one. And then not only that, that's not where we stop, we're also
going to apply some function to it and logistic regression, we looked at the logistic sigmoid function.
But there are many other functions and they are called activation functions. And hence,
this idea of a neuron, you can see now all the dendrites, all the connections coming in,
all the connections going out. And there's some decision as to know what flows through here,
does an impulse go or does an impulse not go? That's our activation function.
Now let's represent things slightly more mathematically.
There is a short video just on linear algebra. Remember, I do have a course of
almost 100 videos on linear algebra. I linked to that as well.
But in case, you know, you've missed all of that, this might not make much sense to you.
Watch that single video. It's not that difficult, though. If we have a look at it, there is my three
input values from my three feature variables. And I'm going to call this a column vector with three
rows and a single column. So it's three by one. And I put this little underscore here, this line
under that represents a vector, rank one tensor. And the solution I want is this Z1, Z2, Z3, Z4, Z1,
Z2, Z3, Z4, depending on where you live. And so my output here is a four by one column vector.
One, two, three, four rows times one column. My weight is now my weights is now not just beta sub
zero, beta sub one, or maybe beta sub two. It's a whole matrix, a rank two tensor of values. And if
you look at this W sub one, one, W sub one, two, if we look at node one, it has one, two, three,
four values coming out of it. And there we have one, two, let's just look at the first row there.
One, two, three, four values coming out of it. Now, if we do this tensor multiplication,
we have a matrix on the side and a vector on the side. So we have to actually transpose it at the
moment, it is one, two, three, it's a three by four, I cannot have an inner product, a dot product of a
three times four matrix with a three times one vector, I've got to transpose that so the rows
become columns and columns become rows. So that changed from changes it from a three by four to a
four by three. Now these inner two values are both three, I can do this in a product,
and the result will be these two values on the outside, a four by one matrix exactly,
or column vector at least, exactly what I wanted four by one, four by one, there we see it.
And I can if I wanted to even add this bias node, and it'll also be also have to be if I add that,
it also has to be a four by one column vector. And if you look at up here, of course, one, two,
three, four, it's got to have these four connections. So four by one, and that leaves
me with a four by one column vector. Now I need to apply to each of these values z1 to z through z4,
I've got to apply an activation function, here we call it g. Now there are many, you've seen the logistic
sigmoid function, one of the most common activation functions though is is the ReLU function, rectified
linear unit, you see certain r and then lowercase e and then uppercase lu. And this is what it looks
like. No matter what value I input, if that value is zero or less, it just outputs a zero. So whatever
this value, this value that I calculate here, all these z's that I calculate through this whole equation
five, if that was a negative one, the activation function spits out zero, if z2 was negative one
million, it's never going to be that but just imagine, it's going to output a zero. So it'll
really be zero, zero. If it's more than zero, it just takes on that exact value, this line goes up at
45 degrees. So you can see an input of 0.62 is an output of 0.62, input of 1.26 is an output of 1.26.
This is called a rectified linear unit. And what we do is with each of these values, z1, z2, z3,
we pass through this very easy, the output then is this. And then right at the end, you know, we'll
combine this in some way so that there's an output. And that output can just be as is in a regression
problem. It can go through an activation function itself if we have a binary classification, which is
all we can do if there's a single node. But you'll see later, there can even be more than one node
here as an output. And then we'll use a different kind of activation for these last values, something
like a softmax function. And we'll get into that a little later. And that's it. There is a single
hidden layer neural network. And you can see the differences from this to from this to a logistic
regression network that we built before many more connections. And now you can see that there's
richness built into this because imagine I had more nodes, more feature variables, more nodes,
and then more of these layers. We'll get so many of these parameters and they all are going to in the
end, remember, we're going to get a value here, a y hat value, which is quite different from might be
quite different from the actual y the ground truth y. And we're going to see, you know, sum up in some
way or average in some way, all these errors, that gives us a big cost function, which is now a function
here already. And let's see just here, there's 12, and four, there's already 16 connections here. And,
you know, there's even more here, depending on what happens with deeper layers, you can see how many
parameters unknowns there are in this massive equation. And we're now really talking about
multi dimensional space. And we have to use back propagation, gradient descent, and then we'll
optimize all of these values. And we'll go through again, hopefully our cost function will show that
the error is now less, we do gradient descent through differentiation, partial differentiation of
all of these how many ever they are, weights they are, we now get better values, and we go forward
again and backwards and forwards until our error is as small as possible. And these values all take on
an optimum value. As I said, there's really a richness built into this, you can this, this algorithm can
learn a lot more than a simple logistic regression model. It can really try and mimic at least a simple
connection inside of your brain in some way. I look forward to speaking to you again.

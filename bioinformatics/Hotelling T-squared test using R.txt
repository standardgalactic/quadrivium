Well it's a lovely afternoon here in Cape Town, it's lunchtime so I thought I'd just
step outside quickly and just make this recording.
Last night I did a screencast on Hotling's T-squared test using R, that is a generalization
of the normal T-test, a normal T-test, that's a univariate test, it's a single variable
that we're comparing between two samples with Hotling's T-squared, we're comparing multiple
variables for two samples, something I think we should do a lot more. Human beings, biological
systems are complex systems, we're not just a single variable, we should probably make
use of these much more, we should see more of these in the literature. So I'm going to
show you one article but then more specifically how to do it using R.
So here we are in R studio, that is our trusty development environment to write our code in
R. So I'm going to do this as an R markdown file. So if we were just to go to RPUBs, we've
already published this, let's go have a look at the document when it's done.
Here is my RPUBs public page and you'll find most of the documents that I create for these videos
here, the majority of them will be on deep neural networks, but I also do this regular type of
statistics if I can use that term and I post these documents here as well. If we go look at it,
there is the document, the R markdown document printed out as a nice HTML page. Now remember,
the code will also be available on GitHub and the links for all of this will be down below. So there
is the document that we are going to look at and we've seen some nice plots there etc. You'll see some
equations there, of course you can just ignore those equations if you want to know more. They are
definitely there, but we're not going to concentrate on them too much if you're not interested in them
at all. So let's go back to R studio and let's put this document together. So at the top there's the
usual YAML. So we are going to talk about multivariate analysis of the means for two groups. Now we know the
means of two groups very well with the students t-test or the varieties of t-tests. We take a single
continuous numerical variable, we take data point values from two samples for that single variable
and we compare the means between those two samples. Here though we're talking about something else. We are
generalizing that very specific case of a single variable. I can compare the cholesterol in one sample
a sample of patients to the cholesterol in another sample of patients from the two populations.
But I needn't stick to just that very specific case of a single variable cholesterol. A human being is much more
complex than that or for that matter any organism. A lot of bioscience is all about very complex systems.
And you certainly are not just a single variable. You consist of many variables. And it is that
measurement, that comparison of all of the means of say for instance cholesterol, total cholesterol, total
some other forms of lipid and blood pressure and serum glucose. So you measure all of those combined
against each other. So it's not one one-on-one. It's not the cholesterol with the cholesterol for
the placebo and the control group. And not just the systolic blood pressure against the systolic blood pressure.
But that patient as a whole with all of those variables, we're going to compare two samples
across all of those variables. And that is multivariate analysis. So our dependent variables, our outcome
has many variables in them as opposed to just one when we talk about the student's t-test.
So with the YAML and back to my R Markdown, just the author there, the output that I like to do is HTML,
of course, the table of content to be true. So that when we do see the R Markdown file on our pubs,
there's a nice table of content and I don't like numbering of the sections.
This is the normal setup. Remember, this comes standard when you open an R Markdown file. I do set
the working directory to the get working directory. So it's going to find where on my file system,
on my internal drive, this notebook, this Markdown file is saved. And it's going to set that as the
working directory. And I've got everything else, all my other files that I want to use. For instance,
we're going to import a CSV file that exists in the same folder. So I can just set the working
directory to the single directory. Here are all the libraries that we're going to use,
Tibble, Reader, Plotly, DT, MVNormTest, DPLIER, IC, SNP, and Hotling. And it's the Hotling's T
squared test that we're after. So if you haven't yet installed those libraries, remember, use the
package here, click install, type in the name and install those. I like to just colorize my headings,
heading one, two, and three in my HTML files, that deep navy blue and a bit of a gold color. That's fine.
And that is how we import a logo. Remember, it's the exclamation mark,
and then a set of square brackets, and then just the name of the file without quotation marks dot
png. And that also that asset also lives in the same folder. So I don't have to refer to its whole
address on my internal drive because I'm setting the working directory to this current directory that this
markdown file lives in. Everything is in that same directory, no problem.
So let's just get back to this issue of this more generalized form of a t-test. If we just take
one variable, the student's t-test or the other variations of t-test that we know, it's a single
variable measured, the means of those at least between two samples. Here we are generalizing the
situation and saying, well, we needn't just stick to the specific case of a single variable. We can
generalize it to more. So let's just have a look at an example in the literature. So here we are
in PubMed. Let's just increase the size here a bit so we can see properly. And I'm searching in PMC,
that's PubMed Central. And if you search PMC as opposed to just normal PubMed, you're only going to
get articles that are open access, that you don't have to pay a fortune for to view, which in my opinion,
as in the opinion of many others is just the wrong way to go. Anyway, here's a nice little article in
the Journal of Oral Maxillofacial Surgery. And we see the PMC 2011 October the 1st published here in
the journal itself in 2010. And then comparing actual surgical outcomes and 3D surgical simulations.
So of course that was 2010. That software wasn't too advanced. I'm sure we could do a lot better today,
but that's not the point of this discussion of ours. What they did, if we just look down at the
methods section, you see the Hotlings T-test. It should actually be Hotlings T-squared test.
But what they did, if we scroll down, they operated on patients, they had imaging,
they also did the simulated virtual surgery, looked at the imaging after that, and scroll down,
scroll down. And this is what we look at, this imaging after the surgery, and just different
landmarks are colored differently, if there was a spatial difference between the two images after
it. So operating on a patient, operating on the virtual image, and then superimposing those two,
that virtual image was from the actual patient. And then seeing if there are differences, spatial
differences, how many millimeters were certain landmarks away from each other in the two images.
And if it's green, it's very low into the blue, and then onto the red side, it is more.
Now we can just take certain landmarks. So take the maxilla and the left maxilla and compare just that
landmark in that space between these two sets, the virtual image afterwards and the real image
for the real surgery image afterwards, and see what that is. And we can do a T-test there if we meet
assumptions for the use of a T-test, that being a parametric test, and we can just have a look and see
those specific differences. So we can go to each of these landmarks. But what we can also just imagine,
well, this is a complete and whole patient. The patient does not just consist of that one landmark,
and just doing a T-test individually between all of these, that kind of doesn't make sense. And if you
think about it, it doesn't really make sense that we just see this abundance of univariate T-tests.
Multivariate is probably what we should see more of, but we don't. And anyway, so you can take all of
these landmarks, and so there'll be a column of for each and for the two samples, and we can compare the
means across all of them at once. That one patient has many measurements. So we are comparing many
means against many means all at once. So let's get back to our studio there. And if you want to read
this, I explain a little bit about that. The assumptions, the Hotlings T-squared test being a
parametric test, we are looking at sort of the similar set of assumptions for the parametric T-tests.
And one is this, the main one probably, is this assumption of the sample taken from a population
in which that variable is normally distributed. And we can use, for instance, the Shapiro-Wilk test,
as we always do, to see that that is true. You also get a multivariate version of the Shapiro-Wilk test,
and we're definitely going to use that here, just to look at that, at that assumption. Now it's not
as hard an assumption as you might imagine. It is slightly robust against the data not being from
a population in which those variables are normally distributed. So it's not the end of the world if
it isn't perfectly normally distributed. Now, it just reminds me, we haven't really spoken
while I'm doing the screencast about multivariable, the term versus multivariate.
When we use the variate, we're talking about the outcome or the dependent variable side. If that is
multivariate, we are measuring more than one outcome, more than one dependent variable. When we
talk about multivariable, that means these independent variables that explain something.
So if we look at linear regression or logistic regression, we have a model in logistic regression
of many variables that can have an impact on a single outcome. So we would have odds ratios and
p-values for each of those as they impact this single outcome. Those are multivariable variables
that impact this single outcome. When we talk about multivariate, it is the outcome
that we are interested in and that has more than one variable in it. So don't get confused between
multivariable and multivariate. So in that sense, students t-test is a univariate test. The outcome
variable is the same variable that we measure the means of between two samples. So the second
assumption is just that the determinant of the variance-covariance matrix must be positive. Now
that is a bit of deeper mathematics there. I'm not going to burden you with that. So what we see here
is the variance-covariance matrix for a multivariate analysis where there are three outcome
variables and I'm going to call them A, B and C. And as we construct that, what you can see here along
the main diagonal, these matrices are always square and that means that the number of rows and the
number of columns are equal and we can see we have three variables and then we have therefore three
rows and three columns. Along the main diagonal, square matrix has a main diagonal. That's the one that
runs from the left top to the right bottom. That's the variance just of the one variable, the variance of
the second variable and the variance, just the normal variance. Remember the square of the standard
deviation along the main axis and then symmetrically around this we find the covariates.
So A and B, A and C and B and C. And because the covariance between A and B is the same as the covariance
between B and A, I've just put A, B, A, B and both. But strictly speaking, it should be A, B and B, A.
But that doesn't matter because there we go, there's the equation for covariance in case you've forgotten
it. You just take each value, you subtract it from its mean and the same for the second variable
and so on. And you multiply, for all of them at least, and you just multiply those, you sum it all
up and you divide it by the total number of subjects minus one and that's your covariance. And that's going
to go in into all of these. And you can well imagine if you're just looking at the variance of one,
that is just the difference squared. So variance, covariance. We should also probably just talk about
the dependence between these variables that will impact the results. If some of these variables
in your multivariate analysis are dependent on each other, of course, that is going to
have some effect on your results. And if they're totally uncorrelated, then that will really add
the unique variance to your results, which you actually want to see. Multicolinearity, for that
matter, and the independent variables, that will explain vermation amongst themselves. But we're really
talking about something different here. Just read that. I've put that in the notes. And there's some other
assumptions, sphericity, compound symmetry, etc. You can read just a little bit about that if you want
to know more. Obviously, you can look it up. So let's just have a look at Hotling's T-squared test.
So this is just a reminder of a normal T-test. And by the way, I like to show these. There's my LARTIC
there. So that is what you would have to type in to see this nice little mathematical equation on your
screen. LARTIC is very easy to learn. You can have a look at that there. So remember T-squared,
that is the normal student's T statistic. That is just the difference between the two means
for the two samples of the same variable divided by this pooled variance. And you see the pooled
variance here in the denominator. So the T-squared is different. Here we don't just have a single mean
minus the other mean. We have a vector of means. So a vector of means just means there are, in our case,
there was a, b, c. That would be three. It was mean number one, mean number two, and mean number three.
And you put them in a little column. That's a column vector. And you put and make another column
vector for the other sample with its three means. And that's what we're really interested in. So what
we're doing here is we're subtracting these two vectors from each other, x sub one and x sub two,
and we transpose that. So transpose means if I have a column, a single column of three,
and I transpose that, I will now have a single row with three columns. So you just like turn it on its
side. That we multiply by the inverse of this pooled covariance matrix. And I show you there if you're
interested how to calculate the pooled covariance matrix. And we multiply that. These are vector and
matrix multiplications, again, by the untransposed vector minus the other vector. And if you know
something about the dimensionality of vectors and matrices, you know that we're going to end up with
a scalar. Scalar means a single, a single value, not a vector of values. And there's our scaling factor
there that has to do with the sample size. And then we get t squared. And we express this t squared
in the form of an f distribution. And an f distribution is always going to depend on
degrees of freedom. So to get the f distribution, we see there it's d1 divided by d2 times this t
squared. Or just in another way, if we just look at the number of sample, total number of samples,
that is n and k being the total number of variables, there's another way it works out exactly the same.
So this d1 is just the number of dependent variables. And then d2 is n1 plus n2, that's the
two sample sizes minus this d1 variable, the number of variables minus one. If you multiply that by t
squared, you get f. And then the f follows a certain distribution dependent on these degrees of freedom.
And therefore we can work out a p value. Very simple. So let's run a few things. I'm going to go
all the way back up because I haven't executed any of these code chunks. So let's go there. Going to
execute this first code chunk. Let's import all the libraries. You can see the green bar moving down
there. That is executed beautifully. Let's move down. Nothing to execute here. Nothing to execute
here. And here we go. Let's import the data. And I'm going to use read underscore csv. That is from the
reader, r-e-a-d-r library. And that creates a tibble as opposed to a more old-fashioned r data frame.
And it's data.csv. It lives in the exact same folder as this rmarkdown file. So with my set wd get wd
functions there. I don't have to worry. And then the data table function here comes from
the dt library. And that's just going to print a nice HTML
nice little data table to the screen. Let's have a look. And we execute that. Let's be imported. And
we have this nice data table. You can search. And you can do ascending and descending. Show all the
entries move to the second page, etc. It's just a nice way to view a table on a web page.
Now let's just have a look at our data here. We seem to have three. Don't worry about what the
values are or what the actual variables are. But every subject has three variables. And we want to
just compare patients in group one and patients in group two. We want to compare all those three
combined. So this is a value that lives in this for this patient that lives in three-dimensional space.
Not in on a single line. It's not a single variable. But let's look at them one by one. I'm going to use
plotly for my data visualization. So I'm going to create a plot and I'm going to call it f1. And that
will be the plot underscore leaf function. The data is the data that I'm passing to it. The type is a box.
So I want a box plot. Plotly is fantastic because I can use this pipe operator here. So I pipe to that a
box plot. On the y-axis the complain variable. On the x-axis the two groups. And I give it a name
complaints. And then I pipe that into a layout. And the layout will have a title. And it will have an x-axis
with a list which only contains one keyword argument, the title. You can view my videos on Plotly.
It's for me the most fantastic library because it's very interactive. You've got all these buttons
there. I can save it as a PNG so that I can use it in a report that I'm writing. You can zoom in,
zoom out, do all sorts of things. View it online to change it online on Plotly's website. But it's
very interactive if you hover over that. I really love that. So we can see the sort of normal
distribution for the complaints variable. We can see there's a bit of difference between those two.
Let's just have a quick look.
Let's just have a quick look here at the second variable. We can see its distribution there.
Let's have a quick look at the third variable. And there we go. We can have a look at the
distribution of it there. So let's just do some of our tests. Now the first one we're going to do is
the multivariate version of the Shapiro-Wilk test. And that comes from the MVNormTest library.
What it wants though is something very specific. It only wants a data frame or a table of the actual
numerical variables. So remember we had a fourth variable that contained the categorical variable,
the groups. You have to remove that. It only wants, unfortunately, it only wants
a sub data frame or a sub table that only contains the variables that we're interested in.
So I'm going to create a second variable called data.vars. And I'm going to pipe my data through
the select function. And I'm going to use this argument minus one underscore of. So it just means
remove, please remove from this data table, one of the columns. And it's the group column that I want to
remove. So if we were to do that, if we just view these very quickly, there's data vars. And it now
only is those three numerical variables. Whereas if we looked at data, it has that group with it. So
I've just removed that. No problem. Let's just close that up. So now we can use the MVNormTest library.
And I put this MVNorm colon colon mshapiro.test just to show you what library has been used. Of course,
you needn't put that. You can just use mshapiro.test. And another little quirk, it wants that data in
transposed form. So if we look at it here, we have the columns at the top of the variables,
and then the rows down. It's going to shove that on its side. So we're only going to have three rows
here. And then all the values are going to be across. So that's transposing it. So you have to
transpose it before you pass that. And let's run that test. And we see a W score of 0.9. That gives
us a p-value of 0.05. That's very close, but it's still not significant. It's not less than 0.05.
So we can say that it passed this test at least, and we can accept this idea of normality.
The other assumption that we like to test is just to check that the determinant of the variance
covariance matrix is positive. So COV, it's covariance that was going to give me the matrix,
the variance covariance matrix. And the debt is just the determinant of that. And we want that
to be a positive value. And plus 7,853, that's definitely positive. So we've passed those
assumptions. And let's just go for the Hotling's T-squared test then. As simple as that.
Now I'm going to show you two versions of the Hotling's T-squared test. And they come from
different libraries. There's the ICSMP library and the Hotling library. So you can use either of those
two libraries. They both have the Hotling's T-squared test. And they just use different
functions for it. And the ICSMP library, it's HotlingT2 and Hotling.test for the Hotling library.
So what we want to pass is the two sets of data, the two matrices of values. So we're going to use
LYR, the filter function. So I'm going to filter the data according to the group column,
where the group column equals equals 1. In other words, that's a boolean question I'm asking.
Take only the ones out that return a true value. So the ones with the group equal 1. And I want all
the rows, comma, only columns 1 to 3. Because once again, I just want those columns. I just want the
numerical columns in there. And then filter data group equals equals 2. And then again, all the rows,
comma, columns 1 to 3. And if we execute that, we see Hotling's two-sample T-squared test.
And there's our difference that we think. We think that the difference between each of the three
variables should be zero. So we have this vector written here as a column vector, as a row vector,
and 0, 0, 0. That's what we expect the difference between that to be. And it's the three of them
combined, not individual, by individual, by individual variable. It's the three of them
combined. And we see the value of df1 there and df2. That's degrees of freedom 1 and 2. And we see a
p-value of 0.77. So there's no statistically significant difference between these two samples,
as far as this multivariate analysis is concerned. And then just from the Hotling library, just using
hotling t.test. And we're going to see the same value there, 0.771. So we really can't reject the
null hypothesis. And we state that there's no multivariate difference between these two samples.
So in my opinion, a much more exciting way to do your analysis. Biological systems, human beings,
are really not single variable entities or systems. And we should probably make use of this type of
analysis much more commonly. Of course, what we do do is we do our analysis where we say we
correct for all the other variables so that our two samples are very similar as far as all the other
variables are concerned. So that we can just drill down on this univariate, especially when we do
randomized controlled trials, we just want to drill down on this univariate single variable analysis.
And we correct for or we take out patients so that all the other variables are similar. But it's
probably a better way or we can make an argument for the fact that it's probably a better way just
to really concentrate on all of these variables that make up this area of interest that we're studying.
And you can well imagine that there are times that we can really make use of this. So what we are doing
here is just comparing the means, sets of means, vectors of means of two samples. Of course, you can
do that for more. And now we're going to get to multivariate analysis of variance and you get
multivariate analysis of covariance. And even when we talk about regression, you get multivariate forms
of regression, where we're going to try and predict a vector of outcome variables. So all of these tests
really have multivariate versions. And I really want to make the case for the use of most of them.
If I have some time, I might make videos on the multivariate analysis of variance and covariance
and some of the multivariate regression as well. Very easy. All of these things are so easy to
implement here in R. So I hope you've learned something. Have a look at Hotling's T squared test.
Look in the literature. Find some multivariate analysis. Just make sure that whoever published
that didn't make a mistake misusing the term multivariable and multivariate. I've seen that.
That does happen. But just to keep things simple, try and stick to those multivariable
means those are your input variables or your independent variables in regression models.
And multivariate, that's more your outcome or your comparisons as we have here. So try and keep
those two terms separate just to keep things clear.

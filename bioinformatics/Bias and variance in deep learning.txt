Now let's continue our look into deep neural networks and I want to pause a
bit and discuss poor performance of a deep learning model. So if you come
across this video and it's the first one you see please go back to the beginning
that really is my suggestion you know watch the whole series one lecture just
builds on the other. Now in the preceding lecture we we looked at
building our very first neural network we had these two hidden layers we wrote
the code inside of RStudio using Keras with the TensorFlow backend and that to
me is very exciting. In the real world though no one ever ever ever writes a
single neural network and just runs the data through it and everything works
perfectly it's beautiful. There are usually problems with the results the
code works well the neural network performs well but the choice of the
hyperparameters and how the network is constructed must be changed based on
the results that you get. There must always be this attempt at improving the
model. So in real life there's data pre-processing usually with the help of a
domain expert. Now that doesn't always happen but having a domain expert present
when the data pre-processing is happening really makes a big difference. A network is
designed later on we'll also see that you can just import some already existing
networks. We run the model put the data inside of the model we get results we have
to look at those results somehow we have to interpret what they mean and how good
especially how good they are so that we can subtly change things to that model
change the hyperparameters change the architecture of the network do something
else to improve the results. Now for that to happen we must have some way of
understanding what went wrong and I don't mean what went wrong in the execution or
writing of the code why why is the results not as good as we think they
would be or or better still how can we think about improving them. So first of all
let's just go back to the the training and the test sets and there's something
specific about these that we might have mentioned before but but we should spend a
little bit more time because it is this pre-processing of the data that is really
important before you start assessing how well your model did. I want to remind you
that you have a data set and you then split it so that you have a training set
that is the data that's going to be passed to the model and it learns from that a
big black box that learns from the data and then you must pass a set of data to
it that it has never seen before so the model never learned from these new cases
and that's the test set that we keep totally separately. Now you don't always
need to split that from an existing data set it might well be that you have a data
set and you use that data set and while the network is being developed and while all
of that takes place the data collection continues and it's this new data that
is going to be the test set so it needs to be a splitting of the original so that
can also happen. One thing though is to consider just the size of the data set we
mentioned before that if the data set is very small the norm used to be this we can
see it there the 70-30% split because we needed enough data in this test set to
make it worthwhile to make the testing accurate but in modern days where we have
perhaps millions of samples we can really have this five percent or one
percent split in making that and making that test set that will still contain
enough samples to be representative of the whole data set in itself. Now a few
problems might arise here it might very well be that the test set and the training
set are not the same especially when they are collected that data is collected at
different times. Think for instance of cases where images form the data that we
want to train on and that the training set might be very nicely selected high
resolution images so that we have this fantastic results when we do the training
but then when we pass real-world images they might be blurry high the lower
resolution we get very poor performance so there's this difference between the
training set somehow in the test set and those differences must be minimized. In the
end we want a model that will work well on real-world data. That brings us to this
point of a class imbalance one of the types of distribution where we think about
distribution in our data. Think for instance of a situation where one of the
elements in the target variable occur very infrequently that 95% of the target
variable is just one class one of the elements and less than five percent is the
other. That just means if that really exists I might as well just guess the
majority class every time I'm going to be right to 95% of the cases why do I need
a deep deep deep neural network. So if that is the truth in real life then yeah
perhaps you don't need a neural network but if it's because there's something
wrong with the data collection that there's this class imbalance then you
have to do something about that and one way to go about it of course is just
better data collection but if that is not possible to look at something like data
augmentation which we'll discuss with that we'll discuss later. Another point to
belabor is that the training set is not absolutely required and some data
scientists will only will take the whole data set initially and just do the
validation split inside of the model and some even refer to that validation set as
the test set another name for it also is also as a holdout set but that holdout set
validation set that is or development set that's also called that can be done
inside of the model and we saw that in the preceding lecture that that validation
set can be just be extracted to when the model is training and we can just see
that as our test set so when you see that occur don't worry too much about it
just to be formal here we're going to talk about a training set from which we
split a small little validation set during the training and we have the
separate test set just make sure when you see that that you're not confused about
it so really put these thoughts into the the before designing just think about the
data for a minute think about the things that can go wrong with the data and
specifically the the splitting of this data the next important thing to talk
about is just this the idea of the ground truth have you ever set just thought
about it somehow and somehow someone or by some means every sample in the data set
had the target variable denoted in a spreadsheet in a database doesn't matter
how but someone or something decided that that is the actual value that has to go in
there that if this is a CD scan and that is a benign nodule or malignant nodule someone
mark that as benign or malignant and that that there might be an error there that
might have been wrong and what we refer to as the ground truth the labels that
exist in the data set that we have might not be absolutely correct so we're
training on something that has an inherent mistake in it now there is this idea of an
optimal error that's the maximum theoretical the theoretically the smallest
possible error that exists in the target data so it's also called Bayes error and
that is what we are trying to work towards we want our models really to
achieve we get close to this Bayes error optimal error and at times this can be
different from the human error rate so imagine we have a bunch of CT scans and
it was a radiologist to just set a couple of radiologists and they just mark this
one's benign that one's malignant there's going to be some human error in
that so at the very least we really want our models to exceed the capabilities of
our human so the human error must be exceeded and we want to really approach
this theoretical optimal Bayes error one way to think about the ground truth
though is when you sit down and you look at the data set and you want to try and
and think about it and evaluate it yourself think about how was that target
variable how was it decided so examples of human it is coming very close to the
optimal error is if we have a group of experts looking at every sample so you
can't get much better than a group of experts say a whole group of radiologists
to then sit together and label every CT scan that's as good as it gets when it is a
piece of apparatus that just makes a measurement you want the best possible
piece of equipment they're the best apparatus to record that target value so
think about when you see it makes a difference what the error rate is in
your actual data set and what this theoretical optimal error is it makes a
difference so how do we evaluate the result we've data is pre-processed we've
fit the data we tested now against the validation or the test said how do we how do
we know how how good or bad it is and there are two things we have to discuss
here and that is bias and variance and these are you know they're easy to
understand but there's some subtlety to bias and variance this is quickly start
with bias that we can see here that's also called underfitting and that is where
the model does not separate two classes very well so I want to just draw your
attention to these little samples they come from the scikit-learn website and
there's some Python code you can just write it in Python it'll produce these
images and I got them from writing that code from their website now I just want
you to suspend the there's a bit of a difference here between this and what
happens in machine learning so all this is is these data points and they fit
this orange line and that's the actual the actual function and because this
apparatus that measured these data points is perhaps not that absolutely
accurate there's a bit of noise in that but we what we want to do now is just to
fit a line a model to this data and if we have a degree one polynomial that means a
straight line that goes through this so given any x input you know what is the what
does it predict the y output going to be there's going to be a lot of mistakes made
here so the straight line is a very poor fit it under fits this data now if we
just move this slightly just use your imagination change this to a machine
learning scenario what this line will be it will be what we call a decision
boundary and if we have lots of data points the one side of this boundary will
be predicted to be of the one class let's just imagine a binary target
variable and anything on the other side of this line will be predicted to be
something else and then you might have ones going on the wrong side of this line
as far as the predictions concerned so that would be a poor that model or
boundary line would be a poor model neural network but in this instance where
we're just fitting a line to data it's also poor because there's big it makes big
errors if I give an input value here down here there the actual values up there
there's a big difference between those two it's a poor fit now we can make our
machine learning model more complex so we move away from a straight line decision
boundary to something that's more curved and in the instance that we've drawn here
there's a much better fit to the actual real world behind the scenes line this
orange one that that was the true line and this model it's a very close fit that
was somehow the optimum now remember that's what we're trying to achieve with
machine learning we never know what this orange line really is but again if this is a
a decision boundary it's going to be you know it curls around some of the data
so imagine again suspend or use some imagination so we see this graph in a
different way that there'll be points on either side of the line and again on one
side and on the other side they will be predicted as different classes and if it
sort of squirms around these data points on either side that's a better
decision boundary but look at the right hand side now we've upped the degree of
the polynomial so much that this equation this blue line which is an actual
equation actually goes through each and every almost each and every one of these
points and that is complete overfitting so complete overfitting means that if you
then see this blue line as a decision boundary it's going to curl around the
training set data points so well that it completely separates the two classes but if
you give it new data that's way too convoluted that overfits the training data but it's going to
be very poor when it comes to real world data we call that overfitting or this model will have what we
call a high variance I actually started off by wanting to discuss bias so let's get back to that
this is a bias problem that we have here that there's a total underfitting it really does not separate the
classes very well if this was the decision boundary there isn't a good separation of the two so you're
going to make a lot of error there so see bias on the one side underfitting and variance overfitting
on the other side this is also called memorization so if you pass data to a model and it does very well
in the training data it might very well be just that it's just memorize it's called it's memorize
the data and it will perform very poorly on actual data now how are we going to know when we deal
with bias underfitting or high variance overfitting well we're going to look at two things the training
set error and the validation set error now if you think back to the preceding video we had those two
come out the error or the two sets that went into the model the whole set but then the validation
set that was split from that so let's look at overfitting with a very low training set error
so that really indicates then there's a large difference between the error rates of the training
and the validation set so when you look at those lines that formed when we ran the model in our studio
that the training set that error just drops and drops and drops and it's you know it becomes very
small but unlike the example the contrived example that we had with the 50,000 data point
samples we might find that the the error on the validation set that green line was much higher
much more much different so imagine the error rate of the former being about one percent and the error
rate and the latter being about ten percent that's high variance so it's trained it's overfitted it's
trained to the training set too well and it doesn't work well on unseen data the validation set
now imagine there is poor error between both of these so they both have in the order the training
set and the validation set are both here in the order of about 15 to 16 percent under the assumption
that the error in the target variable was very low so it was very accurate data that was fed to it so
just bear in mind that little assumption because that's going to make a difference in a short while
so the difference between the two the the validation set and the training set the error is very close
to each other and they are much higher than you would expect the up that the the error inside of the
target variable itself so this model is said to have high bias so it is not even it's not even doing
well on the training data that it's being given and then we might get a scenario where both the
variance and the bias is high that is still assuming this velo human error or optimal error inside of
the target variable and then say a 15 percent error on the training set and a 30 percent error on the
validation set and that's going to give us both high variance and bias so let's just think about this
influence of the optimal error though so if we moved now when we had 15 and 16 percent up here
we had both we had the the training and validation set errors being very high but that was under the
that gives us bias that was under the assumption that there was no error in the target you bring up
the area in the target say to 14 percent that we know that there might be a misclassification in the
data that we bring in and we still sit with an error 14 to 15 percent that's actually a fantastic model
with low bias and low variance same error rate so so you've got to see this in context of what you
think as a domain expert the error rate is in the target variable itself in short though this is what
we're after look at when you run the model what the training set error is and what the what the
validation set errors and what the difference between them are keeping in mind what the baseline error the
ground truth error might be just to mention in older reports older research documents you might see
this trade-off between bias and variance you change your model and if you change it you go more in the
one direction and change it going to the other direction but really in a modern world where we have
big data sets and where we have very sophisticated deep neural networks that trade-off thing no longer is
really the norm you can get very low bias and very low variance in the same go so if you get this these
errors which you should understand you can read this material again until until it becomes part of you so
that when you run this these models you know exactly what's happening just a few pointers as to what to do
if you see high bias so this model really underfits the training data and ways that you can go about
solving this problem is just the following i just make you write this down some way and this will become part of
you as you do more and more create more and more networks if you have this pipe bias just create a bigger
network that means put in more layers putting more nodes in every layer that's just one way to go if that
doesn't work just train for more epochs it just might be that you haven't gone to the bottom of your gradient yet so just
train through more epochs and then lastly you can try a whole different architecture
so when the input was for images don't do a normal deep neural network try a convolutional neural network
for instance and we'll get to designing and writing code for convolutional neural networks in the future
when you have high variance large difference between the error rate of the training and validation sets
you can try the following possible solutions number one capture more data that is king
you can augment the data if you can't get your hands on more and there is this class imbalance you can
really think about augmenting your data for both those problems and then vary the interesting stuff
because this we can manipulate in code and that is implementing within the design of your neural network
implementing regularizations dropouts batch normalizations batch normalizations and other
techniques and in the very next video we're going to look at this very exciting concept of understanding
regularization so that was a bit quick it's as i said it's easy concepts to understand but there's a lot
of subtlety to it don't let these confuse you they i'm trying to depict two things here one is just this
fitting of the this actual data but i want you to use your imagination to also see these blue lines
as a decision boundary where it's going to be predict a class on one one or either side now remember
in higher dimensional space it's not just going to be a line but some hyperplane but you can imagine
it going from just a straight thing to something that is convoluted and cooled all around that's what
we call the decision boundary and that decision boundary can just become too complex and you have this high
variance on the side it can also be totally not complex enough and you have this high bias on this side
so you've got a sort of aim for this middle ground here read this document again so there was no coding
in this so read this document it is going to be available on our pubs the actual file on github
i'll put the links down below and i'll speak to you in the next lecture
so
you
you
you
you

My name is Jean Klopp and I'm really on a mission to teach deep learning, deep neural networks.
I'm a surgeon. I'm involved in research using deep neural networks because I think the time has come
for doctors, nurses, healthcare professionals, everyone involved in healthcare to start learning
about deep learning. It's usually the space where we'll find mathematicians, computer scientists,
but to solve problems, we must understand that the domain knowledge lies on our side,
on the medical specialist side, and we can contribute to this. We can work together with
our colleagues in mathematics and computer science just to improve the overall aims and what we can
achieve with deep learning. So really, I think the time has come for everyone interested in healthcare
to get involved in deep learning. Now, these videos are not only for those who are from the
healthcare professional side. Anyone interested in deep learning should also find these videos helpful.
Now, these videos are based on documents that were created in R and published here in RPUBs.
So they are available. The actual RMDR studio files will be available on GitHub. You can view these
on RPUBs as we're going to do here. You can download the actual files on GitHub. I'll show you the links
there. There'll be videos on YouTube. I'll talk about it on Twitter and also disseminate it on LinkedIn.
So connect with me on LinkedIn. Follow me on Twitter. Sign up and subscribe on YouTube and also view these
files in RPUBs and also download them from GitHub. So I'm really trying to put these everywhere just to
sow this net as far and wide as possible because I really want to get everyone involved in deep learning.
So this is the document. I'm not going to read from the document with these RPUBs, but I'm going to use
it as a baseline just to discuss a few topics here on YouTube. Really, if you want a bit of extra
information or you want to hear a bit of explanation instead of just reading the document. So let's carry
on. We'll move down. And what I'm going to do really here is just use linear regression as a first
step to understanding deep learning. There's a lot of concepts we can take from simple linear regression
and it'll teach us those concepts and we're going to use them later when we develop deep neural
networks. So imagine we are in a scenario where we want to use deep learning. What would such a
scenario look like? Well, one of the first ones is actually an example that will refer to something
that we will call supervised learning. In supervised learning, we have this single variable which we
want to predict. Now I've created a very simple example here. You see I've created a computer
variable or an object called sales and I've put these 10 values into this numerical vector here
using C there. So I've got these 10 values. Now they can represent anything. Just imagine they represent
the sales in units of tons. So some medical company selling something that goes into the production
of some medication and they sell so many tons of each product and there were three tons sold and four tons
then two tons. It doesn't matter what these are but they represent a variable called sales and they are all
these values and I might have a lot of other variables associated with each of these that would predict
that the sale was going to be three tons or four tons or two tons. So that makes this variable here
something we call a target variable. I'm going to use the other words, the other terms for it, but I'm going to
I'm going to use target. So this becomes a target variable and these values, these 10 values, I'm going to try and
predict them. Now without looking at all the other variables that might be there that might I might use to
predict these, let's just consider a very simple model. A very simple model to predict that and that is the mean
of these 10 values and we know how to calculate a mean you sum over all the values. The x sub i
there's 10 so x will be 1, then the second one, then the third one, x sub 1 would be the 3, x sub 2 would
be the 4, x sub 3 would be the 2 there, x sub 4 would be the 4. I hope you didn't get those mixed up
anyway. 1, 2, 3, 4 until 10. That's what the summation sign means and I'm going to go from 1 to n and n is 10
here, their sample size is 10 and then I'm going to divide by n how many there are and that's just
the calculation for the arithmetic mean. So when we get this mean sales I can just say mean here,
mean, say mean, let's do that mean and sales as an argument, save that in the mean.sales computer
variable or object and indeed the mean is 4.9. I can suggest that as a very baseline model that no matter
what my input variables are I will always predict 4.9. So I would predict that all these values are 4.9,
4.9, 4.9, 4.9, 4.9, 4.9. I haven't really learned much. I'm just predicting that every output given any
set of input I will always sell 4.9. Now of course I'm making an error here because if I predicted the
sale of 4.9, the actual sale was 3. There's a difference between 4 and 9 and 3 and here I sold
4 and again there's a difference between 4.9 and 4. There's a difference between what my model predicts,
it will always predict 4.9. This is a very simple model but there's an error every time and I can
check what the differences are by just subtracting this one from that one in each instance. So I can
say 3 minus 4.9 and 4 minus 4.9 and 2 minus 4.9. That difference is the error that I'm making
and I can sum over all these errors but just have a look at one thing. I mean 2 is less than 4.9 but
5 and 6 and 9 and 12. They're more than 4.9 so I'm going to get some negative numbers and some
positive numbers and lo and behold if I'm just using rounding here otherwise we're going to get
something to the power negative 15 which is basically zero. I'm just subtracting all of those and I'm
adding all those subtractions. I'm going to end up with zero because that is just what the mean is
doing. Some values are below and some values are above and this one sits in the middle. So if I sum up
all the differences I am going to end up with zero. So zero is definitely not the error. I'm not making
an error of zero. I'm definitely making an error. So a better way to do this is what we call the sum
of squared errors. So I'm going to take every value again here and I'm going to subtract that from the
mean as we did before but I'm going to square every value. Remember if you square anything the result is a
positive number. So minus 3 times minus 3 is positive 9. So I'm always going to end up with a positive
and then I'm going to add all these squared values. So that's the sum of squared errors and now that gives
me if I do that the sum of sales minus 4.9 square it's going to do a bit of broadcasting. In other
words element by element it's going to subtract 4.9 from each one of these, square that, and then
I'll have 10 of those squared differences and then sum all of them and I get 100.9. So that's closer to
the fact that I am making an error here. There's one problem though is that that error is not in
proper units. So say for instance those with how many pounds I sold. Now I have pounds squared. What
is a pound squared? What kind of thing is that? So that doesn't work. Number two is if I have more,
I only have 10 here, but you can imagine if I added more, I'm just adding all my errors. So the errors
are going to be related to how many variables I have. So again, I can't compare two errors with two,
you know, in two different scenarios because it's just going to depend how many I have.
I might be woefully wrong with something that just have five errors and be very close with something
that has a thousand of these values, a thousand errors, but you know the thousand might be much
more accurate one in the end even though it has a bigger error. So we've got to do something and we
solve that problem as we see down here by dividing by the number of the sample size. Now we don't do the
sample size. We actually do n minus one and that has to do with the concept of degrees of freedom which I
don't want to get into here. So if I divide by how many there are, my sum of squared errors, which is still
in the numerator there, divide by how many there are, now I'm stabilizing this thing. So it doesn't matter how
many errors I have, how many elements I have in my target vector here, my target variable here, I'm dividing by
how many there are, so it doesn't matter. Now I can compare two different scenarios with each other.
And because I had pounds squared, remember, I'm just going to take the square root of that.
And we call that the standard deviation and of course without taking the square root we call that
a variance. Now that is a different way of looking at variance and standard deviation. We used to look
at it in statistics just as a measure of dispersion, but we're not looking at it as a measure of dispersion
here. What we are looking at it here is an indicator of how wrong my model is. If my model always predicts
the mean, then I can use the standard deviation or the variance, the variance is this without the square
root, of how wrong my model is. So the variance in the error of my model, how bad my model is, that is what
variance and standard deviation really is. And if I do that here, I'm dividing by 9, there's 10 minus 1 is
9, and the sum of squares. So I have a variance here of 11 that describes the error in my model.
And this brings home a very important concept that the actual target, now there's 10 of these,
so Y1, Y2, Y3, Y4, are these numbers up here. My target, if I can represent them as a Y,
Y, each of them, is going to be the mean plus some error. The difference between those two is some
error. And so I can run through all 10 of those, and each will have its individual error. And if I add
that error to the mean, I'm going to get the actual Y. And now, this is really important concept in
supervised machine learning. I'm going to have some target variable, and I'm going to have some model,
and if I add to that some error, I'm going to get to the target. And that is, that's machine learning
right there in equation 5 here. Now let's learn a few extra terms by just looking at improving our
model. Now what I'm going to do here is to set the random seed, and so just if you run this, you get
the same results. I'm going to create an input variable and an output variable. My input variable
is going to be 100 values with a mean of 10, a standard deviation of 2, and one decimal place.
And I'm just going to add some random noise to each of these to get my output variable. I'm just,
it's been a contrived creation here of an input variable and a target variable. Sometimes it's
also called an output. A target might also be called an output. I'm using plotly here just to show you
all these values. So I've got my input variable on the x-axis, and I'm predicting what the output
variable is going to be. So in the end, this would have been my sales, my 10 sales values up here.
And now we are putting an input. So this one for an input of 7, I will have a result of 6.1. For this
one, an input of 6.4 will give me an output, a target value of 7.4. And I'm asking the question,
can I use this 6.4, or that 7, or this 6, or that 7.6 to predict what the second number,
the output variable, the target variable is going to be. And you've seen this in linear regression.
I can perhaps draw a straight line through here. And that straight line is going to represent my
model. So that anywhere on that straight line, given whatever the value here is on the x-axis,
I can read from the y-axis what the output, the predicted output is going to be. And that is going
to be different from the real output. If the line goes, you know, just imagine here, I didn't draw it in
here. But imagine the line goes down here. And right about here, it's predicting just over,
say, 8.1, 8.2, given a 7.5. Look, I'm in the line with a 7.5 here. So down there, maybe here,
it would have been 8.2. But in reality, it was 8.8. So my model would be out by 0.6. If it was 8.2
minus 8.8, negative 0.6 would have been my error. And so if I draw a line in here, there'll be an error
between each of these. And remember, I can square all of those. So that's the sum of squared errors.
And I can divide that by n minus 1, which would be 99 in this case, and get the variance to describe
the error in my model. But what I'm going to try and predict here, my aim here is to predict this
line. And remember a straight line from school algebra? A straight line will have a slope.
You know, this will be a positive slope going up. If the line went down, it'll be a negative slope.
And remember, slope is just rise over run. So if I draw a line from here to there, it will be the
difference in y divided by the difference in x. That gives me the slope. And when x is 0 here,
it's going to cross through the y line somewhere, and that's the y-intercept. So if I have those two
called parameters, the y-intercept and the slope, I'll be able to draw this line. So those are the two
things I have to learn. And there's the learn in deep learning. We're trying to learn what those
values would be, so that when I draw this line here, and the aim here is to minimize the error,
the error between what the predicted value is going to be on this line, my model, and the true value
there. That is the aim, to learn the very two best values for that slope and that intercept to give me
the best line to minimize my error. And again, I just show you another term here. It's called
deviation that's observed what is the real value minus what my model is going to suggest squared,
and I sum over all of these, same story. Now if I were to run this over these 100 sample values that
I did, the sum of squared differences, I get 681. That is really the variance in my model.
Now, let us just through blind interpretation, and again, just to reiterate, I used the mean
of these sales here, the output variable. I used the mean of that as my model. So I'm subtracting each
one of these from the mean, squaring that, summing that, so the sum of squared errors. And I haven't yet
divided it by 99, so this is not the variance. This is 681. Let's improve this by introducing
just blindly a slope and an intercept, a slope of 0.8 and an intercept of 0.1. So if I plug it into that
yi, and then we put a hat on because this is now the predicted one. Remember, it's beta 0,
which is the 2 here, and beta 1, which is the 0.95. I'm just plugging those in. I should just
change what I've written here because I've changed this to 0.95 and 2, so I'll just change that to 0.95
and 2 when I republish this. So there we go, 0.95 and 2. I'm plugging those in just as blind as I can,
and I do the sum of squared errors, and I get 383. So that's an improvement on the 681. And I can
actually just divide those two with each other just to get the r squared, which will give me an
indication of the improvement in my model. And we call this ratio actually the systematic variance
relative to how much variance there was to begin with. That's the baseline model, the unsystemic
variance. Statistical terms, we needn't care too much about that. Now I can abuse the system a little
bit by cheating and using the linear model function in r, and I create this output variable
is dependent on just a single input variable. If there were more, I would just plus the next one,
plus the next one. And that gives me these two values right here, the best ones, an intercept of
1.94 and a slope here of 0.99. That's what these two values under the estimate was. So in actual fact,
the prediction is going to be 0.9982 times my input variable plus 1.942. That's going to give me the
predicted y. And those are the best values for beta 0 and beta 1 to take to give me the least number of
errors. To get to the true value, you'll see there's no hat on this one. I have to add an error that's
the difference between each prediction and the actual value. And we can just expand this if I had
more than one, if I had more than one input variable, also called a feature variable, that would just
expand to beta 1 x1 and beta 2 x2 and beta all the way to n, how many input variables I had, plus some
error is going to give me the true y output. The concept here really is to learn the best parameters,
beta 0 and beta 1, so that I can draw the best line here to have the least number of errors. Now by the way,
if you hear the banging outside, I've mentioned a couple of times, they're building a new neuroscience
center right outside my office. And it doesn't matter that I come in early in the morning before
work or leave after work, the banging continues and it's driving me nuts. So I do these recordings
out of hours, but it doesn't matter. You're still going to hear this banging as they build the new
center. And please just forgive that. I cannot run away from that here, my office at all. So a few
basic concepts that we take away here, that there's something we can learn, something we can
learn. And one way to learn was to create this sum of squared errors. And in some way, which is going
to come in the next section, as we continue this look into linear regression to teach us the
fundamentals of deep learning, we're going to learn, I'm going to show you how to learn what these
parameters are by minimizing something we're going to minimize the error. And I'm going to show you
the concept behind minimizing the error. So I can get the best line to fit through here. And that
line is just a model. It predicts this target variable. And that's what we're going to do in
deep learning. We're going to shove in a lot of data. And we want to learn, we're going to predict
some outcome. And we want that outcome to be as close as possible to the truth. I might give a lot
of images, some with a nodule in the lung that's cancerous, and some are not imagine some CD scans.
And I want the model to predict if I give it an x ray in the end, or CD scan in the end, is this
tumor or is it not a tumor? And I want that to be as accurate as possible. Same concept is going to
apply. I'm going to create some error function. And I want to minimize as this, as this deep learning
network that we're going to develop as it learns from the data, because every time I'm going to give
a data to learn from first, which has the target variable known. And it's going to learn these
parameters akin to the beta 0 and beta 1 here to predict the best possible accuracy. And it's
going to change them and change them and change them until it gets the best values of something
that is akin to this beta 0 and beta 1 to give the best prediction for that CD scan if this really
is a tumor or not. So it makes the least amount of mistakes. That is what we are after. And you can
see that these concepts, these basic concepts here that we're showing here in linear regression
is really the basis of what we need to do to develop our understanding of deep learning.
So that's it for this video tutorial. I'm going to move on to the next one. I just want to
warn you, there are going to be some additional videos on YouTube. Two of the concepts that we're
going to come across are linear algebra and calculus, some derivatives, multivariable
derivatives. Now, strictly speaking, you do not have to understand linear algebra or derivatives
to be able to write code or to create a deep learning network. Don't run away from it because
you don't understand linear algebra or derivatives. I'm going to make two separate extra little
videos just to show the very basic concepts of the derivatives in a multivariable system and
to the basics of linear algebra, just to remind you from stuff that you might have seen at
school or you might have seen early on in your university career and you've just forgotten
them or you've never seen them before. It doesn't matter. I just show you the very basic concepts
just to ease the understanding. But again, you don't need to know it. You're going to write
a line of code and the computer is going to do this all for you. But I think it is nice
if you have that understanding. If you want deeper understandings, I've got two massive
playlists on YouTube, one on multivariable calculus and one on linear algebra. And if
you really want to understand it, I think there's over 100 lectures in each of those two playlists
that I've put out there. If you really want to understand linear algebra, you really want
to understand multivariable calculus, you can watch those. But again, I reiterate, it's not
necessary. We're going to write a line of code and the computer is going to do all this for us.
But perhaps just watch those two very basic videos. It really is going to help you to
understand how this deep learning, deep neural network that we are going to construct, you
know, what the basics of it really is, just to give you that intuitive understanding of
what is happening here. Actually, without you knowing, you already have it just from this
video. I look forward to speaking to you again. As I say, subscribe on YouTube, follow me on
Twitter, connect with me on LinkedIn. At least the format that you see is available on our
pubs. And it's going to be and it's available on GitHub. I'll put all the links in the video,
it'll all be written on LinkedIn, and I referred to it on Twitter, etc. As I said, I want to
spread this word as far and wide as possible. I want as many people to get to grips with deep
learning. And I want us to start using it in our research and answer some fundamental questions
and solve some problems in healthcare if we all come together and we make the effort to learn about
a deep learning.

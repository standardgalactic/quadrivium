So in the previous video, we looked at a few ways just to improve training, a few concepts.
You can still read the RPOPS documents, download the RMD file from GitHub, have a look at those.
What I want to do in this video is just to show you how we can implement at least a few of those
improvements that we saw in that video. I'm also going to introduce you to a package called tfruns.
Let's just move down here. We can see the tfruns. Install it via your package manager.
And it is going to help us to look at the effect of changes that we make to our model.
Now this demonstration, in this demonstration, I'm going to use just a normal R script. So that would
be file, new, and then we see our script here. Just an R script. And there are two files here.
You see the file that I'm working in, but there's a second file that just contains the model.
It should be familiar to you. There's our Keras model sequential. We compile our model,
and then we fit the model. So that's all that is in the second file, just the actual model.
Look, we're not even importing Keras here or TensorFlow or anything. This is just the model
in the second file. So let's go back to the first file and let's get things started. First line of
code, I'm just going to set my working directory. And I'm just going to click, hold down control and hit
enter or command and hit enter on a Mac. And that just executes that line of code. And we can see at the
bottom here in the console, that line of code has been executed. So my files, both of them,
live inside of this folder or this directory. And now it's easy just to reference the files
that are in there. I'm going to import Keras and import the reader packages.
Reader, remember just to read a CSV file, Keras, because we need to do some machine learning.
Now the training and the test sets for this example exist in two separate spreadsheet files,
comma separated value files. So they've already been split. And I'm using the computer variables
train.import and train test.import to hold those two CSV objects. Read underscore CSV, not
dot CSV because this comes from the reader package. So let's do that control enter. And the second one
control enter. And we'll see we have two imports here, test import, train import. That's what I called
them. And you can click on those two buttons. And you can see, we noticed that they're 27,020 observations
over 13 variables in the training set. And then in the test set, there's 2943 observations
along the same across the same 13 variables. As per usual, I want to change these into matrices.
So I'm going to say train.import. And that's going to be as matrix, the train import. Let's do that.
And then I remove the names, they were column headers, and I'm going to remove them. And I'm doing
the same for test import, and I remove the column headers. So I'm left with these two matrices.
What I want to do now is to create my train and test data. And for columns one to 12,
that was the feature variable, and the target variable is in column 13. Remember,
it's row comma columns, square brackets indicating that I'm using indexing. So I'm creating two train
and two test sets here that two, two of each train data and train labels, I should say, and a test data
and a test labels, just to hold the features, and then the target variables separately. So I'm going to
execute all four of those. Now remember, in the previous video, we spoke about now just normalizing
those with the way that we're going to normalize in this video, is take the mean and standard deviation
of each of the feature variables, so each of those 12 columns. And what we're going to do, that's for
the training set, remember, we're going to for each value, subtract its mean and divide by its standard
deviation for that variable. So I'm going to create this empty variable, this empty vector, I'm going to call
it feature dot means, it's a vector, and it has a length of the number of columns in my training data,
which was 12. So it's this empty vector of 12 spaces, control command, enter. And we can see it
here feature measure, it's just one to 12. And it's all false, false, false, it's just this empty,
it's just this empty vector of 12 values. Now what I want to do is I'm going to iterate through each of
the columns. So for an I, so a for loop, I in one to the length of the feature means, remember,
there are 12 of them. So I'm going to iterate from one to 12. And I'm going to start populating feature
means, remember the 12 empty spaces. So during the first loop, that will be spot number one, will be
the mean of my training data's all the rows in column one. So I'm looking down column one, I'm calculating
the mean, and I'm putting that in position one in my features mean, then it'll be two. So it'll look
at column two in trained data, get its mean and place it in position number two. And so until we get
to 12, and then we see here, we have feature means up here, it's 12 numbers numbers one to 12 in the
indexing, and it's the mean of each of the columns. And I'm going to do exactly the same for the standard
deviation. So standard deviation and run my for loop as well. So I've got 12 means and 12 standard
deviations. Because what I want to do now is to create this normalized feature set. So first of
all, I'm going to create an empty matrix that's going to be the same number of rows and columns.
Remember, it was quite a lot in in when we first imported it, it was 20,000 27,020. So rows over 12
columns, I'm creating this empty matrix to hold that. And then I'm going to do two for loops and nested
for loop. So on the outside for loops, I'm going to go through all the columns. And then the inside,
the inside for loop, I'm going to go through all the rows. So what I'm doing with this, I'm going
to populate all of these empty values in this matrix. And I'm going to start with row one column
one, and that is going to be that position, train data, subtract from that the feature mean,
divided by the feature standard deviation. So the end refers to this columns. So I'm going to go down
each and every step, each and every row in the first column, each one of those sample data,
data point values, and I subtract from that the mean for that column or variable, divide by the
standard deviation for that column or variable. And once I'm done through all of the first columns,
all of its rows, I come outside back to the first for loop, and we go to column number two,
and we repeat it for all the rows and column two, etc, etc, etc. So we just run through all of those
very quick. Now we're going to do the same for the test data. So I'm going to create this empty
matrix of the same number of rows and columns as my test set, my feature test set. And then I'm going
to loop through all of those, this nested loop, same story. But I'm using the feature means and the
feature standard deviation, that's from the training sets mean and the training sets standard deviation.
So I'm subtracting the test data. So from each individual sample value data point value in the
test data, I subtract the mean and the appropriate mean and standard deviation, but from the training set.
Now let's import TF runs. And you see we're using the training underscore run function from training runs,
I don't have to put this training runs and two colons, TF runs and two colon symbols, but just to show you
that's that's where this function comes from. And now I'm going to reference the model that I actually
have. So let's run through the model. A model is a, I'm going to call a baseline model. It's a
curi-sequential model. It has a dense layer, another dense layer, so two hidden layers and then an output
layer. The output layer is a single node because this is a binary classification. So I'm using the
sigmoid activation function there very nicely to go from zero to one. Let's have a look. I've got 48 units
and 48 units, so quite a bit more nodes there than the 12 nodes of my input vector. I'm using
rectified linear unit as my activation function in both. And remember in the first one I have to
input the shape, which is equal to the number of features, 12. But I've got this new thing here
called kernel initializer. And remember the first time that we get the first multiplication during forward
propagation, it's just going to initialize random values for the weights the first time we go.
But we can also set those. We can have a specific one. And this time I'm going to specify kernel
initializer and I'm going to set it to init underscore w, which I created up here. So instead of this typing
this here, I could have just put all of this right here. And I've just done it separately to show you
that's possible. So I have this initializer and this is a Keras function, initializer random normal.
So it's going to use a random normal distribution for all the weights in that very first tensor that
is going to be multiplied by my input vector. And I'm saying that the values for that tensor must
have a mean of zero and a standard deviation of 0.05. And I'm setting a seed value here 123. That means
every time I run it and you run it, you're going to get exactly the same weight values.
By default, the bias nodes, they are set to zero, all the biased values are zero initially. And you see
here, I'm not even referencing it here. But you could put it in here as well and just initialize zeros.
That is for the bias nodes. But it's zero, doesn't matter. So what I want to do is just bring up a
page for you here. Let's bring up this page. So this is keras.rstudio.com. And I've moved all the way
along and I've clicked on the reference tab at the top. And you can see the contents here of
of this whole long page. But you can just go to the right hand side and you see the initializers.
You see regularizers there, you see losses there, optimizers. Let's have a look. So here's
one of the initializers. We used initializer random normal. You can see how it works. There's a mean,
a standard deviation and a seed. And you can set those any way you want. And you can also see all
the other initializers. So you can try any of these. And if you click on them, here we have
a Lacoon normalizer from Jan Lacoon from a publication. And you can see, you can read more about
those, read the sufficiency back prop by Jan Lacoon, I think. Yeah, that was. And we can try that
normalizer. It just has a seed. So there's no other argument to set there. If we look at
some of the optimizers, this is the RMS prop optimizer. You can see, we've discussed some of
these arguments in the previous video, you can read up more about them. Let's get back to our model.
So we're going to get to the compile step. And I'm going to use the optimizer, the optimizer RMS prop,
and I'm only setting two things, the learning rate, I'm setting the learning rate,
and I'm setting row value here. So 0.01 and 0.9, those would be the defaults. My loss is going to be
binary cross entropy. I'm dealing with two categorical variables. That's a good loss function.
And then we're going to fit and we pass the training data normalized. And the training labels
to the fit and our validation set is going to be test data normalized and test labels. Now this
is a data set that contains a lot of noise, very difficult to learn from this data set.
So let's go back here. And all we're going to say is training run and the file equals the name of this
file because it lives in the same directory or folder. And up here, we set the working directory,
I don't have to refer to its full address, I can use it as such. Now it's going to open that file,
and it's going to run that file. And then something else is going to happen. Let's have a look.
We see that the model starts running. And we can see there's a big gap. There's a lot of
variance here, a bit of overfitting, I would say. And we can see the model run over the 40 epochs that we
have set. There we go. And then this page opens up very nice. We can see some information here on the
right hand side, we can see an accuracy of the training set of 75. So we can certainly let this
run a lot longer. And then 70% for the validation accuracy. So quite low there. So a few things that
we have to do, we see the model here at the bottom, the model with the layers created. And if I go to
output there, we can see what the model was. And the code will give me the code of my model and the code
of the file from which it was called as well. So very good. Let's close that.
Now we can go back to improvements and we can say, let's make some changes to this.
I want to make some changes. So let's change the
let's make this a smaller network, we're going to put 12 in each of these. Let's put only 12 nodes
in each of these. And let's make our learning rate slightly smaller. 05. Let's set the road to 0.5.
Just changes you can make. Look at those pages on the RStudio Keras website. Think about some of the
concepts that we've discussed before. I'm just going to illustrate some for you here.
So let's just save this. Go back to our original. And let's run this again. Just to show you if you
do latest run. Let's just run that. It'll just give you some information down in the console as
to what had just happened. But we saw that in the overview that came up. So let's train this again.
Same file, but the changes have now been made. Let's run that. And there you go. It's run. And we can
see what has happened as we did before. What I do want to show you, though, is just instead of latest
run, let's use compare runs. And if we do that very nicely, we're going to see both of the two previous
ones come up. And we can see the changes that were made. So where the units were 48 before,
they are now 12. When the second hidden layer was at 48 units and now has 12. The learning rate,
you see the change and the change in the row there for the RMS prop. So you can see the changes that
were made. And you can do a side by side comparison to see if you've improved things. So we can see we
can do quite a few more epochs because this was still going down. So I'll certainly do that. We see that
we've certainly improved our variance here, our high variance in that these are closer. Pay attention to the
scale, though, because that might trip you up, that the scale might might might make this look a lot
better than than what it really is, the improvements really are. So have a look at these. But that's a
beautiful way to go about it. Just to do these, make changes and compare the runs, make changes and
compare the runs. Let's try something else. Let's go back. And for the weight initializer,
we're going to use this Lacoon. Let's bring that back up again.
Lacoon and you see what what happens here, it is going to be a normal distribution,
but it is going to vary according to the number of input units. And we spoke about that in the
previous video. So it's going to do this according to the input units that the input units for this weight
tensor. So let's just have a look at that. All we have to do is say initializer. Lacoon normal.
Let's do that. So let's save this. We go back and we're going to run our model again.
And there we go. We can see what I would perhaps do here is introduce some dropout or regularization.
And I would certainly let this model run for quite a bit longer. But let's have a look.
Compare runs now. And now we can see from the previous one, the changes that we have made where
we had the normal and random normal initializer before. Now we have the Lacoon normal. And
we just see the change that we made there. And we can see the difference that it had caused. So it is a
beautiful way to play around with your model and see the changes that are made.

So if we go to the races, part three, introduction to matrices.
If you've watched other videos, you know, when you just land up with this one, you know, go to the start.
The first lectures on vectors, which for me is the ultimate basic fundamental thing of linear algebra.
Now the second one is matrices, but there's so much to say about matrices, I've split it into different videos.
So, you know, do the whole playlist if you just suddenly, and if YouTube just suddenly shows you this video, there's much more to it.
And of course, if I do remember, in the description down below is going to be a link, or you can just search for it on my YouTube channel, on how to set up a Julia environment, which we've done here, and then how to open these Pluto notebooks and activate said environment, which you can see here in the setup.
So I'm not going to repeat myself, because it's part of all these videos, and I don't want to repeat it in all these videos and bore you to death.
But anyway, we're activating our environment that contains the packages specifically to this project of linear algebra using Julia.
And what it's all about is teaching you linear algebra, but also showing you a modern way of doing it, using code to check your work and to gain a deeper understanding and make it a hell of a lot more fun.
So what we're going to use here is just a single one of the packages, the linear algebra package you've seen using linear algebra there.
So this one's quite easy.
I mean, the previous video where we used it to solve linear algebra, to solve linear systems, I mean, it takes a while to get used to.
You're just doing simple algebra, you're just writing it in a different way, doing these elementary row operations.
Anyway, so I promised you that matrices themselves, they are mathematical objects, which in some way has nothing to do with Gauss-Jordan elimination and solving systems of linear equations.
They are mathematical objects in their own right.
And so what we're going to do here in this video, it's just going to look at very simple stuff to do with them as mathematical objects.
And the first thing we can do is just add and subtract two of them.
So you see two matrices there in equation 1, A and B, and you know now that they're rows of values and columns of values.
And adding them, the way that we've defined as human beings, the addition of two matrices is that we do it element-wise.
So the element in row 1, column 1 adds to the element in row 1, column 2, and the element in row 1, column 1 and row 1, column 1.
What am I saying?
And then the element in row 1, column 2 goes as to the element in row 1, column 2 of the other one.
So you can see there, A and B, and then A plus B.
And it's addition, just as we did in grade 1 or whatever, where you start your school career.
Very simple addition.
Fundamental importance here is they've both got to be of the same dimension, or in computer language terms, the same shape.
In other words, equal number of rows and equal number of elements.
So I can't add one if it had more rows and more columns, because what am I adding it to?
You can't just say, well, just pad it with zeros.
If you pad it with zeros, it's a different matrix.
So they've got to both have the same number of rows and same number of columns.
Otherwise, addition and subtraction are not defined.
So if you get that little, if your teacher or lecturer is kind of that mean, and asks you to, in a test, to add these two matrices, and they don't have the same, don't pad the one with zeros, and then try and give a solution.
You write there, this addition is not defined, because we cannot add, by definition, two matrices if they don't have the same number of rows and columns.
End of story.
No calculation required.
So problem 1 there, create two 2x3 matrices in code, investigate the results of the addition.
So I've just created one there.
We see the column vectors there, 1, 2, 2, 4, 3, 3.
And I've saved that as the computer variable called matrix underscore 1.
And you see the result there is a 2x3 array of 64-bit integers along two axes.
And I've created another one, which is 3, 0, negative 1, 4, 0, 2.
But I've created it row-wise.
Remember how to create these, where the space is in between, and the semicolon, meaning jump to the next row, etc.
And I've just said matrix 1 plus matrix 2.
And if you look very carefully, 1 plus 3 is 4.
4, the 2 there plus the negative 1 there is 1.
3 plus 0 is 3.
So it's element-wise addition.
And the same goes for subtraction.
I mean, there's a bit more to it.
And we'll get to that one first.
But intuitively, it's just subtracting the second one from the first one.
Nothing going on there.
And then in problem 2 there, I've said, oh, let's correct this.
I like to correct the errors as we go, as I pick them up.
Because we save these to GitHub, so you can download them.
So the fewer mistakes that are in there, the better.
So subtract the second matrix in problem 1 from the first matrix and investigate the results.
So it's matrix underscore 1 minus matrix underscore 2.
And you can just see a subtraction of those elements.
But what's happening here in disguise is there isn't any, there isn't really something, well, I can't say that.
But let's say it.
There isn't really something called matrix subtraction.
It's actually just addition in disguise.
Because what we're doing is scalar matrix multiplication.
Now, we've seen scalar vector multiplication.
We take a scalar.
We multiply each element of the vector.
We're doing the exact same thing here.
Taking a scalar.
We're multiplying each element in the matrix by that scalar.
Such that we get equation 3 here.
Very neatly written out.
So it's an M by N.
So it's all genetic.
So whatever you want to make M and N.
You multiply scalar K by it.
It's each element multiplied by K.
So if we make K negative 1.
A minus B is actually A plus negative 1 times B.
So it still remains matrix addition.
So it's actually the only thing as I say.
Don't worry.
I'm just, you know, spitting out some words here.
A plus negative B.
It's A plus the scalar negative 1 times B.
You get it.
It's not difficult.
So that is what we do there.
Scalar matrix multiplication 3 times matrix 1.
That's just going to give you 3 times each of the elements.
Nothing's, really nothing special.
The only thing, remember, they've got to, when you do addition or then this pseudo subtraction,
is they've got to be both of similar number of rows and columns.
Otherwise, you just write there, not defined.
Okay, that brings us to the fact that the additive, there's an additive identity.
So if you think about real numbers, the additive identity is the element 0 because anything plus 0 or 0 plus anything remains anything.
Okay, there is this idea of an additive identity for matrix addition that if you add this matrix, this identity matrix, additive identity matrix to any other matrix, that matrix remains unchanged.
And because, you know, you know now that it's element wise addition and we're just dealing with numbers here, of course, it's just going to be the matrix full of zeros.
And also, there's this additive inverses.
And what the additive inverses does is says, any matrix that I have, I can get this unique additive inverse, such as when I add the 2, I end up with the identity matrix.
In other words, all zeros.
And it's just the same as with numbers.
Because, I mean, the additive identity of 3 is negative 3, because if I add negative 3 to 3, I get the additive identity, which is 0.
Okay, so in problem 4, I say, add the appropriately sized 0 matrix to the first matrix.
And remember, in the first video, I showed you these little shortcut functions, the zeros function, 2, 3, because it's got to be the same size, remember.
And if I add that to matrix 1, matrix 1 doesn't change at all.
And this matrix addition, we'll see that it actually commutes.
So I could also say the 0 is 1 plus matrix 1 is going to be the same thing.
And then every matrix has its unique additive identity.
Negative, for A, it'd be negative A.
So that A plus this negative A equals the 0 matrix, all zeros.
So solution 5 is at the additive inverse of that.
And, of course, the additive inverse of that is just negative 1 times that whole matrix.
And if I add those two together, I get all zeros.
Nothing, nothing more.
And what I say here is because of the inheritance, and that's a very important little sentence I put in there,
because we're going to talk about proofs now.
And before you run away, we're not doing any proofs here.
And introducing proofs in linear algebra is both good and bad, and I'll tell you why.
But what we're doing here is we're just inheriting from adding and subtracting and multiplying real numbers.
And because we know that there's commutativity and associativity there,
we're also going to get commutativity and associativity when it comes to matrix addition.
And let's just get back to the proofs a little bit.
I'm digressing, but I think it's important.
Sometimes in the textbooks and in your class, you're just going to be asked to do proofs as well.
And you're just thrown in the deep end of these proofs.
And that's just absolutely nonsense because you start hating proofs,
and you try just to memorize them.
And it's just a shamble.
So if you want me to make a video on the proofs, I'll definitely try and find some time to make those.
But what it boils down to is we're inheriting from something else.
And what we're inheriting from when it comes to here is from an example of a field,
and an example of a field would be the real numbers.
Now, when I talk about field, I'm talking about fields, rings, and groups.
And that's part of something called abstract algebra or modern algebra.
And abstract algebra is probably one of the most beautiful subjects or topics in mathematics.
And it puts a firm ground into these things that we just at school learn to take on face value.
Oh, but we have this thing called additivity or multiplication on real numbers.
There's some much deeper S-H-I-T going on there.
And that is, oh, I don't know if I should delete that.
I probably will leave it in.
But anyway, there's some deep stuff going on in abstract algebra.
And we establish these properties, and we say if something obeys these properties,
then they are an example of this thing.
And one of these things in abstract algebra is a field.
And we have these properties of a field, and then we check the real numbers actually obey all these properties.
Therefore, a real number must be a field.
And the way that we set up the reals, now, I mean, to set up the reals, you need to set up the integers.
And to set up the integers, you need to set up the natural numbers and piano's postulates and all those things.
I mean, it all goes back so far down the rabbit hole, which is actually where you need to start.
And actually the things that they need to teach you in the beginning just make mathematics much more fun and understandable
than this rote sort of stuff that you try to do.
And that's just nonsense.
But anyway, it boils down to such a deep thing.
And abstract algebra is absolutely the basis of all of that.
And that's a fun thing.
But I'm talking all about proofs here.
So a little bit up from the bottom depths of this rabbit hole is the fact that the real numbers are a field.
And a field has these properties.
And one of the properties are, as we set up the real numbers, is the fact that we have associativity and commutativity.
I mean, 2 plus 3 equals 3 plus 2.
And 2 times 3 equals 3 times 2.
And associativity is also there.
And because we inherit those properties, that makes it very easy to do these proofs.
Because all you really have to say in a proof is, number one, that's how we define addition, element-wise.
And because we inherit from the properties of the field of real numbers, we can prove that these things are the way that they are.
It's just this sort of inheritance that makes these proofs quite a bit of fun.
And I think you have to spend some time sort of considering these things.
And that makes these proofs, which sometimes, as I say, Lenny Adel was just chucked at you.
And then you have to try and memorize what's going on there because it makes no sense.
And actually, they're quite easy.
And actually, they're beautiful.
And it's such an opportunity to be introduced to proofs in a non-threatening, easy, and fun way.
And, of course, because of constraint time, most of the time they're just chucked upon you.
And then you hate proofs.
And then it's just a disaster.
Anyway, where was I?
Problem number six.
Investigate commutativity of two matrices.
Now, remember, this is not a proof.
An example is never proof.
A counterexample, you know, that's very important.
If someone, you know, states this postulate or, you know, saying that something is something and you find a counterexample.
And then that theorem, you know, then that thing, if it was a theorem before because someone approved it before, that theorem disappears because there's a counterexample.
So counterexamples are very important in the disproving of a theory or making a postulate never become a theory.
But examples are never proofs.
But I'm going to show you an example here because I just want to show you, you know, help you with this intuitive understanding.
So there we create another matrix there and all matrix one and two, which we've already created.
And we say matrix sub one plus matrix sub two and matrix sub two plus matrix underscore one, I should say.
And you see the results are exactly the same here in solution six.
And then as far as associativity is concerned, we also create a matrix three.
Remember, they all have the same number of rows and columns.
Otherwise, it's not defined.
And you can see I've set those two up.
First, add one and two and then three.
And then secondly, add two and three and then add one to that.
And you can see if I use this double equals operator, that's a logical operator asking is the left hand side equal to the right hand side.
And the result is this bit value true.
Okay, then in the next one, I can also do the scalar multiplication over addition.
So I've got this addition of the two matrices.
I can add them first and then do the scalar multiplication.
Or I can do the scalar matrix multiplication first to both of them and add them.
I end up with exactly the same thing.
And if you're asked to prove this, well, you just inherit from the properties of the field of real numbers.
And Bob's your uncle.
Well, anyway, there's a problem, investigate the distributivity of a scalar with addition of the two matrices.
There we go.
So again, three times first addition and then three times plus three times.
And you are both going to end up, you're going to end up with the same thing.
And as I say, the proof of that is very easy.
Then at the end here, we come to matrix, matrix multiplication.
That's a lot more fun.
And it really boils down to the vector dot product.
Because in the resultant vector, in the resultant matrix, if we multiply two matrices, we're going to end up with the resultant matrix.
And each element in that is the result of a row and a column dot product.
So that we have these two vectors that we dot product with each other.
And video one is all about that, you know, has that in it.
So you can go watch that again.
But just as a reminder, I'll put it out for you here.
So if I have these two column vectors, u and v, I can do their dot product.
And that would be the same as the transpose of one, all in video one.
But what it is, is u dot v.
So I take this v and I turn it from a column vector into a row vector.
And then it just becomes u1 v1 plus u2 v2 plus u3 v3.
And something hidden here is the number of elements.
Remember, because if you want to do dot product, both of those vectors have to have the same number of elements in them.
Otherwise, you can't.
And that brings us to this fundamental factor.
It's just written here.
Easy peasy.
As if it's nothing.
But it's fundamental.
That the consequence is that matrix-matrix multiplication is only defined for two matrices A, MN, and B and P.
So see the two ends there are the same?
So the column number of the first one has to equal the number of rows in the second one.
Otherwise, it's not defined.
So the number of columns in the first must equal the number of rows in the second.
It's not defined.
And what you end up with, the resultant one, has the number of rows that the first one has
and the number of columns that the second one has.
So you have this AB multiplied.
That its dimensions is now going to be MP.
So the row number of the first one and the column number of the second one.
And it's only defined if the columns of the first rows of the second are the same.
So remember that little sentence.
And it comes down to the fact that this is the dot product.
You know, for each element, we're doing a little dot product.
So in problem line here, we're going to create a 3x4 and a 4x2 matrix.
And in that order, because we'll see commutativity doesn't hold when it comes to this bugger,
which is matrix-matrix multiplication.
So the column number there is 4.
The row number there is 4.
So in that order, we can multiply it.
And the result will be a 3x2 matrix.
So there we create one, a 3x4.
It's just a thumb suck 1.
And 5 as a 4x2 one.
So 4 rows and 2 columns.
And then we just simply use this multiplication symbol, which in the computer languages is the star symbol, shift 8 on my keyboard.
And we just multiply them.
And lo and behold, we get a 3x2, just as promised.
So how did this happen behind the scenes?
I'm not going to do this whole thing here.
I'm going to do the, I think I described the first two here.
But again, these are one of those things you have to sit down or stand at the board or whatever you do and just do a bunch of them.
And just get familiar with how to do it.
But let me just construct the first two solutions for you.
See the 15?
And then we're also going to go for that 9 there.
So let's do that.
How do we get to that 15?
So read here, in the solution above, we note that the first element in the result matrix is 15.
This is the dot product of the first row of matrix 4 and the first column of matrix 5.
Okay.
And they both, if we look at the first row, there we see the first row, 1, 2, 3, 4 in the first row, the first column, 1, 0, 2, 2.
So if I do that, I can do that dot product because it's both four elements long.
So 1 times 1 and 2 times 0 and 3 times 2 and 4 times 5.
And if I add those four multiplications, I get to 15.
And you see there, I do the same for the element in position 2, 2, this 9 here.
It is a dot product between, so that resultant 1 is in, where are we, that 9, in row 2, column 2.
So it's going to be row 2 in the first one, column 2 in the second one gives me row 2, column 2, that solution in the last, in the resultant matrix.
And it's really as simple as that.
It's a bunch of dot products.
And remember this little simple sentence here.
So when is it defined and how do we do it?
If we want to go for solution in the solution, say row 1, column 2, we take row 1 of the first, column 2 of the second one.
And if we do that dot product for those two vectors, the row vector and the column vector, if we do the dot product there, we get that solution.
As simple as that.
So here I've got create two 3 by 3 matrices.
And if they 3 by 3, both of them, I can do A times B and B times A.
Because we're going to get the 3, 3 in the middle.
And no matter which order I do it, I get the 3, 3 in the middle.
In other words, the column of the first one, the row value of the second one.
But you'll see if I create two ones, I call it matrix underscore 6, matrix underscore 7, I multiply them, I don't get the same results.
And if I use the logical operator there, I get a false.
So even if it is defined by the number of rows and columns, the commutivity does not hold.
It's not commutative.
Commutativity does not hold for matrix, matrix multiplication.
So do a bunch on your own.
It's really not that difficult.
Well, there's just a bunch of arithmetic and you mustn't make any arithmetical errors.
And just check your work.
Just create them in code and multiply them by each other and see the result.
And come back to this.
Make notes in Julia, like I've done here.
It's very nice for me.
I like notes to be neat.
And that's actually why I do this for myself.
And all sorts of other works that I do.
I don't write on paper.
I do it like this because it looks neat in my handwriting anyway.
But then again, I am a surgeon and, you know, doctors have horrible handwriting.
So there we go.
I have to make my notes like this.
I hope you enjoyed it.
Very important that you have to leave a comment.
And equally important you have to, if you haven't already done so, subscribe.
So this is a whole series.
It uses Julia, but you can use other languages as well.
If you want me to do more or use another language or whatever the case may be, discuss this.
And of course, if I made little errors anyway, tell me about it so you can correct it because
these notebooks go on GitHub so you can download them.
Subscribe, leave a comment, like or dislike twice.
I've got to make that little, apparently I have to make that little YouTuber as if I'm
a real YouTuber comment, a little funny comment.
Anyway, I hope you enjoyed this one and part four is coming up.

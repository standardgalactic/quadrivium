Linear Regression
Let's have a look in this lecture at the very exciting topic of linear regression.
Let's just import our fancy style sheet there. Off we go. What are we going to require here?
We're going to import numpy as the abbreviation np, pandas as pd,
and then from scipy.stats we're going to import this new thing, linregress.
Because it is a method inside of a sub-module there, linregress is on its own and I can just use it as is if I import it this way.
Our old friends matplotlib, seaborne and warnings.
matplotlib, the magic command there and ignoring the filter warnings.
Off we go. Linear Regression.
Now remember when we did the t-test we had two categorical groups
but the variable inside of those groups that we wanted to compare to each other were
the ratio type numerical data. Then we did the chi-square test where everything was categorical data.
But what if all the data is just numerical?
That's when we're going to use linear regression.
So we're just going to compare one set of
numerical variables against the other set of numerical variables.
So just for this little example, I've created this regression.csv
so we're not doing our, we do it in the end but here we're not using our usual
MOOC underscore mock.
We're going to use regression.csv
and read it in as a csv into this computer variable called regdata
which will then become a pandas data frame.
Let's run that and have a look.
So there we go. It is just two columns.
PCT, CRP, procalcitonin and C-reactive protein.
Now what does linear regression do?
It's just going to plot these two values on the x and y values.
So you might choose either as x and either as y.
But you're going to say this is my x value and this is my y value
and I'm going to make a plot.
So in the x, y a dot at the intersection of this x value and this y value.
Now you set up the x and y depending on what you want to say with your data
but more about that later.
But that's in essence what's going to happen.
There's something very odd about this data set though.
There we have not a number.
So either that was left blank or something weird was filled into our spreadsheet there.
But certainly I have a CRP value there but not a PCT.
And here I have a PCT but not a CRP.
Where am I going to put this dot?
So it's very important with linear regression to get rid of any rows that contain non-numerical values
or nonsense values.
That is very important.
So let's go ahead and do that.
And this is the way to do that.
I'm going to create a new data frame called reg underscore data underscore to the second one.
I'm going to take my original one reg underscore data dot drop in a
and the argument that it takes is subset equals and then the two columns.
So if any row that contains in either of those columns a value that's not numerical it's going to delete it.
Now let's look at the data set.
So just pay very careful attention to the way to that syntax there.
So there we go.
And you'll see beautifully down both.
You'll see for instance 11, 12, 13 was thrown out.
But for each x value there's a y value.
Whichever one you choose x and y to be.
Now let's just describe this.
We just want to see what our data looks like.
We've never looked at this data.
Again it does not come from any particular patients.
It's just random values.
So we see that there are now 26 PCT values.
And of course there should be 26 CRP values as well.
See the mean is 6 and the standard deviation is 2.
And for the CRP mean is 61 and standard deviation of 25.
We can plot that by our old friend the violin plot from SNS.
The data that I want to plot this is the Pandas data frame.
And I want it to group by PCT and CRP.
And I give the names PCT and CRP.
And there we go.
It's a bit of an odd plot because obviously the values are not very comparable.
This goes to 150 and this only goes to what was the max there?
Only to 9.8.
So it's a bit of an odd plot.
Not a very good plot to submit for publication.
Anyway it gives us an idea.
Now here comes the linear regression function.
LinRegress.
What you want to tell LinRegress is the x value and the y value.
That's what you want done.
So I want from Registator to the PCT values and from Registator to the CRP values.
But now remember they're both 26 and for each one there is a corresponding other one.
LinRegress gives me back 1, 2, 3, 4, 5 values and I've got to put them in order.
The first one is the slope.
Now let's just look there.
Remember from school the equation for straight line y equals mx plus c.
Where the m value is the slope.
Remember that is how steep the line is.
That was a positive slope.
This was a negative slope.
And the higher the steeper the slope the higher the m comes.
C was your y intercept.
When you make x zero then y was just going to be this value that cuts through the y axis.
So this is what we're going to have initially.
Because what LinRegress is going to do is linear says line.
We're going to draw the straight line and it'll have a slope.
In order to draw the line it'll have to have this intercept.
That's why I call it intercept.
You can call these whatever you want.
But giving them names that are meaningful helps.
Intercept.
There's going to be this r value which we'll discuss.
A p value which we always want.
And standard error.
It's going to give you back those 5.
So let's have a look.
This is one way that we can do it.
I'm using the print command here.
Remember this is exclusive to Python 3.x.
So 3, 3.1, 3.2, 3.3, 3.4.
That you have to put these.
The print command works very differently from the 2.6, 2.7 versions.
So print.
The slope of the line is.
Comma.
When there's a comma there.
It's going to put a space there automatically for you.
So don't worry about putting a space there.
Slope.
Then it's going to put that value.
Comma.
And then this funny thing.
Backslash n.
That's an empty line.
So it's like hitting return or enter twice.
The y intercept is.
And then it's going to put the intercept.
The correlation coefficient.
That was that r value is r value.
The p value is p value.
The standard error is standard error.
Let's run that.
Oh, it's not going to work because I never ran that command.
Let's run that.
And then this.
There we go.
It would not know what slope is because I never executed the cell.
So the slope is 10.
That's quite a steep line.
The y intercept is negative 3.
The correlation coefficient is 0.96.
The p value is 1.2 times 10 to the power negative 15.
That is extremely low p value.
Way less than 0.05.
And then the standard error 0.56.
So let's plot this.
The beautiful plot to do linear regression is called regplot.
SNS from seaborne dot regplot.
It takes these important arguments.
Below we're going to use our appendicitis data set.
And I'll show you a lot more arguments.
But most of those are default values.
Just want to show you how complicated things.
And it's complicated in one way.
But makes it very powerful because you can change so many things.
But these are the quintessential ones you want.
You want whatever you want the x value to be.
That's reg data 2 pct.
Reg data 2 crp column will be my y values.
I want to construct a 95% confidence interval in my graph around my straight line.
That's a beautiful thing to do.
And in order to do that.
Now this is not part of scikit's bootstrap.
This is something that's built into regplot.
So the two are not the same.
So I can just say ci equals 95.
And n underscore boot is 1000.
Please do 1000 bootstrap values for me.
And you remember what bootstrap values are.
And look how beautiful that graph is.
Beautiful straight line.
And you can see most of the data points are very close to this line.
Hence my very low p-value.
So the slope is 10.
And so what does that say?
It would say because I chose.
Remember because I chose.
This is the wrong way around.
In the x-axis we have pct.
That's quite correct.
And on the y-axis we have crp.
So for every one increase in pct.
I'm going to get a 10 increase in crp.
It's very well correlated.
So it's this positive correlation.
Because the slope is positive.
Now the intercept of almost negative 4.
Remember that doesn't really help us.
It's just a mathematical thing.
We can't say when the pct is 0.
We can have a crp of negative 4.
So you have to use a bit of common sense there.
But if we use our y equals mx plus c for this line.
We have this beautiful predictive model now.
We can say we can predict the crp model.
If you give me the pct from my data set.
I can predict what the crp is going to be.
With this 95% confidence interval.
Nicely drawn in that light blue there for us.
So I'm going to take any pct you give me.
I'm going to multiply it by 10.
And I'm going to subtract 4 from that.
And that's going to give me a crp value.
So beautiful model there.
And now you can see depending on which one you made crp.
And which one you made pct.
Because I can deconstruct this algebraically.
And get pct all on its own.
Or I could have just changed the two values around here.
And that would have given me another model to work with.
Another model to work with.
Let's look at this r underscore value computer variable that I got back.
That's called the Pearson correlation coefficient.
And that's a number between negative one and one.
And that tells us how well correlated these two are.
With negative one meaning a perfect negative correlation.
So the slope would have been negative.
But all the dots fall exactly on that line.
And positive one irrespective of the slope.
All the dots fall on it.
And there's a perfect positive correlation.
And everything in between.
So in this example we had a correlation coefficient.
Oh, if only I could spell.
See?
You just double click in there.
Co-efficient.
There we go.
Would that work?
What?
Not yet.
What?
A spelling mistake to make.
There we go.
See how easy this is to correct?
A Pearson correlation coefficient of about 0.97.
This means the variables are very well correlated indeed.
And there's a positive correlation.
As one increases the other one increases.
We saw our p-value there.
One very important thing is that remember the dictum.
Correlation does not mean causation.
Just because the summing is so well correlated.
Or two variables are so well correlated.
You cannot say that one caused the other.
It is not proof of causation.
The rise in PCT does not cause physiologically the rise in CRP.
Okay?
You have to think about the mechanics of what really happens inside of your test tube or
inside of your test subject.
Let's use our appendix data.
So let's import again.
We know how to do that.
And just checking that everything is imported okay.
No problems there.
Let's make data set two by dropping the NAs.
So take data.
Drop NAs from the temperature and white cell count.
I want to correlate temperature and white cell count.
And in one cell I'm going to go on to just doing this reg plot.
Now I'll show you how complicated the reg plot can be.
Look how many arguments it takes.
Fortunately, and there's even more.
I didn't even put them all in.
There's a lot of them.
But remember from the one up top we want the X value, the Y value, and we want confidence
intervals and bootstrap values so that we can have a nice graph that we can send for publication.
Let's look at that.
Whoa!
Whoa!
What does that look like?
So this is increase in white cell count on the X axis.
So put white cell count first and then increase in temperature.
I could have done it the other way around, of course.
Probably would have made a bit more sense to do it the other way around.
Let's run our slope, intercept, R value, P value, standard error.
And let's see what these are.
Let's see what these are.
So my slope was 0.04.
So a very slight rise in white cell count is going to give me a raise in temperature.
The R value is very low, 0.16.
Still a significant P value though.
There's still a correlation between temperature and white cell count.
That's very good.
And then, of course, just the standard error.
So I can write the Y equals MX plus C little model again and you can give me a white cell count
and I can predict what the temperature is going to be.
As I say, you could have also done it the other way around.
There you have it, linear regression.

Now in my series on doing deep learning specifically for people interested in healthcare related problems,
but for anyone who really is interested in deep learning and really struggling just to get to grips with what this is all about,
what the mathematics is all about, I just want to make this video, I'm going to make another one as well,
this one is just about, let's just call it a video about tensors.
Now with tensor, we're going to build our framework of deep learning,
or we're going to build our deep learning models on the framework called tensorflow from Google.
So what is this tensor? Now tensors come in ranks and you actually know quite a few tensors.
So let's just start off with a rank zero tensor, a rank zero tensor.
And that's nothing other than what we call a scalar.
And a scalar, S-C-A-L-A-R scalar, that's nothing other than a number like four or seven or minus one.
Those are all numbers. That's a rank zero tensor.
Now we move on to a rank one tensor.
And a rank one tensor is nothing other than a vector.
Now let's think about vectors just for a minute.
Vectors can represent many things.
One way to visualize this representation of a vector is to think of a plane.
Most importantly, we have these two axes and they are orthogonal.
In other words, they're at a right angle to each other.
And that can represent two dimensions.
And I can have a point in two dimensions and I can represent that by this arrow with its base here and its head there representing a vector.
And that vector in these two dimensions will have a very specific, you can say, an address for instance.
And that would be this x-axis value and this y-axis value.
In other words, this vector, let's call this vector v, I put a little line under it to demonstrate that it's a vector.
It has these values for x and y and there's different ways that we can write this.
For instance, I can write vector in this format, an x value and a y value.
And this is called a column vector.
And you know from spreadsheets, this is a column and those are rows.
So this column, this column vector will have a size, it has a size of two rows times one column.
It's a two times one, always row first, then column.
So it's a two row, one column vector.
Well, we'll call that a matrix.
And we'll get to that now.
But a column vector has size two, one.
We could also just, what we call transpose this.
And if we transpose this, it becomes x and y.
And that we call a row vector.
Just for you to become slightly familiar with these terms, that will be one row and two columns wide.
And we say we transpose, we just turn the rows into columns and the columns into rows.
That's not what it's about.
We're representing a vector as this point in here.
It's two-dimensional space.
So a vector here, a rank one tensor, will be a vector.
And that is a vector.
And a vector, depending on how many elements there are, can represent points in space.
If it's one-dimensional space, that will just be a point on a line.
That's two-dimensional space.
In three-dimensional space, which we usually draw like this, that's orthogonal.
So, for instance, that's on the floor.
That's standing up.
So we usually make that the z-axis, the x-axis, and the y-axis by convention.
And any point in space here will have these values here and this value there.
And remember, it stands out.
It stands, it can be in the y-z plane there, but it can also stand out.
In other words, it will be some point that stands up inside of the three-dimensional space.
And it'll have a z-component two here, making it a three-by-one column vector.
And depending on how many elements there are, we can represent multidimensional space
much more than we can just perceive in our three-dimensional world by just adding more of those.
Then we get a rank 2 tensor.
A rank 2 tensor.
And that's actually just a matrix.
And that's a very interesting thing.
We've seen matrices before.
Just think of all the rows and columns in a spreadsheet.
So I can have values 3, 2, minus 4, 1, 6, 7, 2, minus 3, 4.
And I can put it like this.
And that is a matrix.
A matrix here of three columns.
So I can see this as three column vectors combined.
There's many ways these matrices are so useful they can present so many things.
Again, I count the rows and columns, which makes this a three-by-three matrix.
And then we get a rank 3 tensor.
Now it becomes really interesting because all I do now is I add layers behind this one.
So I would have this representing here.
And then behind it, another one.
And behind that, another one.
This will make this, if that was three-by-three as well, a three-by-three-by-three.
Rank two tensors is a matrix.
And the rank three tensor is where I have these multiple layers of these matrices behind each other.
And so I can go on to rank n tensors.
And it becomes very complicated.
But you can just add in more dimensions more spaces.
What I don't want you to get confused with, though, is how many elements here are gives me,
how many dimensions in space that is different from what we have here.
Rank two tensor, rank one tensor is a vector that can live in multidimensional space.
That doesn't mean that's one-dimensional.
This is just the rank of the tensors.
And that is what we're going to work with.
Our data is going to be represented in the way that we manipulate values,
the way that a deep learning network learns these parameters that we're going to look at.
And they are all going to be inside of some other rank of a tensor.
I'm going to clean the board and we're just going to do a few what we call linear algebra operations
on specifically these two, rank one and rank two tensors,
just so that you become familiar with it.
Now, the first thing we're going to look at is the systems of linear equations.
Let's put that here, systems of linear equations, not strictly necessary for deep learning,
but it helps us to understand some of the concepts that we are going to deal with.
Systems of linear equations.
Now, linear equation, remember those of the three simple algebraic things.
Let's construct our own one.
Let's make it, let's make something like one and four.
Let's put in, let's put in minus two and one there.
And let's make this three and three.
And so that's a plus.
And we make this, let's make that, let's make that two and two and three and three.
Let's see where that ends up.
Let's say that is two.
So that's going to be two minus six.
That equals negative four.
And we're going to make this that is eight plus three.
And that's 11.
So I'm just cheating here.
I'm creating my own system of linear equations.
Now, let's imagine that these, that we know the answer for, we know that's two and that's three.
Let's change those into two unknowns.
We'll call that unknown X and unknown Y.
So what we have there is one X minus two Y, and that is minus four.
And we have here four X plus Y, and that's going to equal 11.
And that's what I mean by a system of linear equations, a system, because there's more than one.
And I can get a solution for both of these.
And that solution, it will satisfy both of those.
It will satisfy both of those.
Now, how can we solve that, that we get values for X and Y that solve both of these?
By the way, this is linear because I don't have X squared or X times Y.
It is some constant multiple of X and some constant multiple of Y.
In this instance, four times X.
In this instance, negative two times Y.
It's just constant multiples of that.
And I know if I plug two in there, and if I plug three in there, it is correct.
And if I plug two in here, and I plug three in there, it is correct.
So let's try and solve this.
What are things we could do?
Well, one of the first things we could do, we could just swap these two around.
Let's have that.
Four X plus Y is 11.
And I have X minus two Y equals minus four.
So I've swapped these two around.
That's one thing I could do.
I could multiply this throughout by a constant.
So let's still have four X plus Y equals 11.
And here I'm going to multiply this throughout by negative four.
So that gives me, you know, if I multiply it throughout, I'm not changing it.
I'm multiplying this side by negative four and that side by negative four.
Nothing has changed.
So I'm going to have negative four X.
I'm going to have positive eight Y.
And I'm going to have 16.
So here I swapped out.
Here I have a constant multiple of one.
And now what can we do?
Remember, these are equations, meaning this side equals that side.
So if I do something to this side, I must do exactly the same thing to that side.
So let's do something to that.
I'll stay with four X plus Y equals 11 in my first row.
But in my second, this second bit, I'm going to add this to that.
But this is also equal to this.
So I can also, I can do this to that.
That still means I did the same thing to either side because these two things are equal.
So if I were to do that, if I were to add this to that, that's going to cancel out.
And this is going to become, so that makes it 0 X.
And this makes it 9 Y.
And on this side, we're going to have 27.
And I have these now.
Let's carry them on here.
So I have 4 X plus Y equals 11.
And I have this.
But I can simplify this one by just dividing throughout by 9.
0 divided by 9 is still going to be 0 X.
9 divided by 9 is going to give me Y.
And 27 divided by 9 gives me 3.
9 times 3 is 27.
And lo and behold, I have a solution for Y.
And we know we're right because we cheated in the beginning and we constructed it that way.
So that gives us a solution for Y, which I can now plug into that.
So I have 4 X.
And I know Y equals 3.
So 4 X plus 3 equals 11.
So 4 X equals 11 minus 3, which is 8.
And X equals 2.
Beautiful.
But look at what we've done.
There's three very elementary things.
We call those elementary row operations that we did.
We swapped things around.
We multiplied throughout by constants.
Here we multiplied throughout by 1 over 9.
And we added this constant multiple of one row to another one.
Remember, these two things are exactly the same.
That's why there's an equation sign.
So if I took this side and I added that, then I can take this side and add to that.
Then I've done both things, the similar thing on both sides of the equation sign,
because these two things are equal to each other.
So let's clean the board.
And let me just show you if we use rank 2 tensors, how we can also do that.
So let's use this and write this as what we call an augmented matrix.
So an augmented rank 2 tensor.
So I'm going to just have X there, remember, and minus 2Y.
Let's write just the coefficients.
So I'm going to have 1 and a minus 2, and that side a minus 4.
And on this side, I'm going to have a 4, and I'm going to have a 1, and I'm going to have 11.
And that is an augmented matrix.
So I'm just shorthand.
I'm leaving out the X, and I'm leaving out the X, and I'm leaving out the Y, and I'm leaving out the Y.
What can we do?
One of the elementary row operations, we just swap these around for 1, 11.
Technically, I didn't have to do this, but I just wanted to show you the R3.
And I'm going to write this one down below, but I'm also going to multiply it out by a constant,
which was negative 4, which was 16.
And it's just shorthand for what we did before.
Those are the elementary row operations.
So I can stick to the first one.
And for the second one, I can add the first one, elements of the first one, to the second one.
So 4 plus negative 4 is 0.
1 plus 8 is 9.
And that gives me 27.
I can multiply this throughout by 1 over 9, which gives me 4, 1, 11, 0, 1, 3.
And that's how we got to read off the Y value.
Remember, this is X, this is Y.
So 1Y equals 3.
Y equals 3.
Now I can multiply this throughout by negative 1.
So that will give me 0, negative 1, 3.
And now I can add this to the first one.
4 plus 0 is 4.
1 plus minus 1 is 0.
And 11 minus 3 is 8.
And I can multiply through this throughout by negative 1 again.
So I'm back with 0, 0, 1, 3.
And then I can multiply this first one through up by a quarter.
That will give me 1, 0, 2, 0, 1, 3.
And there's my solution.
It is in what we call row-reduced echelon form.
In other words, there's just leading 1s.
And below and above every leading 1, there is 0.
So that I can finally get my solution.
1x equals 2 and 1y equals 3.
Exactly what we have there.
So elementary row operations.
So they have shown you the system of linear equations.
Let's just get back to some simple things.
Let's just add rank 1 tensors.
Rank 1 tensors.
So to add rank 1 tensors, they must just be in a similar dimension.
So I can have a three-dimensional rank 1 tensor.
4, 3, minus 1.
And I can add to that a second one.
Let's have a 0, 1, 4.
And that's very simple.
It's element-wise operations.
So it's 3.
4 plus 1 is 5.
Negative 1 plus 4 is 3.
And there I have my solution.
Very easy to do rank 1 tensor, which are vectors, vector addition.
They must just have similar dimensions.
Now let's multiply, let's multiply a matrix times a vector.
Now there's various reasons why you would do that.
We can call this an operation on a vector.
If we look at physics, it doesn't matter what we do.
We just want to do.
And I want to show you that there's just something that must be kept in mind.
And this is very important when we get to tensor flow and when we get to designing or thinking
about at least our deep neural networks.
If I have a matrix 3, 1, and let's make it 2, 2.
So what size is that?
That's 2 by 2 matrix.
2 by 2 rank 2 tensor.
And I'm going to multiply that by a vector 2, 1.
And I'm going to put that down there, and that is 2, 1.
The reason why I put that is because it makes the actual doing it on paper very easy.
But what I'm doing here is I'm multiplying a matrix by a vector.
Very important that this second value, the column number, and the row number of that one,
they must be equal.
If they're not equal, you cannot do that multiplication.
These don't matter.
It doesn't matter what these two values are.
But if I have this matrix, and let's call this matrix A, and we call that vector V.
If I'm doing A times V, the matrix comes first.
The vector can technically also come first, but there's something else about that.
But the second number here must equal the first number there.
So the column number of this one and the row number of this one must be equal.
Otherwise, you cannot.
And the solution will be what is left.
The solution will be a 2 times 1 tensor.
And, you know, depending on what rank it is, all these things, from a single number
till all these multidimensional things, here at the bottom, they're all tensors.
So let's just call everything a tensor.
So the result is going to be a 2 by 1 tensor.
And what we do is very simply, I've written it like this because it makes it simple.
I'm going to look for this space.
We've already seen it's going to be 2 by 1.
So those are the two values.
And it's very simple because I can look along this one and along that one to get to this one.
So it's going to be 3 times 2 is 6, plus 1 times 1 is 1, 6 plus 1 is 7.
And 2 times 2 is 4, 2 times 1 is 2, 4 plus 2 is 6.
And there's my solution, 7, 6.
That is a matrix times a vector.
Very important and very important that you understand that that number and that number
must be exactly the same.
Otherwise, you cannot do this multiplication.
And that is going to be a very important first step in a neural network.
By the way, if it sounds like the world is coming down, right outside my office,
they're building a new neuroscience center.
And it is very, very noisy and really driving me nuts.
So a very important thing, this matrix times a vector.
I can also have two matrices, like to multiply them with each other.
And as long as, say for instance, this one is of size 4 by 5,
that means this one must be size 5 by whatever it makes that 7.
It doesn't matter.
Those two must be the same in that order.
The result will be a 4 by 7 tensor.
The result will be a 4 by 7 tensor.
And as I say, those are very important things.
So that's a brief recap of what we call linear algebra.
There's some very basic concepts in linear algebra.
And we know that this is very important for us in designing neural networks.
Now, you don't have to know more than that.
If we write the lines of code using TensorFlow or Keras,
as long as you have these basic concepts in the back of your head,
you really don't need this full understanding of this.
Just in case you have not done this before, you did it a long time ago,
just a little bit of a refresher of what these things are all about.
I'm going to make another short video just on partial derivatives
because it is linear algebra is the one important part of deep learning
and the other part is just derivatives.
We've got to be able to do derivatives once again.
We're just going to write a line of code and the derivatives are going to happen automatically.
But I think it's still important that you just have some basic concept
of what happens with linear algebra, specifically this,
and what derivatives are.
OK, thank you.
.

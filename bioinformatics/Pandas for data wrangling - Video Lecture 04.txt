Now we come to a very exciting lecture and we're going to see how to import data into Python so
that we can work with that. We're going to work with a very famous and successful package called
Pandas. Now Pandas is really one of the big success stories of Python and one of the main
reasons why it's become so popular as a language for data science. So in notebook number four here,
importing and manipulating data, one of the most important tasks that we have in data science.
So I've already mentioned this package called Pandas and it really is for any kind of tabular
data. So think of a spreadsheet. You've got a spreadsheet file and we're going to import that
spreadsheet. Even if you store your data in a database such as RedCap or any kind of SQL SQL
database, you can export that as a flat file. In other words rows and columns of data as in a
spreadsheet and we can import that. Of course with Python you can also import directly from a
database. But what we're going to do here is stick with the basics. So we're going to import data from
just a spreadsheet file. So we're going to start off with the packages for this notebook and you can
see there we're going to import Pandas with that namespace abbreviation PD. And as I said you can use
anything you want other than PD but it's just the industry standard. Everyone just does PD. And then for
numerical Python NumPy we use the namespace abbreviation PD. So that imports all the
functionality of these two massive and very important Python packages into this environment
of ours that we're working with at the moment so that we can we can make use of that. So specifically
to Google Colab if you have your data files stored in your Google Drive you'll have to import this
function from google.colab. So Google itself is a package. It has a module called Colab. So we say
from google.colab we're going to import the drive function specifically because what we want to do
is to maintain security. We only want to connect to our own Google Drive and we want to keep that
secure. So this notebook as it stands it's not a hundred percent secure so we have to log in,
re-log in and give this notebook instance, this notebook that's running now, permission to enter
our Google Drive and read some of the files there. So it keeps things very nice and secure. So we've got
to import that function as well. And also very specific to Google Colab is this data table. So
google.colab.datatable and we use this magic command. So anything in Python that you see start with this
percent. There's just a couple of them. So percent load underscore ext and all that's going to do is
just reproduce a table very nicely inside of Google Colab. If you don't you don't have to do that and if
you use Jupyter Notebooks on your local system there's another way to do that just to have the the tables
print very nicely to the screen. That's all this is about and it's really not not hundred percent
necessary but I like to do that because sometimes I like to work with those tables right inside of my
browser here. So now we have to import our data. Now our data lives in our Google Drive. It's a csv
file comma separated values file. So I just want to spend a moment or two just talking about that. A
comma separated values file. If you're in any kind of spreadsheet whether you are in Google Sheets, whether
you are in Microsoft Excel, when you say save as you can save it not in the proprietary format. For
instance Microsoft Excel would have SLSX as a file extension. That adds a lot of extra information to
the file. You know some people color there, add colors to their their spreadsheet and format it in some way.
Print out values so that it prints out as a percentage to the screen or add monetary values to that.
Underlying that data that formatting is the real actual data. And when you export a file as a csv file
it strips away all that extraneous information which we really don't need. We are just after the raw data.
So always save your files as csv files. It's also universally readable. So no matter what software
someone uses they can still read those files. First off remember that file that function we imported drive.
So it has a mount method. So drive dot mount. And now we're going to mount our g drive. And internally
on Google servers that's forward slash g drive. And I always add this little extra argument. So remember
arguments are these things that we pass to a function or a method. So in this instance the mount method
we're going to pass all these arguments. These two arguments we separate the arguments by commas.
And the second argument is force underscore remount. And I'm setting its value. I'm assigning to it
the value true. And that just means if I run this notebook a little bit later and it's already
connected. I don't know if it's still connected. It just forces the remounting of or mount means access
to that drive. So we're going to run that and that is going to provide us with ability now to connect
to our google drive. And this is what it looks like. It says go to this url in your browse and you're
going to click on that. And that's going to open a second tab. And on that second tab it's going to
allow you to re-access your accounts. You have to sign into your google to your g drive all over again.
And that's really just for safety purposes. So it's going to show you a couple of pages actually.
There's going to be a tab as well that's going to show you all the access that it gains from you
logging into your drive again. So this notebook is going to have all sorts of access to your google
drive. And that's all fine. It's very secure. You need not worry about that. And what we're going to
do then is give it the permission. And then a tab is going to open up with a secret key. And you can
see there enter your authorization code. So on that page it's going to be a very nice little button. So
you don't have to highlight the whole key because it's quite long. You can just click on that button
that's going to be the same as copy. And you can come here and paste it in here. So just click and then
hit ctrl v or command v and that'll copy this authorization code. You're going to hit enter. So I'm
going to do all of that. I'm not going to show you on the screen. So we're going to do all of that.
I'm going to copy and paste that in here and just hit enter or return. And that's going to give this
notebook access to my g drive. Now one thing I just do want to mention here, of course, depending on
where you work in the world, it might be a bit difficult to store sensitive information about people
on Google Drive. We might have laws that prevent us from storing human information outside of the
borders of a country. And for security reasons, we don't want to save data in this way. So make very
sure about how you save your data if it pertains to human beings. And make sure that you can save it
on a Google Drive. Now Google also has some features that you can pay for. So you can pay for your Google
Drive to get extra functionality and extra space. And that's more secure, of course. And so please
find out about the laws before you enter any human data on your Google Drive. Of course, this is a
course just to show you how Python works, what data science is all about. And none of the data that we
have in this course pertains to real human beings. So not a problem for us in the course. So I'm going to
click on this and it's going to take me to a new sign sign in page. I'm just going to re-sign into
this very same Google Drive that I'm using. And there you go. I've copied and pasted into that cell
and just hit enter or return. And now my mount, it says mounted at G Drive. So I've got access to my
Google Drive now. Now if you scroll down to the next section here, what I'm doing here is using a
terminal command. So on your computer, you'll have a terminal or a command prompt. And that's exactly
what we're using here. And what we're doing is we are just changing the directory to the folder or
directory on the hard drive. Now that won't be our hard drive. It's up in the cloud where this notebook
and where our data is stored. And I'm going to use, as you can see, quotation marks here because this is a
string. So you can see it starts with a forward slash g drive. That's the home folder. And then
my space drive. And then I'm typing this whole folder structure. So I'm just going to take you
to a different tab so that you can see where we get all of this. So this is my Google Drive. And you
can see all the notebooks here that we're going to work in. And that's in a folder structure. So on my
Google Drive, it's under my drive. Then I've got Stellenbosch University as a subfolder or subdirectory.
Inside of that, I have another folder or subdirectory, School for Data Science and Computational
Thinking. Inside of that, I have another subfolder or subdirectory, Data Science. And that's where all
my notebooks live for this course. Inside of this, though, I also have other folders. Remember, you can
always say new and then folder. And then, of course, by the way, write down there more. If I go
down Google co-laboratory, if you want to start a new notebook, I also have a data folder. And
inside of this data folder is where all my CSV files are stored. So I keep things very nice and neat. But
it's this whole address here. My Drive, Stellenbosch University, School of Data Science and Computational
Thinking, Data Science, Data. That's the whole folder structure to get to that file. And that's exactly
what we're going to do. So I'm seeing %cd. So that's a magic command. Change directory. And I'm
typing all of that in G Drive, My Drive, Stellenbosch University, School for Data Science and Computational
Thinking, Data Science, Data. So my CSV file is there. So I'm just telling this notebook,
change internally to that directory. And now what we can do is use the %ls command. And that's going to
show us all the files. It's going to give us a list of all the files inside of this directory,
which we've changed into. So think about just your explorer on your normal computer. You can
click around and go to different folders or directories on your hard drive. This is exactly
what we're doing here. So I'm not going to run that ls. But if you do, it'll list all the files.
What I want to do is I know the file that I'm after. It's called data.csv. So there we have the csv,
comma separated value spreadsheet file. And the function that's going to import that for us is the
read underscore csv file. And that comes from the pandas package. So pd, because we use that namespace
abbreviation. So pd dot, one of its functions is read underscore csv. As argument, inside of quotation
marks, because we use strings for file names. So it's data.csv is the name of my file. And by the way,
I mean, you can just drag a file from your internal drive onto your Google Drive and it'll upload and it'll
live there. So I'm going to run this cell. And by the way, look there, I'm assigning it to a computer
variable called df. Remember the assignment operator? There we go, the equal symbol. Whatever
it's to its right, it assigns to what is to its left. So it's going to read this data file for us
and import it and save it in the computer memory with this name of that little space of memory called df.
So now that is done, I can look at the type. Remember the type function is going to tell me the type of
of this object, this df object that we've now stored in memory. And we can see it is a pandas type object,
pandas.core.frame.dataframe. So it's a data frame object. And that's what pandas does for us. It creates
this data frame object that we can now manipulate. Now, as far as this data frame object df is concerned,
it has many, many, many methods. So we can say df dot and then a method. And the way that you can see
all the methods, not something that we do all the time, but if you want to learn about all the methods
that are available to you with your df now, in other words, what can you do with this data frame object,
just pass it to the dir or directory function. And if you do that, it'll give us this long list of
things that we can do with this object. I can say df dot f, f, i, l, l, that's forward full, or eval, or empty,
or d types, or drop na. And there's a long massive list. And that's what makes pandas so powerful,
because there's so much we can do with our data. We can manipulate the data that we have so much.
So one of the first methods is the head method. And I'm not even passing any argument to it,
because it does have a default argument. And that's five. So df dot head, that's going to show us the
first five rows of our data frame. And we always do this, because we just want to make sure that
our data object imported properly. And there we see our first csv file that we've imported. So at the top
in the spreadsheet, that will be the first row of these header columns. And we see that this is what
we call tidy data. Now it's very important, as much as you can, work with tidy data. Now the tidy data
really comes from the R world. Hadley Wickham described this in a lovely paper. And it's all about
every row in your data set pertains to an observation. And that observation might be
something in your chemistry lab. It might be something in your biology lab. It might be something
about economics. No matter what kind of science you're working with, as long as each row pertains to
a single observation. And all the columns are very neat variables. So you've read up about variables,
the specific data types. So in down that column, that data type is fixed. So on the age, those would
be integers. And we don't put anything else in there. We don't put words in there. You know, we don't put
any other kind of data type in there. That is a continuous numerical variable inside of age. And
that's all that goes inside of vocation. We have nominal categorical data. So it's all the same. And
this is called tidy data. So each row is an observation or a subject in our data set. And each column pertains
to a very specific variable. And we don't put variables together. Many times you'll see a data
set where there's lots of commas or ands, almost free entry of data inside of a cell. And there might be
a variable that has lots of potential entries that you could make. What you have to do is all those have
to be separate entries. And that column can just have in zero ones or yeses and nos in it. So it must
be very clear what the data type is of that. And it is a single data point value that goes into each
cell. And by the way, because we've used that percent load underscore ext, this is the printout in
Google Colab that I was talking about. Because look how nice it is. I can say how many
values I want, how many observations per page. Now we've used the head function, so it's only going
to show me five. But I can really work with this table and look around with the data quite nicely.
So I like to use that magic command. So the .head method is what we're going to use just to make
sure that our data imported correctly. The next thing I want to use is the shape attribute. Now remember
an attribute or property that is not a function or method. So it doesn't have parentheses after it.
So it's just df.shape. And what the df.shape is going to do for a flat file is going to tell me how
many rows they are and how many columns. So the rows are how many subjects I have in my data set or
observations. That's the number of rows. And the columns are my number of variables. So we can immediately
see with the shape attribute or the shape property that we have 200 rows and 13 columns. 200 rows and
13 columns. Sometimes we also want to just have a look at all the variables that were collected. And
for that we use the columns attribute. So df.columns and it's going to give me this index of columns as
a list object. And we see name, dob, age, vocation, smoke, hr, sbp, etc. Now there's also a ndim number
of dimensions. And we have two dimensions, row and column. So it's a two-dimensional data set along two
axes. In other words, there is the size attribute as well. And that's just going to show me how many data
points I have. So it's actually going to be rows times columns. 2600. The dtypes, or data types,
is very important. It's a very important attribute. And it's going to show me all the variables that we
have. And it's going to show us what python thinks or pandas thinks that data type is. So name, it thinks
it's an object. An object is just a keyword for a string. And strings are going to be nominal categorical
variables. Age, it sees as an int64. So that's a 64-bit integer. Now the 64-bit, that just gives us
what is the maximum and the minimum, the highest and the lowest value it can save as 64-bit integers,
which is actually quite huge numbers. So it's still an integer. Vocation, again, an object or a
categorical variable. Smoke is a bit difficult because it saw that as a 64-bit int. So let's go,
just go back up just a little bit, and you see there's smoke. Well, these first ones were all zero.
We can well imagine that whoever created this data set had in mind that zero means no smoking,
and one means smoking, and two means smoke before, for instance. So that's encoded as a number. But that
very clearly is not a number. I cannot say what is the mean smoke value, because that'll come out as
perhaps 1.25. No, it has no meaning. So that is a categorical variable encoded with numbers. So that
might cause a few problems. And we have to keep that in mind. That is definitely not a numerical
variable. That is a categorical variable. So we really have to watch out for that kind of problem
in a data set. We really have to be concerned about how these things were saved. Then the float,
remember float is decimal point values, and it's also 64-bit float, which gives us a lot of decimal
places. 64-bit will also be 32-bit, 16-bit, 8-bit. And that just means less and less and less
values after the decimal place that this kind of data can keep. Next, though, we're going to talk about
extracting some of the rows and the columns, because that's what this data set, this data frame
object in pandas is all about, is for us to manipulate the data. So the first way that we can
refer to just one of the columns, we might be interested in just one of the columns. And this
is how we would do that. We use square brackets. Well, there's actually a couple of ways, and we'll get
to that. DF, and then a set of square brackets, denoting that we're working here with type of indexing.
And then inside of quotation marks, I'm just passing the name of that column that I'm interested
in. So DF, square brackets, age. And I'm going to assign that to a computer variable that I'm going
to call age underscore column. So I'm just going to extract that single column, and I'm going to
assign it to a new computer variable. Let's see what this new computer variable of this, the single
column inside of a data frame, what kind of type it is. And we see it's a series object. So in pandas
you get a data frame object, and you get a series object. If it's just a single column with an index
still, on the left hand side you'll see zero, one, two, three, four. Remember Python is zero index, so the
first row will be row zero. That's just a single column. So that's one way to do it. There's also a
shorthand notation where we don't use the square brackets. We just say DF dot and then the name of
the variable. We can only do that though if we don't have any illegal characters in that column name.
And spaces are the big, big problem. Either spaces between words or an invisible space after
a word. We can't have that. So if our column variables are nice and neat, and always try and
do that, don't be overly verbose and have a long sentence there as your column header. And definitely
don't use illegal characters such as spaces. If we have this, we can just use the dot notation. So DF dot age.
And that's going to give us exactly the same thing. So by the way, we can, because it's a series object,
this is like a data frame. We can use the head method. So dot head, and that's going to give us
the first five values now in that age column. It is a series, and that's why I said a series will also
still have the index on the left hand side. The first row, the second row, the third row, up till
the fifth row. Python zero index is starting at zero, starts counting at zero. Now we might want to,
in some instances, not have a column that we extract like this to be a series object.
We just want it to be a numpy array. Remember those? For that, we have the to underscore numpy method.
So I can say div dot age, which is going to extract that series for me, just that single column.
And then we have the dot to numpy, dot to numpy method there. I don't have to pass any arguments to
that. And I'm going to assign that to a computer variable called age. So instead of a panda series,
I now have a numpy array. So just make sure that is true. I'm passing it to the type,
and I see it's a numpy dot nd array. And that's exactly what we expected it to be. So it's not a
panda series anymore. It's not going to have, it is not going to have a little index down the left hand
side. Each value in a numpy array still has an index, but it's not going to be printed down the side.
So we can also look at all the many, many, many, many methods that are available to numpy arrays.
So you can see they're passing it to the dir function quite a bit.
Here's a couple of them. The minimum, the min method. So I'm just saying age dot min,
and I'm going to get the minimum age in that numpy array. Dot max gives me the maximum age. Dot mean
is going to give me the mean of that array of values, 53.07.
So that is how we extract first the series and how we convert it to a numpy array.
Now you don't always have to do that. I just want to show you what is possible and how you have to
think about the data type that you are working with as far as what, what these objects are in Python.
So next up, I'm going to show you two very important methods, and that's the ilock and the lock method.
So ilock is integer location. And after that always comes square brackets because we are dealing with,
with, with the indexing here. So df, my data frame object dot ilock and zero.
So let's see what that does. Integer location. The zero means the zeroth observation. So the zeroth row.
And what that's going to return for me is all my column headers there, the variable names.
And then for that first observation, that first row of data, the zeroth row is going to show me all the
values that are inside of that first row. So if you know exactly which one you're going for,
you might be interested in your lab and a very specific data entry, you can just use this ilock and
pass a single value to it and it's going to give you that row of data. You might not be only interested
in a certain row. You might have a couple of rows that you are interested in. Say row 3, 4 and 6.
Remember that would be index 2, 3 and 5. Python being zero indexed. And I just pass that as a list
object inside of the ilock square brackets. So note there that there's a Python list inside of the
outer set of square brackets. So d of dot ilock. And I want rows 3, 4 and 6. In other words, index 2,
3 and 5. And now it's going to just return for me those specific rows. You see the index 2, the index 3,
and the index 5. So now I can see in a nice column, I can see only those rows that I was interested in.
So what if I know that the rows, the subjects, the observations that I'm interested in are actually
sequential? For that we can use that range object, remember, with a colon in between,
that jumps in increments of 1. So it's going to be 0, 1 and 2. Just remember the last value is not
included. So it's only actually going to be the row 0 and row 1. So if I know that they're contiguous
in this fashion, I can just use that range object with a colon. And now again it's showing me the first
row with index 0, the second row with an index 1, but it doesn't show me 2. That 2 is excluded.
So that's just the rows. But what if I'm interested in not getting back all my columns? Well, this is
one of the powers now of indexing. It's always row comma column. Because see there, we're bringing in
a little comma there. And if you see a comma inside of ilock or lock, that means row comma column. Always
in that order. So what am I asking for here? I'm asking for rows with an index 0, 1, 2, 3, 4. Remember,
the 5 will be excluded, comma. And then I'm passing a Python list for the columns. I want
the columns with index 1 and index 2. Now even the columns are 0 index. So the first column
is going to be the 0th column. So I'm actually asking here for column 2 and column 3. But this
means I should know what those columns are, because we're not using their names. We're just using their
index. So let's see if Panis did the right thing, if it did what we wanted it to do. And there we go,
0 to 4. So 0, 1, 2, 3, 4. And if we look at name, that was my first column. Remember,
the index doesn't count as a column. The name was my first column. So that's the 0th column.
That's not what we asked for. We asked for index 1 and 2. So 0, 1, 2. 1 and 2 will be DOB and age,
date of birth and age. And that's exactly what we got back. DOB and age. So that's integer location,
which is sometimes useful as far as the columns are concerned. But the just .loc, that property,
is actually much more useful, because there we can actually use the column names. So here I'm asking
for exactly the same thing as we did with our previous iLoc. I'm saying df.loc, or location,
0 to 5 as far as the rows are concerned, comma, and then I'm passing as a list the two column names
that I'm interested in. And that's pretty much more useful than the iLoc. And I get back, as you can see,
exactly the same thing. So remember that difference between iLoc and lock. So I'm going to show you
there's also an iAt, integerAt, and I'm just passing the row and the comma, row comma column. So I want the
third row, second column. I want that specific data entry. I can use iAt. And let's see that was 43.
Now we're going to come to one of the most important parts, or the most interesting parts,
at least, of pandas. And that's how we're going to filter some data. We only want to extract
certain observations or subjects from our data set. And that is filtering the data. Now for filtering the
data, we're going to use these conditionals. Remember the greater than or less than that it returns
a true and false value for us. So let's see, is 3 greater than 4? No, it's not. We return a false.
Is 3 equal to equal equal? So we're just checking whether left hand side and right hand side are
equal to each other, and 3 equals 3.0. It's exactly the same value. So true. So remember these conditionals,
we're now going to use them. So first up, if we have a categorical variable, we might want to know the
sample space. Remember the sample space are all the possible values that that variable can take.
So we know beforehand, this is the set of values that that variable can take. And now if we start
entering data, whether that be from human beings, from chemicals, from laboratory, in the laboratory,
some organisms, it doesn't matter what it is. If it's a categorical variable, there's a sample space and
we're going to, any observation that comes in is going to have one of those values. So for continuous numerical
variable, there's going to be an interval from the minimum to the maximum, and every subject will have
a value in that interval. So those are, that's the sample space, the possible values that a variable
can take. So for us, we know the smoke, as human beings, the smoke, we know it's going to be whether
this participant was smoking or not smoking. So if I just want to know the sample space, specifically for
categorical variables, we want to know what the sample space is. For that we have the dot unique,
so the unique method. So df.smoke, remember what that's going to do, that's going to just give me
the smoke, that smoke column as a series. And on that new series, df.smoke, I'm using the unique method.
Don't have to pass any arguments to there. And I can see the values that are down in that column,
and that's why it's so important to have tidy data. I see there are three sample space elements.
No matter what observation, the subject, the row is in my dataset, they will only have values 0, 2, and 1.
And what Pandas is going to do, it goes down that column and it just starts returning it in order
of how it finds it. That's going to be the order that it returns it in. Because this is not numbers,
these are categorical variables, so they mean something to which there's no order really.
So there's actually a nominal categorical variable there. So imagine then, in my dataset, I'm
interested in the ages of only those people who are non-smokers. So by the way, that would be
the participants that was encoded as a 0. So I want to know the ages only of the non-smokers.
How would I go about that? And we're going to use conditionals for that. And there we see the code
for that. So first of all, on the left-hand side, I'm going to create a computer variable
with my own, of my own making, non-smoker-age. So I'm only looking, you know, it's very descriptive,
that name. I'm only looking for the ages of the non-smokers. And there's more than one way to go
about this. I'm going to show you this one. I'm calling df the data frame and then immediately
a set of square brackets. Now, in reality, I think many people would use .iloc here or .loc,
but you don't strictly have to. So I'm just saving a bit of typing here. So I'm just saying df and then
immediately a set of square brackets. What does that mean? Well, if there's a comma in that square
bracket, it's going to be row comma column. So there's no comma in there. So I'm only referring
to rows. As soon as there's a comma, it's going to be rows comma columns. If there's no comma in
there, it's only rows. So it's df and I'm indexing the rows. And which rows do I want pandas to look at?
I want it to look at the df.smoke column. I want it to go down that row. So df.smoke and then equals
equals zero. And you can well imagine what this is going to do. It runs down that smoke column and
only when the entry is zero, which in our case, those are the participants who do not smoke,
it's only going to extract them. And then another set of square brackets. Because now I've said once
you've done that, once you've selected only the people where the smoke, that smoke variable is set
to zero, was captured as a zero, only those are going to return trues. All the others are going to
return false. If it's a one or a two, it's going to return a false and it's going to be ignored. It's
only looking for the true values. Now from those that have smoke equal to zero, please extract the age
column for me. And then export that to a numpy array. So I'm using the dot to underscore numpy method
on that. So that's a whole sentence you've got there. It says take my data frame df, go down the
smoke column and only select those who don't smoke, being encoded as a zero. For those that you've then
selected, please take the ages and convert it to a numpy array. So nice little English sentence there.
And we've extracted that as code. And this is what computational thinking is all about.
So I've saved that and let's just print that out by calling that. And we see it's an array of values.
And it's going to be the ages of only those who in the smoke column have been encoded as a zero.
So that's one way to do it. I'm going to show you a different way where we use
the lock function and we can use rows, columns. So this would be another way to do it. Now it's
confusing in the beginning because there's so many ways to do it, but you can find the way that works
for you. And it really makes it just that much more powerful. So I'm going to say df.lock. So
location. That means when I have a comma in there, after the comma I refer to the columns, I can actually
pass the column name instead of its integer index value. So I'm going to say df.lock,
and then df.smoke equals equals zero. So that's my row value. So it says go to that smoke column,
go down all those rows, only select the ones where it's zero. So only those are going to return a true,
comma, the age column, please. And then again, convert that to a NumPy array for me, please.
So there we go. Exactly the same values, the values of those people who do not smoke. And now that I have
that as a NumPy array, I can just use the dot mean. And that's going to give me the average age
of the non-smokers. It really is as simple as that. And once you get used to it, these things just run
automatically, where you can start having these wonderful research ideas. And you can extract
that specific data that you're looking for from a very large data set. And that's the power
of using a computer language such as Python. So now we're going to get to the different kind of
question. Non-smokers where the survey choice is three. So what are we asking here? We want the ages,
but they're two conditionals. They've both got to be non-smokers and they've got to have a survey
choice of three. So both things have to be true. Both have to be true for us to then include that and
just extract those ages. That's very different from asking the non-smokers or the survey choice of
three. When we have or, either of the two can be true and that observation or subject will still be
included. So in Python, we have this idea that Boolean logic is and or. And for is, we have the
ampersand symbol. You can see there. And for or, we have this pipe, the line up and down. Search for
that one on your keyboard. That's all. So again, if it's and both have to, or you can have more than
just two, but they all have to return true for that row to be included. And the or, any one of my
conditionals can be true. So let's have a look at how we do that. Let's look at the right hand side first.
It's actually quite intuitive. So it's df.lock so that I can use rows comma columns and then columns I
can use the actual column name. And you see there is my little ampersand there. So I'm saying go down the
df.smoke that series, that column where that equals zero and go down df.survey where the values are
three comma, which column am I interested in? I'm interested in the edge column. And eventually I'm
just going to export that as a two numpy. Now, in reality, I almost never convert this to a numpy array,
but to keep things neat here, I'm just, I will keep on doing this as two numpy. And then we have the
assignment operator and we're assigning it to a computer variable of our own choice. And I've called
it non underscore smoker underscore satisfied underscore age. This is a little bit descriptive.
And now that is going to give me the ages of those that have zero for smoking and three for survey.
Both of those sets of things have to be true. And then I'm going to get the ages back of those.
Now a little bit more difficult. Let's look at never smoked or satisfaction score greater than three.
And I'm going to show you a slightly different way because I think you can already know what this
line of code is going to look like. I just want to show you a little bit of a different way to do it.
I'm going to save my, my filtering as a computer variable that I'm just going to call crit. To that,
I'm going to assign what we had before in the comma and before the comma, as far as the lock was
concerned. So the rows, I'm going to say df smoke equals equals zero and df dot survey is greater than
three. But I'm saying, oh, never smoked or satisfaction scores three. Either those two can be true. So
therefore we use this little pipe. So not the ampersand here. And then when I actually do this,
given it a name there, I'm going to say df dot lock. And then instead of writing this criteria,
I've now saved it as a computer variable crit. So I'm just going to say crit comma age,
as simple as that. So if we, if we run this line, so we saved our crit, we do it there. And the reason
why you might want to do this is you want to build up different criteria. You might have criteria one,
criteria two, criteria three, and you just want to rerun this line of code and just enter different
criteria all the time. And this makes them the info a bit more easier to work with if you've saved this
in this way. So you don't have to continuously change this whole line of code, but in certain,
most definitely instead of that crit, you can have typed all of that. No problem. What I want you to
see is the difference between and and all. So let's have a look at this one, non smokers and satisfaction
score three or less and still the age, by the way. So that score three or less, that's actually quite
easy because that's just going to be less than or equal to three, of course. But what about the
non smokers in as much as the ones who are non smoking are not only the zeros, but they are the
twos as well, because the two actually signified they smoke before, but they stopped. They still
non smokers currently. So I need to include the zeros and the ones. Now, of course, I could write
three little sets of, of conditionals. So I can have smoke equals equals zero and smoke equals equals two
and df.survey less than or equal to three. So certainly no problems there. But I just want to
show you there's not only and and all, but there's also a not. And this is what I'm going to do here.
I'm going to choose exactly the opposite of what I want. So I want the smokes of zero and smoke of two,
but I don't want smoke of one. So that's exactly what I'm going to choose. df.smoke equals equals one.
And what do we want for survey? We want three or less, and I'm choosing exactly the opposite,
greater than three. And I want both of them to be true. So I pass the little ampersand there.
And now when I do df.loc, I have this little tilde symbol there, tilde crit. And what that means is
not the criteria. So the exact opposite of the criteria. So I'm actually looking for all the false
ones. So when we have df.smoke equals equals one, that's going to return a two. The falses are going
to be the zeros and twos. That's exactly what I want. The survey of greater than three,
those are all going to be twos, but that's not what I want. I want all the others, all the falses.
And that's why I use that little tilde symbol there. So not the crit. So all the false values.
Then age, and then we're going to return all of that. And that's going to give me the list of
my non-smokers, which have zero and one in it, and my survey scores of three or less,
by choosing exactly the opposite. So sometimes that can really help you if you remember that
you can choose all the false values that get returned down a row. So next one, create a new
data frame object that only contains participants younger than 50. And that's a very common thing,
that we are going to use one or two or three or four large groups in our data and save them as
separate data frames. Why not? What you have to be concerned about though, if these data frames are
very, very large, because as it stands here with pandas, they have to fit into your, into the computer's
memory. So if you just continue to create new ones from old ones, you're going to run out of computer
memory. So it's just something to be cognizant of. But this is a small little data set, no problem here.
So how are we going to do that? With conditionals again, I'm going to say DF.
Now, strictly speaking, I should say DF.ILOC, but you can just drop that in simple cases like this.
I'm going to say, there's no comma there. So it's only the rows, which rows I'm interested in,
the rows in the age column. So DF.age less than 50. And I'm assigning that to a computer variable
new underscore DF. And if we now call that, we're going to see if we look down the age column,
no one is going to be older, 50 or older. So it's less than 50. So 50 is obviously not included.
Now, as you can see here, I didn't even export or change this to a NumPy array just to show you
that you don't have to do that. So I'm just saying new underscore DF dot age. So I can use the dot
notation here for that column because age doesn't have any legal characters in it. And then dot max.
So there's a max method, just as there was for NumPy array. And now if we see what the max
age is, it was 49. So definitely only extracted into this new data frame of mine,
only those participants that were younger than 50. And remember the alternative, if you do have spaces
in there, just put it inside of quotation marks, inside of a set of square brackets,
it's going to do exactly the same thing for us. Just to show you another way to do it. There's so
many ways to go about this. I'm saying new underscore DF dot lock. So location, so I can use row comma
columns. So the colon symbol on its own there, it just means all the rows. Remember all the rows. So
all the rows comma the age column and give me the max of that. And I'm also going to get 49. So really
very powerful because there's so many ways you can go about this. So don't be intimidated by the
fact that there's so many ways to go about it. Because when you sit down and you want to try it
yourself, the sort of all these ways that you can do it in your head and you're not quite sure
which one is which one, it comes with time and experience. And you're just going to find the way
that works for you. So next up, create a new data frame for participants with a restricted list of job
titles. How would we do that? So for instance, if we go down that vocation column, there's lots of
vocations in there. I'm only interested in the participants that were either IT consultants,
energy managers, or clinical embryologists. So I'm going to save that as a Python list,
sign that to the computer variable jobs, and then I'm going to use the isIn method. And I'm going to set
that as a as a computer variable. I'm going to sign that to this computer variable crit. So I'm saying
df.vocation isIn. So that whatever you pass to that, it's now only going to return true values
if that specific subject is one of those three in that Python list. So I'm actually passing a Python
list jobs to that isIn method. So that's my criteria. And now I can create a new data frame,
jobs underscore df, and I assign to that df.loc crit. So it's going to use this crit. So what is there,
I could have just typed that in there. And instead of that, I could have just typed all of that. So
I could have had one long line of code, but I just break it up like this. And that's quite a common
thing to do. And if we now look at jobs underscore df, a new data frame, if we look at location,
they're only going to have energy manager, IT consultant, clinical embryologist, energy manager.
So it's only going to select those. So if you have a column where you have these words in them,
as far as the sample space elements are concerned for that variable, and you're only looking for
certain ones of those, use the isIn method. Very easy and powerful to use. Now you might want to get
a slightly more granular than that, slightly more granular, that in a word like that,
I might only be interested in a single word in that tick, in that string. And for that,
we're going to use the str.contains. So look at this. We only want those participants that had the word
manager in their vocation. So that's what we're going to build a new criteria. I'm going to say
df.vocation. So down that column, that pandas series, the str.contains method, string.contains.
And what I want it to contain is the word manager. And then you see, I've got a second argument there,
na equals false. That just means just exclude those in which nothing was entered, empty data. And we're
certainly going to have a look at empty data still in this very notebook. And then I'm going to create a
new compute, a new pandas data frame, vocation underscore df. And I assign to that df.lock my
criteria. And if we have a look at that, if we have a look at the dot hit method, so the first five,
there's energy manager, tourist information center manager, estate agent or land agent,
or estate manager, land agent, logistics and distribution manager, passenger, passenger transport
manager. So it contains that word manager. And if you have very complicated data sets, and you want to
extract only certain values from there, this str.contains can become very, very powerful.
Next up, let's talk about just updating or changing values in a data frame. That is a very common thing
to do. The first one is just to rename some of your columns. So in this first instance, we have a name
column, I want to change that to participant. Now, not a very useful example here, but you might have
now a horribly named column and you just want to rename that. So for that, we use the rename method.
So df, my data frame dot rename. And one way to do it is to use the columns, the columns argument.
And I'm setting that to a dictionary, because a dictionary is very useful if I want to do more
than one replacement. Remember dictionary is a key value pair. So the key is the old name,
and the value is the new name. And they are strings, so they go in quotation marks. So name was the old
one. Participant is going to be the new one. And then I'm setting this in place. You're going to see
this in place quite a lot. In place equals true. That means make the change permanent. If you don't
put that in place equals true, it's just going to do it for the cell. But when you use
the data frame again, it's going to revert back to the old one. So if you want to overwrite
values in the data frame, use the in place equals true. And that's going to do it permanently. So
let's look at the df.columns, the columns attribute there. And now instead of have name in my first
column, I now have participant. And if I look at my data frame, the first couple of rows, you'll see
that the name is changed to participant. Good. Next thing we might want to change is a certain value,
but we want to change all the values down that column. So one way that we go about just
putting some anonymity inside of data. Remember data security and laws pertaining to data on humans
are very strict in most countries. One way we can go about that, for instance, a very simple example
that I'm giving you here. We might decide that when we capture the ages, we mentally going to subtract
two from every age. So that first participant there, we could say that they were actually 45
years old, but we capture 43. And that just adds a little bit more complexity. If someone found this
data and, you know, they want to figure out who these participants are, it makes life a little bit
more difficult for them. Of course, we'll have to strip away the participant name and the date of birth.
We'll have to delete those columns. And I'll show you how to delete columns. But that's just one way to
change things. When we analyze this data, though, we've got to remember that we've got to add
two to each age again, to get to the right age. So I'm going to show you a couple of ways to do that.
The first and more useful way, but that's also the most complicated way of doing it, is writing
our own little function. So Python comes with a bunch of functions, of course. And when you import
other packages, it's going to have even more functions. But we can create our own. And we do that
with this DEF for definition keyword. So I'm saying DEF, and then I give my function a name.
And I'm calling it ADD2, really descriptive function I'm going to create. Then a set of parentheses,
and then an argument. Now, this is a very simple function. I only have one argument,
and it's just a placeholder. So that says whatever someone uses my function, they're going to put a
value in there. And so that's just a placeholder. So then a colon. And if you put that colon, it's
going to enter this white space for you, something that is very important in Python. And then the
return keyword, and what it does is X. So whatever the argument value was, it's going to add 2 to that.
So that's my very own function. So let's have a look at what the values were before. So I'm calling
df.age. So it gives me back a pandas series. And I'm calling the head method on that series so that
I only get the first five values. So 43, 53, 33, 43, 46. Now I can use the dot apply method. I'm going to say
df.age dot apply. And then I'm just going to pass the function name to it, add 2. That function knows
what to do. It's going to take every value and add 2 to it. And what I'm doing here is I'm overwriting
that column. So because I'm assigning it to that column. So I'm saying df.age equals df.age dot apply
add 2. And what that is going to do for me, it's going to permanently overwrite that column because
I'm assigning it to that specific column. So let's call the head on that column. And now we're going
to see instead of 43, we see 45. Instead of 53, we see 55, etc. So that's one way to go about it.
There's a, perhaps a more convenient way. And let's choose a lambda function. Now lambda is an
anonymous type of function. It's part of Python. Not something we use quite often, but I want to
show you that it exists. So this time I'm going to subtract two values. So I'm going to say df.age.
So I'm overwriting that by the assignment operator. So I'm saying df.age dot apply. And what I'm
applying is this very shortened, it's a short form of a user defined function. So I'm saying lambda,
that means create this temporary thing called x. And after the colon, what do you do with this x?
Well, just subtract two from it. So that apply is going to go row by row. And that value in that
specific row is going to subtract two from that. So what we're going to see, we're just going to be
back, back to normal. Okay. So those are complicated ways, but sometimes you do really need these more
complicated ways because there's something more complex you want to do to all the values.
But now I'm going to show you the one that we, all of us use most often. That's just the simplest way.
df.age equals df.age plus two. As simple as that. So on the right hand side, I'm going to get a panda
series, df.age, and it's going to broadcast this plus two to every value down that column. So all the
rows in that column is just going to add two to it. And the assignment operator df.age, which means I'm
overwriting what is in there. And now suddenly I'm going to have the added two again to it. So very,
very simple. Now I might want to change a nominal variable to an ordinal variable. So how do we go
about that? So if we look at the group column, there were two sample space elements in that group
column, by the way, and you can check on that doing df.group.unique, called the unique method.
And you'll see this control and active. And I'm doing this as a, I'm doing this as a dictionary,
key value pairs. So the old value and the new value is what these key value pairs are going
to represent in this dictionary. So all the participants were either in the control group
or the active group, but I want to change that to zero and one. And later on in this course,
we're going to do some machine learning, some artificial intelligence, and some of those
algorithms work only with numbers. So I have to convert my categorical variable to a numerical variable.
And this would be one way to go about it. So I'm going to call df.group, and then I call the dot
map method. And what I'm passing to it is this dictionary. So if we do that,
we're going to see that the group is now not going to contain control and active control.
Active is going to contain these values zero and one. I just want to warn you against this map function.
The alternative to the map is the replace, as you can see there. So df.group.replace.
What the map does, if it finds an empty cell, it might have a little bit of a problem with that.
And if you do the dot replace, it's going to keep those values. So what the map will do is it'll
delete that whole row. And with the dot replace, it's just going to leave it alone. It's not going
to do anything with it. So sometimes I would use dot map, sometimes we can use the dot replace.
But pass a dictionary to that of old values, new values. Now it need not only just be categorical
variable and then change it to a numerical encoding, you can go the other way around as well or change
things completely. The dot map and the dot replace is something that we use quite often.
So let's just talk about changing columns. And this might be very useful for you to do. You have a column
in which the data is not tidy. Different things were entered the comma with commas in between or
semicolons in between or spaces in between. And you want to separate that out into different columns.
So we had this name column, which we changed to participant, remember. So what we're going to do
is we're going to split that because it was a first name, a space and a last name.
First name and a surname. And we're going to use the dot str dot split method. So I'm saying go
div dot participant string dot split. And what do I want to split it on? I want to split it on the space.
And expand equals true. That means it's going to make two separate things.
It's going to create two separate columns, a first name and a last name.
So I'm calling that new data, new underscore data. And that is the splitting that I want to do.
And that's going to give me these two values, the first name and the last name. And remember,
those would be indexes. The first one would be index zero. The second one would be index one.
So what I'm going to do is I'm going to create two brand new columns in my data frame,
df. And the way that you do that is you pass a string inside of a set of square brackets.
So this first name column does not exist in this data frame. I'm creating a new one. And this is how
you do that. And what I'm assigning to that is this new underscore data, the zeroth index. And that's
going to be all the first names. The second one is going to be all the last names after the space,
which is our split. Now very fortunate here that there's none of the columns have, you know, second
names in them, et cetera. So, you know, this is a kind of a simple example, but I think you get the
point. And if I now call df.head on this. So right at the end here, we see two new columns, first name,
last name, Dylan and Patton. It was split on that participants, that space in there. So sometimes you
are lucky someone put semicolons or commas in there. And then instead of that space there, you can use the
semicolon or just the comma, whatever the case might be. And I just want to show you that you
can concatenate those all over again. So this time I'm creating a new column called name. So df, and
then inside of square brackets, my quotation marks name. And what I'm going to do is concatenate two
values. So df.lastname, then the plus to concatenate. Remember, we concatenate strings with a plus
and then a comma and a space, and then the first name. So let's just have a look at what that's
going to do. So if we just look at our dot name, df.name, I'm going to have Patton, Dylan,
Howard, Sandra, Williams, Samantha. So that works very well. Next thing I want to show you is how to
create a categorical variable from a numerical variable. And sometimes that's called binning.
So I have a numerical variable, but I want to change it to a categorical variable. And that doesn't mean
I'm changing every single value. I'm creating these bins, a minimum and a maximum value in which
that specific value might fall. So let's go down this cholesterol and we just look at
the cholesterol before that series, that column in other words, and call the min method on that.
So we see the minimum cholesterol was 1.2 and the maximum cholesterol before was, let's have a look,
11.1. So we know what the smallest value was and the largest value. So this might not be units that you
are familiar with in different countries use as the standard in the laboratory's different units.
So, but for this data set, it doesn't matter what the variable was. Our minimum in this continuous
numerical variable was 1.2 and the highest was 11.1. So we know that all the values fall inside of this
interval with 1.2 being included and 11.1 being included as well. But what if we want to chop this
up into, say, three different bins and we're going to call them low, intermediate and high. So we're going
to have this new variable and it's going to be a categorical variable. And it's going to put
that specific value of that observation inside of one of these bins or buckets.
So I'm going to create a new column in my data set, my data frame, df, square brackets, quotation marks,
cholesterol before level. And now I'm going to use a pandas function on this. So to this new column,
I'm going to assign this pd.cut. So I'm using the cut function in pandas, pd.cut.
The first argument is the numerical variable that I want to cut, df.cholesterol before. Then I'm going
to say how many bins do I want them to be divvied up into. Now, what it's going to do, it's going to
look at the minimum and the maximum, and it's just going to do a third, a third, and a third.
I'm not setting those values. It's going to be purely dependent on the values that are there. So this
doesn't make physiological sense, what we're doing here. I just want to show you how it works. And
then the labels I'm going to give those three bins as low, intermediate, and high. So if we do that,
let's have a look. There we go. And let's just do a NumPy array of that so we can see. So 1.2 fell into
low, 1.2 fell into low, 2.1 fell into low. So if you print out a few more, you'll see ones that are a
bit higher, they're going to be intermediate, and ones that are even higher are going to be
high. So those are going to be our two columns, our new column, cholesterol before level.
So just a little bit of mathematics, because you might wonder if a value falls right on this
edge between the two, which side does it go to? So mathematics, we have this idea of the different
types of intervals, and we use a notation for that. So if you see this notation here, square brackets,
10 and 20. So let's just use those values 10 and 20 on this range, on this interval 10 to 20.
If in mathematics we put square brackets on either side, so that's not computer square brackets,
python square brackets, I'm just talking mathematical brackets here, the 10 and the 20 are included.
If I put in mathematics, if I put these parentheses around 10 and 20, it means on that interval from 10
and 20, but the actual value 10 and the actual value 20 are not included. So that's an open interval.
This is a closed interval, and then we get semi-open and semi-closed intervals.
So if I have this interval 10 to 20 here, that means the 10 is not included, but the 20 is included.
And we can swap those around the other way as well. So remember those half open intervals.
So now consider these values that I have here. 11.2, 12.2, 13.2, 15, 6, 16, 16.9. I think I wanted to put a 16 there.
Let's correct that one. So there's another 16. There we go. So we have those values. And I want to create
these bins. So what I want to do is that everyone below 13, I want that to be low. From 13 to 16,
I want that to be normal. And from 16 and above, I want that to be high.
So a proper interval that I could have is 11.2 to 13, but 13 is not included.
11.2 is included. So the next bin that I have to have is 13, but with a square bracket. So 13 is
included in this new middle group of mine, my normal group. And then 16 is not included.
So as soon as someone is 16, they fall into the next one, the high one. And this is what we do with
these semi-open or semi-closed intervals. And then on the top side, as long as I put in a very large
value that I know is not even in the data set, I can close it off on the right hand side, or I can
just put infinity there. The point being, someone who has 16 will still be included here.
So we can do that in code. So I've gone about here creating three bins. So I'm still using pd.cut and my
column that I'm interested in is still df.cholesterol before. But now I put these bins, bins equals,
and now I'm going to put four values there for my three bins. So then as a Python list. So I go
ridiculously low on the zero side, on the smaller side. So I start at zero. I know no one was zero.
So just go lower than you think. And then a five. So that first interval is going to be zero to five.
My first bin, my second bin is going to be five to 10. And my third bin is going to be 10 to 20.
But what happens to someone with a value of five, with a value of 10? You know, what happens to them?
So I've gone, you know, very low on this side and very high on this side. We know the minimum and maximum
is not zero and 10. It's well inside of that. So just choose values that are way outside.
It's these middle two that we're interested in. So what happens to someone who's five and what happens
to someone who's 10? Now, in which of the bins does it fall? And for that, we have this right
argument. And I'm setting right to false. That means the right side, so the five between zero and five,
that right, that five is not included. As soon as the participant has a value of five,
it goes into the next one. And the same from five to 10. Someone who's five is now included in this
second bin. Someone who's 10 is not included. Someone who's 10 is going to fall into the next one.
And then I put labels, low, normal, and high. So if we do that, now we can be very precise as to who
we include in this new categorical variable of ours. So you can take a numerical variable and you can
change it into a categorical variable with specific sample space elements. Our sample space elements
here were low, normal, and high. So deleting a column. Sometimes we just want to get rid of a column.
Perhaps it has some information there that we're not interested in, or we want to take,
we're concerned about anonymity of the data, et cetera. And for that, we just use the drop,
the drop method there. So dataframe.df.drop. And we have this columns argument,
and we're going to set that to a Python list. And inside of that list, we just list
as a string. So inside of quotation marks, all the columns that we want to drop.
And we want to make this change permanent though. So we say in place equals two. So if we do that,
and we look at the columns now, the name column, remember that was one that we created, is now gone.
That's not in our list anymore. Sorting, very easy to do. Sort values is the method. So df.sort values,
and then there's the by, the by method. And the one that we want to sort by is the last name.
So what that's going to do, it's going to see that this is an object, and it's going to see that it's,
it's letters, so it's going to do it alphabetically. And if we look at last name at the end, we now have
Abbott, et cetera, et cetera, et cetera. We can also change that around, you know, go in reverse order
by setting the ascending argument to false. So we're doing exactly the same thing, but now we're going to
start with the z's or the z's, and you see there the last name right at the corner there of the screen
on the right hand side. All the last names start with a z. Now we can also do, you know, we can do
the sort by more than one, and it will be that order. So this one we're going to sort by age and
then by SBP. So those are two numerical variables. So let's see what happens there. So if we go down age,
32, 32, 32, and then 33, 33, 33, 33, 34. So certainly that goes in ascending order,
but there's a couple of 32's. So then it's going to go to SBP. And for all those 32's,
it's going to put those in order, 133, 159, 168, and then 122. So you'll come across this when you do,
um, when you want, when there's more than one value that you want to sort by. So 133, 159, 168,
and this 122 goes to the next one. There's 33, 33, 33, 33, 33, but now these will all be in order.
So you can really sort by, you know, sort to your heart's content.
And you can also make any ascending or descending order for these. So I'm passing
another argument here, ascending, passing a Python list to that, and it's true and false. So the age
is going to be ascending, and then the SBP is going to be descending. So you can really, really
manipulate your data to exactly, to exactly what you need. If you very quickly want to see the largest
values or the smallest values, there's the n largest and down here you'll see n smallest as well.
So I'm saying df, go to SBP, show me the 15 largest values in that series, and you'll see the 15
largest, or you can do it this way as well. Give me a df dot n largest. So I pass this SBP, the column
that I'm interested in as one of the arguments. This is those two ways to do it. Now, if you do it this
way, of course, it's just going to sort of be exactly like the sorting then. And as I said,
there's also the n smallest method there. So in this section, we're going to talk about missing
value. It's so common to see a dataset file, a spreadsheet file, and there's this missing data,
or that missingness has been encoded by a value. Let's see how to deal with this missing data. So in
the numerical Python library, there is a value nan. So if I say np dot nan, that's just nan,
not a number. It's missing. So if I create a Python list and there's a np dot nan value in there,
there we see the list one, two, three, nan. If I now do the sum function using numpy dot sum on my list,
the answer is not a number because, you know, that value can be anything. So it's impossible to sum over
all of those. So let's import a spreadsheet file that contains some missing data. It's called missing
data dot csv. I'm going to use the read underscore csv function inside of pandas. So it's pd dot read
underscore csv as a string past the name of the common separated values. And we're going to assign
it to a computer variable called missing underscore df. So let's have a look at this missing df. And there
we go. We see an age column, a salary column, and a previous company column, and there's NANs all over
the place. So those cells were empty in the spreadsheet. This is what they're going to look
like. So how do we deal with this missing data? Well, number one is to delete it. And what we're
going to do here is any row that contains even a single NAN value, that whole row or that whole
observation, that information on that whole subject is just dropped. So it's certainly something you can
do, but it's quite aggressive. And the way that we would do that is say missing underscore df,
that's our data frame dot to drop na, open close parentheses. So the drop na method we use there,
no arguments, and it's just going to drop all of them. And I'm going to assign that to a new data
frame. And it's complete underscore data underscore df. So that if we do that, then we're going to have,
you know, a lot of data missing, but at least the data that's there now, everything would be there.
Now, if I didn't assign it to a new computer variable and I did not put in place equals true
there, it is not going to do that permanently. So the original missing underscore df still contains
all that missing data. I just reassigned it to a new computer variable. So very importantly,
the missing underscore df data frame was not touched because we did not put in place equals true
right there. So if we don't do that, the original does not get changed.
Now, that would certainly be one way just to drop any row that contains an na. What we might want to
do is only drop a row if a certain column has an na value in that. And for that, we use this subset
with the subset argument. I'm going to say missing underscore df dot drop na and subset equals and then
inside of a python list object, so square brackets, I'm going to pass the columns that I'm interested in.
So now if we look down the age column, we'll certainly see that there are no missing values.
So it's only when those were missing that the row was dropped. But certainly here we can see
there's salary, there's still nans in there. In the previous company, there's still nans in there.
So that would be one way to go about it. Now there is this dot isna method. So if I say missing df dot age,
so remember if I do that, I get the age column or the age series then, and I then use the dot isna method,
it's just going to return a bunch of true and false values. So it goes down every row. And if it's an na,
it's going to say true. So you see the couple of trues there. And if it's not na, not missing,
it returns a true value there. Now internally in python, false is represented by the number zero
and true is represented by the number one. And that means we can add true and false values to each
other. So as I say missing underscore df, remember that wasn't changed originally. I had assigned it
to another computer variable name. So the original wasn't attached. So it still has these na values and
then dot age, dot isna, and then the dot sum after that. So it says take my missing df data frame,
go down the age column, do all the trues or falses with the isna, and then finally sum over all of
these. As I said, false is zero, true is one. So if we sum over all of those, all the ones, the ones that
were missing, we're just going to sum over all of these. And then the result we're going to see is
four. There were four missing observations in that variable. So what can we do to replace missing
values? That's called data imputation, and that's a very, very big subject. There are certain
classifications for missing data, such as missing and completed random, missing at random, and missing
not at random. Not very descriptive terms, but there's a whole science behind how, you know, how
values are missing. And that really is a big topic. What we're going to do here is something very simple
though. We're just going to fill in the missing values. We're going to impute the missing values by
very easy methods. So for that, we're going to use the missing underscore data frame, go to the age
column. So remember, that gives us a series. And then from there, we're going to use the full na,
full na method. And it has a keyword argument method. And we set that to f, f, i, l, l, forward
full. And all that's going to do is going to look at the value just before it. And it's just going to
just replicate that number. So certainly if we do that, it's just going to forward full all of those.
So you see there, the 46 year was missing. It's just the 46 in front of it. It just replicated
those values. So there's also a backward full that you could also use. So it's just the value after.
More commonly, we'll just use the median. So we'll look down that column, that variable,
calculate its median, and then all the missing values are just replaced by that median.
And that works to a great extent in as much as some of the nonparametric statistical tests that we can
do. They work on the median. And it's really not going to make this dramatic change to your data,
although that's not always the best idea. But what we're going to do here is missing underscore df dot age.
So we get the age column or the age series dot median. And that's going to calculate for us what that median is.
It's 57. And now with a full NA, instead of the method, we just put the value that we want all
the missing values to be. So if I put 100 there, it'll put 100 in all the missing values. But here
we're saying, please put the median in all of those missing values. So if we ran that, we're going to see
the median was 57. And there we see that number three that was missing, it is now 57. So all four
values that were missing are now suddenly going to be 57. Remember, if you want to make these changes
permanent, you've got to do the in place equals true argument there as well. What about default
missing values? So there's another spreadsheet file, default missing data dot CSV. I'm going to use the
read underscore CSV function from pandas. I'm going to import that. And let's print it to the screen to
see. So if we look down the age column, there's a couple of triple nines. So when someone captured
this data, if the age of this participant was not known, they just entered 999. Now that causes a
problem if we want to do any kind of calculations on that column, of course. If we look down the salary,
every time the salary wasn't known, someone actually typed their nil. Now this is a big problem,
because this is no longer tidy data. Because if you look down this column, the salary variable,
we see integer values and we see strings. Certainly nil, the word nil, that is a word. Someone typed that
in. That's not a missing value. It wasn't blank in the original spreadsheet. Someone typed the nil.
That's a string, part of a categorical variable. And you can't mix different data types when you're
working with data. So that's certainly a big, big problem. And the same goes for previous company.
When it wasn't known, someone physically typed in the spreadsheet missing. So again, you have this
categorical variable and you have numerical variables there. Now remember, these are not
really numerical variables. They're just encoded. One means something, two means something, three means
something, four means something, five means something. So this is categorical, but it is represented by
numbers. And then we have these strings in between. So that's certainly not going to work as well. But that was
decided when this data was captured to put something in when the data was not missing. So we want to
replace these. And we can do that right when we import the data. If we were the designers of that data set,
that spreadsheet file or that database, and we knew that we had done this or we received this data
and someone told us this is how it was done. We can actually put an argument when we use the read
underscore csv function. We could say na underscore values and set that to a python list. And now
pandas is going to look for these values 999, null and missing. 999 being a number, null, missing being
strings. It's going to look for those when it imports that. And we're going to assign this to a new
computer variable. And now look what it's done. When it found the 999, now it's put an nan value there.
When it found nil, I put an nan. When it found missing, I put an nan value there. So we can encode that
right from the get-go. Remember there's always the map and the replace. Remember with the dot replace
method, we can add a little dictionary there with the key value pairs and the key value pairs being old
value, new value. And you can certainly put np.nan, if you've imported NumPy as NumPy, as the replacement
for any one of those values. So that would be a different way for you to go about it. Working with
times and dates is probably one of the most difficult things to do. Whether you've used Microsoft Excel
or Google Sheets or computer language such as Python, very difficult to work at times with dates and
times. And that really depends how that data was entered in a database or spreadsheet. So what we're going
to do here, we're going to import a new data set, data dates times dot csv, again using the read
underscore csv function from pandas. And I'm going to save that as the computer variable dt. So let's look
at this data set. We see an ID column. We see a batch column. We see a specimen date. So this looks like it
came from a lab and might be organisms or some other microbiology laboratory or some such a test date and
a test time. So to us as humans, this looks very good. We see 2025. So the simulated data set way into the
future 2025.04.21 test date 2025.04.26 and the time recorded there is 12.23. So a bit of a military time
there, 13.45 etc. So it looks like when someone entered this data it was done properly in a data set
in the spreadsheet file. Please just remember if you're the designer of the spreadsheet
and you're using something like Microsoft Excel or Google Sheets, don't set the formatting. Don't click
there in the top and set and say this column is a date time and try and save it as that. Just type
the text as it is. Just leave it as a normal column. Just a little heads up there. So look, let's look
what pandas thought about all of this though. So we're going to use the dtypes attribute and the batch
was a 64-bit integer. That's quite correct. The specimen date is an object and the test date's an object
and the test time is an object and objects are categorical variables. So it's strings. It's seeing
those values there as strings specifically because there's I suppose that forward slash symbol and
there's a colon symbol. So pandas are seeing these not as dates and times but just as normal strings. So
we've got to change that up. So what I'm going to do here, first of all, just remind you that we can
concatenate two columns and so I'm going to create this new column. Remember how to create a new column.
I call the data frame dt and then inside of square brackets and quotation marks I'm going to give my
new column a name and then I'm going to assign to that the following. Seeing that test date, this test date
and test time that's been seen as objects or strings, I might as well concatenate them. So what I'm going to
do is I'm going to say dt.testdate plus and then a space but this is a space remember that is a string
so it goes inside of quotation marks and then this string. So just concatenates them all together.
So now when we look there's this new date time column and it's taken test date and test time
and it's concatenated that with a little space in between. If we were to look at the d types though,
it's most definitely still going to be a string, an object. So if I do dt.date time it's my new column
so that returns a series for me and I call the d type attribute on that I see it's d type o, o for object.
So now we've got to change this so that it's recognized as dates and times and fortunately for us
there is a pandas function to do just that. So what I'm going to do is I'm going to create a brand new
column dt and then call it date time but you see this time I'm using a lowercase d and t so that's
going to be seen as a different new column and to that I'm doing the following. I'm just calling the
to underscore date time function from pandas. So pd dot to underscore date time and it's going to take
two arguments. My first argument is going to be the specific column that I'm interested in. That's dt dot date
time. The uppercase d and t so the one I've just created by concatenating the strings and then I'm
going to have a format argument. And now let's just look at this what we have here. We have 2025
forward slash 05 forward slash 02. So it's the full year 2025 not just 25. So it's the full year and then
the month as a number 05 and then the day 02. A space and then hour colon minutes. And that's exactly
how we build our formula. Have a look at this inside of quotation marks because it's a string and we say
percentage uppercase y. So the uppercase y refers to the fact that we used 2025 and not just 25.
If we had a lowercase y there it would just be 25. But we can see the format there is 2025. So it's
uppercase y. Then a forward slash because that's the forward slash we have there. And then it's month.
So that's percentage m. So lowercase m. Then another forward slash and then percentage d. Then there's a
space. So I'm going exactly by what the string looks like. Then percentage hour uppercase h
colon percentage uppercase m. We're using these uppercases because the hours go into 13 14 15 etc.
So I'm using this formula that I'm writing here. Assigning that to the format argument exactly how
my text appears there. And we're setting that assigning that to a brand new column. And if I
now look at the new column it looks a bit different. Look at this. It now says 2025
and then a dash or minus 04 dash 26 space 12 23 dot zero zero. And if we look at the d types now
what's going to happen is this new column that we've just done the date time with lower cases
is now a date time object. It's now a date time object. Now we can eventually work with it. So I really urge
this workflow. If you fill in that spreadsheet file just do it as normal text. Don't change the cell type
or that column type by formatting it in any way because that creates a lot of pain.
So now this date time column or the date time series inside of this pandas data frame is very
easy to work with. So I'm going to create yet another new column in my data frame. So I'm going to say
DT and this time I'm going to call my new column month and I'm going to assign to that the following.
I'm going to say DT it's my original data frame dot date time. So I'm looking at that new column of
mine which is now a date time 64 object or data type I should say. I'm going to call the following method
on it dot DT dot month underscore name. So it's going to extract from that the month. It knows now this
is date time and knows what every part of that of that value means. And then I'm going to do a dot string
dot slice stop equals three. So what is this going to do? Now the first part here let's just do this.
This DT dot date time. Now that DT refers to the computer variable I gave to this data frame. The
original data frame as we imported the spreadsheet file. So it says take that data frame go down the
date time column. So this is now a series. On that I'm going to call this. Now that DT has nothing to
do with the name. This is one of the methods inside of date time objects. So it's DT dot dot month underscore
name. So if we look at this one it says 05. Of course that's May. And then all I want to do then
I want to then just stop at the first three after the first three letters. So let me show you what
that is. Let's call the DT and if we look right at the end we see what it's done. So see there's APR,
APR, APR for April and indeed that says 2025 04. So that's the fourth month and that's April.
So if I didn't put this in this last little bit the string dot slice that was going to do the whole
of April. Because that DT dot month underscore name that's going to extract the name of the month.
That month was given as a number the fourth month. It knows because that's a date time object that
that obviously refers to April. And that's very useful because sometimes we just do want to create some
plots and graphs. I'll do some analysis based on how many times you know a month appeared in our data
set. Now I can do something like this DT that's my data frame dot date time. So that's that column
or series and then I'm calling DT dot year. Just DT dot year and you can well imagine I can assign this to
another new column such as DT square brackets quotation marks year. And now it's just going
to extract the year for year for me or DT dot hour and now it's going to extract just the hour for me.
And again in data science that might be very useful for me to know something about that specimen and
the hour in which something happened to that specimen. So once you have a date time object properly set up
as actually then becomes a pleasure to work with them. There's a lot of information hidden in dates
and times. So that was a very good cool quick introduction to to the pandas data package I should
say. And we're going to use it for the almost the rest of this course. Import data using pandas because
it's so easy so powerful to manipulate your data with pandas. Easy when you're used to it. I'll add that
caveat and as much as you have to gain some experience. I will say this to this course probably
have said it a couple of times and will say it a couple more times. This is like learning a language
and you don't learn a language just in a week and you don't learn a language by not speaking it.
You have to speak that language as much as you can for you to pick it up properly. So this was a good
introduction and a notebook for you to refer back to every time. You can't just exactly remember what to do.
As always remember on the pandas website itself there's an enormous amount of data. Pandas is a huge package
and it gets it just expands almost by the day as new functionality gets added.

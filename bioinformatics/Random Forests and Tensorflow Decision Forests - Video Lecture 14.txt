We've come to the last lecture, a very exciting one, and that is on random forests. It's really
one of the better and more exciting machine learning algorithms that we can use. And these
days it's becoming more and more important. People are understanding how useful it can be.
And in many cases it outperforms something like deep neural networks. So in this last notebook
we're going to start with this building blocks of a random forest and that is a decision tree.
So notebook number 14, decision trees and random forests. So random forests and something very
similar called gradient booster trees, they are very commonly used machine learning techniques.
And they are built up from this basic building block, random forests, built up from this basic
building block called a decision tree. And the reason why it's become so popular of late
is that it's not only very accurate and useful, but it's also very interpretable. Because a decision
tree by nature looks very much like a flow diagram. So I want to show you this little
representation here, schema, where we have a bowl of fruit and there's some green apples,
some origins and bananas in there. And we don't get to see the bowl, the bowl is there.
And we instruct someone to pick up a fruit. Now before they do and answer some question pertaining
to that, we have very little information about what the fruits in that bowl. And our aim is to ask
questions so that we can improve our knowledge, we can gain information from asking those questions.
So here's a very simplified decision tree analog in this image below. And as much as you can see here,
we have feature variables in a target class. So when it comes to these feature variables,
we know the color and they're either green or orange or yellow. We know the shape of them,
they're either round or oblong, and we know their weight. And that's in grams, say for instance,
from 50 to 100 grams, something like that. In our target class, we have green apples, oranges and bananas.
We don't get to see the bowl and we just ask someone to take a random fruit and tell us something
about this answer at least one of our questions. And one of the first questions we can ask and that
pertains to these three feature variables that we have is what is the color? Or we could actually
just say in this simplified analog, is it orange? And if they say yes, we move down here. And of course,
then it is an orange. It can't be a yellow banana and it can't be a green apple. It's orange.
On the other side, if they said no, it's not orange, we have a next question that we can ask is,
is the fruit round? And if it's yes, well, then it's got to be a green apple and no, it's got to
be a banana. So that's not quite how a decision tree works. But it gives you that intuitive
understanding of what is going on that we ask these questions based on the feature variables that we
have. And that allows us to gain information about what, you know, what's in the bowl.
The terminology that I want you to get used to here is this idea that this very first question
that we ask, we call that the root node. And all these are nodes. Every time we ask a question,
that's a node. And then every node will have child nodes. And child nodes will have a parent node.
And we also have this idea of the depth of our tree. And that is how many levels of these nodes
are there beyond the root node. So there's one, two, so the depth of this tree would be two.
We also have this idea of what do we have left behind. There's a big difference between here where
we only have oranges and here where we might have bananas, well, we have bananas and green apples.
So this node we will refer to as being a pure node. And this node will be an impure node. It still
contains more than one class. And then all these nodes right at the end that are completely pure,
or we might just decide to stop at some point, even though they're not pure. We call these leaf,
leaf nodes, leaves or terminal nodes. So that's sort of the terminology that we have to get used to when
we talk about decision trees. Of course, we're not going to ask questions like the color. We're just
going to ask, what is the color? And that is going to split up for us three nodes. One would be green,
one would be orange, one would be yellow. And that's how a true decision tree works. But in this
analog, understanding that we ask questions, and from that we gain information. So let's have a look
at the packages that we're going to use, NumPy and Pandas, as we always do. And then a bunch of things
from scikit-learn, which is our favorite library as far as machine learning algorithms are concerned.
So let's import those. Plotly we're going to use for our plotting. We might use matplotlib as well.
I've put seaborne in there as well. I want you to have a look at seaborne. Of course, we're not going
to use it here specifically. We're sending the back-end display in case we're using a retina display.
And then, as usual, to just display tables. So let's look at a proper decision tree.
I have hand created a Pandas data frame here. It has three feature variables and a target variable.
And I've called it cat1, cat2, cat3, as far as my three feature variables are concerned, and target.
And you can see there how it's constructed. And of course, I'm just going to print that to the screen.
So there's cat1. It is a variable there that contains three unique classes, or three sample
space elements. Roman numerals, one, two, and three. For cat2, there'll be a, b, and c. And for cat3,
numerical discrete values, one, two, and three. And our target variable is only going to be no and yes.
So binary target variable. Here we're dealing with a binary classification problem. And of course,
it is part of supervised learning, and as much as we know what the actual target is.
So let's just have a look at the frequency of each of our sample space elements. We can see there
very few cat2s or Roman numerals too, as far as the sample space elements of cat1 is concerned.
So we're looking here at cat2. We're looking at cat3, even though it's numerical variables. Of course,
it's discrete variables. And we see there one, two, and three. And then let's have a look quickly at
our target variable. No class imbalance there. 11 no's and 10 yes's. Now the question is our root node.
Which one of those three feature variables should be in the root node? How do we decide? Well, for now,
we don't know how that works. So let's just go one by one. And we're just going to use the group by
function. So what we're going to say is, let's make cat1 our root node. And let's see what pops out.
So we're going to say, df for the data frame, dot group by cat1. Then once you've done that group by,
look at the target variable and give us the value counts. So let's see what happens. So for cat1,
of course, it found the three values, Roman numerals, one, two, and three. And if it found one,
the child node will then have six no's and four yes's. So that is going to be impure.
The second child node will have two for the answer there, Roman numerals, two. And it'll be pure
because all three that end up there, as far as the target is concerned, would be no's. And three would
be impure again. So let's represent that with this nice little schematic here. So our root node was the
elements in cat1 variable. That was our question. So we put, we asked the question about all the
sample space elements. So the child nodes are just going to represent for us the, all the, the sample
space elements for that variable. And we get one, two, and three for cat1. So when it comes to this first
depth of one child nodes, we look at each of those. And this, when it, when cat1 was one,
we had an impure node. So four yes's and six no's. When it was two, we had a pure node because all three
were no's. And when it was three, we also had an impure node. So that is how we would go. This would
be a terminal node because it's pure now, but these two are not pure. We have to go on with them.
So what shall we do? So let's start with this one. It now becomes a parent node because we are going to
ask a question and that's going to split into child nodes. So we've done cat1. So let's just
for, for, for this one, let's use cat2. So remember, we're going to say df.loc and then df cat1 equals,
equals one. So we're only looking at these ones that turned out to be one. We're going to group by
now category two. So from this one, we're asking, what is your category two like, but only from these
that were one for cat1. And we look at the values a. So let's have a look. And we see in cat2,
there were a, b's and c's. And if it was an a, it gives us an impure node. If it was b,
it gives us an impure node. But if it was c, it gives us another pure node. So that's far,
that's fair as far as this node is concerned. Let's look at this child node and make it the parent node.
And for that one, we might ask, let's ask cat2 of it as well. So we'll say df.loc. So we do df.cat
equals, equals three. So it's only this node here when it was three. Let's group it by cat2 as well.
And then look at the value counts as far as the target column is concerned. And you see,
we get only b's and c's from there and neither of them are pure. So let's decide instead of using cat2
on that split, let's use cat3 there. So it's still this cat1 equals three. So we're still looking at
this node. Instead of splitting it by cat, asking the question about cat2, we'll ask the question about
cat3. So if we do that, oh, we see we have one, two, three child nodes and they're all pure.
So that gives you an understanding that asking about cat3 at that level was better somehow than
using cat2 as the way to split this node. So there must be some idea where we can measure how,
you know, which ones to choose at every level. And that has to do with, with gaining knowledge,
with information gain. So we see these, this equation here for information gain, ID.
And that is very, very important. And we have to develop some sense of what this is without knowing,
you know, exactly what is going on here. So in general terms, we have this IDP. That is the impurity
at the parent node minus, and this bit here is the average, that's the average of the impurity
of the child nodes. So the impurity we have at the parent node minus the impurity, the average impurity
of the child nodes. So we just look at impurity in each of the child nodes, and we average over those.
And that gap, that difference is the amount of information we gain. Because here we'll have a higher,
at the parent node, a higher impurity, and we're trying to get to a lower impurity.
And if it's a lot lower, that gap, that difference is a bunch of information that we gain. So that is
how a decision tree will decide which one at every root node, or at every parent node,
what to choose, which of the variables to choose at once maximum information gain. And we saw that
intuitively here, choosing cat 3 to split there versus cat 2 gained us more information. We can
intuitively see that because these child nodes are all pure, and here the child nodes were not pure.
So definitely from the parent node, which is also impure to these two, of course,
we're going to choose this one, because we have more information gain. So how do we measure impurity
then? Well, there's two common ways. One is using entropy, and the other one is using the Gini score,
Gini index. And you can see the two of them there. The Gini index that really is only for categorical
variables. So very useful, very powerful, used many times. But we're going to discuss this idea of
entropy. And we see it has the summation symbol in it. So we have to talk about that a little bit.
And it has the log, logarithm base 2 in there. So I just want to spend just a few seconds on those
two. So we just have some idea of what is going on here when it comes to measuring impurity of a node.
Remember what the log means. If I write y equals the log base 2 of x,
what we're asking for is 2 to the power of what, remember how to take powers, 2 to the power 2
means 2 times 2. And that's 4. So 2 to the power of what will give me x. And that what is y.
So y equals log base 2 of x means 2 to the power of y gives me x. And it's that asking 2 to the what
gives me x. And that what is the y. That's what we're trying to get to. Summation symbol
is just very shorthand. So we have this counter. And the counter just counts up in increments of 1.
So we start our count at i equals 1 and we end at 3. So whatever's at the bottom where we start,
at the top is where we end. So this would be i1, i2, i3. And then we put whatever we want to iterate
over. And there'll be some value. And we put a subscript i. So that will be x sub 1 plus x sub 2
plus x sub 3. You know, just iterate over those as a summation symbol. So it sums over all of these
terms. And we're just incrementing some i. And that i might not be a subscript. It might be somewhere
in this expression here. But every time you sum over, you go to the next term in the sum,
you just increment that counter by 1. And then you have some end. So that's just a shorthand way of
writing all of this. And you can imagine that can be quite long. This right hand side,
you do that by writing something very short like that. So let's talk about entropy or
more commonly, or more specific, I should say Shannon entropy. And that's what we see here.
Let's go back there to equation two. So we have this negative up front. And that negative one, we
multiply by this summation. And now i equals one to c. Now that c is going to be each of the classes
that appear. And we have p sub i, p sub i. p sub i is that probability of one of the classes.
And let's just go down there and look at an example because that will make a lot more sense.
Let's just have a look at what we did here again. For our first root node, we said group i cat1. So
cat1 was our decision. And we wanted to see what that turns out to be. Now we see there were three
of these. So for, you know, we're going to go through them one by one. And let's start where cat1 was
one. We had six no's and four yeses. So if we go way back up here, that's what we have at this one.
So we're looking at the entropy. How do we express this four versus the six? We have to express some
entropy for it. Remember here we had 10 and 11, 11 and 10. Yes, that was our target over there.
So that'll give us an entropy. Now we have to calculate some entropy for four versus six,
zero versus three, and one, two, three, four, five, six versus two. We have to express the entropy.
And we're going to calculate the entropy of that parent node. And we're going to average over
these entropies and we subtract that and we want the maximum information gain. So let's go here.
So let's look at the first one. And that is where cat1 was one. There were six no's and four yeses.
So there were the two classes, no and yes. So that is going to be our p's that we
sum over. So i is going to go from one and c is going to be two, no and yes. This is two of them.
So there's going to be p1 log base 2 p1 plus p2 log base 2 p2. It's only the two of them that we have.
So if we look at this, it's actually very easy. We're going to make p1 the yes and p2 the no. So that
was the probability of yes times the log base 2 of the probability of yes minus. Remember,
there's a minus sign up front. So this becomes if you multiply the minus throughout the summation,
it's going to all be minus p of no log base 2 of p of no. So what was the probability then of yes?
Well, there were four yeses. And in total, six plus four is 10. There were 10 solutions there. So four
over 10, 0.4. That's the probability. And then that makes six over 10, six divided by how many
there were for p2 for the node. So minus four over 10 times log base 2 of four over 10 minus six over
10 log base 2 of six over 10. So you see, it's really not that difficult. So there we just do that.
And this is the entropy, the impurity using entropy of that first child node, 0.97.
Now, let's look at the second one, which was quite pure. Now, the logarithm of zero is not defined.
That's something that you can't calculate. So we just leave that out. We make that to be zero.
So we actually have three over three, because all three of them were no's. So we don't sum them over
the yeses as well, because that was the zero over the three were yeses. And zero times anything is
zero. The log is not defined for zero. So all we have is just these no's. So the three out of the
three was no times n times the log base 2 of 3 over 3. And remember to take the log base 2,
that's numpy dot log 2. We do that subtraction. So we see what the impurity is using entropy
of the second node, and that's zero. Minimum entropy. And that's really what we owe me for.
That means it's a pure node. And then let's look for when it was three. So six out of the eight
were no's and two out of the eight were yeses. And if we calculate the impurity there, it's 0.81.
And let's just have a look at what the entropy then was for the parent node. Remember I said there was
the 11 and 10. So in this case would be the root node. So let's work that out. 10 over 21 and 11
over 21. That's 0.99. So what we want, as far as our information gain was concerned, that very ugly
first equation, we take, and I say that I signed this impurity entropy there as the computer will
start. So it's start minus the mean of all of those three impurities. And if we do that,
we're going to get the information gain if we chose as root, as our root node, if we chose cat one.
Let's choose as our root node cat two and see what happens. So we're going to group by cat two
and have a look at that. So we have five no's and a single yes, eight yeses and three no's,
three no's and one yes. And I very quickly use my equation there for entropy and we calculate the
three entropies and we take the mean of all of those and subtract that. And let's look at our
information gain now. So our information gain was 0.404 if we chose cat one as our root node.
And now it goes up to 0.46. So this will be a better choice as our root node. Let's quickly go
through cat three as a choice of a root node. And that looks a lot better. And indeed, you know,
there's a lot, there's lots more purity there if we look at it. And if we look at that information
gain much higher, 0.767. And this is how a decision tree will decide that this would be best for the
root node. And then for each of those three child nodes, then it's three in this case, because we
have for cat three, three classes, one, two, and three, for each of those will now become a parent node.
And we'll go through all of them again and see for each one of those, which would be the best one to
select. And I just want to let you know, you can reselect. You can absolutely reselect one of the,
it's not that you've used it once and now you can't use it again. And that's how you build this
decision tree, because you want this maximum information gain. Now, you know, how, how long
do we go? We can go on and on and on. We had a contrived little example, but in the bigger data set,
you can go on and on and on. And the depth of your tree can be enormous
until you get to pure leaf nodes. And what will you get there? I think you can guess we had that
in the previous section, you can have overfitting. So you can have high variance in your model. It's
going to overfit your data that you have. That's going to learn everything about the training data
and is going to perform very poorly on unseen data. So that doesn't really work as far as the
decision tree is concerned. A single decision tree is also very poor in averaging out over complicated
data sets. And it also does not generalize well to unseen data. So one thing we could do is we could,
we can set a minimum information gain so that if successively it doesn't reach that drop in our
entropy, then we say stop, we stop the bus right there, or we can build the whole tree
right out to the bottom, and then we can prune it back. So those would be
hyper parameters. Yes, hyper parameters that we can set in the design stage to decide
how to build these trees. So let's look at a decision tree classifier based on the data that
we have. With these decision tree classes that we have in scikit-learn, we have to tell it what type
of data we're dealing with. So we get these two classes, label encoder, we've imported that,
and label binarizer. So I'm just going to instantiate them. So we just pass them to
computer variables. And then we're going to encode all of our variables. So we're going to say
label underscore encoder, which is now this one, it is an instantiation of this class.
We're going to use the fit underscore transform. So it's going to fit the values and change them
for cat1. And we're going to assign that to the computable variable encoded cat1. And what the
label encoder does, labels would be the sample space elements, it's going to encode them because
some of them we had a, b's and c's, some of them we had roman numerals, some of them we had
one, twos and threes. We just have to tell the algorithm that these are labels. They are nominal
categorical variables. And we do that by the fit transform method of the label encoder class. So we
do that, and we do that for all three. And our target variable had a binary classes, it only had two
classes. So there we use the label binarizer just to tell our model, look, we take y and we fit
transform it to this label binarizer, the target. And we actually also have to call the dot flatten
method there. We also have to call the dot flatten method there because it is going to give us a
list of lists and we just want a single list as far as y is concerned. Now we have to build up our
feature set again, row by row, because we've changed all these values as individual NumPy arrays. And what
we're going to do is we're just going to build a for loop. So we have this empty list x, and then we
say for i equal in the range of the length of y. So each one of those rows, we're just going to iterate
over one, two, three, four, for row one, row two, row three, row four. And for row one, it's going to
take the first one of encoded cat one, the first one in encoded cat two, the first one in encoded cat
three, put them together in a row. Now i increases by one, so we jump to two. So the next row it builds
up the second value in encoded cat one, the second value in encoded cat two, the second value in encoded
cat three. So we just run through all, we just run through all of those and we build x back up. Now we have
that. We can actually instantiate our decision tree classifier. There's our class there. And the
criterion we want is how do we measure impurity? We want entropy. And we're going to assign that to
computer variable d underscore tree. And it remains for us just to fit our data to the tree. d dot
underscore tree dot fit x and y. And it's going to do all those things that we discussed. How it chooses,
by maximum information gain, what to put where. And it's going to build that whole tree out for us. And you
can see all the hyperparameters that we can set. That's all the arguments in the decision tree
classifier. We left all of them just as a, as the default and, and we get a solution. So by the way,
if you run this on your local system and you have a pi dot and graph this installed, you can actually
draw a nice little graph. And I just leave you there with the code that you can use. We can't do that
just straight off here in colab. But anyway, let's give a new tree now. Now remember all those
values were, we use the, the, the label encoder. So we can't use Roman numeral one,
two and three anymore, or a, b's and c's or one, two's and threes. They were all transformed into
values one, two, three, one, two, three, one, two, three, because remember, there were only three
classes or three sample space elements in each. So unknown sample that we're going to pass,
that that would be Roman numeral one and b and the value one. So now I've got to put the encoded
values in. So if we pass that to the predict, it's going to predict that this would be class zero
based on how, how well it did. So we can do that for all the values. So we remember we haven't split
anything. We're just taking all of our values. We're passing that to the predict, all our feature
variables, pass that to y pred. And now we can sort of measure how many were correct.
So we're going to say y, that was the actual values. We use this, are they equal to y pred?
So remember that gives us a true or false and falses are zeros and ones are true. So we can sum
over all the ones that were true divided by how many there are. And that's instantly going to give us
the percentage, the fraction that it got correct, our accuracy. So our accuracy is 90.47%.
And if we look at the confusion matrix of this, very simply, it's exactly what we saw previously.
The two labels here on the, on the left, the predicted values on the right, on the main diagonal,
all the zeros that were correctly predicted as zeros, the ones correctly predicted and across there,
the ones that were incorrectly predicted. So once again, we can attach to one of these are
positive and the other one a negative outcome. And we can do all the things that we spoke about
before, sensitivity, specificity, positive and negative predictive values, F1 score,
everything, other metrics other than just the, than just the accuracy. So let's quickly look at how
a decision tree regressor would look like. It's not very different. We just going to create a
data set for ourselves, this time making use of the make underscore regression function
from scikit-learn. And you see where we set these 1000 samples. We want four feature variables,
two of them being informative. And this time there's a noise argument. We set that to 90%
just to scramble up our data a little bit. And we set a random state as well. And then I'm just going
to create columns var1, var2, var3, var4, and just create a little data frame for us so we can have a
look at that. And we can see we have numerical variables as far as our feature variable is concerned,
but our target variable now is also continuous numerical. And of course, just doing a little
scatter matrix there. So we can just see, just looking at it, is there's some,
some data if we look at, for instance, target, the target as far as variable three is concerned,
there seems to be good correlation there. So a bunch of information that we can get there.
We're going to instantiate the decision tree regressor class there, so that we have an instance
of it there. And we assign that to the regressor variable there with descriptive name. And then
this time around, we're going to go the full hog. We will do a train test split because that is very
important. So train test split. We've seen how that worked before in the previous notebook. We just
pass our feature variables, our target vector and the size. Let's split off 20% and we set a random
state. And remember that that gives us four objects and we have to name them appropriately. It would be
the two feature variables, the training and test feature variables, and then the two targets,
the train and the test targets. So we have to put that in that order. And if we just quickly look at
the shape, just to verify always that everything worked out properly, it looks good. So now we're just
going to use the fit method. So our regressor, the class that we've instantiated, we just fit our data
to that x train and y train. So the features in the target of our training set. And we assign that
to the computer variable. In this instance, I'm calling a dt underscore reg underscore model.
And then we can just see how accurate it is. And now, of course, we can't say how many were correct
and how many were incorrect because our target variable now is a continuous numerical variable.
So in this instance, we're going to express a coefficient of determination.
And that's what the score is going to do for us there. And we see our value there, 97, quite,
quite high. Now we can also do the prediction. So I'm going to take X test and I'm going to pass
that to the predict method. So we take our trained model and we use the predict method, pass all the
X tests, and that's going to give us predicted values for each of our observations. And it's also
continuous numerical. And what we can build now very nicely is a scatterplot, a scatterplot of the
actual numerical variable against the predicted numerical variable. And what we want to see is
very good correlation between those two. And indeed, we do look at that. There's all our actual values
on the bottom. And then as far as our predicted values, very much in line, very accurate there as
far as our r squared at least is concerned. So now that we've done that, let's look at a random forest.
Now, what that is going to do is that is just going to combine multiple decision trees in one. So
what the computer is going to do for us, what our code is going to do for us, what scikit-learn is
going to do for us is just build a bunch of trees and it's going to somehow average over all of those
trees. And that makes it what we call an ensemble techniques, a technique such as XGBoost as well,
or booster trees. It's just an ensemble of a bunch of trees. And by bringing that together, averaging
over that somehow, we get much, much better models. Now, the way that random forest works is it is going
to just sample some of your data for every tree. It's not going to use all of the data. It's not going
to use all of your variables. And you're going to select some of the variables. And each time it builds
that tree, it's going to use different subsets of those variables. So it's quite an intricate
design there, the way that random forests work. And I encourage you just to read up a little bit more
about it. Easy enough for us here in code. So we're going to instantiate the random forest regressor here,
assign that to a computer variable. And with that computer variable, we're just going to fit
to this instance of the random forest regressor. So that's just a different class than the decision
tree class. So there we go. It was as simple as that. And now let's look at our score. Again,
we pass X test and Y test and look at that 99.9, 0.99, I should say, very close to one. And if we just
create predicted values for our test set and plot that out, you can see the scatter plot
very well indeed. So all we're doing here is in a very specified way, the decision forest
builds a lot of decision trees and it averages over all of them, giving us a much better model.
So in this last section, I want to introduce you to TensorFlow's decision forest. Now TensorFlow,
a couple of words about that. TensorFlow is a deep neural network architecture,
which is a sophisticated form of machine learning. It's an open source
source architecture written by and designed by the friendly folks over at Google. There's also
PyTorch that is developed by Facebook. And these tools are openly and freely available. It's written
in a variety of languages, but usually it has what we call a Python wrapper around that. In other words,
it's Python code that we type. Under the hoot, though, there'll be some more sophisticated
language. And the one that I'm going to show you here is C++, a much more advanced, and I use that
term very loosely, language, much faster language, compiled language, all sorts of things that you
can learn about. TensorFlow then is all about deep neural networks, but they've just released
at the time of this recording, also a new module for random forests. And it's called the TensorFlow
decision forests. If you run a Linux machine, you can install that right now. And it's a bit more
tricky if you're running Windows or Mac. Of course, here in Google Colab, that runs on Linux machines
in the cloud with Google. So we can use this. So it uses what is called the Ictrasil decision forest.
And some people would know what the term Ictrasil refers to. So it's the Ictrasil forest C++ libraries,
and we can just write very simple Python code to make use of that. And we're going to use that. And
that has rapidly become my favorite tool for designing random forests. So we have to install it
first. And this is how we install. If you install your own version of Python on your system, you can
create these virtual environments for all of your projects. And the way that you would install things
are either through a package manager called conda or through pip. So here we'll have exclamation mark
pip or bang pip. And then install tensorflow underscore decision underscore forests. So when you
install for yourself packages like numpy and plotly, you'll use either conda install or pip install.
And you can certainly learn how to do that. So there we go. That's installed.
And now I'm going to import tensorflow underscore decision underscore forest as tfdf, tensorflow
decision forest. And I'm also going to import tensorflow itself, tensorflow as tf. So let's do that
import. And now we have those namespace abbreviations. And because it's so new, there's new versions
coming out as well. I just always just like to see what current version Google Colab has installed on
that's back end. And we see it's version 0.1.7. So if I read up anything about it, I just make sure
that you're reading up on the right on the right version. So the data set that we're going to work
from is we're just going to download from the internet. And that is a data set on lovely penguins.
So the bang, that's exclamation mark wget, and then the URL for where the data is, we can just
use that command. If you download a curated data that's already in the correct format from certain
websites, you can just use the wget function there. That makes it available in memory on your Google
Drive. And we can just use read underscore CSV to import that temporary stored CSV file that is in
our Colab memory now. So let's have a look at the shape of this data set. I've called it penguins
instead of the usual DF. So penguins dot shape, we have 344 observations and eight variables. That's
a very small data set. And decision trees, random forests, very good for smaller data sets as opposed
to deep neural networks that require, they are hungry for data. So let's have a look at that. We have
information about the species. That's what we're going to try and predict from these set of six feature
variables. And that is the island was noted, what the bill length was of the observations,
the millimeters, the bill depth, the flipper length, and the body mass and grams, the sex or gender,
and then the year in which that was done. So that's our data set. And you can very much see there's some
data missing. And there's 344 observations. But as far as bill length is concerned, there's only 342.
And the beauty behind this Yggdrasil algorithm is we don't have to worry about missing data.
Python will take care of all of that for us. So let's have a look at the first five observations
there to sort of get a sense. There's the species, there's the island, and then the bill length and
millimeters. And you can see all the rest of them there. So let's have a look if there's any class in
balance. And we see the three different species there. And we see that the chin strap was a bit
underrepresented, but not too, not too badly. So what we have to do to use this algorithm, though,
when it comes to the metrics on the end, is we just have to convert these names, these nominal
categorical variables just into values. And a quick and easy way to do that is to call the unique
method. So it's penguin dot species. So that series, that column dot unique, and then we convert that to
a NumPy list. So or at least a Python list, I should say. So we see the three species there.
And then we're going to just use the map. So we're going to use the map function there.
And each of these three, remember Python list, each element has a index, so that would be index zero,
gen two would be index two, index one, I should say, the second one, and then chin strap will have
index two, because Python is zero index, so zero, one, and two. So we're going to map these values,
zero, one, and two, to each of the values there in the penguins column. So penguins dot species,
so we get that column back, or that panda series. And we map that to either zero, one, or two,
depending on each observation, each row that it goes down that column, it's just going to assign
the three numbers to those three. So that's a very quick and easy way to, you can use the
replace function, the replace method, of course, as you remember before. Now, as always, we're going to
split our data. This time, we're not making use of scikit's learn train test split. There's a much
easier way to go about it, I'll show you here. First of all, we're going to define a user function.
So I'm just showing you different ways, of course, you can use the train test split as well. So we're
going to define def, our own user defined function, I'm going to call it split. It's going to take two
arguments. One is ds, which is going to be our data frame. And the second one is a variable that we
put a default value to. So if the user leaves that out, it'll take a default value of 0.3. And we're going
to call that r equals 0.3. And then what we simply do, we create a local variable, local to this
function, it doesn't exist outside of that function, I'm going to call it test underscore ind for test
index. And it's going to split out for us a random value. So numpy.rand. So rand is a uniform
distribution on the interval from zero to one. So decimal values between zero and one. And how many do we
want? Well, the length of the data, the data frame that we're going to pass to that. And so it's the
number of observations. And what we want back as this conditional, is it less than 0.3. So you have
those values between zero and one. And equally likely, it's a uniform distribution. So 0.3 of them
are going to be less than 0.3. The other 0.7 will be higher than that 0.52, 0.735.
So what this is going to do is just build up all the yeses and noes for us. So it's either
in a 0.3, 30% of the cases, it's going to be less than 0.3. And in 70% of the cases,
it's going to be more than. So you'll have these 30% false values and 70% true values,
but they're completely at random. And then we're going to return our data frame, two of them,
the data frame that does not contain that index, and the data frame that contains that index,
the true values. So that gives us the split in the data, two separate data frames. So very simple
there. Instead of using train test split, you can build your own one. So I'm just going to see the
pseudorandom number generator. And then I'm going to use penguins underscore train and penguins underscore
test, because I'm returning two data frames. And I'm passing my penguins data frame to the split
function that we've just created. And lo and behold, we have a 70-30 split as far as our training set is
concerned. There you see 237 observations versus the remainder 107 observations in our test set.
So let's just make sure about this underrepresentation, just to see that we have enough in each of those,
and we can see the number there. And that bar chart, we can see the number here in the bar chart for our
test set. So yeah, we do have the underrepresentation, but not too bad. Now,
if you use TensorFlow and do using deep neural networks, there's a beautiful part of the architecture
that takes your data frames and passes it into the correct format to your model. And that's very
important for TensorFlow. And of course, we're using the decision trees here. So we're going to say
tftf.keras. Now, keras, very importantly, we have TensorFlow. And then we also have,
I'm going to call it a simplified version of the code of TensorFlow. Very successful. It's called
keras, built into TensorFlow. TensorFlow can go, you can go quite deep with a code, quite complex code.
And then you get keras, which is a module inside of TensorFlow, which uses simpler code. You can still
do all the powerful things you can do with the deeper TensorFlow code, but it's just much easier
to write. And it has a function called pd underscore data frame underscore to underscore tf underscore
data sets. So it takes a data frame and turns it into a TensorFlow data set. And that is the correct
format that TensorFlow models want the data in. So it's a very easy way to do this. And all we have to say
is what is the training set. Now, remember, the training set still for us has the feature and the
target. So we just have to tell it what the target is. And that's the label argument. And we set that
to species. And then we're also going to build penguin underscore test. And again, it's this pandas data
frame to TensorFlow data sets. It converts a data frame into the correct format for use with these random
forests and with TensorFlow. So now that we have them in correct format. Now, these random forests would know
what the feature variables are, what the target variable is, it's all in the correct format. No problems there.
All we have to do is to instantiate once again. And the same as with with scikit-learn, we have tfdf. Remember,
that was our namespace abbreviation dot keras dot random forest model. That's the one we're going to
use, we assign that to computer variable name, it's now instantiated. Now, we don't have to do this step,
but I like to do it. And that's where we compile the model. If you use a deep neural network, you have
to compile your model before you can use it. So we take our model that we've instantiated, we use the dot
compile method. And here, all we have to do is just pass the metrics that I want. And I'm interested in
this instance in accuracy. But there are other metrics that you can use as well. And now, everything is
ready, our model is ready. Remember, it had missing data, it had all sorts of different types of variables,
no problem whatsoever. We're just going to say, rf underscore model dot fit. And we pass as x, our x argument
here as the penguins train. Remember that penguins underscore train is now a TensorFlow data set. So it's in the right
format, no problems, it can just run. And there you'll see it start building, it runs through it a
couple of times, gives you some information there. And what we can now do is do rf dot model, it's now
fully trained model, random forest model, we can call the dot summary method on that. And that gives us a
bunch of information. So all sorts of things that you can read there, this one right at the top here,
let's go right back up to the top. Oops, that was too far. So it gives us a little bit of information
about our random forest, and what the input features were. We didn't add weights, so weights
would be where we could correct for the fact that we had a bit of class imbalance, but we didn't do
that here. But it gives us this variable importance. How important was our feature variables in order
in contributing to our solution and contributing to the prediction. So we see bill length and millimeters,
we see all the little hashtag symbols there, the more they are, the more important it was.
And you can see it's in order there of the importance. And when you have a big data set and
many, many feature variables, it shows you which ones you can build simpler models from,
by only including those ones that were quite important. So it's a very useful
thing that we can do here. And if we scroll down, let's just get there. If we scroll down,
more and more and more, you'll get some more indication of what's going on. We get to this
very important bit. Training OOB, out of bag. Now, remember I said we didn't go fully into the detail,
because it's quite complex about how we take a bunch of decision trees and build a random forest from
them. Remember that I said it only uses at each and every split only a subset of your feature
variables for every tree and at every split. But it also only uses a subset, a random subset of
the data inside of your training set. And because it only uses a subset, there's a set that's left.
And that's like having a training and a test set inside of your training set. You can also refer to that
in other architectures, we refer to that as a validation set. So you can break up your training
set while the model works also into two sets. And that's then called training and validation set.
And as far as random forests are concerned, we call that out of bag values, the extra little ones that
are then not used in each one of the trees. So every time it uses those values that were not selected to
be part of the trees that you are building in the forest and it tests against them. So what we want to
see as we go through building more and more trees, there's the accuracy there. And what you see is,
as the model builds more and more trees, more and more trees, the accuracy goes up and up and up and
up. Okay, there's a few dips here and there, but it goes up and up and up. So as the model builds,
and you can see the trees there, we went for 300 trees. You see, as your random forest gets more
complicated, more information from more trees, your accuracy starts going up. And then we have the log loss.
And that is an indication of the error rate that's going down. So it's getting more and more accurate
as it runs. So let's, we have this idea of the out of bag errors and the accuracy then, because we
compiled it with the metric accuracy. So we get that accuracy information. We still have our test set,
remember, so we can have a look at that set as well. So it's a sort of two ways that we're testing our
model here. So I'm going to create this computer variable evaluation. And my trained model now is
RF underscore model. And I'm going to call the evaluate method on that. And I passed my test set.
Remember, my test set is a TensorFlow data set. We used, you know, we inserted our data frame and it
spits out a data set ready for use. And return dict is true. That just gives us a dictionary of metrics,
because that's what we import in important files. So now it takes our test data, runs it to all those
trees in our random forest. And let's just have a look because we set return dict equals true.
We get the keys, which is loss and accuracy, and then the values for each. This is now for the
test data and the loss was zero and the accuracy was 100%. Absolutely phenomenal. Random forests are a
very powerful approach to machine learning tool. And the more sophisticated form of those using XGBoost
can be even more so. And I really encourage you if you're interested after this fundamental
data science course to really look into this. Very nicely, it can give us some overview of our decision
structure, how this was done by using the tfdf.model underscore plotter dot plot model in colab function.
Right. And that gives us an idea of how the node structure eventually panned out as far as
the overall view of our random forest is concerned, giving us some overview of the combination of those
trees. So very good information. You can see flipper length, remember that was highest up,
when we looked at the importance of all these variables. And you can see that was chosen then
as the root. And you can see the three classes here. So an impure node to start off with. And what we're
trying to get to do is to get to pure nodes. And you can see there's a pure node, there's a pure node,
and there's a pure node. So when we pass the test data, I put it through this sort of idea here.
And then we can use the variable importance, and that'll give us that same idea, flipper length,
then build length, and then build depth, and then the island, and then the body mass. Very, very nice,
very nice thing to do. We can also call from our RF model, the make inspector method. And from there,
the evaluation method, and it just gives us all this information that we've talked about before.
And that's it. An introduction to data science. It really is a massive, massive field that gives us
the ability to tackle the problems that we're dealing with. And data science has been very
successful in democratizing working with data and finding solutions from data. In the end,
finding the story that the data is trying to tell you.

so in this next video we're going to look at distributions when you collect data point values
some occur more commonly for variable than others and there's some pattern to this distribution
we've all seen the bell-shaped curve from the normal distribution where it's nice and symmetric
that bell-shaped curve but there's lots of others more importantly in this video i'm going to talk
to you about sampling distributions and that is going to be at the crux of the matter that allows
us to it really allows us to do inferential statistics and what you're going to see what
that is all about is that if you go out and you do some research and you have a bunch of subjects
and for some variable you collect all their data and you calculate a mean for that variable that
mean is only one of many possible means because think about it if you started your study one week
later you would have had different subjects in your study and you would have different data
points for that variable and you would have a different mean and if you started another week
or if you did your study in some other place every time you did that exact same study you would have
gotten different data point values and a different mean and all of those means also if we could do a
study millions of times over would also be part of a distribution and that really is at the heart of
the matter that the mean that you find or the difference between two means is just one of many
many possible ones and what we are looking for if we have a small p-value is one of those differences
that is very unlikely to occur that if we look at a bell-shaped curve would fall somewhere out here
maybe you only do your study once but there's some beautiful mathematics that allows us to estimate
where our one study would fall where our difference in means or or mean or whatever your study is about
where it would fall on some curve so a very important video this we're going to use the i'm going to use
the notebook write some code and you'll see you'll start to understand first of all just distributions
the patterns in your data but then more importantly the sampling distributions
i've named this notebook distributions and if we scroll down let's look at these libraries that
we're going to use from numpy we can we import numpy as np as always from scipy scientific python
we're going to import the stats module that's a module inside of the library scipy library and it
contains many many many statistical functions and when we actually start doing some real statistical
analysis that is the module inside of the scipy library that we can use
from pandas i'm just going to import data frame and then from iter tools that is part of python it's
not something that you have to install separately import chain we're just going to use that to
what i'm going to call flatten our arrays there are more than one way to do it i'm just going to show
you this one just for an extra tool in your arsenal and then from the math library we're going to import
the factorial function so i'm going to go ahead and import all of those and then i'm also going to
import plotly.graphobjects as geo as we've always done plotly.io is pio and plotly.express
as px and then plotly.figure underscore factory that's a new one as ff so we're going to use
some of the plots or one of the plots at least that's available in that and i'm going to say
pio.templates.default and i'm going to set that to plotly.white so that we get nice white figures
now the first part here of this notebook remember you can download it from github and there is a video
on this playlist of how to do that and i'll leave that in the description down below it's all about
probability theory and this is the basics things that i would assume are just common sense about
probability theory and we're just tossing a coin there and it landing head up and tail up and it's
really easy a bit of code to simulate you throwing or flipping the coin many many times over and seeing
what what lands face up and head tails up and heads up so just some basics about probability theory
that you can go through in your own time because this video is all about distribution so let's start
talking about random variables and even though i've used the flipping of a coin there and the
probability theory what we're going to do here and i'm going to talk about is the rolling of a die
but not only one die two dice a pair of dice we're going to stick to the term dice because it just
rolls off the tongue a little bit better than die or just sounds better at least so what we're going
to do we've got our pair of dice normal dice with six sides one two three four five six and we roll
the one and then the second one and we note on the first one what lands face up we note what
lands up on this the second die and what lands face up and we just add those two and there you can
see the 36 possibilities and i think it makes intuitive sense i can roll a one on the one and
the one on the other one and i add that and i get two so two would be the lowest number and on the other
end of the spectrum i can roll double sixes and we add those two we get 12 so the possible outcomes
here are from 2 to 12 and this outcome we're going to call a random variable i don't want you to get
confused with what we're doing here as far as the definitions and terms are concerned
random variable remember we've used the term statistical variable the variable so age is a
variable your cholesterol is a variable and you can collect data point values for a bunch of your
subjects for that one variable and that one variable has a type but you'll come across the
term random variable and the random variable actually refers to the actual data point values
they can come at you at random if you take a bunch of participants in your study they'll all
have different ages and every age is a random variable what i like to call when i explain these
things a data point value that's a random variable random variable actually the definition goes a bit
deeper because it is actually a function remember y equals x squared it's actually a function and it
maps some outcome to a value that we can actually jot down and that's exactly what's happening here
i've got some experiment running i roll one die and it's the second die and i add the two and the
function maps those two values i add them and that maps it to something that i'm capturing so if i roll a
three and a four that's a seven and i capture that seven and that's my random variable so just
mind these terms that thing that i capture in my in my spreadsheet file if that's how i'm collecting my
data that is a data point value that's the way i like to describe it but in actual fact it's
the outcome of an experiment i take someone and i ask them their age and the random variable would
be the age it's the outcome of an experiment i've taken that person and i've taken the age anyway
we leave the definitions there i think we needn't allow it to confuse us so there they are i can roll
it two three four five until twelve but i am going to now map this function thing and this value this
outcome to a variable and we're going to call that variable x so this uppercase x i might have called
in a long word with underscores in between as my column header the sum of two two dice whatever but
just to keep things short i'm just going to call it x so x is my as our random variable then i can
take these values my sample space elements is anything from two to twelve but we note that
certain numbers have more than one way of getting to them there's only one way to roll a two and
that's double ones there's only one way to roll a twelve and that's double sixes but there are two
ways to get a three the first die can be at one and the second die two or the other way around the
his die could be a two and the second one one that's two separate things i mean the die fell on
individual ones fell on a different number so there are two ways to get a three there are three ways to
get a four there are four ways to get a five etc and the most common one is a seven so if there are
more ways to get a seven we would suggest then that well yeah you are more likely to roll a seven than
you are to roll twelve or two because there's just so many more ways in which those die can fall
the dice can fall that will give you a seven so if we look at all those 36 we we we we say the
probability of x x meaning the sum of the two faces lining up of the probability of x being two
there's only one out of the 36 ways to get a two so the probability of you rolling a two is just one
over 36 and the same goes for 12 it's only one over 36 but if we look at the probability of x being a
seven there we go of x being a seven that's six over 36 that's much more common so let's look at
these probabilities i'm going to use list comprehension here so you see here i'm going to
use these outside set of brackets and i'm going to say i divided by 36 for i in one two three four five
six five four three two one that's the ones i have here the one two three four five six five four three
two one in the end so i'm just going to run that and that's going to give me this these probabilities
so there we go we see this highest probability is at 16.7 percent here and that's to roll a seven
16.7 percent you have a 13.9 percent then to roll a six or an eight and it goes all the way down to
about a 2.8 percent probability or likelihood we should really say of rolling at a two or a 12
and there's a nice hierarchy there's a nice pattern and the word we can use is a nice probability
now the nice thing about these probabilities also they sum to one in other words you can't roll a one
or you can't roll a 13 that's impossible from 2 to 12 that encompasses the whole lot is it's um
mutually exclusive and collectively exhaustive is the term that you'll come across mutually exclusive
in other words rolling at 12 and rolling at 2 that's two separate things and and i include all of
them from 2 to 12 and these probabilities and then and they collectively exhaustive in other words
and that's all there is there's nothing other than that so if i sum all those probabilities i better
get to one and because they're mutually exclusive it means i can also ask what is the probability
of my outcome being 10 or more and that's what we see here what's the outcome that's the probability
of being 10 or more and i just add those probabilities of the 0.083 the 0.555 the 0.031 well i should just
check that that's a little typo there and you can add those probabilities up there and there you get
there we we uh we add them all up and we say that's 16.6 so really i've got to correct these typos
that slipped in here it's an ideal opportunity to do that let's put in there 16 that's 16 0.166
and we said this one over here was instead of 0.31 it's 0. i don't know how that slipped in but let's
make it 2.8 and you see how we get these little um nice mathematical typeset here if i put two dollar
signs next to something that in and tells the notebook here that i want mathematical typesetting
using what we call latex l-a-t-e-x you can also pronounce it latex i suppose but latex is the
perhaps the proper way to do it and i can add all these symbols so to have a left parentheses is
backslash left and then the left parentheses and then greater than or equal to is backslash ge
and the right parentheses is backslash right and then right parentheses and i enclose all of that
in a dollar notation so that when i do run this i get this nice mathematical typesetting there
but we see the probability of rolling at 10 or more at 16.7 percent now let's do something let's
simulate our experiment because this what we have up here that's a theoretical distribution
and we'll see now there's a difference between a theoretical distribution and an empirical
distribution and the empirical distribution is the one that you actually found when you did your
study so here's our study we're going to roll this die a couple of thousand times if i scroll up
there we see 10 000 so what i'm going to say is np.random.seed and i'm going to use three if you use the
same integer there you're going to get the same random values back i'm going to lock this all up in a
data frame my data frame is going to have this one column the role total and i'm using as you can see
here i'm using a dictionary now don't worry just look at this code you can just copy and paste it
and play around with it yourself this is not the type of code we're going to do when we do the actual
statistical analysis i've been promising this for so long but we will get there so i'm losing list
comprehension for my 10 000 rows of data i'm using random integer between one and six so the highest
seven remember the highest is excluded and i'm rolling two of them at a time so give me two random
values back between one and six it's exactly what we're doing with the die and that's all passed as
an argument to the np.sum function so sum those two roll two sum them roll two sum sum them
and convert that to a list so that we can just have this list of items and that is part of my
data frame and i'm doing that for i in range 10 000 so from zero to 10 000 excluding the 10 000 but
that gives me my 10 000 values so let's just look at the head of this series more it's more of a series
than a data frame because i only have this one column and there i see my first roll was a four my second
always a six then a two then a seven then a 10 if you use that same seed here you're going to get
exactly the same values if you don't put the seed there or you use a different number you're going
to get different values so let's look at the value counts method so roles underscore totals that is my
data frame name dot roll total that's my single column so i'm getting back a series and i just created
it as a series anyway and then i use the value underscore counts method and i set that to normalize
equals true because i want back the probabilities and there we go this is our empirical distribution
our empirical distribution that is the distribution of our actual 10 000 values versus the theoretical
distribution now i showed you a way before of how we can sort this the quickest way because my numbers
that i'm rolling i created as a series so that the index is actually the the total values so if i just
say dot sort index if i use the dot sort index method i'm going to get them back in a sorted order
and there you see two three five six seven eight nine two twelve and there you can see the actual
how many times of that 10 000 so you can see there the 16 1699 and that's very close to the
the 16 what was our theoretical distribution there's 16.7 so we get a 16.9 or 17 there
so we close our empirical distribution is very close to our theoretical distribution and the larger
that 10 000 number the more we make them the closer it's going to get to the theoretical distribution
there we go let's just create a quick bar chart of this i'm going to use plotly express and we
imported it as px and so i'm using the bar function there my x range is from 2 to 13 remember the 13 is
excluded so that'll just be 2 to 12 and my y i'm going to put this um the the how many of these
there were as far as my bar chart is concerned so there we go a beautiful distribution you see the
pattern the seven occurred most commonly and the further away you get from seven on both sides
the less commonly they occur so there were 282 twos and on the other side there were 299 12s but the
1699 7s that is a distribution your the random variables came in a certain pattern some were
more likely than others and that's why you get games of chance with dice and based on the number seven
there are games like that so let's look at these random bit closer or more examples of this random
variable distribution so i'm going to create a computer variable called height and there we go
get height i'm going to use from stats i'm going to use dot norm dot rvs now this is beautiful this
rvs function in as much as it allows us just to create random variables as if we were doing a study
it's not always possible for you to go out and get data on on some subjects or participants sometimes
you just want to simulate some so that you can exercise and this is a great way you can try and read
up on this rvs so stats dot norm the stats has many many name distributions one of them is the normal
distribution so i'm going to say stats dot norm and then the rvs means give me back some random
variables from this distribution and i can set a few arguments there loc lock that's the mean so i'm
saying give me a mean of 160 a scale of 10 that means standard deviation standard deviation of 10 and i want
200 of them and i'm setting a random state equals one so if you do the same random state you're going
to get the same 200 heights back so i'm simulating here the height of 200 people and so that the height
is follows a normal distribution that's the bell-shaped curve we'll look at that with a mean of 160 and a
standard deviation of 10 okay boom it's easy as that i have my 200 values let's create a plot of this
i'm going to call my plot height underscore hist for height histogram and we're going to use that
figure factory one of its plots is the create underscore dist plot very nice so i'm going to
pass height i've got to pass a list of lists so there's the square bracket there and the first list
that i'm passing to it is height it is a numpy array actually it's not a list so i'm passing that as
one element inside of a list and i'm going to call it height and my bin size is i'm setting that to
five let's plot that i'm going to see it's a beautiful plot this create this trade this plot
it's called a distribution plot so what you can see is the normal histogram there and it seems most
people were about the 160 mark just as we said we wanted that mean because we've set the bin size to
five it's actually grouping too many people into one little bin and you can see that that the bins
are actually five years apart but what you can also see is this nice idea of this smooth curve
and this is based on the data itself so it's not a theoretical normal distribution this is
an empirical distribution it's called a kernel density estimate and it just uses some mathematics
just to smooth out this curve so that we get this idea of the spread in the data better than we
might just get from the histogram and what you see at the bottom is a rug plot r-u-g that gets added as
well and that's just all the individual 200 values that we have we can see they densely dense around the
160 mark and they further out and because we've used the norm function here that's not norm it was taken
from a normal distribution that's exactly what we would expect and here i talk a little bit about
the empirical distribution and the theoretical distribution so this is our empirical distribution
the actual data versus some theoretical distribution there are some moments about these distributions you
can read a little bit about them i've written something here about the expected value and a little
bit about skewness and you can certainly read up on that we're not going to use that when we do our
actual data analysis i don't think that's quite essential but i've put that in there for you
if you if you want to read about it so let's get to some actual some actual work we're going to look
at the theoretical normal distribution and the one we're going to start off with is not just the
normal distribution but the standard normal distribution very special distribution and statistics
a lot of what we deal with are based on the standard normal distribution so the standard normal
distribution has a mean of zero and a standard deviation of one so very prototypical i'm going
to create two very mystery computer variables here lower and higher on the lower is stats.norm.ppf
0.01 and the higher is stats.norm.ppf 0.99 i'm going to set that to lower and higher i'll keep it as a bit
of a mystery and you can see it's the symmetrical about 0 0.0 negative 2.326 and positive 2.326 on the
other side so symmetrical around the zero mark and you can start thinking to yourself what this is
going to be about remember i used the term mutually exclusive and collectively exhaustive such that if i
combine all my probabilities they'll sum to one now that was okay for discrete variables like the rolling
of a die but what about continuous variables remember with a continuous numerical variable i can't say
that 0.4 means anything because it can be 0.399476 very close to four i can infinitely divide my numbers
so an actual single number makes very little sense it is more about between two values and you'll see
what i mean a little bit later but look at those numbers 0.01 and 0.99 what they what i want you to start
thinking about is think about this bell-shaped curve in your head and imagine we can calculate the area
under the curve and you might remember that from calculus that's the integration but don't worry we're not
going to do integration but think about the area of a circle by r squared or the area of a cube
one side squared or the rectangle the base times the height every area geometrical area has a has a
has an a geometrical shape has an area but this nice little curve we drew here with a this plot
it also if i if i looked at the area under the curve we're going to do the same here with this very nice
curve for the normal distribution standard normal distribution and the area under the curve is going
to equal one 1.00 and remember that's just the area like the area of a triangle or the area of a
circle that's a physical area and this nice little bell-shaped curve that we're going to draw from what
we're going to call a probability density function is going to have an area under the curve of one
and if you think about it goes down to the x-axis on both sides it gets flatter and flatter the further
way we get from the the bulky middle this 0.01 and 0.99 that's going to if you think about it it's going
to be some marker on that bell-shaped curve for which to the left of it i have one percent of the area under
the curve and on the other side if i go from that towards positive infinity side i'm going to have
another one percent of the area under the curve okay just just have that in your mind now here again
this has nothing to do with the code that you're eventually going to do i'm just going to create this
values variable computer variable and i'm doing from lower to higher so from negative 2.326 to positive
2.326 and i want 100 values in between that i'm going to flatten that we need to flatten it and
that's why i use this iter so chain dot from iter values and that just flattens it as a single list
of values you needn't worry about that we could also have just said values dot flatten see there
values dot flatten that would be another way to do it and i'm just going to show you the first
five values so we started negative 3.26 negative 2.279 negative 2 so i've just divvied up from negative
2.3 to positive 2.3 into 100 values and i just want them as a list that's all this thing is here
is doing just flattening it as a single list but i could also just have said list and then values dot
flatten open close parentheses and now that i have these 100 values i'm just using them and i'm going
to create this probability density function values for them so i'm going to say pdf underscore values
equals stats.norm.pdf values i'm going to flatten it again by this using the iter tools but as i said
you can just use dot flatten and now what i get for every value here i now get the corresponding value
for this probability distribution and though while it might not make much sense these words that i've
done here this is what i get from that this is what it's all about so if you look very closely you see
it's just a bunch of little straight lines but if you move far away it looks like a smooth curve the
reason is i've only simulated 100 points here but plotly is doing its best to make it look like a
curve and there's the standard normal distribution and you can see most values are at zero and the
further away we go the less likely a value would be if i were to draw randomly from this distribution
and what we had with the negative 3.2 and the positive to negative 2.36 positive 2.36
and we're just these values here as cut off
because i've just asked it to plot from there to there but think it also it's it extends really
to the left side and extends to the right side such that this whole area under the curve from negative
infinity to positive infinity is 1.00 100 of the area under the curve but if i were to just go to
this little point here on the left side and this little point here on the right side to the right
of this would represent 1 of the area under the curve to the left of this would represent 1 of the
area under the curve and the area under the curve from this point to this point is 98 98 so that's what
that dot ppf is doing it just gave me a little markers on the left and right hand side just to
choose between now let's have a look at this figure it's a long figure that i'm going to do here
for the normal distribution and again i'm just going from this negative 2.3 to 6 and positive 2 point
what i want to show you though is this imagine that i do a research study and i get two groups of
participants one group got one intervention and the other group got a placebo intervention and i have
some variable and i calculate the mean for each and i look at the difference in the two means
that difference is going to fall some way on this distribution now here i've simulated that the
difference between the two groups the means of the two group is negative one i've got to be slightly
careful though because if i subtract the lower from the higher i'm getting a positive value but i can just
as well who said the placebo group has to go before the active group i can subtract either one from either
one and then i'm going to end up with a positive or negative number and that's just by by the way
which i decide which to subtract from which and that's why we've got to actually reflect this red
line on the right hand side as well so you can imagine here on the plus one that there is a line
going up here as well as well as the negative one and what we now say is if we look from this negative
one towards the left and from positive one towards the right and we look at all this area under the
curve to the left of the red line and to the right of the imaginary red line on the other side that is
my p value that's actually what a p value is it's an area under the curve so what we're going to do is
we're going to have this theoretical distribution that we're going to work with with students t-test we're
going to use the t distribution i'm going to show it to you here we just have the standard normal
distribution and we're going to say that our difference in means was minus one or minus whatever
we plot it on our chart we reflect it on the other side and we work out the area under the curve towards
the outsides and that's a p value that's exactly what a p value is so i'll let that sink in a little
bit it's actually as simple as all of that now to calculate this area under the curve we're just
going to write a single line of code you're never going to do all of this but what we do is we create
a cumulative distribution function so you see the cdf and let me show you what the cdf is going to look
like when we plot it it just starts counting from the left hand side so we start here up here
with nothing of the area under the curve and as we go along we pick up more and more and more and
more until we get to the end which is then 100 of all the cases so that's a cumulative distribution
function and from something here like navigative one we can just read off exactly where it is there
and you see the 0.156 that'll be the area under the curve to the left of this negative one
and what we usually do here or what we do here is we just multiply it by two because we have symmetry
on both sides we're going to multiply that by two and that's our p value and that's exactly how
the p value is calculated so i say stats.norm.cdf for negative one that's 0.158 i multiply it by two
because i've got the symmetry on the other side and that's my p value if my difference between the
means followed a normal distribution and i found a difference between the two groups of negative one
i would get a p value of 0.32 and later we'll see that is not statistically significant because we
want one of those rare outcomes so you've learned a little bit more now it's all going to it's all
and we're going to reiterate this by just using the actual statistical analysis and all of this will
come back to you a little discrete distribution the one that i do want to show you is the binomial
distribution because it's quite a bit of fun i'm not going to go through these functions that i've
created but what we're going to talk about with a binomial distribution is that you have a categorical
outcome that is binary it's only two possible outcomes sample space only two elements so imagine
you're taking a drug and we're just looking at did you get a side effect did the participant get a
side effect yes or no and that's all we do and um to do this you see the little i've created two
functions they do exactly the same and they're based on this equation for the probability
so how it works is the sentence i want to show you let's calculate the probability of two side effects
among 10 people given a probability look at that horrible spelling of a side effect in any
individual of 0.05 so just think about it i've got this drug and we know that if someone takes that
drug they have a five percent probability of getting a side effect every individual five percent so
studies were done in efficacy or safety studies were done before given to a couple of hundreds of
people and we notice that five percent get this so my research question is yes i take 10 participants
if any one of them individually has a five percent probability of getting a side effect that's five
percent so my question is what is the probability that out of those 10 people two of them have a side
effect that's a valid question and from these functions that i've created there i put that in my binom i've got
10 people i'm interested in two of them and any one of them had to have a five percent likelihood of
getting this and i can work out the probability of two of them getting it and the probability is very
low it's only 0.75 7.5 percent i should say it's 0.075 7.5 probability of two amongst the 10
have a side effect it'll be different if i wanted three or four and that's going to get less and less
because each individual only has no point five percent but you can see i don't just multiply
0.5 or five percent by two now i say there's a 10 that's not the way that it works that's not the
way a binomial distribution works by the way you'll come across terms like failure and success
success is the thing that we're after and that's the the probability of success that we put there
and but success and failure has nothing to do with the actual words english words their definition
success is the thing that you are interested in so it might very well be that i'm looking at survival
and death and the thing that i want to investigate is the probability of death
success would then be the death so don't read anything into the term success and failure
it's just that the outcome the success is the probability that i'm that i'm concerned about
so look at this below so i'm creating a list of object a list object using list comprehension
looking at the probability of zero one two three and four side effects in the 10 individuals
so what was the probability of none of them none of those 10 people getting the side effect given
that each individual's probability is five percent what is the probability out of those 10 that only
one people one person gets what's the probability we've already seen that two people get it that
three people get it that four people give it and i can use the binomial distribution for that
so let's just have a look this is just for interest sake and you see that the highest probability is for
none of those 10 people to get it and that's at 60 percent there's a 31.5 percent probability that
one person gets the side effect and then 7.4 as we've seen 7.5 for two people and really goes down
the chances that five people get the side effect having only a five percent chance for each individual
is very very low so let's have a look at that and i've plotted some of them and you can see this
the k here that is the binomial distribution so it's actually quite a bit of fun and you see
some of the moments there for the binomial distribution we now come to the most important
part of of this lecture and that's sampling distributions and that's going to tie everything
for us together as far as how p-values really work so what i'm going to do is i'm going to create
a population so i'm going to have some delusions of grandeur and imagine that i have my brain own
brand new planet and i am the ruler of that planet so much so that i'm a deity and i create everything
on that planet on my planet there's only 10 000 people and or beings that i'm creating and they are
they are all they have a certain height how tall are they so i'm going to say population underscore
heights my computer variable and i'm going to simulate them being a height of 140 centimeters to
200 centimeters and remember this is a uniform distribution so 140 is just as likely as 146 is
just as likely as 170 i don't have a mean and a standard i don't have a normal distribution here so
that they're all clumped up in the middle so it's this equal spread over 10 000 and an empirical
distribution is going to not completely be a uniform distribution but very close so there's my 10 000
beings on my planet and you see their different heights and you see it's not a bell-shaped curve
some heights are just as likely as other heights it's this is an empirical uniform distribution this is
very close to a uniform distribution so i am now going to do some tests some studies on my 10 000 beings and
what i'm going to do i'm going to select a hundred of them at random and i'm going to measure their heights
and i'm going to save the mean of those hundred heights and then i'm going to chase them all back
to their village their villages and tomorrow i'm going to gather another hundred of them at random
so some might be the same as before it might all be brand new hundred and i measure their heights and i
capture only the mean of those hundred heights and i chase them back to all their villages
what a colorful example i'm coming up with anyway and i do this a thousand times over i'm in charge of
this planet i can do what i want so that i have a thousand means that's what i've done here with
less comprehension mean heights nprandom.choice i take from the population 100 replacement is false
in other words that 100 is 100 individuals i don't take one individual and throw them back in the
in the pool so that i can choose them again but at the end of the day after i've done my 100 they all
go back in the pool because i do this 10 a thousand times over so there's i'm simulating there my thousand
heights now remember this comes from a uniform distribution it wasn't like most of them 170 in
height and so now i have these thousand heights thousand means in other words i have a distribution of a test
statistic the mean is a test statistic it is something that i've calculated from a sample
and i have this distribution of means now i wonder if you can guess what's going to happen here
so you can see there from the actual beings out there on my planet they are just as likely to be two
meters tall than they are to be 140 meters tall they're not bunched in the middle and i take a hundred of them
calculate the mean another hundred calculate the mean and i've just now got a list of a thousand
means that's a sampling distribution a distribution of a test statistic not of the actual values
themselves and you might have guessed what this is going to look like but let me draw this as a
histogram for you and here we go look at that there's a different pattern to this this is no longer
uniform distribution my sampling distribution of test statistics in this case is a mean is almost
normally distributed from something that was never normally distributed before that is amazing and that
is part of what we call the central limit theorem and it allows us to do a lot of inferential statistics
and use parametric tests that and here's the crux of this whole video the whole video is that you
only get to do one study and imagine that study then calculates the difference between two means
if you could do the study 10 000 times over or a million times over which is financially impossible
time constraint impossible you only get to do your study once but your finding is one of many many many
possible ones some of which are much more likely than others no matter what i say that in quotation marks
inverted commas you're going to have your test statistic your difference in means is going to be one of many
and it's usually going to be based on this normal distribution and what you are hoping for is the one that you
find is one of the rare ones and if it is one of the rare ones all of this is going to be an area under
the curve all of this is going to sum to one and yours is going to be one of the unlikely ones and
then you have a small p-value and you say well there's a statistically significant difference between those
two groups as simple as that
so the distribution that we've seen before the standard normal distribution we use that in what we call the z
distribution and we change it from standard normal to the term z when we are referring to the fact that
we are looking at this statistic distribution we're looking at a sampling distribution of sample statistics
then we change the word standard normal to z distribution but it is really going to be based on the z
distribution my difference in means are going to fall somewhere i duplicated on both sides replicated on the
on the symmetric other side and i calculate my area under the curve from that cutoff towards the outside
now this the z distribution does require that i know the standard deviation of that variable in the
population now we're seven billion people on the planet and for most variables we don't know what
all seven billion values are so in that case we make use of the t distribution from william gossett
in the beginning of the 1900s he worked for the guinness brewing company he worked out some
statistical tests for small sample sizes and the guinness brewing company wouldn't didn't want him
to let all the secrets out the bag because they were using that in their own business but he persisted and
he wanted to publish academically and they said to him well you can publish but then it has to be under
a pseudonym and he chose student they might have given him student and teacher as two possible ones
to to choose from whatever the situation might be he chose student as his pseudonym and hence we know
the student t distribution or students t test his real name was william gossett then from the guinness brewing company
so i'm going to use stats.t.ppf in this sense so not stats.norm but t and the beauty of of his his mind
and what he created was this idea that we don't care about the standard deviation out in the population
we only care about how many participants are in our study so based only on that do we draw this nice
bell-shaped curve so here i've got to pass two arguments the 0.01 and 0.99 just to get those two
cutoff points of one percent and 99 but it's based on i want those value based on there being 30 participants
in my study so i've got to say 30 and i'm going to create that same 100 values and i'm going to create
the pdf values and then i'm going to plot them and there you go and you see the t distribution
distribution looks very much like the normal distribution but it's something we can use when
we only have 30 participants in our study or fewer and that's the beauty of it all now when you get
to more than 30 these two distributions are very close to each other and when you have really large
numbers you might as well use the z distribution you might not use the t distribution but by and large
we stick with the t distribution and let's plot this one here and i'm comparing the z's and the t's
and there you go you can see they're very close to each other except when we get to the tails here
that i mean it's small here on the screen but that starts to get quite significant and if we talk about
you know p value of 0.05 this gets a bit crucial so we usually stick to that which brings us to this idea
of the confidence intervals so we might say that we do a study we capture cholesterol on 100 people
we express some mean for that group and let's say that the mean for our cholesterol in international
units was 7.4 but i said to you we're talking about inferential statistics so you want to
infer your results on a larger population or someone else's population so you you want to know what the
real if if you were to look at all the people what do you think the real mean was compared to the mean
of your 100 participants in your study so we have this plus minus idea so the 7.4 plus minus two for
instance and that is what we call a confidence interval and if we say a 95 confidence interval
you've seen that many times in the literature we create this bounds of 95 either side of 7.4
and we suggest that the population mean would be between those two upper and lower bounds
we are not 95 be very careful here we're not 95 confident that the real population mean is between
those what it really means is if i were to do my study 100 times over every time i'm going to get
a slightly different mean and i'm going to get slightly different confidence intervals up and lower
bounds in 95 of them the real point patient population population mean would be within those limits that
i've said and in five of them it won't be you don't know the one that you have whether it is or not
so you can't say i'm 95 confident that the real patient the real population mean is between these
two values no no no you just it doesn't work like that is if you could do it a hundred times over 95
of them that will be slightly different we'll actually have the real population mean in it
and five won't so how does this confidence interval work does work where we take our mean and it's plus
minus some value and we do this value by the z or the t and i say the alpha divided by two and what
that means think about this nice bell-shaped curve i want 95 confidence intervals which means 100
minus 95 i mean means i've got five percent left and if i divide five percent in two i get 2.5 on the one
side and 2.5 on the other side and those are the red lines i can draw at minus the one side and positive
the other side such that on the left side 2.5 of the area under the curve will be on the one side
towards negative infinity and 2.5 will be on the positive side to positive infinity that is how i get
95 percent of the area under the curve in the middle and we multiply that by the standard deviation of
our sample divided by the square root of how many people or participants are in our study
so let's get the norm ppf of 0.975 and i say 0.975 because 97.5 if i add 2.5 to that i get to 100 so
that will be on the right hand side and you see that's 1.96 you'll come across that value many
times 1.96 1.96 standard deviations away from the mean of a standard normal distribution or z
distribution then i should say will give me 2.5 percent of the area under the curve towards the
right and negative 1.96 on the left towards negative infinity will give me the other two and a half percent
such that combined i get five percent of the area under the curve which means in the middle i've got 95
percent of the area under the curve so let's draw that and you can see there that's exactly what
i've done so um and we've done this here for the z distribution in other words we have a mean of zero
and a standard deviation of one but you can see there where the two cutoffs are so that to the left of
this red line the area under that curve will be two and a half percent of the total and to the right
will be two and a half percent of the total so that in the middle i have the remaining 95 of the area
under the curve and that's where we get these values for 95 under the under the curve so here we've done
it and i'm multiplying by two and i'm dividing by the square root of 60 imagining that i have 60
participants in the study and a standard deviation of two can we calculate the confidence value of 95
percent so there's my confidence level it's 0.05 060605 etc now i subtract that from the 7.4 remember our
study had a 7.4 mean for our cholesterol and i added to that and now i get the bounds for my 95 confidence
interval and let's run this one as well it should print to the screen there and there i see 6.89 to 7.9
so in my study i can write the average cholesterol level was 7.4 95 confidence interval 6.9 to 7.9
a 95 confidence interval of 6.9 plus 7.9 and that would be exactly the same as what we did here we
just converted it to that we do have the 7.5 in the middle as instead of the zero but that's exactly
how we will do it again we won't have to do any of this this is just to explain to you what we're doing
we are just going to write a simple use a simple function inside of scipy.stats and it's going to
do the confidence intervals for us we need to do this or some other function i'll show you a couple
of ways to do that so let's do just this i just want to show you here for the t distributions i'm
going to say stats.ppf t.ppf and because i have 60 participants and that leaves me with something
called 59 degrees of freedom and i won't go into what degrees of freedom are and just take it for
now that it's true but you can see with 60 participants and a 7.4 mean i'm going to get
to almost exactly the same or very nearly exactly the same confidence intervals whether i use the
z distribution or t because i've got 60 now it's more than 30 so these two distributions get very
close to each other but the same thing applies so there you go i hope that was the explanation
that meant something to you and even if it didn't completely once we start doing the actual
statistical test which is getting closer and closer by the way these things will come back to you and
you'll have this concept in your head very clearly that a p-value is nothing than an area under
the curve the difference between two groups that we find or whatever measurement we do is going to
be one of many possible ones and what we're hoping for is one of the rare ones and it's all based on
this mathematics of these theoretical distributions the z distribution t distribution we're going to get
chi square and all sorts of other distributions and the result and we can calculate all of that
purely based on the small sample that you have but yours will then fall somewhere on that and we
can calculate an area under the curve for that which is going to be our p value

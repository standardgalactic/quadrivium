This is the last video in the seminar series on linear models, where I just discuss these basic
four fundamental model types. And I call them fundamental once again, just to remind you of
the fact that I think that if you understand these four, you can go on to more complex models.
Now in this one, we're going to change things up a little bit. Things are quite different from
linear regression analysis of variance and analysis of covariance. So let me show you
why this is so. I've opened a Jupyter notebook inside of Visual Studio code. Again, you can see
that I'm running Python 3.9.10. And these are the packages that we're going to use. As always,
we're going to import pandas, so that we can create a data frame, we're going to import the stats module,
we're going to import NumPy and Patsy. Now one of the only or the only namespace abbreviation that I'm
going to use is SM. So statsmodels.api, I'm going to import that as SM. And then something brand new.
So from statsmodels.api, I'm going to import this function, the logit. It's actually a class that
it's going to create. And we're going to fit our data to this instantiated class logit. So let's run
that. And later on, we'll see how that works. Here's our old friend, the plotly plotting package,
and nothing other than you have seen before. So I think it's quite important to remember that this
is the fourth of a series. And you really have to watch video tutorial number one, two, and three.
So linear regression analysis of variance and analysis of covariance so that you understand
what is going on here. There'll be a link in the description down below to those three videos,
you can read up about it on the website. Or if you're watching on a computer, I'll put a little card
right up there in the left hand corner. So here we are, we are busy with binary logistic regression.
So that's a regression technique. And what we see here in our familiar old table, we write,
you see the italics right at the bottom binary logistic regression. So instead of an interval
type or a numerical variable as our dependent variable data type, we're going to have a binary
categorical variable. So on or off 01, yes or no, or anything, any categorical variable that has two
levels or two classes. And as far as our independent variable is concerned, that can be numerical or
categorical, doesn't matter. And we're going to try to understand the relationship between our
independent variables. And then we have to figure out a way how we understand this relationship with
these two binary values. Now, it might become a little bit difficult. You know, I can give them
numbers three and 42, seven and eight, zero and one, one and two. So these two levels of my dependent
variable can be encoded with any number. And so we had better choose carefully such that this makes sense.
And the way that we do it is what I've written down here, we see that we can have two levels, we can also
sometimes refer to them as classes of the dependent variable, and we're going to encode them with a
zero and a one. Now there's two of these, and you have to decide which one you encode with zero and
which one you encode with one. And it's a very, very important decision that you have to make based on
your research question. So the former the zero, we sometimes refer to that as the failure, and the one
as the success. So you have to choose one of these two values that you have for this dependent variable,
this binary, categorical dependent variable, you've got to choose which one is the failure,
and which one is the success. And it has nothing to do with the actual definition of the English words
failure and success. It's just that the success is the one that we are after. We're trying to see
which one of our and to what extent our independent variables, how it has something to do with the
success. Now, it might very well be that success is death or recurrence of disease. So as I said,
it has nothing to do with the definition of the word success. It's just the one of the two binary
categorical variable values that we're interested in examining. So we can choose either of the two
levels or the two classes as the success. And we're going to encode that with a one. As I write there,
this should be our class of interest. So once we've done that, we've got zero for failure and one for the
success that we can view these encoded values as probabilities. So we have a subject in our sample
with a failure outcome, as far as the dependent binary dependent variables concerned, we'll code
that with a value zero. And then we can think about it as saying that this specific subject had a
zero percent probability of being in the success or the one level, the one class. So whether, you know,
it doesn't matter what word that was, that was the value for your dependent variable, one of the two,
if that's the failure, they had a zero percent probability of being in the success class.
That should be very, very clear. And a subject in the sample with a success or one value for the
dependent variable, they had a hundred percent probability of being in the success or one level
class, level or class. So now we can build a model that will use the independent variables and estimate
a probability of being a success or one. Because now we have this very nice interval from zero to one,
and we express this as a probability. What is the probability of being a one, a success?
Yes. Now one problem we'll have immediately based on making this decision is that we cannot have this
straight line in a graph that is our model, we have to come up with something new, and indeed we will.
So the data that we're going to do in the first two lecture video tutorials, we designed our own
random values, and the third one on analysis of covariance, we actually imported, I showed you how
to import a CSV file. But yeah, in video lecture number four, logistic regression, we're going to
just generate our own data once again. So we're going to investigate a research scenario, and our research
is going to consider whether a participant required a second operation. So we're investigating,
this is all simulated data, we're going to simulate the values. We're investigating surgery of the
abdomen for necrotic bowel. So a piece of the small bowel becomes devoid of blood flow,
and that becomes ischemic and eventually dies and causes sepsis and all sorts of severe complications,
and is managed by doing a doing surgery on the abdomen and removing that ischemic bowel. And there's
various ways we can go about restoration or creating stomas, etc. But some of those unfortunate
patients would require a second look laparotomy, or we call a real look laparotomy. So the index
procedure that first surgery wasn't enough, and they might require a follow up procedure. And there's
various ways that we can go about that we can do that on demand or, you know, as a planned procedure or
on demand. In other words, if the patient doesn't do well, only then do they go back for their second
surgery. So that is going to be our binary dependent variable. Yes, they need a second look or no, they don't
need a second look laparotomy. And we're interested in making the success class the ones that do need
a laparotomy. So you can once again see that has nothing to do with the meaning of the term success.
In actual fact, that would be a failure, wouldn't it? And in some sense. So we are choosing that yes
for a real look laparotomy as our success class. And we're going to encode that as one. And we want to know
from our independent variables, what is the probability given values for them? What is the probability
of the success class of needing a second look laparotomy? And for our independent variables,
we're going to look at the length of the bowel involved. That's termed the ischemic bowel length,
as well as the seniority of the primary surgeon at that first procedure, that index procedure.
And we're going to have three levels of that treatment or three classes for that categorical
independent variable. And that is a senior resident, or an attending surgeon, or someone who's
an acute care surgery specialist. So over and above being an attending, having undergone extra training,
and as an acute care specialist. So that depends really on what part of the world you are in.
There's not always this differentiation between surgeons. But certainly, this is what we're going
to do for the simulated study, we're going to have a senior resident, and then they might be an
attending surgeon, or they might be an acute care surgery, a sub specialist or super specialist,
depending on which terms you'll use. So we're going to create these random values. So first of all,
I'm going to create a numpy array with 30 values of yes, and then 30 nos, those would be as strings,
and I'm going to assign that to the variable dependent. And then I'm going to create a numpy array
of 30 string elements that are either senior attending or specialist. And those are going to have a
probability of 0.5, 0.3, and 0.2. You can see they sum to one, which we have to do in this numpy array.
And I'll show you the code, or you can actually see the code down there. So for these 30, they can
be attached to the yes. So you see, I'm sort of preempting the fact, or I'm simulating this data,
that for those that did need a relook, that there was going to be a higher probability of them being
senior. So I'm just fudging the creation of this data. But that's why I say it's important that you
know how to simulate your data, especially when you start learning these things, because you can
take control over the data and for the specific outcomes that you want, and it helps you understand
that. Then we're going to have another numpy array also of 30 elements, also with either senior
attending or specialist, but the probability is a bit reversed now, 0.2 for senior, 0.3 for attending,
and 0.5 for specialist. So that when those random values are selected, there's a higher probability
that it'll select specialist. And then I'm going to have another numpy array of 30 numerical variable
variables, random variables from a random distribution with a mean of 120 and a standard deviation of 20,
120 and 220. And then another numpy array. But this time, I'm dropping the mean a bit to 100.
So let's see the pseudo random number generator, as you can see there. So there is numpy.repeat.
So that's going to repeat, yes, no, 30 times. So I'm going to use yes, yes, yes, yes, 30 times,
no, no, no, no, no, 30 times. And then the seniority, it's the numpy.random.choice function,
I pass a list of strings there, I want 30 of those. But it's going to be more probable that it selects
senior, probability of 0.5, 0.3 for attending, and only 0.2 for specialist. And I'm assigning that to the
variable seniority underscore yes. And then for seniority underscore no, you know, I just changed
that probability. So now I'm going to have 30 values, and another 30 values, and I'm doing the
same for length. It's the stats.norm.rvs function. Loc is for the mean, scale is for the standard
deviation, I want 30 of those. And I'm just putting that as a first argument in the numpy.round
function, and I want zero decimals, just so that I have integer values of those. Right,
now we can put all of that in a pandas data frame. So I'm going to have three columns, and I add these
to the pandas.dataframe function as a dictionary with key value pairs, the key will be the column header,
or the name of my statistical variable, and the key the values would be the actual values that we just
simulated. So there's going to be a relook required column, and it's going to have those 60 values.
The seniority is I'm going to make a numpy array, and I'm passing this list of 30 and the other 30
values. So there were 30 seniority yes and 30 seniority no values. And then I'm doing the dot
flatten method there. Because I have two separate numpy arrays, I want to combine them and then flatten
them so that they have one array. That's what the flatten method will do. And then I'm going to have
ischemic bar length and I do exactly the same numpy array of this Python list, which is two numpy arrays,
and I flatten them so that they 60 values all in a row. So there's my data and it's now it is now
in a pandas data frame. Now we've got to think about our base classes, the class for our categorical
variables against which the other is measured. Now both the relook laparotomy and seniority,
they're both categorical variables. So we're going to use the pandas categorical function to indicate
that these are categorical variables, and we're going to state the base class. So when it comes to
seniority of the surgeon, we're going to choose the senior resident, the most junior of those three
categories of surgeon as the base class, and we want to measure being an attending and a sub
specialist or super specialist against the attending. I should say against the junior surgeon, which we
term the senior resident. So you have to choose that base class. And we've seen that before when we did
dummy variables, one of them has to be the base class. And then when it comes to our dependent variable,
we're going to choose yes, as our, our success, that is the the class of our categorical variable,
our dependent binary dependent variable that we're interested in. So this is what we do. So we say
df.relook required, that gives me back a pandas series object, which I'm now going to overwrite,
I'm going to say pandas.categorical, take df.required. And these are my categories, no and yes.
So the one that you state first here is going to be the zero. And the next one is going to be yes. And
because yes was relook, that's the success class. Again, success has nothing to do with the definition of
the word success. So relook laparotomy, that's going to be encoded with a one. So I've got to put
them in that order. And when it comes to seniority that I'm overwriting, df.seniority, the categories
are now senior for senior resident attending and specialist. And so this senior is listed first,
and that is going to be the base class. And we're going to measure the other two against this base
class. So that's a very important decision to make. So let's call the info method. So we can see
what our data frame looks like. We've got three columns, relook required, seniority and ischemic
bowel length, each have 60 values in them. And we see relook required as a category,
seniority as a category. So that stands for categorical variables. And so if we didn't change
them using the pandas.categorical function, they'd be objects, that's not what we want, we want them to be
categories, and then a 64 bit float for our continuous numerical variable there.
So let's just do some explanatory analysis of the data. It's always good to understand your data
before you start working on that. So let's start off with the relook required, and we're going to use
the value underscore counts method, that's going to count for us each of the unique values in that
panda series, df.relook required. And we see we find no 30 and yes, 30. It's exactly how we designed it.
So that works out well for us. By the way, as you can see, this type of research, this is not an
experimental design, we're taking data that already existed, and we're just going to analyze that.
So if we use the pandas.crosstab function, and I pass these two series objects, df.relook required,
that column, and df.seniority, that column, I'm going to get a nice table of observations,
a contingency table of observed values. So you see relook required no and yes, and you see
seniority, senior, attending, and specialist, and you can see how many. That's a frequency table.
So it just counts how many people were in that intersection of not requiring a relook and had a
senior resident as their primary surgeon. So that is a contingency table, and we can actually do
a chi-square test for independence. On our hypothesis, there's no dependence between whether a
participant had a relook and the level of seniority of their principal or surgeon in that index
procedure. And it's very easy to do, by the way, in Python. So I'm using the stats module in scipy,
stats.chi2 underscore contingency. And I just pass this table of observed values, and we get back a chi
square statistics there, statistic, and a p-value, which for an alpha value of 0.05, we failed to
reject the null hypothesis. And here is a table of expected values. So that's our table of observed
values. Here's our contingency table of expected values. And we see those two are not that different
in as much as we failed to reject the null hypothesis. So we state that these two variables
are independent of each other. Let's look at ischemic bowel length. We're just going to use the .describe
method on ischemic bowel length. But we're going to group by the three levels of our treatment,
our relook required. And two, I should say, no and yes. We see 30 in each as we expected. And as we
designed, we see the mean for those that did not require relook was 97.9. And the average bowel
length of ischemic bowel was 121 centimeters, exactly as we designed it. Here's a box and
whisker plot. And that shows us this difference between the two. Once again, with Plotly, you can
hover nicely over these two. So you can see what the results are. By the way, if you think about it,
we can look at these two groups that we form by the categorical variable, the dependent variable,
and we can say, let's, let's use those two values to divide our data set into two. And we're going to
compare the mean of the numerical variable in those two groups. So that's just a t-test.
So what I've done here, I'm using stats.ttest underscore ind. So that's an independent
t-test, students t-test. And I say, df.relook required equals yes equals equals yes. So go down,
go down all the rows in that column, find all the values that are yes, and take the ischemic bowel
length. And then the second group, all the ones that were no, as far as relook required was concerned,
and get the ischemic bowel length. So I'm just doing a t-test. And so you can always think of
analyzing your data in this way. And we see a p-value much, much smaller than an alpha value. And we
reject our null hypothesis, which would state that there's no difference in mean ischemic bowel
length between these two groups. And we accept alternative hypothesis that there is a difference.
So just a little segue or a little something different for you there.
Now, before we understand our data, we've created our data, we understand that. And now there's a few
things that we have to just touch base on before we get to logistic regression. So we're going to look at
the idea of probability odds and odds ratios. So the probability is a value that we give for the
occurrence of an event. So the event can either occur with a certain probability, or it doesn't
occur. There's nothing in between it occurs or it doesn't occur, but there's a probability for it
occurring. And remember, as it is a probability, we constrain it to this interval from zero to one.
If we multiply that by 100%, that'd be zero, zero percent to 100%. And we express,
we use this letter p to express the probability of the event occurring. So if the event occurs with
the probability of p, and you know, we constrain it on this interval zero to one, the probability
of it not occurring must be one minus p. That should be simple enough to see. And that gives us this idea
of the odds of something happening, the event occurring. That is this ratio of the probability
over not the probability of the event occurring over not the event occurring p over one minus p,
that is the odds. So the odds of the occurrence of an event is the ratio of the probability that
the event occurs p over the probability that the event does not occur, that's one minus p.
So if we have a specific event, you can see in this example, if we have a specific event that occurs
five times in 15 attempts, so it occurred in five, and it didn't occur in 10. In other words,
that means it has a probability of about a third, because five divided by 15, the total, that's a
third. So that it not occurring would be two thirds, wouldn't it? So there's our p. But let's look at the
odds then of it occurring. That's p over one minus p. So let's save this value p. And that's a third.
And p over one minus p, that's a half. So what we see there's round off error, you're always going to see
that. As you know, if you use Python, some other time, you're going to see it other languages as
well. But clearly, 0.4999, it was just a round off error, it is 0.5. And so the result is a half or
one to two. And you know, that's what you would expect, isn't it? I mean, five, it didn't occur 10,
it did occur, it's five to 10, which is one to two. And we can see there, you know, how the solution,
how we got that solution was one over third, a third, in other words, over one minus a third.
And I just do a little bit of algebra, then you see we come out to a half there.
Now we have now that we understand probability, we understand odds, the more difficult of the three
is the odds ratio, that is a ratio of odds. So even though we're talking about a ratio of
probabilities here, now we're talking about a ratio of odds, not of probabilities, but of odds.
And an easy way to do that is just to consider an unfair coin,
with a probability of heads of say of 70%, 0.7. And then a fair coin,
with a probability of 0.5. So let's save those, because the odds ratio is then going to be for
heads given the unfair coin, over the fair coin. So let's do P underscore unfair is 0.7,
P underscore fair is 0.5. Let's work out the odds for each of those. And I'm going to save those as
odds underscore unfair and odds, odds underscore fair. And again, it says P over one minus P for
each one of them. And now we can work out the odds ratio, which is now the odds of the unfair over the
odds of the fair. So you can see that's a ratio of odds. And that gives us 2.33. So we can say that
the odds ratio is two and a third to one, 2.333 to one for heads given the unfair coin over the fair
coin. That's the odds ratio. Now, since this odds ratio, we're going to abbreviate it OR is greater
than 1.0, we subtract 1.0 from it. So we get 2.333 minus 1.0. That's 1.333. And so what we do,
we multiply that by 100% because in words, we can state now that the odds of heads, given the
given the fair coin is 133 and a third percent higher in the unfair coin over this fair coin.
So the fair coin is our base. And we increase the odds by 133 and a third percent.
There's 133 and a third percent increase in the odds of heads given the unfair coin over the fair coin.
And we can verify this. So the odds increase is the odds ratio minus one. So 2.33 minus one. So I'm
just saving it there. So if I take this odds of the fair coin, remember what that was, plus
fair coin that we won to one, but plus the odds increase times the odds of fair. And if I do that,
I get back to 2.333. And we can say I've increased the odds by 2.333 or 133 and a third percent.
So come back to this often as you start working with odds ratios. This coin example makes it very
easy to see. So once again, I'm just taking this odds increase times the odds of the fair. And I add
that to what the odds of the fair was already. And then I get back to 2.333. That's what it means.
The odds ratio is 2.333. Now in this first of the three logistic regression models that we're going
to build, we're going to start off with, with just selecting the continuous numerical variable,
the ischemic bowel length as a predictor of whether a participant required a relook laparotomy.
So what we have to do is we're just going to select this get underscore dummies function. So pandas.get
dummies. We're going to take this df.relook required. And I'm saying dot yes. Now remember,
I can just say dot yes, because I don't have illegal characters. So I don't have to put that
inside of square brackets inside of quotation marks to be a string. I can just say dot yes,
which means what pandas is now going to do is going to see that yes is now my success. And that's going
to be encoded with a, with a one. So let's have a look at what my data frame looks like.
So there we go. You see relook required was yes. And then relook encoded was just one,
I don't need the no, the zero. No relook was required because, you know, if it's one,
there was a relook. If it's zero, there wasn't a relook. So with the dummy variables, I only need this
one dummy variable. And the reason why I've done that here is not for our analysis, but I want to plot
a scatterplot of what is going on here. So there's the zero. All these participants did not, or patients
did not require relook. And these did require relook. So they had 100% probability of relook.
These had zero. And we have ischemic bowel length. And the reason why I encoded it with the zero and the
one and using that column is just so that I have this nice y axis. Otherwise, a plot was going to flip
those around for me and it would have the yeses at the bottom and the noes at the top. So that's
the reason why I did that. But what I want you to see here is the fact that we can't draw a straight
line here. And we also can't use the means as we did with analysis of variance. I have to come up with
a different strategy to build a model that will give me the probability of a relook. But certainly,
I can't draw a straight line because if I were to do that, I'm going to have probabilities of less
than zero and more than one. And we can't do that. There's no way to have that kind of line.
Our aim, though, is still to create a linear model beta sub zero plus beta sub one x. And that x,
remember, is our ischemic bowel length. The problem is, we can't have, we don't have a numerical variable
as our dependent variable, we have to come up with something different. So we want to, and this is the
magic word link, we want to link this linear model on what on what we're going to have on the right
hand side, beta sub zero hat plus beta sub one x, we want to link that to a probability. Because we
can't equate it to a continuous numerical variable, we link it to a probability. And for that we need a
link function. We still want a linear model. But now we have to link it to a probability. It's not an
equation, we don't equate it to a continuous numerical variable. And there you can see
the logit function in three here. That's the log, the natural log. So it's base E, Euler's number.
That's the estimated probability over one minus the estimated probability. So what is that? That's the
odds. So we also call this the log odds. So this is the link between probability and the linear model on
the right hand side. So we see we still have our linear model on the right hand side, we can have an
intercept and a slope. And we link that to the probability, such that link is, we can equate
that then to this log odds. And we can solve for this estimated P, just as we had estimated values
before, remember our dependent variable was continuous numerical, and our model then gave us
an estimate of that, we can do that for this estimated probability. And I show you the little bit of
algebra here, it's not difficult to exponentiate both sides. By the law of that, we can bring this
odds out of the E to the power natural log. And that means we'll have E to the power, our linear model
on the right hand side, I can multiply both sides by one minus the estimate of P.
Once I get that multiplied out, I can isolate the estimated probability on the left hand side,
take it out as a common factor, divide both sides by the one plus E to the power of the linear model,
and there we get our estimate for probability. So instead of whereas, as I said before, we just
had a continuous numerical variable estimate, and that was going to equal our linear model on the
right hand side, we still have our linear model, but we see the big difference that we have there.
Now let's use our legit function so that we can create a model. First of all, though, we just need
to create these design matrices. So remember, we've used them before y comma x equals Patsy dot d matrices.
And I'm using a little formula relook required given the ischemic bow length from the data frame.
And now I'm just going to convert them to a numpy array to numpy arrays. So let's just have a look at
what the dependent variable is, it's just 1111. Or if they didn't require relook, there'll be zeros in
there. And as far as our design matrix is concerned, we'll have our column of constants one, and our
second column is the ischemic bow length. So this should be really, you should be very comfortable
with this, you see, we're still going for the right hand side, which is basically nothing other than a
linear model. So let's have a look, we can use the legit function, just as we used OLS before, although we
can't use OLS before, you can see why and actually what is used here is maximum likelihood estimation,
that's how we'll find those best values. In this instance, we use the dot fit method. And where there
we have a model. So let's have a look at the summary of this model. And there we go, we just need to
learn how to interpret this, we still see coefficients there, we see a standard error for the coefficients,
coefficient divided by the standard error in this instance gives us a z statistic, and we can express
a probability for that z statistic given the parameters for the z distribution. And then
we can see the 95% confidence intervals. But we have to be very careful about these,
because the values that we have here for the coefficients, this is how we would write them.
The logit function on the left hand side does log odds equals, there's beta sub zero minus 5.7102
plus beta sub one 0.0519 time ischemic bowel length. But that linear model on the right hand side,
know that equates to the to the link function. And so we still have to solve and we saw how to solve
for this estimated probability. So now that we've done that, let's just take a participant and we suggest
that they have an ischemic bowel length of 120. If I now pass this with the constant one, of course,
so my list there, my Python list has the two elements in it, my constant comma ischemic bowel
length, and I pass that to the predict method of my new linear, my logistic regression model, I see a
probability of 0.6275. So given 120 centimeter ischemic bowel length, there's an estimated, estimated
probability of 0.628, 62.8% of a relook laboratory. So that's actually, you know, that's very, very,
very nice. What I want to do here, you don't have to worry about the code, I'm just adding a constant
to this whole range of values. So I'm using numpy.lin space. So I'm going from the minimum bowel
length to the maximum bowel length of my data set, and I want 50 numbers in between. That gives me an array,
I just add a column of constants with the sm.add constants in the front. And then I'm going to
predict. I'm going to use the predict function and predict for that whole range of ischemic bowel
lengths. I'm going to create an array that contains all these probabilities because I can now do a nice
little scatter plot because look at this. So this is what we have instead of our straight line. So we use
maximum likelihood estimation, and that calculates all these values for us. So we can see our model
here. And as the bowel length gets longer and longer, you can see the probability of
of the relook laparotomy occurring or being required. And what we have to do, and that becomes
very important. And as you learn more about these, you know, there's techniques to look at
where we make this cutoff, because what this model is going to give us is a probability,
say there of 0.6407. Now, do we put that in the one class as a prediction or in the zero class as
the prediction, we have to decide what our cutoff is. Generically, we could suggest that it would be,
you know, at 0.5. So anything above 0.5. So that's about 110 centimeters.
So that will flip over 0.5 as far as our probability is concerned below that under 0.5.
And we can say, well, one does predict and the other one doesn't predict. But it really depends,
you know, if you don't want to make mistakes, you want to be conservative. So you might choose
something like 25. If it's above 0.25, it predicts that there is a relook laparotomy
would be required. So you can decide where you want to place your cutoff. But that's a sort of a
bit more of an advanced topic. So now we can still look at our parameters. So we can call the params
attribute there for our model and we see beta sub 0 and beta sub 1. So we can write a solution to our
research equation here. So we have the estimated probability is this e to the power and you see the
linear model on the top there in the exponent and 1 plus e to the power. So very easy to see how we
then answer our research question. By the way, remember in seven there, we see how we calculate
the confidence intervals for these, just to use the standard error there. Now, what does this mean,
though? I mean, it's in an exponent. And it's part of this logit function, this part of this link
function, how do we understand that value beta sub zero and beta sub one? Well, what we do is we
exponentiate those values. So you can see a numpy dot exponent, I take my log underscore model underscore
one, that's the model we've just built the params. And I'm taking the second one. So beta sub one,
in other words, remember Python is zero indexed. So square brackets is indexing, I want the second
value back, but I'm exponentiating that. And now I get a value of 1.053. And that's my odds ratio.
And you can look back at the mathematics why this would be so.
Important to remember, though, that if we exponentiate that beta sub one, we're going to get
the odds ratio. So 1.053. Still, again, how do we interpret that? And how we interpret that in the case
of a continuous numerical variable such as this, we say that for every one unit increase
in that variable, so every one centimeter increase, our units for the measurement of ischemic
balance was a centimeter. So every one centimeter increase doesn't matter if it's from 120 to 121,
or 150 to 151, doesn't matter, a one unit increase is going to increase the odds
the odds of a relook by 1.0533. So the 121 centimeters over the 120 centimeters would increase
the odds of a relook by 1.053. And you can see we can subtract one from that and multiply by 100,
that'll be 5.33%. So every one centimeter increase increases the odds over the lower centimeter
by 5.33%. So that's a very powerful thing to do. So let's create, you don't have to do this,
but let's create a user defined function. So I'm going to use the dev, keyword dev,
call my function prob, and I'm going to have a variable that I pass to at x, and it's going to
return for me the log underscore model underscore one predict one comma x. So that's just going to
return for me the probability if I give it a value, ischemic bow length value. And now we're going to
use that inside of a second function, I'm going to call this one odds. And so I still pass in the
ischemic bow length, I'm going to assign this value p to calling this function prob with this x,
so it's going to turn a probability for me so that I can calculate the odds. So you can have a look at
that code. So now that I have this odds function, I can pass 120. And what it's going to do is going
to spit out an odds for me of a relook given 120 centimeters. Now, remember, we said the odds ratio
was 1.053. And I want to show you that as so so let's pass 121 centimeter over the odds of 120
centimeters. And lo and behold, I see 1.0533. That's the odds ratio. So let's do odds of 156 over 155,
1.0533. Every one unit increase increases the odds of a relook by 1.0533. And then we said it's above
1, so I can subtract 1 from it as 0.53, multiply by 100. That should be 0.053 there. And that gives us
5.3%. So 5.3% increase in the odds, not the probability, the odds of a relook for every one
centimeter increase. By the way, just for clarity and completeness sake, if that odds ratio was below
one, say, for example, 0.8, I would subtract that from one, so not one from it, but it from one,
so one minus 0.8, that's 0.2. And that's 20%. And I would say it would decrease the odds of a relook
by 20%. So just check which way you have to subtract either it from one, if it's below one,
or one from it, if it's above one, multiply by 100%. And that gives you the change in the odds.
And we can do exactly the same thing with the confidence intervals around the coefficients.
So I can get the conf int values log underscore model underscore one dot conf underscore int,
but I'm passing that to the exponent function numpy dot exponent, so that I can get the 95% confidence
intervals around the odds ratio, which is the exponent of the coefficient. You take e to the power,
the coefficient, which gives us the odds ratio. And you can see the 95% confidence intervals around
the odds ratio, the exponent of the coefficient. So that means we can express p values for these. And
we see here for beta sub one, we have 0.000652, that's less than an alpha value of 0.05. So we
reject this null hypothesis. And by the way, our null hypothesis here would be that beta sub one equals
zero. Or in words, we would say that ischemic bowel length is not a predictor of the need for a relook
laparotomy. I just want to show you visually, I'm just going to create a z score. And we get that from our
table, it was 3.406. It's a coefficient divided by the standard error. I just create a bunch of these
x values. And I'm going to use that to plot the z distribution. You can see these dotted lines,
that was our critical z values. And we see ours, our z values is way beyond that. And that's why we got
our p value that was so much smaller than 0.05. So just some code for you to have a look at that,
it's nice to express visually, why we get this significant value. Now, let's ramp it up. And
we're going to use a categorical variable as our independent variable. So we can use that seniority
of the surgeon, principal surgeon at this index procedure to see to understand that relationship
between that independent variable and the probability of a relook. So there we go, we're going to create our
design matrix. So y comma x equals patsy dot d matrices. There's my little formula relook given
seniority, the data comes from the d of data frame. And let's print some of those out.
So you can see my constant for my design matrix x, my constant of one as I always have.
And then we see the two dummy variables. So that will be for attending, or for
sub specialist. Because remember, the base class with the senior resident, we don't need that one,
we only need these two. And you can see I've just printed out from the data frame itself,
it was attending, attending. So that's why it's one zero, one zero, zero, zero,
was senior resident, zero, zero, a senior resident, and then attending again, one zero.
And if it was zero one, that would be a sub specialist. So we're going to convert those to
numpy arrays, as we always do, because I want to use this legit function once again,
and I'm going to call my new model log underscore model underscore two. And let's have a look at this
summary now because we need to learn and understand how we interpret this. I still get my coefficients.
And I have beta sub zero, so 0.55, beta sub one negative 0.221, beta sub two negative 1.19.
I see the standard errors of those, take the coefficient divided by standard error, I get the z
distribution. And from that, I can calculate a p value. And then we see the 95% confidence interval
values around the coefficient. Remember what this all means. It means I can fill in this research
equation of mine, I would say the log odds, my link function equals, there's my beta sub zero,
there's my beta sub one, there's my beta sub two, straight from these coefficients. And remember,
this is times attending, and this is times specialist. So if it was a senior resident,
it'd be zero zero. And the log odds would always be 0.5596. If it was a senior resident,
if it wasn't attending, this would be one, and this would be zero. If it was a specialist,
this would be zero, and this would be one. So you can see, I can only get those three different
probabilities. So let's look at taking the exponent of those coefficients. Remember,
that's in the params attribute. And if once I take the exponent of that, so e to the power of
these coefficients, I get the odds ratios. So let's have a look at that. I see for beta sub one,
an odds ratio of 0.8. So that is, if it's an attending, I get an odds ratio of 0.8. And if it's
a subspecialist, we find an odds ratio of 0.302. So how do we interpret these? Now,
both of those are below one. So remember, we're going to subtract that from one. So one minus 0.8,
that's 0.2 times 20 times 100% is 20%. So that we can say there's a decrease in the odds of a relook
laparotomy of 20% if it isn't attending as the main surgeon as opposed to the senior resident. See how
it's now become so crucial for us to have chosen one of those three levels as our base case because
we are comparing, this is an odds ratio comparison as far as, remember the unfair coin and the fair
coin? So that is the attending over the senior resident. And have a look at if it's a subspecialist,
it's 1.0 minus 0.303, that gives us 69.7%. That's a 69.7% decrease in the odds,
not probability, in the odds of a relook laparotomy if the main surgeon is a specialist as opposed to,
once again, the base case, a senior resident. And we can do that for the confidence intervals as
well. So I exponentiate all of them. And we can see the 95% confidence intervals around the odds
ratio now. It's exponent, so it's the odds ratio. Now have a little look at this. Something is going on
here. Look at beta sub one and beta sub two. So the odds ratio is 0.18, that's below one,
so that's a decrease in the odds, till 3.37, that's above one, so that's an increase in the odds.
So within the 95% confidence interval limits, there's both a decrease in the odds and an increase
in the odds. And once you have that, you cannot have a significant P value. So let's go back,
there's our model, have a look at this, this x sub one, remember, that's our beta sub one,
look at that P value, 0.76. Look at these confidence intervals now still expressed as coefficients,
negative to positive. So within that range is both a decrease to an increase in the odds.
Now look at what happens to beta sub two, we have a P value of 0.048. So that is less than 0.05. And have
a look at this, our odds ratios around the coefficient is both negative. And if you
exponentiate that, that means it's both going to be values below one. So look, have a look at this.
The 95% confidence intervals in the case of a specialist shows a decrease in,
we subtract from one, one minus 0.99, that's 0.01, that's 1% to 91%. And because they are both a
decrease, the confidence interval shows both a decrease, they both fall below one. But it is
from 1% to 91% decrease. And now we have a significant P value, if just barely. So those are
all very important factors. Now finally, we're going to use both the continuous numerical and
the categorical variables as our dependent, independent variables. So we're going to use both
the seniority of the surgeon at the index procedure and ischemic bowel length as a predictor of
whether a relook is required. Once again, we're going to create our design matrices. So we say,
and our formula relook required given ischemic bowel length plus seniority. Now the order here,
we just have to make, you know, make or realize later that the order that we give it in here is not
going to determine which is beta sub one, which is beta sub two, and which is beta sub three,
we have to just be alert, when we look at the values, what happens. So let me just show you
what these values look like. And you can see that my constant, my column of constant, and then we see
a one and a zero. So it's made attending beta sub one, it's made subspecialist beta sub two,
and it's made ischemic bowel length, even though I put ischemic bowel length first there,
it made ischemic bowel length beta sub two, so just make sure of that order. Which means,
and I want to write it out for you here because we haven't done so before. Here's my research
question. I have the probability, this estimated probability of a relook, there we go, it's beta
sub zero plus beta sub one times an attending plus beta sub two times specialist, and those can either
be zero or one, plus beta sub three times bowel length. So I've got four parameters there, beta sub zero,
beta sub one, beta sub two, beta sub three, over one plus e to the power of that. Or, if you want to
think about this still as a linear model, that's beta sub zero hat, beta sub one hat, beta sub two hat,
beta sub three hat, and that equals this log odds of this estimated probability. A null hypothesis is
still going to be the same, and that's why I have these four models for you all in a row. It's still the
same idea as we had with linear regression, analysis of variance, analysis of covariance,
that all these beta sub one, beta sub two, beta sub three estimates are equal to zero. But in words,
we would state that seniority and ischemic bowel length are not predictors of the need for relook
laparotomy. So let's do that, we're going to create numpy arrays from our design matrices, we can use the
logit function. And this time, we're just going to assign it to the variable log underscore model underscore
three, this is our third model. Lo and behold, we can call the summary method there. And there
we have our beta sub zero, beta sub one, which remembers attending beta sub two, which is subspecialist
and beta sub three, which is ischemic bowel length. So watch out for the order there. Still have my
standard error, z statistic for each, probability of each and 95% confidence intervals around the
coefficients. So the coefficients I filled in for you here, it's just taking these values that we have for
the coefficients there. And there is the solution to our research question. And you understand now what
you have to do, you have to know, exponentiate those values, these coefficients, those are going to give
you odds ratios, you have to exponentiate these 95% confidence interval, lower bound and upper bound
values of exponentiate them, you get the 95% confidence intervals around the around the odds ratios.
And you know, now if an odds ratio is more than one, it increases, if it's less than one,
one it decreases, the confidence interval straddles that one, you're not going to find a significant
p-value. If it doesn't straddle one, you are. And you can see here, the p-value for the attending
was not significant, 0.66. For the 0.67, for the subspecialist, it was 0.53. And then for ischemic
bowel length, it was significant. And once again, you can see the negative to positive, negative,
positive, and then positive, positive. So you can see why those two were not significant,
but this ischemic bowel length, given those other two. Now you have to understand that in this model,
we're not seeing these things as separate, they are now combined into a single model. So it's given
that it is an attending or not, given that it is a subspecialist or not over a senior resident with
the ischemic bowel length, only then does it give us this significant value there. What I wanted to do
for you is just to create a bunch of values. You can have a look at the code there so that I can
finally express this nice model for you there. There's the result of our model. So for each of
the resident attending and the specialist, we can see the probability of a relook,
given this ischemic bowel length. So they each have their own, they each have their own line. And
you can see this beautiful sigmoid shape curve there. It's constrained between zero and one,
exactly what we needed. Finally, finally, if you want to save these models, remember this is plotly.
So you can really just click on that button, right there, it'll download it as a PNG for you,
you can zoom in, you can pan around, you can zoom in further, zoom out, go back home,
you can even zoom way in, let's go back to home, we can switch these off, one by one, decide which ones
we want to show in isolation. So that's why I absolutely love plotly. And then you can save it.
And here I've instead of using the save button there, I say fig dot write underscore image,
give it a name, it's going to see that it is a PNG file, my format is PNG explicitly stated there,
and I can specify a width and a height. So if I were to run that block of code in this folder or
directory, it's going to create that nice PNG file for me and it's ready for use in a report.
So I really hope you enjoyed the seminar, we looked at linear regression, we've looked at analysis of
variance, analysis of covariance, and now logistic regression. And I hope you saw that progression,
how one just builds on the other, and why I call them the four fundamental linear model types in
quotation marks. If you understand the basics of what I've shown you in this seminar series of four
video tutorials, you're well on your way to making use of these types of models and looking at more
complex models.

So with the help of Python and this Google Colab notebook I'm going to explain the process of QR
decomposition or QR factorization of a matrix of whose columns we have mutually independent
or independent set of column vectors and we're going to change that basis then because if they
are mutually independent remember their basis usually of some subspace because a is not squared
and we're going to change that basis so that each new column vector is going to be orthonormal.
So they're all mutually orthogonal and they have unit length one and to get that we're going to go
through what is called the Graham-Schmidt process and that gives us this matrix Q and it will also
help us to develop this matrix R which is upper triangular so we have this multiplication of
an orthonormal matrix times an upper triangular matrix and that's going to give us back A so
we've decomposed A or we've factorized A. So what I'm going to assume is just that you have
knowledge of vector arithmetic just how to add vectors and how do you project one vector onto
another and we usually use the dot product for that if you remember. Linear independence those
are the column vectors of A. What the rank of a matrix is because we can use that to help us just to make
sure that our column vectors in our matrix A are all linearly independent. Orthogonality what it means
to be orthogonal and then also the normalization of a matrix. So if you have some idea there
it'll be easy enough to follow along. The packages that we're going to use in this notebook I'm going
to explain it using the sympy or the symbolic python package makes it very easy to to do symbolic
mathematics or just normal numerical mathematics as well. So I'm going to import the matrix function
that's uppercase M and the init underscore printing function and if I call that function it's going to
allow sympy to express Lartec as the output of your computation. So let's run those and set that init
printing. So here's our example matrix three columns three column vectors there one one one zero one one
and zero zero one and we that's a basis for our three or three space in this instance but we want
to change that basis for three column vectors that are mutually orthogonal and they are of unit length as
I mentioned. Of course we have to first make sure that these three are linearly independent and we'll do
that but first let's create the matrix. I'm going to use the matrix function from sympy as a list object
I'm just passing all nine values and then I'm calling the reshape method and putting in three comma three
so that we have a three by three matrix. If you can see the print out there is using Lartec so it's nice
and crisp lovely design there to the printer. So let's just make sure that these are linearly independent
column vectors and one way that I can do that is just to do row reduced or or use a elementary row
operations and Gauss-Jordan elimination just to make sure that we get to reduce row echelon form
and if I use sympy the matrix there and then the RREF row reduced echelon form method there it's going to
do elementary row operations Gauss-Jordan elimination and what we're going to end up with is the
as the identity matrix and what that means it's only the zero vector that is a solution to this
to the homogeneous system involving this matrix A. In other words those three column vectors
are linearly independent. The other thing that we could do is also just call the rank method
so a.rank open close parentheses and that's got to give me three
where it spans all of three space so they are linearly independent. Now just to show you if you want
to do QR decomposition you can use the dot composition there the decomposition method on my matrix so
there's my matrix that I create and then I'm just calling QR decomposition on on that matrix and that's
going to give us two matrices back which I've named Q and R and you can see Q there and you can see R
and you can we're going to see that Q all these column vectors are orthonormal so orthogonal and of
unit length and then we have R this upper triangular matrix just to verify that Q times R is back to A
I'm saying Q times R and I get back to A no problem whatsoever so let's do this by hand of course when we have
a set of basis vectors we're just going to choose one of them to be the start one and all the others subsequent
to that must be orthogonal to this first one so since we have our first one there we can just use it and
this is what we're going to do so A sub i that is just going to be the column vectors my initial set of
basis vectors U sub i that is going to be my new author orthogonal set and if I normalize it in other words
divided by its norm then I'm going to get U hat sub 1 or U hat sub 2 that's the orthonormal version of
that so going to be mutually orthogonal but they're also going to be of unit length so let's just create
A1 a computer variable that I'm using here for A sub 1 the first column vector inside of A and I'm just
using square bracket notation there all the rows in column zero and that's just going to give me the
one one one one the first column one one one and I'm just going to set that to be U1 I've got to take
one of them to start off with as long as the others then are mutually orthogonal orthonormal to U1
of course as I said we have to divide by the norm of U of U1 or A1 I can call it anything here
seeing it's the same thing so I'm just calling the dot norm method there so if we take the norm of U1 or A1
we see a square root 3 so if we take A1 and we divide it by the norm of A1 I'm going to call
that U1 normalized so it's now normalized so from 1 1 1 we go to square root 1 over square root of 3 or
square root of 3 over 3 for all three elements now I just want to remind you of this how we do a
projection so if I take a projection of vector A sub 2 onto A sub 1 it is this dot product of the two of them
divided by the norm of the one that you're taking the projection to times that vector let's just
have a look at a google drawing here we have a google drawing there's my A sub 1 so it's not the
one we're dealing with in this problem it's just a schematic and there's A sub 2 the blue one to the top
and if we use the projection remember that's an orthogonal projection which leaves me with a pink
vector here along A1 which is the projection of A2 onto A1 but because it's an orthogonal projection
U2 is actually very nice because it is orthogonal to A1 and we've just set A1 to be U1 and now we have
something that's orthogonal to it just given to us such that this projection the pink at the bottom plus
U2 has got to give me A and that's what we see there A sub 2 that's the projection of A2 onto U1
plus U2 and if I get U2 on its own then I have everything I need to calculate U2 it's A2 which
I'm given in the problem and it's very easy for me given that I have U1 because we decided U1 is A1
I have been given A2 so to take the projection of A2 along U1 is very easy and just that simple
algebra leaves me with U2 so that's what we're doing here we're saying the projection of A2 along U1
or A1 then doesn't matter is A2 dot product with U1 divided by the norm of U1 squared times U1
so let's extract A2 there as the second column beautifully we can see it there rendered to the
screen that's just going to be our second column 0 1 1 so what I need to do is just to take this
dot product divided by the norm squared of U1 multiplied by U1 and you can see it right there
so how to do the dot product you take one of your vectors A sub 1 use the dot method so dot dot and
then pass this parameter or argument the other vector that you're interested in so if we look at
the projection of A2 onto U1 again I'm giving it very simple names there so that I can remember if I see
this code down the line or give it to someone else sort of can figure out what I meant by this
computer variable name and it's two thirds two thirds two thirds and now as I said very conveniently we
have this orthogonal decomposition basically in using that projection onto the other vector
and that's what I have here U2 plus the projection of A2 onto U1 exactly what we had in the picture
that's got to equal A sub 2 and all I'm doing here is just putting in the definition so it's going to
be U2 plus in this middle bit here A sub 2 dotted with U1 times the norm of U1 all squared remember
that's the projection there and that makes it very easy let's you know U2 is just going to be A2 minus
this projection that we calculated so lo and behold there's U2 and all we want to do now is just to
normalize it or divide it by its norm so U2 divided by the U2 norm and there I see my second
my second new basis vector and it is going to be orthogonal to the first one and it's orthonormal
seeing that they're all in one so what I've shown you in five that's sort of the most
the the what you have to understand or at least just memorize it put in your head but it's easy
enough to understand U sub any of the ends that I'm looking for we've just looked at U2 but next up
we're going to look to U3 well just as we had A2 where we just said A2 minus the projection
it's going to give us U2 we do exactly the same here so it might be U1,000 that's just going to be A1,000 minus
A1,000 minus and then we're going to do the summation we're going to start I equals one
and then we're going to go all the way up to n minus one so not all the way up to n and if you
think about all the ones from one to two to three to four this U sub n better be orthogonal to each one
of the ones that it came before it's got to be mutually orthogonal it's going to be orthogonal to
each and every one before and that's how we do this that's why we have the summation there
we have to now take a sub n so if I'm looking at U3 I take A3 minus and then A3 dotted with each of
the previous ones U1 U2 only up to two because we only go to n minus one so it's got to be mutually
orthogonal so we we've got to have each one of those mutually orthogonal ones in there so if we look at
a sub just the projection of A3 onto U2 and then also the projection of A3 onto U1 so A3 has to be
projected onto the one before and the one before that and you see the equation there such that it's
very easy for us to write the code there's A3 and then there's the dot product of A3 onto U2
there's A3 onto U2 and we see that and then A3 onto U1 we see that one those ones and now we're just
going to subtract it from A3 and you can see the algebra very clearly there because just as we had
in the two-dimensional case that you can add the two vectors to get to the final one this is what
you've got to do now it's orthogonal to two other vectors so you better bring them both in which makes
it then very easy through simple algebra for us to calculate U3 and all we have to remember is to
divide it by its norm and that's going to give us the normalized version of that so now we have it we
have the whole of U remember we're just calling it Q because it's QR decomposition I'll call it U hat
here and I'm just using the matrix function and putting all three column vectors next to each other
and just to show you that they're all mutually orthogonal I'm going to take in each pair of them
there'll be three pairs and I'm just going to do the dot product between each of them and they
better all just be zero because the dot product of orthogonal vectors is zero and we see zero zero zero
if you use numpy it's going to do it not symbolically like here it's going to use numerical calculations
and it's going to have a little bit of overflow error and you might not get exactly zero but you're
going to get something times 16 or 17 to the power negative one and that really is to zero
so we can see here all three of them are mutually orthogonal and there's R this is how you calculate
R this is an upper triangular matrix and you can see the pattern repeating it's just the normal dot
product and that's how you would calculate R such that if you then have Q and R you can get back to A
now up till now what we've done now is we've looked at a very very simple case where we had this matrix
over a field of real values so this is in essence then not the full picture you've also got to consider
the field of complex numbers and how do you do that so the dot product we just use that because we
we were just talking about Euclidean space here so that wasn't a problem but as soon as we go to
other spaces as soon as we include complex numbers for instance remember it's not the normal dot product
we're talking about but the inner product so you've got to define the inner product between the two
vectors in the case that you're doing and as long as you take not the dot product but the inner product
of all of those nothing changes so in short that is QR decomposition using the graham-schmidt process
it really is an easy process you just have to do a couple of of them and just not make a silly
mistake but it's really easy easy to remember and it's all about the projection of one vector onto
another but to remember that you have to keep on adding them in as much as every new one that
you calculate has to be orthonormal to all the ones that came before
you
you
you
you
you
you
you

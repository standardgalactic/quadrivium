In this video, I'm going to use Python to talk a little bit about the Jacobian.
And the Jacobian is all about partial derivatives, of course.
So we're going to run through just a couple of examples just to show what we do with partial derivatives,
depending on how many variables are in our function and how many functions we have.
And then we're going to just look at, as far as data science is least concerned,
just look at some example how we would use the Jacobian in a neural network.
So we are here in a notebook, a Colab notebook in our Google Drive.
And we'll go to full screen mode there so we can see a little bit more.
So the packages that we're going to use here in Python, I'm going to use symbolic Python.
And as always, we just use the namespace abbreviation SYM.
And then I'm also going to just use some Lartec printing to the screen as far as our SymPy code is concerned.
So we're going to just use init underscore printing there and just call that function as well.
So before we do all of that, of course, let's just connect to a Google Colab.
And we are connected.
So let's run import SymPy as SYM and then also init underscore printing.
So I always just like to check what version is installed at the moment,
because, of course, there's new versions all the time.
And you just want to make sure that some of the functionality that you use is available to you.
So let's have a look at the time of recording of this video.
We see we are using SymPy.
Well, you just use SYM dot and then the dunder double underscore version double underscore.
And that should tell us that we currently have version 1.7.1.
So let's talk about partial derivatives and the Jacobian.
Now, the type of functions that we can talk about, you can see the table of contents here on the left hand side.
There's single variable, single function.
So that's not usually how we would refer to this, but I just want to make the classification very clear.
So it's a single variable and just a single function.
And then we're going to have a multivariable, but only a single function.
And then, of course, your vector-valued functions where we have a multivariable, multiple functions.
So first of all, just that single variable, single function.
This is a mapping f, as you can see in one here.
We're just mapping f.
F just maps the reals to the reals.
So giving me any real-valued input and the output is also going to be a real number.
And, of course, one of the easiest ones is the parabola y equals x squared.
And you can see here in 2, then, if we just take the derivative of f with respect to x, we all know that rule.
It's just going to be 2x.
But we can use SymPy, of course, symbolic Python, turning our Python instance here into a computer algebra system.
So we're going to use the symbols function.
As we can see, a symbols function.
And we're going to set mathematical symbol x to the computer variable x.
So if we do that assignment, we can now use x as a mathematical variable.
So I'm going to show that I can create a computer variable f here to hold this expression x squared.
Remember the double star symbols for exponentiation, or the exponent at least, the power, I should say, of a variable.
So x to the power 2.
And then we print f to the screen.
And because we've set init underscore printing, we've called that function.
We can now see x square printed in Lartic there.
No problem.
Now we can just use the diff, D-I-F-F method, on our object f there.
So f dot diff.
And then we pass the variable with which we want to take the derivative with respect to.
So this is f, or take the derivative of f with respect to x.
And if we do that, very neatly, we should see that the answer is just 2x there, as we would expect.
So very easy just to use the diff method on our expression there.
So let's go to a multivariable single function.
So what we're doing here in 3, as you can see, we're just mapping r squared to r.
So you give me a Cartesian product, any two elements in r, and it'll map to a single value.
Here we have an example, also a function called f.
It takes two variables, x1 and x2, both real numbers, and it's going to output a single real number.
Hence the mapping that we can see there.
So we're going to use x underscore 1, x underscore 2 as our two variables.
So 2 times x sub 1 times x sub 2 squared plus 3 times x sub 1 plus 2 times x sub 2.
And if we want to take the derivative of that, remember we have to take partial derivatives,
because we can take the derivative of this function with respect to either of our two variables.
And that's what we see in 4.
So we have the partial derivative of f with respect to x1, comma, the partial derivative of f with respect to x2.
And what we end up with is this little row vector here, in as much as we now have two values.
And taking the partial derivative, of course, you just treat all the other variables as constants.
And that's why from 2x sub 1, x sub 2 squared, if we take that first expression, and we take that with respect to x sub 1,
x sub 1 is a, x sub 2 is treated as a constant, so that's just going to stay, so it's 2x sub 2 squared.
And then from 3x plus 1, we're just going to get the plus 3.
And because 2 times x sub 2 is treated as a constant, of course, that the derivative of a constant is 0.
Now we can check our work.
First, what we'll do is we'll just set these two symbolic variables, x1 and x2.
And we're going to use a bit of Lartic there just to indicate that it's x sub 1 and x sub 2.
So if we pass that as arguments to the symbols function, now we can use x sub 1 and x sub 2.
So I'm going to show that I've assigned my expression, the expression that we have here in 3, to a computer variable called f.
And if we print f to the screen, you'll see a beautiful Lartic printing of our single function with multiple variables here.
And now we can take the two partial derivatives.
So I'm going to say f dot diff, and then with respect to x sub 1, and then comma, another one, f dot diff, with respect to x sub 2.
And that's going to give us our two partial derivatives.
And those two partial derivatives, exactly the same answer as we had there in 4.
So now let's go to a multivariable, multiple functions.
So we're talking vector-valued functions here.
So we express these two functions as a column vector.
So now we have two functions, and as you can see in 5 here, f sub 1 and f sub 2.
They both have two variables, x sub 1 and x sub 2.
And you see them there listed as a column vector.
So those two multivariable functions.
And now, if I have two functions, they each have two variables.
So if I'm taking partial derivatives, of course, I'm going to have four expressions here.
And that's what we call the Jacobian.
Now, we can refer to, I suppose we can refer to this in 4 up here as a Jacobian as well.
But usually when it gets to 2 by 2 and larger, then we refer to it as a Jacobian.
But we won't be strict on that.
So what we have on the first row of our 2 by 2 matrix here, n6, is the first function.
So you see f1 and f1.
And then it's taken with respect to x sub 1 and x sub 2.
And then the second row, we have function 2.
So just as they appear in your vector value function up here,
function 1 is on the first row, function 2 is on the second row.
And you just keep it the same.
And then you're just going to do all the partial derivatives.
And then you end up with a 2 by 2 matrix.
And that is the Jacobian.
So it is just this matrix of our partial derivatives.
And that's first all the partial derivatives.
And we would write 6 as j sub f, x sub 1, x sub 2.
There are various notations you can write as that.
So if we just want to use SymPy to do this for us,
I'm just going to set up a little SymPy, SYM dot, call the matrix function.
And then I'm just going to pass this as a list, my two functions with a separated by a comma.
And if we print that to the screen, we're going to see, just as we had our original problem,
we see the two multivariable functions there.
So now I've just got to tell SymPy the variables that I'm going to use as far as my derivatives are concerned.
So I'm setting that up as a computer variable called VARS.
And then a matrix function again, and then pass as a list x1 and x2.
And now I can call the Jacobian, once we've run that.
So we've got VARS.
Now we can just call the Jacobian method.
So f dot Jacobian.
And then we pass the VARS variable here.
And that holds this matrix of x sub 1 and x sub 2.
So it knows what to take the partial derivatives of.
And then we see, we see our Jacobian there, a 2 by 2 matrix, just as we would expect.
So now we're going to go on and we're going to look at how to use the Jacobian when we deal with neural networks.
So here we're going to have a simple neural network.
We see there are two deep layers.
So we have our input x.
We have two hidden layers.
And each of them have got two nodes.
And then we have O there.
That's our output node.
So if we look at the first node, we can just represent that as a column vector, a sub 1 and a sub 2,
which is exactly what we do there in 7.
And what we will do, you know, how do we calculate the values that are in a sub 1 and a sub 2?
Well, we're just going to set up this nice little matrix times a scalar plus a vector notation.
So if we can get a matrix W, and as you can see here, we're going to just call it the W transpose.
So we'll set up W and then it'll take its transpose, multiply it by x, which in our case, remember, is a scalar.
Here's a very easy input.
And then we're just going to add an appropriately dimensioned column vector to that.
And that's what we see here in 8, just to keep the dimensionality correct.
So W, it's transposed as 2 by 1, two rows in one column.
So technically it's a column vector.
But let's keep on just calling it a matrix.
So that's going to hold the weights, the values that we want updated during backpropagation in our neural network.
So we multiply that by a scalar.
And I'm going to see the scalar as a 1 by 1 matrix.
And then we add to that a column vector, C, which is also two rows, one column.
And the result is going to be A, two times one.
It is a two row, one column vector, column vector, as we can see A there in 7.
So you can spend a little bit of time just looking at A, just to make sure that you understand what we're doing here.
And as I mentioned here, 9, we can see that X, we're just going to see it as a 1 by 1, you can see it as a 1 by 1 matrix, but it is just a scalar.
And if we look at W now, but I'm going to use W again.
So what we usually do uses the superscript notation inside of parentheses, where this just denotes that this W refers to our first hidden layer here.
So it only pertains to multiplying X with something, adding a bias node to get the values that are in A1 and A2.
So if W superscript 1 there, it has only two elements, W sub 1 and W sub 2, again with a superscript to denote, that's the hidden layer we're working with.
So it has dimension 1 times 2, one row, two columns, but we take its transpose, so it's two rows, one column.
And then C must also be a column vector, two rows, one column.
And remember this is a bias node, so C sub 1 equals C sub 2.
So it's both the same values.
So this is what A would look like.
So in 12 here we have A.
So A, remember, it's a column vector, so it's going to have A sub 1 and A sub 2 in it.
Both of them are going to be functions of X.
And what we do is we take our weight matrix here, it's transpose, multiply by X, and add our bias vector there.
And that's our two row one column result there.
W 1 X plus C 1 and W sub 2 X plus C sub 2.
And this is what we get.
That is A sub 1.
That's the value for A sub 1 and A sub 2, row 1 and row 2.
And that is our vector A.
And that's, remember, how we get that.
And this is what the neural network is all about.
At least a densely connected neural network, we need to find the best values for W and for C.
And for the weights and the bias values.
Now if we take this A and we take the derivative with respect to X.
So all we're going to do here, and again we'll call this a Jacobian, we'll do Jacobian superscript 1.
Now we don't have to take partial derivatives here, we can just take the derivative of both of our functions.
And if we take them with respect to X, I mean all we're going to have is W sub 1 and W sub 2.
Because we just have X there as a scalar, it's not raised to the power or anything.
And C is just a constant.
So all we're left with is that.
And let's call that our first Jacobian or our Jacobian of our first hidden layer.
If we go to the second hidden layer, B, the vector B is also a column vector with two rows.
But it's a bit more complex, the values B1 and B2, because they are now functions of A1 and A2,
which in their own right are functions of X.
So it's getting much more complicated now.
And to do that, if we just go back up to the little picture, that's always nice with these densely connected neural networks.
We can see four lines connecting the first and the second hidden layer there.
So our weight matrix will have to have four values.
We have to multiply each of these connections.
That's all a weight.
So this is going to be a 2 by 2 matrix.
And we see it then in 14.
W, now superscript 2, because this is the weight matrix for our second deep layer.
So there we have W underscore row 1 column 1, row 1 column 2, row 2 column 1, and row 2 column 2.
And if we take the transpose of that, of course, those subscripts just change around.
C2, our bias vector for the second hidden layer, that's also going to be C1 and C2, but the superscripts 2.
And remember that the two values are also going to be the same.
So our vector B now contains B1 and B2, but both of them are functions of A1 and A2.
And now we can get to what we had as far as the Jacobian that we spoke about is.
Because if we look now, remember B being a 2 by 1 column vector, if I take a weight matrix 2 by 2 multiplied by my column vector,
which is A1 and A2, and I add my bias node, I now get B, which is a 2 row 1 column column vector.
So it's W subscript 1, 1 times A1 plus W21A2 plus C1.
So that is really just a multiplication of a 2 by 2 matrix and a column vector.
And you can sit and do that on a piece of paper.
You'll see we come out exactly as where B is concerned.
But now we can really take partial derivatives.
I have two functions.
Remember F1 on top, F2 at the bottom.
And they both have two variables.
And in this instance, my variables are A1 and A2.
So I'm really going to have what we termed Jacobian before, the 2 by 2.
So it's just the partial derivatives.
B1 on the first row, B2 on the second row.
And we take those.
And if we do that, we see that we just end up where we started with,
because A1 and A2, they're not raised to any power.
C is just a constant.
And we end up with this that we're going to call J2.
It's not our original weight matrix W.
It's transpose, as you can see there.
And then for our output, which we've called O, it's just a scalar in the end.
We're going to call it a 1 by 1 matrix.
And what we need here is a weight matrix.
That's now, we're going to call it superscript 3,
because technically that is a third layer then for us.
And there are the two weight values.
Our constant here, our bias term is just a single scalar.
So we can call it a 1 by 1 matrix.
So O becomes very easy.
It's W3 times B plus C3.
Now, W can have all 1s.
It can just be 1 and C can be 0.
I suppose we can have that.
But irrespective of what our weight values in our bias term is,
and 21 there in the second line,
we'll see what the output is then.
And then if we do this output with respect to B, our vector B,
that's just going to be taking over with respect to beta sub 1
and with respect to beta sub 2.
So those two partial derivatives.
And what I'm left with is this, again, just our weight matrix,
which is a row vector.
And I'm going to call that my third Jacobian.
And now what we want in the end is to see
how the output changes depending on the input.
So what we're actually asking for is dx, do.
And if we look right at the bottom,
that is where we eventually end up, do, dx, I should say,
how the output changes with respect to the input.
And all I'm going to do is just multiply my three Jacobians.
And because what we are asking in the end is just a scalar,
if you look at the dimensions,
1 by 2 times a 2 by 2 times a 2 by 1,
well, what we're going to end up with is a 1 by 1.
And we can certainly multiply those three matrices with each other.
And that's how we use the Jacobian when it comes to neural networks.
and that's how we use the Lego wheel and the parallel networks.
So I think if I'm starting to
hey, did you know that this one establish by me?
Well, that's how we're going to play through the next 2 by 2.

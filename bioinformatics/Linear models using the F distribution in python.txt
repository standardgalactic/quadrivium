And while the sun shines brightly over here, I want to give you a brief introduction to this next video.
My name is Dr. Jean Klopper and I'm a Research Fellow at the School for Data Science and Computational Thinking at Stellenbosch University.
Now this video is going to be about linear modelling using the F-distribution.
So we're going to do some linear regression.
We're also going to do some t-tests and then analysis of variance.
So I'm going to give you a brief show on the whiteboard just what this is all about.
And I'm going to assume that you've watched one of my videos just to know what a sampling distribution is and how we use that to build our intuition for how to calculate a p-value.
So let's go to the whiteboard and then we're going to open a Jupyter notebook and I'll show you how easy the code is just to do all of these things.
So in this video I want to talk about linear models.
So we're going to use Python and I'm going to show you a couple of things.
We're going to do some linear regression, we're going to do t-tests and we're going to do analysis of variance.
And it's all going to be based on this little beauty here, the F-distribution.
And you can see always in green is all our definitions.
There is our equation for the F-distribution.
And it says X, whatever our value is going to be, depends on two parameters.
So parametric distribution.
And you remember how these work.
Well before we get to it, there is the long distribution.
You see D1, D2, that is our two different degrees of freedom that we're going to pop in there.
And of course we're going to use some code so you never have to worry about this equation.
I just want to show it to you in all its glory.
And the B that you see there, well that is the mathematical beta function.
So depending on these values, D1 and D2, you're going to get different sampling distribution curves,
their probability density functions.
So we can imagine given any two specific D1 and D2s, if we could do a study over and over and over again,
we're going to get a sampling distribution of these F-statistics.
And you see that they're the probability density function.
And what we're going to do, we're going to do a study and our F-value is going to be somewhere,
say for instance it's right there.
And the P-value, because this is a PDF, the area under the whole curve is 1.
And what we're going to have is we care about this area under the curve on this side.
And that of course is going to be our P-value.
And as always, when we do use code, we start doing it from the left-hand side
because it is the cumulative distribution function that we're actually worried about.
But we needn't go there.
So the whole area is 1, we subtract to this part.
So 1 minus this part leaves us with that part.
So that is, in essence, this probability density function of our F-distribution.
So we're going to talk about, as I said, linear regression.
So this one for us would be linear regression.
Linear regression.
And what we have here is a simple univariable model.
So we can imagine all these dots, they belong to a single observation,
whether they be patients, organisms, doesn't matter what the situation is.
But for any one of our observations in our study, they have two numerical variables.
We're going to gather data for two numerical variables.
So two columns in the spreadsheet.
And so for this individual here, or let's do this individual here,
for the one variable, they're going to have this value.
And for the other variable, they will have this value here.
So every single observation will have these two numerical variables.
As I say, if you think about a spreadsheet, there's going to be one variable,
say variable 1, and there's going to be another variable 2.
And we have all our observations here, patients, whatever they may be.
And we have values for each one of those.
And each dot belongs to a separate individual.
And those are the two values there.
And what we want to do eventually is build this model, this red line.
And this red line is what we call a best fit model.
And what we do is we say, for any given instance here of the independent variable,
our model predicts that to be the value for the dependent variable.
So that goes out all the way there.
Excuse me, my lines are not very parallel there.
So there's this difference.
We'll get to the difference shortly.
So there is this idea that this is every single individual's pairs of observation.
Of course, we can have more than one independent variable.
Then we have a multivariable logistic regression.
But we're going to keep things simple.
So given this, we want to try and predict this value.
Why would we want to do that?
Well, that's modeling.
This might be very expensive or very difficult to collect.
And it might be very easy for us to collect these.
And then we can just predict, our model can predict what this value is going to be.
Now, the way that we go about it with linear regression, remember, is about fitting this line is.
And remember, what is these are all linear models.
So we're all going to see straight lines.
What this is, y, if you remember from school, y equals mx plus c or b, whatever the case might be.
So y was this value, remember, x were these values.
And m is the slope of this line.
So if the slope is 1, we'll get a difference in 1 there for every difference in 1 here and a difference in 1 there.
And then c is if x is 0, anything times 0 is 0, so that falls away.
And y equals to c, so right here where x is 0, that is where we have this y-intercept.
And that's very important.
So if we have a slope and we have a y-intercept, that's all we need to draw the straight line.
But how is the straight line done?
Well, it is done by minimizing the error.
So you've already seen an error right here.
It's the error between what this person's or individual's observation's true value is versus what the model predicts.
That there is the error.
And we can just draw it in here.
That would be the error for that individual.
And here we have an error for this individual and for this individual and for that individual and for that individual and for that individual.
So all those are errors that our model is making.
And these are also called, so let's have it out there, those are also called residuals.
And how do we get this line?
Well, there are different methods.
There's gradient descent.
There's ordinary least squares.
We're not going to worry about that.
We're going to write some code and that's going to happen for us.
We're going to use stats models for that.
But somewhere we've got to relate all of that back to this.
And with linear models, we're also going to express something called the R squared.
That's the coefficient of determination that tells us how well our model does versus a baseline.
And this is our baseline.
We could always draw a straight line.
And that is just going to be the mean of this variable here.
I should say this variable here.
What am I writing?
So let's have that out.
The mean of our dependent variable.
So we calculate the mean there and we say whatever this individual's independent variable is, it predicts the mean.
So they all predict the mean.
And of course, that means we're going to have very large, we're going to have very large residuals.
And the idea behind this one, so the scale is a little bit different.
Use your imagination.
So of course, here we're going to have the fact that we minimize these.
Now remember, some of them are on top and some of them are on the bottom.
So if we have this value minus this value, that's positive.
But if we have this value minus that value, that is a negative.
So we have positive, negatives, positives, negatives.
And you just add all of them, you're going to end up with zero.
So what we do is we square these residuals.
We square them.
Or the error terms, we just square them.
And then we add all of them up.
And that's called the sum of squared errors.
Sum of squared errors.
And we're going to use that quite a bit, just telling you these terms when we get to them.
Of course, what happens here?
If we look at the difference between every value, every value and its mean,
while we're working towards the variance.
And of course, we're going to use this idea of the variance when we get to r squared.
Irrespective of all of this, this is all going to be some f value that we can work out.
It's going to fall somewhere in our probability density function.
And we can use the cumulative distribution function to work out this little area right there.
And that's going to be a p value.
Now, we're also going to use the f distribution to do a t-test.
And you might say, well, we have students, we have students t-test and we have a t-distribution.
That is this beautiful curve that looks like a normal distribution.
Well, we can also use the f distribution to do this.
And what we've done here is, of course, with a t-test.
Let's say, for instance, students t-test, we have a bunch of numerical variable values here.
But we have two groups.
And remember, that group comes from the sample space elements.
I'm going to say here, sample, this is horrible writing, sample space elements
of a categorical variable, a binary categorical variable.
So this categorical variable, again, another one here.
So let's make that cat 1.
This is only going to be 1, 2, 2, 1.
And remember, it's that categorical variable, that binary categorical variable that gives us
our groups when we do students t-test.
So we'll have group 1 and group 2.
And for each of these individuals, they have one single numerical variable.
And we're going to compare that very same variable between these two.
So imagine that would be the situation.
And what we're going to do is we're going to work out the mean for this group.
And we're going to work out the mean for that group.
And that is what we're going to compare with each other.
And that is how we're going to get to an f statistic that, again, we can plot here.
So not a t statistic for the t distribution, but it's going to be on the f.
So we're going to have those parameters there as well.
And then, of course, when we do analysis of variance, so this was a t-test,
but we're also going to do analysis of variance.
So there we go, ANOVA.
And for analysis of variance, we can, of course, have more than two groups.
So if our categorical variable there had three or more sample space elements,
then we'll have another group and we can compare all those means together with each other.
And again, we're going to compare that to putting the whole bunch together and its overall mean.
And I'm going to show you how all of that works and how easy it is to do.
Just use Python.
So let's open a Jupyter notebook and let me show you how this is done.
Now that you have some idea of what this lecture is all about,
let's have a look at our Jupyter notebook that we've opened here.
Remember, there will be a link in the description down below to GitHub where you can get your hands on this notebook
because there's all sorts of information here written down for you.
We can see the F distribution there and that we talked about on the board
and a lot of explanatory text there.
But let's start at the beginning.
We're going to import some packages.
So we're going to import NumPy as NP.
So we're just using the namespace abbreviation NP there for NumPy.
From the SciPy packages, we're going to import two libraries,
the stats library and the special library.
And then from pandas, we're also going to import the data frame function.
Then to do our plotting, we're going to use Plotly,
one of my favorite packages for data visualization.
So we're going to import both the graph objects,
so graph underscore objects and the express libraries in the Plotly package.
And we're going to use the namespace abbreviations,
go, go and px.
And then from, we're also going to import Plotly.io as pio
and then immediately use the templates.default setting there
and set that to Plotly underscore white.
Since I'm using a white theme here,
we're going to have plots with a white background.
And then from Patsy, we're going to import the dematrices function.
Remember, I do have a video out also in the link down below
that shows you what these dematrices are all about.
And Patsy, they really make it easy for us then to use our data
inside of packages such as the statsmodel package.
Statsmodels, as you can see there,
and we're going to import that statsmodels.api as sm.
So let's do that.
Now that we've imported all our packages,
just a reminder then there of what def distribution is.
And what I've created here, remember the function on the green board,
is there an equation, the function, the green function on the white board.
It is written for us there.
And I'm creating a little Python function there
just to calculate all of that for us.
And you can see special.beta there.
That is the mathematical beta function.
And then I'm just going to generate a couple of these distributions,
just given different values for our two parameters.
And there we go.
We see three different distributions there,
given D1 values for D1 and D2, as you can see there.
And once again, we're going to find an F statistic for our test,
an F ratio that's going to be somewhere here.
And we're just calculating the area under the curve right there.
So let's do one example.
I'm going to use D1 as 1 and D2 as 10.
And I'm going to create an F statistic right down here at 3.5.
So we're actually calculating the area under the curve there.
But we use not the probability density function.
Remember, we use the cumulative distribution function.
And that's how you would do it.
As I showed you on the board, it's 1 minus stats.f.cdf.
3.5 was our F ratio or our F statistic and the two parameters.
And we get a P value for that.
So that was just all a bit of a reminder.
I'm sure that you have knowledge of all of this.
So let's get going and talk about what this tutorial is all about.
We're going to start with simple linear regression.
So we're going to have univariable linear regression.
We're going to have a single independent variable predicting our dependent variable.
And let's create two of them.
So I'm using the random.seed function inside of NumPy.
And I'm setting an integer 7.
If you do the same, you're going to get the same pseudo-random numbers as me.
And I'm using the NumPy.random.uniform.
So I'm taking from a uniform distribution.
And with a low of 10 and a high of 100.
So on that interval from 10 to 100, we want 20 of those values.
And I'm wrapping all of that as first argument in the round function.
NumPy.round.
And then comma 1.
So we're just going to get one decimal value.
And then the dependent variable.
What I'm going to do, again, wrap something inside of the round function.
And again, comma 1 there at the end for the second argument for the round function.
Just so that we just have one single decimal value there.
And what I'm doing, I'm taking every independent value.
And I'm adding a bit of random noise to it.
So that'd be broadcasting the element-wise addition of these two arrays.
So NumPy.random.normal.
So that comes from a normal distribution with a mean of 0 and a standard deviation of 2.
And again, 20 of those.
So I have 20 in both arrays.
And we'll just do element-wise addition of those.
So let's just visualize in the scatter plot what that's going to look like.
And you can look at the code there for the go.scatter function.
And as you can see, there's beautiful correlation here.
As the value for the independent variable increases, so does the value for the dependent variable.
And I remind you that each of these markers would be one observation in our data set.
So one row of data for that individual.
There were two numerical variables.
And we have a representation of both of these.
So each of these markers belongs to a single observation.
And now I'm just going to convert this into a data frame.
And I'm using, as you can see here, curly braces.
So this denotes the use of a Python dictionary to create this data frame.
So now we're going to assign that to the computer variable df.
Then use the data frame function.
And here's my dictionary.
I've got a key value pair right there.
The key being independent in quotation marks.
So it's a string.
And that's going to be the name of that column, the name of that statistical variable.
Independent, and I'm assigning the value to it, all my 20 values in my computer variable independent.
And then dependent, dependent, and then group.
This is going to be assigning a key that's a list of all these individual characters or strings.
So it's just doing that by hand.
And you see C there and E.
Let's imagine that one group of observations are the control group and the other ones are the experimental group.
And I'm using very easy, simple indexing there.
I'm just going to see the first five rows of data.
And there's our lovely data frame.
So we can see these values.
16.9, 18.0, and then that observation was in group C.
Now we're going to convert this into a format that's ready for use when we do linear regression.
That's very important.
And for that, we're going to use the dematrices function inside of the Patsy package.
And Patsy creates, you can use as first argument, these very easy formulas,
which reminds us of the formulas that we can use in the R language for statistical computing.
So it makes it very easy.
And it uses this little tilde symbol, which is in different places on different keyboards.
You'll have to find yours.
And it says dependent by independence.
I'm trying to predict the first one given the second one.
And if we had more, if it was multivariable logistic regression,
we just put a plus there and add a new name, a name of one of the other columns
and a name of the other columns.
And please watch that video on Patsy if you want to know exactly how to do this.
And then a second argument, we've got to specify where these variables come from
that are used in this formula, comes from the DF data frame.
And what this is going to do is it's going to create two separate entities for us.
So we're going to have two variables there, a Y and an X.
Y is going to be my dependent variable vector, and X is going to be my feature variable,
my feature matrix, the matrix that contains both my intercept and my independent variable.
I'll show you what that looks like.
So let's just do that.
Let's have a look at Y.
And indeed, Y, as you can see there, that is just my dependent variable.
18, 80.4, 50.0, and we see the values there.
OK, so it's a design matrix, that's the data type when we use the D matrices function in Patsy.
But let's have a look at X.
So that's my feature matrix, also a design matrix data type from Patsy.
And we see our list of independent values there, 16.9, 80.2, 16.9, 80.2.
You see them up there, 49.5, 49.5.
They're all there.
But we've got a new column called the intercept column, and that's all ones.
And if you look at the linear algebra, if you wanted to look at the linear algebra behind the scenes here,
you need that column of ones to do your matrix multiplication.
And I remind you, just this is what we're trying to achieve here.
We have all the values here, 18, 80.4.
I hope I jotted them down correctly.
If not, this first vector, that is my Y vector, my dependent variable.
And we're trying to predict its value.
And for that, we need this column of ones, and we need this column of my independent variable, as you can see there.
So what we're looking for is these two parameters, beta sub zero and beta sub one.
Such that, if I look at that 18 right up there, I'm saying if I have the right value for beta zero,
and I multiply it by one, that first one there,
plus if I have the right value for beta sub one, times 16.9.
There we go, times 16.9.
And that should give me very close to 18.
There'll be a bit of an error, though.
It won't be exactly 18.
But if I choose my values for beta sub zero and beta sub one very nicely,
my error value is going to be very small.
And it's in bold there because it's also a whole vector of values, in case you were wondering.
And if I just have to get the right beta sub zero and beta sub one,
and beta sub zero, that's going to be my Y intercept when X is zero, remember.
And my beta one is going to be my slope.
So if I choose those correctly, it'll minimize the error,
and I'll get as close to, for each individual value,
I get as close to my dependent variable value as is possible.
And again, this is what we had on the whiteboard.
All our values, our model is going to be this blue line.
And again, for any independent value down here,
the model predicts on the blue, on the line, this predicted value.
But the actual value for that observation was way up there.
And this difference there is called the error or the residual.
And there we go.
There's those residuals, the difference between every value
and what the model would predict for that value.
And as I said, if you add all of those up, you're going to get to zero.
So we actually square all these values.
Remember, if you square anything, minus three squared becomes positive nine.
And so we square all of those.
And this is what this little graphic is trying to say.
So we square all of these.
And what we want, we want values for beta sub zero.
That's our y-intercept, beta sub one, which is our slope.
That determines then this line.
And it'll be the best fit because these squares will be the smallest that they can.
So if we add all of them, they will be at their very smallest.
And then there's a way to determine how good our model does.
I mean, any model like this we can fit a line to, no problem.
But we've got to somehow express how good this is doing.
And we do that through what is called the coefficient of determination or r squared.
And what r squared does is just this little ratio.
And sigma squared, as you can see there, that's the variance, in other words.
So we're going to take the variance of some mean model.
We're going to see that.
And then we subtract from that the variance of our best model divided by the variance of
our mean model.
So it's just this fraction of variance that we're trying to express here.
And in the end, we'll see that our model is going to explain a certain fraction of the
total variance in the dependent variable.
Don't worry about that.
Let's have a look at how to do this.
Now, you'll remember on the board, we had this mean model.
So that's the one, irrespective of what independent variable value is.
It's always going to predict the mean of the dependent variable.
And that's what we're doing here.
So I'm just calculating the mean of my dependent variable up there.
So I'm using numpy.mean, the mean function, passing my array of values there for the dependent
variable.
And I'm assigning that to the computer variable mean underscore dependent.
And we're printing that to the screen.
And we see it's 52.89.
So if you set your pseudorandom number generator also with a seed of the integer 7, you're going
to get exactly the same values.
And you'll also have 52.89.
So let's have a look at a scatterplot of this model that we're doing here.
So the line colors have changed around.
But there's my mean right up there.
This is more towards what we had on the board.
So whatever the independent value is, it's going to predict 52.89 for each one of these
independent values.
And you can see the residuals, the errors, they're going to be quite large because I could
draw a line straight up there that's going to look a lot better at an angle going up towards
the top right from the bottom left.
And that's going to have residuals that are much smaller.
And we'll see that.
But this is the mean model.
And for this, it's R squared.
The coefficient of determination, that is how we calculate how good our model is because
it's based on this very bad model, which is the mean model.
And I remind you then, if we look at this one down here, it has a dependent value of 10.9.
And the model is predicting all the way up here.
It's predicting, what was it, 52.89.
That's a big residual.
And you can see a little calculation there for it.
One thing I want you to do, though, is just consider the dependent variable on its own and
its mean and all its values.
So it's just the dependent variable here on the vertical axis, on the y-axis.
The residual is nothing other, if I square all the residuals, add all of them up, and
divide by how many there are, what do I have?
I have the variance.
So if I remember what the variance is, it is just this average difference between each
value and its mean.
But we square all of them so that we can add all of them up, and it remains a positive value,
and we just divide by how many there are.
That's the variance.
So it's the sum of squared errors, the difference between the actual value and the mean.
We square that, and then we add all of them up, and we're going to divide by how many there
are.
That's just the variance, as you can see there in equation four.
So let's just have a sample size n.
We're going to assign the len, L-E-N, length, of the dependent variable.
That was 20, remember?
We had 20 numbers in there.
I'm just going to save that as a value.
And let's do this little equation.
We're going to say dependent minus the mean of the dependent.
And remember, Python will do this element-wise.
It'll take the first dependent value and subtract 52.89 from it.
Then the second one, subtract 52.89, etc.
It'll carry on like this.
Square each of those.
So I've put those in parentheses there.
And then to the power, remember in Python, two stars there.
Two, that would mean square.
So I square all of those, and then I pass that to the numpy.sum function.
So I'm squaring all those differences.
Then I'm summing all those squares, and I'm dividing by how many there are.
And remember, that's nothing other than the variance.
In actual fact, I could use the var function in numpy.
So np.var, pass the dependent to that, and I get exactly the same value.
And that's only because we're using the mean that this turns out to be the variance.
So let's look at stats models and get our best fit model.
So stats models are a wonderful Python package, and it can do all sorts of statistical tests.
And the one we're going to use, now remember we used the namespace abbreviation sm for statsmodels.api.
One of its functions is OLS, all uppercase, OLS, ordinary least squares.
So that uses linear algebra, and we're just closing down the span of the vectors that we're dealing with.
Don't worry about that.
That's all linear algebra.
But I'm passing my vector of dependent variables and my feature matrix X there.
And then I'm saying .fit.
I'm using the fit method there, and that is going to fit my data so that I can get these values for beta sub 0 and beta sub 1, the intercept and the slope, that are the best possible.
And I'm assigning this to the linear underscore model computer variable.
And there we have it.
We have a linear model with a snap of a finger or the hitting of a key that was done.
So this linear model, it has a bunch of attributes and methods to it.
One is with summary 2.
So I'm just calling the summary 2 method there on my model.
And it prints out a beautiful summary of our model.
And what you can see hiding there, first of all, there's an F statistic on the right-hand side, 2,418.
We see a p-value for that F statistic, and that's just abbreviation, something that's 10 to the power negative 20.
That's just computer truncation.
That means it's basically 0.
So a tiny, tiny little p-value.
So a very significant model.
And of course, you would expect that because, you know, we just added a little bit of random noise when we created those values.
But you also see an R-squared value here on the left, 0.993.
Now that coefficient of determination, it goes from 0 to 1.
0 means the model will be as bad as the mean model.
And anything better than that, as those residuals get smaller, the R-square starts to climb.
And if it's at 1.0, it's perfect.
And then you want all those dots in a line with no errors whatsoever.
And then when we look at the coefficients here, we see the intercept, 0.2150, and the slope.
That's the independent variable's coefficient, 0.9880.
So what we're saying here is that if we take 0.2150 plus this 0.9880 times the independent variable value,
that is going to equal the predicted dependent variable value, just as I showed you using those vectors before.
So let's have a look at this.
You can have a look at the code there.
You can pause the video and just have a look at the code.
And there we have it, the same data.
And now we have this line, very different from the mean model.
And you can well imagine that these residuals are a lot smaller than when we talked about the mean.
And we can do sort of the same thing as work out the variance in all the residuals.
But first of all, we can just list them all by using this .resid attribute of our linear model.
So linear underscore model.
That's the computer variable that we used when we created the model, .resid for residual.
And you see all the residuals there for all the values.
And then as an aside, I can actually use the .predict method and pass my independent values to this model.
And it's actually going to show us the predictions that it makes.
So all of these values lie on this red line.
Given all the independent values, that's what I've passed it.
It shows me what all the values on the red line is going to be, what the model predicts.
But what we want to do is we want to calculate the variance in this best model.
So you can just say numpy.var of all the residuals.
I just want the variance in those residuals.
And we see that there.
And now we can calculate R squared.
Remember the equation that we saw before?
So I take the variance in the mean model, subtract from that the variance in the best model,
divided by the variance in the mean model, and we get the same 0.99.
And if we round that 2 up to a 3, because there's a 6 that follows,
we get exactly what we got here, R squared right there, 0.993.
So we can really just calculate this by hand.
And it gives us a good intuition of what this R squared is.
And as I said, the interval for R squared is between 0 and 1.
And what we can say is adding this independent variable to our model
explains 99.3% of the variance in the dependent variable.
That is the interpretation of R squared.
How much, or what fraction of 1, or then 100%, if you want to multiply it by 100%,
does the model, all these independent variables that we put in,
what fraction of the variance in the dependent variable does it explain?
And our model obviously explains a lot of the variance, because it's a very good model.
Now we saw an F statistic, or an F ratio, and a P value there.
So let's have a look at how to do that.
And that depends on these two parameters.
So it's tiny here on my screen.
And in the video, you're probably not going to be able to read that.
But have a look at the notebook on GitHub, and you'll be able to see it for yourself.
It's a ratio.
So we see a numerator and a denominator.
But the numerator has a numerator and denominator.
And the denominator has a numerator and denominator.
And both in the numerator and the denominator,
its numerators stay the same as what we had with R squared.
But we're dividing each of that numerator and denominator by something else.
And those are D1 and D2 that we saw when we looked at the probability density function of our F distribution there.
And what that says is, right in the tiny little bit down here at the bottom,
in that little piece that I've highlighted in blue now,
its denominator says P of the best model minus P of the mean model.
Now that has nothing to do with P value.
It's the number of parameters.
Now the number of parameters in our best model,
remember that had two parameters, an intercept and a slope.
And our mean model only had one parameter.
It's only the mean.
So that's 2 minus 1, and 2 minus 1 is 1.
So we have one of our D values there.
And then the bottom one says N minus P best model.
N is the sample size.
And P best, remember how many parameters are there in the best model?
They're two parameters.
So we had 20 cases in our study, minus 2, that's 18.
So we have values for D1 and D2.
And those two parameters define their probability density function.
So let's just save them there, P best and P mean, as 2 and 1.
And then I've got my F value that I'm calculating here,
the variance in the mean model minus the variance in the best model.
That's over P best minus P mean.
And we have on this side the denominator, which is the numerator and denominator.
It's var best model, the variance in the best model, minus N minus P best.
So if we do that, we have an F statistic, 2,418.
And that's exactly what we had.
Let me go up and prove it to you.
There we have an F statistic, 2,418, right there.
So we've calculated by hand.
And we know the easy equation now for our specific F statistic.
All that remains now is to plug it into 1 minus the CDF.
Remember, we want that little bit of area under the curve to the right.
And for the CDF, we say we want an F statistic because we're using F dot CDF in the stats library of SciPy.
F dot CDF, it wants an F ratio or an F statistic, 2,418.
And then it wants this P best minus P mean.
So that'll be our D1.
And N minus P best, that'll be our D2.
And if we do that, we get something that's truncated as well.
Remember, it was times 10 to the power minus 20.
But I mean, that all is just truncation.
It's basically a P value very, very close to zero.
Now that we've looked at the univariable model, we have a single independent variable.
Now let's add another independent variable.
This makes it a multivariable linear regression.
You also get multivariate, but that refers to how many dependent variables there are.
So we're just talking about more independent variables.
So I'm going to create a data set for us there.
You can have a look at the code.
What that does for us is variable 1, variable 2, and a dependent variable.
So we're going to try and use variable 1 and variable 2 to predict variable 3.
And we're going to do exactly the same thing.
So let's just have a look at one way to visualize it.
Perhaps not the best way.
We have variable 1 on the x-axis, variable 2 on the y-axis.
And then the color of that is the dependent variable.
We can also do a matrix plot, matrix scatter plot.
And then we look at pairs of these.
So of course, variable 1 against variable 1 would be this very nice correlation there.
But we see not so good for the other pairs.
So let's see how this model does.
First of all, we're going to create these design matrices using the dematrices function in the Patsy library, Patsy package.
And there's our formula.
And as I said, we just start adding them.
It's not plus as in 1 plus 1 is 2.
No, it's just listing all of the variables.
And please watch that video on Patsy to see how these work.
So I have my variables, my design matrices.
And if we look at x, we have our column of 1s for the intercept.
But we also have variable 1 and variable 2 listed there.
So let's use ordinary least squares.
Again, very simple.
sm.ols pass my vector that is just my design matrix of my dependent variable.
And now my feature variable x, that's a design matrix with the three columns.
Use the .fit method.
And I'm going to save that as a computer variable.
And then also use the .summary2 method.
And we can see we still have an F-statistic there, a P-value for the F-statistic, and an R-squared value.
Very poor R-squared.
Look how close that is to 0.
And we're going to do exactly the same thing.
We're going to create a mean model and look at the variance of the dependent variable.
So it's this df for my data frame, dot-dependent.
So just the variance of that.
And then the variance of the best model.
Again, I'm going to use multi-lin-model dot resid.
That gives me all the residuals.
And I'm looking at the variance in the residuals.
And I'm going to save that.
And my R-squared is going to be exactly as we did before.
And we get what we had in the model, 0.07.
So a very poor model.
As far as the number of parameters are concerned, there are 3 in the best model.
Remember the coefficient and variable 1 and variable 2.
So that's 3.
And in my mean model, there's still 1.
It's just a mean.
And length is the number of cases we have there.
So I'm just passing df dot dependent, any one of the three columns.
And just taking the length of that for n, which means we can do an f-statistic, 3.71, just as we had before.
And then the p-value of 0.0279, just as we had when we used OLS.
We get exactly the same values.
And I think you now know how the R-squared function works.
And how the f-statistic or f-ratio function works.
So that we can calculate an f-statistic and a p-value.
And that being said, let's now use it instead of the student's t-test.
So I'm going to create for the same variable two sets of observations.
And I'm going to call them group 1 and group 2 using Roman numerals there.
And both come from a normal distribution.
And we're going to make a mean of 100 and a standard deviation of 5 for the one group.
And for that very same variable, 103 and 8.
And I have 100 observations and 110 observations.
And then I'm just going to create another NumPy array that just puts all those two together.
So I'll have 210 values, all of them together.
So let's have a look at that.
There we have for the same variable for group 1 and group 2 individuals.
And we see, you know, you can see the difference there.
So let's do student's t-test.
It's the stats module or the stats library, whatever term you want to use.
Stats module in the SciPy package.
And I'm using the function t-test underscore ind, independent variable t-test.
And I'm passing the two sets of values there.
And we see a p-value of 0.014.
Now let's see if we can use the f-ratio to do exactly the same.
Again, I'm going to have this mean model.
So I'm grouping all 210 individuals together.
And I'm looking at that single numerical variable.
And I'm just calculating the mean for that.
And now I'm looking at the sum of squared errors.
So I'm not doing variance anymore because I can't divide by the same sample size.
It's easy enough when we did linear regression because we have pairs of values or two independent
variables and a dependent variable.
But they all come from the same observation.
So the sample size is always the same.
But here we had a sample size of 10 and 110 for the two groups.
Can't do it anymore.
So I've just put all 210 values together.
And it's each individual value minus its mean for that whole set and square all of those.
So it's the sum of squared residuals.
And I'm using residuals from as far as the mean is concerned.
And we're going to save that as ss mean.
So we're not dividing by n to make this into variance anymore.
And then I'm individually looking at my two groups.
So for that variable minus the mean for that group and then squaring each of those and summing
them all up again for group two.
And then the best model just adds those two sum of squared errors.
So once again, it is only the sum of squared errors.
So just to actually take those residuals, if I can use the term residuals where my model
just predicts the mean, and that's all I do.
Now, in my best model, I have two parameters because I have two means, one for group one
and one for group two, which I added together.
And for my mean model, there's only a single mean.
And then I just need to have a length for all of them, the all 210 combined.
So n is going to be 210.
So if we do that, now this is my f ratio now.
So this is not the same as with the linear regression.
Now, with the linear regression, I can tell you now, you can also drop the n, dividing by
n, and just do sum of squares as well, because all your n's there are the same anyway.
So here we have sum of squared errors for the mean model minus the best model, divided by
the d1 at the top, and then sum of squared errors for the best model, divided by the sample
size minus the number of parameters.
So that'll be d2.
So there we save it as an f, and then we're going to do a p-value for that.
And if we go up and we look at students' t-test, the p-value there, 0.014, exactly the same
as what we calculated now with an f-statistic.
And that's quite wonderful, isn't it?
And when it comes to analysis of variance, remember we can use that to compare the means
of more than three groups.
So I'm just going to create a data frame for us there.
Let's have a look at the first 10 observations, and what I've created is a single variable
and a group A, B, C, A, B, C, etc.
And now I'm just going to save them once again as separate NumPy arrays, so you can see the
code for that there.
So we have the three lists.
And then we can just describe that per one of these groups.
So we get group A, B, and C.
We get how many there are in each group.
And we get a mean for each of these three, a standard deviation for each of the same
numerical variable.
And we can do a box plot of those three.
And we can see we're probably not going to find a difference in means between these three.
So there is a f underscore one way for one way analysis of variance in the stats module
inside of the SciPy package.
And we see a p-value there and a statistic.
And once again, as we did before, we can sum all of them together, get a mean for that.
We'll put them all together, get a mean for that.
And we can do that for each of the groups.
And eventually, we'll just add all of them to get our sum of squared errors for the best
model.
Now, once again, we have three groups.
So the number of parameters in the best model will have a value of three.
And for the mean model, only a single mean.
And the sample size we save there.
And again, we're going to use that exact same equation for our f statistic and for our p-value.
It really is as simple as that.
So we can use this idea of the f distribution and the sum of squared errors when we're talking
about numerical values for linear regression and when we compare between two categorical
sample space elements for the same numerical variable.
So I hope you enjoyed this explanation of the use of the f distribution when it comes to
linear modeling.
So, thank you.

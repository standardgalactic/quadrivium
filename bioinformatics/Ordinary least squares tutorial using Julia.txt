So here we are in a Plutonotebook and we're going to use Julia to try and understand ordinary
least squares, at least as it pertains to creating or solving for quadratic, cubic,
and linear equations given some points in the plane. So I'm going to assume that you have
knowledge of the following. Vector operations, as you can see there, specifically adding vectors,
vector addition. Matrix operations, and that includes taking the transverse of a matrix,
taking the inverse of a matrix, and when you can take the inverse of a matrix when it exists.
Matrix vector multiplication, importantly the column space of a matrix. I'm going to show you
a little bit of a schematic just to remind you what that's all about. Linear independence,
so if we think about the column vectors that make up a matrix, that those column vectors are linearly
independent. And then also a projection of a vector onto a column space of a matrix. So that subspace
of a matrix, how to project a vector onto that subspace. So the Julia packages that we're going
to use is the plots package, and I'm using plotly as my backend, as you can see there. I'm using the
linear algebra package or library that is part of Julia, and then also the row echelon. So if you're
going to set up your environment, also add or install the row echelon package. So here's our example
problem, and we can see a nice neat little plotly graph here of that. So I want to find a quadratic
equation, later on we'll do cubic and linear as well, through the origin of two space, and the points
1.5 comma 1, this little one here, the point 2 comma 5 out there, and the point negative 1 comma
negative 2. So I've got to find this parabola basically, that's what a quadratic equation is,
the order 2 polynomial, that goes through these three points and the origin. So I'm not going to have a
y-intercept, my y-intercept is going to be zero. And you can see the code there, in case you're
interested, and how I generated this neat plot. So let's think about the quadratic approximation.
First of all, how would you set this problem up? And you can see how it's set up there in equation
number one. Because if you think about a polynomial, a quadratic equation that goes through the origin,
so we don't have a y-intercept, that's just going to be here.
y equals a sub 1 times x, so some coefficient of x plus some coefficient of x squared.
And what I'm going to do is use these beta sub 1 and beta sub 0 and beta sub 1 with these hats on,
and I'm doing that because when we do, when we use this building linear models, and we're using
samples from a population, those would usually be the symbols that we use. So I'm just going to use
those beta sub 0 and beta sub 1 with a little hat on. It's all just a reminder that these are
approximations and parameters or values that I calculate from a sample in trying to get the
idea of what's going on in a population. So if we write it out like this, I'm going to put the
approximate symbol there, and you'll see a little bit later why this is so. I'm going to say some
vector y equals beta sub 0, that's a constant, a coefficient, times this vector x plus beta sub 1,
again, it's just going to be a coefficient there, scalar, times the second vector, which is just
going to be the square of each element of the vector. And in 2, it makes intuitive sense what's
happening here. I'm saying, take my point 1.5 comma 1 in the plane. I'm saying 1 is going to be
approximately equal to, I'm going to get very close to 1, but I won't get to 1, and we'll see why,
if I take my x value and my x squared value, and I multiply each of those by the two coefficients,
and my point 2 comma 5 exactly there, and my point negative 1 comma negative 2 in the third line there
of equation 2. And you can see here in 3 how we set this up as vectors. There's my vector y of my y
values. It's going to be some constant multiple of my vector of x values plus some constant multiple of
my value squared, each value squared. And if I write this as a matrix, as vector y, it's going to equal
these two column vectors x and x squared, which we see here in the third line, times this matrix,
creating a matrix of that times a vector containing the elements beta sub 0 and beta sub 1. So in the
end, I'm going to have a very neat way of writing it. Y, my vector y is going to be approximately
equal to some matrix times a vector b. And now I just want to show you the schematic of what is going
on here. So here we are in just doing a google drawing here. It's just a schematic of what's going
on. So it's not the actual vectors in our problem. And what we can see, we have three space. We have our x
coming out towards us here, our y coordinate and our z going up. And we're representing the schematic
idea of these two column vectors of A. They are linearly independent of each other, and we'll prove
in our problem that they are. And if they are linearly independent, but there's only two of them,
we cannot possibly span, they can't possibly span all of three space. They're going to span a subspace of
three space. And that subspace is this plane that goes through the origin. So a linear combination of
those two column vectors will get me anywhere in the plane, but they certainly can't get me to y.
Y is outside of the plane. So I will not be able to find this beta vector such that if I multiply
matrix A with that vector, I'm not going to get out of this plane. What I will be able to do is get to
this y parallel. Y parallel is in the plane. So let's look at this y parallel in orange here. It is in the
plane. So a linear combination of those two column vectors of A will get me to y parallel.
And how do we set up y parallel? Well, that is a projection of y, an orthogonal projection of y onto
my plane. So this vector y parallel that goes out here, it's parallel, and it's going to be parallel to
any vector in the space. And that's almost the shortest distance. And that's why we get this idea of
least squares when we talk about ordinary least squares here. It is this distance here that we're
trying to minimize in essence, but we're setting this up geometrically. So we don't have to worry about
about explaining it in that form. So this is a perpendicular vector out there. And we've decomposed y
such that y equals y parallel plus y perpendicular. And one thing I want you to think about when it comes
to y perpendicular, remember y perpendicular is orthogonal to any vector here in the space.
So if I do the dot product of any vector in this column space, this plane, with y parallel, I'm going
to get the zero. I'm going to get zero. And think about what happens if I take the transpose of A. So
every column vector, and I just said a column vector dotted with a dot product with this perpendicular
vector should give me zero. So if I take the transpose of A, now each of those column vectors
becomes a row vector. So I'm taking a row vector times this column vector, and that is going to give
me zero because I'm dotting two perpendicular vectors with each other. And if I do that for all
the rows in A transpose, which remember are actually all the columns in A, I'm going to get zero,
zero, zero, zero, and I end up with a zero vector, as you can see there. So let's look at the bottom
here. Remember y, I can deconstruct it as y parallel plus y perpendicular. And if I just isolate y
parallel, that's y minus y perpendicular. And remember we said we have this approximation,
A times beta is approximately equal to y. I can now make it an equal symbol because I'm just solving
for y parallel here. And we see y parallel is in the plane. In other words, there is going to be a
linear combination of these two column vectors in A that will give me y parallel. But I'm rewriting
y parallel as y minus y perpendicular. Now look what happens, and this is the beauty of this geometric
interpretation of least squares. If I take A transpose, left multiply A transpose by both sides
of the equation. So it's A transpose A times beta. That is going to be, I'm just distributing A
transpose into y minus y perpendicular. So it's A transpose y minus A transpose y perpendicular. But
we've already seen that A transpose y perpendicular, that just ends up being the zero vector. So I can
throw it away. And in the end, this left multiplication by A transpose leaves me on the right hand side
with an equality symbol to A transpose y. And that's really beautiful. Because if you think about
A transpose A, first of all, if we multiply those two, and A is not even a square matrix,
and our instance is going to be a three by two matrix, if I left multiplied by its transpose,
I end up with a square matrix. And if I have a square matrix, I might potentially be able to take its
inverse. And if those columns of A are linearly independent, which we see in our problem that it is,
that A transpose A, that square matrix, will have linearly independent columns, and the inverse will
definitely exist. And that's going to help us a lot. So let's get back to our problem.
So there we see in four, we go from this approximation to this equality that A times
beta hat, that's going to give me y parallel, which is just y minus y perpendicular. And now we
can see the approximation. So I've left multiplied by A transpose on both sides. And then this A transpose
y perpendicular falls away, it's just the zero, it's just the zero vector. And lo and behold,
A transpose A, if all the column vectors in A are linearly independent, A transpose A is a square
matrix of which we can take an inverse. And we take the inverse left, left multiplied by the inverse on
both sides. So on the left hand side, I have the inverse of something, the result of A transpose A,
times itself, well that just gives me the identity matrix. And the identity matrix
times this beta hat, we're just left with the beta hat there. And on the right hand side, eventually,
we get to our equation for ordinary least squares. That beta is going to be A transpose A,
it's inverse, times A transpose times y. Now that's an equality, but that beta, remember,
it's just going to eventually be, as far as my problem is concerned, an approximation. So let's
see our specific problem. I have A there, my two column vectors, I have my vector beta, and I have
my right hand side. This is what we're going to end up with. It is still going to be an approximation,
even though I can solve for beta sub 0 and beta sub 1 now. We've just shown that we cannot get to that
vector. That vector is outside of the column space. Let's just prove that. If I take my matrix A,
1.52-1 and 2.2541, and I take its rank, remember, the rank is going to tell me whether those columns,
column vectors are linearly independent. And we see we get a value of 2 there. In other words,
the rank is 2, those two are linearly independent. Another way to show that these three are linearly
independent, in other words, y cannot be in the column space of those two vectors, is just to take
all three of them together as three, as three column vectors. I'm just adding y to this column vector,
and I'm taking the row echelon form, R-R-E-F, from the row echelon package or library and applying that,
and I get the identity matrix. That means it's only the zeros that solve for the homogeneous system. In
other words, those three are linearly independent, and they will span three space. In other words,
that y cannot be in the column space of A. Another way to look at it is just to construct this plane,
an equation for the plane, and remember, for that we need a point in the plane and the normal vector,
a perpendicular vector. How do we get the perpendicular vector? Well, we take the cross product
of the two vectors in the plane, and we have that. So if we take the cross function there of our two vectors,
we end up with a normal vector, and you see here in 8 how to construct, remember, the equation of a plane
through the origin in three space. So A, B, and C, those would be the components of this normal vector,
and then x sub 0, y sub 0, z sub 0, those would just be a point in the plane, and I have two points.
That's just the tips of any of the two vectors in that plane, and if we solve for that, we get,
in the way that is set up, it's obviously just going to be A, B, and C from the normal vector there.
Those are going to be the components. And what we can see, if we plug 1, 5, negative 2 into there,
it doesn't solve that. That point, that vector is definitely not in the plane.
So I hope that makes sense and you understand what this problem is all about. So let's just
use Julia and just solve this problem. I'm going to create a matrix here called A underscore quadratic,
and those are my two column vectors in it. I'm creating a vector y, and you can see it there,
and now I'm just using equation 6 to solve for that. The inverse, as you can see there,
of the transpose times the matrix itself, if I take that inverse and I multiply it by the transpose
of my matrix, and I multiply that with y, I'm going to get the solution, and it's going to be a 2 by 1
matrix. In other words, a column vector with two elements. And it's going to be, very neatly for us,
beta sub 0 and beta sub 1. So what I'm doing in this column here, I'm just saving them individually,
and then so that I can plot this, that we can see here. So I'm just creating x valves and y valves
quadratic here, and I'm using the very first equation we looked at. It is beta sub 0
times the x value plus beta sub 1 times the x value squared, and I'm just creating a lot of points
here using a range object there, and I'm just using collect on that so that I get this all this
vector of values, or array of values, and getting all my y values so that when I do plot it,
it's going to create a line for me of all these x, y values. So there you can see my three original
points, and this is going to be a quadratic approximation. This is going to be the best fit
to these three points. They are not going to, it's not going to go through the three points,
because those three points, remember, as a vector, the y values at least, as a vector is not in the
column space of a. So if we have these three, and remember, we didn't add a y intercept,
so it is going to go through the origin. Now with these three points, if I have a cubic equation,
that's going to be a solution to this problem. It will go through all three points. Let me show you.
In 10, very easily, we're now setting up our column vectors as x, x squared, and x cubed,
and now we need three coefficients. Now I'm just going to show you that x,
if I take x as a column vector, x squared as a column vector, x cubed as a column vector,
so remember each element is a squared and then cubed. If I do a row reduction on that to row reduced
echelon form, we see we end up with the identity matrix, and those three are definitely are linearly
independent. So I'm creating a matrix here called a cubic. You can see it there. My first column
vector is just my x values, and then I square each x value, and then I cube each x value,
and then I'm using equation six again, just ordinary least squares, which gives me three
solutions, beta sub zero, beta sub one, and beta sub two, which I save individually there,
and I'm just doing the exact same thing.
I don't have to run x values again, because remember, we already created it, and now I'm
just setting up my cubic equation, as you can see here, as we set up, and there we see our solution.
We're just plotting with a code down here, we're just plotting the three points again,
and we're plotting this line from all the points that we've created up here,
and that cubic equation is going to go through all the points. And again, I have not set up a
y-intercept, so it's going to go through the origin.
Now, this might look all beautiful, and if I have four points, I have to go to a fourth order
polynomial, et cetera. It might look beautiful, but it's kind of useless when it gets to applications,
because what we're doing with least squares, one of the applications is to build a model,
and we want a model to not overfit our data. Here's our data, and this model clearly overfits,
because it exactly fits this data. And when we try to put unseen data in this model, for instance,
if we had x values, our model's going to calculate what the y should be, but it might be way off,
because this method that we've used here, it's just memorized, as we said, the data,
and it fits it perfectly. And usually, in practice, that leads to overfitting. What we would want in
a situation like this is just a linear approximation. And how do we set that up? So I'm just
reminding you, a straight line equals y equals mx plus c, or mx plus b, whatever you had at school,
m being the slope, c being the intercept. And the slope m, we're just going to set that to beta 1,
and then you're going to have this y-intercept beta 0. To set that up as a vector here in 11,
I've got to introduce this column vector of all ones. So if we look at the first point, 1.5,
comma 1, that's going to be 1, that's approximately, that's going to equal beta sub 0,
remember it's just times 1, so it's just beta sub 0, plus 1.5 times beta sub 1. So that is this
equation, y equals mx plus c, so for each three of those points. So setting up this column of 1
makes complete sense. And I'm just, you know, just a shorthand notation of what's happening here,
and I'm just using slightly different notation, just to show you that it exists, in essence,
in the end, if I do multiply a times, and this is x here, but remember that x is going to be not the
extent of the x points, but the beta value that we're looking for, the beta values that we're
looking for, if I do multiply a times the beta values that I find, I'm not going to get y exactly,
I'm going to get some predicted y, some approximation of y. So I'm just showing you
some different way to think about what we do with the final solution, but this is not part of the way
that I've explained it here. Again, just showing you very quickly, if we want to set up an equation
for that plane, it's the cross product of those two column vectors, that's going to give me then
the normal vector, and the coefficients of those are going to be used in the plane. So there's the plane,
just to show you, and then you can plug in 1, 5, 2 there, and see it's not a solution,
it's certainly not in that plane either. So let's set up a linear, my matrix there,
a underscore linear, my column vector of all ones, my column vector of all my x values,
I use ordinary least squares equation six, and we solve for beta sub zero and beta sub one,
which we have there, and again I'm just creating a bunch of points here, so that I can just plot my
linear equation, so it's beta sub zero linear, and then dot plus, because I want element wise addition of
the scalar times all my x values, and there we go, we see beautiful linear approximation, the
best fit, the line of best fit, and if you talk in statistical terms, I'm going to have
minimized my residuals, the difference between what the y value is, and what the model predicts
for any given independent or x variable input. So there's my linear approximation, it is just an
approximation of the data here, but it's probably going to fit better to unseen data, we're not
overfitting, we're not memorizing the data, so these are terms I'm using from machine learning, I'm using
from statistics, I'm just borrowing all these terms, just to show you that what we have here is very
applicable in the real world. So there we go, just using Julia to understand ordinary least squares,
the concepts of that, and how to use very simple lines of code to get these solutions.

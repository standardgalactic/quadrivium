The Z and the T distributions.
Now we've just seen the central limit theorem.
It said that I did some analysis and I calculated a mean value,
but my mean of my 30 patients was just one of many other possible ones
which will fall on this distribution.
And the central limit says that distribution will be normally distributed
and will be bell-shaped, bell-shaped curve.
Now there are two main forms of distributions.
Now I don't want to confuse you.
We're not talking distributions, normal, Poisson, hypergeometric here.
I'm talking about that little curve that the computer can draw
that will pertain to your specific research that you're doing
and where yours will fall.
Now let me run through that again just to make it very, very clear.
Imagine you are just looking at comparing the means of two groups.
You've got two groups of patients and you're comparing their mean.
What you're actually doing is just comparing the difference between the two means.
You would have 30 patients in one group, 30 patients in the other.
They each have a mean. What is the difference between that mean?
That difference. If I could now do this study over and over again
making two new groups, two new groups, two new groups, two new groups
and I calculate the difference of means between the two groups.
If I can take that difference and I start plotting it, plotting it, plotting it
that distribution will be normally distributed.
And that distribution comes in two forms or the two equations Z and T
and I'll explain to you which ones those are.
Just you tell the computer which one to do
and it really is about the situation that you are in.
It'll draw that little graph for you.
It'll see where yours falls and work out the area under the curve.
Let's go.
We're going to run the style sheet as always and just
I'm going to show you. I'm going to import numpy as np.
Numerical python as the abbreviation np.
Pandas as pd.
From scipy I want to import stats.
So I'm importing the whole sub-module stats from the scipy module.
Matplotlib we know well.
Seaborn we know well.
Filter warnings we know well.
I want to render my plots on this web page.
So I'm running that magic command and I want to ignore those ugly pink pieces of result to the screen.
Introduction. There we go.
Let's start by talking about this ability as a mind, a thought experiment.
Imagine we can take a million healthy individuals and we can do their white cell count.
Now, certain values are going to come up more often than others.
It's going to be much more common to see a white cell count of say for instance 5
than it would be in healthy individuals to see a white cell count of 1.
There's going to be this distribution of values.
I can get a very powerful machine though or I can get an underpowered machine.
One machine will only tell me 5.5.
One will tell me 5.56.
The next one 5.564.
The next one 5.5648 etc.
It can go deeper and deeper and deeper.
And that is when we start talking about continuous variables.
A white cell count is a form of continuous variables because I can go deeper and deeper.
Now, for sure there is a single white cell count that can be no further divided.
But that becomes so ridiculous because we're talking something in 10 to the power of 9 individual cells per liter.
So it becomes a bit ridiculous.
So we do talk about this as a continuous variable.
And we have to lump some of them together.
We can say well anything from 5.5 to 5.55 we're going to group together.
We call it a bin.
We're going to put it in a bin.
So we've got to get away from seeing individual values as just a single value.
There's this seamless range of values which we now start to look at.
Now random values come in patterns.
We call them distributions.
I've said that.
Most variables pertaining to healthcare come in the normal form.
This white cell count will be normally distributed.
Now let's play with some data.
I want to make all of this theory I've spoken about.
Now I just want to make it clear visually.
Now I'm just going to warn you this is not a code that you are going to write when you do your own data analysis.
It is just for us to play around with so you can get a feeling for what variables are and what distributions are.
I'm going to create this computer variable called rv1.
And inside of it I'm just going to put stats.norm and no arguments whatsoever.
So I'm just attaching the normal distribution to rv1.
There are no samples attached to it.
There are no values attached to it.
It's just the normal distribution.
But I can call this method rv1 dot mean.
If I were to type rv1 dot and hit the tab key you see there's some things attached to it.
The one I want is probability density function.
I want the mean with no arguments.
I just want that.
This normal distribution is very special.
It is the standard normal distribution.
You see there's a standard normal distribution.
It has a mean of 0.
It has a median of 0.
And it has a standard deviation of 1.
I don't know what these things are.
We will run through them.
From this distribution though let's take a few samples.
So the computer understands when it gives me these values
that some of them should occur more often
and some of them should not occur too often.
So I'm going to ask it to give me 2,000.
Look at the code here.
Computer variable rv1 underscore values equals stats dot norm dot rvs.
We've seen norm dot rvs before.
A random variable set.
The argument that it takes a size and I want 2,000.
So I'm going to ask the computer to give me 2,000
but make it come from a normal distribution please.
And in this instance it's going to be a standard normal distribution.
So the mean is going to be 0.
I'm going to put all those 2,000 values that are inside of a pandas series.
So again a new computer variable name pandas dot series.
And I put all those 2,000 in there.
And remember we can then take a pandas series
and we can just describe it.
Give us normal descriptive statistics.
Let's run that.
Indeed I get 2,000 samples.
The mean is very close to 0.
Every time you run this code you'll get new values.
Standard deviation very close to 1.
A minimum of about minus 3.7.
Maximum of 3.2.
And you see the median there.
Let's plot this out in a graph just to show you.
Plot dot figure.
Tell the computer to get ready to draw me a figure of width 8 and height 6.
What type of figure do I want?
I want an SNS distribution plot.
SNS dot this plot.
What do I want to plot in the arguments?
I'm going to say all those 2,000 values.
I don't want the bins for now.
Let Python just do its own calculation for the bins.
And let's show that plot.
There we go.
So see it was much more common to find the values around 0.
There's my kernel density estimate.
I want to hop back to this.
This is continuous random variable.
In other words this is now what we become interested in.
No longer a single value but between this value and that value.
I can put more.
I can increase the size of those bins by making the bins fewer.
Let's run that.
So there are more values inside of those bins though but this is what I'm interested in.
This normally distributed curve.
I can now ask the computer the following.
Let's do that.
RV1.
Remember now what RV1 is.
RV1 is just that.
Stats dot norm.
It's just a normal distribution.
It doesn't contain any values in it.
RV1 dot CDF.
Cumulative distribution function.
What does that mean?
Now just think back.
It might very well be that we did this analysis and our little value falls.
The mean.
The difference between our two means was here negative 2.
What was the probability of getting a value as extreme as negative 2?
The way that the computer does that it always counts from the left hand side.
So it's going to start counting the area under the curve from negative infinity up until this point negative 2.
And it's going to tell me the area under the curve of this.
How likely was it?
You can see it.
The likelihood is very small because the largest area under the curve falls here.
So that's a cumulative distribution functions.
RV1 dot CDF negative 2.
Control enter.
Control return.
And it says 0.02.
So 2.2%, 2.3% chance of getting a value of negative 2.
Isn't that a thing of beauty?
Now I can ask the computer.
What was the probability of getting a value as high as 0.7?
Now where's 0.7?
0.7 is about there.
But remember what the computer is going to do.
It counts from the left.
So it's going to give me the probability of getting a value as extreme as 0.7 and less.
But that's not what I want.
Because I'm falling on the right hand side of the middle.
So I want this area under the curve towards this side.
Towards this side.
That's what I want.
But I know that the area under the curve is 1.
So if I take 1 and I subtract from it this area up until this 0.7 spot.
What I'm left with is this that I'm looking for.
So I can say 1 minus RV1 CDF 0.07.
And I get a 24% probability of a p-value of 0.24 of hitting a value there, 0.7 about there, and more.
Just to show you that things are, let's say 2.
Now it's symmetric about the 0.
Negative 2, 2.
And you'll see the probability was exactly the same.
To get a negative 2 would have been this area under the curve here.
To get a value of 2 or more as this area here.
I just have to remember that if it's on the right hand side, it's 1 minus.
Because it always starts counting from the left.
It always starts counting from the left.
So there we go.
Your value that you find is going to be somewhere there.
I want you to remember the white cell count example that you can go into it deeper and deeper and deeper and deeper.
It is a continuous variable.
So you can only ask the question, what was the probability of not getting a 2?
We're not rolling die here.
It's not discrete.
So I can only ask the question, what was the probability of getting a value as high as 2 and more?
Or negative 2 and less?
Or 0.7 and more?
Now it's not about it being negative or positive number.
It's just you divide the graph in 2 by the median that runs right down the middle.
And you can calculate what the probability was of getting a value as extreme as yours.
As I say, it might very well have been that you did the analysis on two groups and the mean between those two were 2.5.
Let's do that.
Let's plug in 2.5.
2.5 there.
And lo and behold, you found a statistically significant difference because your p-value is 0.006 if 0.05 was our cutoff.
Okay, if that was our cutoff, I want to leave it at 0.7 where it was.
Now, there are other forms of distributions.
Not only the normal distribution or the standard normal distribution, I should say.
We can also create our own other ones to play with.
And we can use these arguments.
Now, I've combined this whole thing in one go.
I'm going to say rv2 underscore set equals pandas dot series.
And what do I want?
I want all of this.
Stats dot norm dot rvs.
So I'm going to ask it to give me out of the normal distribution some random variable set.
Loc means mean.
Scale means standard deviation.
And I want only 200 samples.
Let's run that.
Let's just describe that because we've put it inside of a pandas data series.
So we can describe that.
200 values, mean of 9.97, standard deviation of about 4.
We see the values there.
Let's plot it.
And there's our plot.
You can see it's only 200 values now.
So it's not as beautifully randomly distributed.
But once again, you can see there is now a standard mean of 10.
It was more common to have 10.
So I've just shifted it up.
I needn't go just to the standard normal.
So if you want to play with these things, you can plug in your own standard deviation,
in your own mean and your own standard deviation,
in how many data points, really how many data points you want.
What I want you to take away from this is this is where p-value comes from.
From your value more, from your value less.
And that gives us significance.
I'm going to stop this lecture here and we'll continue in part 2
where we're going to just be looking at completely random sets
and then explaining exactly what a z and a t distribution are.
One is falling for a zero,
it is coming that way when a net happens.
A unut-like room, I think.
Into the other one.
I would imagine that I want you to say something.
In a moment the dream.
This is where all you dream are,
it is coming that way as yourself is not !!
By this one, you'll see how many elements are down in there.
This is because we're looking at the revolution field,

This video is about understanding p-values and confidence intervals through resampling
from the data that we have for a research project. My name is Dr. Jean Klopper and I'm a data
scientist and research fellow at the School for Data Science and Computational Thinking
at Stellenbosch University. So imagine then we have subjects taken at random from a population,
that gives us a sample, and we're going to gather data for two variables from our sample.
The one is going to be a nominal categorical variable and it only has two sample space elements,
a and b. And that's going to allow us, you know, whether a subject is in group a or group b,
to create these two groups. And then our other variable is going to be a continuous numerical
variable. And what this is going to allow us is to compare this continuous numerical variable value
between the two groups. And one way to do it is to compare the means. So our test statistic is going to
be a difference in means. Now for our subjects we are going to find a difference in means and we
want to know how likely was it to have found this difference. And that's what we're going to do
through resampling. If we combine all our subjects we have a continuous numerical variable value for each
of these and we can express a mean for them as well. Now if we think about the population from which
our subjects were taken, they have that variable and a value for each member of the population as well.
And of course if we knew all of that we could calculate the mean in the population and we
would say that that is a parameter. And somehow our statistic has to be compared to what it might
possibly be, what this parameter might possibly be in the population. So we have to express uncertainty
in our statistic. And the way to do that is to calculate confidence intervals. And we're going
to do this through bootstrap resampling. Now of course to do all of this we can use a computer
language and in this video we're going to make use of the Wilfram language. We've opened our Wilfram
notebook. I've called it test statistic estimations where we're going to do some bootstrap resampling
to calculate confidence intervals. That is of course a measure of uncertainty in our results,
seeing that we only have a sample from the population and not the data from the whole population.
And then also calculating or simulating the probability of having found the test statistic
that we have found. So let's just simulate some data. And we're going to imagine that we have a
continuous numerical variable. And for that continuous numerical variable we have two groups.
So we have a categorical variable with a two element sample space and from those two unique elements
we form two groups. And then for each of them we have a single continuous numerical variable and
that is then collected for each group. So let's just simulate that we have a group in the number of the
samples in that group is 100 and in our group two there is 105, the 105 subjects. So we're going to
just assign those to the computer variable names n1 and n2. So let's just simulate some random values
and we're going to assign them to the computer variables var1 and var2. As you can see there both
will be taken from a normal distribution. The first with a mean of 100 and standard deviation of 10
and we want n1 number of those, so that's 100. And the second comes from a normal distribution with a
mean of 103, a standard deviation of 13, and we want 105 of those. And we also see the pseudorandom
number generator. If you said 12 you're going to get exactly the same results. And the next thing I want
to do is because under the null hypothesis we're going to reassign each one of these values to a random
group. We're just going to flatten those two after appending that. So we're going to take var1 and
we append to that var2 and we pass that as parameter to the flattened value. So what we're going to have
here is 205 values as a single list object. Now our summary statistics and data visualization remember
that comes first so that we can get some idea of what is going on. So we know where we took the random,
what distribution we took the random numbers from, random values from. But let's just see what the
means were because we only took 100 and 105 values. So we see 101.05 and 103.919. Those are the two
means. And that is the test statistic that we are going to use both for our estimation and our uncertainty
and for calculating the probability of having found the test statistic, which in our instance is going
to be the difference between these two means. Data visualization always very important. So let's
just do a box and whisker plot of these two and sort of start seeing whether we think there will be
a statistically significant difference between those two if we can use that term. And we see the box and
whisker plot there. So let's talk about the uncertainty in our results. Let's just concentrate on group one
one there. And for group one, we have a mean. Let's just recall what that mean is. And we see it's 101.5.
Now those subjects, they came from a population and we need to express our uncertainty in what we think
this statistic is in the population. So what's the parameter for this continuous numerical variable in
the population from which the sample group was taken. So we have to set confidence levels. And as is common
in many fields, we'll use a 95% confidence level. So how do we go about this? We don't know what the
variable values are for the whole population. But we want to, we want to put some bounds, a low and an upper
bound to express a 95% confidence level for an interval which we think the population parameter
will fall between. And we're going to make use of bootstrap resampling. So what is bootstrap resampling?
It says, take all the values that we have. So for group one, all, all those 100 random variables. So those
values for our continuous numerical variable. And we're going to select one subject from that. So we
throw them all in a hat. We draw one and we write down the number. We take that number and we throw it
back in the hat and mix it around, draw another one, capture that value, throw it back in the hat,
draw another one. So that's with replacement. It's resampling with replacement. That means there are going
to be values in there in that hat of those 100 values that we have for that sample, for that group
are necessarily going to be drawn more than once. And that's exactly what we want. If we didn't do
replacement, of course, if we just take one out and not throw it back, you know, resample will be
exactly the same as the original. And that's not what we want. And because we have computers, we can do
this many times over. So how do we do that? Well, we use the random choice function, as you can see there,
we pass var1, which contains all our values. And how many do we want? Well, we want length of var1.
So exactly the same number of subjects. Now, if we do that, if we have this list here and the length
of the list in random choice, the Wolfram language does do resampling with replacement. So no concerns
there. And then we're going to calculate the mean. So we're just going to do that resampling so that
we have exactly the same number as the original, same sample size as the original, we calculate the
mean, we pass all this to the table function, and we do it 1000 times over. And that means we're going
to have 1000 means. And they're all going to look slightly different because the resampling with
replacement means every time we do this, every time we're going to get a different mean.
mean. Because we are going to draw this random choice. Now, it's going to it's going to draw
exactly 100 samples for us, exactly the same length as the original. And we calculate the
mean from that next time we do this, you know, we're going to have different values, different
ones are going to be repeated. So let's do that 1000 times over. And now we now have these means,
plot histogram of these means. So that's a distribution of the bootstrap and means for
group one. So there's all the means for group one. Now we found a specific one, of course 101.5.
But because we've done this bootstrap resampling with replacement, in other words, with replacement,
we get all these other means as well. And what we want to do now is to look at putting them all
in order, as you can see here on the histogram on the x axis, put them all in order.
And then we're going to look for if we're dealing with a 95%, a 95% confidence level, we want to know
the value in these means, all these means that we have now that represents the 2.5% percentile and
the 97.5% percentile. Because that will give us this 95% in the middle. So percentile 2.5% percentile
97.5. So we put all these values in order, and we now have to do but we have 1000. So we have a
little equation in here that we have in in one here. So it's 2.5 divided by 100. So that means 0.25.
That's the 25th percentile times our sample size. And that gives us a value k. And that is going to be,
if we put the values in order, if we look at the kth value, that is going to be represent
the 2.5 percentile value. And we'll also do the same for 97.5. And if we do that,
we'll pretty much soon find out that what we are looking for is value number 25
and value number 975. So we use those indices if we put them all in order. So we're going to sort
all those means, 1000 means that we have, and we want the 25th one. That's going to represent
the 2.5 percentile value. So let's do that. And we get its 99.039. And we want the upper one as well.
And that's 102.795. So what we can do now is just plot this. And there you can see I've plotted it.
There's our dark black line in the middle. That was the actual mean for that group. And we see
the 95% confidence interval. So we can say that the mean was about 101.1 and the 95% confidence
interval is 99.2 to 102.7. Now that does not mean we are 95% confident that the sample,
the population from which the sample was taken has this parameter, this mean for this continuous
numerical variable between these two, it really means as if we were to repeat our study 100 times
over, which of course is impossible to do in real life, we don't have that kind of money or time or
resources. Every time we do that, we would get a different mean, slightly different mean if we have
new samples, if we start our study a week later, we'll have new subjects in our study.
And 95 of those, each one will now have slightly different confidence intervals. 95 of them out of
100 would have the population parameter for that variable within those confidence intervals. We
don't know if this is one of those 95. But certainly we can express now our uncertainty in our results.
It was just over 101, but we see this 99 to 102 or 103, thereabouts. We see the uncertainty in our
results expressed as confidence intervals. So let's now just calculate the probability of our
test statistic. So under a null hypothesis, we would say that there is no difference in means between
these two groups. And we see our null hypothesis up here. And our alternative hypothesis, there is a
difference between the two means. And we've done the mean for group one as this x sub one bar and the
mean for a group two x sub two with a little bar over. So under this null hypothesis, which says there's
no difference in the means, it means we can take all those 205 values and randomly reassign them to
group one, group two, group one, group one, group one, group two, group one, group two, group two.
You can just dish them out. At random, it doesn't matter what group they belong to, because we just
said under our null hypothesis that the two groups are the same. So we're going to do that by creating
a little function. And we have it here, a user defined function, random groups, it takes a variable,
and we're going to call it var underscore one. And we assign to that, the module, a module just gives
us these parameters that are local to our function that we can work with, and we'll call them g1, g2.
And what do we do? Well, we use the take drop function. And we do a random sample of var all remember,
that is where we put all of them together, and we flatten that list. And we want to draw in one from
them. So what the take drop is going to do, it's going to randomly select, mix them all up, var all,
and take random in in one, remember, in one is under. So take 100 random values from there.
And that will go to g one, and then they just rest will go to go to g two. And then a semicolon,
because thereafter, what do we want this function to return? Well, we want the mean of g one minus the
mean of g two. So that's our function. And now we're just going to use this function 1000 times
over. So I'm going to say means 1000, that's going to be my computer variable name, I'm going to create
a table of 1000 values, as you can see there. And what do I want, I'm calling my function with my
flattened combined list. So what is going to do 1000 times over, it's going to randomly reassign
subjects to group one and group two, calculate this new the new difference of means because the
difference of means is my test statistic. So let's run that. And let's create a histogram of
what we would call a sampling distribution of the test statistic.
So in most cases, we'll see there'll be no difference between the two. And as it goes out,
it becomes less likely to have such a value. Now, let's capture our test statistic in a computer
variable actual mean diff. And we just do the difference between those two means. And we see
it was minus 2.86952. We do remember though, that our null hypothesis is that they equal and our
alternative hypothesis is that they're different. It doesn't say one is more than the other. It's not a
one tailed hypothesis. It's a two tailed alternate hypothesis. So we could also have said mean for
var two minus mean for var one, and we would get positive 2.72. So we would just, you know,
have to remember, have to remember that, that fact. So if you look at minus two point,
I should just correct that. If we look at minus 2.86, it's way out here,
and positive will be way out there. And all we want to know now is if we put all these difference
in means that we've just calculated, those mean one mean, this one means 1000, if we put them all in
order, we want to know what fraction was less than this. And we add to that the fraction that was more
than the positive of that positive 2.86. And that's exactly what we do here. Fraction lower and fraction
high is what we're going to calculate. We do a numerical computation there, we want a numerical
value, we say the length of the following. Select, use the select function, means 1000,
and then this placeholder, remember the placeholder, simply just the hashtag symbol there, pound symbol,
the ones that are less than the actual mean diff. Don't forget your ampersand there because
we want element wise. So it's just going to select all the ones that of all my simulated differences
here. We just want, under the null hypothesis, we just want the fraction of those because we divide
by 1000 there. We want the fraction of them that are less than this difference. And then the fraction
that are more than positive that difference. And that is going to give us 0.028 and 0.023. And if we
combine them together, that gives us the probability of having found this difference that we have.
Under the null hypothesis, we are now simulating what fraction would be this difference that we found,
and more extreme on either side. So let's just do a little histogram of that. And there we have it,
we have these values here. That is our test statistic on either side. And, you know, we now have a
simulated p value there from our resampling. Let's just use the students t test and just get a p value
for these two and we see 0.051. And we calculated 0.051 very, very close. These two, these two values.
So that's it. It's very simple to do bootstrap resampling, just so that you can express the
uncertainty in your test statistic. And in the case, we only looked at the mean for group one,
but you can also look at standard deviation, variance, whatever test statistic you're interested
in. And then for our actual research question here, we had a null hypothesis, alternative hypothesis,
and we do a sampling distribution of possible means under the null hypothesis, where we just
reassign at random our values to the two groups, because under the null hypothesis, we say that the
means are equal in both groups. So I hope this gives you a good understanding of how we can simulate
this uncertainty in our results and how we can use the sampling distribution of our test statistic
to determine how likely it was to find this test statistic we did, and more extreme on either side,
under the null hypothesis.
Thank you.
You

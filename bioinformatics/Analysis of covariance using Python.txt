Welcome to a very windy but very warm late summer's Saturday afternoon here in Stellenbosch.
Now this is the third video lecture on these four fundamental, what I call fundamental, linear model types
and we're going to talk about the analysis of covariance.
If you haven't watched the first two videos, please do so because we just build on top of what came before.
Linear regression, then analysis of variance and now we're going to combine those two in analysis of covariance.
So look out for some more information on our website or look in the description below.
If you're watching on a computer, there should be a card right up here that you can click.
Please watch those first.
So here we are in Jupyter Notebook.
Once again, I'm going to make use of Visual Studio Code and you can see I create environments,
special separate environments for each of my projects.
And so this would be a data science environment set up by Miniconda.
And for your interest, I'm running Python 3.9.10.
So let's learn.
We're going to use code because I actually heard a very nice quote by a very famous person in the NumPy development community
to say that we see the knowledge inside of our world these days through the lens of code.
We extract knowledge.
We learn to understand things through the lens of code.
And I thought that was a very powerful statement.
So we use Python in this instance just to make life very easy, first of all,
to learn how to do these things, what they mean, to understand what's going on when we talk about these kind of models.
And secondly, just to show you how easy it is in the end, just to run a few lines of code and do your own analysis.
So as always, we're going to start with by importing our packages that we're going to use in this notebook.
Once again, you notice that I do not make use of abbreviations.
So I just import these packages and their full names.
And if I have to reference one of the functions inside of those packages, I have to type that full name.
So we're going to import pandas.
That's for our data import and data wrangling.
We're going to import the stats module from SciPy.
We're going to import numerical Python.
And we're going to import the Patsy package as well.
Let's run this code cell.
Let's move on.
We're going to do a bit of plotting.
As always, I enjoy the interactive plotting of Plotly.
So I'm going to import from Plotly.
I'm going to import the Express module, the Graph underscore Objects module, the Subplots module.
So you'll see that in action.
And then also I'm going to import export, so IO, so that I can use IO.Templates.Default
and set my default plotting style to Plotly underscore Dark.
And there's a string there so that I can plot nice dark plots on this dark theme IDE.
So from statsmodels.formula.api, we're going to import the OLS, Ordinary Least Squares function.
And then from statsmodels.stats.anova, we're going to import the ANOVA underscore LM function
because that's just going to give us nice tabular results.
So here we are.
We're back with these four fundamental types.
Now, you won't find them really as fundamental.
I'm just using that term because I think these are the four fundamental ones to understand.
And then it'll be so much easier for you to understand all the other models.
So in italics here, we see we're busy with the third one, one-way analysis of covariance.
Once again, remember, this is modeling.
We're going to understand this relationship between our independent variable, in this instance,
variables, and a dependent variable.
And you see the data types there.
So we're going to have an interval type or continuous numerical type and a nominal.
So a categorical and a numerical variable.
So we're going to have two independent variables here of these two very different types.
Our dependent variable is still going to be interval type.
So a continuous numerical.
So what we're trying to do is we investigate the effect of the nominal categorical, that's
that independent variable, on a continuous numerical variable.
So we've heard that before.
That's just analysis of variance.
But this time around, we are going to correct for a covariate.
And that's the term that is used, the covariate.
And that's where nCoVa comes from, analysis of covariance.
We have a covariate.
And that's a numerical independent variable.
Now, usually we should try and control for these independent variables in our experimental design.
But sometimes it's impossible to correct for, control for at least, in the experimental design
for all our independent variables.
And if we fail to do that, we can control for them to some extent statistically.
So that's what we're going to do here.
We're going to control for this covariate.
And what happens then is it helps us discover the true relationship between, so we're just
going to call it the treatment, because we're using healthcare examples here.
But by treatment, remember, we just mean the independent variable and all the unique sample
space elements of that categorical variable.
We just call levels.
There's so many terms and synonyms for the same thing.
Don't get confused, you know, between all of them.
But we're going to try and understand this relationship between that treatment, the categorical independent
variable, and the continuous numerical dependent variable by controlling for the covariate.
And that brings out a true relationship between those.
So what that does mathematically, and again, if you've watched the previous two videos in
the seminar series, you'll understand what's going on here.
It's very important.
Mathematically, we're going to decrease the sum of squared errors.
And remember, that isn't the denominator of our F-ratio.
You're going to get a higher F-value.
And, you know, that's going to improve things for us.
So let's have a look at our simulated project.
So this simulated research project, that's on blood loss during trauma surgery for major
vascular injury during penetrating trauma.
And our dependent variable is blood loss in milliliters.
So that's a continuous numerical variable.
That's our dependent variable.
So that fits the bill of our little table of data types.
Our independent variables is really that of the categorical variable.
That is one of three levels.
And during the experimental design, we could control for that.
So some of the participants would have received a placebo drug.
Some would have received a low dose of a drug investigated for decreasing bleeding.
And then a higher dose of a drug, same drug investigated for decrease in bleeding.
So the search is always on to help decrease active bleeding during these big surgeries for
penetrating abdominal trauma, major vascular injury.
Now, during the experimental design, we suggest here that the researchers could not control for
the age, so there was no sign for age as participants were entered into the study.
So we're going to control for it statistically.
So that's our covariate, that's age.
And that is a continuous numerical variable.
So you can see how, remember with linear regression, our independent variable was continuous numerical.
And with analysis of variance, our independent variable was nominal categorical.
And now we bring these two together.
The nominal categorical variable is what was controlled for during the experiment.
Could not control for the continuous numerical variable.
We call that the covariate.
And we're going to control for that covariate.
Instead of using Python to generate some data for us, we've already got data.
It's in this CSV, comma-separated-values file.
That's a very stripped-down spreadsheet file.
If you haven't seen those before, always the best format for your data.
So it's in the EBL underscore data CSV file.
And we're just going to import that with a read underscore CSV function in the pandas package.
So pandas.read underscore CSV, that function.
We pass as a string this CSV file.
Now, this CSV file, very importantly, lives in the same folder or directory on my computer as this Jupyter Notebook file.
So I don't have to put in a long address on one of my hard drives, you know, to this file.
It's just the file, this CSV file, and this Jupyter Notebook lives in the same,
they exist in the same folder or directory on my computer.
So I can just reference this file directly.
And I'm assigning that, remember, equals sign is assignment.
And I'm assigning that to whatever's on the left-hand side.
And that is a computer variable named df.
So we're going to assign this, what it's going to create for me as a pandas data frame object.
And we're going to assign this to the variable df.
So let's do that.
It's done.
We can use a bit of indexing.
So square brackets, colon 5, that is give me up to row 5.
Remember, Python is 0 index.
So that's going to give me the 0 width, the first, the second, the third, and the fourth rows of data there.
And there we see 0, 1, 2, 3, 4, as far as the index is concerned.
The first five participants here were all in the placebo group.
There's the ages that we couldn't control for, and there's the blood loss during the surgery.
Now that's measured by suctioning the blood out of the abdomen or absorbing the blood in abdominal swabs.
And those are weighed to give us a fairly accurate amount of, or measuring at least, the blood loss.
Now this group variable is categorical.
So we're going to use the pandas.categorical function just to make sure that we tell pandas that this is not a, we don't want it to be seen as a string.
We want it to be seen as a categorical variable.
So we're going to call the first argument there, df.group.
So we know what series we're talking about, this column here, group.
We want it to be ordered, and we want the categories to be placebo, low, and high.
Now that's very important, that order, because we are going to use dummy variables, of course, and we have to have the base case, which in our case is going to be placebo.
And next time around, when we get to logistic regression, I want to say even more important to choose that base case, but as important here.
So the base case is going to be placebo, and we're measuring the low and the high.
So let's, once we've run this at least, let's call the info method there, df.info.
And we see now that we've got 45 participants.
We have these three columns, group, age, and blood loss.
We see group is now a categorical variable, and we have integers, whole numbers for age, and blood loss.
So let's look at our research question.
We're asking, is treatment level a predictor of estimated blood loss after correcting for age?
So that's very important.
Now, with our independent variable, we have to do a dummy variable.
We can't do arithmetic with names.
And so we have our treatment variable there.
That was called group, remember, in our data set.
So it's called group.
So we're just calling a treatment there.
So participants could either be placebo, low dose, or high dose.
Now we create three new variables, dummy variables.
Placebo, low dose, high dose.
Remember, that will be the list of all the unique sample space elements in this nominal categorical variable.
And if it's placebo, we'll get a little one under placebo and zero for the others.
Notice, and you've seen this before, dummy variables are quite easy to understand.
There's a redundancy.
And remember, we are setting this placebo as our base case so that it falls away.
So our two dummy variables are only low and high.
So participants getting the low dose of this experimental drug and those getting a higher dose.
And if it's 0, 0, there's no other choice but for that first participant to be in the placebo group.
And if it's 1, 0, then the low dose group.
And 0, 1, then the high dose group.
So we certainly don't need that placebo there.
It's redundant.
So let's state our research question now as an equation.
So what we're going to have is estimated blood loss is going to be an estimated intercept.
All the hats on top.
Remember, this is not the true values in the population.
We're estimating those true values, at least, from our sample.
So that's the intercept plus beta sub 1 times the age.
Age, also an estimate.
These are all estimates.
Plus beta sub 2 times low dose plus beta sub 3 times high dose.
Now just think about it for a moment.
This low dose can only take a value of 0 or 1.
High dose can only take a value of 0 or 1.
So it's either going to be beta sub 0 plus beta sub 1 times the age.
It's going to give me an estimated blood loss.
Or beta sub 0, that would be 0, 0 if it's a placebo participant.
So beta sub 0, beta sub 1 plus age plus beta sub 2 times 1.
That high dose will be a 0.
So I'm just adding 1 if it's low dose.
Or I'm just adding 1 if it's high dose.
So think about that very carefully, depending on what these values for beta sub 2 and beta
sub 3 are.
So it would be 1 times beta sub 1 or 0 times beta sub...
Let's say that again.
0 times beta sub 2 or 1 times beta sub 2.
And 0 times beta sub 3 or 1 times beta sub 3.
So they can only take values of 0, 0, 1, 0 and 0, 1.
Now see how that would influence this value for the blood loss.
So these are fairly constant values.
The only value that will change a lot is this continuous numerical variable.
And you can see where it comes from, the understanding of it comes from, that we're correcting for the age.
This covariate of ours.
So let's state our hypothesis.
And I'm going to state that in terms of this mathematical statistical approach, which we've been taking so that you can clearly see how linear regression, analysis of variance, analysis of covariance fit together.
So, in terms of the coefficients, the null hypothesis should state that the treatment level is not a predictor of blood loss after correcting for age.
So beta sub 1 equals beta sub 2 equals beta sub 3 equals 0.
Alternative hypothesis, at least one of them is not 0.
So let's be a little bit more verbose.
Because as you can see, this is all we are after mathematically.
To be verbose about it, we state that the null hypothesis would state that there is no difference in the mean of the dependent variable between the different treatment levels after correcting for the covariate.
Or in our example, we would say there is no difference in mean blood loss between the three treatment groups after correcting for age.
So definitely this last statement is what you would see in a research project.
But we know now, after understanding linear regression and analysis of variance, it's these coefficients that we're after.
And we can see from stating our research question as an equation why we say these things.
You know, if beta sub 1 is 0, beta sub 2 is 0, and beta sub 3 is 0, that means that we're always just going to have the intercept as a predictor of the blood loss.
So all the blood losses are going to be equal to each other, no matter what group the participant fell in, because beta sub 1 and beta sub 2 and beta sub 3 is 0.
And that's our null hypothesis, that none of those things matter.
And if one of them matters, there'll be a substantial difference from 0.
So I hope you can see how clear this can be to understand and how easy it can be to understand.
So as always, we're going to start with some exploratory analysis.
So let's call the values underscore counts method on this df.group.
That is going to be a pandas series object.
As soon as you call one column, what gets returned is a pandas series as opposed to pandas data frame.
The difference being there's only one column.
It still has an index.
Every row is an index.
But on that column or on that series, we call the value underscore counts method.
And that's going to show us the count, the frequency of each of the values that that nominal categorical variable could take.
So placebo, there were 15 participants, 15 on a low dose and 15 with a high dose.
Let's look at the blood loss per treatment group.
So we're going to do some summary statistics on just the blood loss.
It's a continuous numerical variable.
And so we're going to say df.groupby.
So the groupby method, we want to groupby the group column.
So we're going to separate out the participants in the placebo low and high dose group.
And from once we've done that separation or the groupby, we're going to look for the bl column and then call the describe function on that.
This is sort of a little bit of a thing to get used to with pandas, how you string these things together.
And there you see, we've done the groupby the group column.
So there we see placebo low, high.
And then on the blood loss column, we're going to use the describe method.
And that gives us the count, the mean, the standard deviation and our minimum and maximum and our three quartile values.
So you can see the mean blood loss.
So we want to know if there's a difference between these three means after correcting for age.
So let's visualize this data.
We're going to use a box and whisker plot.
So I'm calling the express module dot box.
First argument's my data frame.
On my y-axis, I want blood loss.
I'm going to color it by the group.
That means I'm going to get a box and whisker little marker for the placebo group, the low dose group and the high dose group.
I'm giving my plot a little title.
And I'm replacing the labels that will appear generically inside of my plot.
Based on the column headers in my data frame, I'm just replacing them.
So if it finds BL, I'm replacing it with blood loss and then the units.
And I do that as a Python dictionary object, as you can see there.
So let's create that.
Beautiful plot.
Love these.
You know, as you hover over them, you obviously get all this information.
Remember, you can also click on these legends on the right hand side so that you can take some of them away.
And, you know, bring them back in whatever order you want.
And for a variety of plots and plotly, that might be very useful.
But as we hover over them, we can see the median.
We can see quartile 1 and quartile 3 there.
And since we don't have any suspected outliers here, our whiskers will go out to the min and max values.
And you can clearly see here for the placebo group and the low-dose group, there doesn't seem to be that big of a difference.
But when we look at the high-dose group, they seem to have had substantially less blood loss.
And we have to find out whether, you know, the level of treatment was in some way predictive of the estimated blood loss after controlling for age.
So let's look at that age.
Let's do the describe method for age after grouping by the group column.
And we see 34, 33, 34.
So the ages were quite close.
So if we do a box and whisker plot just there of the ages, you see that there wasn't visually at least a big difference in the ages between our three groups.
Lastly, let's just visualize this correlation between our covariate and our independent variable, so age and blood loss.
But we're going to group by those three groups.
So I'm using the color argument there for my groups.
And have a look at this, because this is a very, very important plot.
So we have age and years on the x-axis, and we have the blood loss on the y-axis.
And we can see there is really this linear relationship.
And that linearity is sort of parallel for each of the three groups.
And that's a very important thing.
Because what we're going to do this time around, we didn't do it for linear regression and analysis of variance.
But here it becomes very important.
We have to look at the assumptions for the use of analysis of covariance.
And I'm really going to highlight the ones that I want you to always consider.
The first one is linearity.
And that refers to a linear relationship between the covariate and the dependent variable.
So we can do that visually.
Let's do that.
And we've already done that by these three groups.
But if we do for the whole data set, there seems to be a linear relationship.
At least as age increases, there's also an increase in the blood loss.
So that assumption we really want met, we want that linear relationship.
We want to correct for the age.
So we want this relationship between age and blood loss.
And what we could actually do, just, you know, let's just remind ourselves of how easy it is to create a linear regression model.
I'm using the OLS function.
I'm using my little formula there, blood loss given the age.
My data is in DF.
I'm calling the fit method on that so I can fit the data to my model.
And I'm just calling the summary on that.
And there we go.
We see our age there.
We see a p-value for that age is very small.
And so at least we reject the null hypothesis here.
And we say that that coefficient for age is significantly different from zero.
And you can see the coefficient there.
It's 145.
So quite high.
We see an R-squared value there of 0.552.
So age explains, or our model here, which is just age, explains 55.2% of the variance in the estimated blood loss.
So I think we're fairly satisfied that there is a good linear relationship between our covariate and our independent variable.
The next one is homogeneity of the regression slopes.
So let's look back here.
Homogeneity of these regression slopes, remember this model line of ours?
That's a regression slope.
And homogeneity means the slopes are equal.
And look at these lines.
They are parallel.
That's what we want.
What you don't want is scenarios where these lines just crisscross, you know, forming a big old cross over each other, like a multiplication sign, or a divergence or convergence, like they start off far apart and then meet on this side, or all start from the same point and then move out.
That means that there's a difference between this relationship between the covariate and the dependent variable, dependent upon which group they are in.
And that is what we don't want.
We understand what happens if we see graphs like that.
It's very easy to interpret, as I've just explained.
But we don't want to see that.
We want to see homogeneity of the regression slopes.
And there's one good way to look for that.
And we're already going to build a model here.
And we're going to see, doing these assumptions, we're already going to do ANCOVA, encode, before we actually get to ANCOVA.
But it's the way that it works.
I want you really to think about these assumptions.
And I'm going to call it the interaction underscore model.
I'm using the OLS function.
And I'm saying blood loss given age multiplication.
So that's the star symbol.
That's multiply.
Age multiplied by group.
And that's going to give us an age as a predictor.
It's going to give us the groups as a predictor.
And it's going to give us the interaction between aging groups.
And I'm calling the ANOVA model.
I'm using type 3 there, as you can see.
We don't have to go into that at the moment.
But there you see age colon groups.
So that's the interaction between those two.
That's an interaction term.
If we go to the p-value for that, it's 0.9.
And our null hypothesis, which we won't reject in this instance, states that there is homogeneity of variance.
I should say homogeneity of the regression slopes.
Don't listen to that little slip of the tongue.
Homogeneity of regression slopes.
And we can clearly see this.
They are homogenous.
They are parallel to each other.
And that is an assumption for the use of analysis of covariance.
And what you do, as I say, you look for this interaction term.
See, there's group.
There's age.
But then I have this interaction term.
And the way to do that is not to say age plus group, which is what we're going to do for ANCOVA.
We say age multiplied by group.
And so what the code would do, it will take group and age and the interaction term.
But clearly we can see a p-value that is larger than a chosen alpha value of 0.05.
We fail to reject the null hypothesis.
And we have this homogeneity of the regression slopes.
Next one is we're going to look at the residuals.
Now, this is where I mean we actually have to build ANCOVA model so that we can calculate these residuals.
Remember the residual or the error.
That's the difference between the actual dependent variable value and the estimated one due to our model.
So we're actually doing ANCOVA right here before we do ANCOVA.
So just say bloodloss given age plus the group.
Data is the df data frame dot fit method.
And I'm assigning this to a computer variable called model.
So if we run that, we have our model.
I'm now going to add a new column to my data frame.
So I've got to put it in square brackets.
I'm going to call it residuals.
And I'm using the resid attribute of my model.
So model dot resid, that gives me all the residuals and that's going to go into a new column in my data frame.
Which means I can look at a little histogram of all these residuals.
And, you know, it's a, you know, we sort of have to agree that there is some form of a dull shaped curve here.
So it looks like these residuals at least, you know, are sort of normally distributed.
We can be, you know, it's a bit difficult because it depends how wide we make these bins.
But we can be a bit more precise and use something like the Shapiro-Wilk test.
A null hypothesis would state that these data points are from a population in which that variable would be normally distributed.
So I'm calling stats dot Shapiro, passing this PANAS series object, df dot residuals.
That's just the value in the residuals column.
So passing that to the Shapiro-Wilk test, we see a statistic there and a p-value of 0.89.
So we don't reject that null hypothesis.
And we say that we've passed this assumption for normality of the residuals.
But it's important that we look at the residuals there.
We can also look at homogeneity of variance.
And I want those three groups of residuals to have sort of equal variance.
Levine's test we're going to use for that.
The thing about the stats dot Levine, it wants these three Python lists that I'm going to pass.
So I'm going to call it result placebo, result low, and result high.
And I'm just using a bit of notation here.
df dot lock for location, square brackets.
So it's column comma rows.
I want the column to go down this df, rows comma columns.
I should say df dot group equals equals placebo.
So it's going to go down all the rows of that group column and only select those, double equals sign.
So only the ones that return true are going to be included.
Comma, I want the residuals value for that.
And again, at the right at the end, I'm using the to underscore list method.
So it's just going to export this as a Python list object.
So I've got three lists.
And those will be the residuals of each of those groups, which I then pass as three arguments to the Levine function.
And there we go.
We see a p-value of 0.40.
Then our hypothesis of the Levine function is that these variances are equal.
And then lastly, we care about outliers.
And sort of as a rule of thumb, we don't want too many values as far as these residuals are concerned
to be more than three standard, the standardized residuals at least,
to be more than three standard deviations away from the mean.
And you can see this long function that I have to do.
So what I'm doing here is I'm calling the resid underscore studentized underscore internal values
so the model dot get underscore influence, open, close parentheses, dot resid studentized internal.
And I want to know how many of them, once they are, I have the absolute value of them,
how many of them are more than three?
Because remember, some will be more than minus three standard deviations away
and some more than plus three standard deviations.
And I'm just, you know, that's going to return either true or false, which is one or zero.
And if I sum over all of those, I'll get how many of them are more than three away.
And I see none of them.
And we could actually see that with the box and whisker plots.
There were no suspected outliers.
And when it came to the actual data, you can plot these residuals as a box and whisker plot as well.
So very importantly, these are the assumptions that I really want you to consider
any time that you are considering using analysis of covariance.
Check for these assumptions.
So we're eventually going to come to building our model.
Now we've already seen that we've used a code to build a model.
We've assigned it to the computer variable model.
But let's just start off with just building a normal analysis of variance.
I'm not normal.
Let's just build an analysis of variance model because I want to show you the differences
between correcting for that age and not correcting for that age,
or at least not bringing age into it at all.
So let's use the OLS function.
So blood loss given the group, just normal analysis of variance, one-way analysis of variance.
And I'm going to say data equals df.
I fit the data to my model.
And I'm calling the ANOVA underscore lm function,
passing this model that I've called ANOVA, assigned to the variable ANOVA.
Let's have a look at it.
And there we go.
We see our sum of squares due to the regression and our sum of squares due to the error.
So let's just save these as a computer variable.
So we can just compare that to our model, which, remember, is our ANCOVA model.
So first of all, it's the sum of squares due to the regression.
Remember, that is the difference between the predicted value and the mean of the dependent variable.
So in each one of the groups, we just take the mean.
That's our base model.
And there is an attribute called ESS.
So again, as I say, many synonyms for all of these things.
Here in stats models, the sum of squares due to the regression is called ESS.
I'm going to assign that to the computer variable SSR ANOVA so that I remember that this is the sum of squares due to the regression of the ANOVA model.
And you see there, let's have a look.
One, two, three.
One, two, three.
Four point, about 4.5 million there.
So let's look at the sum of squares due to the error, the difference between the predicted value or the estimated value in the model versus the actual value.
Unfortunately, the attribute is called SSR, but we know we won't use the term SSE in the seminar, sum of squares due to the error.
So I'm going to save that as SSE ANOVA.
And you can see how big that error is.
So my sum of squares total for my ANOVA, it's the sum of the sum of squares due to the regression and the sum of squares due to the error.
And we can see it there such that we now can express the coefficient of determination, you know, what fraction or percentage of the variance in the dependent variable does our model explain.
And that is the ratio between the sum of squares due to the regression and the total sum of squares.
And we see there about 40%.
There's also the R squared attribute that we can use.
So we can see exactly the same value we know now where it comes from, how to calculate it.
Let's go back to the ANCOVA model, though.
Remember, we saved it as the variable model.
So I'm calling the ANOVA underscore LM function from stats models, passing the model as an argument.
So let's have a look at that.
That looks a bit different from the ANOVA model.
First of all, there's age there.
We are correcting for the age.
But we see that the sum of squares due to the error here by the residual, remember the error, that's a lot smaller.
We've taken away some of it by correcting for the age.
So let's save the sum of squares due to the regression of our model.
I should say the error.
That's the SSR, unfortunately named attribute there.
That's the sum of squares due to the error.
I'm going to save sum of squares due to the error of my ANCOVA model.
Remember, we've already done it for the ANOVA model.
So let me save that.
And you can see that's a lot smaller.
It went from nine and a half million almost down to 716,000.
So I'm actually printing that to the screen so that you can see those two values.
9.7 million versus 716,000.
I've taken away a lot from the sum of squares due to the error.
So my predicted value, my estimated values are much closer to the actual values as far as my dependent variable is concerned.
Let's look at the sum of squares total.
So I'm taking the sum of squares total.
That's the sum of squares due to the regression and the sum of squares due to the error add them to each other.
This is for the model.
So this will be sum of squares due to the sum of squares total due to ANCOVA.
Let's save that.
And you'll see that it has not changed between ANOVA and ANCOVA.
The sum of squares due to the regression is absolutely the same between the two.
We have two error terms here.
We now have one for group and we have one for age.
Adding all of this, it has to stay the same.
I've made the error sum of squares much, much smaller.
So, you know, it has to go to the numerator there.
But if we look at it, the sum of squares total remains absolutely the same between the ANOVA and ANCOVA model.
Nothing happens except me making my sum of squares due to the error much smaller.
Look at what happens to my R squared value now, where we had 40% before, we jumped to 95.6%.
So we've got this bit, well, to some extent, our goodness of fit test here shows us a much better model.
We see a truer reflection, see how all these words come back, a true relationship between the dependent variable and the independent variable.
The independent variable being the treatment or the group as a predictor of the dependent variable.
Let's have a look at the coefficients.
There's a params attribute that's going to give me those three coefficients.
And now, remember, we can fill that in as far as our equation, our equation for our research questions constant.
Our research equation, as I like to call it.
So the estimated blood loss is going to be minus 53.63, that's beta sub zero, my intercept, plus 147 times the age, minus something for the participants in the low group, or minus something much bigger for someone in the high group.
And you can see why that's going to drag down someone who has a zero for low and a one for high.
That's going to drag the estimated blood loss down quite a bit.
And you can also see here what we mean by we are correcting for age.
I mean, we can have low zero and high zero, so they're both zero.
That is just going to be basically a linear regression model because we just have the beta sub zero and beta sub one there, just as we saw before.
But if it's one zero or zero one, we're going to subtract one of these two terms.
So we are correcting for age, extracting a better understanding, a better relationship between my independent variable and my dependent variable.
Remember how these intercepts work.
We can calculate a standard error for each of them.
And there's different equations for all of these different model types, but I can just use the BSE attribute.
So model.bse, remember model is my ANCOVA model.
And there we have 2 to 4.413, 48 and 48 and 6, meaning I can calculate a p-value.
That's just, remember, the coefficient divided by my standard error.
That gives me the 3t statistics.
And then given a parameter, which is the degrees of freedom, we can work out what these p-values are.
And we can see there, as far as beta sub one, beta sub two, and beta sub three, they are substantially different from zero.
And that is really what we're after.
We also see, of course, our confidence intervals around our coefficients.
And we saw in the first two lectures how to calculate those.
So let's have a look at this effect that this correction for age has on our fitted values.
So model.fittedValues.
I'm going to use my model.
It's going to pass all the values to it as far as the independent variables are concerned.
And return for me the estimated blood loss.
I'm going to assign that to a new column in my data frame called estimated.
And now I'm going to plot that.
So I'm going to use subplot.
So you can have a look at that code for Plotly.
But now you can see this is the actual values.
And this is the estimated blood loss.
So we've corrected for the age.
And you can see a little bit of a difference when we hover over these for these values.
You can do that.
But if you just look, I've plotted as well.
See there, box points equals all.
So I can see all the actual values.
And you can see that they are different now that we've corrected for age in our model here.
So let's have a look at the linear algebra of all of this.
I want to show you that it is nothing different from what we've had before.
With linear regression, analysis of variance, now analysis of covariance.
I'm going to create my vector of dependent variables in my design matrix using the D matrices function from Patsy.
I pass that same formula, blood loss given age plus group.
Data comes from the DF data frame.
And so let's have a look at X, just the design matrix.
Remember that first column of constants, which is what we multiply beta sub 0 by the 0, 0.
That would indicate as far as dummy variables is concerned that these first lot of participants are all in the placebo group.
And then we have age.
I'm going to convert these to NumPy arrays so that I can just work with them as matrices and vectors.
So remember, how do we get this vector of estimated beta values?
Well, it's this equation that we saw before.
Remember, if you understand a little bit of linear algebra, you know that we cannot possibly span all of the vector space in our matrix X.
So we have to do this projection onto the subspace that is spanned, that column space, and we'll get the best possible values for our coefficients that way.
This is how we express it in code.
Now, there's obviously much easier ways to do it.
I want you to investigate that code so you can understand how I just build parts of these all together so that I eventually get my vector of beta values.
And there they are.
We see the same coefficients as we had seen before.
Let's have a look at the params attribute.
It's exactly the same as what was calculated in using stats models.
So there you go.
I hope you have a very clear understanding now how we've built from linear regression to analysis of variance now to analysis of covariance.
The terms haven't changed.
Almost the mathematics behind it hasn't changed.
We interpret it in a different way, but you can see where these words come from.
We want to understand the relationship between our dependent variable and our independent variable after correcting for some covariate.
And you see, you saw at least exactly how that worked and how that helped us actually to decrease our sum of squared errors.
So next one's going to be a bit different.
We're going to look at that logistic regression.
So on the Saturday afternoon, as the sun is setting, you know, it's getting towards wintertime here in the south and it gets darker quite a bit sooner.
And yeah, we're going into winter and we wish we were in the northern hemisphere so we can go into summer.
I'll see you for the last of the series, the seminar series on what I term these four very important types of linear models.
I'll see you for the last of the series.

hi there in this second video on the fundamental subspaces of a matrix we're going to look at the
column space remember the first video was on the null space and please go and watch that video i'll
link it in the description down below and there'll be a card up here if you're watching on a computer
you'll see that card so have a look at that video first we're going to use sort of the similar code
and i will review a little bit about the null space but best you view that video first now we're
going to use jupyter notebook again and that's inside our visual studio code as you can see here
and we're going to use the simpy library first of all at the top you'll see mathematics python 3.9.2
so i do create special environments python environments using conda for all of my work
and so that's just for you to see in the top right hand corner there if you want to set up your system
similar way that would be python 3.9.2 so let's set up those packages i'm going to use
simpy once again and i import simpy with any without any abbreviations there so just import simpy
and that means if i want to use one of the functions inside of simpy i have to type out
the whole word simpy dot so just for your interest as well you can see i'm using
um the dunder attribute version there so 1.10.1 as far as simpy is concerned and then we want this
latex to be printed to the screen nice mathematical notation so i'm going to call the init underscore
printing function inside of numpy so let's just think about to start off by thinking about a vector
in rn so we're talking about mostly here about the geometric interpretation of
a vector but we can also just think of it as a array of of numbers whichever way you want to look
at it rn can be a representation of one of or each of least of the columns column vectors we would call
them in a matrix so if we look at our matrix in equation one here i have a column vector one two
five a column vector three three one and a column vector one two two so if you think about it each one
of those geometrically at least will exist in three space and that really has to do with the way
the way that we set this matrix up matrix up in as much as the only three rows so if we talk about an
m by n matrix m by n matrix as you can see there we know that the result which is b if you think about
a x equals b as we see there we'll term x the solution always and b we turn the result so we have
a result that is an element of of rm purely because there are three there are three elements in each of
these column vectors the three rows in my matrix so critically i we really need to think of
multiplying a by some vector x and because we have a square matrix here x will also be an element of r3
but it is this linear combination of these column vectors so it's x sub one times the first column
vector x sub two times the second column vector x sub three times the third column vector and that's
going to give us the result for some solution x and x in our instance as i said because it is a square
matrix there are three columns it'll also be an element of r3 now what we have to think about here
before we get there is is to think this vector b the result vector what values can it take
if we think about all of three space because our example here is is really just three space what
what areas in that geometrical space can this result vector b take that is what we want to determine
so a little bit of revisit of the null space we're going to create a matrix here and as you can see
i'm again using the simpy dot matrix function i'm passing a python list and then i'm using the reshape
so dot reshape method and i want that to be reshaped as a three by three matrix so that's one way to go
about it and you can see we have assigned to the computer variable a we've assigned this matrix that
we had in equation one so you can see the column vectors there one two five three three one one two two
now this is a matrix in which these three column vectors are linearly independent not one of them is a
linear combination of the other so the addition of constant multiples of one of the other ones and
we can look at that as far as the rank is concerned to confirm that so we can use in simply here the
rank method so a dot rank we see a rank of three that means there are three linearly independent columns
or in a mathematical textbook that might be there are three pivots
now we can set up also the homogeneous system so we'll have a result as a zero vector and see if
the solution to x is a zero vector if that is so that means that's the only solution that will give us
a zero vector as a result and we know that there's linear independence we also remember the fact that
we have that the rank plus the nullity must equal the number of columns and we have a rank of three we have
three column vectors therefore we're never going to have a basis vector or more than one basis vector
we're not going to have any basis vector when it comes to the null space and if we call the null space
method there on our matrix a we see we get back this empty python empty list and that means we don't have
a vector that spans any space so the the null space does not exist although remember we will always have
the zero vector there so that the zero vector does not span any space so let's then dive into this column
space remember we said that it is the result b that we asked what values can b take so we're thinking of
three space can it be any vector in three space or is it constrained to some plane or even some line
and so what we're going to call here just on a is the column space so very fortunately we have this idea
here at least a method called the column space in simpi and you can see it's just going to return
these three column vectors for me as is they're linearly independent in other words they are a basis for the
whole of our three there are three of them which is what i would require as a set of basis vectors to
span the whole space and that they are one version of basis vectors you can of course this is three space
so it can even have the three standard basis vectors that will also span r3 and that's something i think
that we just have to talk about a little bit and that is this idea of if we do elementary row operations
on our matrix is that going to affect the column space so the end result of elementary row operations
gauss-jordan elimination would be the reduced row echelon form so that's as kind of as many elementary
row operations as you could perform so let's do that we've seen the rref rref row reduced echelon form
method so i'm going to call that on a and that's going to return two things for me the reduced row echelon
form of of our matrix and it's also going to return well that's a tuple that it returns the first element
in that tuple or the zeroth element would be the reduced row echelon form matrix but the second
element in that tuple is going to be a tuple itself and that's a tuple of the indices the column indices
that contain the pivots and that's not what i want so i'm going to use indexing and i just want the
zeroth element in that tuple that gets returned so that i only get the reduced row echelon form of
the matrix and i'm going to assign that to a variable called a underscore rref and as you can see
that as we suspected that we are going to get back the identity matrix that's a square matrix
we have linearly independent matrix all the three columns are linearly independent if we do if we do
gauss-jordan elimination we are going to end up with the with the identity matrix and that is exactly
what we do get now let's have a look at the at the basis vectors for the column space of this matrix
that is now undergone elementary row or at least elementary row operations were performed on this
and what we get back are these three standard basis vectors and of course they span r3 and that would be
a different just a different set of vectors that are the basis as opposed to the three that we had
before so in our instance it does not change the space that is spanned but this is a very special case
so i want you to remember you know very important here that elementary row operations they are going
to affect the column space they're going to give you a different column space once you do any kind
of elementary row operations so as before with a null space we're going to go through all the kinds of
matrices that you might come across so our second one here is going to be a square matrix but now we have
we don't have linearly linear independence so i've created this third column which is just the addition
of the first two column vectors so definitely we have linear dependence here any one of these column
vectors can be written in terms of the other two so if i do the rank i better get back a rank of two and
indeed we have a two there now this has consequence for us remember that i have three columns i have a rank
of two which means i must have a nullity of one so there's going to be a vector that's a basis of the
null space but what does that do for our column space well we cannot we can no longer have a basis
that will span all of our three we're only going to get back two and what this column space is going
to do again it's going to return the first two of two column vectors that are linearly independent of
each other and the first two that it found was the first one and the second one or the zeroth one
and the first one when it comes to python indices and you can see them they're one two one and two
four one and if you think about those two vectors in three space they they will span only a plane
through the origin and that plane i want to use in inverted commas it's sort of an angled plane it's not
on the xy or yz or xz planes it's sort of angled and any solution vector x if i take this matrix and
multiply it by any solution vector that i can think of which is a vector x i cannot get to all of three
space there is a constraint now placed upon the possible result vectors and that is the column space
my b vector b is now constrained and that is that constraint is the column space and the two vectors
that we see there are basis for those now what if i were to do the column space
on the row reduced echelon form so now i've performed elementary row operations on a
and you can see the a dot rref the rref method but i use the zeroth index of that because i only want back
the matrix and not the tuple of pivots and then call the column space on this so this is me looking
at the effect of elementary row operations on my matrix does that change the column space
and yes look at that i have again two vectors they are now the basis of the column space of this new
row reduced echelon form of this matrix and we can see there the first vector is in three space and it
points along the x axis if you think geometrically the second one points along the y axis so any linear
combination of them has to be a vector in the xy plane but we've just said that these two vectors
here that are basis for the column space of the original matrix a that's going to be slightly angled so
on this angled plane through the origin in three space so these are not the same subspaces
they are different subspaces so please remember that if you do elementary row operations on a matrix
you very likely are going to change the column space because it's very rare that we deal with a
square matrix that is totally linearly independent we're going to deal with matrices in most cases
that have many more rows and columns that sort of quite normal and we are going to affect the column
space so don't make that mistake elementary row operations do not affect the null space or the row
space and the next video tutorial is going to be about the row space it does not affect those subspaces
but it does affect the column space it's only in that very neat example of a linearly independent
square matrix that we're not going to change the column space but in other instances be on the watch out
for changing that column space because what the problem is and i think what the crucial bit is
that you sort of have to realize is this is the following so let me take a matrix i'm just going
to create a solution matrix x there two to one there we go now i can use the simpi.matmul so that's
matrix multiplication of matrix a and the solution vector x so i'm doing ax and i'm going to get back some b
now if i just were to just do that it's just going to neatly print that a and x to the screen for me
so i've got to do that dot do it method there and that's going to do the actual calculation for me
and i get this vector b this result vector 9 18 6. so what i did is i took two times column vector one
of a plus two times column vector second column vector of a plus one times the third column vector of a
that is what ax means so i've done that for you here in print in equation three so it's two times
the first column vector plus two times the second column vector plus one times the third column
vector we do remember though when we set up this that this third column vector was just the addition
of the first two so i can this this three six two i can rewrite that and that's what we see on the
second line is one times one two one and one times two four one it's just the addition of those two
if you were to look at it one plus two is three two plus four six and one plus one is two so nothing
changes but these two column vectors they're the same as the first two column vectors so i've got
twice this column vector plus one that column vector that gives me three times the first column vector
two times the second column vector plus one times the second column vector that gives me three times
the second column vector and then there's nothing left zero left of that and this means i have a new
vector look at this three three zero that's a different solution vector than the first one now
first one was two to one now we have a separate one three three zero but look what happens if i do this
this taking matrix a and multiplying it by this solution vector i get back the exact same b so there
are more than one solution vectors that when it's acted upon by my matrix so a times that gives me the
same b there was some constraint on b in this instance and now you can clearly see why we won't have an
inverse for this matrix because i can't go back from b to a because they're two solution vectors that we've
seen here two different solution vectors x that get um well we'll we'll we'll use those terms in a in
in a different lecture when we talk about or a different way to look at the the multiplication of matrix
a with a vector x that linear transformation of x that two of these column vectors or two of these
solution vectors um they get this translation to the exact same vector there's we don't have all of
three space available to us now it's constrained to the subspace that is this plane through the origin
and clearly two different solution vectors x they get they get translated to this very same vector 9 18 6
i hope you can see that that is that constraint and hence this is not one to one and onto this this
linear transformation as far as the function is concerned and therefore it's not a bijection and
it won't have an inverse i can't get back because here we've shown at least there are two ways to get
back or there are two different vectors to get back to if i took the try to take the inverse of a and
we will not have an inverse of a it's impossible to calculate an inverse of a so that's sort of the
nice part of understanding this idea of a column space so let's move on we're going to talk about
rectangular matrices with more columns than rows now we saw what happened there before if we look at this
first matrix simply matrix object that we create three by four matrix as you can see there by the reshape
i have 12 elements here in my python list i reshape them in three rows and four columns and we see
that so each my column vectors think about it they exist in are three in three space i need three basis
vectors so three of these i only need three of them to be linearly independent i have four so there's no
ways that at least one of them is not going to be a linear combination of the other that that should
just be logical so if i look at the rank now i've created this such that three of them are linearly
independent my rank is going to be three so again in this semi-special case i'm still going to span all
of our three but i now have four column vectors and and i can express any one of them as a linear
combination of the others i have four columns though and i have a rank of three which means i must have
a nullity of one there must be a basis vector in the null space and that is two first of all let's
just look at the column space and we can see it's going to return just the first three linearly
independent columns that it can find and it is those first three one two one two four one one three seven
the third one there at least so you can see what happened here so look at this third one it's one
plus two is three two plus four six one plus one is two so the second one was you know looked at the
first one then looks at the second one and says well it's not a linear combination of the first one
looks at the third one it says oh well that is a linear combination of the first two so it's going
to skip that one and then the third one finds well it's not a linear combination of those three that
come came before so it's returning those three that are linearly independent and that is the basis for
all of r3 so again i can span all of r3 but i've got four columns so a nullity of one so there must be
a vector that exists that at least is a basis for my null space look at that though that this solution
vector x that is a basis for the null space is an element of r4 not r3 anymore so that null space when
we have this kind of matrix shape is not necessarily then going to be in the same
or part of the same vector space as those column vectors now let's have a look at taking this matrix x
and there we go that was the basis vector and let's say three times that three times that vector
which is now a different vector minus three minus three minus three if i were to multiply this new three
times x by a it's still going to give me zero zero zero and that's what we mean it is a basis any any
scalar multiple of that and if there was more than one a linear combination of those basis vectors for
the null space if i were to take matrix a and multiply by any one of those vectors in that subspace
it's always going to result in a result vector that is b so that should be quite clear
once again let's do elementary row operations here and then call the column space and we see
that those are the standard basis vectors of r3 so this is once again a special case i did already
span all of r3 by my column vectors in as much as at least three of them were linearly independent the
fourth one absolutely whichever one of the four it is must be a linear combination of the others but
i'm still talking about the same columns column space it's still r3 but once again it's not always
so so watch out elementary row operations most definitely most of the time is going to give
you a different column space now we get to matrices that we'll see much more commonly we have many more
rows than we have columns so in our first instance we create a simpy matrix object and we have one two
three four five rows and three columns and that means that each of these column vectors are an element of
r5 and if we look at the rank we see that it's three so these are linearly independent column vectors
now let's call the column space of that and what we're going to find once again it's going to return
the first or at least all the first linearly independent column vectors that are confined
we have a rank of three we only have three columns so all three of those vectors now those each of
those vectors are an element of r5 but i've only got three of them so that is a basis for a subspace of
r5 i cannot span all of r5 now let's have a look at what happens when we do elementary row operations
on this matrix and as i said the most extreme form of that is to use gauss-jordan elimination
until we get the reduced row echelon form now look clearly at the difference between these three these
are not the same column space this is three basis vectors and another three basis vectors and they do
not span the same subspace of r5 because look at this if i look at x sub 4 and x sub 5 they're non-zero
in these first three but here we're looking at standard basis vectors and there's there's no way for me
to have any element in x4 and x5 they are all zero all zero so these you should clearly see that these are two
different column spaces two very different column spaces so do not do any elementary row operations
on your matrix before you look at the column space of that matrix so now let's look at a rectangular
matrix again with more rows and columns and now we have one we have linear linear dependence so let's
have a look at this one and you can see one plus two is three one plus three is four one plus five is six two
plus two is four two plus four six so clearly we have linear dependence here and if we look at the
rank now those first two are linearly independent so the rank is going to be two i am definitely going
to have a matrix in in the null space but let's have a look at the column space here once again returns
for me the first linearly independent vectors that it finds and now we can see there's only two so my
subspace is now even more constrained as far as r5 is concerned i only have these two basis vectors
once again let's do the reduced row echelon form take the column space of that once again look at this
x sub 3 x sub 4 and x sub 5 there's no way that a linear combination of these two basis vectors is going
to get anywhere in you know within x sub 3 x sub 4 x sub 5 in r5 they're never going to be values there but i do
have that with these two so these are not the same column space now let's have a look at the null space
of course i do have to have a vector in the null space because my rank was two i have three columns
so i have to have a nullity of one and there we can see i do have a vector in the null space by the way
let's do this and you can do that with any of the others before and now let's just do an elementary row
operations on this vector and then call the null space and you see nothing's changed that's still
the same basis vector so the null space doesn't change and in the next section we'll see that
the row space doesn't change but the column space definitely changes in in most cases only in those
special cases that it won't change so as before we're going to end off with a little proof we have
to show that this column space is a vector space so for any matrix a that is the shape m times n the
column space is a subspace of rm because there's m rows and my result vector will always be a m by one
and we know that rm is a vector space you know if once you've learned about vector spaces you can show
that rm has all those properties of a vector space they're all all fulfilled all those properties when
you look at rm so to show that the column space is a subspace we have to show any three things that
the column space contains at least one vector and remember there's always the zero vector even though
that doesn't span a space there is still a vector there more importantly we should talk about addition
that holds and scalar multiplication that holds so when we know that we can always find the solution
x equals zero for the homogeneous system b equals zero that's what i said so the there's always that
zero vector b now we want to know that if b sub 1 and b sub 2 are in the column space of a we must have
that b plus b1 b sub 1 plus b sub 2 is also in the column space of a so that's easy enough to prove
because by our statement ax sub 1 equals b sub 1 and ax sub 2 equals b sub 2 we've stated that those both
of those are in the column space so we can write these two and then we just do the addition a x sub
1 plus ax sub 2 that must equal b sub 1 plus b sub 2 and we know that we can rewrite the left hand side
as a times x sub 1 plus x sub 2 the right hand side is going to be a new vector but as we write it there
this new vector b sub 1 plus b sub 2 is in the column space of a because the way that we can write it there
and then the same for the scalar multiple c which is an element of the real numbers so if i multiply
left multiply both sides by this scalar c and by definition we still have that b is in this column
space so we can write ax equals b and then we just rewrite c ax as a times cx from the properties of
matrix vector multiplication and that's going to equal this new vector cb so cb is also in the column space
so we've shown that this row space is a vector subspace it is a vector space it's a subspace of
r m in these examples that we've looked at so i hope you learned something from this video on
the column space remember it's all about the result vector b that's what the column space is about you've
learned that once you do elementary row operations you're going to get a different column space unless
you have a special case and you can now clearly see at least as far as the square matrices are concerned
if you have linear dependence why you can't have an inverse why you'll have a determinant of zero
because it's this idea of translating our matrix x into this column space b and that there would be
more than one vector that can go to the same column b and then that means if you want to do an inverse you
know which one of those two are you going to go to so you can't really it's it's a clear way to see
why you won't have an inverse in the next video we're going to look at the row space of a matrix

Welcome back we're going to continue our continuing our look here into linear regression really to
cement our understanding of deep neural networks i want to apologize again for all this banging
that you're going to hear they're building a neuroscience center right side out my door
they are here very early in the morning they stay until very late so even recording these before
work recording these after work does not matter the banging goes on every time i come to my office
if i have a free moment here i almost have to bring earplugs it's just become it's just an
absolute nightmare so if you hear all the banging nothing i can do about that the reason behind
these videos remember i want everyone to get involved with using deep neural networks to solve
health care problems i'm a surgeon it doesn't matter i know about deep learning i can use deep
learning in my research and i want everyone to be able to do that i want people with domain knowledge
in healthcare or those very interested in healthcare to reach out and work with people in in computer
science and mathematics who are already doing deep learning i want us to work together but it is us
with the domain knowledge with the interest that really has we have to bridge that gap we have to
learn about deep learning there is no excuse the time is now to make the effort and to learn about this
now these videos are not only for healthcare professionals or those involved in healthcare
even if your interest is well outside of that you only want to learn about deep learning these videos
will serve you properly as well now as i mentioned in the previous video this is our pubs our studio
document using markdown that i've published on our pubs i will write about it on twitter i will mention
it on linkedin these videos on youtube they are available here on our pubs and the actual files
are available on github so subscribe on youtube follow me on twitter connect with me on linkedin look
at all my files here on our in our pubs and also download the files for your own use on github i really
want to spread the word about learning about deep neural networks so in this video i'm going to continue
this looking at our at linear regression just as an example of the basic concepts behind deep learning
before we get into any models i'm going to use linear algebra and multivariable derivatives in this
video don't run away i'm going to make two extra videos just on the very basics of linear algebra and
the very basics of derivatives calculus just to show you what it's about you needn't do that
or you you really don't need to understand it though you're going to write a simple line of code
and the computer is going to do that all for you but i think it's worthwhile at least just to watch
those two just to just to remind yourself if you've seen it before or or just get behind you know
understanding it before you're using it i think it's worth the effort although a trade is not absolutely
necessary now if you really want to know about linear algebra and you really want to know about
multivariable calculus i have two playlists on youtube i'll link to them they each have i think over
a hundred lectures in each of those playlists if you're wondering about linear algebra or
multivariable calculus watch my videos on that now back to linear regression and how that is going to
help us i'm not going to read from this you can read this all on your own in case you don't want
to read it and you want to hear my voice explain a few extra things you know continue watching this
video remember that we are trying to predict some outcome called a target variable and we can also call
that those actual values the ground truth we're going to try and predict those ground truth by
creating some model and when we have only a single input variable to predict the target variable this
is how we can predict it this is a mathematical model of that so given any of the input variables
if i multiply that by some unknown called beta sub one here and i add to that some other unknown beta sub
zero it's going to give me a prediction of what it thinks the target value must be and remember if i
added an error term here i'm going to get the actual value and this a model is a model of a
straight line remember from school y equals minus a half x plus two that is the slope and that is the
intercept and that's exactly what this is beta one is a slope and beta one is a y-intercept it is a
model of a slope of a you know it's a model of a straight line my model in predicting an outcome variable
a target variable the ground truth if i only have a single variable is the straight line as simple as that
we've looked at this before where we look at the error that i make every time so if i whatever
predicted this y hat sub zero given any input value minus the actual ground truth value and i square
that we are now going to call this this error and we're going to call it something else we're going
to call it a loss function so if my target value is a number and my prediction is also a number i can
subtract those two so my loss function is just the difference between two and i remember i square it
because i don't want negatives so they all cancel out if i sum over all of the errors but i'm going
to call that a loss function if my prediction was something else it was you know is this a malignant
nodule on a ct scan or is it not my loss function would look different this would not be the loss
function for that so every kind of problem that we are going to develop the deep neural network for
will have a different loss function it just depends on the very type of variables that we're dealing
with this is not a ct scan these are two sets of numbers a number i'm predicting and a real number
ground truth number so this would be one loss function every problem will have its own type
of loss function now some people make this distinction between the two some say loss function
cost functions is the same you know it's a new field there's so many terms and there's no real
standardization of what these terms are so i'm separating these two terms out just for clarity's sake
i'm saying this is my loss function but remember i have more than one sample
so i've got to somehow combine and express all the errors in one and one way to do it
is this i'm going to sum up all my errors so sum up each of these over each of the eyes in the
in the sample i'm going to sum all of them and divide by how many they are and i'm going to call
this my cost function so the cost combines all the individual losses so what this is what is it well
it's just the average of all the individual losses and you get other cost functions
so this cost function is particular to this problem where i have a numerical output and a
numerical predictor a numerical target which is predicted by a similarly numerical value so this
would be a cost function in this instance other loss functions will have other cost functions and
we can to look at them and the computer code in the end it's going to be very simple they each have
names and you just type in the name and the computer knows what to do no problem but i want
you to understand the concept now this l remember was this so if i plugged in this into this equation
and the y hat was actually that if i plug that into that and that into that this is what i'm left
with this is the full thing so the cost function given some these two unknowns beta sub zero and beta sub
one is this thing i'm going to do beta sub zero plus beta someone sub one they're going to stay the
same over all these i values sample one sample two sample three but i do that multiplication i do this
addition i subtract that from the real value i square that i sum of all of these i divide by how
many there are and that's going to give me the cost now let's put this into action what we're trying
to do though here is we're trying to get values for beta sub zero and beta sub one so we have this
very best line to make the minimum error my cost function has to be the the value of of all of
this has to be as small as possible and if i pick the right value for beta sub zero and beta sub one
i'm going to have the smallest possible cost that is my aim and to put it that way makes it very easy to
transform that into mathematical equations and that is really what we want so let's just create
a contrived example i'm going to have five values 1.3 2.1 2.9 3.1 3.3 those are my feature variables
and i'm going to just add some random noise to that you can look at this line of code so that i
have a target value based on the feature value i know that there's some connection between the two
and right here is where we plot it so here's my first one the input variable was 3.9 and the output
variable is 0.7 it was 2.1 for my feature variable and my target value is 2.2 for this one it's 2.9
feature 3.4 target 3.1 feature 1.9 target 3.3 feature 3.5 target and i can actually just write
them here you see the 1.3 i have to create something that will predict 0.7 i have a 2.1 it has to predict
2.2 given 2.9 i want to predict 3.4 given 3.1 i want to predict 1.9 given 3.3 i want to
predict 3.5 so let's plug that in into this equation that we have here this equation it's
just the sum over all of these pairs that the x y pairs that we've got here and we see them here
each of these are x y pairs there's the x y pair so let's plug them all in so there's the first one
the 1.3 and the 0.7 is the true value 2.1 my predicted 2.2 the true value 2.9 3.4 we subtract
that we square each of those we sum and we divide by how many there are and there's my cost function
now you can do that all on paper and you'll see it comes out to this there's my equation my cost is
6.55 minus 4.68 beta zero plus beta zero squared minus 13.1 etc etc etc this is equation in two
unknowns beta sub 0 and beta sub 1 and i can graph this an equation in two unknowns i can graph this
and this is going to be some shape in a two-dimensional space two dimension because i have two unknowns and
this is what it actually look like looks like this equation looks like this and how can i minimize
this cost function c is here on my z axis my up and down axis well it will be a minimum
when these two values beta sub 1 and beta sub 2 gives me the lowest value on my z axis my z axis
goes up and down here and somewhere along this curve you can see from left to right it curves up
and it curves a little bit there somewhere there's a value for beta 1 and beta 0 that will make
c here on my z axis the lowest possible and those are the values i'm after i'm trying to learn what
these two values should be beta sub 0 and beta sub 1 so that my cost function is at its lowest that will
give me the best lines the best values for my parameters beta sub 0 and beta sub 1 and remember
i can use those as slope and intercept so i can draw a line through these points that will minimize the
error because remember if the line goes here for a given input here of 2.1 it might
predict 1.75 right here and there's a difference between 1.75 and the actual 2.2 but i want all of
these errors along this line to be minimized and the way to do that is to change it into a mathematical
equation here we have two unknowns it draws a shape for me in three-dimensional space and i want where
the this value on this what values for beta sub zero here beta sub one there will give me a little
point here somewhere and i want it so that it is the lowest point on the c on the z axis
how do i do that we do that through taking what we call partial derivatives now i'm going to make
that little video on partial derivatives for you so that you can just if you want to know where it is
if i take the partial derivative of c with respect to beta sub zero i keep beta sub one as a constant
if i do the derivative with respect to beta one i keep beta sub zero as a constant and there i get
the two derivatives now what is a derivative it is a slope and i'm very interested in those slopes
because at the minimum my slopes are going to be zero i'll show you down later on down i'll give you a
better explanation for that but they are my two partial derivatives and i want to set each of them equal to
zero so there's the first one and there's the second one and i'm just setting them to zero i want
where the first derivatives are zero so what i can do is this minus 4.6 take that over to the other side
the negative 13 i can take that to the other side so i'm left with these two equations this one
this it won't draw this one here and this one here so there's two equations and two unknowns i can
write that as an augmented matrix and that's why we need linear algebra so we need both
uh derivative calculus calculus calculus of derivatives multivariable derivatives and we
need linear algebra i can do elementary row operations and reduce this to reduced row echelon
form and again watch the video on that which gives me a solution for beta sub one which is negative beta
sub zero which is negative 0.5 and beta sub one which is 1.1 and there we have beta sub zero is negative
0.5 and beta sub one is 1.3 that's this intercept and the slope for the best possible line because
by putting the slopes to zero don't worry about it it'll give me the values for the best possible
line and let's cheat again using linear models here in inside of r and i give it a little formula for that
and we see the two values there as predicted the zero minus 0.5 and the 1.13 for my intercept and my
slope no problem that's the basics of it how did it do that let's just reduce this instead of this
three-dimensional space because we have two unknowns let's reduce this to a single variable so we have
two-dimensional space and not three-dimensional space and i want to make it so simple by this very
simple equation that you must have seen at school y equals x squared there it is for every x that i
put in here one i'm going to land at one there because one squared is one and two squared is four
so i'm going to land up with four there etc very simple now don't get this x and y confused with what
we had above the y being the target and the x no no no i'm using something from school x and y but remember
they represent on this just a beta one in fact the x here is i'm just going to use it to be beta one
and i'm trying to find where on this beta one line this x line where must i go on this x line to get this
this curve that my model created just as my model created this curve up here the three-dimensional curve
through my loss and cost functions imagine that my cost function is now just this thing i want
to know where on the x-axis is this thing at its minimum now it's very easy to see the minimum is
right down here where x is zero but imagine you know it's a con convoluted shape as we had here it's not
easy just to see there where the minimum is going to be and as we go with more and more and more
predictive values feature variables you know it's going to be this convoluted shape and
multi-dimensional space you wouldn't know where to start to get to the minimum and that's that
minimum where we're after now one way to go about it is what is called gradient descent here as opposed
to what i used up there to do it and the gradient descent says let me just blindly start anyway let's
start here at negative two what is the slope here of negative two well i've got to take the derivative
of x squared the derivative of x squared is just two x watch the video on derivatives if you can't
remember and at negative two two times negative two is minus four so if i draw a line that just touches
this point here where x is negative two or i should say beta one in our analogy here is negative two it'll
be the slope right here and the slope is negative four you can see a line a tangent line that just touches
the minimum would be a slope of zero so we're trying to get to a place where the slope is not negative
four but zero so how do we get from a point now it's again as i say just looking at this is easy but
imagine in multi-million dimensional space it's not that easy but i can use this slope here to get closer
to this idealized point where x is zero and the way that i do that i take this negative four
and i multiply it by a tiny little step called what we call a learning rate say that's 0.01
so i'm going to take 0.01 minus the negative four and that gives me minus 0.04 but i subtract that i add
a negative to that which gives me plus 0.04 and that is my step so i'm going to go from negative two using
my slope which was negative four there and i'm going to step to the right 0.04 places that's going
to bring me right about there to negative 1.96 at negative 1.96 i'm going to get the slope again
i'm going to plug that slope in here multiplied by the learning rate and subtract that from where i was
before at negative 1.6 which is going to bring me closer down and so i'm going to go closer down closer
down closer down closer down and every time i'm going to see my slope getting closer and closer
to zero until i approach this spot here where the slope is zero i've used gradient descent by using the
derivative of each point to get to a lower and lower and lower and lower point and look just look back
just just think about it at the moment i can construct a cost function no matter in how many
dimensional space i can then use this idea of gradient descent to go down on this slope as i
say as we go up and up in dimensional space we have to use these partial derivatives not just a
single derivative here i said the derivative of y is with respect to x is just 2x i have to use partial
derivatives and i have to walk in each direction so if i go back up here oh let's go let's go if i just
randomly started at a point and that's what we do we randomly start at a point i look at and i walk
in this direction and i walk in that direction and if i combine these two separately so i work from here
to here i hope you can see the cursor and then i walk from there to there that would have been the
same as just walking straight whoops that same as walking straight down there but i'm still walking
straight downhill but instead of one single step i take two steps one in this beta sub zero direction and
one in the beta one sub direction and if i had multi-dimensional space i would combine separately
all of those and all those steps together would eventually be somewhere lower down the slope and
i'll use through all the partial derivatives and all the directions i will combine all of them and
walk further down the slope and further down the slope and further down the slope until i get to a
point where the slope is zero so think about it you're standing somewhere in a valley close your eyes and
you have to walk down the valley you can what you need to do is to pick a direction and just in that
direction just a straight line through your feet which way goes up and which way goes down just through
your feet so you'll take a step in the direction across that line straight down you'll turn 90 degrees
orthogonal and draw another line through your feet and at that line which is now perpendicular to the
first one just draw a line that line see which direction is upper the slope and which is down
just take a small step down so combining those two little steps you took would have been just
stepping across once now again draw a line again parallel to the very first line you had and decide on
that line which side is up which side is down walk a little bit down turn that line 90 degrees
and again is that which way is up and down let's take a little step down those two combines will be
down and so you can combine these these two these steps at 90 degrees with each other say left right
left right left right and so you can work your way blindfolded to the bottom of this valley just
thinking where you're standing and which way is down which way is up but in two different ways if you
were being that can live in multi-dimensional space of course you would have to do this for each
for each dimension that you are and that's what we're doing here with partial derivatives we're just
looking at a single direction take a step for that one take a single direction take a step for that one
and for all of those combined would have been one huge leap and that's what we're trying to do
in simplifying it here by just showing you how we step down using the slope the the partial derivative
on each of these and eventually we're going to get to this bottom and that minimum that minimum is
where the values that we're after the parameters that we are after is at its best where the prediction
is going to be as close as possible to the ground truth so that's it we really looked deeply now into
linear regression and how how that fundamentally in our head makes us understand that we can we can
create this cost function and we can minimize this cost function in a way so that we walk down this
slope of this cost function to get the very best values to minimize our error and that is what we're
going to do with deep learning we're going to create this model and we're going to have you know
in some of these there are millions of parameters not just beta sub zero and beta sub one there are
millions of them and we call them weights in deep learning same thing we're going to have millions of
them and we have to optimize them so that all of their values are at the very best so that in the
end our cost function is at a minimum and the error that we make in our prediction is as close to the
ground truth as is possible and there you have a deep learning so from here we're going to start
looking at deep neural networks themselves but hold on tight to these very basics watch the video on linear
algebra and the video on derivatives they will help you out you don't have to as i said if you really
want to get behind this if you really want to get to doing some research really on deep learning
itself as opposed to apply deep learning which we do in healthcare we apply to a problem then you've got
to understand multivariable calculus you've got to understand the algebra and i've got those videos
out i've made plenty of those lectures you can watch them and i'll put the links down below i'll speak to
to you again

In this video, I really want to discuss, very briefly, some improvement techniques in the training phase of neural networks, as you set it up just to improve training.
We have this problem that when you do training, that it is a very empirical process.
We're not talking about inferential statistics where we have well-defined equations and tests.
It is also iterative. You have to run a model, see what happens, and look at the results, and then change things.
And you have to repeat this over and over again.
And for very good training, you also need very big data sets.
So all of this really takes a toll on your computer resources and your time.
And we have to look at things that we can do just to improve this learning process.
So in this video, I'm going to mention a few things.
I would rather you read the document that's available on our pubs that we are looking at now,
or download the RMD file from GitHub and have a look at it yourself.
There is some mathematics in it.
I've kept it very, very easy.
And I'd rather just mention a few concepts so that when we do get to the code,
you understand it, at least have some intuitive feel for what is happening and why we are using it.
So one of the first things that we want to do when we bring the data in is to what is called normalize the input features.
We can also call it standardized.
There's different forms of scaling of the input.
But imagine you have a number of feature variables, and they're all at very different scales.
Some might be just fractions.
Some might have data point values that are in the thousands.
Those are very different scales, and that is going to perhaps lead to a multidimensional gradient that we're looking at,
a cost function, that is in some way very elongated in some directions as opposed to others.
And what you want to do is just bunch those all up so that they're all within sort of the same scale.
So one way to do would just be to normalize it.
This is actually standardization.
Standardize it.
So you calculate the mean and the standard deviation for each of your feature variables.
And then you go feature variable for feature variable, data point value for data point value.
That's what the X sub I has.
Subtract from that the mean for that variable and divide it by the standard deviation of that variable.
So that's a very common thing to do.
Where we start looking at images, for instance, and if an image is made up of pixels,
and the pixel is just some brightness value from 0 to 255, we can just divide every value by 255.
That's the maximum.
So that would be another way of scaling.
But this is one of the common ways of scaling, just standardized.
So you can imagine if you do that, you're going to end up with that variable having a mean of 0 and a standard deviation of 1.
That's why we call it a standardization.
There's an important thing that you have to note, though, is when you split up your training and your test sets,
you calculate the standard deviation and the mean for the training set.
That's what you do.
And then you use that very same mean and standard deviation on the test set.
So don't do the mean and standard deviation on the whole data set.
And don't take the test set and do its mean and standard deviation for its normalization.
You use the parameters from the training set.
And you also apply those same parameters to the test set.
That's a very important point.
And sometimes that gets missed.
Another thing I just want to mention is just vanishing and exploding gradients.
So just imagine, you can all think about fractions.
If I take a half and I multiply it by two thirds, these are commutative.
It's multiplication with real values.
I can switch those two around.
But in any way, I'm going to get a third.
Or a quarter times 9 tenths is 9 over 40.
And if you think about it, if I have this A over B times C over D, I'm going to get AC over BD.
And if I put some constraints on this, so that A is always smaller than B, so this is always going to be some fraction less than 1.
C and D is also going to be some fraction.
And I make A, B, C, and D.
They're all positive integers.
So those are my constraints.
If I do this multiplication, what we're always going to find is that A over B is going to be less than or greater than AC over BD.
And C, D is also going to be bigger.
So what I'm just trying to say is this value here is always going to get smaller and smaller and smaller.
And if you think about what we do with the weights, just forget the biases at the moment.
We start with this weight matrix times the input vector.
That gives us something when it goes through an activation function, addition, all of that.
And then I take the weight 2 and I multiply it by these two.
And so on and so on and so on.
And what you can see with these weights, if they're all very tiny between 0 and 1,
what is going to happen, it is going to get smaller and smaller and smaller.
And that is what we call the vanishing gradient problem.
So these weights, they are just going to get smaller and smaller and the derivative smaller and smaller.
And you get that it vanishes.
What you could also get for the same argument as we used before here, if these are all more than one,
this is just going to get bigger and bigger and bigger.
If both of these values are more than one.
And then you get the exploding gradient problem.
So for the same argument, you're going to get that.
And then the similar arguments hold for the backpropagation step, of course, with updates.
The whole system just means that you get this either vanishing gradient problem or the exploding gradient problem.
And one way that we can mitigate that is this.
When we set up our problem, we can initialize our weight values.
Remember, if we start a neural network, those initial weights for that first multiplication,
those are just random values.
We can normalize that.
And what we do is we set the variance of the weight matrix to the reciprocal of the number of input nodes,
which is to be multiplied by that matrix.
You can read that sentence.
It's not that difficult.
And just a little caveat is that if we use the ReLU, we divide by or we use 2 over N instead of N.
So what we do is we take that N is the number of nodes and that we are going to be multiplied by in that layer.
And we just multiply each value in the matrix by this.
For ReLU, it's 2 over N.
For the others, it's just 1 over N.
And for tanh functions, that used to be called Xavier initialization.
So that just means what you do is you just calculate the variance of the matrix or set the variance of the matrix.
And you do that just by multiplying each of the elements.
If it's ReLU, you multiply it by 2 over N, where N is the number of nodes in that layer that you're going to work with.
And for the others, it's just 1 over N.
And that is just something we can set in code.
The next thing I just want to mention is mini-batch gradient descent.
So if you think about an epoch, before you take any step in any of the directions in your cost function to go down the gradient,
you are going to have to work through all of the values.
And if we have very big data sets of millions of samples,
that's going to take a lot of computation before you can take one tiny little step.
So there was this invention of what is called the mini-batch gradient descent,
where you break up your data set, all the rows, into little sections called mini-batches.
And instead of using the whole data set, when you do your forward propagation and back propagation,
step to update your weights and your biases,
you don't do that on the whole data set.
You just do that on a small batch.
And then the next batch, and the next batch, and the next batch.
And in each of those, of course, you take a little step.
So running through one epoch, which means going through the whole data set,
you've already taken many steps before you get to the end of that data set.
And we call this mini-batch gradient descent because you create these little mini-batches.
But when we write code, usually it's just referred to as batch size.
But in reality, if you do batch gradient descent, it refers to using the whole data set.
And when you use sections of it, it's called mini-batch.
But in code, we set the batch size, which refers to the mini-batch.
The extreme of this is a mini-batch of one.
So every example, you're going to get a Y predicted.
You do your cost function and you update.
Through every sample, that's called stochastic gradient descent.
And you can well imagine that if you have a million rows of data,
that's going to take a million little steps through one epoch.
And it's just going to wander around almost aimlessly.
It's not aimless.
But it's going to wander around quite a bit.
And there's ways that we can mitigate that.
So the usual thing is to go somewhere in between.
And we set these mini-batch sizes to be powers of two.
It's usually here in the range of 128, 256, 512.
And that works really well with the memory architecture of many systems.
What you need to do, though, is, depending on the type of data that you have,
is that these batches fit within the memory capacity of your CPU or GPU.
If that is so, if you can set these batch sizes so that it just maximizes
that potential of the memory of your CPU or GPU,
it has to fit in there.
Otherwise, you can't run it.
So that's the idea behind mini-batch gradient descent.
Then there's gradient descent with momentum.
Somehow what we're trying to do is we just want to speed up the gradient descent
in the eventual direction which it needs to go.
And this is quite technical.
The idea behind this, and if we scroll down,
we're going to get to root mean square RMS prop,
is that we use something called the exponentially weighted moving average.
This is something I just want to stop at and just try and explain very quickly.
Is, imagine you have a set of data points.
It's very easy to calculate the mean.
But you can also have a moving mean, a moving average.
In other words, you start at the first value.
Imagine it's 10, and its average is just 10.
You go on to the second one, and the second one might be 20.
So now the average between those is 15.
The next one might be 17,
and the average between 17 and the previous 15 is 16.
So you can have this moving average as you go through the values.
So that's one way.
What you could also do is to weight some of the previous ones
so they don't all count equally towards what the current mean is
as you run through all the numbers.
You can weight them.
And a very good way is to exponentially weight them
so that the ones that are just prior to the one that you're calculating at the moment,
where you are at the moment,
counts more than ones further down the line,
further back in time.
And that's called exponentially weighted moving average.
And that's the idea that we do here with momentum.
So we're somehow going to keep track of the weights,
and we're going to average over the last couple of weights.
But the further back in your gradient descent the weight values go,
the less they contribute to the current average.
So we're going to use averages for the mean, for the weights,
and not the actual weight that you're doing during that gradient descent step.
You actually do an average over the last couple,
and somehow you retain some of them
so that you move forward quicker.
And that's called momentum.
So yeah, I just have some data point values.
You can see that you can look at the code.
That's very easy to set up some code X and Y.
And we're just going to add some random noise to this.
And it's in the form of a sine function with some random noise.
And you can actually see the actual sine function value there.
It's plus added to one.
Every value is plus one.
So it starts at one, not at zero as the normal sine function.
But you see this.
And if we add some moving average, exponentially weighted moving average,
and you can look at the code to do that,
you actually see this green line,
which takes these data points from left to right as it goes,
and it calculates a moving average.
But it does the exponentially weighted moving average.
So of course, it's going to start at zero
because at that first step, you have no average
because at the first step that you take,
there's nothing that came before.
So you start at zero.
So it takes some time.
But you see that it always lags behind the true value,
which in this instance is just the sine function.
It's always going to lag a little bit behind.
And that's what you want.
And that's the equation there for it.
So that the moving average is you take some beta value,
some fraction,
and you multiply it by the previous moving average that you had
plus one minus beta times the current value that you are at.
And by setting beta,
these updates can be much more ragged up and down
as it pays much more attention just to the previous ones.
And the decay is a very quick decay backward.
And you can play with this value.
Usually we set it at about 0.9.
And you can play with this code as well.
Just put in different values here for beta
and you'll see this green line differ quite dramatically.
And when you expand this
and we just approximate a little bit,
what we can see here is that
the number of previous data points
over which the average is computed as approximately given
by 1 over 1 minus beta i.
And that i is whatever step you are at,
how many you've included up until that point.
So what we do with these,
with a momentum,
is we take these updates in the weight.
So that's the gradient
according to that specific weight,
the partial derivative for that specific weight.
And we keep tabs of it
as we run through all the gradient descents.
And the same thing,
we're going to get this beta value between 0 and 1.
And so we look at what the previous average was
and we add that 1 minus the current partial derivative
and that gives us a new value.
And that's what we're going to plug in
when we do the update of the weights.
Another way to go about it is RMS prop.
And what we do is almost exactly the same
other than the fact that we just square
this partial derivative in the end.
And when we do the update,
is we take the partial derivative
and we divide it by the square root
of this value that we calculated up here.
It's a very simple actual concept.
What is very powerful is to combine
this idea of momentum and RMS prop.
And when we combine them,
it is called ADM.
And you've seen we've used ADM before.
That stands for Adaptive Moment Estimation.
Adaptive ADA, Moment M,
and then there's no E for the estimation.
But it's ADM.
And we just combine these two.
One thing I just want to show you
is there's a way to get rid of this initial
having to catch up.
And that is, no matter what you use,
if you use this equation,
the V equation here,
or the S equation here,
for the different two,
no matter which one you use,
I've just used the row here
to indicate it's either one of those,
you take it and you correct it
by dividing by 1 minus beta to the power T.
And T is whatever step you are at now.
So wherever you are,
we used I before,
but whatever step you are from left to right,
if you plug it in there,
as T gets larger,
this is a fraction,
this disappears,
it approaches zero,
so you're just dividing by one.
So the further along you go,
it will have negligible effect
and will end up with a green line.
But initially,
it will move this one way up
and it will start
at a much more appropriate spot.
So we do correct this.
And then what we do
is we just combine this.
So the update is going to be
the V corrected from the momentum
and the S corrected
from the RMS prop.
And we divide those two
and that becomes
what we multiply the learning rate by.
So we've got two hyperparameters
that we can set.
There are defaults, of course.
And the defaults is,
what most people would use
is 0.9 and 0.999.
You have to look at the
current documentation
to see what the defaults are,
but you can set them
when you use Adam.
The next concept
is just learning rate decay.
And we said,
always said alpha
just to be 0.001
or 0.003.
But what you want to do
is initially
you can have a large
learning rate,
but as you move along,
you make it smaller and smaller
so that when you get
to the theoretical minimum
that you don't overshoot.
That means in the end
you're going to take smaller steps
as you approach this minimum,
but at least you don't overshoot.
And that initially
at least speeds up
the learning,
but then doesn't overshoot.
It doesn't keep
these big steps that you take.
And there's various ways
to go about it.
You can see one equation there,
but there are many types.
There's exponential decay,
there's staircase decay,
there's all sorts of decay.
And the best way
that we're going to look at it
is just to use it
as we do the code
with all of these.
We can start using them,
but at least you've heard
of them before
and you have some idea
of what is happening.
That's all I'm trying to do.
The last way
to try and improve your learning
is just called
batch normalization.
Just as we normalized
or standardized
our input values,
our input variables,
we can also normalize
the weights
that we are going to use
at each deep
or hidden layer.
And what we do
is we just normalize
the values in the nodes
and those are the values
before the activation function
kicks in.
So you're going to do
all your multiplications,
your weight matrix
times your vector
of the input values.
And then you're going
to normalize them
and then apply
the activation function
to each of those values
for each of the nodes.
and you can see
what happens there.
There's also a way
to write this value,
this updated value
before the activation
takes place.
Just to mention,
you can also do it
after activation.
There's some papers
on that as well.
You can do that as well.
But is to take this
and to add parameters
to it
so that you get this
Z that I've written
here with a tilde
and have these
two parameters
which are learnable
parameters.
And we'll see
how to set that
in the future video
as we make use
of batch normalization.
So these are not
hyperparameters
but they're learnable
parameters.
And we can use
those very effectively
just to try and set
these values
before the activation
kicks in
and optimize
those values
and that really helps
gradient descent as well.
So we're really trying
just to improve
on the computer
resource consumption
and hour time consumption
by implementing these.
And as I've mentioned
before now
we are going to start
using them
in the very next video
I'm going to show you
some of these
and as you start using them
just refer back
to this very simplified
version of it.
Of course you can read
the original papers
about all of these
if you're interested
in the mathematics
but as long as you have
some understanding
of what we're trying
to achieve here
that is fine
as you start
using the code
and you see
the effect
that these things
have
that's the important part.
So in the next video
we're going to start
looking at implementing
some of these.

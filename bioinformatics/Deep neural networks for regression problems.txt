So welcome back to the series just for domain experts trying to learn something about deep
neural networks. So up until now we've just looked at classification problems and we've also just
stuck to our multilate perceptron that is just a densely connected neural network. And I want to
draw to a close by just moving away from these classification problems where our target variable
is actually categorical in nature to a target variable that is numerical in nature. So we're
trying to predict a continuous numerical data point value. And that is called regression problems.
So I'm going to show you just how to construct a regression problem, how to construct a deep neural
network to handle this regression problem. So one or two little changes we have to make from the
classification problem that we looked at before. And I just want to show you one more piece of
insight into how these layers are constructed and just something to show you that you can explore
on your own, something else you can change about your models. This is going to be the end of the
section on the densely connected neural networks. And we're going to move on to the, I think, much more
exciting world of convolutional neural networks very soon. So let's have a look. We're going to use the
reader library, QRS, of course, and plotly. Let's have a look. So I'm just going to import some data.
And I'm going to use the read underscore CSV from my reader library. And I'm going to import this file.
Of course, this is on our pubs, this document that we're looking at. And the file, this actual RMD file,
and this data file will be available on GitHub. You can just look at the links in the description below
this video. So regression data.csv column names is false. So it's just rows and columns of numerical
values. Now, actual, the first row does not contain column headers. There's no variable names.
And if I look at the dimensions of that, I see I have 4,898 samples over 11 columns. And those 11
columns will be my 11 variables. That's 10 feature variable. And the last one, just to tell you,
is the target variable, which is numerical in nature. As per usual, this is my way of working.
I want to change it into a matrix. And I'm also going to remove the dimension names. Because even
if I change it into a matrix, and if I look at a view that's with a capital V in RStudio at this,
I'm still going to have the X1, X2, X3 at the top. So I remove all of that. This is a pure matrix of
numbers. That's the way I like to use it. Just to look at the summary of column number 11,
which is my target variable, we see that it's numerical variables with a minimum of 2.5 and a
maximum of 9.3. It's a medium of 5.9. And we see the first and third quartiles there. If I use plotly,
remember plotly, open and close brackets, that's just going to give me this empty canvas. And I use
my pipe operator there. So I'm going to add a histogram. On the x-axis is going to be the data set,
all the rows, column 11. I'm going to give it a name. The layout will have a title. We see the
beautiful title here at the top. And the x-axis as a list will have a title and the zero line being
false. I don't like these big black lines across my plots and graphs here. You can do what pleases
you. And look at this wonderful, we see these peaks that it forms my numerical target variable
ranging from the 2.5 to the 9.3 at the end. So we see this beautiful histogram. Lovely.
Now the train test. And we're going to do that split. I'm going to do it in the usual way. I'll
set the random seed. So when you run this code, you get exactly the same split. Remember, I'm going
to just create this index of values 1 and 2. And that's what the 2 is for. We'll automatically
start at 1. And I do that over and over again until the number of rows of the data set. That's
the 4,900 odd. And we're going to get here, I want an 80-20 split. So that 80% of the data
is going to make it into the training set and 20% into the test split. We've looked at this
many times before. I'll just move over quickly. And I'm going to create my x-train, x-test,
y-train, y-test variables. And doing that by using this index that I created up here. Very
easy to do. I've also got to normalize my data. And remember, I'm going to calculate the
mean and standard deviation of my training data set. And I'm going to scale my test data
set according to the mean and standard deviation of the training set, not the test set's mean
and standard deviation. And then, of course, I've got to just scale x-train as well. And
the scale will do that for me. It'll bring all 10 of the feature variables. It'll bring
them into a mean of 0 and a standard deviation of 1. Great stuff. Let's build our model because
that's where we're going to see the first little difference. Still a sequential model. I'm going
to have one dense layer, two dense layers, three dense layers. It's just for demonstration
sake here. Nothing special. I have 25 hidden units in each. I have the rectified linear unit
as my activation function in each. And my input shape for my first hidden layer is just the
number of feature variables, which is 10. I've got 20% dropout in each of these. And then
here's the special one. The last layer, because this is a regression problem, I'm just going
to have a dense layer with one unit. No activation function. Nothing. Nothing else. Just one unit.
That's it. Let's look at the summary of that model, which then gives me 1,601 learnable parameters.
And we can see the last layer there. Just a single node. No activation function. Nothing.
I want that pure value that comes out of there. And this is the bit of deeper insight I wanted
to share with you this time around as we end off the section on multi-layed perceptrons
or densely connected neural networks is this get config function. So I'm using pipe just
to pass model as the first argument here to get config. And look at this. Let's start with
our first layer. It has a class name. This is behind the scenes because we just did this.
There's lots of other arguments that I can pass to layer dense. And here they are. So the
class name was dense. The config was name dense one. I didn't give it a name. Remember, we could
give it a name. So just the default. Trainable. That is something we're going to look at in
the future, especially when we get to convolutional neural networks. It says that all these weights
and biases, I want them to be trainable. With backpropagation, I want them to be updated.
Now, what we're going to do with convolutional neural networks, you can actually download pre-trained
neural networks that will have these values already in them. And they can form the first part
of your neural network. And those weights are cast in stone. Someone else created them,
used huge numbers of data, and has already trained those weight and biases values. And
we're going to use them. So we can actually set that our weights for that layer is non-trainable,
especially if we bring that layer in from a pre-trained model. We see D type there is float
32, and that's the default for TensorFlow. It takes 32-bit floating point values. The
units, 25, 25 units, the activation. We actually said, ReLU, use biases that by default is set
as true. You can also set that as false. So there's no bias vector in your deep neural network
or tensile, I should say. And the kernel initializer, remember the first time this runs, it just initializes
random weights, random weight values. And there's different ways you can go about these. And the
way that is default is by variance scaling. And the config is a scale or standard deviation
of one. And the mode is this fan underscore average. And the distribution is uniform. And there's
no random seed. So look into all of these. These are actually quite fascinating. And there are a lot
of these distributions you can use to set your weights initially. The bias initializer is going
to be set to zeros. The kernel regularizer is set to none. The bias regularizer is set to none as well.
The activity regular... I'm not going to say that word today. This is impossible for me. None.
Kernel constraints and bias constraints, also called clipping, those are set to none. Also something
you can look at. Then we get to the dropout layer. The dropout is also trainable. Set to true,
this trainable. The rate is 0.2 which we set. We set no noise shape and we didn't seed it. So that the
same values are used every time. And then we get to dense layer number two. So there's lots of these
arguments which we never ever used when we constructed this. Look into those. They are quite amazing.
So we've got to compile now and that's our different, the second difference other than the dense layer
with a single node, no activation functions. Our loss is just going to be we'll choose mean squared error here.
And our optimizer is going to be RMS propagation, the RMS prop there. And we see there the RMS prop
empty parentheses there because we're just accepting all the defaults and the defaults are a learning rate
of 0.001, a row of 0.9, epsilon null, decay rate of 0 for our learning rate and a clip norm and a clip
value of 0 and 0. Now the metric we're going to set to mean absolute error. Now the metric remember
that's like a type of loss function but it does not form part of the actual gradient descent in the
back propagation. That just gives us our view of the error as this runs. The loss function that we're
actually going to use is not mean absolute error but mean squared error. Let's fit that data. We ran it
through 50 epochs here, batch size of 32, validation split of 0.2 and I put in a callback. My callback was
early stopping and I was monitoring the mean absolute error and if five values did not do any better it would
terminate those epochs. Verbosity at 2. I ran my model, do that and then let's just print this out.
So something new, if I use this backwards pipe, so look at this, it's less than and a minus sign.
So this evaluate function is going to give me two values, it's going to return two values and I'm
placing them inside of this list lost and mean absolute error and that is what it's going to return
for me, the loss and the mean absolute error. So I pass x test and y test to that. I didn't want
anything printed to the screen and I'm just going to do that evaluation and that's going to give me
these two values, loss and mean absolute error. And I'm going to print those two out using this,
you haven't seen this before, in R, the paste zero function. So I have my string there,
mean absolute error on test colon space and I'm pasting this with the print sprint s print f function
and that this percentage dot 2f means two decimal places and what I want of these two is the mean
absolute error and it prints me out a mean absolute error. Remember it was about three point something to
nine point something, an absolute, a mean absolute error of 0.62 so not too bad. So there you go, in short,
a regression problem. We don't often deal with regression problems but now you know how to do it
and you know how to create that last layer of your multi-layer perceptron, your densely connected
neural network and you know what which of these last layers the MSE mean squared error is a good loss
for that. You've also learned about all these other arguments that you can put that you can change
inside of the creation of your model. Next up we're going to start looking at convolutional neural networks.
Now, in a previous presentation
I guess we've already noted and do it first.
Traditional действительно, we know it represents the impoverished
connection to balance and storage of your model is the verso-лонnerer to business
– which means that they self-ука crue dir in Attack. – it can be very slightly different
と思います, but that's the way that we are in terms of what technology is used to Кон Casa
of the MSE. The other thing that you learn now,

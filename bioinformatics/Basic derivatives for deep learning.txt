So in this series of mine just on deep learning for people who are interested in deep learning
perhaps more specifically for medical personnel, healthcare personnel, people involved in healthcare
who want to solve problems in healthcare using deep learning
just making these first, these couple of videos just to get behind some of the mathematics
now I have a short video just on linear algebra which is very important in deep learning
and this one is just going to be on derivatives
Derivatives
Derivatives
There we go
Now if it sounds like the world is coming to an end
I write outside my office here they're building a new neuroscience centre
and over the last couple of months it's already, well it's driving me insane
There we go
So derivatives are very important
We're going to have this concept in deep learning called backpropagation
and that really depends on derivatives
I want you to relax though you don't actually have to do these derivatives
you're going to write a line of code and the computer is going to do the derivatives for you
You should have some basic concept
and if you haven't done calculus in a long time this is it
Now I do have two very large playlists on YouTube
One on linear algebra and one on multivariable calculus and I'll link them below
If you're really interested you can have a look at those more than 100 videos
More than 100 videos
If you want to get really deep into the topic I've got those playlists for you
Let's do a few short minutes just on derivatives
Now let's imagine an equation I have y equals x squared and that's it
or let's make it plus 4
x squared plus 4 so if we have this it's going to be something like this
and that's going to be 4 and we have this nice x squared plus 4
Remember behind the scenes there's another x here but it's to the power 0
Anything to the power 0 is just 1 and 4 times 1 is just 4
And remember how derivatives work
You must have seen derivatives at school
If I have x to the power n this is just simple
And if I take dy dx
dy dx is one way to write it
or y prime you might have seen
I bring that n forward and I take 1 away from the exponent
So if this was y is x squared
y prime is going to be bring that n which is now 2 forward
That's x and I'd subtract 1 from it
That means 1 that's 2x
And the same this is exactly what we have here
y prime is going to be 2 times x
And if I bring the 0 forward
That's 0 times 4 is 0 and 0 times anything is 0
So if we have a constant
The derivative of a constant is 0
It is gone
Okay
So if I have something like y equals 3x squared
Plus 2x plus 5
I have that dy dx is going to be equal
Bring that forward
2 times 3 is 6
Subtract 1 from that
So there's just 1 left
Bring the 1 forward
2 times 1 is just 2
And there's a 1 there
1 minus 1 is 0
So there's 0 left at the top
Anything to the power 0 is just 1
So 2 times 1 is just 1
So it's 2x
6x plus 2 is that derivative
Now let me just clean the board
Now that's very simple if I have a single variable
But what if I have two variables
We usually write that as z
And let's have z equals 6x squared
Plus 4xy plus 3y squared
Now I have two
And how am I going to take a derivative of this
If I look
Those will be in three dimensions
And we usually have this as the xy plane
And that's orthogonal to each other
So you can think that's in the corner of the room
And there we have z
So we're going to have that
This thing
I can't draw that
But imagine it is just going to be
This three dimensional thing
And space is not going to look like that
Believe you me
But just imagine
Now what we need to do in linear algebra
Is we're going to
The system has to learn certain values
And it has to learn those values
And the way that it learns it
Is it writes a multivariable equation like this
Now some neural networks we're going to write
And they're going to have more than a million parameters
More than a million of these
X, Y, Z, ABC
There's only 26
But imagine we carry on
And there's over a million
So it's going to be this equation of over a million
And it's also going to eventually
You could if we had access to so many dimensions
Imagine a million dimensional space
But it will be some convoluted shape
In this multidimensional space
And what we really want is the minimum
Where in this
Now this is very easy
In two dimensions
Where is this whole multidimensional thing
At its minimum
And when it's at that minimum
That's the point we're after
All the values
Of X, Y, Z
All those millions of parameters
If I had a value for each of them
And so that
That whole equation
Which is some graph in multidimensional space
Is at its lowest point
At its minimum
That is the value
That a deep learning network is after
If it gets that
It has the best values
To answer the question
That we're trying to answer
Through deep learning
So how do we get a derivative
How do we get to the lowest point
Well if we just have
One dimensional space
Let's have that
We can start at any little point
The derivative remember
Is the tangent point
The point of a straight line
That just touches
In three dimensional space
It will be a flat plane
Imagine a piece of paper
That just that is flat
And it just touches
Wherever the space
Just in one little point
But that will be a plane
In four dimensional space
It will be a hyperplane
In multi-million dimensional space
It will be
A very convoluted hyperplane
But it touches our graph
In one point
And I can use that slope
To get to a point
Here
I can
There's something I can do
To get to this point here
And at this point
The slope
Of that tangent line
Is going to be zero
And the plane
That lies here
That touches just there
Is going to have
This slope of zero
So in one dimension
As I said here
In this dimensional space
It's easy
I have a slope
And we use the slope
To update
To get to a new point
Which will have its slope
Which will have a new point
Which will have its slope
And we're going to
Change the slope
Change the slope
Change the slope
Until we get to a slope
Very close to zero
And then we know
We're okay
We know that we are okay
Now this is easy to see
But imagine a multi-dimensional space
Which we can't
Fathom in our brains
You don't know where it is
It's like
You are in the space
And you start walking blindly
Now imagine
Blindfolding yourself
And you're in a place
Where there's a bit of a valley
And you have to get
To the bottom of the valley
You know
You can wander around
Until you go lower
Lower
Lower
Until you get to the bottom
Of the valley
And that's exactly
What we're going to do
We're going to be somewhere
And then we're going to
Start taking steps
Now remember
When we looked
Just
Well in my video
If you watch that
Just on the
Simple linear algebra
For deep learning
You know
If I want to get to this point
And I see this point
As a vector
What I could do
Is walk that way
That way
But I can move this one here
So that's there
So to get to this point
I might as well walk
This way
And that way
So that will be
So many units of x
I walk in the x direction
And so many units
In the y direction
If I make that walk
I end up in that same place
As if I went straight there
And that's what we're going to do here
We're going to walk
In the x direction
Which points downwards for us
And then we're going to turn
90 degrees
And walk in the y direction
Along the slope that goes down
And if we do those two things combined
We've gone further down
And the more dimensions it is
You'll just have to do this
In one dimension
But what we do is
When I walk along the x axis
I keep y constant
I don't change on the y axis
And when I change the y axis
I stay in one spot on the x axis
I see those two as constants
And that's exactly what we do
When we do derivatives
And we write partial derivatives
Like this
And I'm just going to take
The derivative with respect to x
I'm going to see y as a constant
I'm not changing on my y axis
I'm just staying straight on the x axis
That means all the y's here
Are actually constants
So if I do this derivative
That x I can do
That's going to be 12x
By bringing the 2 forward
Now that is a constant
There's a 1 there
I bring the 1 forward
1 times 4 is 4
The x disappears
Because it becomes a 0
And the y is still there
Because it is a constant
That whole thing is a constant
And the derivative of a constant
Is 0
It's gone
Because remember
There's an x 0 here
If I bring the 0 forward
That's nothing
And that is my derivative
Of z with respect just to x
And if I do the partial derivative of z
With respect to y
I see x as a constant
That means there's a y 0 here
So that's going to disappear
There's a 1 there
So that's going to become 4x
And that's going to become 6y
And that's going to become 6y
And those are partial derivatives
And all you do with partial derivatives
You keep all the others constant
So if there was also w's and v's
And a's and b's
And whatever in there
You keep them all constant
You only look at the derivative
With respect to this
You keep the other one constant
And eventually
You'll get this multidimensional
Pointing things
And you combine all of them
You do the one
Then the other one
As you walk there
And there
You walk along this way
Then along that way
And in multidimensional space
If you have basis vectors
That are all orthogonal
Everything is going to lead you
In a different direction
But if you combine all of those things
You're at a lower point
And now we've got a new point
On this thing
We can take a new slope
Do all the same thing again
And eventually we'll be lower
And so we will go
Until we get lower and lower and lower
If we just repeat
We repeatedly do this
And that is why we have
These things called derivatives
So together with linear algebra
Derivatives are very important
Now if this was all Greek to you
Don't worry about it
If you're interested in it
Watch my over 100 videos
Just on multivariable calculus
It's quite interesting
Multivariable calculus
But really you don't need that
For where we are going
As if you just have some
Mild slight concept
Of what is happening here
That's all that's required
We don't have to take
Any derivatives when we design
We just have to understand
That that is what the algorithm is doing
It is trying to find a path
That lowers the slope
So that we can get to this bottom point
And that in any dimensional space
Is what we are after
Because that is going to be
The best values that are learned
By the deep learning network
To give us the most accurate solutions
那個 Сам让
The algorithm
Is where we're at
The nächsten
Another tool
Maybe
From the normal speed
What we are then
With a good performance
Would not let
Build up
With a good performance
That will make
Think of it
For the mastery
Than that would help
The perfection
Answering
The интерakh
Theowner
The
Dare

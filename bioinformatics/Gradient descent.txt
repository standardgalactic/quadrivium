In this video I want to take a closer look at Gradient Descent.
Now this is a series for healthcare workers, healthcare professionals,
those perhaps people, well everyone, perhaps not people who have an in-depth knowledge of mathematics and computer science
but really want to contribute to this field.
So this is not absolutely necessary for you to be able to write the code in the end,
but I still feel that it's important and you can watch that.
Now on the video which I'll link to up here, the video on just looking at linear regression
and to develop that intuition we really looked at developing this cost function.
The cost function, remember we have this set of feature variables,
we're going to multiply them by these parameters and we're going to get to a result.
That result is going to be different from the target, we subtract those two from each other,
we square them and then somehow we go through all of the samples in our data set
and we can either, you know, one of the best ways is just to average those
and that gives us a cost function and that cost function will be a function of all of those parameters.
And when we just had a single variable, remember we just had a single feature variable
that still gave us two parameters that we had to find that is in two-dimensional space
and if we have many more features we're going to move into multi-dimensional space.
So we have this function in multi-dimensional space, we can draw it in multi-dimensional space.
What we want to do or what we are saying mathematically is at the bottom of this function
at the minimum because remember on the x, if we boil this down to a single variable
which is not realistic but a single variable, on this axis I'm going to have my values for beta
and on this axis my cost and I want to minimize, I want to see where the cost is at its absolute minimum.
Now as I said, this is idealized, I'm bringing it down to a function of a single variable.
f of x at school, y equals f of x and here we have x squared minus 3x plus 2, something to that extent.
Doesn't matter.
We have this cost function and I want the point where the cost is at its lowest.
I mean for me that is beautiful that we can break down this problem
where we want to create this model that is as accurate as possible.
We can break that down into some function that we want the minimum of.
So when we have that, there must be a way that we can say, well initially we just give a random value.
We're saying we are starting right there.
Let's start right here with a certain beta value.
That certain beta value is going to give me a certain cost.
And there's the cost.
And I want to now know, well that is, just at random that point was chosen and that's how it's done in deep learning.
I want to get there.
How do I get there?
Well I have to use what I have and I have to update this value.
There was the, in reality, this is the point I am looking for.
In reality, I have got to move from this point to that point.
How do I do that?
And we use this idea of gradient descent.
Obviously, the gradient, if I'm standing on top of a hill, there's a gradient down.
I'm using gradient descent.
I want to go down.
I want to descend along some gradient.
And a gradient at a specific point on a graph is nothing but a derivative.
A derivative.
And we've looked at derivatives.
I need a derivative.
But let me just show you then how this really works.
Because I can use this derivative to generate a new value for beta, which will be here, which will give me a lower cost.
But I've got to get from there to there.
How do I get from there to there?
And eventually, in these tiny little steps, how do I get there?
Again, as I say, just by looking at this, it's easy to see it's got to be there.
But in multidimensional space with lots of beta values, it's impossible to do.
How do I physically do that?
Because what I will have then is a better beta value, which I will put into my cost function, running again through all of my samples, averaging all my errors.
And I'll have a new cost function with this new beta, and that will be slightly better, and better, and better, and I run through it again, and again, and again, until I eventually approach this point.
So what I've done here is I'm just expanding this so that we have this better view.
So I start with this random value of beta right there.
I have some value of beta, which I've plugged into my cost function, and that is going to give me wherever this is, and it doesn't matter which side I start on.
It doesn't matter which side I start on.
So I start with a given beta value.
Now, how do I get from this one to this one, which might be slightly better?
The trick is to take this point and to get the derivative of this function.
So I'm going to get the derivative of my cost function with respect to this parameter of mine, the first derivative.
And remember, the first derivative is nothing other than a slope.
So I'm going to get a tangent line which passes and touches that point just there.
That's all I'm going to have.
And what I'm going to do is I'm going to say I have an old beta.
Let me say this beta old, the one that I start with.
And I'm going to update it by taking that and subtracting from it some learning rate, which I'm going to just put a Greek symbol for alpha, and we'll see what that is about.
A learning rate multiplied by this derivative, dcd beta.
And why negative there?
What is this?
Why this negative?
Well, what I'm going to do, I'm going to say beta new.
My new one, which I'm going to move towards, is take wherever I started that value and subtract from that minus the product of these two things.
Now, if I'm on this side of the ideal point, my slope will always be negative.
Remember, this is a negative slope, and that's a positive slope.
Because we go from left to right, so we're going downhill, that's a negative slope, that's a positive slope.
So if I have a negative slope, this value here is going to be negative, and a negative times a negative is a positive.
So my new one is going to be the old one plus something, this value plus this one, so I'm going to move slightly in this direction.
If I was on this side, it would have been a positive, so there will be a negative times a positive, which will be a negative.
So my new beta will be the beta minus something, so I'm going to move in this direction towards this point, so it works out beautifully every time.
So that no matter which side I am, if I do this negative, I'm always going to move in the right direction.
This alpha, as I said, is called a learning rate, and that determines, you know, how big these steps are that we take.
And usually we have alpha values of, say, 0.01, 0.001, we can even 0.1, there's these orders of magnitude difference between them.
Some people, like Andrew Ng, likes 0.3 and 0.03 and 0.001, he goes up in steps of 0.03 there.
He just multiplies by 3 every time, and goes up from there.
It doesn't matter, it's in this sort of range.
And that ensures that this step is not too big, and I'll show you why we don't want this big step.
But this is a thing of beauty, because I now have a new beta, which sits right here.
I can go up and see what the cost is there.
I see the cost is less.
And this now becomes my old beta.
And at this place, I take the derivative, which I can now plug in there, multiply it by the alpha,
subtract that from the old, which is this one now, and get a new one, which is now going to be here.
And I'm going to repeat that story every time going through that whole algorithm.
And every time I'm going to update my beta, update my beta, by using whatever the slope is,
the derivative at that point, at that point.
And please remember that this graph is just a graph that comes from the creation of my cost function.
Now, just to show you here the reason why we don't want to take big steps,
because you would think, well, then we can very quickly go there.
What happens is we overshoot.
And we go from this point to that point, and then back to this point, and back to that point, and back to that point.
And you keep on overshooting, overshooting.
And you can't do that, because then you're never going to settle on this lowest point.
We also don't want this to be too small.
Otherwise, we're going to take so many steps, and we won't move for very long.
And that takes, computationally, that takes a long time.
So it takes a long time for our computer program to get towards that point, and we might never even get there.
So that is all very nice, and I hope you really get it.
I hope you have an intuitive understanding of what is going on here.
It's just this beauty.
I mean, if you just think of every step that we've constructed here, it is such a beautiful thing that we can construct this,
and we can go look for that minimum of our cost function, which is really, remember, just an average of all the errors that we make.
It is a beautiful system, but please now understand that we use the derivative of every point, which is that slope at that point, to build into this thing.
And we keep on updating.
We get a new beta by taking the old one where we started, and we subtract from this 0.001, whatever we choose for alpha, times what the slope is.
The slope is going to be negative on this side, positive on that side, which means we're always going to walk, we're guaranteed to walk towards, you know, in the right direction.
Now, here I'm trying just to do a three-dimensional, very difficult for me to draw.
I have not an ounce of artistic ability.
Anyway, this is the x-axis, the y-axis, and then the z-axis, and what I'm trying to show here are these dotted lines that I've got this plane, which is parallel here to my x-axis.
Because, unfortunately, it's not that easy.
We don't just have this simple one.
And even for a single variable, we can have this shape, for instance, this bowl shaped here in three dimensions.
So what do we do?
Well, now we have a water crisis in Cape Town, so in my office, I only drink from paper cups.
Now, take a little paper cup like this.
I don't want to because I want another cup of coffee, and I'm trying to spare this.
And we use the coffee because we don't want to use water because we have no water in Cape Town.
And we have a water crisis, so we have a little bit of water.
We're trying to save.
We're all doing our bit.
But if I imagine this is my cup, and it won't look like this.
Imagine it's round at the bottom.
But imagine if I cut through it.
Now, this is what I'm doing here.
I'm saying there's this three-dimensional object here, and there's this plane cutting through it.
And think about that just for a moment.
If I have this plane that lies here, obviously, on the x-axis, which is here, there's plenty movement on the x-axis.
But it cuts that y-axis in one point.
I'm keeping y stable.
It is not moving at all.
And if I were to cut through that, imagine, take a pair of scissors and just cut, like, try and cut a line through this.
There's going to be a very nice little graph there.
On that cutting edge, there's going to be a knife.
So, a knife's edge there.
And that is what you do with the partial derivatives, if you looked at the video on partial derivatives.
So, if I take the partial derivative of my cost function here with respect to x, it means I keep y completely separate.
And if I keep it constant, and if I take a derivative with respect to that constant, it disappears.
It's zero.
So, it will tell me in what direction I have to come on this axis.
And I flip it around, and then I cut through there, keeping the x-axis constant somewhere here, just going through somewhere here, and going through a specific spot there.
And if I cut through it there again, there will be another little graph of that line of intersection.
And I can check what slope I wanted there.
And remember, when we talked about vectors, if I wanted to get there, what I could do is first walk in that direction, and then walk in that direction.
And that's what we're going to do with deep learning.
No matter how many dimensions it is, we keep all the other dimensions constant, and we just look at where we're going to walk in that direction.
Then we're going to see what we walk in that direction, and then in what direction?
In multi-dimensional space, multi-million dimensional space.
You can all break it down by keeping all the others constant.
And think about this analogy of this cup.
If I were to cut through it, where that cut line is, it's going to be a nice little graph for me.
So, no matter how many dimensions I work in, there's always going to be that little bit of a slope.
And through this analogy, we can walk in the right direction to eventually get to the bottom, where my cost function is going to be at a minimum.
I really hope that this helped you.
As I said, I'm not aiming this series at someone who's a hardcore mathematician or computer scientist who's studied these things.
I want everyone involved to help us use deep learning to solve medical problems, healthcare problems.
And you don't necessarily have to know this, but I think there's an elegance, there's a beauty in this.
And it would be lovely if you did understand it.
Leave me some lines in the comment if I can explain it in a different way, because I really want you to grasp these concepts.
They are beautifully else.
I'll speak to you in the next video lecture.
Thank you.

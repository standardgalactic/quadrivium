In this network, we're going to do some summary statistics, some of the most basic things that you
can do in data science. We're going to import some data, and we're just going to describe that data.
And that really is one of the first things you can do when you start analyzing data, once you've got
the data in Python and you've cleaned it up, just try and understand the story, the information
that's hidden inside of that data. We are on Notebook 05, Descriptive Statistics. And as I
just mentioned, one of the first things that you'll do when you analyze data, it's very difficult
for human beings to stare at, for instance, a big spreadsheet file or massive amount of images or
audio waveforms, no matter what our data is and data science, very difficult just to stare at it and get
some, you know, get the information that's hidden from it. We have to tease that knowledge, that
information, that story that the data wants to tell us. We have to get it out. And one of the first
things that we do is just we summarize that data. And that's what the Descriptive Statistics or
Summary Statistics, that's what it's all about. So let's look at the packages that we can use in
this Notebook. We're going to import pandas. So import pandas is PD, the namespace abbreviation.
So we've imported that namespace with all its functionality into Python here. Then we can use a
new one, SciPy, Scientific Python. It just expands on what NumPy can do even further. It's got a module
inside of it called stats. And in stats, you get a bunch of statistical functions. I'm going to say
from SciPy import stats. So I'm not using a namespace abbreviation. So if we want to use the functions
in that stats module, we'll have to say stats dot. From google.colab, we're going to import the drive
function because our spreadsheet is in Google Drive. So specifically to Google Colab and our data being in
our Google Drive, we have to use that drive function. And then as always, I use this percent
load underscore ext, that magic command, just to then print my spreadsheet files, my data frames
out to the screen nice and neatly. Next up, we're going to mount our drives. Remember that's drive dot
mount. And then we pass this argument, which is a piece of string. So it goes inside of quotation marks
forward slash G drive. And I showed you in the notebook before that that folder structure where
this file is saved in this right at the end, this data subfolder there. I've got to put all of that
as a string after the percentage CD change directory. I want to change to that right at the end, that
subfolder, because that's where my spreadsheet files are. So I'm going to run the cell. Remember that's going
to open a new login for me. I have to re-log into my Google Drive, give it the permissions that it
requires, and then copy that security key that it's going to give me to enter in the little block that
is going to appear here. And there you go. It's happened. And I can all, you can also see the
percent LS magic command there. That's just going to show me all the files that are inside of that. And
the one that we're going to use in this section is the client underscore data CSV file. So once again,
I'm using the read underscore CSV file from the pandas package. So PD, my namespace abbreviation,
PD dot read underscore CSV. And because I'm already in that folder, I changed directory to that folder,
I can just reference the CSV file directly. And we're going to assign that to a computer variable
called DF to hold out data frame object. Let's just have a look at the data frame object just to make
sure it imported properly. And there we see, we have a first underscore name, last underscore name,
very important to have those underscores when you design these so that you don't have these illegal
characters. So we can just use the dot notation when we refer to that column. And we see age, number of
children, email address, all of this is fictitious data, job title, home loan, more than one vehicle,
financial literature review, savings, investments, etc. So some economic data pertaining to specific
customers or clients. So let's examine this data frame object as we always do. We're going to call
some of the attributes we've seen before. First one is the shape attribute. And that's going to tell us
how many rows of data we have. So how many data on how many subjects or how many observations,
all those are synonyms for the word rows. So there's a thousand rows along eight, or should say
11 there, 11 variables. So if I want to see the names of those 11 variables, remember that's the
columns attribute. So DF dot columns. And that shows me all the variables that we're dealing with.
This is tidy data. So every row is an observation or subject. Every column is a very specific variable
with a very specific data type. So let's look at those data types, at least what pandas thinks they
are. So first name is an object, which is a string, which is a categorical variable. So it is for last
name. Age is a 64-bit integer. Children, 64-bit integer. Email is an object, in other words, categorical
variable or string. So it is with job title. And then home loan, it sees those as bool values, boolean.
It's true or false values. More than one vehicle, it sees that as true or false values. So let's just
look up there. If we look at those, indeed home loan and more than one vehicle, it was captured as
true or false. So pandas correctly identified those as boolean values. And then you can see the rest day.
So the first thing that we're going to do is we're just going to do counting. And we're going to refer to
these as the frequencies. How many times does one of the sample space elements or each of the sample
space elements in a variable? How many times does it occur? And we've seen the unique method before.
So I'm going to say df.invest. Remember, that gives me back just that column in a pandas series format.
So df.invest, now that I have that column as a series object, I can call the dot unique. So the dot
unique method on that. And that's going to show me the sample space elements inside of that invest
column. So I see it's 3, 1, 2, 5, 4. And remember, it's in the order in which it counted that going
from the top to the bottom. Now, a much more useful one method, I should say, when it comes to
frequencies is the value underscore counts. Value underscore counts. So once again, I'm going to say
df.invest. So it gives me a pandas series object of that column, the invest column. And then I call this
method, value underscore counts. So not only is that going to give me back the unique elements,
you see there, 2, 5, 4, 1, 3, but it gives me the frequency of each of those. So I can see how many
times the value 2 appeared in that column, 5, 4, 1, 3. And again, what it's going to do here, it is just
going to go down the frequency in a descending order. So the sample space element in my invest column,
in my invest variable that occurred most commonly was 2, and that occurred 210 times. So that's a
frequency. We can also do relative frequency. So what we're going to do is just divide by the sum
total. Remember, there was a thousand observations. So just divide by a thousand so that I can get a
proportion or a fraction of each of these. Instead of the absolute value, which is the frequency, the
actual count, that's now going to give me a fraction of the whole, which is what we call a relative
frequency. And all I have to do there is use one of the keyword arguments for the value underscore
counts method. And that's the normalize with a Z. So this is all American spelling. So df.invest,
again, that gives me the invest column in pandas series format. And on that, I call the value counts
method. And I pass the argument normalize. And I set that to true. Remember, true and false,
as far as the actual Python words are concerned, the keywords is uppercase T and uppercase F.
So if I do that, I now see the fractions. And the fractions must always, the proportions must always
sum to 1, 1.0. That's the sum total of them. And if I wanted to do that as a percentage, I'm just,
if you really wanted to do that, I don't really see why. But we can just do some broadcasting where
we multiply each value by a thousand. And once we do that, by a hundred, and then we get percentage,
21%, 20.8%, 20.1%, et cetera. So a little exercise for you there, if you're interested,
just to stop the video and do that for yourself. More interesting though, we're going to do grouped
frequencies. So how do we do grouping? Now grouping occurs by the sample space elements of
a categorical variable. So you take a categorical variable, and it'll have a sample space, some
unique values, and we're just going to group by those. And then we might count the frequency of
some other categorical variable. So what we have here in the first instance, we're going to create
this, what we call a contingency table. And we're going to do home loan, which is a categorical variable,
and more than one vehicle, which is also a categorical variable. And we're going to pass
those two categorical variables in series format, because I'm using df dot, and then the name of
the column, and df dot, the name of the column, that gives me back, remember, a pandas series object.
And I'm going to pass those two elements to the cross tab function. So pd for pandas,
pd dot cross tab, and then pass those. And look what it's going to do.
It gives me this nice little table. So it says home loan here, false and true. And then the second
one is going to be across the top there. So that's more than one vehicle, false and true. So if we look
at no home loan, and not more than one vehicle, there were 268. And a home loan with not more than
one vehicle, 239. And then the true values, true for more than one vehicle and true and false for home
loan. And then right at the bottom, the 246, more than one vehicle and with a home loan. So that's a
two by two contingency table, two rows, two columns. And that's what you can see there with these four
values, then two rows and two columns. Now measures of central tendency, we've talked about how to count
things frequency and relative frequency. And as the course goes on, we're going to see many more
examples of those. So remember, this is just an introduction. So let's do some point estimates or
central tendency. And what we do there is we take a variable and all its values, and we calculate a
single value that is representative of the whole. And that's what a mean or an average is, taking a bunch
of numerical variable, variable, variable values, and I express a single value that is representative
of all of them. And the first one is the arithmetic mean, or the average. And remember how you do that,
you just sum up all the values and divide by how many there are. So let's see what the average or mean
age was of all the customers. So I can just say df.age. So that df.age is going to just give me
just that column as a panda series object. And then I call the mean method on that, not passing any
arguments, and that's going to give me the mean age in our data set, and that's 44.739 years.
Now, what is the mean value of customer savings? Now remember, I could just say df.savings, but I'm just
reminding you here, if you do have illegal characters like spaces in your column names, or your variable
names, you can always put them inside of quotation marks, inside of square brackets. So df, and then
savings, and then dot mean, and that's going to print out for us 30,000, 30,136.49, as far as the average
is concerned. Now, let's do something much more interesting. I want to know the age, the average
age of only the customers that have a home loan. And remember, we do that with conditionals. And I'm
using a bit of a shortcut here, and then I'm not using dot loc or dot iloc. I'm just going for it
directly df. And then all the rows that I'm interested in, let's go down the home loan column and look for
all the values set to true. So equals equals true. So that's the only ones that are going to be selected.
From that, give me the age column, and of that age column, give me the mean. So that's how we would
construct that. That's going to give me the mean, or the average age, only of those customers that have
a home loan. So we can also do the mean average, or the mean age of the customers without a home loan,
so that equals equals false, as far as that conditional is concerned. And then instead of putting age like
that, just showing you a little bit of a variation, I can just say dot age, because age contains no
illegal characters. Something a bit more interesting, what if I wanted to do those two steps all in one?
And here is a really a method that you're going to use a lot, the group by method. So I'm saying df.groupby,
and then inside of parentheses, because this is a method, I'm going to pass one of my columns inside
of quotation marks this time. So home underscore loan, group by whatever you find in the home loan,
and then the age, and then the mean. So let me show you what it looks like, and that'll make a little
bit more sense. So in the home loan, it found two sample space elements, false and true, and then it gives
me the mean age of each of those. So the customers without a home loan, the average age was 45, and
those with a home loan, the average age was 44. So that group by becomes, it's going to become something
very useful to us, and we're going to use it a lot. Remember, there's also the geometric mean,
and that's calculated a little bit differently, and that comes from the stats library. So I can't call
that as a method on a panda series object. So I'm going to say stats.g mean, for geometric mean,
and then I'm going to pass my panda series, df.age, and that's going to give me the geometric mean.
So let's just multiply all of those together, and then take the nth root, where n is the number of
observations that we do have. Median, now what the median does, it puts all our numerical variables
in ascending or descending order. And then, if it is an odd number of values, it's going to take the
one right in the middle, for which half of the values are less than, and half are more than.
And if it's an even number, it's going to take the middle two, and just take their average.
And that works very well when our data is very skewed. When data is very skewed, we have
outliers. We're always going to pull that average towards the outliers, and that value is now no longer a
proper representative. Remember, that's what we're trying to do with point estimates. One value,
that's a proper representative of all the values. And in that instance, the mean might not be so. So
we have the median. So let's have a look at this. What is the median saving of customers older than 50?
Again, we're just going to use some conditional there, df, and then df.age greater than 50. So it's
going to go row by row in that age column. And then from that, we want the savings column. So I could
also just have said dot savings dot median. And anyway, dot median will calculate the median for
me. So let's look at the median savings of the participants, or the subjects, or the customers
that were older than 50 years of age. Now remember how we put those conditionals together as and and
or. So what we're looking at here is where the home loan is true, and the more than one vehicle is
true as well. Give me the median of the savings. So referring you back to the previous notebook,
where we looked at these, we just concatenate them together and and ors. So the next one,
we're going to say either the home loan is true, or the more than one vehicle is true. And then give me
the savings column and the median of that savings column. Excellent. And there's a little exercise
and solution set for you. Now we come to the mode. Now the mode is where we don't have continuous
numerical variables. Remember, continuous numerical variable can have many decimal places. We just
truncate those. Maybe the apparatus that we use in the lab can only give us until a certain
number of decimal places. But in actual fact, the decimal places go far beyond that.
And we don't do a mode for that. Mode means the value that occurs most commonly. So yeah,
we're really going to go for either discrete data or categorical data, anything that has a very fixed
sample space elements. And if there's one sample space element that occurs most commonly, that's going
to be the mode. Sometimes two of them are equally have the equal highest frequency. And then we'll call
that a bimodal variable. And then you also get trimodal and even more multimodal. Anyway, let's
look at the number of children. So I'm going to say DF dot children dot value counts. And remember,
it's going to do that in descending order. So it sees three children that was the most common. So three,
you can think, well, it is really numerical, but you can't have a fraction of a child.
So it doesn't really help to express the average number of children. Now you see that all along,
but really that is a very difficult concept because what is 2.18 of a child? Very difficult. So I tend
to use any kind of discrete data, rather use the mode. Next up, we're going to look at measures of
dispersion. Now that we know that we can take a data set, a variable, and we can calculate a single
value that's represented of the whole, we also need to find out how spread out that data is. And
those are measures of dispersion. So the first method of dispersion that we're going to talk about is the
standard deviation and the variance. And they actually refer to the same thing, and you actually
calculate the variance. First, take the square root of the variance to get the standard deviation.
So what is the standard deviation? Well, that's the average difference between each value and the
mean for that variable. So I just take all the differences between each value, value number one
and its difference with the mean, and I sum all of those up and divide by how many there are. So
that's the average. The problem is some values are less than the mean and some are more than the mean.
So we're going to get negatives and positives, and we sum all of those up, you're going to get to
zero. So the standard deviation is zero, and that doesn't really help us. That's the way that the
mean is calculated. So what we do is we square each difference. And remember, when you square a negative
value, it becomes a positive value. So all our values will now be squared. And if you add all
those squares, all those positive differences, and divide by how many there are, you're going to get
to the variance. And what you'll have to do is to take the square root of that to get to the standard
deviation. But that's exactly what the standard deviation is, the average difference between each
value and the mean for that specific variable. And it's very easy to do, and I'm just showing you one
example here. We're going to just group by the home loan, so all the sample space elements of this
categorical variable, we're going to group by that, then take the age column and take the standard
deviation, and that's the std method. And if you can see up here, there's also a dot var method there.
And so it's either dot std or dot var for standard deviation and variance. And you'll see the two
standard deviations there. The range is just the difference between the minimum and maximum value.
And remember how we get min and max? There we go, df dot age. So that gives me our
our column there of only that column, the values for the age column, or in series format. And then
I call the dot min method on that. So the minimum age is 25. As we can see then, the maximum age is
65. And the range is going to be the difference between the minimum and the maximum. So I'm going
to take the maximum minus the minimum, and that gives me the range 40. So when it comes to age, age of
animals, age of organisms, age of human beings, I like to express the range. Many times in the reports,
you'll see the standard deviation, but that doesn't really tell me that much. I want to know what the
minimum age and maximum age was, because I want to infer the results of your lab to my lab. And it's
kind of neat to know the youngest and the oldest age. That brings us to the quantiles. Now, remember
the median that just chopped up all my values new continuous numerical variables in half. I put them
in ascending or descending order, chose a value for which half were less than and half were more than
that value. But I needn't only chop it up into into halves, I can also chop it up into quarters.
And for that, we'll get the first quartile, the second quartile and the third quartile.
So the first quartile is going to be a value for which a quarter of my values are less than and
three quarters are more than. The second quartile is going to be the median in the middle. The third
quartile, three quarters of the values are going to be less than and one quarter more than. And just
for completion's sake, we actually also have the zeroth quartile and the fourth quartile. That'll
be the minimum and the maximum values. And the way that we do that is with the quantile. You can see
there the quantile method. So for age again, that's df.age, the quantile, and then inside of a Python
list, I'm going to pass all these values 0.25, 0.5, 0.75. So we actually put that in fractions. And that's
going to give me those values. So as far as the age is concerned, we see that at 0.25. So we know
that a quarter of customers were younger than 35, half were younger than 45, and three quarters were
less than 55. Or you can obviously see it in the opposite direction. A quarter was older than 55.
But I can express any of those fractions. So here we just go at the 95th. Now we call it percentiles.
So I'm going to say df. This time, I'm going to group by the true or falses in the home loan,
take the age column, and give me the 95th percentile. That means 95% of the participants,
and we see 63 for both. So both those with and without a home loan, 95% were younger than 63. Or we
could say 5% were older than 63. And interquartile ranges, the last one I want to talk to you about,
and that becomes very important when we look for outliers in our data, or when we want to create
box and whisker plots. And that's coming up in the next notebook. Very exciting notebook,
visualizing data. And all I'm going to do is I'm going to subtract the value of the first quartile
from the value of the third quartile. So it's that difference between the third quartile and the
first quartile that gives us the interquartile range. So I'm just going to say df.h.quantile
0.75 minus df.h.quantile 0.25. And that gives me the interquartile range of 21. So there you go,
just starting to tease out that story that the data is trying to tell us by summarizing it in some way,
either calculating a single value that's representative of the whole, or then giving
us some idea of how spread out the values are.

So in this notebook we're going to talk about linear modeling. There really is a lot to unpack.
So notebook number 11, linear modeling. And what we're going to discuss here is a little bit about
a new distribution called the F distribution. As you can see there, linear modeling using the F
distribution. But we're going to stick to this idea of resampling so that we can understand what
is going on. So we're going to talk a little bit about linear regression. We might revisit the t-test.
And also analysis of variance, ANOVA. Something that is very useful when it comes to data analysis and
data science. So let's look at the packages that we're going to use. So numpy, scipy. So from scipy
we're going to import stats. And also from scipy we're going to import the special module. And then
we have pandas there as usual. And then our plotting libraries as per usual. Now here's something brand
new. The patsy package. So from patsy we're going to import the dematrices, the dematrices function.
And then from the statsmodels package we're going to import the API model. So statsmodels.api,
we're going to import that as sm. And what we're going to talk about now that we've imported all of
this is we're going to start off with by talking about correlation. So previously we took a categorical
variable and we looked at its sample space element and from that we could create different groups.
And then we took a numerical variable and we compared it between those two groups.
Here we're doing something a bit different though. So we're taking each observation in a data set
and for any one of those we only look at the numerical variables. And we're going to compare
those numerical variables to each other. So if it's just two numerical variables for each observation
we'll have a pair of values and we're going to compare the pair of values. So yeah, let's start
with just two. And if we do, one of them we're going to term the independent variable and the other one
we're going to term the dependent variable. And our aim is, with these linear models, to predict
eventually a value for the dependent variable given only the independent variable. And that becomes very
useful because it might be that the dependent variable is very difficult or expensive to get
hold of that data. And we can model what it should be based on what we do have. And that's a very useful
thing to be able to do. We're going to start off with by just talking about correlation. And I think most
people have an intuitive understanding of what that term means. So we're going to create two numerical variables
and we're going to do that in the following way. We're going to seed the seed of random number
generator, as you can see there. This time I'm using the integer seven. And now I'm going to create
an array of values and I'm going to assign that to the computer variable independent. And the way that
we're going to do that is right on the outside I have np.round and right at the end it has comma one. So
it's just I'm just going to have one decimal place. But what's inside of here is more interesting. So
I'm using the random.uniform function. A low of 80 and a high of 100. So that's my interval 80 to 100.
Give me a uniform distribution. So every value between 80 and 100 has an equal likelihood of
being chosen. And I want 50 of those. And then to each of these values, I have those 50 now. I'm just
going to add a little bit of random noise. And that is how I'm going to get my dependent variable.
So np.round again, I just want one decimal place. So I take every independent variable,
value, and to that I add a little bit of random noise. And that random noise has a mean of zero and
a standard deviation of five. And there's 50 of them. So I'm just adding a little bit of random noise
to each of the independent values. And I think you'll see it's quite easy then to calculate,
you know, what the dependent should be. It just adds a little bit of noise. In case not,
let's have a look at what the data looks like. There we have all our independent values on the x-axis
and our dependent variable values on the y-axis. So this is a scatter plot. So I'm using graph objects.
So go.figure and the data is going to be a scatter plot. Go.scatter on the x-axis is my independent
values and my y-axis is dependent variable values. So I must have an equal length of both. Each
each pair there, I must have both values for each pair. I can't have more independent than dependent
variables. They have to be pairs. And that's what it looks like. So you can clearly see I have between
80 and 100 for my independent values. And then my dependent variables, we just added a little bit
of random noise, a little bit of jitter up and down. And as the independent value increases,
so does the dependent variable, because we just added a little bit of noise to that.
Now, we all know by now what the variance is. The variance is we just take every value in a variable,
numerical variable. We subtract from that the mean for that variable. We square that
so that we have positive values. We add up all these squares and we divide by 1 minus the sum total
of values that we do have. Why n minus 1? Because we're only dealing with a sample. If you're dealing
with a whole population, it's just divided by the number of subjects in the population. But if it's a sample,
it's always n minus 1. So that's our variance. So here you see in equation 1, just the variance. We're
just going to run through all of them. So it's this idea of the squared average of the difference
between each value and the mean for that variable. So let's just calculate that for our case here.
And we're going to do the variance just of our independent variable. And we do that according to
equation 1 here. So what we're going to do is take our independent minus the mean for our independent.
And that's inside of a set of parentheses. That's subtraction. And then we square that.
And then we sum over all of those with the numpy.sum function. And then we divide by the len. Remember
that tells us how many elements we have in that set. And we just subtract 1 from that. So that
obviously gives us now the sample variance. 27.3082. Of course, why did we do all of that? We can just use the
numpy.var function. So let's do that. numpy.var. And I pass all my values to it, my numpy array of
values. But I've got to set ddof equal to 1. That's our degrees of freedom. We set that to 1. And
that means we have this one sample. We subtract 1 from the sample size, n minus 1. And that gives us the
sample variance. If we don't put that, we're going to get a population variance. So that just divides
by n. So, and then we see 27.3082, exactly what we had before. So let's look at the variance for the
dependent variable. That's numpy.var, pass the dependent, comma, ddof equals 1. And we get 51.599.
So now we're going to build this on this idea of the variance that we have for each of our variables.
And we're going to look at how they co-vary. So somehow we combine this idea of how they vary.
And then in equation two, you see the covariance for two variables. So what we do is the following.
We take each and every value. Remember, we have an equal number of elements for x and for y,
for both our sets of values. So what we do is for the 1, we just subtract from the mean,
and then for its pair, for its friend in the same subject, we do the multiply,
we do the subtraction again. And instead of squaring, we multiply these two by each other.
And then we sum over all of them. And then we divide by the sum total.
And there's a function for that. Fortunately, it's just mp.cov covariance. And we just pass
our two x and y's there, our independent variable, our dependent variable, and we get this matrix back.
And two of the values you'll see are familiar. There's the 27.3082 and the 51.59557551.
That is the variance in each one of these independently. So that 27.3082, that was the
variance in our independent variable. And the 51.6 there, 51.59557551, that was the variance in the
dependent variable. So on that main diagonal of this matrix across, we see the same value. 26.88,
26.88. That's the covariance. So that's how they vary. That's the variance in the two of these
variables combined, we can say. 26. Let's make it 26.9. So what we have here is two rows and two columns.
So we can use indexing. What we're after is either one of these 26s. So this top one is in row 0
and column 1 by them being zero indexed. So I can just use indexing there 0, 1. And if I do that
on the covariance, that just gives me the 26.88. That's the covariance that I'm after.
So we have a strong understanding now of what the covariance is, and how does that help us? Well,
it helps us to express how well these two variables are correlated. Does one change as the other one
changes? Or is it just a complete random mess? And for that, we can express this correlation, the
strength of the linear connection or relationship between these numerical variables. And we can
express that using this covariance as what we call the Pearson correlation coefficient. You can see that
in bold here, Pearson correlation coefficient. And we have a little symbol for that. That's a lowercase r.
And we see the equation for it there in three, in equation three. So we take this covariance,
and we just divide by the product. In other words, the multiplication of the two standard deviations.
So standard deviation of the one times standard deviation of the other one. It's just that ratio
between the correlation between, I should say the covariance between them, divided by the product of
their standard deviations. So let's do that in a little equation. So numpy dot covariance, dependent
and independent. But we only want one of those four values that we're going to get in the zeroth
column and zeroth row first column. And we divide by the standard deviation of the independent variable,
and the standard deviation of the dependent variable. But because it's a sample, we remember to do that
ddof equals 1. And that gives us the Pearson correlation coefficient. Fortunately for us,
we didn't have to do that. There is a function for that in the stats module of scipy, and it's
stats.pearsonR. And all we have to do is just to pass the two arrays to it. And it's going to return two
values for us, both the r, this correlation coefficient, and the probability of that, a p-value for that. So let's
do that. r, p, because we get two values back, we can state two computer variables there. And what we're
printing out is just the correlation coefficient 0.716. And that's exactly when we did our calculation.
That's exactly what this function does, 0.716. But it also provides this p-value. And if you do print
out the p-value, let's do that here, you'll see it's tiny. It's one of those round-off errors. No, it's
0,805. So 0.0000000005. That's just a round-off error. That's just for zero.
So it was very unlikely. It's very unlikely to find this r-value.
Now the correlation coefficient is on an interval. It goes from, because it's a ratio, it goes from
negative one to positive one. So if it's negative one, that's an absolute perfect negative correlation.
As the one increases, the other one decreases in step. And positive one is a positive correlation.
As the independent increases, so does the dependent in step. There's none of this little jitter, none of
this noise to the data. Now we have to talk about uncertainty again. What, you know, how certain are we,
because we're just working with a sample of this r-value of ls. So what we're going to do here is
a null hypothesis. And we're going to state that, well, you know, it doesn't matter how these values
are paired up. I can reassign them and there should be no difference. So that's one way that we can go
about it. And that's exactly what we're going to do here. So I'm create this empty list, r underscore vales
underscore zero. And then I'm going to do 5,000 shuffles here. We're not going to use the shuffle function,
because that, remember, that changes the original. So we're just going to do a random choice.
And how we're going to do the random choice is over here. So numpy dot random dot choice from independent,
but we set the size to 50 and replace equals false. So we get the exact same ones back. They're just
going to be in different order. So what I'm saying is these pairs now don't belong to the same subjects
anymore. I'm shuffling them around. And under the null hypothesis, that should make no difference.
So let's do that. And we do that 5,000 times over. And every time we append the correlation coefficient.
So there's my numpy dot correlation coefficient. And at the end, I'm taking an index zero one. So
I just get that one value back. And there we go. We've done that 5,000 times. And now we have a
distribution of possible r values, correlation coefficients. So let's do that and plot that out.
And the one that we found. So there's a distribution of all the correlation coefficients. You can see all of
them here, going from negative, towards negative one on this side, towards positive one on this side.
And the one we found was way out here. So again, we can just express this idea of how many of them
were our value and more. And that's exactly what we do here. So let's just say the r value, just to
remind us it was 0.716. So all we say asking here is, I want the, in that array. So remember r underscore
VALS underscore zero is a numpy, is a Python list. And we pass that to the array function in numpy.
So it becomes a numpy array. And I want to know how many of our simulated ones were greater than r,
our r that we found. And then we divide by how many of these simulations we did. And we did 5,000 of
them. So I can get this fraction, the proportion of ones in the simulation of r values that was greater
than ours. And remember our p-value was zero. So let's see our simulation zero. There was none of
those reshuffles of the pairs, the values and the pairs, that gave us the r value that we got. So
that was very unlikely to have been found. And we see that very small p-value.
So let's get to the uncertainty then in our correlation coefficient.
And what we're going to do here is we're going to do bootstrap resampling, of course.
So we've got an r value. We know that it was an unlikely one to have been found. Now,
how certain are we about those r values? Can we set some confidence intervals around this r value?
So we're going to use the numpy.stack function. And we're going to pass as a Python list the
independent variables and the dependent variables. And we're going to set the axis to 1.
And I'm assigning that to the data function. So let me show you what that does. Very neatly,
it's going to create this array that is a list of lists. So each subject, it has the independent
variable value and the dependent variable value for that subject, for the next one, for the next one,
for the next one. So I've just stacked them together. So they're in a neat, nice stack. Of course,
if I had this as a pandas data frame, you know, it would be much easier just to extract those two
columns. But because they turn into separate arrays, I'm using this stack function. So now that I have
them, I can actually do resampling. So bootstrap resampling, that means with replacement. So this is
what we're going to do here. So I'm creating this computer variable r underscore vales underscore
boot. And I'm using list comprehension to create 2000 resamples. Remember with bootstrap resampling,
though, each one must have the equal sample size as we had before. And that's why we set the size to 50.
And the way that we're going to do this is we're going to say random.randint. So I'm taking a random
integer, the shape of my data. Remember that was 50 long and two wide, two columns. And I'm asking for
the zeroth index. So that's the 50. So in a random, you take a random integer from 0 to 50. In other words,
0 to 49. That's 1 to 50. And I want the size of 50 of them. And we've got this row var equals false,
as far as this as this randint function is concerned, as suppose as this correlation
coefficient function is concerned. And you can see the row var equals true as the default. But when we
scroll down, we see row var is true by default. And that's not what we want. We want false, because
we have here each column represents a variable and each row is an observation. And that's why we say
row var equals false there. So I take a random integer from 0 to 49. In other words, I'm looking for
between rows 1 and 50. But in Python speak, Python being 0 index, it starts at 0. I want 50 of them.
And I want to calculate the correlation coefficient of each of these. So remember now with this bootstrapping,
I'm going to have repeats in each one of my 2000. So definitely going to be repeats.
So let's do the list comprehension of that. Now let's set a confidence level of 95%.
Remember, if you want a good reminder, just go to the previous notebook.
So we're interested in the 2.5 percentile and the 97.5 percentile. So as an index,
which one of those values will be. Remember how to calculate k. So we have, we want the 50th value.
And we want the 1950th value. Of course, zero index. So we're actually looking for 49 and 1949.
Let's just remind ourselves of what the correlation coefficient was. It was 0.716.
So what we're going to do is we sort the values. Remember, we have to sort them. And then we take the
49th indexed value. And that is going to be our lower bound for our r. And we see that's 0.58.
Let's look for the upper bound, the index 1949, once we've sorted them. And there we go. We see
the upper value is 0.82. So we have a lower bound and an upper bound for 95% confidence interval
around our correlation coefficient. So let's plot that out. And by the way,
we can state it there. Our correlation coefficient was 0.72. 95% confidence interval 0.58 to 0.02.
And the p value is very small. So mostly we wouldn't say it's 0. We just say it's less than 0.01.
So there we go. We see our plot. There's our correlation coefficient. We see this distribution
of possible r values from bootstrap resampling. And we see our bounds, our lower bound and our upper bound
for 95% confidence intervals. Absolutely great. So there's various ways to tackle this problem.
And this is one way. And there's a nice paper that explains this using bootstrap approach to
correlation analysis. So in this next section, I'm going to show you a brand new distribution. You know
what a normal distribution looks like. Nice bell-shaped curve. T distribution, which is very close,
that we use because we don't know what the population looks like. And now we're going to just have a look.
I'm just going to show you the f distribution. So we're not going to get into the nitty-gritty of
the f distribution, but it is a very useful distribution, especially when it comes to
linear modeling. So I'll show you what it looks like. You'll see one or two equations. But then,
you know, once you know that it exists, that's all we need to know in this course,
we'll start using it. So there we go. We see the f distribution in equation 4. There we see this whole
thing and a very impressive looking thing. It makes use of all sorts of weird symbols. We see a beta,
a beta function there. What we're interested in, though, is that d subscript 1 and d subscript 2.
So it comes in the numerator a couple of times and then in the numerator denominator.
And those are two degrees of freedom. So there's actually these two degrees of freedom
in this function. And that's the only thing we're really going to be concerned about. What I've done
for you here is just creating my own Python function, f underscore pdf. And it takes the
arguments f. A d1 is set by default to 1 and by d2 to 19. So if we use this function and we don't pass
values for d1 and d2, the defaults are going to be 1 and 19. And it takes this special dot beta,
so the beta function. So it's going to take all of that and I've created that little function. And
the reason why I do that is I just want to show you what this distribution looks like. And it really
just depends on d1 and d2. Those are the important ones. And we see different d1s and d2s. There's a
d1 of 1 and a d of 10 and a d2 of 1, a d1 of 5 and a d2 of 2, a d1 of 29 and a d2 of 18. So you'll see
what sort of what they look like. They start off with, you know, bunched up here on the small side
with these long tails, but they can also start starting to go towards normal distributions.
And what we do is we're going to calculate an f statistic this time. And that f statistic we plot
somewhere on our graph. And so here we have one where we have a d1 of 1 and a d2 of 10 and an f
statistic here of 3.5. And how we're going to calculate this proportion of how likely was it
to have found the results that we have is from this value towards positive infinity. So this tiny
little area under the curve there. And of course, it always comes from the left hand side. So what
we're going to do is 1 minus all of this. That tells us how much is there. But remember,
when we do our sampling techniques, we just have to ask, in all of our simulated recounts, for instance,
you know, how many were from the outwards under this null hypothesis of ours. So let's see what
that would have been. I'm using stats.f.cdf. Don't worry about that. We see that that would have been a
p-value of 0.09. So that's the f distribution. Now, you know that it exists now. And I suppose that's
all that's important for us in this course. So we've spoken about correlation. We know now how,
you know, one changes, one variable changes as the other one. But now we want to use this idea in
modeling. We want to do some modeling on this. So let's have a look at our data again. I've got my
independent variable, my dependent variable. So what I want to do is I want to create this model
that given any independent value, I can use this calculation to give me some dependent variable
value. That's what modeling is all about. So let's put this in a data frame. So I'm just using,
I'm just using the dictionary there. I've got two columns, independent and dependent. And I'm just
adding a third one called group. And I'm just going to repeat c and e. So that some are going to be c and
some are going to be e. Starts off with a bunch of c's and then the e's. And that's what we have.
Our data frame, they're independent, dependent, and then a group variable.
Now, as I said, I want to build this model that given any independent value, it's going to predict what
the dependent variable should be. And for that, we need this thing called design matrices, because
we're going to use the stats models. And these design matrices are just perfect for use inside of
stats models. So here they are. There's the D matrices function. Remember, we imported that from
Patsy. And it's going to return two values for us. It's going to return this Y value. We're just going to
call it Y. And then we're going to call the second one X. But that Y is just going to be our dependent
variable. As simple as that. But X is going to be this matrix. And it contains the independent
variable, but it contains a second column as well. So let's unpack this. I'm using the D matrices function.
And then I pass a little formula. And there's my formula. I'm saying, I want this matrices designed such
that the dependent variable. I'm trying to calculate that given the independent variable.
So it's dependent tilde independent. So of course, you're going to work with data. And these columns
are going to be named differently. But this is really, these names are really illustrative of what
we're trying to do here with this formula. I'm trying to predict that value in the dependent variable,
given the independent variable. And the data comes from the DF data frame. And that's going to give me
these two design matrices. So let's have a look at them. Let's have a look at Y. And Y is now a design
matrix data type. But it's just my column of independent, my column of dependent variables. So
you can see they're dependent. And if we look at X, it's now a design matrix. But this is what it looks
like. I still have my column of independent values, but it's added this intercept column of all ones.
And that's all about using linear algebra behind the scenes to do these calculations for us.
But I just want to show you that's nothing other really than still the dependent and still the
independent columns. And the D matrices just makes these design matrices. So the linear algebra behind
the scenes are very easy. So let's just think about how linear regression works. So in this equation five,
it's a beautiful illustration of what's going on here. I have all my dependent variable values there.
73, 86.6, 90.7, they're all there. And I'm saying I can actually calculate each of those values. Now
they're represented here as something that we call a column vector. Don't worry, it's just the shorthand
notation rather than writing out 50 of these equations. It's just very shorthand. It says this,
if I know these two unknowns, and we usually call them beta sub zero and beta sub one. If I know all of
them, I can do this calculation. In other words, that 73 is going to equal to beta sub zero times one.
So I can take a scalar here. That's a single value multiplied by a made by a vector. That just means
I multiply each and every value by this beta zero plus if I knew this beta one, what that value was
plus 85.1 plus some error, I'm going to get 73. So beta sub zero plus beta sub one times 85.1 plus some
error is going to equal 73. And 86.6 is going to be beta sub zero times one plus beta sub one times 95.6
plus error two. That's going to be exactly 86.8. And that's the way linear regression works.
Now, of course, this error, we have to throw that away because in a model, all we're trying to do is
to get values for beta sub zero and beta sub one. And if we know those two, and if I do beta sub zero times
one plus beta sub one times 85.1, I'm going to get very close to 73, not really 73, but close to it.
And then the next one, I'm going to get very close to 86.6. So instead of this vector here of actual
values, I'm going to have a vector of predicted values. So all we're trying to do is just this
approximation. So for this first one, it's going to be approximately 73. I just need to find these two
unknowns. And it's two unknowns, because I have one independent variable. If I had two independent
variables, it's going to be three unknowns, because there's always this beta sub zero. Now think back
to school, we had this idea of y equals mx plus c, just a straight line, the slope plus an intercept.
And the intercept is when x equals zero. And if we look at this line here, this is what we're trying to
predict. So for any given independent variable value, if I go up to this red dot there, I know
exactly what the actual independent variable is. But I'm going to build this blue line, this model.
And it says, given the independent variable, I go up to my model, go to the left. And if I read that off
on this y-axis, that gives me the predicted value. And this is what this model does. But what we need is,
you know, where do we draw that line? You know, does it be a bit more slanted? Does it go up or down?
You know, we want the best possible one. And the best possible one is the one that makes the least
amount of errors. And that's exactly what the error is. It's this difference between what the model
would predict and what the actual value is. So if we look at this blue line, you know,
it under-predicts some and it over-predicts some. So it's just that difference there between those two.
From what the prediction is to what the actual one is, that difference. So everywhere we go,
it's just got to be that little difference. And we call those errors the residuals.
Those are called the residuals. And we want to minimize these residuals. And the way that we
do that is to look at the least squares. So what I'm just trying to represent here graphically is,
there's an error there, and we just square that error. Because remember, some are going to be less
than, some are going to be more than. And if we add all these errors, we're going to get to zero.
That's not what we want. So we square everything to make it positive. And that's what we do. So all
we really want, and remember, squares is going to give you a square. So what we want is these little
squares, we want the minimum size for all of them. We want to minimize this error that we make. And that's
what linear regression is all about, minimizing those. Now, there are various techniques to go
about it. We can do either gradient descent, or we can use something like ordinary least squares here.
And this is where this A and A comes in. Those are our matrices, and the matrix has the one in it.
So it's just a way that we're now going to get a vector from this. And that will be a vector of two
elements, a value for beta sub 0 and a value for beta sub 1. Don't worry about that. In case you're
interested, you can go look up that. Or we can use, as I said, this method of gradient descent.
It doesn't, doesn't matter. What we are interested in here is this thing called r squared. So this is
an uppercase r squared. And that's the coefficient of determination. And that tells us how well our
model does. That's the one thing we're interested in. Just as we had the lowercase r, Pearson's
correlation coefficient to tell us how, how good that correlation is. Here, we expressing how good
our model is. And that's the coefficient of determination. So in this course, I'm not going
to, it's not about this ordinary least squares equation that you see in six or how to use gradient
descent. We're just going to use a line of code and it's going to give us our model. But we need to
express, you know, how good that model is. That's what we're interested in here. And we express that by
the coefficient of determination, r squared. And r squared is simply this equation here.
It is the, this variance in the mean model residuals minus the variance in the best model
residuals divided by the mean of the mean model residuals. So it's this, this fraction, this ratio.
So what am I talking about? So let's just, let's just go do this.
The way that this works is we want to compare our best model to what we call a null model,
the worst possible model. So let's come up with a worst possible model.
A model that will do the following. It will say, give me any independent variable
value. And my prediction is just going to be the mean of the dependent variable. So give me any
independent variable value. The, the, its prediction is always just going to be the mean. So let's
calculate the mean of the dependent variable. There we have it. It's 90.46. So it says, it doesn't
matter what you give me, what independent variable value you give me, I'm always going to just predict
the mean of that dependent variable. So let's look at what a model like this looks like. Is this a flat
line? So no matter what value you give me, I'm always going to predict this red line.
90 point, what was it? 90.456. That's all we're going to do. Now that's going to be a bunch of
errors. That is definitely not the best line, but that's our null model line, our worst line. And we're
just going to see our model that we come up with the best possible one, how this compares as a ratio
to this one. So if we look at this very first value, it had, I think, yeah, what is that?
89 as far as what it was, but our model predicts this 90.456. So, you know, we get that little error
term there. And so we're going to go across all of these and we're going to get all these error terms.
So let's look at the variance of our mean model. Because if we go all the way up here, remember,
we want this variance of the mean model residuals and the best model residuals. Now, because we're
working with the mean, the residuals, those errors that we make, that's just going to be the variance
of that variable anyway. So my var of my mean model is just going to be the variance of the dependent
variable, how each of them is different from the mean, just subtracting everyone from the mean,
squaring them. And that's our variance. So our variance in the mean model, there we go. It is 50.56.
Now we've got to get our best model. And as I said, this course is not about how to calculate this.
We're just going to use an equation. And that's SM. Remember from StatsModels. We imported StatsModels.API
as SM. It has a very nice function, OLS, Ordinary Least Squares. And all I have to do is to pass Y and X.
Remember, Y was all my dependent variable values, and X was this design matrix with my dependent variable values,
but another column of all ones. So I pass that, Y, X, and then I use the .fit model.
And I'm assigning that to a computer variable linear underscore model. And it's as simple as that,
that one line of code is going to give me the best model. No problems whatsoever.
Let's look at a summary of this linear model. And there we go. There's quite a bit to unpack there,
but the interesting part is right here where it says coefficients. Right there, I have my two
coefficients. My intercept, a coefficient, and my independent coefficient. So the top one is my beta
sub zero, and my bottom one is my beta sub one. The y-intercept and the slope. That's all we're
interested in here. And we can see all sorts of other things. We can see our F statistic there. Remember,
our F statistic is right there, and the probability of our F statistic. So there's our F statistic 50.53.
And the p-value, the probability of having found this one, is basically zero. So it's a very, very good
model that we have there. So let's plot this out. Don't worry about this code. I'm just doing this
to have some X and Y values so that I can plot out a line. And that line uses this idea of our beta
sub zero. And that's our y-intercept and our slope beta sub one. And that is the best model.
So if I look at all the residuals, all these differences, they are going to be, if I square
all of them, add all of them, that's going to be at a minimum. This is the best model.
So I can look at all these residuals, the little errors that it makes, by just using the
resid attribute. So linear model dot resid. And I see all the errors there. Some are negative,
some are positive. And I can actually see what the predictions would be if I just use the dot predict
method. And I pass all my X values to it. Remember, that's my design matrix. If I pass that to it,
I see all the predictions. And I can, you know, look at my predictions. And I can look at what the actual
values were, so that we can see that comparison. What we're interested in though, to calculate R squared,
is this idea of the variance of these residuals, the variance of these minimum errors that we made.
Let's look at their variance. And there we go. That's 24. And remember now how to calculate R squared?
Well, that's just going to be the variance of the mean model, minus the variance of the best model,
divided by the variance of the mean model. And then we're going to get R squared. And that's 0.512.
And that's exactly what the model summary showed us. It's always going to be on this interval of 0 to 1.
We do get an adjusted R squared. That's something slightly different. And it penalizes for having
too many variables in it. But in essence, it goes from 0 to 1. And 0 being, well, it's no different
from the bad mean model, up to 1.0. And that's a perfect model. So our coefficient of determination,
R squared is 0.513. We've calculated ourselves. We understand what's going on here. And how do we
interpret that? Well, we say our model, given this independent variable values, explains 51.3%
of the variance in the dependent variable value, because we can see that from this little equation
that we used here for R squared. So whatever the R squared value comes out, that is the percentage,
just multiplied by 100. So 0.51 times 100 is 51.3 if we round it off there. Our model, given values for
our independent variable, explains 51.3% of the variance in the dependent variable. So as simple as
that. And I just want to show you the equation here for the F statistic. I just put this in here for
intersect, not what the course is about. That's how we calculate the F statistic. So it's the variance
of the mean model residuals minus the variance in the best model residuals divided by P best model
minus P mean model. And that P stands for how many parameters. How many parameters went into the best
model? Well, that was two, remember. I had beta sub zero and beta sub one. How many went into the mean
model? Well, just one. This is summing up all of them and dividing by how many there are. There's
just one there. So we can do that. I'm saying P best equals two and P mean equals one. Just one
parameter went into the mean model, but two parameters go into the best model. Why only one?
Well, it only has a y-intercept. There's no slope. It's just a straight line. And the number of
cases we have is just the length of either one of them. It's just the sample size, because
in the denominator of this is another little ratio. That's the variance of the mean model residuals
divided by the sample size minus the number of parameters in the best model.
So in case you're wondering, that's how the F statistic is calculated. So in our instance,
let's do that. Oh, let's save that first. Those computer variables there. And then let's calculate
an F statistic. There's our F statistic, exactly what our value said. And then to calculate a P
value, it's stats.f.cdf. I pass the F value, the P best minus P mean. So I've got to have that
difference. And then the N minus P best. So the two denominators, the denominator that was in the
numerator and the denominator that was in the denominator, we pass those and we're going to
get a P value. And you can see it's the same. It's almost zero. So it really is as simple as that,
but I want you to have that intuitive understanding of what this linear model is. It's this idea of
given independent values, we predict the dependent values. And as I said, we use this in circumstances
where it's very difficult to get the dependent variable values. And it might just be much more
useful for us to have this model because now with our model, you know, we can look at other scenarios.
We can do a lot. And this is just a very simple linear regression. Of course, under this term of
generalized linear models, there are many models that we can build, much more sophisticated models.
And that can really help us out. And of course, in this day and age of artificial intelligence,
machine learning, that's another approach to this. There's so many approaches to building these models
to predict some outcome. And that becomes very useful. And as much as it can predict what movie
you would want on your streaming service, a self-driving car, that's all about machine learning.
But linear models is really the basic element of all of this. So it's good to build a linear model
like this, but there are some assumptions. So we just have to look at one or two diagnostics
once we've built these models. So a linear model, we've built the straight line model. There's this,
you know, we have, we're, we're accepting the fact that there is some linear relationship between these
and that might not always be, be the case. And one way we can find out about that is let's just save
the residuals as a computer variable. So that's the linear model dot resid. That resid would be, um,
what we save inside of this residuals computer variable. And let's have a look at the plot of this.
So there we go. And, uh, we see that these residuals are scattered all over the show.
So all over the show. So that's just the residual now versus the independent variable.
And we see the zero line there and there's good representation all over the show. So there's no,
no relationship between the independent variable and these residuals. And it's quite important to
have that. So we can also just look at the correlation coefficient here. If we just work that
out, we see, uh, it is very small. It's zero there times 10 to the power negative 16. So that's between
the independent variable and the residual. So it's very important in this kind of plot that we don't
see any patterns. Sometimes you see this pyramid pattern, pyramid lying on its side there,
might be indicative of some, what we call heteroskedasticity, etc. So important,
I think one of the important things to look at is that these residuals are nicely spread.
So now we've seen what it looks like if we have a single independent variable. What, what if we have
more than one independent variable? Let's look at an example of a, of a model with two independent
variables. So we call these, these models multivariable linear regression. So just, just
bear in mind, there's also a term called multivariate linear regression, and that refers to more than just
a single dependent variable. So here we have, still have a single dependent variable, but we're going to
have more than one independent variable as far as what we can use to predict our dependent variable,
and we call that multivariable linear regression. So here we're going to have two variables. I'm going
to call them var1 and var2, and they go, both going to be taken from randint, and we see them there,
and our dependent variable is going to be this nice linear relationship. It's just the one plus the other,
plus a bit of random noise. And we're going to save that as a data frame, and we're going to look at the
first 10 elements there. So I have my variable 1 and variable 2. So I have two, two independent
variables, and then I have a dependent variable. And the way that we design this is that there is
this relationship between these. So we can do a little bit of a scatter plot, and here's one way to
go about it. And perhaps for this data is not the best, because we're trying to do three numerical
variables here. So we have variable 1 and variable 2, and then a heat map, a color map, as you can see
there on the side for the dependent variable. Perhaps this would be a better way to represent
the data as a scatter matrix. So px.scatter matrix. Now if you have many variables, this can become a
bit unwieldy, but for us we only have these. So we can see variable 1 against variable 1. Of course,
that's going to be a beautiful correlation there, and 2 against 2, and dependent against dependent.
But here we have variable 1 and variable 2. Now is there any kind of correlation between those two?
It certainly doesn't look like that. And variable 1 and the dependent variable? Not so much. And if
we look at variable 2 and the dependent variable, there seems to be a bit of correlation there.
So once again, if we want to build a linear model, we're going to create these design matrices.
So once again, I'm going to say D matrices, and there's our formula. Very easy to read. It says,
take my dependent variable, and please use variable 1 and variable 2 as predictors.
The data comes from the dfDataFrame object, and it's going to return for me two sets of values. The
one is just going to be a vector of my dependent variables, and the other one is going to look
slightly different. So let's have a look at those. So remember, put the 1 in the first column. That's
exactly what we're going to have here, the 1s in the first column. But there I have my variable 1 and my
variable 2. They're still both there. It's just about the linear algebra for this OLS function.
So there we have sm.ols, ordinary least squares. I'm passing y and x, and then the .fit method.
I'm assigning that to the computer variable multi-lin-model. And let's run that model
quick and easy. And let's look at a summary. So the .summary2
method there. And once again, we're going to look at these intercepts. I see
my beta sub 0 there, 8.9. I see a beta sub 1 value, and I see a beta sub 2 value.
Very nice to do. So let's do this. Let's have the var mean model. So we're going to have a mean model
and a best model once again, because we're interested in that r squared value. So by the way,
there you can see the r squared. It is there, r squared right there, 0.091. And we see our f
statistics and the probability of the f statistic right there. So let's create those two variances.
Now remember, of the mean model, this is going to be the variance of the dependent variable,
because I'm just using the mean of that as my predictor. But as far as the variance in my model,
my best model is concerned, we're taking the variance of the residuals. So we have that save
when we're using that equation 7 for the r squared. The variance in the mean model minus the variance in
the best model divided by the variance in the mean model. As simple as that. There's our calculation.
Looks exactly the same as in the summary. And just to remind you of this f statistic. Now,
my best model has three parameters in it. Three parameters. It has a beta sub zero, a beta sub one,
and a beta sub two. My mean model still has any one. It just has a y-intercept and my length there.
So if I just save those, then I can do my f statistic and one minus. Remember when we looked at that
first one of those first graphs, it's one minus the total on the left hand side gives me the right
hand side. And we see that p value there. So very, very simple to do when you use Python.
So for the sake of interest, I'm now going to show you how we can use the f distribution instead of the
t distribution to compare two means to each other. And then we can use analysis of variance
and the f distribution. And I'm going to show you how to compare the means of more than two groups.
So it's just a bit of interest in the last section of this lecture. So revisiting the t-test. So what
we're going to do here is just to simulate some values. I'm going to have group one and group two,
and both of them are going to be taken from a null distribution. One with a mean of 100,
standard deviation of five. The second with a mean of 103, standard deviation of eight.
And I have 100 subjects in my first group and 110 in my second group. And then I'm going to have
this numpy array here. I'm going to call it group all. And all I'm going to do is take group one and
append to it group two. So I have this very long list then of 210 elements. So let's run that and let's
create this box and whisker plot. And what we can see here is group one. So I have a continuous
numerical variable, same for both groups, but I have two different groups, group one and group two.
And we can start to think, is there a difference between these two? So students t-test was very easy.
That's the stats.ttest. underscore ind at least. And I just pass the two arrays to that. And it's going to
give me a t-statistic and a p-value. But instead of that, let's use the f-statistic. And we calculate
a p-value for that. And the way that this works is we're going to have this idea of the sum of squared
errors for the mean of all the values as a model. And then we're going to have a best model. So look,
let's look at the sum of squares for the mean. Let's put that together. It's going to be the sum of,
I take the group all, and I subtract from that the mean of group all. So I've put all the subjects
together in one group, and I'm just doing this sum of squares, sum of squares. So I'm not dividing
by how many there are. We're not doing that. So it's just this very simple idea of take the whole
lot, throw them together, and we're just going to do not the variance, just simply I'm going to take
that difference between each value and the mean of all the values, and I just square that, and I sum
all of those up. And that's the sum of the squared errors there. And that's what we see.
Now let's do this individually for each group. Same thing. I'm taking group one, minus the mean for
group one, and squaring each of those, and summing each of those. That would be the sum of squares
for group one. And I'm doing exactly the same thing for group two. And now what we have is the
following this idea of best. It's just the sum of those, the sum for group one, and the sum for group
two. And that gives me the sum of squares as far as the best model is concerned. And there's my F
statistic once again. It's the sum of squares of the mean model, minus the sum of squares of the best model,
divided by these number of parameters. So let's have a look at those parameters. In my best model,
there's going to be two parameters. One for the one group, and one for the other group, as far as the
mean is concerned. And then for the mean model, where I just throw everyone together, there's
just going to be a single mean. So my parameter there is one. And of course, my number of samples,
the 210, is just the length of all of the group. So if we save that, we can calculate an F statistic
again. And there we go. We have our F statistic. And once we have that, it's one minus that little
area on the right hand side. And we're going to get exactly the same p-value here,
as when we use something completely different, an equation that looks completely different using
the, just using the t-test and the t-distribution. So just a bit of interest there, just how useful
the F distribution is, because we can expand on this and do some analysis of variance. So here,
what we're going to have here is I'm going to have three groups, A, B, and C, we're going to call them.
And we're just going to create some random values. And let's have a look, once we put that inside of
a data frame, what this looks like. So I've got group C, B, C, C, A, B. And then I've got this
variable right down here. And it comes from random.randin. So that is this idea of a standard
normal distribution. So mean of zero, standard deviation of one. So, you know, on the minus side,
and on the positive side of zero, we have this normal distribution. And that's where my values
come from. So let's separate all of them out as NumPy arrays. And I'm going to call mine group A,
group B, and group C. And I use df, and then these conditionals that we know very well now,
df.group equals equals A, and then B, and then C of the variable column. And I'm extracting those,
or converting those to NumPy arrays. So there we go. So let's just have a look at a little bit of
descriptive statistics for these, just using group by. So we can see the mean for those three,
and the standard deviation for each of those. Let's have a look at a box and whisker plot.
Because if you look at these, you should have some idea of whether you think there is going to be
a difference between these three groups. So one thing I want you to be very aware of when we use
analysis events like this is, I'm not making pairs of these. I'm not comparing group A and B,
or A and C, or B and C individually. No, it's a comparison of all three of them together.
And only if we find a very small p-value here will we go on to do what we call post-hoc analysis,
where we just do pairs of them to see where the difference actually is. But when we have more than
two groups like this, we've got to analyze them all in one go. And that's what we do
with analysis of the variance. Now, once again, very easy to do. I'm just going to show you the
code for this. It's stats.f underscore one way. So it's a one way analysis of variance. I pass my three
values there. One, two, three, my three arrays. And there I find a f statistic and I find my p-value.
And so very, very easy to do. Now, if we want to do this by hand, I'm just going to show you how to do
that. I again have this idea of ss mean, this mean model, and that df.variable. Remember that gives me
a pandas series object. That's all of them. And so each one of them, I'm just subtracting from that
the mean of all of them and squaring them. So this idea of the sum of squares, and then I'm going to do
it individually for each of the groups. So in group A, I subtract from that the mean of group A, square that,
sum over all of those, and the same for B and C. And then my sum of squares for my best model just
adds all three of them. Now my mean model, remember it just has a single mean. So one parameter, my best
model has three. The length is all of my sample space combined. No difference in how to do the f
statistic. No difference in how to calculate the p-value for that. So this f statistic, very, very important.
So I hope you took away from this, at least this notebook. I put in a lot of extra stuff,
which are much more advanced topics, and I want you to be aware of them. I wanted you to be enthused
about them, to start learning more about them. What it was about in the beginning though, is understanding
that there's a covariance. Then from that we can look at the strength of that covariance by the correlation
coefficient. And once again, we can calculate the proportion of, if we were to simulate that over and over
again, the proportion that will be more than the value that we found. In other words, we can simulate
a p-value and we also, you know, can measure this uncertainty that we have by bootstrap resampling.
And that is, you know, the deeper understanding of what really goes in here and builds on exactly what
what we did in the previous notebooks.

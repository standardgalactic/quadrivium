In a previous video, which I'll link up here, and that'll be down in the description,
we looked at simulating the difference in means for the same numerical variable between two groups.
I'm going to show you how we can do that just with the f distribution, or calculating the f ratio.
And one way, one place that we can use that is if we want to compare more than two means.
And in this example, we'll see three means, and I'll show you how to build up a sampling distribution of f statistics.
And we can see where our f statistic falls on that sampling distribution,
and hence get some sense of how likely it was to have found our f statistic, or more extreme.
As in the previous video, we're going to use Julia, and I'm going to code in Visual Studio Code.
Once again, there'll be a link in the description down below where you can see a video where I set up Julia and Visual Studio Code.
So as always, I'm going to have at the start of my scripting file here, all the packages that we're going to use,
and we're going to use dataframes, random, distribution, stats plots, and stats base.
So I'm just going to hold down shift and hit return after each of these.
Now, first of all, you'll see Julia will start, and that dataframes package will be imported.
There we go.
I'm just going to install all the other, or at least use all these other packages, import all of them into this active session of Julia.
So let's have a look at the f distribution.
Now, the f distribution takes two parameters,
and that has to do with how we look at the numerator and denominator of the f statistic,
and the f statistic is a ratio, and I'll show you how all of that comes together.
So I'm going to use the plot function there, and then from the distributions package,
you can see I'm using the fdist function, and I'm passing those two parameters.
I'm also going to do a little bit of coloring there, and just put a label.
And there we can see the f distribution for those two parameters.
So you can certainly see this is not a normal distribution.
It's not symmetric, and it's certainly not centered around a mean.
And what we're going to calculate is an f statistic or an f ratio, which will fall somewhere on the x-axis.
And the way that we will think of how likely it was to have found our test statistic or more extreme
would just be, you know, if it was here at 2, just draw a line straight up from 2,
and then everything towards positive infinity.
That area under the curve there, that is going to be this likelihood of having found our f ratio or more extreme.
And in that way, with randomization, we're going to be able to simulate a p-value.
So let's set up our data.
In this instance, we're going to have 300 samples in our data set,
and we're just going to have an equal size.
Now, that's obviously not necessary, but in this instance, we're just going to have 100, 100, and 100.
And if I divide 300 by 3, I'm going to get back a floating point value in Julia.
So I just have to stipulate that I want an integer.
And it becomes important when we use the repeat function to use an integer.
So there we have, I have n and n group, n underscore group, so the values 300 and 100.
I'm also going to set an id column, because we're going to build a data frame.
And I'm just going to use a range object there, 1 to 300, because 300 is in the value n.
And then I'm going to create a computer variable called group,
and I'm going to vertically concatenate three vectors.
And for that, we use the VCAT function.
As you can see there, VCAT.
And I'm just going to pass three vectors,
and I'm going to create each of my vector with a repeat function.
So I'm being very verbose here, just so that you can clearly understand how I put this together.
So the repeat function, I pass a vector there.
It contains one element, which is a string, which is the Roman numeral 1.
And how many of those repeats do I want?
Well, I want 100, and this is where it becomes important to place an integer there.
And if I didn't use int there, I was going to have 100.0, which is a floating point value,
and you can't use that with a repeat function.
Then I'm going to repeat the Roman numeral 2 100 times, and the Roman numeral 3 100 times,
and I vertically concatenate all of those.
So I have this long vector of 300 elements.
So let's execute that.
And I have this vector, as you can see there, if I hover over that,
it starts with 100 ones and then goes on.
Now, my next variable is going to be a continuous numerical variable.
I'm going to call it mass, just so that it can fit in with anything you want,
whether it's a product on the shelf or from a factory floor of some animal or organism.
It doesn't matter.
So I'm going to take all of these values from a normal distribution,
and because I want three of them, I'm just going to stipulate three means and three standard deviations.
So we see there 98, 100, 105.
So I'm just setting these values.
There we go, all six of them.
Because now I'm going to use the VCAT again,
and I'm going to assign that long vector of 300 elements to the mass computer variable,
and each of them that I'm vertically concatenating is going to be from a normal distribution.
So I'm going to use the RAND function.
First argument is from what we want to take these random values.
So the distributions package, I'm using the normal function there,
and I'm stipulating my mu1 and sigma1.
That is my mean and standard deviation.
So they'll be 98 and 20.
And how many do I want?
Well, I want 100 of those.
And then from 115 for the second one,
and 105 and 10 as far as the mean and standard deviation is for the third one.
So I'll just do all of those.
And now you can see that it'll very neatly fit in with my group 1, group 2, and group 3.
And I'm just also going to just seat the pseudorandom number generator.
And if you use 12, you know, there's nothing special about using 12,
but if you use 12, you're going to get exactly the same pseudorandom numbers.
So I'm just going to highlight all of them, hold down shift and hit return or enter,
and I have my 300 element vector.
And then from these three computer variables, id, group, and mass,
I'm going to create three columns in my data frame.
I'm assigning my data frame to the computer variable df.
From the data frames package, we're using the data frame function.
So I always just show you where these functions come from by, you know,
using the package name as well.
So remember, we don't have to put symbol notation there
or use quotation marks when it comes to the column header
or the name of that column, which will then be a statistical variable for us.
We can just write out the word or the character.
So id, I'm going to assign to that my 300 element vector id.
To group, I'm assigning my 300 element vector group.
And to mass, I assign that 300 element vector mass.
So if we execute that, I'm going to have a data frame that's going to be 300 by 3.
So you can see the three columns there, id, group, and mass.
Id is a 64-bit integer.
Group is a string.
And mass is a 64-bit floating point number.
So that would, you know, mean from the group, which is a categorical variable,
I can create my groups.
And the variable that we're going to compare between these groups
is a continuous numerical variable.
So no problem there.
As always, I like to create these sub data frames.
They just, you know, if I'm working on different parts of a data project,
it's just easy to work with data frames that only contain the samples or subjects that you want.
So I'm going to use the filter function.
And the filter function takes this little argument there.
And that r is just a placeholder.
So r.group.
And, you know, I think last time we used the word row.
You can use x.
You can use whatever you want.
And it's just going to go down each of the rows in the group column.
It's going to look for those that have one in them.
If that returns a true, that conditional, that row will be included.
Otherwise, not.
And the second argument there is just my data frame, which this filter function would refer to.
So I'm going to create these 300 by 1, 100 by 3 data frames.
And they will all now only have, you know, values in for the specific group.
I also like to extract vectors of the values that I'm going to work with.
For that, I use the collect function.
And I'm going to extract df.mass.
So that's all 300 masses.
And that dot notation just allows me to refer to that column.
And I'm going to assign that to mass underscore all.
So that will be a vector of all 300 masses.
And then I'm going to do 100 element vectors for each of those.
Now, you know, we had that before when we created the repeat function.
But I'm just assuming that you're importing a CSV file and that you have a data frame.
And you just want to extract those vectors from the columns.
So very quickly, I think we start off with some descriptive statistics.
Now, we chose, you know, we set the parameters for the normal distribution from which we took these values.
But, you know, so we know what they're going to look like.
But let's use the describe function.
And that comes from the stats base package.
And you see that's going to print out here in about with the default settings here in Visual Studio Code.
That's going to print out in the REPL down below here.
And we can see the 100 values in mass underscore one.
And that vector, there's no missing values.
We see a mean of 99.
And we see the quartiles there.
One thing that you don't see is the standard deviation.
But as I've mentioned many times before, just use from the statistics package, you can just use the standard deviation function STD.
So let's just have a look at those as well.
And we see 104.95 and then 100.52.
So what we're going to try and look at is the difference between 99.71, 100.52, and 104.
So we've got three means that we're going to compare.
And, of course, we can't just subtract them from each other as we did when we only had two samples.
So we're going to just simulate here, you know, many possible if ratios.
Let's just do a little bit of data visualization as well.
So I'm using this macro DF, referring to my DF data frame, and I'm going to do a box plot of all three of those.
And we create this box plot just to show me the masses of all three.
And the idea is there is a difference in the means between these three groups.
Of course, here we see the median and the first and third quartiles and then, you know, suspected outliers there beyond the whiskers.
So let's generate a test statistic.
Now, our test statistic is going to be an F-ratio, and I'll show you the equation for it in a little while.
What we need for that equation, though, is the mean of all of the values combined.
So that's going to be the mean of all 300 values, and I'm going to assign that to mean underscore mass.
Then we need the mean of all three of the individual groups.
So I'm just going to assign them there, and we saw them before.
Now, let me show you this equation.
Now, this is a PDF of the Pluto notebook that I used to do this exact same analysis.
And what the Pluto notebook, of course, does, there's Jupyter Notebook.
It allows us to make these nice little notebooks.
And I'll certainly make that available in GitHub together with this .jl file that we're working with in Visual Studio Code.
But if we scroll down here, you can just see everything we've done before.
It's all there.
And what I want to show you here is to scroll down, there's a little bit of information for you about the F-ratio and the F-statistic that you can certainly read.
But this is what we're after.
It's a very simple idea.
First of all, you can see there that I have all the values there.
So I'm just representing them in very schematic form, and we only have two groups in this little schematic.
But you put all the values together, and you calculate a mean.
And that's going to be our initial model.
We call it the mean model.
That's what you can call it.
And that says that whatever this mean is will be the predicted value.
So if you're thinking about this variable in our instance as its mass, given any random subject, we're going to predict that its mass is the mean of our model.
And that's a very poor model, of course.
And what we're interested in is the difference between each of the values and the mean.
And because we subtract one from the other, and depending on the order, we might get negative and positive numbers.
We square all of those.
So if you think about it, we're well on our way to thinking about the variance.
Because the variance is going to be the difference between each value and the mean, and you square all of those.
You add all of those squared terms, and you divide by how many there are, minus one, for a sample variance.
So we're not going there.
We're only here considering the difference between each value and the mean squared.
So just keep that in mind.
And then on a fitted model, and we use these terms because it's very similar to thinking about linear regression.
So if you know a little bit about linear regression, certainly keep that in mind.
And what we do here, because we don't have a numerical variable on the x-axis, so we can't do this least squares line.
So we think about it in a slightly different way.
But we now separate the two groups.
Or in our instance, we've got three.
And if you've got more than three, you obviously separate that out.
And for each of those now, we calculate a new mean.
So every group will now have its individual mean.
And that mean will be the predictor for that.
So given the group that the subject is in, we will use the mean as the predictor for the numerical variable.
Which again is a very poor model, but it's at least now going to be better.
Because given the group, we're going to have a better understanding or better prediction of the numerical variable by its specific mean versus this overall mean.
And that's what we call the fitted model.
And we just look at each of those individually.
But once again, for each of those, and you can see I'm only highlighting in each group just one of the differences.
But again, it's the difference between the value and the mean squared.
And we're just going to sum over all those.
And again, it's well underway.
We're well on our way to looking at a variance here.
We're just not dividing by how many there are, minus one.
And then F-ratio is this idea behind the difference between these two models.
This mean model, which is very poor.
And then this fitted model, which is obviously, well, let's use the term obviously in quotation marks, better.
And it's this ratio between these, and you'll see the term between and within, this ratio between the between variance and the within variance.
And that might be slightly confusing.
So I think this is just a better way to think about it.
Somehow, this F-ratio is just going to demonstrate or express at least the difference between these two ideas,
this very poor model, which is the mean model, and this better model, which we're going to call the fitted model.
But for all of them, all we want to remember is the fact that we are going to sum over the squared differences.
So we have this mean, and we're just going to take each individual value, and we're just going to do the square difference.
And there you can see the equation there.
We're going to call it SSM, sum of squared errors around the mean.
And that's just the mean minus each of the values squared.
And then there is the F-ratio.
So what we have in the numerator is the numerator and denominator, and what we have in the denominator is the numerator and denominator.
So in each of these, you can think of it sort of as a variance over a variance, but there's a bit of a difference here.
So SSM minus SSF.
Now, I haven't told you what SSF is.
We've seen SSM.
All we're going to go do is just go back here to this nice little schematic.
So what we're doing here is the summing of all the squared differences.
So from each of the values from the top to the bottom, you know, subtract the mean from it or it from the mean.
It doesn't matter because we square all of those, and we just add all of them together.
With the fitted model, we have these separately for two groups, three groups, four groups.
Yes, indeed, you can do it for two groups.
You don't have to use the T distribution.
So there we go.
We have, you know, for each of them, the sum of squares.
For the next one, the sum of squares.
And all we do for those individual sum of squares, we just add all of them.
So SSF will just be the sum of each of these individual sets of sums of squares.
And that's all we do for SSF.
So SSM is going to be larger than SSF.
SSF is going to be better.
So the sum of squared errors are going to be less.
But once again, very simple, just do the squared errors for each of these individually.
So in this instance, you'll end up with two.
In our example, we're going to land up with three.
And you just add all of those three.
It's just a simple summation.
And then we divide that by, in the numerator, we divide that by this symbol here called P underscore fit minus P underscore mean.
And that P stands for parameter.
So how many parameters did it take for our fitted model?
If we go back here, well, it only had one parameter.
There was only one mean in there.
So that P mean is only going to be one.
P fit, in this instance, well, we had two means.
So we had two parameters in calculating that SSF.
So that will be two.
In our example, we're going to have three.
So P fit would be three.
There are three parameters, three means.
And if we look at the denominator, it's this SSF.
So if you think of SSM minus SSF over SSF, that is just this proportion that we're trying to express.
It's denominator, though, is the complete sample size minus how many parameters there were in the fitted model.
So keep that in mind.
That is what this, you know, calculating this F distribution is all about.
And we're going to do that in code.
So back to our Visual Studio code here.
And that's where we see P mean, P underscore mean, and P underscore fit.
There was only one mean in my mean model.
And there were three means.
There are going to be three means in my fitted model.
So I'm just saving that.
So let's calculate SSM.
I'm going to call it SS underscore mean for descriptive purposes.
So I'm going to sum over all the square differences.
So there's my difference, differences.
I'm taking mean mass.
Remember, that was all of them together.
Dot minus mass.
That dot minus means do that in each instance.
So it's going to take the mean.
And for each of the values in the mass, all 300 values, it's going to subtract that from the mean.
So you've got to put that, because, you know, that is only a single value.
That is 300 values.
So you've got to put the dot there to indicate that it's done per element.
And then each of them dot square.
So each of those differences, I'm squaring each of those.
And all of that combined goes into the sum function.
So I'm summing over all the square differences.
And I get 73,888.
Now I'm going to do it for each of the three individual groups.
No difference there, other than I've got a different mean now.
It's mean mass 1.
And I'm subtracting that from that, each element in mass underscore 1,
again with a dot minus and a dot square.
So now I'm going to get these three values.
And all I'm going to do in the end for ss fit, I'm just adding all three of those.
There we go.
And now, eventually, we can use our f ratio function that we saw.
It's s is m minus s is f divided by p fit minus p mean.
And that goes over s is fit divided by n minus p fit.
And we see a f ratio there of 3.267, 975, etc.
That is our f statistic.
And if you've looked at textbooks or tables before, you'll have seen an f statistic.
And that's going to fall somewhere on that f distribution plot that we saw,
specifically for the two parameters.
And if you ever wondered where those two parameters come from, well, they're right there.
There's our first parameter for an f distribution.
And there's our second parameter for an f distribution.
So it's as simple as that.
So now it's time to do our randomization again.
Under the null hypothesis, a very simple null hypothesis,
we're going to state that these three means are equal to each other.
Under that null hypothesis, it means it does not matter in what group a subject falls.
You know, we can swap two of them around.
Because our null hypothesis states that there's no difference between these groups,
I can just interchange them.
And that's what we're going to do.
We're going to randomly, well, put them all together in one row,
scramble them up, shuffle the whole 300.
And then for that same sample sizes as before,
which in our case is very easy, it was 100, 100, 100.
If I take all of those values in a row, shuffle them up, down, up, down, randomly.
First new 100 is group one.
Second new 100 is group two.
Last 100 is group three.
I have three new groups.
And I can calculate this f ratio all over again.
Start from scratch.
I've got my 300 values now.
Shuffle them up and down.
Make three new groups.
Calculate a new f ratio.
And if I do that over and over again,
I'm going to get this sampling distribution of f ratios.
So to hold these values, we're going to start off just with an,
just defining here, or creating an empty vector.
And I'm going to call it f underscore ratio underscore sampling.
And I'm going to do 20,000 in this time f ratios that we're going to,
through randomization, that we're going to create.
So I'm just using reassign there, 20,000.
And I do that because if I want to change it later to 40,000 or some less,
it doesn't matter.
I can just change that value.
So once again, we're going to use a for loop.
So you can write much better code than this.
I'm being very verbose.
So that's before, just so that you can see exactly what happens.
So my for loop is going to run from 1 to 20,000.
So the first thing that I do is I do a shuffle of the mass values.
Remember, the mass is all 300 values.
The shuffle function from the random package is going to just reshuffle them
in a random order.
And I assign this new order to shuffle underscore mass.
And then I'm going to calculate the mean of this new shuffled mass.
Now remember, that's going to be no different from before because all 300 are there.
So nothing's changed there.
And I'm going to just do this reassigned SSM,
which is just going to be the mean of the shuffle mass minus, you know,
each of those shuffled ones squared.
So just as we had before for SSM.
Now remember, this SSM is going to be no different because shuffling them around,
they're all together.
I'm not taking a random selection with a replacement.
It's just the same as before.
But now what is more interesting, I'm going to have three new different groups.
So I'm going to create new group one,
and that's going to be the first 100 values in my shuffle underscore mass.
My second group is going to be from 101 to 200,
and the last one, 201 to 300.
So just bear in mind, if your sample sizes were different,
just keep the sample sizes the same.
But because they're all shuffled now,
there are going to be reassignment of the group that a subject is in.
But under the null hypothesis, this should make no difference
because the null hypothesis states that, well,
the three means are equal to each other anyway.
And then I'm just calculating the mean for each of these three new groups.
I'm calculating the new three fitted sum of squared errors,
and I'm just adding all three of those so that I have a new S's fit.
And then I'm calculating a new F ratio.
I'm going to call it reassign underscore F underscore ratio,
and we calculated it exactly the same as we did before for that F ratio.
And then I'm going to append to my initially empty vector there.
There's my initial empty vector.
I'm going to append to that this new F ratio that I've just calculated.
And then I'm going to run through this 20,000 times.
Now, powerful computer, lots of RAM, no problems.
It's very quick.
Now, let's have a look at the histogram.
Let's have a look at the histogram of this distribution of F ratios.
Now, remember, if we come to the left-hand side there,
we could see the plot that we have before for the F ratio using an equation.
This is what it would look like for those two parameters.
So let's have a look.
If we simulate, we've got our 20,000 simulated values there.
I'm going to use the histogram function, those 20,000 values,
but I only want 60 bins.
I want a bit of transparency, and I'm just giving a label to my function.
Let's see what it looks like.
Let's see.
And look at that.
Same or similar sort of shape as we had with our theoretical distribution.
Now, our F ratio falls somewhere here.
Remember, it was 3 point something.
And we're just wanting to know how likely was it to have found this F ratio or more extreme.
So wherever we have our F ratio here on the x-axis,
we want to know how many of our values, this is a histogram,
so it's going to, you know, just in every bin it counts how many of those fell in that bin.
So if we have a line here, 3 point something,
we just want to know what fraction of all these 20,000 values
were equal to our F ratio or more extreme.
So that's towards the positive infinity side.
And that's all that remains.
I'm just going to sum over this.
I'm just going to ask the F ratio sampling dot greater than F ratio.
So it's going to go to all 20,000 and return true or false if it is bigger than the F ratio.
And remember, true is saved as a 1 and false is saved as a 0.
So if I sum over all those 0s and 1s, it's going to sum over,
it's going to give me the sum of all the true values.
So how many of these 20,000 values were more than our initial F ratio?
And I'm just dividing that by 20,000 so that I get a fraction.
And if we do that, we see 0.039.
And that is very close to using just your statistics
with the mathematical equations that come with that
to calculate a p-value from an F distribution given those two parameters.
So that is a very, I think, intuitive way or different way
just to think about how likely it is to find your value
given the null hypothesis.
Now, we don't have access to the whole population.
The only thing we have is our sample.
And we're assuming that that sample looks very much like the population.
So we're using that as a proxy for the population.
And because we're just doing this reassignment
through randomization over and over again,
we're building this distribution of possible F ratios.
And we're assuming that that's going to look very similar to the population.
And if those samples were taken very properly, it should be quite close.
And here we've estimated, we have some idea of how likely it was
to have found our F ratio.
And we can now state that if our alpha value was open 0.5,
this is less than open 0.5, we reject that null hypothesis.
And we say, well, there is a difference between those three means.
And now we can start thinking about post hoc analysis.
Just to show you the count map function here,
it's just going to show me how many trues and falses I had.
So if I look at the 0 is false, 1 is true.
So 773 of those 20,000 were larger than our F ratio.
And that's how we got to that fraction there of 0.039.
so 42,000.
And that's how you can measure it.
So far and closer,
I have anotherib

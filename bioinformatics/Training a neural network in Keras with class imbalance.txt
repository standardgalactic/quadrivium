Hi there, in this video tutorial we're going to talk about training a neural network when we have an imbalance in the classes in our target variable.
So this is a notebook and a video tutorial explaining the Kiras website on class imbalance and you can find the link to that page.
I'm going to be slightly more verbose than that page so that you can really understand how to deal with class imbalance.
So my aim really in this notebook is for you to understand what class imbalance is, to understand how to express class imbalance as weights for a target variable class,
understand metrics such as true positive values, false positives, true negatives, false negatives, recall or sensitivity,
specificity, precision or positive predictive values and negative predictive values
and then how to use the class underscore weight argument when we're fitting the training data to our neural network.
I'm going to assume that you know Python, that you're familiar with TensorFlow and especially Kiras,
that you know how to construct a densely connected neural network using the sequential architecture
and that you understand constraints of the hypothesis space through dropout.
So let's have a look then at the packages that we are going to make use of.
We are going to use pandas and numpy and so let's just run the cell.
We're going to import Kiras then from TensorFlow and just so that we are all on the same page,
depending on what you watch this video, let's just make sure that we are using the same version of Kiras
or at least that you know what version of Kiras is used in this notebook.
So we'll run that cell and we see it's version 2.4 there.
And then also I'm just going to make use of matplotlib there as well.
And I'm just setting the style there to white grid.
And because this is a retina display, I'm just using the config magic command there.
So our data, by the way, we here in Kaggle, so the data is available for us.
As you can see on the right hand side, there is the input folder.
Inside of that, there is a subfolder called credit card fraud.
And we see the credit card CSV file there.
Let's click on that so we can just have a quick look at it.
And it pops up here at the bottom and we can see there's a time variable.
And that is just, I think, the number of seconds since recording the first transaction.
So this data set is all about credit card fraud.
So it's a binary classification problem encoded with a zero and one.
So we have this data and then the two classes in our target variable right here at the end would then state whether the transaction was a real transaction or whether it was a fraudulent transaction.
And we can see that our variable names are just V1, V2.
So there's already been principal component analysis applied to this data as far as I know.
And that is in an effort, of course, to de-anonymize this data.
So we needn't care about what these variables mean because there's already been a projection onto a lower dimensional space.
So what we're not going to make use of, of course, is this time variable.
We're going to just delete that in our model.
So there we go.
We can just import it.
I'm using the read underscore CSV function from the pandas package.
And to reference this file that's inside of Kaggle, it's two dots forward slash.
Then we see the input folder, as we can see up there, then credit card fraud, and then the actual CSV file.
So that's going to import that data set for us.
So we can use just the columns attribute there, and it's going to show us all the columns that are there.
There we can see them, as we saw before, time V1.
And then lastly, it was off the screen, so there's amount.
And then our target variable, which is class.
So what we're going to make use of just is the drop method.
So DF is what I assigned my data frame to, the computer variable that I assigned the data frame to when I imported the CSV file.
So .drop, since I just have the one column that I want to drop, I can just reference it as a string.
I have to say axis equals one.
So that means the column.
And I'm setting my in-place argument to true so that that change is permanent.
As I mentioned, all the data has already been projected onto a low-dimensional space through principal component analysis.
The only, I think, value that is still accurate there is the amount of the transaction.
And we see the amount there, 88 units.
I presume, you know, this would be dollars, but whatever the currency is.
And you can see the summary statistics using the describe method there on the df.amount Panda series.
Remember, if we just reference one of the columns, that returns a Panda series for us.
And we use the describe method on that series, and we can see the summary statistics there.
Let's have a look at the shape of our data frame.
So we're just using the shape attribute there.
And we see we have 284,870 instances or rows of data along 30 columns.
Now, the value counts, if we look at the class.
So df.class, remember, that's going to give us that column as a Panda series.
And we call the value underscore counts method on that series.
And we can see the class imbalance.
So 0, that was non-fraudulent, encoded as 0, the non-fraudulent transactions.
And one was the fraudulent transactions.
And we see only 492 out of 284,807.
So a big class imbalance.
You know, we would love that to be 50-50 when we do our, you know, when we fit our training data to our model or to our neural network.
And so certainly that is not the case.
Remember, if we set the normalized equals true.
So we take the normalized argument, set that its value to true.
It's just going to give us back the proportions and that we can see 99.8% of our target class is 0 or, you know, non-fraudulent transactions.
So what I prefer to do is just to convert this data frame object into two NumPy arrays.
One would be my feature vector.
And we're going to assign that to the computer variable uppercase X.
And our target class, we're going to do that.
Our target column, we're going to convert that into a NumPy array, which is a vector then.
So how I do that is just do df.drop.
We're going to drop the class column.
And we say that it's a column by stating axis equals 1 as far as that argument is concerned.
And then I don't put in place equals true because I'm just, you know, dropping that class from the data frame temporarily and then assigning that to the variable X.
But I do also add the two NumPy, two underscore NumPy method there.
So to convert those columns into just a NumPy array or a feature matrix.
And then my pandas series df.class, I turn that, convert that into a NumPy array and assign that to lower score Y.
So if we do that, we have a feature matrix and a target vector, both as NumPy arrays.
So just as a bit of a sanity check, we'll just use the shape.
We'll just use the shape attribute on both of those X and Y.
And we can see that we still have in our feature set, our feature matrix, 284,870 rows and 29 columns because we dropped the class, which is in this vector.
284,870 values, comma, nothing as far as that tuple is concerned, just to indicate that it's just along a single axis.
Now we're going to do, you know, splitting of our data.
So we're just going to have a training and validation split.
And because the data is already randomized, we don't have to use, for instance, the train test split function in the scikit-learn library.
We can just take the first 80% of the values as our training set and the last 20% as our validation set.
And so I'm just going to work out what, you know, 20% of the values look like.
So I'm just going to save that as an integer in the computer variable num underscore val underscore samples.
So that's an integer.
And what we want is the length of x.
So len, our function for length of this x.
So that gives me the number of rows times 0.2, but just converting that to an integer.
So that would represent 20% of the values.
So what are we going to do is create train underscore features, train underscore targets, val underscore features and val underscore targets.
So what we take is the feature matrix x.
And we use this notation as far as slicing is concerned.
So indexing.
So colon minus this value.
So it says up to that last 20%.
So everything up to the last 20%.
So that'll be 80% of the data.
And that goes into my training set as far as the features are concerned, as well as for my target vector.
And then the last 20, we do this.
So we say the negative, the integer value representing the last 20%, and then colon.
So going backwards from the back up to a level that would represent 20% from the bottom.
And that's how we get the 80, 20% split in our data here.
As I said, we're not taking random rows because the data is already, you know, in a random format.
So now we're going to use the bin count function from NumPy.
So I imported NumPy as NP.
So NP dot bin count.
The train target, train targets.
So only in our training set now, the 80% of the data that we've chosen as our training set.
So let's have a look at that.
And that's going to give us how many are zeros and how many are ones.
So we can still see this remainder of this class imbalance that we have there.
And now the crucial bit as far as this class imbalance is concerned.
How do we convert these values into weights?
And how are we going to do that?
I mean, this counts gives us this NumPy array with two values and they would be index 0 and 1.
We just take the reciprocal of each of these.
So 1 divided by 227,429.
That will turn into the weight for the zero class.
And 1 divided by 417 is going to be the weight for the fraudulent transactions.
So valid 0, fraudulent 1.
We just take 1 divided by those values.
So that's crucially how we're going to set up the weights.
And if we look at those values, we just print them to the screen there.
Of course, you can see because there's this overwhelming number of valid or zero values,
the weight for that is going to be 4.3969766, et cetera, times 10 to the power negative 6.
So 0.0000004.
And the weight for this underrepresented class of which there were only 417 instances,
the weight for that is going to be 0.002.
Do you see the big differences?
So this class that's underrepresented gets a lot more as far as a value is concerned for the weight.
So that's quite crucial.
We're going to do a bit of data preprocessing.
And what we're going to do is just standard scaling.
Remember that we take one column at a time.
For that column, we calculate its mean and standard deviation.
So that's x bar and sigma underscore x.
And what we do each value, we just take each value down that column
and we subtract the mean of that column from it
and we divide by the standard deviation of that column.
So that transforms every value into units of standard deviation away from the mean.
And that's every value gets transformed into this z underscore 1.
So just standard scaling there.
And what we do, first of all, we calculate the mean and the standard deviation.
And I'm just going to use NumPy's mean function and NumPy's standard deviation function.
So .mean and .std.
Assigning that to computer variables that make sense, mean and standard deviation.
And what we do is we calculate that of our training set.
And we calculate both the mean and the standard deviation of our training set.
And then we're going to do this transformation.
So it's train features minus the mean divided by standard deviation.
But you can see from the validation set, we subtract from it the mean of the training set
and divide by the standard deviation of the training set.
So that's quite important.
So let's use Keras.
Let's use the sequential architecture here just to create a neural network.
We're going to assign that to the computer variable model.
So it's Keras.sequential.
And let's have a look at our layers.
We're going to have one, two, three deep layers and then an output layer.
Each of them has gone, the deep layers are going to have 256 nodes.
We're going to use the ReLU, rectified linear unit activation function of each of these.
And our input shape is our training features.shape, the last value.
So let's just put in a little code cell there.
So just make sure what this represents.
So let's take that.shape.
So we can express all of that.
And you see the shape of that is the rows comma the columns, of course.
And what we want with the negative one is the last one.
So our input shape is 29 because we have 29 values going into our neural network at any one time.
Then another densely connected neural network with a rectified linear unit activation function.
But we are constraining our hypothesis space by using regularization.
In this instance, we can use dropout, 30% of the nodes.
Another dense layer, ReLU activation, and again, some dropout.
And our output layer is going to be a single node.
Because remember, we have a binary classification problem there.
And we are just going to use the sigmoid activation function.
So we constrain that output to being between, on the interval at least, from 0 to 1.
And if we look at the summary of this model, we can see it there.
We can see we have only 139,000 or at least 139,521 trainable parameters there.
So now, let's train our model.
And what we are interested in is a couple of metrics.
I just want to spend some time on this idea of how we can look at these metrics.
So in this little column, this little table here, we can see 0, 1 at the top row in bold.
And remember, the 0 represents the fact that this was a valid transaction.
And 1, that it was a fraudulent transaction.
So that's what the model predicts.
On the left-hand side, up and down here, we have the actual 0 and the actual 1.
So across the top in bold, we have the predictions.
Now, when we talk about these metrics, we have to choose either the 1 or the 0 as either the positive or the negative class.
And that really depends on the problem that you're dealing with.
So in each instance, that will be different.
We don't mean positive and negative in a human psychology sense.
We just have to choose one of these as our positive and 1 as the negative in a way that makes sense for the type of analysis that we're trying to do.
So in this instance, we might say, and what we've chosen here is the fact that the 1, the fraudulent, is our positive result.
And of course, we don't want a fraudulent transaction.
So a fraudulent thing is a negative thing, isn't it?
But as I said, that's not the psychology that we attach to it.
So if we suggested that the 1 is our positive class, so the fraudulent transaction is our positive class.
And that means 0, the valid transaction, is our negative class.
So if the actual value was 0, so it's a valid transaction, and the model predicts this bold 0 at the top, on that intersection we see T, and that's true negative.
Because a valid transaction was our negative class.
So if it really was a valid transaction, the model predicts it was a negative or a valid transaction, that's a true negative.
It got it right.
On the main diagonal on the bottom right, we see Tp, that's true positive.
So if it was a positive case, a fraudulent case, and the model predicts, the bold 1 at the top, that it was a fraudulent transaction, that's a true positive.
And then across from that, we have an actual class of 0, but the model predicts a 1.
That would be a Fp, false positive.
And if it really was a fraudulent transaction, and the model predicts it was a valid transaction, that's a false negative.
A predicted 0, that's a false negative.
And we have to decide, you know, how expensive, and in this instance expensive obviously pertains to, you know, financial expense.
But then another problem set, if we deal with, say, human disease expense might be, you know, how bad would it be for a patient if a diagnosis is missed?
Or would you rather just have a higher value of false positives instead of having false negatives?
So you have to evaluate the real-world problem that you are dealing with.
And so here are the metrics.
So these are absolute values.
So how many were true positive, how many were true negative, and then the false positive for false negative rates.
And then we have these proportions, the first one being recall, and in healthcare that, for instance, or in other domains that might be called sensitivity.
So that's your true positive rate divided by true positive plus false negative.
So the false negative, remember, are actually also positive.
So it's the true positive rate divided by how many are positive, are actually positive.
The specificity is the other way around.
It's the true negative divided by how many are actually negative.
So how many are predicted to be negative, correctly predicted to be negative, true negatives, divided by how many are actually negative.
So what we can think of as sensitivity or recall and specificity is, for sensitivity at least then, is how many of the actual positive cases are going to be picked up.
In specificity, how many of the actual negative values are going to be picked up.
And then precision, or positive predictive value, is given that a positive class was predicted, how many of those will actually be positive.
And negative predictive value, if the model predicts a negative, how many of those, what proportion of all the real, of all the negatives is it picking up, or would it likely be, I should say.
So the metrics, we're going to store that as a Python list.
So keras.metrics, we have false negatives, false positives, precision, and recall.
And we've just given them all a name, so fn, fp, precision, and recall.
So we're just storing that as a Python list.
We're going to compile our model.
So we're going to use Adam, adaptive moment estimation, as our gradient descent optimizer.
Our loss function has to be binary cross entropy.
We have a single node constraint in that interval 0 to 1.
So our loss will have to be binary cross entropy.
And the metrics are going to be this Python list of metrics that we want to keep track of.
Now, very importantly for our class weight argument that we're going to use when we fit our data,
is we have to save that as a dictionary where the keys are the actual values, the 0 and the 1.
And the values for each key, that is going to be those values that we had initially
by just taking the reciprocal of each of those values.
So 1 divided by the number of cases or classes in our target variable.
So we save that as a dictionary and we can finally fit our model.
So model.fit, assign that to a function, to a computer variable history.
So train underscore features, target underscore features, our batch size, 2048.
We're going to run through 30 epochs.
We're going to set verbose equals 2 to 2 so that we can see those results as they come out.
Our validation data is going to be our matrices, my matrix, my vector there of my validation data.
And then we have this class underscore weight argument and we're setting that to class weights, remember,
which is this dictionary.
So I'm running this off of a CPU and I have not put my accelerator on the GPU on here on Kaggle.
So this is going to take a couple of seconds per epoch.
And I'll train this and I'll see you on the other side.
So there we go where our model is trained.
So let's look at some of these metrics.
So the first one we're going to look at, of course, as far as the validation set is concerned,
is we look at the false positive rate.
So the model predicted that this was fraudulent.
And in actual fact, it was not.
It was a valid transaction.
Because we assigned this model, this trained model to the computer variable history,
we can look at some of its attributes.
The first one is history.
And the one that we want is val underscore fp.
And we want the last value.
So that's 709 cases of all our cases were predicted as one.
But those were false positives.
If we look at the false negative rate,
so how many were assigned as being valid and they were actually fraudulent.
And here is the important bit here.
So you have to decide how expensive these things are in the scenario that we are dealing with.
It is a trade off in these models.
So we might want to say that we want this false negative rate to be as low as possible.
We don't want to miss those fraudulent transactions.
But because of that, we are going to have a higher false positive rate.
And we have to investigate some of these transactions,
even though, you know, which comes with a cost, time, money, etc.
Even though they were actually, you know, valid transactions.
So you can, you know, play with that trade off.
So what I wanted to do here is just end off with two plots.
So we're just going to use matplotlib here.
And we see the false positives and false negatives across all the epochs.
And as you can see there, the false negative rate, very low.
And that's just what we want.
We don't want to miss these transactions that are fraudulent.
But on the other hand, we see higher rates of false positives throughout the training.
And as much as, you know, we're going to mark these transactions, flag them.
And it's going to cost time and money, as I said, to investigate them.
If we also look at recall and precision here.
So let's have a look at those two.
And we can see the recall at the top.
And that is quite high.
So remember, recall is sensitivity.
So how many of the actual true cases, in other words,
how many of the actual fraudulent cases are being picked up by this model?
And that's quite high.
We're sitting above 80%, 90% there.
But very low positive predictive value.
So given the fact that the model says this is a fraudulent transaction,
how many of them are actually fraudulent transactions?
That's the positive predictive value.
And you see that's extremely low.
And we can see that from these two values.
That 709 were, you know, earmarked as being positive cases.
But that was false positive.
And that is fine in the setting in as much as we want to pick up all of them.
And the price that we pay is that we're going to, in amongst all those that we do predict as being positive,
is actually going to be, you know, incorrectly done so.
All in the setting of the problem that we're dealing with.
So that's it.
A short tutorial.
Just explaining the Kiris website webpage there on dealing with class imbalance.
Here we're not the love.
Got to see for today'sτί एंशू.
She justidente.
And Gaye Eustá Τhalicke.
Okay.
It's our issue to complete the balance.
And make balance.
To advance balance.
Database.
avoirTS, उंशू.
Analy felt這個 error.
Att abusive balance.

This is the second video in the series, a seminar series, on the basics, the fundamentals of linear modeling.
And I'm using Python, and more specifically the stats models package, which makes working with linear models very, very easy to do.
So if you haven't done so already, you'll see a link up here.
It'll also be in the description down below to the first video in this seminar series that's about straightforward linear regression.
With a single predictive variable.
And I want you to watch that video before you watch this one, because we just can use the terminology and the explanations that we had in that video.
Yeah, this video is all about analysis of variance.
You remember that there was an analysis of variance table when we talked about linear regression.
And I want you to understand and see clearly how these two things are, how they one follows from the other, how they are connected.
So I'm using Visual Studio Code here.
It's a Jupyter notebook inside of Visual Studio Code.
And we can see at the top there, Python 3.9.10.
I am using environment, using anaconda or then miniconda.
So there is my first cell packages used in this notebook.
We can use pandas.
Once again, you'll see that I don't use any namespace abbreviations.
NumPy and then from SciPy, we're going to import the stats module and then also the Patsy package.
And that's going to help us create design matrices.
Next up is going to be this section here.
That's all going to be all about our plotting.
So I'm going to import the express and the graph objects modules from plotly.
Also the IO module so that I can set the default plotting style to plotly dark because I'm using a dark theme here.
And Visual Studio Code so that we can have nice dark plots.
And then stats models that's going to do all the heavy lifting for us.
We are again going to use the OLS function, ordinary least squares function.
We're also going to use the ANOVA LM, ANOVA underscore linear model function.
We're going to use pairwise underscore two key HSD.
And then the multiple tests function, all from different modules inside of the stats models package.
So you'll remember this is the table that we're after.
This table tells us about this, what I would term the four fundamental linear model types.
And we're busy with the second one here.
We're going to do one way analysis of variance or ANOVA.
And it's all again, it's a model.
So we have one or more independent variables.
And that data type has to be for one way ANOVA, it's nominal.
So we're going to have a categorical variable.
And that is going to be our predictor, our independent variable.
It's one way and as much as we're only going to have one.
And then our dependent variable is still interval.
So still a continuous numerical data type.
So given certain levels of our independent variable,
which we can term a treatment, so different levels of the treatment
or unique values in this categorical variable,
we're going to use those unique values to try and predict a
value for continuous numerical variable.
And you might imagine the problems that we would have with that.
So let's generate some data.
As I said in the first video in the seminar,
it's good to just generate your own data
because you can control how that data is generated to some extent.
And so you already have some understanding of the type of results
that you would see.
And sometimes that is more powerful when you're trying to learn how to do this
than to just import a CSV file or later in the seminar series,
we will just import data that already exists in a spreadsheet file.
So let's just generate some data.
So we're going to generate data for 30 observations.
As we can see, as we can see this will be 30 subjects,
10 in each of three groups.
So I'm going to have this continuous numerical dependent variable,
and then a nominal categorical independent variable.
And that independent variable, which I'm going to call a treatment,
there are many synonyms for most things in statistics,
it's going to have three unique values or three levels.
So that's what we're going to do.
First of all, I seed the pseudo random number generator there
with numpy.random.seed, and I'm passing the integer 10.
If you do the same, you're going to get the same pseudo random values.
I'm going to have a computer variable called treatments,
and that is going to be numpy.repeat.
And what I want to repeat is a Python list object,
and you can see it has three strings in that list object,
a, b, and c, and I want to repeat that 10 times.
So what that is going to do, it's going to repeat a 10 times,
then b 10 times, then c 10 times.
So that I have a, a, a, a, then b, b, b, b, then c, c, c.
And that is all going to exist in a numpy array.
Then I'm creating a new variable called dep a,
and then also dep b and dep c.
And I'm going to generate 10 random values,
pseudo random values for each of those.
And those are going to be the values for each of the subjects
that has an a or a b or a c.
So I'm using the round function, numpy.round.
In the second argument, there's one, because I just want one decimal value.
But the value that I'm after is the stats.norm.rsv function.
Now, what the rvs, I should say, what the rvs function does is it takes
values from a described distribution.
In this instance, it's a normal distribution.
So it's stats.norm.rvs.
And because it's a normal distribution, it depends on two parameters,
a mean and a standard deviation.
So I have to pass those.
So loc loc, that's the mean.
So that's 100.
Scale, that's the standard deviation, is 10.
So I'm going to take size equals 10,
10 random values from a normal distribution
with a mean of 100 and a standard deviation of 10.
And then comma 1 just to do the second argument of the round function.
Now, for those that will have a value of b in their independent variable,
I'm doing that also from a normal distribution, also 10 values,
but this time from a mean of 105 and a standard deviation of 7.
Again, I'm just rounding that.
And then lastly, from a mean of 95 and a standard deviation of 5.
So let's do that.
We'll run that.
If you use the same seed, you're going to get the same values.
And what we're going to do now is just to build a data frame object.
So I'm going to call my data frame object df.
So I'm assigning this data frame object to the variable df.
And then I'm calling the data frame function from pandas.
So it's pandas.dataframe.
And I'm using a Python dictionary, as you can see here,
so that I have key value pairs.
And if you pass the dictionary to the data frame function,
the key is going to be the column header, column name,
and the values are going to be down the row.
So each of those observations.
So for treatment, this string treatment here, that's going to be my key, colon.
The values are treatments.
And remember, those are these treatment study values.
AAAA, 10 of them, then BBB, 10 of them, then CCC, 10 of them.
And then for the dependent variable, I'm passing a numpy array of these 10, 10, 10 values.
So I'm saying numpy.array, and I'm passing as a Python list these three numpy arrays.
And but then I'm also calling the flatten method, because I just want that to be flattened
as one long numpy array, because then I have 30 of them.
Because remember, I created 10, 10, 10, and I have 30 values in treatment,
so that I just have these two columns that are of equal length,
have the same number of rows down each of them.
So there is my data frame, all created.
All I want to do is just to state very clearly that the treatment column now, which I can refer
to just by shorthand notation, df.treatment, that that is a categorical variable.
So I'm using the categorical function in pandas, so pandas.categorical.
I'm saying take the df.treatment pandas series, that's going to return for me that column.
And I'm stating that that is categories, and the categories are A, B, and C.
The order here really matters, because the first one is going to be your base case.
And later on, we'll discuss exactly what that means.
But specifically, when you get to logistic regression, that becomes very, very important.
But what I'm doing to this, I'm overwriting the df.treatment column.
So if we do that, pandas now knows that that is a column that contains a categorical variable.
So let's do some exploratory statistics.
That's always good before you do any modeling.
Try and understand, you know, some summary of your data, either through
calculations, as we'll do here, and through data visualization.
So some form of descriptive statistics and data visualization.
So I'm going to call my data frame df, and then the groupBy method there.
So df.groupBy.
And what I want my data frame to be grouped by is the treatment column.
So it's going to look at all the unique values.
It'll find A, B, and C.
And then for each of those, take the dependent value, and then describe that.
So the .describe method there, that gives us this whole range of summary statistics.
So first of all, the groupBy worked by finding A, B, and C.
And then it found that there were the count, the frequency.
There were 10 observations in each of those treatment levels.
We'll see a mean.
Now remember, we chose a mean of 100 and a standard deviation of 10.
And using that seed for the seed of random number generator, we got a mean of 100.6.
For B, 106.22.
And for C, 96.78.
You see the standard deviations there, the minimum, and when the quartiles and the maximums.
So very nice.
Let's plot this though.
And I'm going to use plotly express, the express module, and the box function.
So express.box.
First argument is my data frame object.
What do I want on the x-axis?
Well, I want the treatment.
So it's going to be A, B, and C. On the y-axis, I want dependent, the dependent column.
And I've given it a title as well.
Let's see what that looks like.
Because this gives us a very clear indication of the information locked away in our dataset.
So indeed, in the nominal categorical variable treatment, it found three unique values, A, B, and C.
And then the dependent continuous numerical variable, they drew a box and whisker plot of each of these.
And as I hover over these, you can see the quartile values there and then the zeroth and the maximum there as well.
So you see the box and whisker plots and you see the suspected outlier as far as the dependent variable value is concerned for those with treatment level A.
Now I'm just going to plot it slightly differently.
I'm going to plot it as a scatter plot instead of a box plot.
Because I want you to think back about the tutorial on linear regression.
The magic was to draw the straight line that was this best fit model that made the least errors, remember?
And we have a problem when we look at this scatter plot.
Because I don't have a continuous numerical variable on my horizontal or my x-axis.
And I can't just draw a straight line through here.
So I've got to create a different kind of model.
I can't do the straight line.
You can well imagine, I mean, it depends how close A, B, and C are to each other.
You know, there's no ways that there's a interval, a constant interval between A and B and between B and C.
These are not numbers.
Even if you encode them as numbers, such as 1, 2, and 3, or 10, 20, and 30, it doesn't matter what you do, those are not numbers.
And what we do with analysis of variance is that the best fit model is actually very easy.
It is the mean of the dependent variable for each of these three groups.
So you can think there's a little red line here, and a little red line there, and a little red line there.
That is going to be our best fit model.
Just predicting, given a value for the input variable, the independent variable, our treatment variable,
given an input value which is either A, B, or C, it's always going to predict,
the prediction for the dependent variable is always going to be the mean for that group, as simple as that.
What you'll see, though, is it's not so different from linear regression as you might think it is.
So we are dealing with nominal categorical variables.
So we have to discuss dummy variables.
So if we think of the original, our original variable was this called treatment.
And if I look at the first five, so I'm using indexing there.
So I say, go up to the fifth.
And remember, Python is zero index, so it's going to show me the zero and the five is excluded.
So it's going to show me the zeroth row, the first row, the second, the third, the fourth.
And they were all AAA, because remember, the way I used the repeat function in NumPy.
So it's going to be AAA, 10 times, then BBB, then CCC.
So we're only going to see those.
But it tells me it is a category.
There are three objects, and it's A, B, and C.
So what we want to do is convert those into dummy variables.
You cannot encode those with 0, 1, and 2, or 1, 2, and 3.
That does not work.
You have to create dummy variables.
In other words, instead of just the treatment variable, I now have three variables, A, B, and C.
And we use, if we do machine learning, we can call this one-hot encoding.
But if that subject is an A, there'll be a 1 under the A and a 0 under the rest.
And if you can look at those three variables, the three dummy variables, A, B, and C, that clearly shows
you, if it's 1, 0, 0, that it'll be an A. And if it's a 1 under B, so 0, 1, 0, that'll be a B.
And 0, 0, 1, that'll, of course, be a C.
But remember, we started talking about one of these cases has to be our base case,
against which the others are measured.
And really, when we get to logistic regression, that's going to become very important.
So it might make sense for us to choose A, because you can also look at this,
if I just have B and C as my dummy variables, if it is a 0 and 0, it still describes an A.
Because if we have 0 for B and 0 for C, what other option is there for this to be A?
And if it's 1, 0, of course, it has to be B. And if it's 0, 1, it has to be C.
So there's redundancy in putting this A in there. You can think of it in that way. It's really redundant.
Just with these two columns, these two dummy variables, I can still know whether a subject
was in group A or group B or group C, just by these two. So those are our dummy variables.
And we really have, in essence, this comparison against the base case, which is going to be A.
But this is all I need to describe exactly which group or which level of treatment each of my subjects
were in. So let's discuss the research question. And what I've done here is I've written it as
a mathematical equation or statistical equation. And once again, I do refer you to the video lecture
on linear regression, just to understand what is going on here. My research question in words would
be, though, and now we've got to be careful because there are two ways to go about this,
the way that you might find in a traditional textbook that are going to compare the three means,
the means to each other, and talk about this ratio of the between group variance over the within group
variance. Now, that might make sense if you just look at analysis of variance. But I'm trying to
build a bigger picture for you here, one of linear modeling. And as you go on to more complex forms of
models, that if you remember that one just builds on top of the other, we're just expanding our knowledge
of these linear models, it makes more sense to still describe it in some sense as a linear regression
model. So while we might say, or most textbooks would say that we are comparing the three means,
which is exactly what you would use analysis of variance for, we want to state that our independent
variable is a predictor of the dependent variable. So we're still keeping it as a model, as we did with
linear regression. And we're still going to have, and we're going to get to that, our null hypothesis
and alternative hypothesis. What we are saying, though, is we want an estimate or a predicted
value for our dependent variable value. So you see the little hat there in equation one. So we want a
dependent variable value, estimated or predicted, based on these coefficients. I'm still going to have
beta sub zero hat. And remember why we put the hats on there, because this is data from a sample,
not the whole population. So we're only estimating what that true parameter value is in the population.
So still the intercept, still a slope, beta sub one, but that goes with our dummy variable b, which can
only take a value of zero or one. And this is multiplication. So when I have a beta sub one hat
value, and I'm either going to multiply by zero, which means it disappears, or by a one, which means
it's just that value, beta sub one, zero. This b and c are not continuous numerical variables,
they're either zero or one. And then the same for beta sub two, times either zero or one
for the c value. So it's all about the three coefficients here. It's all about the three
coefficients. And if you think about it, because I can only plug in either zero or one, so I have
three cases, zero, zero, one, zero, or zero, one. It means this estimated dependent variable value can
only take three possible values for this model, this research question that we have, can I build this
model on the right hand side of the equation to give me an estimate of the dependent variable value.
So if it is a, we have zero, zero, and beta sub one hat times zero plus beta sub two hat times zero,
this is zero plus zero, which is a zero. So it will always predict beta sub zero, if it's a case
that is in group A. If it's a case that's in group B, it's going to be beta sub zero times beta sub one,
because then I'm multiplying beta sub one by one, and beta sub two with zero, so that disappears.
And with c, you can see a similar thing. So this research question of mine, predicting or getting
an estimate for the dependent variable value, given my dependent variable values, I can only ever, my
model will only ever predict three values. Now, if I had more groups, of course, more levels to my
treatment or my independent variable, I would have more values that can be predicted. So our null
hypothesis, instead of thinking of it in terms of equal means, and you'll see it is actually those
equal means, we are actually comparing means, you'll see that come out. But I want you to still think
about it the way that we've thought about it when we did linear regression, that my null hypothesis is
that beta sub one equals beta sub two hat, both our estimates, remember, equals zero. And if they
both zero, think about that, it means it doesn't matter if B is one or zero, it doesn't matter if
C is one or zero, it doesn't matter if it's zero zero, because you know, then they fall away anyway,
my estimate will always be beta sub zero. So that's my null hypothesis that
the independent variable is not a predictor of the outcome of the dependent variable because
we will always just have beta sub zero. In essence, what we're saying is the three
means are equal, irrespective of what group my subject is in. Think about that. My alternative
hypothesis then is beta sub one is not equal to zero, and or beta sub two is not equal to zero. Because then
we're not going to get the same means out, are we? Because now we might have a zero and a one in
there, and we're going to get different values. So instead of thinking of the means, that these three
means are equal, you can see from these coefficients, it makes more sense to think about the coefficients,
just as we did with linear regression, that if beta sub one is zero and beta sub two is zero,
I can only get a value for beta sub, for beta sub zero, the other two are zero zero, beta sub one hat
is zero, beta sub two hat is zero. So can't have anything other than this estimate of the dependent
variable being equal to beta sub zero. So there's my null and my alternative hypothesis. So let's build
a model. Now for our model, we're going to use the OLS function just as we did with linear regression.
And you can now see, almost start to see why we can just do that. And remember the OLS function,
it takes a formula, or it can take a formula, which makes it very easy. So that is a string.
So it's got to go inside of quotation marks. So dependent given the treatment. So dependent
given the independent, whatever you name that in your data frame. And those are the terms we used.
So the dependent given the treatment, comma, the data comes from the data frame. And I'm also passing
the fit method. So I'm fitting the data to my model. And I'm assigning that to the computer variable
linear underscore model. And there you go. We can now check out our results, especially of the
coefficients by just calling the summary method. So linear underscore model dot summary. And there we go.
Looks very much like what we saw before with linear regression. So if we look at these coefficients,
I see an intercept. And now I only see a value for B and C.
And look at those values. Where did we see 100.6 before? That was the mean of the dependent variable
for group A. And what we see with these two other coefficients, that's the difference between B's
mean and A's mean, and between C's mean and A's mean. So all I'm going to predict, all my model's going
to predict, and remember what I said when we looked just at that scatter plot, the best fit model is just
going to predict the mean of each of those groups. That's all it's doing. And these are just the
differences between each group's mean and group A's mean. That's what these coefficients are. You
still see the standard error. So you know, we're going to take the coefficient divided by standard
error to get a t statistic. And you know, we can use the degrees of freedom of that specific sampling
distribution, which is the parameter for t distribution to work out a p value. And we're interested
here in the p value for treatment B. So that will be beta sub 1 and beta sub 2 hat. And you can see
for a chosen alpha value of 0.05, we do not reject the null hypothesis when it comes to beta sub 1,
but beta sub 4 for either of them. So it's 0.09 and 0.249. So we don't reject that. And we also see
the 95% confidence intervals around the coefficient there. So we're quite familiar with what is going on
here, nothing different from what we saw in linear regression. And there's our ANOVA table. And you
see, we have degrees of freedom, just as we have before, we have the sum of squares due to the
regression and due to the error, they still there, the means, which is just the sum of square divided by
the degrees of freedom. And if we do this division of the mean squared due to the regression over the
mean squared due to the error, we get an f statistic. And if statistic takes the two parameters,
which are these two degrees of freedom, two and 27, given that f ratio or f statistic,
we get a p value. So very, very simple. You've seen these things before. So let's just do this
kind of by hand, just so that you know what is going on. So let's start this time around
with the coefficients. So we're talking about beta sub 1 hat and beta sub 2 hat. Now my linear model
does have an attribute called params. So if I call linear underscore model dot params,
it's going to give me back these parameters, the coefficient values, 100.6, 5.62, negative 3.82.
Now, let's do this by hand, which is not really by hand, we're going to let Python do the heavy
lifting. So remember, Patsy creates these design matrices for us. So I'm calling Patsy dot D matrices,
the D matrices function, again, the little formula, which is the same as the OLS formula,
and the data comes from df. That's going to return two objects for me, y, which is this column vector
of my dependent variable values, and the design matrix x. Remember, the first column is going to
be a constant column, all ones, and the second column is going to be the values for my independent
variable. But this time around, remember that it's nominal categorical. So it's going to do the dummy
variables for us. And there we go. Let's have a look at the first five rows of my design matrix there.
And then you can see the first column is all the constants. And then you see 0000. Because remember,
those first 10 subjects were all A. So it's always going to be 000000. And you can really start to see
that there's very little difference here between what we're doing here and what we did with a linear
regression. Let's look at our dependent variable. And those are the first couple of values for the
first subjects. Now, to do this bit of linear algebra here, the least squares method, I just
convert them to numpy arrays as we did before and overwrite them. And there's our friend. And remember,
our friendly equation for least squares. And remember, from the first video, again,
if you understand a little bit of linear algebra, the column space of my design matrix does not span
the whole space that it can possibly be. I only have a few column vectors in there. Hopefully,
they're linearly independent. We have to have that. So what I need is I need this orthogonal projection
onto the column space of my design matrix. And that is going to give us the best possible values,
beta sub 0, beta sub 1, beta sub 2. And there's the equation. I take my design matrix,
take its transpose, multiply it by itself, take the inverse of that product, multiply that by the
transpose of the design matrix, and multiply that by y, my column vector of dependent values.
That's what I do in code there. You can have a look. The matmul function is matrix multiplication.
So we do all of that. And we get these values. Let's have a look at them. Remember, there should be
three in there. And they are, and they exactly are parameters that we had before. There's the params.
From, from our, there they are. 100.6, 5.62, negative 3.82. Look what we get from the least squares.
100.6, 5.62, negative 3.82. Exactly the same. So let's save each of these. We're just using indexing
and assigning these to a computer variable, beta 1, beta 2, beta 0, beta 1 and beta 2. And now I'm just
creating a little bit of a Python function, just so you can see what a function looks like. I'm going to
call my function research, and it's going to take one argument. And if that argument is the string a,
I'm going to have my estimate as beta sub zero. If that argument is the string b, it's going to be
the estimate is going to equal beta sub zero plus beta sub one. Remember, that's multiplied by one.
And beta sub two is multiplied by zero. So I'm not putting it there. And else, so that's if,
else if, or lf in Python, and then else, if it's not a, not b, then this is beta sub zero plus beta sub
two. And then that means the beta sub one is going to fall away because I'm multiplying it by zero.
So if you're interested, that's just a little function there. So if I pass a, it's always going
to predict 100.6. What happens if I, if we look at the mean of some using df.loc,
so asking for a location, so I go down the column, only select the a's, take the dependent value and
calculate its mean, what is it going to get me back 100.6. So given a, my model always predicts 100.6.
And that is indeed the mean of the dependent variable for that group. If I pass b,
I now get 106.22. And that's that 100.6 plus 5.62. As simple as that. And you can see where all
these things come from now. And if I just look at the mean for the group b's, it's 106.22.
And if we look at c, it's a, there's c, let's call the function and you see it is 96.78. And indeed
it's 96.78. If I just do the mean for those. So you can see that we are comparing three means here,
and our model is always going to predict these three means. And that's exactly how these coefficients
were calculated. But you can see there from the ordinary least squares, we, or the least squares
method using linear algebra, algebra, at least we get the same, exactly the same results.
And that's why we can use what you'll see in your statistical textbook, this idea of comparing the
means, because that's exactly what we're doing. But I still want you to think about this in terms of
the coefficients and the null and alternative hypothesis in terms of the coefficients.
Remember that we can now do the t statistic. Now the magic happens really, as far as I'm concerned,
with the equations for the standard error. And you can look up all the different equations for the
different model types. There is a BSE attribute in the linear model, and that's going to give me the,
these three standard, standard errors for each of my coefficients. And I'm just saving them
separately as computer variables there. Because remember, in the equation for you, how we did the t
statistic, that's just the coefficient divided by its standard error. And there's a attribute called
t values. So there are all the t values. And if I just take beta sub one, and I divide it by
its standard error, I'm going to get that exact value 1.732, 1.732, same for c, and I'm just saving
these or assigning these to computer variables, because I can now calculate the p values. And there is
a attribute p values, remember an attribute doesn't have open and closing parentheses, it's not a
function or a method. And there we go, there are the p values, the three p values that we had.
Remember that these are from a t distribution. So I can use the cumulative distribution function,
given the correct degrees of freedom. And in this instance, it's going to be 27. Remember my sample
size, my number of observations with 30. I'm subtracting three from that, in essence,
because I have three parameters, beta sub zero, beta sub one, beta sub two. But you can think
about this in a different way, the subtraction that you make from 30. So just be careful there.
And because this is tb, if we just look at what b was, it was a positive 1.73. So I'm just making it
a negative. So remember, we're counting with the cumulative distribution function from the, or if you
just think of the PDF, the probability density function, we're counting from the area under the
curve really from negative infinity. So I multiply that by two, because we have a two-tailed hypothesis,
and there I get 0.0946. And there is the p value, 9.46 times 10 times 10 to the power negative two,
that scientific notation, which is just 0.0946, same result. And there we do for the coefficient beta sub
2 hat, and we get the same p value. So in both of these instances, remember, for an alpha value of
0.05, we failed to reject the null hypothesis, that my beta sub zero, and my beta sub one hat,
and my beta sub two hat are equal to zero. We failed to reject that null hypothesis.
So there we go, confidence intervals for each coefficient as shown in five. That is how we
calculate these confidence intervals. You take whichever beta hat you're working with, you add to
that, or you subtract from that for the upper and the lower bounds of your confidence interval, given
a confidence level. And by the way, this t crit, you've got to work out what that critical value is, and
you multiply that by the standard error for that coefficient. So let's have a look what the t crit is,
the critical t value for the t distribution given 27 degrees of freedom. So I pass in 0.975.
Remember, I'm talking of an alpha value of 0.05, two tails, so 0.025 on either side.
So there's the critical t value. And I can plug that into the equation, multiply by the standard
error and add and subtract it from the coefficient value there. There is a confint method to my linear
model that's going to return the lower and upper limits, low and upper bounds for my 95% confidence
intervals, which by default would be 95%. And they are calculated by hand. And you can go through the
code there. And you see, using that equation, we get exactly the same results. And there you go,
the coefficients, you see now how it fits in just builds from linear regression. And you see why we
can talk about comparing the means, which is actually what we're doing. But it's actually about
the coefficients. It's just the dummy variables being 00, 01 or 10. And if we had more, you know,
there'd be more. And those coefficients are just talking about the means. But I still want you to
think about this in terms of these coefficients. So let's look at the analysis of variance table.
This is analysis of variance after all. And if we look at this, you know, it's going to show us the
degrees of freedom, the sum of squares, the mean of the square, mean square of the errors, the f
statistic and the p value for that f statistic. So let's use ANOVA underscore LM. That's what we get,
degrees of freedom. And remember here, we are still talking about the regression and the error.
Nothing different from what we had before. So let's just look at the fitted value. So there's
a fitted values attribute. And that is just going to calculate the estimated dependent variable value,
which in our instance, remember, can only take one of three possible values, which are going to be the
means. So there we go, the first lot will just be 100.6. And I'm going to just add that as a new
column to my data frame. So I'm calling a new column header there, attached to my data frame,
and I'm assigning to that these fitted values. Because what I want to do is again, let's talk
about the sum of squares due to the regression. And remember, as before, the sum of squares due to
the regression, again, just be careful, because different textbooks, different lecturers use
different terms here. The sum of squares for us, the sum of squares due to the regression,
is this difference between the predicted value, the estimate that my model now makes,
and the mean for that dependent variable. So that's exactly what we have here. And remember,
this estimated value y sub hat only takes one of those three values. So for each one of those,
it'll just be that estimate. Now, there is a attribute in stats models for our model,
so linear underscore model dot ESS. If you use that, you see, we get the sum of squares due to
the regression, the difference between what my model predicts, and this baseline mean model. And
again, just what we had in linear regression, there's still this baseline mean model that says,
irrespective of the input, whether it's a, b, or c, we always predict just the mean of the dependent
variable. One single mean, overall mean. And that's what we get, 450.96. And if we look up here at our
table, 450.968. So that's the sum of squares due to the regression. If we now look at the sum of squares
due to the error, remember, that's the difference between the actual value and the predicted value.
So that's the next one, sum of squares due to the error. And we use the E here because regression had an
R in it. But you remember the error is also the residuals, same thing. So it's the estimate,
the predicted value minus the actual value, square those sum over all of those. And there is an
attribute called SSR, which makes it slightly confusing now, because you can see stats models
uses the R there for residuals, and it uses SSE. But I want you to stick with this SSR and SSE so that
we don't get confused. But it is there. It gives me the sum of squares due to the error there,
1,420, which is exactly what we saw now in the Nova table. And if we do this by hand,
assign it to the computer variable SSE, I get exactly the same results. Remember, this code
is exactly just that equation. We do those differences, and I added that new column of
estimates, subtract from that the actual value and df.dependent, square all of those, sum over
all of those. That's exactly what we get. So now we just have to do, that's our numerator and
denominator sorted. We just have to talk about the two degrees of freedom in the numerator and
the denominator. And what we just have to consider is the number of observations, and in the simple
case, the number of parameters, although I said you can think of that in a slightly different way.
So if we just call the shape argument or the shape attribute for our data frame and pass the value 0,
because it's going to return for us rows comma columns, we just want the rows, and I'm going to
assign that to the computer variable M, just to save that value 30. We had 30 observations.
And my number of parameters is 30. You can also think of that as the number of levels in your
treatment. There we go, K. Now, just to show you that the degrees of freedom is in an attribute,
so df underscore model is going to give me back the 2.0, because I take those three and subtract
1 from it for my numerator degrees of freedom, which is just too very simple. There is an attribute
called df underscore resid, and that gives me the degrees of freedom of the residual,
and you can think of that as M minus K. There are three parameters in my model,
beta sub 0, beta sub 1, beta sub 2. I subtract that from my sample size, and I'm going to get 27.
See, simple as that. And remember, the mean square is you take that specific sum of squares and
divided by its degree of freedom. Again, there are attributes for these. MSE of the model is there,
and if we do that by hand, we get exactly the same result, 2 to 5.48. And then, if we just look at that,
there is an MSE underscore resid. There we go. And if we do that by hand, we get the same result as well.
But now you know where these values come from. And remember, our f ratio is then just this
mean square due to the regression over the error, still exactly the same as we did with linear
regression. And this is where we get this idea of the between-group variance. Because remember,
if we do the sum of squares and divide it by the degrees of freedom, we're talking about a variance.
And the sum of squares due to the regression, you can think about that as the mean of a group
divided by or subtracted from that overall mean. So that is, in some sense, a variance between the groups.
And in the denominator, it is the predicted value minus the actual value. And the predicted value
is always going to be the mean. So that is kind of the variance within the groups. And that's why
these terms come from between-group over within-group. But I want you to stick with this explanation
that we've used up till now. There is an f-value attribute. If I do it by hand though,
I'm going to get exactly the same value, 4.285. There is a f underscore p-value attribute,
which gives us the overall p-value. And remember, I can use the cdf function for the f-distribution,
so stats.f.cdf. I pass my f-ratio or f-statistic value, which we've just calculated, and the two
degrees of freedom, which we've saved as df1 and df2. That was 2 and 27. Remember that that area under
the curve? It starts counting from the left-hand side, so we've got to subtract that from 1. We're
interested in that f-ratio and more extreme. And there's exactly our p-value, given a bit of the
small difference in rounding errors there. So that's really powerful stuff. You understand,
hopefully now, all of what is in that table. Just a little plot that I've drawn for you here. You can
have a look at the code, but just to understand, there's the pdf for an f-distribution with parameters
2.27. I've calculated for us a critical f-value there for an alpha value of 0.05. And there's our value
there. And remember, from the left, the area under the curve, but we want from this value out towards
positive infinity, so this area under the pdf here. And hence, we subtract our value from 1.
We can also just talk about the coefficient of determination. Remember, that's still going to be
this ratio of the sum of squares due to the regression over the total sum of squares. And we
remember that the total sum of squares, which is the addition of sum of squares due to the regression
and sum of squares due to the error. There is an attribute for that, r squared. But if we were to
do that by hand, we're going to get exactly the same result. Now, we just have to interpret this
slightly differently. Here, the interpretation is not as straightforward, of course, as we get at
least with linear regression. More importantly, I just want to talk to you about post-hoc tests.
Now, we're only going to do a post-hoc test if we find a significant p-value. If not, then we don't.
Now, in our instance, when we looked at those three means, if we looked at beta sub 1 and beta sub 2,
they were not significantly different from 0. We failed to reject those null hypotheses.
And therefore, we won't do post-hoc tests. But in this instance, I just want to show you what it is.
And post-hoc tests means I'm now going to do pairwise comparisons. We had three levels, A, B and C.
So what's the difference between A and B? Was it between B and C? Was it between A and C? So we've
got to do those three. The problem is that we have these family-wise errors. You compound your
type 1 error. And as we can see here, we call it a family-wise error. So that's the alpha value
inflation or the cumulative type 1 error. And then you see a little equation of it,
that alpha underscore EC there. That's our alpha value that we've used throughout, 0.05. So it's
1 minus 0.05. And you raise that subtraction to the power, the number of p-values that you have.
You subtract that from 1, and that gives us this idea of a family-wise error. So if we do that in
our case, we had three pairwise comparisons. And so our alpha value is not 0.05. If we do all these
pairwise comparisons, it's actually 0.142. So we've got to correct for that. And there are different
ways to correct for that. I'm going to show you two. The first one is Tukey's honestly significant
difference test, honest significant difference test. And there's a pairwise underscore Tukey
HSD. I pass my two variables, set my alpha value and get a summary of that. And now you can see the
adjusted p-values. And you'll see it's between a and b, between a and c, and between b and c.
The other one other way to go about it, and there are quite a few ways, is just do this pairwise
comparison of my t-test with a Bonferroni method. So that's also a correction. What we have to do is
just to save those dependent variables each as a separate Python list, which is what I do there.
And now I do and save my individual p-values for these t-tests. So you see, I have my three t-tests
there. And now I'm going to pass them to the multiple tests function. So I pass them all three
p-values as a Python list. I set my alpha value and I can set a method. In this instance, it's the
Bonferroni method. And there you can see what happens. I can see between a and b,
this is about rejection of the null hypothesis. So we don't reject the null hypothesis. We don't
reject the null hypothesis. But between a and c, we do reject the null hypothesis. You can see the
three adjusted p-values there. And then what you can also see is the corrected alpha value for the
SIDAC method and the corrected alpha value for the Bonferroni method. So just how the corrections
take place. So I hope this was enlightening, that this was a good video tutorial for you. The second in
the seminar series on linear models, this was analysis of variance. And really, if you understood
linear regression, there was almost nothing new here. And you can see how we can use analysis of
variance to compare the means of groups. But what we are considering is just those coefficients,
and then also just our analysis of variance table.

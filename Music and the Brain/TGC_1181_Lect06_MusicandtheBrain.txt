Music
We started this course by talking about the difference between music and musicality.
Music is deeply shaped by culture.
It varies enormously around the world and can change quickly over time,
just like any other art form.
Musicality is the set of mental processes that allow humans to make, perceive, and respond to music.
These processes are widely shared by people around the world.
We'll look now at two processes that are fundamental to musicality,
the perception of pitch and the perception of timbre.
Now pitch is a familiar concept.
It's the perceptual property of sound that allows us to order sounds from low to high.
These two clarinet sounds differ in pitch.
The first is lower and the next is higher.
Music
Timbre is the perceptual property that allows us to distinguish two sounds
when they have the same pitch and duration, like these two sounds.
Music
We can think of timbre as the character of a sound,
and as we'll see later, a lot of different features contribute to it.
Pitch and timbre are two of the main building blocks of musical structure.
In this lecture, we'll discuss some of the mental processes involved in their perception.
We'll mostly focus on individual sounds, rather than on sequences of sounds,
because I want to illustrate how even the basic building blocks of music are mental, not physical.
Pitch and timbre are not acoustic facts.
They are percepts that are constructed by the brain.
Now these percepts aren't just constructed by human brains.
Pitch and timbre are fundamental auditory percepts shared by many animals,
because they're important features of many animal vocalizations and environmental sounds.
This means that pitch and timbre perception have ancient evolutionary roots,
going back hundreds of millions of years.
As we'll see, these deep roots imbue these percepts with certain psychological qualities,
which are part of how we perceive music.
This lecture focuses on pitch and timbre,
but it's worth remembering that these are just two of the perceptual attributes that sounds have.
The other basic attributes are duration, loudness, and spatial location.
These are all important for music, but here I focus on pitch and timbre,
because they have rich psychological properties.
Also, the neuroscience of pitch perception has an important lesson about
how the two hemispheres of our brain play somewhat different roles in music perception.
Let's start with pitch perception.
Pitch has an important relationship to sound frequency.
We can illustrate this relationship with the simplest of all sounds, a pure tone.
A pure tone is a tone with just one frequency, for example, 440 Hz.
Hertz refers to how many times per second sound pressure oscillates at a given spot in space,
like at your eardrum.
For a pure tone, the pressure oscillates in a very simple way,
which can be represented with a sine wave.
In this image, the x-axis is time, and the y-axis is pressure for a given point in space.
At this point, air pressure varies smoothly between a high value and a low value, 200 times per second.
We call this the pressure wave form of a sound.
In this case, it's a simple sine wave.
The basic wave form shape repeats every 5 milliseconds, giving 200 cycles of pressure variation per second.
Pure tones aren't that interesting musically.
They sound very thin to us, but they're important in the study of pitch,
because in pure tones, pitch and frequency have a simple relationship.
For a pure tone, the pitch equals the frequency.
This means that we can measure the pitch of a more complex sound, like a clarinet tone,
by matching its perceived pitch to a pure tone.
Let's listen to a clarinet tone again.
This is a much richer sound than a pure tone,
but we can find a pure tone that matches the pitch of this sound.
Almost all the sounds that we and other animals encounter, naturally, are more complex than pure tones.
From an acoustic standpoint, that means they contain many frequencies at the same time.
For example, imagine you're hiking through a dense rainforest toward a large waterfall,
and after an hour, you finally begin to hear a sound that tells you the waterfall is nearby,
a blanket of sound that's a bit like white noise.
This sound is a complex mixture of many frequencies at once.
In fact, mathematically, this sound is made up of many pure tones,
tightly packed together in frequency, like 1 Hz plus 1.1 Hz plus 1.5 Hz plus 1.9 Hz plus 2.1 Hz and so on,
up to many thousands of Hz.
Even though an individual pure tone has a clear pitch,
when they're packed together like this, the result is a noise without a clear pitch.
That's why we call it noisy.
It's like mixing together many distinct colors and getting a grayish mush.
Many sounds that we're attracted to musically, like clarinet or guitar tones,
have many frequencies at once,
but the frequencies are organized in a very particular way.
There is a lowest, or fundamental frequency,
and higher frequencies which are integer multiples of this whole sound.
For example, if the fundamental frequency is 100 Hz,
then these upper frequencies, or harmonics, would be 200 Hz,
300 Hz, 400 Hz, and so on, in multiples of 100.
In this figure, the x-axis is now frequency,
and the y-axis represents the amount of energy at each frequency.
The figure shows a fundamental at 100 Hz,
and three upper harmonics at 200, 300, and 400 Hz,
all with the same amount of energy.
We call tones like this complex harmonic tones.
They're complex because they contain more than one frequency,
and harmonic because the frequencies are all related to a basic,
or fundamental frequency in a particular way.
They are integer multiples of that fundamental.
A figure like this which shows the frequencies that make up a sound,
and their relative energy, is called a spectrum.
As we'll see, this way of representing sound, a spectrum,
helps suggest why humans are so attracted to complex harmonic tones as musical sounds.
But before we turn to that, let's look at the spectra of a couple of real musical sounds.
This image shows the spectrum of a clarinet sound playing the note F we heard before.
In this case, the fundamental frequency is near 175 Hz,
so the upper harmonics are at 350 Hz, 525 Hz, and so on.
Notice something interesting about these harmonics.
The odd numbered harmonics, 1, 3, 5, and so on,
have much less energy than the even numbered harmonics.
This is characteristic of a clarinet sound,
and if you listen to the great courses lectures on math and music by Professor David Kung,
you'll learn that this reflects the physics of the instrument.
Now let's look at the spectrum of one more real musical sound before we discuss how we perceive pitch.
This is the spectrum of a trumpet playing the same F that you just heard from the clarinet.
It has harmonics at the same frequencies as the clarinet,
but these harmonics have a different energy profile.
Let's put the clarinet and trumpet spectra right next to each other.
They have very different patterns of energy across the different harmonics,
or spectral envelopes.
This is one of the things that underlies the different timbres of the two sounds.
Most of the melodic musical instruments we're familiar with,
like cellos, guitars, pianos, saxophones, and so on,
produce complex harmonic tones.
Why are we so attracted to complex harmonic tones as musical sounds?
Let me show you the spectrum of a human voice,
saying the vowel E, with a fundamental frequency of 175 Hz.
It's a complex harmonic tone.
Now look at this spectrum of the vowel A as in bat,
again with a fundamental of 175 Hz.
It's also a complex harmonic tone,
but with a different spectral envelope,
which is what makes it sound like a different vowel.
I think one reason we find musical instruments with complex harmonic tones so attractive
is that they are reminiscent of human vowel sounds.
Our brains have great interest in such sounds because they're fundamental to speech,
in brain imaging research that directly compared processing of musical sounds
based on complex harmonic tones like clarinet and flute sounds,
to human syllables and vowels,
Amber Lever and Joseph Rauschecker found a good deal of overlap
in regions responding to musical and vocal sounds
in auditory regions of the temporal lobe.
These head-on images of the brain show these regions of overlap in white.
The green areas were especially sensitive to speech sounds
and the blue areas to musical sounds.
But one of the striking things about these images
is how large the white areas are that respond to both kinds of sounds.
Let's return to pitch.
How does the brain assign a pitch to a clarinet or a trumpet or a vowel sound
since each has multiple frequencies?
How does it even know what the frequency content of a sound is?
Sounds are first received as vibrations of the eardrum
and these vibrations mix all the frequencies together.
It turns out that the sensory organ which converts these vibrations into nerve impulses,
the cochlea, has ways of separating out the different frequency components of a sound.
This means that the brain can use the pattern of different frequencies
and their energies to construct the sense of pitch.
I use the word construct again and now I want to tell you why.
By now you may have guessed that in a complex harmonic tone
the fundamental frequency corresponds to the pitch.
Remember our clarinet tone had a pitch of 175 Hz
and also had a fundamental frequency of 175 Hz.
If pitch perception in complex harmonic tones
was just a matter of the brain detecting the lowest frequency in a spectrum,
then pitch perception would be simple.
But a classic phenomenon in auditory perception
teaches us that things aren't that simple.
If we remove the fundamental frequency of the clarinet tone
or a trumpet tone or a vowel, what pitch do we hear?
Let's listen to the original clarinet tone.
And the same tone without its fundamental.
They have the same pitch.
The brain has constructed the pitch from the information in the remaining frequencies.
This is called perception of the missing fundamental
and it's just one of the many ways that we know that pitch is a construct of the brain,
not a physical fact about sound.
Research with other animals like monkeys and songbirds
suggests that they also perceive the pitch of the missing fundamental.
So this constructive nature of pitch perception
is likely to be very ancient in evolution.
One reason I've described the missing fundamental
is that it leads us into a broader topic in the study of music in the brain,
namely differences between the two cerebral hemispheres.
Research with patients with damage to the auditory cortex
as well as brain imaging research with healthy people
suggests that the right auditory cortex is particularly important
for missing fundamental perception.
You might remember from our first lecture that the right auditory cortex
was also important for the ability to judge the direction of pitch change
between two tones, up or down, which is important for melody perception.
In 2002, Robert Zittore and his colleagues reviewed these and many other studies
that pointed to a right hemisphere bias in musical pitch processing.
What made their paper so interesting was that they suggested a reason why this might be the case.
They suggested that anatomical differences in the left and right auditory cortex
gave them complementary skill sets.
The right side was better at tasks that required very precise frequency analysis
of the structure of sounds,
and the left was better at tasks that required very precise temporal analysis of sounds.
They suggested that this was a fundamental trade-off in brain processing.
Precision in frequency analysis came at the cost of reduced temporal precision and vice versa.
Zittore and colleagues said that analyzing speech sounds put a premium on analyzing fast changes.
A sentence like the one I'm speaking now can have 20 or more phonemes
or distinctive speech sounds each second.
To process those fast changes, the brain has to track patterns with very good time resolution.
A melody will typically be much lower in terms of notes per second.
On the other hand, musical melodies tend to use much more precise pitch patterns than speech.
A small pitch change can make a big perceptual difference in music,
but can go almost unnoticed in speech.
Listen to these two short melodies.
Only one pitch is different, and that pitches just one semitone,
or 6% higher, in the second melody than in the first.
Here's the first melody.
And here's the second, with one pitch that is only one semitone higher.
Now listen to these two phrases.
Both have an accent on the word bank.
In one phrase, that pitch accent is one semitone higher than in the other phrase.
Go in front of the bank, I said.
Second phrase.
Go in front of the bank, I said.
I doubt you could even hear the difference.
Zittori and his colleagues suggested that,
since speech sounds put a premium on precise temporal analysis
and musical pitch puts a premium on precise frequency analysis,
the same brain region couldn't analyze both with high precision at once.
So it specialized the left and right auditory cortex for one or the other.
The researchers emphasized that this was a matter of relative specialization,
not an absolute categorical difference.
But this difference is enough to lead to many aspects of speech processing,
having a leftward bias in the brain,
and many aspects of musical pitch processing having a rightward bias.
So there is some truth to the idea that music is more of a right brain phenomena,
and language more of a left brain phenomenon.
But as we'll see later in this course, this is a bias,
a matter of relative weighting, not an all or none distinction.
Language and music both involve processing on both sides of the brain
and are intertwined in ways that are just beginning to be understood.
Now that we understand pitch of even a single sound is a perceptual construct,
I want to discuss how pitch and music has perceptual dimensions that go beyond just high and low.
But before I do that,
I want to take a moment to let you know that the spatial metaphor of high and low
that we use to describe pitch is not universal.
Cross-cultural research has shown that cultures vary in the way that they describe pitch differences.
Tones you or I would call high versus low,
or are called light versus heavy by the Capelle people in Liberia,
small versus large by people in Bali,
young versus old by the Suyat people of the Amazon,
and weak versus strong by the Basi people of Central Africa.
In a fascinating experiment,
Zohar Eitan and Ren√© Timmers collected many of these non-Western metaphors
and asked Western listeners to look at each pair
and choose which word was a better descriptor of a high pitch.
Now these listeners knew nothing about these other cultures and their metaphors for pitch.
Even so, the people who all did the test independently often picked the correct term.
For example, if presented with the pair young versus old,
people tended to agree that young was a better descriptor for a high pitch.
This particular choice makes sense because young children have high voices compared to adults.
But people tended to agree even when the metaphor wasn't so obvious.
For example, if they were given alert versus sleepy,
people tended to agree that alert was a better descriptor for a high pitch,
or if given thin versus thick, they tended to agree that thin was a better match.
The remarkable thing was that people tended to agree about a metaphor
even when they rated it as not a really appropriate metaphor for describing auditory pitch differences.
This suggests that the variation we see in metaphors for pitch differences around the world
is not just arbitrary random variation, but is tapping into some deep multimodal associations that people have with pitch.
But let's get back to the point about pitch having multiple perceptual dimensions.
We can call one pitch height to refer to the high-low dimension.
In music, pitch also has a very important dimension called octave equivalence.
An octave is a doubling in frequency.
The pitch an octave above 100 Hz is 200 Hz, and the pitch an octave above 200 Hz is 400 Hz, and so on.
Pitches separated by an octave sound very similar to humans, and we tend to give them the same name.
For example, in Western music, 110 Hz, 220 Hz, 440 Hz, 880 Hz, and so on are called the same note, A, just in different octaves.
When men and women sing the same pitch, they are usually singing pitches that are an octave apart because men's voices are about an octave lower than women's.
As we'll see in the next lecture, octave equivalence provides the framework for the structure of musical scales.
For now, I just want to point out that this way of perceiving pitch is not just something that happens automatically when you have a complex auditory system.
Recent research with songbirds, who use complex auditory processing in their own communication system, suggest that they don't perceive octave equivalence.
So this aspect of musicality must be younger than basic pitch perception, like perceiving the missing fundamental, which birds can do.
How young is it?
There is one study suggesting monkeys may perceive octave equivalence, but in research on perception, it's usually best to wait for replication before drawing any strong conclusions.
As of now, it's possible that octave equivalence, this seemingly basic aspect of musicality, does not come naturally to any other species besides us.
Before finishing our discussion of basic pitch perception in music, I want to mention one psychological property of pitch that is widely shared by us and other animals because it has an ancient evolutionary history.
This is the relationship between pitch and behaviors of aggression and appeasement.
In many animals, low sounds indicate aggression or dominance, and higher sounds indicate appeasement or deference.
For example, think of a dog growling versus whining. The low growl is an aggressive signal. The high whine is a sign of begging for attention.
These ideas were first explored by the psychologist Eugene Morton in the 1970s and elaborated by the linguist John Ohala in the 1980s.
Think about how humans use their own voices when trying to sound dominant and confident versus deferent or uncertain.
Part of what makes Darth Vader sound so menacing is the low resonant voice of James Earl Jones.
These psychological qualities of low versus high pitches may stem from a connection between body size and pitch.
Larger animals like lions tend to make lower sounds than smaller animals like house cats because they have larger vocal folds.
When animals want to appear aggressive or dominant, they usually want to seem bigger, and making lower sounds can convey an impression of a larger size.
When an animal wants to appear smaller, making higher sounds can convey an impression of a smaller size.
These ancient associations of high and low pitch find their way into music.
If you want to write a musical theme that sounds menacing, you're probably not going to score it for piccolo.
And if you want to write music for a scene of a child playing joyfully, you're not going to rely heavily on trombones.
These decisions seem intuitive to us, but they're more than just convention.
They probably reflect associations built up over millions of years of evolution.
Now I want to turn to discussing timbre, its perception and some of its psychological properties.
As we'll see, those properties can again be related to evolutionary history that long predates human beings.
But let's start with perception.
Let's look again at two spectra that I showed you before of a clarinet tone and a trumpet tone.
I said that the pattern of energy across the different harmonics or the spectral envelope was an important part of timbre.
And that's true.
But what pictures like this don't show you is how things vary over time.
If I could play the spectrum as a movie from the start of a tone until its end,
you would see variation in the height of these bars, which shows the amount of different energy in the different harmonics.
Those variations contribute to the characteristic timbre of the sound because they create a sound's amplitude profile over time.
Just as every sound has a spectral envelope, it also has a temporal envelope, which is its overall pattern of amplitude over time.
For example, if we look at the pressure waveform created by a piano tone, we see an image like this, where the x-axis is time and the y-axis is amplitude.
One thing you can see right away is the sharp attack.
When one of the little hammers in a piano hits a wire to make a single note, many harmonic frequencies are excited at once,
and these have a lot of energy creating this sharp attack.
The energy in the harmonics fades away quickly, so the sharp attack is followed by a fast decay.
This amplitude profile is a big part of the timbre of a piano sound.
We know this because if we change this envelope, the sound of the piano changes dramatically.
For example, if we use a computer to play a piano tone backwards in time so that it starts at the soft end and stops at the loud end,
it has all the same harmonic frequencies but sounds very different from the original tone.
Let's listen to a single piano tone.
And then the same tone played backwards.
Now let's listen to a short passage on a piano played forward,
and backward.
To create the perceptual attribute of timbre, the auditory system has to integrate information about the spectral and temporal envelope of a sound.
Just as with pitch, timbre is a construct of the brain, and perhaps even more than pitch, the timbre of a single tone can have rich psychological associations.
It projects a certain character.
I want you to listen now to a short composition by our course composer, Jason Rosenberg, based on variations in timbre.
The primary contrasts between notes in this melody are in their timbre.
Arnold Schoenberg called this kind of music, which is based on timbre contrasts, a Klang-Farben melody.
You might find this a little jarring at first because we're not used to having music structured in this way,
but it's a great way to appreciate the diversity of timbres in music.
As you listen, think about the character each instrument projects.
As each note plays, you'll see an image of the instrument that created the sound.
I think that one reason timbre is such a powerful perceptual attribute in musical sound is because of the crucial role it played in mammalian hearing for hundreds of millions of years,
long before humans were on the scene.
Our early mammalian ancestors were nocturnal creatures who must have relied heavily on hearing for navigating their world and identifying things in it.
We know that one of the evolutionary innovations of mammals was having three middle ear bones, the malleus, incus, and stapes, instead of just one, like reptiles.
This probably gave early mammals enhanced abilities to discriminate and identify sounds very rapidly.
This would have been important for survival.
As nocturnal creatures, they needed good night hearing, the ability to identify objects by hearing alone.
Was that the sound of a delicious insect they could eat, or a small dinosaur predator, or a member of their own species?
These judgments depended on quick and accurate processing of timbre.
We have inherited this remarkable power to rapidly identify sound sources based on their timbre, and we put this ability to use in music.
This was demonstrated in a remarkable paper published by Carol Lynn Krumhansel in 2010.
Krumhansel was studying musical memory, and she wanted to know how detailed are our memories of music?
To test this, she took popular songs from the 1960s up to the 2000s and cut out fragments to play to listeners.
She asked them to identify the artist and title of the song, if they could, and describe the emotional, content style, and guess the decade of the song's release.
If you've ever heard of the old TV show, Name That Tune, that was on and off of television between the 1950s and 1980s, this should sound quite familiar.
But Krumhansel did something that took this game to a new level. She made the clips less than a half-second long.
She found that with clips of just 400 milliseconds, her listeners, who were college undergraduates, identified title and artist correctly for more than 25% of the clips.
Even if they didn't guess the title or artist, they could make accurate judgments about what emotions the music expressed, what style of music it was, like country versus rock, and what decade it came from.
400 milliseconds of music cannot convey much in the way of melodic, rhythmic, or harmonic structure, but what it can convey is timbre.
Of course, now it's not just the timbre of a single instrument, but the rich combined timbre of many instruments often plus a human voice.
If the experiment had used classical music, we would be speaking of orchestration, which is a way of composing with timbre.
I think one reason musical timbre has such a powerful relationship to musical memory is because of the role timbre played in mammalian evolutionary history.
For our early mammalian ancestors, quickly identifying a wide range of sound sources and remembering what they represented was key for survival.
I want to end by telling you about another study that used short clips to study music perception.
This time the task was to listen to a variety of clips and to sort them into categories based on the emotions that they led a listener to feel.
This study was about emotions evoked by music, not about emotions expressed by music.
The study was published in 2005 by Immanuel Begand and colleagues, and they began with clips that were about 30 seconds long.
All the clips were purely instrumental so that any emotions evoked would be due to the music, not the lyrics.
The researchers also avoided famous pieces so that people wouldn't base their judgments on memories of how and when the subjects had first heard those pieces.
They did the experiment, got the emotional responses to the pieces, and found that listeners grouped pieces in similar ways.
This showed a general agreement in how the pieces influenced their own feelings of arousal versus calmness and of positive versus negative emotions.
Now comes the interesting part of the study.
The researchers wanted to know how quickly people began having these emotional responses to the music.
Did they build up slowly over time, or did they start quickly?
So they got a new set of listeners and re-ran the study, but this time they shortened all the clips to one second long.
With these one second clips, they got results that were very similar to the original study with 30 second clips.
Once again, only a little melodic, rhythmic, and harmonic structure can happen in one second.
A lot of what listeners must have been reacting to is the timbre of the music.
Again, I think this makes evolutionary sense.
Our early mammalian ancestors didn't need to just identify sounds in the night like a predator or a potential mate.
They needed to act appropriately in response to them, and one of the main motivators of action is emotion.
So as we finish this lecture, I want to suggest that you try an experiment at home.
Put on one of your favorite pieces of music and see how quickly you begin to have an emotional response once the piece starts.
If you feel the music having an effect fast, as I often do, remember our little nocturnal ancestors from a few hundred million years ago,
and thank them for helping forge such a powerful link between timbre, memory, and emotion.
Thank you for watching.

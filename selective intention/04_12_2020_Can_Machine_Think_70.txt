See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/347936969

CAN MACHINE THINK -70!

Conference Paper · December 2020

CITATIONS
0

5 authors, including:

Albert Efimoff

READS
339

Leonid Zhukov

National University of Science and Technology MISIS

National Research University Higher School of Economics

80 PUBLICATIONS   16 CITATIONS   

SEE PROFILE

93 PUBLICATIONS   1,655 CITATIONS   

SEE PROFILE

Some of the authors of this publication are also working on these related projects:

UN-usual Robotics View project

Technology Research View project

All content following this page was uploaded by Albert Efimoff on 27 December 2020.

The user has requested enhancement of the downloaded file.

CAN MACHINE THINK – 70! 

INTRODUCTION 

The following page contains a full transcript of the discussion ‘Can machines think - 70?’, 
which was a part of Artificial Intelligence Journey (AIJ), annual Sber’s conference. The discussion 
was  timed  to  70’th  anniversary  of  the  famous  article  ‘Computing  Machinery  and  Intelligence’ 
written by Alan Turing. The article was published in Mind journal in 1950. It contains historical 
review of the topic of AI, contemplations about current situation in branch and some predictions 
for  the  future  directions  of  development.  Alan  Turing  is  the  founding  father  for  the  Artificial 
Intelligence, and his thoughts still provoke discussions and arguments. 
The recording of the discussion can be viewed here. 

This discussion featured the following speakers:  
Albert Efimov – Sberbank, vice-president, head of the R&D department 
Leonid Zhukov – Sberbank, head of the Artificial Intelligence Laboratory 
Gurdeep Pall – Microsoft, corporate vice president 
Michael Wooldridge – University of Oxford, Head of Department of Computer Science 

Professor of Computer Science  

Gary  Bradski  -  scientist,  engineer,  entrepreneur  (main  projects:  Open  CV,  Industrial 

Perceptron). 

Full video can be accessed here https://youtu.be/_fDclfgb600  

TRANSCRIPT 

[Albert  Efimov]  Hello,  my  dear  friends!  Dear  friends  of  artificial  intelligence,  general 
artificial intelligence, Sber, and AI journey. Welcome to the artificial intelligence journey! We 
start our discussion panel on the topic Can machines think? We believe that for this year famous 
article of Alan Turing, published in Mind journal in October 1950, which started actually a whole 
artificial intelligence journey, it's a good reason to discuss what is done on 70 years of artificial 
intelligence.  

Today  with  us  few  bests  in  the  world  experts  on  artificial  intelligence  and  information 
technologies. First of all, you will hear from Gurdeep Pall, corporate vice president of Microsoft, 
Microsoft Research, and our longtime partner with whom we did not only one research project. 

1 

 
 
 
 
 
 
 
 
 
 
We are happy that Gurdeep will join us on the artificial intelligence journey. Everybody of you 
actually, who is a little bit older than 35, are familiar with Gurdeep. Because if you know Windows 
NT and VPN, then you know some of the products which he developed while he's quite a long 
stay at Microsoft (almost 30 years).  

So, some of you younger than Gurdeep is working for Microsoft. Gurdeep, is the inventor 
of Windows, (at least part of Windows NT), inventor of one of the first VPNs, and stands at the 
beginning of the internet era, Windows era, and now artificial intelligence era. So, Gurdeep, please. 
What is the allure of thinking machine, please? 

[Gurdeep Pall] Hello, everyone! Firstly, I'd like to start by thanking Albert for inviting me 
and allowing me to talk to you about one of my favorite topics, which is really in the artificial 
intelligence space. I'm joining you here from the Turing room, which is in the Microsoft Research 
building,  Microsoft  campus  in  the  Seattle  area.  The  topic  of  thinking  machines  has  been  a 
fascination for a very long time. I would start with the intelligence in life itself.  

Now the Earth is about 4.6 billion years old, about 500 million years ago, something very 
interesting happened. At that time, all the life was living inside the oceans. The oceans had very 
low oxygen levels. But around 500 million years ago, there's what is sometimes referred to as the 
Cambrian  explosion  or  the  Cambrian  acceleration  happened,  where  life  suddenly  got  more 
intelligent. Scientists are still arguing about what is, why that happened. But the few things that 
happened at that time, which we know for sure are:  

•  The oxygen level in the oceans went up. It went from about 2-3% to about 10%. 
•  Developmental  genes  developed  through  mutations,  which  allowed  the 
regulation of how creatures were able to control their basic genetic processes. 
It was about 500 million years ago that the complex eye structure was developed 
in creatures.  

• 

It’s not conclusive but that was a very interesting development because once the creatures 
could see, they could find food easily, they could avoid predators, and they could actually go after 
other  creatures.  That  allowed  much  more  complex  organisms  to  be  built.  I  think  it's  good  to 
remember this as we think about artificial intelligence moving forward.  

Now, after 500 million years ago, the first documented idea of artificial intelligence was 
provided by none other than Homer himself, from around 8th century BC. 8th to 12th century BC 
is  sort  of  the  time  when  none  of  his  writings  are  attributed.  He  talked  about  in  The  Iliad,  how 
Hephaestus the lame God had different kinds of robots around him, to help him in his life. There 
were  tripods  with  wheels,  then  they  were  these  robots  shaped  like  the  female  form,  who  was 
helping him with lots of things that Hephaestus was trying to accomplish.  

After  Homer's  writing,  the  next  significant  piece  of  thought,  in  the  area  of  artificial 
intelligence is attributed to Ramon Llull, who was a philosopher and a thinker in the Catalan region 
of Spain. Ramon Llull invented a kind of a seven-disc rotating system, where every seven discs 
had different kinds of concepts. He basically said that if you rotate the different combinations of 
these discs, you can pretty much contain all the ideas and concepts around that existed. It was a 
very interesting idea that inspired a lot of thinking about this even centuries later.  

It was Wilhelm Leibniz in the 17th century, who then took that idea to create, perhaps even 
a more developed system, a sort of alphabet of thought. He wanted a system for the computation 
of ideas. After that, of course, why we are gathered here today, Alan Turing's seminal paper on 
can machines think. In that paper, Alan Turing did a couple of things: 

•  Formulated how to even think about this idea of artificial intelligence, by creating 

this so-called Turing test.  

2 

 
 
 
 
 
  
•  Taking different philosophical arguments about embodied intelligence, and really 

• 

sort of playing them out, I thought that was incredible work.  
It was also very telling, towards the end of that paper where he talks about things 
like telepathy and so on, with which he kind of created this sort of very open-ended 
aspect to artificial intelligence, which I think is something that we should all keep 
in mind as we move forward as we emulate human thinking.  

The last 70 years have been quite interesting in the artificial intelligence journey. It started 
with a lot of the rules-based approaches, symbolic systems that people worked on, expert systems, 
etc. While that showed early promise quickly, this sort of thinking ran out of steam. So, we ended 
up  with  this  AI  winter,  until  some  of  the  data-driven  approaches  started  to  show  results  and 
potential but with very specialized approaches. But it was only in the last decade that we saw the 
neural nets take off, thanks to a lot of more computing power and a lot more digitized data being 
available. We are starting to see some incredible progress with these powerful generic networks.  
Now, if you take a step back from this journey in the last 70 years, you will find there are 
about three different schools of thought or cognitive metaphors. We have the connectionists, who 
are really trying to model intelligence in the sort of graph with units, which are connected to other 
units, and through composite analysis, you're able to learn patterns. Then we have the symbolists, 
who believe in the idea that humans use abstract symbols as the foundation of knowledge and learn 
how to reason with them. Then there are probably the lesser known dynamicists, who believed that 
the  intelligence  of  an  evolving  system  can  be  best  modeled  with  methods  like  differential 
equations. What we've seen in the last 70 years is these different cognitive metaphors, becoming 
popular and pushing forward until they hit some walls causing folks to pursue a different model. 
Generally, you can classify all the work in AI so far in these three buckets.  

Now, let's talk about the connectionists, who are really having their day in the sun as it 
were, with the resurgence of neural networks and with deep learning. Now, the core construct there 
is really the construct of the neuron, which, as you see is a biological neuron inside the human 
brain. Well, how about, we create an artificial neuron as a basic unit, which actually is very simple 
when it comes to the mathematical operation that it does. We should recognize that it is already 
seen that the biological neuron is much more superior than the artificial neuron that we envision 
today. One simple example of that is that a single biological neuron has been shown to exhibit 
nonlinear capability. For example, if you have to learn the XOR function, and you have to do it 
with artificial neurons, you will need at least two layers, but it can actually be done in one in a 
biological neuron. That said, an artificial neuron approximates a biological one for our purposes. 
Now, taking this basic idea of an artificial neuron we create neural networks. In the same way, 
there are biological networks in the human brain, we create artificial neuron networks. And it is 
again  through  a  similar  kind  of  connectivity  of  these  different  neurons,  like  in  biology's  with 
synapse, here it is connections between the layers of the neurons.  

In  the  last  10  years  or  so,  we've  seen  just  a  tremendous  amount  of  innovation  in  deep 
learning architectures, for specialized intelligence tasks. Some of these are actually inspired by 
biology. In fact, if you look at a lot of the work in computer vision, not only, it was inspired by 
biology, it is now explaining human biology when it comes to computer vision and some of the 
structures and processes that we see in the visual cortex. Similarly, some of the work with memory-
based architectures, whether it be LSTM, etc., is starting to really get very specialized, taking some 
inspiration from the human mind. Thanks to the progress in deep learning, we have seen some 
amazing breakthroughs in the last five years. Now, all the breakthroughs that I showed you on the 
slide here, all happened in Microsoft Research, everything from speech recognition, at the level 
that is better than humans, the ability to detect objects better than humans, machine reading and 
comprehension where the AI model reads a corpus of text and can answer questions based on that 
to captioning of video things. All these as good or better than human milestones have happened 
in  the  last  four  to  five  years.  A  tremendous  amount  of  progress  has  been  made  globally  in  the 

3 

 
 
industry, not just in Microsoft Research, but this is just a glimpse to show you how much progress 
we have made.  

Now, none of these tests actually would qualify as a Turing test, by definition, because the 
Turing test basically said that you can ask the entity anything, something that could be outside of 
the domain that these models are perfected for, and they would fail. But regardless they represent 
significant progress, and at the same time highlighted significant limitations. Before we talk about 
some of the limitations, I want to talk about the GPT-3 model, which I'm sure that you've heard 
about. This is the work that has been done by Open AI. This year they launched announced GPT-
3, which can deservedly be called the Grandmaster of language models. It is a generative model 
using transformer architecture with attention mechanisms. It is incredible, the kind of results that 
we can see with GPT-3 is stunning because this thing was trained on about 5 billion tokens of data. 
It has 175 billion parameters, which is surpasses anything before it. This underlines the fact that 
the deep learning-based approaches really became possible because we had this incredible amount 
of computing, that we could bring to bear and also incredible amounts of digitized data. This GPT-
3 has, in addition to being able to write different kinds of long-form text, has been used for many 
kinds of applications, largely in the language domain but not limited to that. For example, there 
are  already  applications  for  writing  Python  code,  there  are  applications  for  automatically 
generating emails, there are plugins where you can automatically fill out Excel data, etc., which 
have all been written on top of the GPT-3 model, which has been very impressive. If you have to 
look at the progress of AI today, you have to use this as the marker of where we are 70 years after 
Alan Turing's paper. We are very happy to be working with Open AI at Microsoft, we have a deep 
strategic partnership with them. And a lot of this work happened on the Azure platform itself.  

Great progress, but we also have lots of lots of limitations in the deep learning approaches 
that are giving us impressive results. I'll talk about some of them, which I have listed here. Well, 
number one - the amount of data required to train these models is just really just too much for us 
to get this level of performance. And we know that humans can do very well, without using that 
much data. The models themselves are opaque all the learning itself is sort of encoded in these 
multi-dimensional  vectors,  which  is  no  one  can  make  sense  of.  We  have  this  problem,  which 
humans don't have, that these models are trained offline and then used for inference, but there isn't 
this notion of continuous learning. There is no semantic understanding, I'm going to talk about 
some  examples  of  that.  The  power  required  to  train  these  models  is  incredible.  Estimations  on 
GPT-3, which I have read on the web take us into megawatts of power for training these, while the 
human brain, seems to do fine with about 20 watts of power. So there are many, many different 
limitations. All these models are trained for a fairly narrow set of tasks, though, with GPT-3, we're 
starting  to  see  that  change  -  these  are  multitasking.  Models  can  be  built  on  top  of  the  single 
representation  that  is  learned,  which  is  quite  impressive.  So  you  can  see  we've  made  a  lot  of 
progress in the 70 years, but there are so many other hard things that we need to solve, including 
causal reasoning.  

So, what I talked, how powerful the GPT-3 model is, but it also has lots of limitations. If 
you ask GPT-3, which is heavier, a toaster or a pencil, it says a pencil is heavier than a toaster. 
When asked when counting what number comes before 10,000 it says 9099 comes before 10,000. 
When asked who was the president of the United States in 1700. It says William Penn was the 
president of the United States in 1700. Okay, let's examine these three questions. Well, the first it 
got wrong about pencil being heavier, was because in all the 5 billion tokens of texts that were 
there, nowhere, it had actually had a direct reference to the weight of pencil and a toaster. So it got 
it wrong. When it comes to counting, it actually does not have an underlying idea of the rules of 
mathematics. That's a lack of semantic understanding. Therefore, it basically tries to find a close 
answer and gets it wrong. In the third case, this was a bit of a trick question, because the United 
States did not exist in 1700. But it made up an answer. It picked a famous person from the 1700 
period history. All these examples are basically telling us that while we have clearly made a lot of 
progress we fail on some very fundamental things. If you ask a nine-year-old kid the answer to 

4 

 
these questions, they will probably get these answers right, which this amazing model could not 
get right.  

Now, Marvin Minsky, who can be called one of the elders of artificial intelligence, in 1970 
said that in 3 to 8 years, we will have a machine with the general intelligence of an average human 
being, this was 50 years ago. So, predicting, when we will have the average intelligence of an 
average human being is a very, very difficult task. If Marvin Minsky got it wrong, most of us will 
probably get it wrong trying to predict this. 

It was very interesting for me to read another prediction by the Turing Award winner this 
year,  Geoffrey  Hinton,  who's  considered  as  one  of  the  three,  I  would  say,  fathers  of  the  deep 
learning movement, who made a statement, that deep learning will be able to do everything. Which 
made me think, could this be true? And I concluded that it is, that he's probably right. And one 
very, very fundamental reason, is that deep learning has given us an approach, which allows us to 
do function approximation better than we have ever been able to do. The place where I feel his 
statement is ambiguous and maybe it was intended is that I believe that some of the approaches 
that  are  going  to  be  needed  to  get  to  artificial  general  intelligence  are  going  to  come  from  the 
symbolists and the dynamicists, even though some of their ideas may be solved best with deep 
learning approaches. So, that's kind of where I see things going.  

I feel that if you look at where do we need to make tremendous amounts of progress if we 
are going to get to the sort of the human level of intelligence. Number one, I think, model-based 
notions of space, time, and physics, this can also be called human common sense. A little child 
doesn't need to learn these again and again. If you throw an object they've never seen before, up 
in the air, they know what's going to happen to that object. They certainly have notions of time, 
they have notions of space, these are fundamental ideas, that somehow need to be acquired and 
encoded once in AGI.  Knowledge representation and organization, we are starting to make good 
progress here. Not just for language, this needs to expand into numbers, into graphs, into pretty 
much how humans have this broad notion of knowledge that we acquire and organize and recall. I 
think reinforcement learning is very, very important, maybe coming more from the dynamicists 
side, I think it's an important approach for ongoing learning, if you look at how children learn, a 
lot of those approaches can be considered as online reinforcement learning. I think causal inference 
work is a critical block that is largely missing today in the popular AI narrative. I'll also emphasize 
sparse learning because I think that this power-hungry approach to building these really deep and 
high  parameterized  models  is  not  sustainable  from  an  energy  perspective.  In  addition  to  the 
performance, we need to emulate human brain efficiencies here too. I think it's something that 
needs a lot more attention, we started to see some, some really interesting work here.  

But Lastly, I would say, I think if we are going to make progress towards a truly thinking 
machine, it will require the connectionists, symbolists and the dynamicists to all come together. 
And perhaps using some of the constructs that we've got from deep learning to solve the problems. 
For example, if you look at the symbolists their entire thinking is based on the idea of symbols that 
are the basis used to construct all the different layers above it. But symbols don't necessarily have 
to be symbols that humans also recognize. In fact, you could argue that some of the breakthroughs 
in GPT-3, which came through learned representations in the latent space with vectors, you could 
say,  are  symbols  too,  it's  just  that  we  just  don't  recognize  them.  Similarly,  deep  reinforcement 
learning. With neural ODE work, we're seeing how neural architectures can be used to learn the 
dynamics of a continuous system instead of using a differential equation. So I think this is where 
we are, these three disciplines working together, using neural nets as a powerful building block, 
we will have a truly thinking machine. Hopefully in our lifetimes. Thank you very much! 

[Albert Efimov] So we keep our discussion panel chewing 70 years of publishing Mind 
article. Right now I would like to introduce our guests who are joining our panel. Our first guest 
is Mr. Dr. Leonid Zhukov, who is the head of the Artificial Intelligence Laboratory in Sber. And, 
well, our closest collaborator on many fields of artificial intelligence, which is running here and 
you can learn about activities of his laboratory, I think, everywhere at this conference. Dr. Leonid 
5 

 
 
Zhukov joins Sberbank quite recently from high school of economics, where he was one of the 
heads  of  faculties  for  computer  science  and  artificial  intelligence.  So  we  are  very  happy  to 
welcome Dr. Leonid Zhukov to Sber.  

Also  today  you  already  hear  Michael  Wooldridge,  professor  from  Oxford  University 
computer science department, head of computer science department. And Michael is the author of 
a wonderful book, best Book of the Year by Financial Times. And I'm very proud that I read it 
from the cover to the end of the book. Please buy his book at Amazon or maybe in Sber shop, 
because it is really worth reading.   

But before discussing this with Gurdeep, with Michael, or with Leonid chewing 70 CAN 
MACHINE think. I thought that I need to give a context of our discussion because this is really 
important. Probably I forgot to introduce myself to those who are not really familiar with me, but 
maybe some of you at least, my students might be familiar with me. My name is Albert Efimov, I 
am working in Sber for more than three years and now I'm heading the R&D block for the whole 
Sber with the vice president title. I wear many hats in my career and I would say I'm just a mature 
scientist and R&D manager here in Sber and trying to be very useful for everybody. So back to 
our topic. 

Computing Machinery and intelligence was published first time 70 years ago. It’s quite a 
long  time,  many  people  might  remember  the  epoch  when  we  have  no  computers,  at  least  my 
scientific advisor for my Ph.D. was born long ago before Turing published his thesis. But it's very 
important to look at the past because we recall the past, it serves the present. Today, it gives us 
fresh  insights  on  what  we  sometimes  overlook,  and  cause  our  attention  to  ideas  that  might  be 
missed for some reason, it might give us new perspectives. And that's why we are looking for new 
ideas.  70  years  passed  since  Turing  publish  this  article  in  October  1950.  So  it's  really  worth 
discussing.  

But first of all, let me remind you, who was Alan Turing. He was a scientific prodigy, some 
of his friends in childhood called him scientific Shelley. At the age of 14, he rode the bike 120 
kilometers  to  his  new  school.  And  next  year  he  actually  published  a  small  article  dedicated  to 
Einstein's theory of relativity. In the picture here you see a drawing by Turing’s mother Sarah, and 
it's on playing a game, but he actually gave up the game and just looking at how daisies grow. And 
this is all true and he always did something contradictory to others. Later on in his life, he was 
famous for very ugly trousers. It was not fashioned at the time in England. Trousers should be 
always very, very well ironed and he always used the wrong trousers, so everybody actually paying 
attention to him. But he was still very much a scientific prodigy. And way early, he was elected as 
a fellow in King's College, Cambridge, and published a paper on computable numbers with an 
application to engineering problem, which is the problem of computability, which got the attention 
of  Alonzo  church,  and  he's  got  his  Ph.D.  visit  to  the  United  States  to  work  for  his  Ph.D.  with 
Alonzo church. He finished it, returned to Cambridge, and actually come to L. Wittgenstein to 
have some discussion on intelligence and the foundation of mass with him. And as soon as World 
War Two broke, he joined the government to court and the cipher school as one of the leading 
coders, code breakers. Foundation of peace, which we are enjoying right now it was actually late 
at the time. It was 1940 when he traveled to France, during the war to meet this Polish crypto-team 
and then he established the so-called hub-A team, which helped actually not win the war but to 
save an enormous amount of lives for the fight of England and North Atlantic. In 1941, he breaks 
transmissions and Bletchley Park where he was allowed to work was working was reading all the 
messages as instantly as German was typing them. In 1942 he created the first automated machine 
to read German messages and it helped to defeat Germans in North Africa. Also at the time, he 
visited the US again and meet Claude Shannon in Bell Labs. In 1943 he works on the first things 
award speech encryption system and Bell Labs and then returns to the United Kingdom, where he 
built the world's first electronic computer Colossus on the premises of Bletchley Park. I advise 
everybody of you to visit Bletchley Park. 1945 he completes his speech encryption system, but 
there is no use for it because he celebrates W-day in May 1945. And then he immediately travels 
to Germany to study cryptology, as well as to deliver a lecture. 

6 

 
This picture shows you how it looks, bless your pockets of time. It was actually the head 
of Alan Turing. And after the war, It was mentioned that he was one of the founders of the Ratio 
Club,  which  was  founded  in  1948,  and  it  was  very  multidisciplinary  and  it  was  very  much 
dedicated  to  new  ideas.  So,  it  says  that  no  professors  only  young  people  were  alone  and  Alan 
Turing was a kind of celebrity there. So, a lot of new ideas were proposed during ratio club, his 
days,  and  he  actually,  many,  many  later  ideas  he  developed  them.  So,  in  1946,  he  started 
developing  the  first  electronic  calculators  in  the  NPL  lab,  which  is  in  London,  and  also  start 
running marathons, 30 kilometers. In 1947 he visited the US again, meet with some US scientists 
including Fon Neiman. And this meeting was actually laying the foundation for the future study 
of Fon Neiman himself, and Fon Neiman architecture. Then, after his, I would say not successful 
presentation to NPL - National Physical Lab, he decided to move to Cambridge for a sabbatical. 
One of his bosses at NPL said on Turing's report on artificial intelligence that it's a schoolboy 
essay and not suitable for publication. That's where well-known fact in Turing historians, and it's 
actually it should be very much encouraging for all young scientists, please keep working and don't 
be upset if old guys like me or somebody else criticizing your ideas, which you are presenting to 
them on internal workshops. So please, please be ready for critics and not afraid of it. 

In 1950, he wrote the first programming manual and then complete publishing Computing 
Machinery and Intelligence in Mind journal. Mind journal is a philosophical journal. And that's 
why Alan Turing become a father of AI, that he also bought the house in Manchester to be headed 
there. 

Now I come to the foundation of life and eternity. In 1951-1952, he gives a few talks to 
BBC on artificial intelligence with some predictions. And then he switches to actually biology and 
I think he also published the first paper dedicated to bioinformatics. So he might be also as well as 
the father of bioinformatics. 

Many modern historians believe that due to an accident, not suicide, but an accident, he 
died on June 7, 1954. Just a little bit not leaving to his 42nd birthday. So he left no note that’s why 
many people believe that it was just a tragic accident. I strongly recommend you to if you are 
willing to know more about Alan Turing, read this book written by one of my good Kevin Valley 
as well as of course, Michael’s book and some other books, which are available widely. So, I have 
some further ideas, but first, just let me tell you two major ideas which gave us Alan Turing. The 
first is the universal Turing Machine and the second is Turing test which laid out the foundation 
for  computer  functionalism.  What  is  computer  functionalism's  basic  idea?  500  years  ago  Rene 
Descartes  believed  that  humans  are  machines,  might  be  machined,  animals  are  machines.  But 
Turing  and  later,  philosophers  who  created  computer  functionalism  believed  that  humans  are 
machines with software. Brains are hardware and software is running on them is what actually 
makes us humans and he created a Turing test, which probably many of you heard of. But universal 
Turing machine as well as a very important one because every machine sooner or later become the 
Turing machine. 

It means we have an emulation of everything possible with that simple Turing machine, 
which is actually going back and forth on the paper, and print, erase and print again. That's it. Very 
simple  algorithm,  a  very  simple  machine,  but  it  can  emulate  everything.  So  what  we  do,  what 
Gurdeep is doing what, Leonid is doing right now is just make Turing machines a little bit faster. 
And that's it, we are not doing anything more.  

So going back to the Turing test is enormously popular. It's millions of mentions on Google. 
Its foundation of Imitation Game, where we can think on, CAN MACHINE think comes from this. 
But we discussed it might be in more detail. I don't want to spend too much time on some of my 
slides. And I go to questions for our discussion, which are very important, and I see is that my 
friend Gary Brodsky, also joined us. So we welcome Gary to our discussion.  

Gary  Brodsky  is  one  of  the  leading  members  of  the  team,  which  won  the  grand 
programming challenge in 2005, and then sold this team to Google, he later sold another startup 
to Google. And Gary is famous for being a founder of Open CV. And Gary has got an enormous 
amount  of  friends  here  in  Moscow,  Russia.  And  actually,  every  young  man  and  girl  who  are 
7 

 
working on computer vision starts with Open CV, which was developed by Gary team already 20 
years  ago.  Gary  also  has  got  many  interesting  ideas  on  what  is  computers  right  now,  what  is 
computer science and artificial intelligence.  

So altogether, we have four great experts, one of the world's greatest experts. You see, I'm 
like, I don't understand what happened to me why I'm so lucky and happy to have you hear from 
our  panel.  And  I  put  in  front  of  you  actually  six  questions  you  might  see  right  now.  But  two 
questions, actually, most important for me right now. The first question is, what was wrong for the 
last 70 years in search of artificial intelligence and thinking machines? Would be Turing surprised 
if he comes today to our conference and sees what is going on? And what his surprised the most? 
And second important question, what should we do to make our research our policy right for the 
next 70 years of research? What we should do right for the next 70 years, and what mistakes we 
should certainly avoid?  

[Leonid Zhukov] Thank you very much! Well, I'm not sure if it was wrong for 70 years, 
right?  We  definitely  got  a  lot  of  achievements  in  those  70  years.  And  the  breakthrough,  for 
example, in of course, in deep learning, and the way we heard today, in all our presentations, it's 
actually proving it. But at the same time, it could be that we interpret it, the paper, the ideas. If we 
interpreted the ideas slightly differently, history could also be different. I think we should start 
with this notion of intelligence, right? And what is intelligence? And I don't want it to be like 
philosophical here. I want to be very, very practical, right? And so intelligence is what?  

When  I  think  about  intelligence  is  really  the  ability  to  set  up  and  solve  problems  and 
achieve certain goals in the real world. And that's it, right? Artificial intelligence - it's a computer 
or  program  that  can  do  it.  Nowhere  in  this  definition,  have  I  ever  said  the  word  human  and 
somehow going back to the Turing test, we put machines in these unfavorable positions. To pass 
Turing tests, they need to know what humans are, they need to know how to simulate and mimic 
and  behave  like  humans.  At  the  same  time,  intelligence  really  doesn't  mean  to  be  human. 
Intelligence means the ability to set up and solve problems. And so I don't think we went in the 
wrong direction. But part of the effort could have been avoided in the sense of trying to make 
machines that executives mimic humans.  

Now while doing so we, of course, make huge, interesting discoveries and learn a lot about 
humans and learn a lot about like, for example, human brain structure, etc. But in general, setting 
up  a  goal  of  building  a  machine  that  mimics  humans  might  not  be  the  right  goal.  And  going 
forward, I think we should focus on things like actually building machines that solve things, that 
can set up problems and solve them, and not necessarily the problems that humans will be able to 
solve.  

In fact, why do we need machines to solve problems that humans can solve? So we need 
to have machines that will solve problems that humans cannot solve? And we need to go and, and 
look in that direction more than trying to replicate humans. And though humans and our brains are 
probably a huge inspiration for computer scientists to build machines, they're insanely complex, 
and they think trying to reconstruct them. In the end to reconstruct, for example, the continuous 
way the brain operates in our sort of digital binary zero ones, that computers that the right, this 
probably is not going to work out.  

So  bottom  line,  I  think,  we  need  to  refocus  a  bit.  Understanding  intelligence  as  ability, 
again, to solve the setup and solve a real-world problem and work into solving intelligence in this 
sense, and not in replicating how human brains work, and what we as humans can do. Thank you! 

[Albert  Efimov]  Leonid,  I  have  a  question.  Can  you  give  us  an  example  of  the  kind  of 

problems that humans cannot really solve?  

[Leonid Zhukov] For example, genomics right now. We do have a lot of things we cannot 
solve, we cannot, for example, map precisely genes on Gen X and the phenotype genes onto precise 
diseases,  right.  There  might  be  a  reason  for  a  huge  number  of  possibilities,  or  maybe  we're 
8 

 
 
 
 
approaching it in the wrong way. And I think the great advantage that could come from AI is being 
able to solve the problems differently than we're thinking about that. That's one thing. 

Another thing is, I think, and as a professor, you perfectly know that. Solving problems 
when it is well-posed, positioned well specified. I mean, when you specify the problem really well, 
it's not that hard to solve it, it's actually hard to formulate the problem precisely that it is solvable, 
right? And so if machines learn how to actually formulate problems before solving them, right, 
that would be a huge advantage. So and that's what AI cannot do like AI cannot formulate tasks 
for AI to work on. 

[Albert  Efimov]  And  I  cannot  put  any  goals.  And  that's  a  big  problem  for  artificial 
intelligence  right  now.  Because  goal  setting,  aim  setting,  the  target  for  moving  forward  is  the 
biggest problem for artificial intelligence right now, I think. I wrote a paper on this, too. Do you 
think Leonid that deep mind alpha something with molecules might work for application AI in 
science?  

[Leonid Zhukov] Well, it's actually a very good example, you're talking about the recent, 
this, the folding example, right, with folding. This is actually an amazing example. And it's also, I 
would say it's on the same level for me, and using it for me, as GPT-3 when you interact with it, 
but the thing is, the truth is – GPT-3 has no clue what it is talking about, right? And so, it learned 
the statistics of the sequences, right? And honestly, if AlphaGo by learning the certain statistical 
patterns is can you manage to predict the right protein folding that allows us to build, for example, 
new  medicines.  That's  fantastic.  That's  terrific.  Does  it  bring  us  closer  to  like  AI  in  terms  of 
understanding? NO. 

[Albert Efimov] Thank you very much! So, actually, Gary, I would like you to join our 
discussion, and also to elaborate on those two things. What was wrong, for the last 70 years in 
terms of our quest for artificial intelligence, and at least I know two things, you sold two startups 
to Google as a good one. But what was wrong? And what should we write to make it better? You 
sell another startup to Google, maybe? And? 

[Gary Bradski] Okay, can you hear me? 

[Albert Efimov] Yes. Excellent. 

[Gary Bradski] Okay, so what was wrong? Well, early on, we didn't actually have a lot of 
computing for number one. I mean, our brain has a fantastic amount of computing in it. We're still 
not  at  that  level  that's  probably  available  anywhere,  but  we're  getting  there.  There  are  some 
understandings of neurology, there’s also what the idea is of intelligence, what it really is? I mean, 
people got enamored with logical proofs and other things and this isn’t a large part. A large part 
of what our intelligence is – is simply survival as a simian in an environment for whatever reason.  
So the goal that we had set out to do was largely wrong. Logical proofs - we didn't have 
the compute to do it, we didn't have a lot of understanding. There weren't any kind of interesting 
machines, like robots that were even capable of hardware. So, what was wrong? It Was too early 
and naiveté.  

So, what do we do? Well, it's, um, it's beginning, to get a little better than that. I started my 
academic career in neural networks and those of us working in it, like an early friend of mine. I 
contacted  him  when  I  was  in  graduate  school  because  I  wanted  a  corner  detector  that  he  was 
working on. But we were working on neural networks when we knew that was the kind of structure, 
not this symbolic AI. What we have right now it's a fantastically useful tool, but I call it's still 
basically, deep associator, it's not intelligence itself. And intelligence itself is a little bit difficult 
to define. 

9 

 
 
 
 
 
 
 
I think there's right now you have to understand, what a mind is, and what a mind can do. 
And I think there is a lot of misconceptions. Can a mind live forever? It's pretty clear – NO. Not 
in any, not in principle, because the mind is built up of structures and those structures suffer a kind 
of decay of when you are supposed to represent distributions, and they become overloaded.  

But an example I'm going to use for another talk is when you're building a robot, you have 
to build a mind. And that mind explicitly or implicitly represents the world and the robots. And 
you're forced to do that every time you want to make something operate in environments. And 
then the very primitive ones have a very primitive model.  

But some of them that I've worked on, let's say, a Stanley, which is a simple example. What 
it did is that it had sensed in the various modes of sensing lasers, GPS, wheel, odometry, whatever, 
in an environment, and it did it and vision and it fused all this information into a world map. And 
then it knew the direction of gravity and in that little world map, so it had this model of itself. It 
had the world and the world was very simple. It was bad things, good things, and things I don't 
know. Then it ran that in a physics simulator.  

Another thing people don't understand - we aren't our intelligence. We are our emotions. 
Intelligence is a how what is always by your emotion. For example, if you love computers, given 
a certain intellectual level, you're going to become very good at computers if that's what you really 
love.  

Stanley's emotion system was GPS waypoints that's what drove it. It wanted to get from 
one point to another safely. And so that was its entire mind. Its mind formed the world of bad, 
good, and I don't know. It knew where it wanted to go next and it's simulated itself going there. 
And then it would actually take that step in the real world. We’re very much like that. You see 
your simulation when you're asleep and I'd like to say when you wake up, your dreams don't stop. 
They just connect to a data source, which is the world. But you look at Stanley's mind and this is 
all mind, so it's a very simple mind. But what can it understand? All it understands is kind of like 
bad, good, don't know. Well, if you want to explain Dostoevsky's Crime and Punishment, what is 
this when the person takes a very bad rash move. Well, a rash move to Stanley, if you wanted to 
explain this Stanley, the robot, a rash move would be cutting across the I don't know the land to 
take a shortcut to not go the long way around, but jump to a further waypoint. Well, that's a rash 
move. And so you could explain those Dostoevsky Crime and Punishment to the robot, but not 
really, his brain cannot understand this, because it's not rich enough, well, to rounding error, that's 
us. 

So  we  will  never  understand  the  world  under  this  theory,  but  we  can  create  these  self-
operating agents, who have a certain level and can do certain things in the same way. If you have 
your cat, it will never understand Dostoevsky. It will never understand it because it doesn't have. 
But if you could talk his language, you could say, well, it's like a kitten that doesn't listen to its 
mother, and it's going to get in trouble. And then the cat will say, I understand you go. No, you 
don't.  

We're the same, like two rounding errors where the same will never understand the world. 
All we get is a model of the world. And that model is a causal model that like Stanley that allows 
us. Stanley needed to drive across the desert, we need to operate in a social simulation. We will 
never understand the world and physics, there'll be no final theories because we don't even see 
what  needs  to  have  a  theory  of  it.  We  have  none  of  that  circuitry.  And  so  we  can  create  these 
machines when we can create a machine that has this interior model, that socially simulates and it 
will become conscious and aware. And it might be more powerful than us and we can say well, 
what's the real structure or meaning of the universe? And it will say well, it's like that's going in 
danger, we won't be able to understand what it understands after a certain level. 

[Albert Efimov] I totally agree with the last point. We will not be able to understand the 
conscious machine if ever be conscious. At this point, I would like probably to return to one of my 
slides, which I prepared for our talk.  

10 

 
 
Turing  Continuum  is  from  the  virtual  world  to  the  physical  world  and  from  nonverbal 
interaction to verbal interaction. Turing was concentrating on the left upper corner of this with this 
noble task of solving mathematical tasks, cryptography, talking, playing chess, learning languages, 
whatever they're doing. But he gave no attention to the physical world because he considered it 
too complicated for realizing and robotics. I think that what I heard in Gurdeep's talk before, and 
what I heard right now in Gary's talk that the physical world and interaction with the physical 
world in a verbal and nonverbal way is might hold the key to creating a thinking machine. It's my 
article position. And of course, you can argue with this.  

But, Gary, I would like a little bit of you to elaborate very briefly about very clearly - what 

we should be avoiding for the next 70 years what is dangerous?  

[Gary  Bradski]  I  think  what's  dangerous  is  not  going  fast  enough  with  AI.  Well,  if 
humanity's existence depends on survival depends on getting these techniques fast for design and 
whatever. I'm not one of those that are worried that AI is going to kill us. I'm worried on the other 
side  that  we're  not  doing  it  fast  enough.  Also,  I'm  not  worried  that  AI  will  kill  us.  We're  a 
technology,  the  primary  thing  of  our  species  is  we  create  technologies  if  that's  not  stable  and 
survivable. So what, then we die. That was what nature intended because that's what we do. And 
yet, like we can't, we can't shy back from this, we have to accelerate it, in my view. And I'm not 
really worried. Also, if AI turns on us and decides to destroy us, well, then we succeeded, right? 
We created another species that can live without us, that'll be our greatest day. So I'm on the total 
opposite side of this like we should be applying much more efforts and funding and acceleration 
and just go. Go all out, we need the robots, the human labor is failing around the world. That's not 
there's not a population problem. There's an under-population problem that's rapidly going to hit 
us. I mean, it's going to hit here. Yeah, it's going to hit us like a freight train. And we need the 
labor. We need AI's ability to solve environmental problems. Global warming problems, energy 
problems, like we just cannot go fast enough. 

[Albert Efimov] Thank you very much, Gary! 
Turing Continuum also allows us to do one thing. I thought out all possible Turing test 
around this Turing Continuum, and you can see is that all Turing test which was created for the 
last  70  years’  group  up  in  the  upper  left  corner,  and  almost  nothing  in  the  physical  world  and 
nonverbal interaction, which is also very important, because the dark matter of culture is what is 
our I would say nonverbal interaction might be as important as verbal interaction chatbots and 
some other things.  

I would like to hear Gurdeep Pall. I listen to your presentation as carefully as I could, in 
terms of I'm also one of the organizers of the AI journey. And I know that Microsoft is betting on 
GPT-3. It's good to know that. But that's when I also know for sure, and I think Gary mentioned it 
a little bit, that GPT-3 is at least not energy efficient, as well as all other frameworks. So my mind 
right  now  is  consuming  only  25  watts  of  energy.  I'm  not  that  smart  as  GPT-3,  maybe,  but  I'm 
extremely energy efficient. So for today, I have only one cup of coffee, and it's just enough for me 
throughout  the  whole  day.  And  robots  need  a  battery  replaced  every  8  hours,  at  least  in  our 
Sberbank mobile robots. Robot battery is replaced quite often. So Gurdeep, do you think that this 
bottleneck, the energy bottleneck is a very serious bottleneck for the next 70 years of development 
AI?  And  of  course,  you  can  also  help  us  to  liberate  what  was  wrong  for  the  last  70  years.  So, 
please? 

[Gurdeep Pall] I was thinking about your question about the last 70 years and I would say 
that humans are very resourceful creatures. We approached after the Alan Turing famous paper. 
We approached artificial intelligence in a very resourceful manner as well. What I mean by that is 
that  we  basically  immediately  looked  at  the  problem  and  said  we  understand  the  logic,  we 
understand rules. So we took that bit, and we ran with it. Then we realized we can only get this far 
with artificial intelligence, with the sort of rules-based approach, then things sort of quieten down. 
11 

 
 
 
 
Then we realized, well, we know math pretty well, so let's start using some of these math-based 
techniques: k-nearest neighbor, some of the clustering approaches, and sort of this classic data-
driven math. So we're pretty resourceful creatures. And now because there is so much digitized 
data, and there is so much compute, of course, we are leaning heavily on deep learning methods. 
And  GPT-3,  I  completely  agree  with  you.  It  reminds  me  of  this  expression  more  trust,  Scotty: 
which is basically - let's throw more and more at it in terms of compute and data, and, of course, 
the performance is going to get better. So that's the sort of the journey of the last 70 years.  

But obviously, this is not a sustainable model as well. And I think that, if there is one thing 
I  would  look  at,  sort  of  critically  in  the  last  70  years  is  that  we  tend  to  get  obsessed  in  the 
community with one method or the other. We go through these different waves, and right now we 
are on the deep learning wave, which I'm a huge fan of, and I think, as Gary said, it's deep learning, 
and these neural network architectures are great function approximators. But we have to take a 
step  back  and  say,  what  are  the  big  gaps  that  exist?  And  are  we  making  sufficient  progress  in 
addressing those gaps, versus just pushing forward with more energy and more data? Towards one 
of my last slides, I talked about some of the areas we need to make progress on specifically with 
the area of power consumption. I think since we are inspired by the brain, let's look at the human 
brain  and  see  how  it  deals  with  work  so  efficiently.  And  I  think  part  of  it  is  that  it  is  actually 
reasoning with a lot of what we could call model-based approaches, where it doesn't need a lot of 
data to understand, that if I throw an object up - it's going to come down because it has fundamental 
notions of physics. Similarly, I think, some of the sparse network-based approaches are something 
that, I think, is pretty important that we think we need to pay attention to even just because we 
have power doesn't mean we should go waste it. The human brain, for example, does amazing 
things, even on power, with things like heavy and sort of learning. And when it finds like two 
adjacent  neurons  firing  it  sort  of  short  circuits  them.  So,  I  think  that  certainly  we  should  be 
impressed by GPT-3, but I think it is just a very small stop on a very long journey. And we need 
to  always  keep  these  things  in  perspective.  What  are  we  really  trying  to  solve?  What  are  we 
optimizing for? As I said earlier in the talk, I think if we define that clearly, I think in the next 70 
years, we will make tremendous progress. 

[Albert Efimov] So now, I'm going to you, Michael. Going to my two original questions, I 
would like to say, what was wrong for the last 70 years? By the way, what is the distance between 
Oxford and Cambridge? About 60 miles? 60 miles?  

[Michael Woolbridge] Well, that's the physic the physical distance, the emotional distance is far, 
far greater than that. 

[Albert Efimov] So this emotional distance between two schools actually quite important 
right now. But going back to the question, what was wrong for the last 70 years, except the fact 
that Alan Turing was from Cambridge? 

[Michael Woolbridge] I think there's that there's a couple of things. And I think my answer 
will resonate with what Gary said and, and, and others today. So the first thing is, I think there's 
an  obsession  in  AI  historically,  with  discovering  one  idea,  and  thinking  that  one  idea  that  one 
technique is going to be a magic ingredient, which takes you all the way. And so as you said, 
Albert,  as  Gary  said,  I  mean,  for  a  long  time,  it  was  the  idea  that  knowledge,  knowledge 
representation, we just write, we need the right way to find human knowledge and to write that 
down. And if we can just give that to machines, then that will take us all the way. So I mean, there 
was a famous experiment, the psych experiment, where they tried to code up the entirety of human 
consensus knowledge. And the idea was, if you could give that to a machine, then AI would be 
solved. So it's that idea that there's one kind of one single technique we have one advancement. 
Wow, this is it. This is the magic ingredient. And that's going to take us all the way. And it doesn't, 
right.  I  mean,  it  just  doesn't.  And  I  guess  I'm  very  I'm  hugely  impressed  as  I'm  impressed  as 
12 

 
 
 
 
 
anybody is by the current advances in AI, the things that are going on in deep learning. These are 
real,  and  they  cause  excitement.  I  mean,  I  think  they  are  real  advances.  But  they  are  narrow 
advances. And they are ingredients, but they're not the whole ingredients. They're not going to take 
us the entirety of the way to artificial intelligence. So I think that's the first thing that's the first 
lesson, right? Don't imagine that there's one single ingredient that is going to take you all the way. 
Second,  I  think  it's  quite  interesting  the  examples  we've  seen  an  example  on  your  slide 
things like chess and GO. That why do AI researchers studying problems like chess and go and 
proving  mathematical  theorems.  They  do  it  because  those  are  the  things  that  they  regard  as 
requiring intelligence as requiring genius, right? Those being a good mathematician is something 
that a lot of computer scientists aspire to be. So they look at those problems. And they said, Well, 
if we can solve, if we can prove mathematical theorems with a computer, then we must have solved 
the intelligence problem. But actually, the truth is, that's not where a lot of the hard problems are. 
So to go back to the driverless car problem that Gary was talking about earlier, and I'd be interested 
to see whether Gary disagrees with me or not, what is the hard problem driving driverless cars? Is 
it knowing whether to speed up or slow down or to turn left? Or to turn your indicators on? No, I 
don't think so. The problems with driverless cars are knowing where you are, and what's around 
you what's going on in your environment, right, all of the problems, that my guess anyway is that 
Gary and his team had to solve back in 2005, what to do with perceiving your environment if you 
have a lot of information, then knowing whether to speed up or slow down and so on, is going to 
be easy, actually, that's that part of it, I think, is a relatively straightforward and conventional bit 
of computer code. And so perception, and actually, that's where neural networks, the current wave 
of AI technology, that's where it's turned out to be very, very good at dealing with problems related 
to perception, understanding problems in computer vision, in understanding speech, and so on. 
These were fearsomely difficult problems. And that's where that technology is proved to be very, 
very successful.  

The third thing, I think that the third historical mistake is not understanding that where the 
difficult problems are, in the world. Right? We have evolved over billions of years to succeed in 
inhabiting the physical environment that we inhabit the planet Earth, the narrow bit of planet Earth 
that we do actually inhabit, imagining that you can build a neural network and train it up in the lab 
over a couple of days and that that's going to be as good at dealing with the world as we are, I think 
he's just a huge mistake. So the world is really important. Let's go back to GPT-3. So what is GPT-
3?  GPT-3  is  this  program  developed  by  Open  AI,  I  believe,  and  it  was  trained  by  giving  this 
program huge numbers of texts, vast amounts of written texts. So let's take an example something 
that you might want to do something that Gary might want to do when for breakfast is make an 
omelet, right? The completely routine task for a human being - making an omelet. So GPT-3 is 
read every omelet recipe that's ever been written, right? It's there's no recipe for omelets out there. 
It's written every is read every essay about omelets. It's probably read books about omelets. So in 
terms of just knowledge, it must surely have all the knowledge about omelets that there exists in 
the world. But could it make an omelet? No, of course, it couldn't. It doesn't know anything about 
omelets, because of all the things that exist in the real world. For it, an omelet is just a symbol. But 
it's seen time and time again, in these huge numbers of repositories that have been thrown at it. 
None of that knowledge that is coded into GPT-3, none of that stuff is actually grounded in any 
experience of the world that we all have. For me the word omelet represents every experience I've 
ever had with an omelet, every omelet I've ever tried to make, every egg that I've ever tried to 
break to go into an omelet every successful or bad on, it reminds me of an omelet I had in Paris in 
1997, and so on. Right? In that sense, the concept of omelets means something to me, because it's 
grounded in my experiences with the world. GPT-3 doesn't have any experience of the world to 
ground itself in. So in that respect, it's just a disembodied I mean, it's very quick to be clear, it's an 
incredibly impressive feat of engineering. And people will do really, really cool things with it. But 
it doesn't understand I think just to reiterate the point and I say it doesn't understand because it 
can't, it doesn't have any experience of the world.  

13 

 
So those are the three things. I think we're obsessing on one idea and imagining that there's 
one simple single technique, which is going to take you to kind of the holy grail of AI. That's a 
mistake.  Focusing  on  problems,  which  actually,  you  might  find  impressive  as  requiring 
intelligence in people but actually is not where the real hard problems are. I think that historically 
there has been a mistake. And finally, not realizing the importance of dealing with the world. Now 
experience,  human  experience,  human  knowledge.  Everything  about  the  human  condition  is 
grounded in our experiences in the human world. And I think, too, if we ever succeed in building 
machines that are self-aware and conscious and centered, and all of those things, machines that 
have understanding, then that understanding will have to be grounded in the world in the same 
way. 

[Albert Efimov] Thank you very much, Michael. So going back, to some ideas, which I 
said before that you seem to think I don't know if you see this slide. But Alan Turing says board 
games, talking is very important for artificial intelligence. But he says also there are real human 
pleasures  -  food,  sport,  and  sex.  And  those  are  very  important  actually  for  understanding  the 
problem.  And  my  question  is,  how  important  embodied  intelligence  to  study  perception  and 
cognition in machines? And how important it for research of  general artificial intelligence and 
artificial intelligence with specific applications? So can we create artificial intelligence with no 
embodiment? Or we really need embodied machines to advanced fully in that direction? So who 
would like to answer this question?  

[Leonid Zhukov] I'll be short, I actually want to support Michael, in the sense that to build 
like true artificial intelligence - machines should gain the experience that human has, right? And 
what's the best way to get an experience than to explore the world? And to explore the world? And 
to do it autonomously? Yes, you need to have those types of systems, those embedded systems 
with intelligence. And when you look at them, for example, the Robot-dogs running around outside 
Boston, you realize that we're actually pretty close to the phase where systems we'll be exploring 
the  world  on  their  own,  gaining  the  experience,  learning  what  the  omelet  is,  eventually,  and 
hopefully will not only read about it but try it. 

[Michael  Woolbridge]  I'm  going  to  bring  in  a  slightly  different  sort  of  aspect  to  this 
problem.  So  there's  a  famous  six-word  exchange,  that  was  formulated  by  Steven  Pinker  -  the 
psychologists and linguists. And it goes like this, Bob says: I'm leaving you. Who is she? And so 
the six words, right, and everybody who hears those six words, immediately has a rich mental 
picture of what's going on, as people will understand that right? Maybe somebody listening to this 
actually just lived it last night? I'm very sorry if that's the case. So what's going on there? How do 
we understand that? We understand that because we've got experience of the human world, we 
understand  human  relationships,  human  beliefs,  human  desires.  And  nobody  trained  us  in  that, 
that's just our experience as human beings living as social animals that have relationships with 
people in the human world. Now, this is a big problem for AI. Suppose that you're talking on the 
telephone to an AI system, which decides whether you get a loan or not. And it says: no, you're 
not going to have a loan. It must surely be if it's going to be any use, it has to be able to understand 
that this is going to make you upset or angry, and so on. I mean, an AI system that took the place 
of the human doctor would have to be able to understand the intricacies of human relationships, 
because being a doctor is not just knowing about diseases and illnesses, it's knowing about people. 
And the lengthy training process that doctors go through is every bit as much about training them 
to  deal  with  people  and  to  understand  the  people  that  they're  dealing  with  and  what  kind  of 
treatment  regimes  are  going  to  work  for  them  and  so  on.  As  individuals  understanding  their 
personal circumstances, their relationships, all of that kind of stuff. So we all have that because 
we've been trained both – genetically, through billions of years of evolution, and since birth, where 
our parents teach us right from wrong, teach us about what's acceptable in relationships, and so 
on. We'll learn about that because we're part of the human world. How are we going to give that 
14 

 
 
 
 
stuff to machines? That is big and technical it's called the theory of mind, right? How are we going 
to give machines a theory of mind?  

[Gary  Bradski]  I'm  unclear  whether  embodiment  again,  there  are  many  kinds  of  mind. 
Right. To the extent, we can be disembodied, but embodied in a virtual world, right. So, it can all 
be happening in a cloud, but I do think embodiment does give you certain things which are primary 
tests, like the fear of death, need to survive and that's a lot of what we're based on. We form this 
causal model that's necessary for our existence, and then we press that causal model for everything 
else we think we understand.  

If  we  want  AI's,  intelligence  that  is  going  to  be  useful  to  us  -  they  must  share  certain 
embodiments and the same kinds of things that we do. And it's unclear, how much a giant body of 
associations. GPT-3 it's kind of shocking how far it can carry you and yet there's like no sold in 
the structure, it's just a chain of associations, right between them. It has all these brittle points and 
unexpected  failures  that  are  hard  to  predict.  Versus  with  us  we're  kind  of  regularized  in  our 
environment by our physical need to the understanding of how we would get damaged, how we 
would survive. What's particular with humans, is a large part of our mind is devoted to a social 
world, which is what we created. And so we're always modeling our place in the social world, and 
how to interact and survive with other entities in it. And that's a very key thing for us. In fact, it is 
a large amount of our whole brain and understanding. And again, you see this social world in your 
dreams, you fully simulate other people and your interaction and your hierarchical place with them 
in your dreams. And so a large part of our mind is devoted to that. And I think like if we want 
robots, I mean, we want intelligence, they have to sort of be embodied with us. And if we want 
them for the human factors, there are many things like protein folding, and whatever, that are a 
very specialized area. We just want the machine to do that. Again, such a machine really needs to 
explicitly or implicitly embody a kind of internal causal model of folding, if we really want it not 
to  just  be  associational,  all  to  sort  of  mostly  like  getting  a  lot  of  stuff,  right,  but  not  really 
fundamentally understanding the 3-D chemical world that these things are in, it will just be a kind 
of weird association or manifold over chemistry without the sort of physics and the causal physics 
that it really needs to understand that. In that sense, it should embody a causal model of the world 
upon which it grounds its meaning. And that's what GPT-3 lacks, it lacks any of this grounding of 
meaning. 

The problem isn't that models lack. This is that you want a model that spans your space 
because we're always going to lack. No superintelligence can know everything and every aspect, 
it's always going to be grounded in a causal situation. And when you try to go too far out of that, 
it's just going to break, it won't, you won't be able to expand this mind indefinitely, it will be based 
and built upon a causal structure. And that will be its limits forever. You need a new mind to go 
into some completely new area.  

[Albert Efimov] Okay. So this slide is summarized three ways to build artificial general 
intelligence.  First,  we  discussed  a  lot.  It  is  a  representation  of  a  symbolic  approach  to  the 
description of the world. The second is connectionism, based on basically neural networks, deep 
learning neural networks. The third one is embodied intelligence and a kind of combination of 
maybe two of these. So I would like briefly to any you ask, which approach you to believe the 
most?  

[Leonid Zhukov] The most important approach here, to me right now, in the nearest future 
is  I  believe,  lies  in  these  newer  symbolic  computations,  which  is,  in  some  sense,  connecting 
symbolic representation and connectionism. And to me, that's probably where the next advances 
will happen. And that's the sort of, I think, the most fruitful direction for the near future. Going 
forward, yes, embodiment, definitely will join to actually gain those experience, and the meaning, 
and understanding of the world around us, physical world. 

15 

 
 
 
 
 
[Gary Bradski] Well for AGI first of all, I don't believe in AGI. It's a specific intelligence, 
I don't think we have a general intelligence because we use our internal structures to extend to like, 
what is quanta, what is a particle that goes like a wave and a particle down at the small scale? It 
does not. This is where our metaphor that we have for our brain breaks down. And so I do think 
you  need  all  of  these  things.  I  think  the  symbolic  representations  derive  from  this  kind  of  this 
necessity to build a world model for yourself. And it's those parts that give you the symbols, again, 
like this would extend to people think, well, math is universal. And I go, no, math only exists in 
here. It's not a property of the world. It's human property. And it exists in the human mind, and it 
works for our models. But it's not some magical thing that exists out in the universe that we tap 
into. It's a structure that works in our causal models. And it breaks down like most of the math is 
unknowable to us. Like most things, the vast majority of the universe is inexpressible by even 
unknowable by our functions and symbols, right. But causally at our scale at our time, we have a 
very flexible model that can extend.  

So, the quick answer is, we need all these things, and they'll always be relative to the job 
we want it to solve. So to achieve artificial specific intelligence, that can be done very general 
tasks. But there's no such thing as this universal intelligence. It's always in terms of loud learn 
grounded models, in my opinion. 

[Michael Woolbridge] Yes, like Gary, I am skeptical about AGI and I don't see anything 
on  the  table  at  the  moment,  which  is  going  to  take  us,  to  AGI.  I  think  all  of  those  things  are 
important: connectionism, symbolic representations. I mean, I think it's pretty widely accepted. 
Now that symbolic approaches and connectionism, I think need to talk to each other. I don't think 
the symbolic approaches that were popular 30 years ago are likely to be the way forward. So I 
don't think that's going to be it. But there will be some form of symbolic reasoning that's got to go 
on. And I think what I would say is, the reason I'm a skeptic about AGI is that, again, going back 
to a point I made earlier we are the product of billions of years of reinforcement learning with 
1000s of generations of our ancestors. When Mother Nature has tuned us over that immense period 
of time to be able to be generally intelligent in the world that we're in the current techniques the 
headline techniques like AlphaGo, and the chess-playing programs that are so successful, and, and 
so on. These are all based on reinforcement learning where a program just experiments and gets 
feedback,  it  tries  to  do  something  does  badly  and  in  a  computer  game,  for  example,  right,  a 
computer game like Space Invaders. So you can have a program that learns to play the game of 
Space Invaders, but actually, it does that by just essentially starting by moving randomly seeing 
what works, what doesn't work, when something works, when it gets a good score. It does that, 
again,  that's  reinforcement  learning.  It's  just  over  time  playing  and  playing  and  playing  it.  The 
problem  is,  reinforcement  learning  doesn't  work  in  the  real  world,  Gary  couldn't  have  used 
reinforcement learning to train his cars in 2005, right, because they could have got through a great 
many  cars,  these  cars  wouldn't  have  got  anywhere.  Reinforcement  Learning  doesn't  work.  The 
kind of reinforcement learning which is so successful in games, for example, virtual environments, 
is very, very successful there, but doesn't work in the real world.  

So I'm somewhat of a skeptic about AGI for that reason. I mean, we are the product of 
reinforcement  learning,  but  it's  billions  of  years  of  evolution,  which  have  given  us  a  kind  of 
reinforcement learning to generate human beings that can successfully occupy the human world. 

16 

View publication stats
View publication stats

 
 
 

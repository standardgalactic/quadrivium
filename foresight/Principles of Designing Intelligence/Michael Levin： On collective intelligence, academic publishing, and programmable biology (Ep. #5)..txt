This podcast is about understanding how collaborative learning networks work and can be designed.
It's part of our effort to describe a methodology, which we call Unify, that can help align human
learning principles, which come from learning science, machine learning principles, and
also how to think about organizations as networks, how to be strategic in how networks
are designed, and what data to collect from networks in order to create new kinds of intelligence.
Our next guest is, in my mind, the authority, and I can't believe I got this interview,
in the field of collective intelligence.
If you haven't seen Michael Levin's TED Talk, it's amazing, I highly recommend it.
This conversation did not go as expected, because I got, frankly, a lot of really good
advice from, frankly, someone who was trying to help me out in terms of how can I propel
my work forward, and for that I'm personally grateful.
I hope you enjoy this.
Thank you so much.
Everything we do is related to this question of embodied minds, so I'm interested in how
very diverse kinds of intelligence can exist in our universe, in all sorts of different
manifestations, different scales, both of space and time.
We use a combination of computer science, developmental biology, biophysics, behavioral
science, computer science, to really try to understand how different degrees of agency
can be implemented in different embodiments.
We have parts of the lab are very theoretical, and do conceptual kinds of models in almost
philosophy, and in other parts of the bright code, and produce various tools and software,
and in other parts do various applications of these ideas.
We have applications in birth defects, and regenerative medicine, and cancer, and we
have some things in AI, and synthetic bioengineering, and so on.
We run the spectrum from very fundamental conceptual things to very practical things
that we hope will end up in the clinic at some point.
My background originally was computer science.
I did software engineering for a long time, scientific programming.
I got a degree in genetics after that, and I've been running a biology lab in the Allen
Discovery Center ever since.
Our goal is to create a collaborative intelligence framework, something that is reliable and robust,
a playbook to give to organizational leaders.
We don't believe that the latest AI algorithm or technology trend that needs to be implemented
is the solution.
We believe that there needs to be an understanding of the guiding principles of how do you design
collaborative intelligence systems?
How do you apply them with minimal or no technology?
Because of the lack of formal definitions, we're hoping we can draw inspiration from
the field, biology, and I'm wondering if there are any principles around collective
intelligence that you feel might be applicable to the domain of business?
Well, I guess because I'm not in this field, I could use a little more guidance as to what
the best case outcome would be.
Can you paint a picture for me of what you want to have happen?
What are we trying to improve?
What are we trying to achieve?
How would you recognize success if you cracked this problem?
What would it look like?
We would like to be able to model an organization using a paradigm from reinforcement learning
that doesn't mean we're going to want to use reinforcement learning, but the idea is to
track agents taking actions in their environments and measuring the outcomes in this case that
can be applied to human sequences of decisions or machine or humans and machines working
together collaboratively.
And one problem we have is that we're applying a reinforcement learning way of thinking to
the macro world.
It's not just applying it to code with a set of data.
It's trying to create a virtual twin of an organization and applying that way of thinking
from reinforcement learning to organizational design.
I think success is going to be having a framework and a methodology, but also being able to implement
it in terms of, let's say, an API, a set of standards.
And step one really is to investigate what are some principles around collaborative intelligence
that work that can be applied to organizations.
I think we're really at the beginning and I don't know if we have even defined yet what
success is.
I like to work backwards and I like to imagine the future that you want to see.
So what does that mean?
Does that mean the organization is functioning more efficiently towards specific goals?
Does that mean something about what those goals are?
Does that mean something about the individual happiness of the people participating in those
goals?
Does that mean something about developing some kind of dominant paradigm that pushes out
competitive views of some particular field?
I think step one is figuring out what, if you did, if you already had a successful theory
of all this stuff and you were able to put it into practice, what would the implications
be for reality, for how people run these things in the real world?
And so I don't know what the answer to this is, but I guess I can talk a little bit about
what we see in biology and then I don't love just blindly taking things from biology and
pushing them into, I mean, people often would like to do this, pushing them into social
and various kinds of societal contexts, but I think it's better to work backwards and
ask what we're trying to achieve and what that looks like.
I find in general that's missing from a lot of the discussions.
A lot of people have critiques about things that are going on now, whether it's AI or
whether it's something else, they can see all the problems and it's going to lead to
this, it's going to lead to that, we don't want it.
What I don't see as much as people articulating, what do you want to see?
What does future humanity look like that avoids these kinds of things that people are concerned
about?
With respect to all of the parameters that are currently under debate and I think that's
what I would do here as well as I would ask, what does the future of an optimal successful
organization look like?
How do you recognize it?
What are we aiming for?
But having said all that, I could tell you some things we learned from the biology.
One thing we learned from the biology is that one reason biology is so successful is that
it often changes the goals.
Pretty much like with artificial life and this notion of perverse instantiation, which
is you think you're trying to solve a problem in a particular way and if your system is
flexible enough, it might do something completely different that on retrospect, you can see
how that would solve the problem, but it isn't at all what you were looking for.
Biology does this all the time, when faced with a difficult problem, one thing biology
sometimes does is switch to a different problem.
If you look at the biosphere, there's every possible way of making a living and biology
is never tied to finding an answer to a specific problem.
They often change, it often changes the problem that it tries to solve and so that may or
may not work for us because you may come up with an organizational structure that is
very good at let's say perpetuating itself, but one of the ways that might do that is
to go off, completely go off script as far as where you thought the organization was
going to go and that may not, that works in the biosphere if your goal is to make sure
that life survives in some particular, you know, on a planet, that may not be that ability
to just completely change goals may not be what you want for an organization.
The other thing is that when you, biological agents and those could be cells, they could
be molecular networks, they could be tissues, organs, whatever, they're very good at combining
into higher level entities that are these emergent selves that do things in other problem spaces.
But they're not, there's a few things that are not guaranteed when that happens.
One thing that's not guaranteed is that the emergent self is smarter than the components.
It doesn't have to be, sometimes it certainly is, but it doesn't have to be and so that's
no guarantee, you know, scaling up is no guarantee that it's going to be more intelligent.
Also, when you create these novel selves, you simultaneously create various goals, preferences
and various kinds of salience and valence for these things that don't necessarily match
those of the subunits.
So in other words, people often say, you know, people often hear the story, I tell of gap
junctions and cancer, this idea that cells are tied together with gap junctions that
basically erase their individuality a little bit so that they're all part of this collective
and they work on making the organs and whatnot.
And then when there's a breakdown of that, cells roll back to the kind of unicellular
form and they go off and they metastasize and so on.
And so people hear that and often they say, well, that's clear what we have to do for
the various ills of society, you know, we can address that the whole selfishness thing
and so on.
We'll just sort of gap junctions ourselves, take it to each other in some format and when
wipe our individuality a little bit and we'll be this collective.
I think that's a, in general, that's a terrible idea because one of the clear things is that
when you as a large scale individual, let's say a human goes off and does various things.
My silly example is rock climbing.
You know, you go rock climbing and you have a lovely day and you meet various social goals,
you meet various personal happiness goals and so on as a human.
But you've left a bunch of skin cells on that, you know, on that rock.
And nobody asked those skin cells whether this was something that they were interested
in.
And just in general, the basic premise of having a multicellular body is that the vast majority
of your body cells are going to die in the service of whatever weird to them incomprehensible
goals that the collective human might have.
So that stuff is very easily scaled up into social structures where you might have persistent,
you know, persistent kinds of government, persistent kinds of companies and so on that
pursue goals that are at odds with the well-being, the happiness and the goals of the beings
that make them up.
And that needs to be, it needs to be clear that when you scale up like that, you are
not guaranteed.
In fact, the big thing is that we don't have a good science of being able to anticipate
the goals of collective systems.
We really can't anticipate that very well now at all.
I talk about these biology examples all the time in terms of like, we make something called
a frog allotle in our lab and so this is part frog and part axolotl.
And so baby axolotls have little forelegs and tadpoles don't have forelegs.
And so when we make this frog allotle, I often say this in my talk, you know, we know the
genome for the frog, we know the genome for the axolotl.
Can you tell me if a frog allotle is going to have legs or not, right?
And there's no one can predict that up front.
And that's because while we understand the molecular hardware to some extent of the
pieces, we do not understand how the collective decisions are made.
We don't know how they decide to make these large scale structures and to say that, you
know, there's a million examples of that.
So that's the thing when we make these, when we make these collectives, we really don't
know what the goal of the, if there is going to be an emergent agent, we don't know what
the goal is going to be.
And we don't know if it at all matches our goals, even if we, I mean, even our own, we
don't match goals with each other, but never mind, or even if never mind that.
Yeah.
So, so I think, so I think that's the danger, right?
So I think biology can teach us some very successful principles about making sure you
stick around, but I'm not sure that's the end all and be all that we want from the structures
that we build, right?
Persistence isn't really what we're looking for.
We're looking for something much more specific.
So you got to, you got to take all that biology stuff with a, with a grain of salt.
So one of my favorite things as a consultant is that I get to speak with individuals where
very aware of their problems, very aware of goals and the financial value of their goals.
But when it comes time to connect the dots between these different teams and more of
a collective intelligence or collaborative intelligence approach, typically there's
no person in charge or there's no system or process or computational way that combines
them all and analyzes them.
And I found this gap to be really intriguing and I want to pursue formalizing a process
that could be something that we could publish and like, let's say an academic journal or
paper to help push this area of research forward.
And I think the one challenge I've had is like simply defining, you know, what is collective
intelligence for an organization, what is collaborative intelligence, because I hear
different people, different startups talking about those two and, and yet it's all very
fuzzy.
So I thought that trying to pursue a more formalized definition that respected kind
of academic perspectives would be a good approach for us to have.
Yeah.
You know, I don't know.
That's that's a whole other thing.
This this like meta meta goal seeking, which is what what should the goals be?
So so gaining respect of the academic community.
I'm not even sure that's an achievable goal per se for anybody because there is no monolithic
academic community.
So there are different islands of academics where like so so so there's a there's a particular
kind of standard talk that I sometimes give and I always know what kind of department
I'm giving the talking based on which part it makes people angry and it's often a different
it's always a different part because there are certain things you can say in a in a neurobiology
department and everybody's like, yeah, no kidding.
So what we knew that and the exact same thing in the genetics department and people throw
tomatoes.
And so trying for kind of generic acceptance, I'm not sure that's feasible until until we
get a lot better at breaking down these these kind of conceptual silos that the communities
are in.
And also, you know, what's the what's the payoff of that?
You know, you want you what I mean, what do we actually what you know, what do we actually
want?
Why we write these scientific papers on what what what is the impact?
I mean, I think there's been a lot of talk of impact in the last decade, maybe more.
And it's on the one hand, I think it's terrible.
And on the other hand, I think it's really good.
It has good kind of effects, which which really forces us to think about what do you really
want to have happen when you write that paper, right, besides the kind of obvious stuff of,
you know, tenure or whatever, you know, get a job or you know, get a good position or
whatever.
But like long term, the change you're trying to make in the world, what happens when when
you write these papers?
So in theory, I mean, what are some possible answers to that?
Sometimes people read it and they get inspired and they do something else.
So that's cool.
But maybe you're better off just doing that other thing yourself directly, right?
That's one possibility.
Another possibility is that maybe you can unify people across fields so that they sort
of bring some interdisciplinary thought to that.
So that's so that's pretty good.
That's that's, and I don't know if that's gaining acceptance, that's more gaining interest,
right?
So you want people to start thinking about things and asking questions that of things
that they may have taken on, you know, as an assumption before that.
So that's pretty good.
But you know, the actual impact of academic papers, even in academia is often not clear,
but outside of academia.
I don't know.
I mean, you know, Chris Fields had a great sentence recently where he said, arguments
are only settled by technologies.
And I think that's I think that's true.
I think I think we can have all kinds of abstruse academic arguments about things and
people have all kinds of commitments to stuff.
But in the end, if the idea is a good one, it's going to the rubber is going to hit the
you hit the road and you're going to make some sort of impact on the physical world.
And there's going to be some sort of application, some sort of practical thing.
And then it's going to be irrelevant, which part of academia likes it and which part doesn't.
So I don't know, you know, I think I think you got it again, it's about working backwards
to ask what's the change that you want to see and how much of that change is requires
I am buy in from academia, which part of academia, whether papers are sufficient.
And there's a whole other thing, which is that there are so many papers now that I,
you know, most people don't have time to even catch up with the things in their field anymore.
Nevermind, you know, sort of other things.
I sometimes get the feeling that all these papers were writing, but not really for us.
They're for some sort of future AI or some sort of augmented future scientists who's
actually going to be able to have the bandwidth to read all this stuff.
Because I mean, I don't know, you can't see it, but I've got behind my chair,
I've got these like stacks of papers that I'm supposed to have read by now.
And they're just getting bigger and bigger because there's no time to do it.
So yeah, I would I would work backwards and I would think about whose eyeballs are we
trying to capture here, right on these papers and what's the what's the what's the goal of that, you know.
This conversation did not go at all like I was expecting.
I am really grateful that you challenged me on some of my current beliefs around pursuing
an academic approach to defining collective intelligence or collaborative intelligence
from kind of a theoretical and academic perspective for organizations to implement.
I have to do a lot of thinking on this.
And as I reflect, the questions that come to mind as I was listening were,
how does your lab pursue success?
And how does your lab think about translating theory into application?
I view academia serving a higher purpose to make the world a better place through creation
and curation and sharing of knowledge.
And we'd love to hear some of what you've learned works.
First, we generate a lot of very basic fundamental knowledge and also conceptual apparatus.
So we come up with very sort of fundamentally almost philosophical like like perspectives
on things and then that gets cashed out as software or various computational paradigms
and then that gets cashed out in new experiments and new new capabilities and so on.
And I firmly believe that to the extent that we're on the right track with any of this stuff,
really kind of transformative applications should follow.
And this is why we now have a spin off company doing regeneration and we have one doing computer
AI designed synthetic living machines and there's another one on cancer.
So like this stuff is moving towards, hopefully towards the clinic.
So we're not in humans patients yet by any means, but we're going in that direction.
And the other thing too is that I kind of try to strike a balance because I'm not even a clinician.
OK, I don't do clinical work at all.
I get emails and phone calls every day from people with the most unbelievable medical needs.
I mean, you just can't imagine what's out there, the need that's out there and the things that happen
to people from all the way from birth defects to do various other things that happen in your lifetime.
The need is intent.
And so I consider it, you know, I would not consider it a success if all we ever did was,
you know, generate some kind of, you know, conceptual stuff and basic science that never helped patients.
I want to see, I want to see in my lifetime Indiana World, we'll see if that happens or not.
But in my lifetime, I want to see actual patients help with the stuff that we do.
So it's kind of a it's kind of a two pronged attack.
And, you know, you can always argue about what the right prioritization is, you know,
and some people say, you know, don't get tied up with this with the with the stuff with these,
you know, spending time with these companies and figuring out how to, you know,
make products and argue with the FDA.
You know, you should be spending your time on basic basic science.
And then other people write me and they say, and they say the opposite.
Like, what are you doing spending time figuring out, you know, things about the nature of the self?
Like, you could have, you know, you should be solving cancer.
And and and so, you know, you kind of you kind of juggle those two things.
But I think I think I think we're pretty fortunate in my group that I think we can do some of both,
actually, and I think they're very tightly related.
I think if you do it right, you can make impacts on both.
So that's how I see it.
I'm imagining the world you see as we will be able to regrow an arm.
If someone were an accident and were to lose it, is that correct?
Embodiment that you got through the vagaries of genetics and and evolution.
You could change it the way the way we change many other things.
You want tentacles, you want you want to see an infrared, you want to live underwater?
Like, why not? Right?
Why not? Who said who said this this random walk that evolution took to to get you here is how it has to stay?
I think down the line, I mean, you know, without like scaring everybody and so on,
because a lot of people find that pretty freaky.
But but I'm certainly not the first person to say stuff like this down the line.
I believe in maximizing freedom, which includes freedom of embodiment.
You should not be locked into some random configuration that's susceptible to weird diseases
and aging and and and you know, dumb stuff that can happen to you from stepping on the wrong,
you know, patch of ground, you know, it's all the stuff is just it's it's it's the way the way,
you know, we we we we started out with antibiotics and, you know,
and wearing clothes against the cold and things like that.
And that was it after that.
Like it became completely obvious that we do not have to stay the way we came into the world.
So so amazing future.
I sign up for that vision.
I think that sounds amazing.
That's amazing.
And to me, it seems like it's leaving the boundaries of what does it even mean to be human?
Yeah, no, that's I mean, so two things, right?
One is that I don't I don't think these are boundaries at all.
There are no boundaries.
The boundary is us not knowing how to work the interface.
So if if you're given a calculator and you have no idea what this thing is or how to use it,
there's a real boundary between you and the capabilities of that calculator.
But it's not real.
It's it's it's it's a boundary of ignorance and and and those things are are are are
improvable, right?
So you can we can we can we can do that now.
We have a process, the scientific process and some other stuff that that can help us get through that.
So I don't believe in these boundaries at all.
Then, you know, and then and and not only, I mean, let's let's let's face it.
We we haven't believed in these boundaries for a long time.
If you you walk into any gym or, you know, a martial arts studio or a call or a university
and you see people removing their boundaries, right?
You don't come out of there the way you went in.
You come out with extra powers.
You come out with whether that be new brain power or muscles or or you know,
you learn to swim and hold your breath underwater or whatever you know,
whatever you're going to learn, we know we can improve ourselves and the only limitation.
And frankly, some people are amazing and they've pushed it even without all this technology
that I'm talking about.
They've already pushed us.
I mean, right, we've all seen there are there are humans that are so on some particular
thing that they've dedicated the life to.
They're so far outside the mean that it's just it's it's unbelievable that even that that's
even possible.
And that's and that's without knowing, frankly, very much at all about about how biology works yet.
So so whether mental, whether physical, whether spiritual, I think that's important.
There are there are no boundaries.
This is all the all these boundaries are defined by by our own ignorance.
And then the other important thing you said is, are we leaving what it means to be human?
So that's an interesting question.
What do we mean when we say human, in particular, because of this whole AI thing,
somebody I forget who it is, but somebody's working on these proof of humanity certificates,
right?
And it's a good it's a good thing to think about.
If if somebody if if you're interacting with somebody and they and they show you their their
little stamp or whatever it's going to be, that that's the proof of humanity.
What is it that you really are looking for?
Right?
When when you're looking for that proof of humanity in someone or when you want to say
whether someone's left, you know, kind of the human category, what are you really looking for?
Are you looking to validate that their DNA is the same that evolution left us in as homo sapiens?
I don't think so.
Do you really care about anybody's DNA?
I don't.
I don't think that's that's relevant for anything.
Is it is it body structure?
Are you do you want confirmation that that that that person hasn't had some percentage
of their organs replaced by various prosthetics?
I don't care about that either, right?
Does that matter to you?
I don't know.
So what are so what is that when we say somebody's human?
What do we actually mean?
And I think and I'm sure other people have different different, you know, different definitions for it.
But I think what you mean is a minimal level of compassion.
That's what I think.
What you're looking for when somebody says they're human, you're looking for a cognitive
light cone on that individual that is able to actively able to care at least maybe more,
but at least to the same level that you can about others, about various, you know, about
various goals that when when if, you know, if I am trying to choose, you know,
say you're going to go live on Mars or something and you're choosing a companion,
and what you want in that proof of humanity is not anything but their DNA or, you know,
whether they have some prosthetic organs or something.
What you want is do they have the capacity to care about the same stuff that I care about
the same degree of stuff?
Because if they don't, if they're a rumba or, you know, or a cat or something else,
nice, but but not the same relationship you can have with a human, right?
So so that's what I think humans are.
And after that, everything is everything else is for is up for grabs.
You've got gills, you've got, you know, your third brain hemisphere with like whatever,
you know, who cares?
So that's my that's my view of it.
I mean, going macro, I think from that, sorry, Ron, but I think it's like one thing that was
like thinking about because you're talking about these, these, I guess, like boundaries or like
what is our like predefined view of what it means to be human.
And I think if you take even that macro to the stuff that, for example, Ron and I are working on,
is that we are working so hard to try to map out organizations to try to understand organizations.
But what does even an organization mean?
Yeah, are we, are we actually thinking about organizations in our preconceived ways of
thinking about them and not thinking what an organization could be?
It's like, are we just mapping the status quo, whereas we could
helping organizations evolve in the same way that you're saying humans can evolve way
farther than everybody's thinking about now.
And I think that's a very interesting, you know, angle.
What do you think, Ron?
That's a fascinating question.
Level one, let's say, is what are the goals of an organization just listing them out
among the individual nodes?
Then you could say level two is what are the connections between what people care about
and how are they mapped together?
And then level three is, well, how are they adapting and changing together?
I think that's new for me to think about from this conversation.
And I don't know how to answer your question, Juan, because this is when we talk about
adapting our biology and thinking about an organization, like being able to
adapt an organization like we would DNA, it's all new for me to think about.
And the thing with goals, too, is they can be explicit or not.
So like any psychoanalyst will tell you that you think you have goals.
They may or may not be the actual goals that drive your behavior, right?
We all know we have all kinds of internal modules that are trying to maximize and
minimize various things that we don't necessarily have direct access to that may
or may not be adaptive in our context and so on.
So yeah, in the future, so I guess, I don't know anything about this field, but if I had
to guess in the future, I sort of envisioned some kind of psychoanalysis of organizations where
there's some way of kind of like we had this project where we were trying to
communicate with an ant colony, not the ants, the colony, which is a completely different
thing. It's very hard. But at some point, there might be techniques, the way that we try to talk
to organs instead of the individual cells and so on. There might be ways to find out what does
the actual organization want? And the individual people will tell you, oh, well, I know there's
these set of goals. You don't know anything anymore than if you ask you the cells or the organs in
your body, what are the goals of the organism? They'll tell you some stuff about physiological
parameters, boundary parameters and some things like that. But they can't even begin to guess
if your goal of whatever, going to grad school four years from now, they can't fathom it.
So who knows what the organization is going to want. So in the future, there might be some,
I think we need it. I think if we're going to survive and as a mature species, we're going to
have to get our science around being able to identify, characterize, relate to
and modify these kind of emergent, very unconventional emergent agents,
which are anything from organizations to evolutionary lineages and things like that.
I know you have a hard stop. So three minutes left. There's something you said, which just
my ears perked up. Can you just spend a couple of minutes? When you say you want to speak to the
ant colony itself and not the ant, that's an amazing visualization. My brain is so curious
about, can you just describe more what that means? Sure. Yeah. And this is, you know, I'm
by definitely not the first person to talk about this. Doug Hofstadter in his book,
Gert Lecherbach had some great thoughts about it. And before that, in the 20s, Eugene Marais
had this book called The Soul of the White Ant, which is just amazing about the kind of the life
of the colony that's distinct from the individual members. So this idea that if we take collective
intelligence seriously and this idea that the colony works in different problem spaces than
the individual ants and they have, it has a memory of those things that no individual ant knows
and things like that. So we wanted to communicate with it. And we took a very simplistic approach,
which is that training is a simple kind of communication. So if we train you to do something,
then we've communicated the fact that something is good and something else is bad. So you know,
one act of communication has taken place, very minimal. So what we tried to do, and we haven't,
this is something that started right during the pandemic, so it never like really got off the
ground, but we'll, you know, we'll get to it eventually, is just imagine a colony and there's
a location where little drops of food get dropped by a computerized system. But the amount of food
that gets dropped there is proportional to the number of ants standing on a platform
at the other end of the colony. So what that means is that no individual ant ever has the
experience of I stand here and then I collect my reward. No ant has that experience. So now,
if we find over time with training that the colony sends a bunch of ants over in this direction to
pick up the food over here, that's a fact, that's a conditioning, that's a bit of learning that
was done by the colony by no individual ant. And you know, it sounds all crazy or anything, but
you know, when you train a rat to push a lever and get a pellet, the cells at the bottom of
the foot are what's interacting with the button and the cells that get the reward are in the
intestine. No individual cell has both experiences. So we've got this thing we call a rat, which is
this collective that's able to do the credit assignment and figure out that these two things
are related. That's the collective intelligence. We're all collective intelligences. And so we
tried to communicate with the hand colony that way. So stay tuned. I don't know if that's going
to work or not. Mind is blown right now. Thank you so much. Cool. Thank you. Yeah. It's a fun
conversation. I appreciate it. Cheers. Thank you so much. Very nice to meet you. Thank you.

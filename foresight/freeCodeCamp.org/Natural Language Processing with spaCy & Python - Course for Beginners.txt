In this course, you will learn all about natural language processing and how to apply it to
real-world problems using the Spacey Library.
Dr. Mattingly is extremely knowledgeable in this area, and he's an excellent teacher.
Hi, and welcome to this video.
My name is Dr. William Mattingly, and I specialize in multilingual natural language processing.
I come to NLP from a humanities perspective.
I have my PhD in medieval history.
But I use Spacey on a regular basis to do all of my NLP needs.
So what you're going to get out of this video over the next few hours is a basic understanding
of what natural language processing is, or NLP, and also how to apply it to domain-specific
problems, or problems that exist within your own area of expertise.
I happen to use this all the time to analyze historical documents or financial documents
for my own personal investments.
Over the next few hours, you're going to learn a lot about NLP, language as a whole,
and most importantly, the Spacey Library.
I like the Spacey Library because it's easy to use and easy to also implement really kind
of general solutions to general problems with the off-the-shelf models that are already
available to you.
I'm going to walk you through, in part one of this video series, how to get the most
out of Spacey with these off-the-shelf features.
In part two, we're going to start tackling some of the features that don't exist in
off-the-shelf models, and I'm going to show you how to use rules-based pipes, or components
in Spacey, to actually solve domain-specific problems in your own area, from the entity
ruler to the matcher, to actually injecting robust, complex, regular expression, or rejects
patterns in a custom Spacey component that doesn't actually exist at the moment.
I'm going to be showing you all that in part two, so that in part three, we can take the
lessons that we learned in part one and part two, and actually apply them to solve a very
kind of common problem that exists in an LP, and that is information extraction from financial
documents.
So finding things that are of relevance, such as stocks, markets, indexes, and stock exchanges.
If you join me over the next few hours, you will leave this lesson with a good understanding
of Spacey, and also a good understanding of kind of the off-the-shelf components that
are there, and a way to take the off-the-shelf components and apply them to your own domain.
If you also join me in this video and you like it, please let me know in the comments
down below, because I am interested in making a second part to this video that will explore
not only the rules-based aspects of Spacey, but the machine learning-based aspects of
Spacey.
So teaching you how to train your own models to do your own things, such as training a
dependency parser, training a named entity recognizer, things like this, which are not
covered in this video.
Nevertheless, if you join me for this one and you like it, you will find part two much
easier to understand.
So sit back, relax, and let's jump into what NLP is, what kind of things you can do with
NLP, such as information extraction, and what the Spacey library is, and how this course
will be laid out.
If you liked this video, also consider subscribing to my channel, Python Tutorials for Digital
Humanities, which is linked in the description down below.
Even if you're not a digital humanist like me, you will find these Python tutorials useful
because they take Python and make it accessible to students of all levels, specifically those
who are beginners.
I walk you through not only the basics of Python, but also I walk you through step-by-step
some of the more common libraries that you need.
A lot of the channel deals with texts or text-based problems, but other content deals with things
like machine learning and image classification and OCR, all in Python.
So before we begin with Spacey, I think we should spend a little bit of time talking
about what NLP or natural language processing actually is.
Natural language processing is the process by which we try to get a computer system to
understand and parse and extract human language, often times with raw text.
There are a couple different areas of natural language processing.
There's named entity recognition, part of speech tagging, syntactic parsing, text categorization,
also known as text classification, co-reference resolution, machine translation.
Adjacent to NLP is another kind of computational linguistics field called natural language
understanding, or NLU.
This is where we train computer systems to do things like relation extraction, semantic
parsing, question and answering, summarization, sentiment analysis, and paraphrasing.
NLP and NLU are used by a wide array of industries, from finance industry all the way through
to law and academia, with researchers trying to do information extraction from texts.
Within NLP, there's a couple different applications.
The first and probably the most important is information extraction.
This is the process by which we try to get a computer system to extract information that
we find relevant to our own research or needs.
So for example, as we're going to see in part three of this video, when we apply spacey
to the financial sector, a person interested in finances might need NLP to go through and
extract things like company names, stocks, indexes.
Things that are referenced within maybe news articles, from Reuters to New York Times to
Wall Street Journal.
This is an example of using NLP to extract information.
A good way to think about NLP's application in this area is it takes in some unstructured
data, in this case raw text, and extracts structured data from it, or metadata.
So it finds the things that you want it to find and extracts them for you.
Now while there's ways to do this with gazetteers and list matching, using an NLP framework
like spacey, which I'll talk about in just a second, has certain advantages.
The main one being that you can use and leverage things that have been parsed syntactically
or semantically.
So things like the part of speech of a word, things like its dependencies, things like
its co-reference.
These are things that the spacey framework allow for you to do off the shelf and also
train into machine learning models and work into pipelines with rules.
So that's kind of one aspect of NLP and one way it's used.
Another way it's used is to read in data and classify it.
This is known as text categorization and we see that on the left hand side of this image.
Text categorization or text classification, and we conclude in this sentiment analysis
for the most part as well, is a way we take information into a computer system, again unstructured
data, a raw text, and we classify it in some way.
You've actually seen this at work for many decades now with spam detection.
Spam detection is nearly perfect.
It needs to be continually updated, but for the most part it is a solved problem.
The reason why you have emails that automatically go to your spam folder is because there's
a machine learning model that sits on the background of your, on the back end of your
email server.
And what it does is it actually looks at the emails, it sees if it fits the pattern for
what it's seen as spam before, and it assigns it a spam label.
This is known as classification.
This is also used by researchers, especially in the legal industry.
Lawyers oftentimes receive hundreds of thousands of documents, if not millions of documents.
They don't necessarily have the human time to go through and analyze every single document
verbatim.
It is important to kind of get a quick umbrella sense of the documents without actually having
to go through and read them page by page.
And so what lawyers will oftentimes do is use NLP to do classification and information
extraction.
They will find keywords that are relevant to their case, or they will find documents
that are classified according to the relevant fields of their case.
And that way they can take a million documents and reduce it down to maybe only a handful,
maybe a thousand that they have to read verbatim.
This is a real world application of NLP or natural language processing, and both of these
tasks can be achieved through the SPACI framework.
SPACI is a framework for doing NLP.
Right now, as of 2021, it's only available, I believe, in Python.
I think there is a community that's working on an application with R, but I don't know
that for certain.
But SPACI is one of many NLP frameworks that Python has available.
If you're interested in looking at all of them, you can explore things like NLDK, the
Natural Language Toolkit, Stanza, which I believe is coming out of the same program
at Stanford.
There's many out there, but I find SPACI to be the best of all of them for a couple
different reasons.
Reason one is that they provide for you off-the-shelf models that benchmark very well, meaning they
perform very quickly, and they also have very good accuracy metrics, such as precision,
recall, and f-score.
And I'm not going to talk too much about the way we measure machine learning accuracy
right now, but know that they are quite good.
Second, SPACI has the ability to leverage current natural language processing methods,
specifically, transformer models, also known usually kind of collectively as BERT models,
even though that's not entirely accurate.
And it allows for you to use an off-the-shelf transformer model.
And third, it provides the framework for doing custom training relatively easily compared
to these other NLP frameworks that are out there.
Finally, the fourth reason why I picked SPACI over other NLP frameworks is because it scales
well.
SPACI was designed by ExplosionAI, and the entire purpose of SPACI is to work at scale.
By at scale, we mean working with large quantities of documents efficiently, effectively, and
accurately.
SPACI scales well because it can process hundreds of thousands of documents with relative ease
in a relative short period of time, especially if you stick with more rules-based pipes,
which we're going to talk about in part two of this video.
So those are the two things you really need to know about NLP and SPACI in general.
We're going to talk about SPACI in-depth as we explore it both through this video and
in the free textbook I provide to go along with this video, which is located at spacy.pythonhumanities.com,
and it should be linked in the description down below.
This video and the textbook are meant to work in tandem.
Some stuff that I cover in the video might not necessarily be in the textbook because
it doesn't lend itself well to text representation, and the same goes for the opposite.
Some stuff that I don't have the time to cover verbatim in this video I cover in a
little bit more depth in the book.
I think that you should try to use both of these.
What I would recommend is doing one pass through this whole video, watch it in its entirety,
and get an umbrella sense of everything that SPACI can do and everything that we're going
to cover.
I would then go back and try to replicate each stage of this process on a separate window
or on a separate screen and try to kind of follow along in code, and then I would go
back through a third time and try to watch the first part where I talk about what we're
going to be doing and try to do it on your own without looking at the textbook or the
video.
If you can do that by your third pass, you'll be in very good shape to start using SPACI
to solve your own domain specific problems.
NLP is a complex field, and applying NLP is really complex, but fortunately frameworks
like SPACI make this project and this process a lot easier.
I encourage you to spend a few hours in this video, get to know SPACI, and I think you're
going to find that you can do things that you didn't think possible in relative short
order.
So sit back, relax, and enjoy this video series on SPACI.
In order to use SPACI, you're first going to have to install SPACI.
Now there's a few different ways to do this depending on your environment and your operating
system.
I recommend going to SPACI.io backslash usage and kind of enter in the correct framework
that you're working with.
So if you're using Mac OS versus Windows versus Linux, you can go through and in this very
handy kind of user interface, you can go through and select the different features that matter
most to you.
I'm working with Windows, I'm going to be using PIP in this case, and I'm going to be
doing everything on the CPU and I'm going to be working with English.
So I've established all of those different parameters, and it goes through and it tells
me exactly how to go through and install it using PIP in the terminal.
So I encourage you to go through pause the video right now, go ahead and install Windows
however you want to, I'm going to be walking through how to install it within the Jupyter
notebook that we're going to be moving to in just a second.
I want you to not work with the GPU at all.
Working with Spacey on the GPU requires a lot more understanding about what the GPU
is used for, specifically in training machine learning models.
It requires you to have CUDA installed correctly.
It requires a couple other things that I don't really have the time to get into in this video,
but we'll be addressing in a more advanced Spacey tutorial video.
So for right now, I recommend selecting your OS, selecting either going to use PIP or Kanda,
and then selecting CPU and since you're going to be working through this video with English
texts, I encourage you to select English right now and go ahead and just install or download
the Ncore Web SM model.
This is the small model.
I'll talk about that in just a second.
So the first thing we're going to do in our Jupyter notebook is we are going to be using
the exclamation mark to delineate in the cell that this is a terminal command.
We're going to say PIP install Spacey.
Your output when you execute this cell is going to look a little different than mine.
I already have Spacey installed in this environment.
And so mine kind of goes through and looks like this yours will actually go through.
And instead of saying requirement already satisfied, it'll be actually passing out the the different
things that it's actually installing to install Spacey and all of its dependencies.
The next thing that you're going to do is you're going to again, you follow the instructions
and you're going to be doing Python dash M space Spacey space download and then the model
that you want to download.
So let's go ahead and do that right now.
So let's go ahead and say Python M Spacey download.
So this is a Spacey terminal command.
And we're going to download the Ncore Web SM.
And again, I already have this model downloaded.
So on my end, Spacey is going to look a little differently than as it's going to look on your
end as it prints off on the Jupyter notebook.
And if we give it a just a second, everything will go through and it says that it's collected
it, it's downloading it and we are all very happy now.
And so now that we've got Spacey installed correctly, and that we've got the small model
downloaded correctly, we can go ahead and start actually using Spacey and make sure
everything's correct.
The first thing we're going to do is we're going to import the Spacey library as you
would with any other Python library.
If you're not familiar with this, a library is simply a set of classes and functions that
you can import into a Python script so that you don't have to write a whole bunch of extra
code.
Libraries are massive collections of classes and functions that you can call.
So when we import Spacey, we're importing the whole library of Spacey.
And now that we've seen something like this, we know that Spacey has imported correctly,
as long as you're not getting an error message, everything was imported fine.
The next thing that we need to do is we want to make sure that our English Core Web SM,
our small English model, was downloaded correctly.
So the next thing that we need to do is we need to create an NLP object.
I'm going to be talking a lot more about this as we move forward right now.
This is just troubleshooting to make sure that we've installed Spacey correctly and
we've downloaded our model correctly.
So we're going to use the spacey.load command.
This is going to take one argument.
It's going to be a string that is going to correspond to the model that you've installed.
In this case, N Core Web SM.
And if you execute this cell and you have no errors, you have successfully installed
Spacey correctly and you've downloaded the English Core Web SM model correctly.
So go ahead, take time and get all this stuff set up, pause the video if you need to and
then pop back and we're going to start actually working through the basics of Spacey.
I'm now going to move into kind of an overview of kind of what's within Spacey, why it's
useful and kind of some of the basic features of it that you need to be familiar with.
And I'm going to be working from the Jupyter Notebook that I talked about in the introduction
to this video.
If we scroll down to the bottom of chapter one, the basics of Spacey and you get past
the install section, you get to this section on containers.
So what are containers?
Well, containers within Spacey are objects that contain a large quantity of data about
a text.
There are several different containers that you can work with in Spacey.
There's the doc, the doc bin, example, language, lexeme, span, span group and token.
We're going to be dealing with the lexeme a little bit in this video series and we're
going to be dealing with the language container a little bit in this video series, but really
the three big things that we're going to be talking about again and again is the doc,
the span and the token.
And I think when you first come to Spacey, there's a little bit of a learning curve about
what these things are, what they do, how they are structured hierarchically.
And for that reason, I've created this, in my opinion, kind of easy to understand image
of what different containers are.
So if you think about what Spacey is as a pyramid, so a hierarchical system, we've
got all these different containers structured around really the doc object.
Your doc container or your doc object contains a whole bunch of metadata about the text
that you pass to the Spacey pipeline, which we're going to see in practice.
In just a few minutes, the doc object contains a bunch of different things.
It contains attributes.
These attributes can be things like sentences.
So if you iterate over doc.sense, you can actually access all the different sentences
found within that doc object.
If you iterate over each individual item or index in your doc object, you can get individual
tokens.
Tokens are going to be things like words or punctuation marks.
Anything within your sentence or text that has a self contained important value, either
syntactically or semantically.
So this is going to be things like words, a comma, a period, a semi colon, a quotation
mark, things like this, these are all going to be your tokens.
And we're going to see how tokens are a little different than just splitting words up with
traditional string methods and Python.
The next thing that you should be kind of familiar with are spans.
So spans are important because they kind of exist within and without of the doc object.
So unlike the token, which is an index of the doc object, a span can be a token itself,
but it can also be a sequence of multiple tokens.
We're going to see that at play.
So imagine if you had a span in its category, maybe group one are our places.
So a single token might be like a city like Berlin.
But span group two, this could be something like full proper names.
So of a people, for example, so this could be like as we're going to see Martin Luther
King.
This would be a sequence of tokens, a sequence of three different items in the sentence that
make up one span or one self contained item.
So Martin Luther King would be a person who's a collection of a sequence of individual tokens.
If that doesn't make sense right now, this image will be reinforced as we go through
and learn more about spacey in practice.
For right now, I want you to be just understanding that the doc object is the thing around which
all of spacey sits.
This is going to be the object that you create.
This is going to be the object that contains all the metadata that you need to access.
And this is going to be the object that you try to essentially improve with different
custom components, factories and pipelines as you go through and do more advanced things
with spacey.
We're going to now see in just a few seconds how that doc object is kind of similar to the
text itself, but how it's very, very different and much more powerful.
We're now going to be moving on to chapter two of this textbook, which is going to deal
with kind of getting used to the in depth features of spacey.
If you want to pause the video or keep this notebook or this book open up kind of separate
from this video and follow along as we go through and explore it in live coding.
We're going to be talking about a few different things as we explore chapter two.
This will be a lot longer than chapter one.
We're going to be not only importing spacey, but actually going through and loading up
a model, creating a doc object around that model so that we're going to work with the
doc container and practice.
And then we're going to see how that doc container stores a lot of different features
or metadata or attributes about the text.
And while they look the same on the surface, they're actually quite different.
So let's go ahead and work within our same Jupiter notebook where we've imported spacey
and we have already created the NLP object.
The first thing that I want to do is I want to open up a text to start working with within
this repo.
We've got a data folder within this data sub folder.
I've got a couple of different Wikipedia openings.
I've got one on MLK that we're going to be using a little later in this video and I have
one on the United States.
This is wiki underscore us.
That's going to be what we work with right now.
So let's use our with operator and open up data backslash wiki underscore us dot txt.
We're going to just read that in as F and then we're going to create this text object,
which is going to be equal to F dot read.
And now that we've got our text object created, let's go ahead and see what this looks like.
So let's print text.
Then we see that it's a standard Wikipedia article kind of follows that same introductory
format and it's about four or five paragraphs long with a lot of the features left in such
as the brackets that delineate some kind of a footnote.
We're not going to worry too much about cleaning this up right now because we're interested
not with cleaning our data so much as just starting to work with the doc object in spacey.
So the first thing that you want to do is you're going to want to create a doc object.
It is oftentimes good practice if you're only ever working with one doc object in your
script to just call your only object doc.
If you're working with multiple objects, sometimes you'll say doc one doc two doc three
or give it some kind of specific name so that your variables can be unique and easily identifiable
later in your script.
Since we're just working with one doc object right now, we're going to say doc is equal
to NLP.
So this is going to call our NLP model that we imported earlier in this case the English
Core Web SM model.
And that's going to for right now just take one argument and that's going to be the text
itself.
So the text object, if you execute that cell, you should have a doc object now created.
Let's print off that doc object and see what it looks like.
And if you scroll down, you might be thinking to yourself, this looks very, very similar
if not identical to what I just saw a second ago.
And in fact, on the surface, it is very similar to that text object that we gave to the NLP
model or pipeline, but let's see how they're different.
Let's print off the length of text.
And let's print off the length of the doc object.
And what we have here are two different numbers.
Our text is 3525 and our doc object is 152.
What is going on here?
Well, let's get a sense by trying to iterate over the text object and iterating over the
doc object with a simple for loop.
So we're going to say for token and text, so we're going to iterate first over that
text object, we're going to print off the token.
So the first 10 indices, and we get individual letters as one might expect.
But when we do something the same thing with the doc object, let's go ahead and start writing
this out.
We're going to say for token and doc, we're going to iterate over the first 10.
We're going to print off the token.
We see something very different.
What we see here are tokens.
This is why the doc object is so much more valuable and this is why the doc object has
a different length than the text object.
The text object is just basically counting up every instance of a character, a white
space, a punctuation, etc.
The doc object is counting individual tokens, so any word, any punctuation, etc.
That's why they're of different length and that's why when we print them off, we see
something different.
So you might now already be seeing the power of spacey.
It allows for you to easily on the surface with nothing else being done, easily split
up your text into individual tokens without any effort at all.
Now, those of you familiar with Python and different string methods might be thinking
to yourself, but I've got the split method.
I can just use this to split up the text.
I don't need anything fancy from spacey.
Well, you'd be wrong.
Let me demonstrate this right now.
So if I were to say for token and text.split, so I'm splitting up that text into individual
and theory individual words, essentially, it's just a split method where it's splitting
by individual white spaces.
If I were to do that and iterate over the first 10 again.
And I would just say print token, it looks good until you get down here.
So until you get to USA, well, why is it a problem?
The problem is quite simple.
There is a parentheses mark right here.
And this is where we have a huge advantage with spacey.
Spacey automatically separates out these these kind of punctuation marks and removes them
from individual tokens when they're not relevant to the token itself.
Notice that USA has got a period within the middle of it.
It's not looking at that and thinking that that is some kind of unique token, a you a
period and s a period and an a in a period.
It's not seeing these as four individual tokens rather it's automatically identifying them
as one thing one tied together single token that's a string of characters and punctuation.
This is where the power of spacey really lies just on the surface level and go ahead spend
a few minutes and play around with this.
And then we're going to kind of jump back here and start talking about how the doc object
has a lot more than just tokens within it.
It's got sentences each token has attributes.
We're going to start exploring these when you pop back.
If you're following along with the textbook, we're now going to be moving on to the next
section, which is sentence boundary detection.
An NLP sentence boundary detection is the identification of sentences within a text on the surface.
This might look simple.
You might be thinking to yourself, I could simply use the split function and split up
a text with a simple period.
And that's going to give me all my sentences.
Those of you who have tried to do this might already be shaking your heads and saying no.
If you do think about it, there's a really easy explanation for why this doesn't work.
Were you to try to split up a text by period and make a presumption that anything that occurs
with between periods is going to be an individual sentence, you would have a serious mistake
when you get to things like USA, especially in Western languages, where the punctuation
of a period mark is used not only to delineate the change of its sentence, rather it's used
to also delineate abbreviations.
So United States of America, each period represents an abbreviated word.
So you could write in rules to kind of account for this, you could write in rules that could
also include in other ways that sentences are created, such as question marks, such
as exclamation marks.
But why do that?
That's a lot of effort.
When the doc object in spacey does this for you, and let's go ahead and demonstrate exactly
how that works.
So let's go ahead and say for scent and doc.sense, notice that we're saying doc.sense or grabbing
the sentence attribute of the doc object.
Let's print off scent.
And if you do that, you are now able to print off every individual sentence.
So the entire text has been tokenized at the sentence level.
In other words, spacey has used its sentence boundary detection and done all that for you
and giving you all the sentences.
If you work with different models of different sizes, you're going to notice that certain
models the larger they get tend to do better at sentence detection.
And that's because machine learning models tend to do a little bit better than heuristic
approaches.
The English core web SM model, while having some machine learning components in it, does
not save word vectors.
And so the larger you go with the models, typically the better you're going to have with regards
to sentence detection.
Let's go ahead and try to access one of these sentences.
So let's create an object called sentence one, we're going to make that equal to doc.sense
zero.
We're going to try to grab that zero index and let's print off sentence one, we do this,
we get an error.
Why have we gotten an error?
Well, it tells you why right here, it's a type air.
And this means that this is not a type that can be kind of iterated over, it's not subscriptable.
And it's because it is a generator.
Now in Python, if you're familiar with generators, you might be thinking to yourself, there's
a solution for this.
And in fact, there is.
If you want to work with generator objects, you need to convert them into a list.
So let's say sentence one is equal to list.
So using the list function to convert doc.sense into a list.
And then with outside of that, we're going to grab zero, the zero index, and then we're
going to print off sentence one.
And we grab the first sentence of that text.
This as we go deeper and deeper in spacey one by one, you're going to see the immense
power that you can do with Pacea, all the immense incredible things you can use spacey
for with very, very minimal code.
The doc object does a lot of things for you that would take hours to actually write out
and code to do with heuristic approaches.
This is now a great way to segment an entire text up by sentence.
And if you work with text a lot, you will already know that this has a lot of applications.
As we move forward, we're going to not just talk about sentences, we're also going to
be talking about token attributes, because within the doc object are individual tokens.
I encourage you to pause here and go ahead and play around with the doc.sense a little
bit and get familiar with how it works, what it contains, and try to convert it into a
list.
And we'll continue talking about tokens.
This is where I really encourage you to spend a little bit of time with the textbook.
Under token attributes in chapter two, I have all the different kind of major things that
you're going to be using with regards to token attributes.
We're going to look and see how to access them in just a second.
I've provided for you kind of the most important ones that you should probably be familiar
with.
We're going to see this in code in just a second, and I'm going to explain with a little
bit more detail than what's in the spacey documentation about what these different things
are, why they're useful, and how they're used.
So let's go ahead and jump back into our Jupyter notebook and start talking about token attributes.
If you remember, the doc object had a sequence of tokens.
So for token and doc, you could print off token.
And let's just do this with the first 10.
And we've got each individual token.
What you don't see here is that each individual token has a bunch of metadata buried within
it.
These metadata are things that we call attributes or different things about that token that
you can access through the spacey framework.
So let's go ahead and try to do that right now.
Let's just work with for right now token number two, which we're going to call sentence one,
and we're going to grab from sentence one, the second index.
Let's print off that word.
And it should be states.
And in fact, it is fantastic.
So now that we've got the word states accessed, we can start kind of going through and playing
around with some of the attributes that that word actually has.
Now when you print it off, it looks like a regular piece of text, it looks like just
a string, but it's got so much more buried within it now because it's been passed through
our NLP model or pipeline from spacey.
So let's go ahead and say token to dot text.
And I'm going to be saying token to dot text.
If you're working within an IDE like Adam, you're going to need to say print token to
dot text.
When we do this, we see we get a string that just is states.
This is telling us that the dot text of the object, the pure text corresponds to the word
states.
This is really important if you need to extract the text itself from the token and not work
with the token object, which has behind it a whole bunch of different metadata that we're
going to go through now and start accessing.
Let's use the token left edge.
So we can say token to dot left underscore edge.
And we can print that off.
Well, what's that telling us?
It's telling us that this is part of a multi word token or a token that is multiple has
multiple components to make up a larger span.
And that this is the leftmost token that corresponds to it.
So this is going to be the word the as in the United States.
Let's take a look at the right edge.
We can say token to dot right underscore edge, print that off, and we get the word America.
So we're able to see where this token fits within a larger span in this case a noun chunk,
which we're going to explore in just a few minutes.
But we also learn a lot about it, kind of the different components, so we know where
to grab it from the beginning and from the very end.
So that's how the left edge and the right edge work.
We also have within this token to dot int type.
This is going to be the type of entity.
Now what you're seeing here is a integer.
So this is 384.
In order to actually know what 384 means, I encourage you to not really use that so much
as and type with an underscore after it.
This is going to give you the string corresponding to number 384.
In this case, it is GPE or geopolitical entity.
We're going to be working with named entity a little bit in this video, but I have a whole
other book on named entity recognition.
It's at NER dot pythonhumanities.com, in which I explore all of NER, both machine learning
and rules based in a lot more depth.
Let's go ahead and keep on moving on though and looking at different entity types here
as well.
Not entity types, attribute types.
So we're going to say token to dot int IOB, all lowercase and again underscore at the
end and we get the string here, I.
Now IOB is a specific kind of named entity code.
B would mean that it's the beginning of an entity and I means that it's inside of an
entity and O means that it's outside of an entity.
The fact that we're seeing I here tells us that this word states is inside of a larger
entity.
In fact, we know that because we've seen the left edge and we've seen the right edge.
It's inside of the United States of America.
So it's part of a larger entity at hand.
We can also say token to dot lima and under case again after that and we get the word
states.
Lima form or the root form of the word.
This means that this is what the word looks like with no inflection.
If we were working with a verb, in fact, let's go ahead and do that right now.
Let's grab sentence.
We're going to grab sentence one index 12, which should be the word no and we're going
to print off the lima for the word or sorry, it's a verb and we see the verb lima as no.
So if we were to print off sentence one specifically index 12, we see that its original form is
known.
So the lima form uninflected is the verb no K N O W.
Another thing that we can access and we're going to see that have the power of this later
on.
This might not seem important right now, but I promise you it will be.
Let's print off token that I call this again token to we're going to print that off, but
we're going to print off specifically the morph.
No underscore here.
Just morph.
What you get is what looks like a really weird output a string called noun type equal
to prop.
In fact, this means proper noun, a number which corresponds to sing.
We're going to talk a lot more about morphological analysis later on when we try to find an extract
information from our texts.
But for right now, understand that what you're looking at is the output of kind of what that
word is morphologically.
So in this case, it's a proper noun and it's singular.
If we were to do take this sentence 12 again and do morph, we'd find out what kind of verb
it is.
So it's a perfect past participle known perfect past participle.
For being good at NLP is also being good with language.
So I encourage you to spend time and start getting familiar with those things that you
might have forgotten about from like fifth grade grammar, such as perfect participles
and things like that.
Because when you need to start creating rules to extract information, you're going to find
those pieces of information very important for writing rules.
We'll talk about that in a little bit though.
Let's go back to our other attributes from the token.
So again, let's go to token two, and we're going to grab the POS part of speech, not
what you might be thinking.
So part of speech underscore POS underscore, and we output PROPN.
This means that it is a proper noun.
It's more of a of a simpler kind of grammatical extraction, as opposed to this morphological
detailed extraction, what kind of noun it might be with regards to in this case, singular.
So that's going to be how you extract the part of speech.
And the thing that you can do is you can extract the dependency relation.
So in this case, we can figure out what role it plays in the sentence.
In this case, the noun subject.
And then finally, the last thing I really want to talk about before we move into a more
detailed analysis of part of speech is going to be the token two dot lane.
And what this grabs for you is the language of the doc object in this case, we're working
with something from the English language, so in every language is going to have two
letters that correspond to it.
These are universally recognized.
So that's going to be how you access different kinds of attributes that each token has.
And there's about 20 more of these, or maybe not 20, maybe about 15 more of these that
I haven't covered.
I gave you the ones that are the most important that I find to be used on a regular basis
to solve different problems with regards to information extraction from the text.
So that's going to be where we stop here with token attributes, and we're going to be moving
on to part 2.5 of the book, which is part of speech tagging.
I now want to move into kind of a more detailed analysis of part of speech within spacey and
the dependency parser and how to actually analyze it really nicely either in a notebook
or outside of a notebook.
So let's work with a different text for just a few minutes.
We're going to see why this is important.
It's because I'm working on a zoomed in screen, and to make this sentence a little easier
to understand, we're going to just use Mike and Joy's plain football, a very simple sentence.
And we're going to create a new doc object, and we're going to call this doc two.
That's going to be equal to NLP text.
Let's print off doc two just to make sure that it was created, and in fact that we see
that it was.
Now that we've got it created, let's iterate over the tokens within this and say for token
in text, we want to print off token dot text.
We want to see what the text actually is.
We want to see the token dot POS, and the token dot DEP helps if you actually iterate
over the correct object over the doc to object.
And we see that we've got Mike, proper noun, noun, subject, and Joy's verb.
It's the root plane.
In this case, it's a verb.
And then we've got football, the noun, the direct object, and a period, which is the
punctuation.
So we can see the basic semantics of the sentence at play.
What's really nice from spacey is we have a way to really visualize this information
and how these words relate to one another.
So we can say from spacey, import, displacey, and we're going to do displacey, displacey
dot render.
And this is going to take two arguments, it's going to be the text, and then it's going
to be the, actually, it's going to be doc two, and then it's going to be style.
In this case, we're going to be working with DEP, and we're going to print that off.
And we actually see how that sentence is structured.
Now in the textbook, I use a more complicated sentence.
But for the reasons of this video, I've kept it a little shorter, just because I think
it displays better on this screen, because you can see that this becomes a little bit
more difficult to understand when you're zoomed in.
But this is one sentence from that Wikipedia article.
So go ahead and look at the textbook and see how elaborate this is.
You can see how it's part of a compound, how it's preposition.
You can see the more fine-grained aspects of the dependency parser and the part of speech
tagger really at play with more complicated sentences.
So that's going to be how you really access part of speech and how you can start to visualize
how words in a sentence are connected to other words in a sentence with regards to their
part of speech and their dependencies.
That's going to be where we stop with that.
In the next section, we're going to be talking about named entity recognition and how to visualize
that information.
So named entity recognition is a very common NLP task.
It's part of kind of data extraction or information extraction from texts.
It's oftentimes just called NER, named entity recognition.
I have a whole book on how to do NER with Python and with Spacey, but we're not going
to be talking about all the ins and outs right now.
We're just going to be talking about how to access the pieces of information throughout
kind of our text.
And then we're going to be dealing with a lot of NER as we try to create elaborate systems
to do named entity extraction for things like financial analysis.
Let's go ahead and figure out how to iterate over a doc object.
So we're going to say for int and doc.n, so we're going to go back to that original doc,
the one that's got the text from Wikipedia on the United States.
We're going to say print off int.text, so the text from it, and int.label, label underscore
here.
That's going to tell us what label corresponds to that text.
Then we print this off.
We've got a lot of GPEs, which are geopolitical entities, North America.
This isn't a geopolitical entity.
It's just a general location, 50, a cardinal number, five cardinal number, nor Indian in
this case, which is a national or religious political entity, quantity, the number of
miles, Canada, GPE, as you would expect, Paleo Indians, nor once again, Siberia, Locke.
Then we have date being extracted, so at least 12,000 years ago.
This is a small model, and it's extracting for us a lot of very important structured
data.
But we can see that the small model makes mistakes.
So the Revolutionary War is being considered an organization.
Were I to use a large model right now, which I can download separately from Spacey, and
we're going to be seeing this later in this video, or were I to use the much larger transformer
model.
This would be correctly identified most likely as an event, not as an organization, but because
this is a small model that doesn't contain word vectors, which we're going to talk about
in just a little bit, it does not generalize or make predictions well on this particular
data.
Nevertheless, we do see really good extraction here.
We have the American Civil War being extracted as an event.
We have the Spanish American War, even with this encoding typographical error here.
And World War being extracted as an event, World War II event, Cold War event.
All of this is looking good.
And not really, I only saw a couple basic mistakes, but for the most part, this is what you'd
expect to see.
We even see percentages extracted correctly here.
So this is how you access really vital information about your tokens, but more importantly about
the entities found within your text.
And also, Displacie offers a really nice way to visualize this in a Jupyter Notebook.
We can say displacie.render, we can say doc, style, we can say int.
And we get this really nice visualization where each entity has its own particular color.
So you can see where these entities appear within the text, as you kind of just naturally
read it.
And you can do this with the text as long as you want, you can even change the max length
to be more than a million characters long.
And again, we can see right here, org is incorrectly identified as the American Revolutionary War
incorrectly identified as org, but nevertheless, we see really, really good results with a
small English model without a lot of custom fine tune training.
And there's a reason for this.
A lot of Wikipedia data gets included into machine learning models.
The machine learning models on text typically make good predictions on Wikipedia data, because
it was included in their training process.
Nevertheless, these are still good results.
If I'm right or wrong on that, I'm not entirely certain.
But that's going to be how you kind of extract important entities from your text, and most
importantly visualize it.
This is where chapter two of my book kind of ends.
After this chapter, you have a good understanding, hopefully, of kind of what the dot container
is, what tokens are, and how the doc object contains the attributes such as since and
ends, which allows for you to find sentences and entities within a text.
Hopefully you also have a good understanding of how to access the linguistic features of
each token through token attributes.
I encourage you to spend a lot of time becoming familiar with these basics, as these basics
are the building block for really robust things that we're going to be getting into
in the next few lessons.
We're now moving into chapter three of our textbook on Spacey and Python.
Now in chapter three, we're going to be continuing our theme of part one, where we're trying
to understand the larger building blocks of Spacey.
Even though this video is not going to deal with Spacey machine learning approaches, our
custom ones, that is, it's still important to be familiar with what machine learning is
and how it works, specifically with regards to language, because a lot of the Spacey models
such as the medium, large and transformer models, all are machine learning models that
have word vectors stored within them.
This means that they're going to be larger, more accurate, and do the things a bit more
slowly, depending upon its size.
We're going to be working through not only what kind of machine learning is generally,
but specifically how it works with regards to text.
I think that this is where you're going to find this textbook to be somewhat helpful.
What I want to do is in our new Jupyter Notebook, we're going to import Spacey just as we did
before, but this time we're going to be installing a new model.
We're going to do Python, the exclamation mark, Python, M, Spacey, download, and then
we're going to download the Ncore Web MD model.
This is the medium English model.
This is going to take a little longer to download, and the reason why I'm having you download
the medium model, and the reason why we're going to be using the medium model, is because
the medium model has stored within it word vectors.
Let's go ahead and talk a little bit about what word vectors are and how they're useful.
So word vectors are word embeddings.
So these are numerical representations of words in multi-dimensional space through matrices.
That's a very compacted sentence.
So let's break it down.
What are word vectors used for?
Well, they're used for a computer system to understand what a word actually means.
So computers can't really parse text all that efficiently.
They can't parse it at all.
Every word needs to be converted into some kind of a number.
Now for some old approaches, you would use something like a bag of words approach where
each individual word would have a corresponding number to it.
This would be a unique number that corresponds just to that word.
There are a lot of tasks that can work, but for something like text understanding or trying
to get a computer system to be able to understand how a word functions within a sentence in general,
in other words, how it works in the language, how it relates to all other words, that doesn't
really work for us.
So what a word vector is, is it's a multi-dimensional representation.
So instead of a number having just a single integer that corresponds to it, it instead
has what looks like to an unsuspecting eye, essentially.
It has a very complex sequence of floating numbers that are stored as an array, which
is a computationally less expensive form of a list in Python or just computing in general.
And this is what it looks like, a long sequence.
In this case, I believe it's a 300-dimensional word that corresponds to a specific word.
So this is what an array or a word vector or a word embedding looks like.
What this means to a computer system is it means syntactical and semantical meaning.
So the way word vectors are typically trained is, oh, there's a few different approaches,
but kind of the old-school word-to-vec approach is you give a computer system a whole bunch
of texts and different smaller, larger collections of texts, and what it does is it reads through
all of them and figures out how words are used in relation to other words.
And so what it's able to essentially do through this training process is figure out meaning.
And what that meaning allows for a computer system to do is understand how a word might
relate to other words within a sentence or within a language as a whole.
And in order to understand this, I think it's best if we move away from this textbook and
actually try to explore what word vectors look like in spacey.
So you can have a better sense of specifically what they do, why they're useful, and how
you, as a NLP practitioner, can go ahead and start leveraging them.
So just like before, we're going to create an NLP object.
This time, however, instead of loading in our Encore Web SM, we're going to load in
our Encore Web MD.
So the one that actually has these word vectors stored, the static vectors saved, and it's
going to be a larger model.
Let's go ahead and execute that cell.
And while that's executing, we're going to start opening up our text.
So we're going to say with open data wiki underscore us.txt, r as f, and we're going
to say text is equal to f.read, so we're going to successfully load in that text file and
open it up.
Then we're going to create our doc object, which will be equal to NLP text.
All the syntax is staying the exact same.
And just like before, let's grab the first sentence.
So we're going to convert our doc.sense generator into a list, and we're going to grab index
zero.
And let's go ahead and print off sentence one, just so you can kind of see it.
And there it is.
So now that we've got that kind of in memory, we can start kind of working with it a little
bit.
So let's go ahead and just start tackling how we can actually use word vectors with
spacey.
So let's kind of think about a general question right now.
Let's say I wanted to know how the word, let's say country is similar to other words within
our model's word embeddings.
So let's create a little way we can do this.
We're going to say your word, and this is going to be equal to the word country, country.
There we go.
And what we can do is we can say MS is equal to NLP.
So we're going to go into that NLP object.
We're going to grab the vocab.vectors, and we're going to say most similar.
And this is a little complicated way of doing it.
In fact, I'm going to go ahead and just kind of copy and paste this in.
You have the code already in your textbook that you can follow along with.
And I'm going to go ahead and just copy and paste it in right here and print off this.
And what this is going to do is it is going to go ahead and just do this entirely.
There we go.
And we have to import numpy as MP.
This lets us actually work with the data as a numpy array.
And when we execute this cell, what we get is an output that tells us all the words
that are most similar to the word country.
So in this scenario, the word country, it has these kind of all these different similar
words to it from the word country to the word country, capitalized nation, nation.
Now it's important to understand what you're seeing here.
What you're seeing is not necessarily a synonym for the word country, rather what you're seeing
is are the words that are the most similar.
Now this can be anything from a synonym to a variant spelling of that word to something
that occurs frequently alongside of it.
So for example, world, while this isn't the same, we would never consider world to be
the synonym of country.
But what happens is, is syntactically they're used in very similar situations.
So the way you describe a country is sometimes the way you would describe your world, or
maybe it's something to do with the hierarchy, so a country is found within the world.
This is a good way to understand it.
So it's always good to use this word as most similar, not to be something like synonym.
So when you're talking about word vectors similarity, you're not talking about synonym
similarity.
But this is a way you can kind of quickly get a sense.
So what does this do for you?
Why did I go through and explain all these things about word vectors?
If I'm not going to be talking about machine learning a whole bunch throughout this video.
Well, I did it so that you can do one thing that's really important.
And that's calculate document similarity in the spacey.
So we've already got our NLP model loaded up.
Let's create one object.
So we're going to make doc one, we're going to make that equal to NLP.
And we're going to create the text right here in this object.
So let's say this is coming straight from the spacey documentation.
I like salty fries and hamburgers.
And we're going to say doc two is equal to NLP.
And this is going to be the text fast food tastes very good.
And now what we can do is let's go ahead and load those into memory.
What we can do is we can actually make a calculation using spacey to find out how similar they
actually are these two different sentences.
We can say print off doc one, and we're going to say this again, this is coming straight
from the spacey documentation doc two, so you're going to be able to see what both documents
are.
And then we're going to do doc one dot similarity.
So we can go into the doc one dot similarity method and we can compare it to doc two.
We can print that off.
So what we're seeing here on the left is document one, this little divider thing that we printed
off here.
On the right, we have document two, and then we can see the degree of similarity between
document one and document two.
Let's create another doc object.
We're going to call this NLP doc three, and we're going to make this NLP.
Let's come up with a sentence that's completely different.
The Empire State Building is in New York.
So this is when I'm just making up off the top of my head right now.
I'm going to copy and paste this down, and we're going to compare this to doc one.
We're going to compare it to doc three, and we get a score of point five one.
So this is less similar to than these two.
So this is a way that you can take a whole bunch of documents.
You can create a simple for loop, and you can find and start clustering the documents
that have a lot of overlap or similarity.
How is this similarity being calculated?
Well, it's being calculated because what spacey is doing is it's going into its word
embeddings, and even though in these two situations, we're not using the word fast
food ever in this document.
It's going in and it knows that salty fries and hamburgers are probably in a close cluster
with the biogram or a token that's made up of two words, a biogram of fast food.
So what it's doing is it's assigning a prediction that these two are still somewhat similar,
more similar than these two, because of these overlapping in words.
So let's try one more example, see if we can get something that's really, really close.
So let's take doc four, and this is going to be equal to NLP, I enjoy oranges.
And then we're going to have doc five is going to be equal to NLP, I enjoy apples.
So two, I would agree, I would argue very, very syntactically similar sentences.
And we're going to do doc four here, doc five here, and we're going to look and see
a similarity between doc four and doc five.
And if we execute this, we get a similarity of 0.96.
So this is really high.
This is telling me that these two sentences are very similar, and it's not just that they're
similar because of the similar syntax here, that's definitely pushing the number up.
It's that what the individual is liking in the scenario between these two texts, they're
both fruits.
Let's try something different.
Let's make doc five.
Let's just make doc six here, and do something like this NLP, I enjoy, what's another word
we could say.
Something that's different, let's say burgers, something different from a fruit.
So we're going to make doc six like that, and we're going to again copy and paste this
down, copy and paste this down, we're going to put doc six here.
And we see this drop.
So what this demonstrates, and I'm really glad this worked because I improvised this,
what this demonstrates is that the similarity, the number that's given is not dependent on
the contextual words, rather it's dependent upon the semantic similarity of the words.
So apples and oranges are in a similar cluster around fruit because of their word embeddings.
The word burgers while still being food and still being plural is different from apples
and oranges.
So in other words, this similarity is being calculated based on something that we humans
would calculate difference in meaning based on a large understanding of a language as
a whole.
That's where word vectors really come into play.
This allows for you to calculate other things as well.
So you could even calculate the difference between salty fries and hamburgers, for example,
I've got this example ready to go in the textbook, let's go ahead and try this as well.
So we're going to grab doc one, and print off these few things right here.
So we're going to try to calculate the similarity between french fries and burgers and what
we get is a similarity of 0.73.
So if we were to maybe change this up a little bit and try to calculate the similarity between
maybe just the word burgers rather than hamburgers and hamburgers, we'd have a much higher similarity.
So my point is, is play around with the similarity calculator, play around with the structure,
the code I provided here, and get familiar with how spacey can help you kind of find
a similarity, not just between documents, but between words as well.
And we're going to be seeing how this is useful later on.
But again, it's good to be familiar with kind of generally how machine learning kind of
functions here in this context, and why these medium and large models are so much bigger.
They're so much bigger because they have more word vectors that are much deeper.
And the transformer model is much larger because it was trained in a completely different method
than the way the medium and large models were trained.
But again, that's out of the scope for this video.
I now want to turn to really the last subject of this introduction to spacey part one, which
is when we're taking this large umbrella view of the spacey.
And in the textbook, it's going to correspond to chapter four.
So what we go over in this textbook is kind of a large view of not just the dot container
and the word vectors and the linguistic annotations, but really kind of the structure of the spacey
framework, which comes around the pipeline.
So a pipeline is a very common expression in computer science and in data science.
Think of it as a traditional pipeline that you would see in a house.
Now think of a pipeline being a sequence of different pipes.
Each pipe in a computer system is going to perform some kind of permutation or some action
on a piece of data as it goes through the pipeline.
And as each pipe has a chance to act and make changes to and additions to that data,
the later pipes get to benefit from those changes.
So this is very common when you're thinking about logic of code.
I provide it like a little image here that I think maybe might help you.
So if we imagine some input sentence, right, so some input text is entering a spacey pipeline,
it's going to go through a bunch of things if you're working with the medium model or
the small model, that'll tokenize it and give it a word and vector for different words.
It'll also find the POS, the part of speech, the dependency parser will act on it.
But it might eventually get to an entity ruler, which we're going to see in just a few minutes.
The entity ruler will be a series of rules-based NER named entity recognition.
So it'll maybe assign a token to an entity.
Might be the beginning of an entity, might be the end of an entity,
might just be an individual token entity.
And then what will happen is that doc object, as it kind of goes through this pipeline,
will now receive a bunch of doc.ins.
So it'll be, this pipe will actually add to the doc object as it goes through the pipeline,
the entity component.
And then the next pipeline, the entity linker, might take all those entities and try to find out
which ones they are.
So it'll oftentimes be connected to some kind of wiki data, some kind of standardized number
that corresponds to a specific person.
So for example, if you were seeing a bunch of things like Paul something, Paul something,
maybe that one Paul something might be Paul Hollywood from the Great British Bake Off,
and it might have to make a connection to a specific person.
So if it's the word Paul being used generally, this entity linker would assign it to Paul Hollywood,
depending on the context.
That's out of the scope of this video series, but keep in mind that that pipe would do something
else that would modify the ints that would give them greater specificity.
And then what you'd be left with is the doc object on the output that not only has entities
annotated, but it's also got entities linked to some generic specific data.
So that's going to be how a pipeline works.
And this is really what spacey is.
It's a sequence of pipes that act on your data.
And that's important to understand, because it means that as you add things to a spacey
pipeline, you need to be very conscientious about where they're outed and in what order.
As we're going to see as we move over to kind of rules based spacey, when we start talking
about these different pipes, the entity ruler, the matcher custom components, regex components,
you're going to need to know which order to put them in.
It's going to be very important.
So do please keep that in mind.
Now spacey has a bunch of different attribute rulers or different pipes that you can kind
of add into it.
You've got dependency parsers that are going to come standard with all of your models.
You've got the entity linker entity, recognizer entity ruler, you're going to have to make
these yourself and add them in oftentimes.
You've got a limitizer.
This is going to be on most of your standard models, your morphologue, that's going to
be on on there as well, sentence recognizer, synthesizer.
This is what allow for you to have the doc.sense right here span categorizer.
This will help categorize different spans, be them single token spans or sequence of
token spans, your tagger, this will tag the different things in your text, which will
help with part of speech, your text categorizer.
This is when you train a machine learning model to recognize different categories of
a text.
So text classification, which is a very important machine learning task, tote to VEC.
This is going to be what assigns word embeddings to the different words in your doc object.
Organizer is what breaks that thing up and all your text into individual tokens.
And you've got things like transformer and trainable pipes.
Then within this, you've also got some other things called matchers.
So you can do some dependency matching.
We're not going to get into that in this video.
You've also got the ability to use matcher and phrase matcher.
These are a lot of the times can do some similar things, but they're executed a little differently
to make things less confusing.
I'm really only talking about the matcher of these two.
If there's a need for it, I'll add into the textbook the phrase matcher at a later date,
but I'm not going to cover it in this video.
And if I do add in the phrase matcher, it's going to be after this matcher section here.
I have it in the GitHub repo.
I just haven't included in the textbook to keep things a little bit simpler, at least
if you're just starting out.
So a big good question is, well, how do you add pipes to a spacey pipeline?
So let's go ahead and do that.
We're going to make a blank spacey pipeline right now.
Let's go ahead and just make, we'll just work with the same live coding notebook that we
have open right now.
So what we're going to do is we're going to make a blank model, and we're going to actually
add in our own sentenizer to our, to our text.
So let's go ahead and do that.
So I'm going to say NLP is equal to a spacey dot blank.
This is going to allow for me to make a blank spacey pipeline.
And I'm going to say Ian so that it knows that the tokenizer that I need to use is the
English tokenizer.
And now if I want to add a pipe to that, I can use one of the built-in spacey features.
So I can say add underscore pipe, and I can say sentenizer.
So I can add in a sentenizer.
This is going to allow for me to create a pipeline now that has a sequence of two different
pipes.
And I demonstrate in the textbook why this is important.
Sometimes what you need to do is you need to just only break down a text into individual
sentences.
So I grabbed a massive, massive corpus from the internet, which is on MIT.edu.
And it's the entire Shakespeare corpus.
And I just try to calculate the, the quantity of sentences found within it.
There are 94,133 sentences, and it took me only 7.54 seconds to actually go through and
count those sentences with the spacey model.
Using the small model, however, it took a total amount of time of 47 minutes to actually
break down all those sentences and extract them.
Why is there a difference in time between 7 seconds and 47 minutes?
It's because that this spacey small model has a bunch of other pipes in it that are
trying to do a bunch of other things.
If you just need to do one task, it's always a good idea to just activate one pipe or maybe
make a blank model and just add that single pipe or the only pipes that you need to it.
A great example of this is needing to tokenize a whole bunch of sentences in relatively short
time.
So I don't know about you, but I'd be much happier with 7 seconds versus 47 minutes.
However comes at a trade-off.
The small model is going to be more accurate in how it finds sentence boundaries.
So we have a difference in quantity here.
This difference in quantity indicates that this one messed up and made some mistakes because
it was just the sentenceizer.
The sentenceizer didn't have extra data being fed to it.
In fact, if I probably used larger models, I might even have better results.
But always think about that.
If time is of the essence and you don't care so much about accuracy, a great way to get
the quantity of sentences or at least a ballpark is to use this method where you simply add
in a sentenceizer to a blank model.
So that's how you actually add in different pipes to a spacey pipeline and we're going
to be reinforcing that skill as we go through, especially in part two, where we really kind
of work with this in a lot of detail.
Right now I'm just interested in giving you the general understanding of how this might
work.
Let's go ahead and try to analyze our pipeline so we can do analyze underscore pipes.
We can analyze what our analyze, there we go.
We can actually analyze our pipeline.
If we look at the NLP object, which is our blank model with the sentenceizer, we see
that our NLP pipeline ignore summary, ignore this bit here.
But what you're actually able to kind of go through and see right away is that we've really
just got the sentenceizer sitting in it.
If we were to analyze a much more robust pipeline, so let's create NLP two is equal to spacey
dot load and core web SM, we're going to create that NLP two object around the small spacey
English model.
We can analyze the pipes again, and we see a much more elaborate pipeline.
So what are we looking at?
Well, what we're looking at is the sequence of things we've got in the pipeline, a tagger
after the talk to VEC, we've got a tagger, a parser, we keep on going down, we've got
an attribute ruler, we've got a limitizer, we've got the NER, that's what it's designed
the doc dot ends, and we keep on going down.
We can see the limitizer, but we can see also a whole bunch of other things.
We can see what these different things actually assign.
So doc dot ends is assigns the NER and require, and we can also see what each pipe might actually
require.
So if we look up here, we see that the NER pipe, so the name to the recognition pipe
is responsible for assigning the doc dot ends.
So that attribute of the doc object, and it's also responsible at the token level for
assigning the end dot IOB underscore IOB, which is the, if you remember from a few minutes
ago when we talked about the IOB being the opening beginning or out beginning inside
for a different entity, it also assigns the end dot end underscore type for each token
attribute.
So you can see a lot of different things about your pipeline by using NLP dot analyze underscore
pipes.
If you've gotten to this point in the video, then I think you should by now have a good
really umbrella view of what spacey is, how it works, why it's useful.
And some of the basic features that it can do and how it can solve some pretty complex
problems with some pretty simple lines of code.
What we're going to see now moving forward is how you as a practitioner of NLP cannot
just take what's given to you with spacey, but start working with it and start leveraging
it for your own uses.
So taking what is already available.
So like these models like the English model and adding to them contributing to them.
Maybe you want to make an entity ruler where you can find more entities in a text based
on some cousin tier or list that you have.
Maybe you want to make a matcher so you can find specific sequences within a text.
Maybe that's important for information extraction.
Maybe you need to add custom functions or components into your spacey pipeline.
I'm going to be going through in part two rules based spacey and giving you all the
basics of how to do some really robust custom things relatively quickly with within the
spacey framework.
All of that's going to lay the groundwork so that in part three, we can start applying
all these skills and start solving some real world problems.
In this case, we're going to look at financial analysis.
So that's going to be where we move to next is part two.
We are now moving into part two of this Jupiter book on spacey and we're going to be working
with rules based spacey.
Now this is really kind of the bread and butter of this video.
You've gotten a sense of the umbrella structure of spacey as a framework.
You've gotten a sense of what the container can contain.
You've gotten a sense of the token attributes and the linguistic annotations from part one
of this book and the earlier part of this video.
Now we're going to move into taking those skills and really developing them into custom
components and modified pipes that exist within spacey.
In other words, I'm going to show you how to take what we've learned now and start really
doing more robust and sophisticated things with that knowledge.
So we're going to be working first with the entity ruler, then with the matcher in the
next chapter, then in the components in spacey.
So a custom component is a custom function that you can put into a pipeline.
Then we're going to talk about regex or regular expressions.
And then we're going to talk about some advanced regex with spacey.
If you don't know what regex is, I'm going to cover this in chapter eight.
So let's go over to our Jupiter notebook that we're going to be using for our entity ruler
lesson.
So let's go ahead and execute some of these cells.
And then I'm going to be talking about it in just a second.
First I want to take some time to explain what the entity ruler is as a pipe in spacey,
what it's used for, why you'd find it useful and when to actually implement it.
So there are two different ways in which you can kind of add in custom features to a spacey
language pipeline.
There is a rules based approach and a machine learning based approach.
Rules based approaches should be used when you can think about how to generate a set
of rules based on either a list of known things or a set of rules that can be generated through
regex, code or linguistic features.
Machine learning is when you don't know how to actually write out the rules or the rules
that you would need to write out would be exceptionally complicated.
A great example of a rules based approach versus a machine learning based approach and when
to use them is with entity types for named entity recognition.
Imagine if you wanted to extract dates from a text.
There are a finite, very finite number of ways that a date can appear in a text.
You could have something like January 1, 2005, you could have one January 2005, you could
have one Jan 2005, you could have one slash five slash 2005, there's there's different
ways that you can do this and there's a lot of them.
But there really is a finite number that you could easily write a regex expression for
a regular expression for to capture all of those.
And in fact, those regex expressions already exist.
That's why spacey is already really good at identifying dates.
So dates are something that you would probably use a rules based approach for something that's
a good machine learning approach for or something like names.
If you wanted to capture the names of people, you would have to generate an entity ruler
with a whole bunch of robust features.
So you would have to have a list of all known possible first names, all known possible last
names, all known possible prefixes like doctor, Mr and Mrs, Miss, Miss, Master, etc.
And you'd have to have a list of all known suffixes.
So junior, senior, the third, the fourth, etc. on down the list.
This would be very, very difficult to write because first of all, the quantity of names
that exist in the world are massive.
The quantity of last names that exist in the world is massive.
There's not a set gazetteer or set list out there of these anywhere.
So for this reason, oftentimes things like people names will be worked into machine learning
components.
I'm going to address machine learning in another video at a later date, but right now we're
going to focus on a rules based approach.
So using the rules based features that spacey offers, a good NLP practitioner will be excellent
at both rules based approaches and machine learning based approaches and knowing when
to use which approach and when maybe maybe a task is not appropriate for machine learning
when it can be worked in with rules relatively well.
If you're taking a rules based approach, the approach that you take should have a high
degree of confidence that the rules will always return true positives.
And you need to think about that.
If you are okay with your rules, maybe catching a few false positives or missing a few true
positives, then maybe think about how you write the rules and allowing for those and
making it known in your documentation.
So that's generally what a rules based approach is in an entity ruler is a way that we can
use a list or a series of features, language features to add tokens into the entity, the
dot ints container within the dot container.
So let's go ahead and try to do this right now.
The text we're going to be working with is a kind of fun one, I think.
So if you've already gotten the reference, congratulations, it's kind of obscure.
But we're going to have a sentence right here that I just wrote out.
West Chesterton Fieldville was referenced in Mr. Deeds.
So in this context, we are going to have a few different entities.
We want our model or our pipeline to extract West Chesterton Fieldville as a GPE.
It's a fake place that doesn't really exist.
It was made up in the movie Mr. Deeds.
And what we want is for Mr. Deeds to be grabbed as an entity as well.
And this would ideally be labeled as a film.
But in this case, that's probably not going to happen.
Let's go ahead and see what does happen.
So we're going to say for int and doc dot ints, print off int dot text, int dot label,
like we learned from our NER lesson a few moments ago.
And we see that the output looks like this.
It's gotten almost all the entities that we wanted.
Mr. was left off of Deeds.
And it's grabbed the West Chesterton Fieldville and labeled it as a person.
So what's gone wrong here?
Well, there's a few different things that have gone wrong.
The NCORE Web SM model is a machine learning model for NER.
The word vectors are not saved.
So the static vectors are not in it.
So it's making the best prediction that it can.
But even with a very robust machine learning model, unless it has seen West Chesterton
Fieldville, there is not really a good way for the model to actually know that that's
a place.
Unless it's seen a structure like West Chesterton, and maybe it can make up a guess, a transformer
model might actually get this right.
But for the most part, this is a very challenging thing.
This would be challenging for a human.
There's not a lot of context here to tell you what this kind of entity is, unless you
knew a lot about how maybe northeastern villages and towns in North America would be called.
Also, Mr. Deeds is not extracted as a whole entity, just Deeds is.
Now ideally, we would have an NER model that would label West Chesterton Fieldville as
a GPE and Mr. Deeds as a film.
But we've got two problems.
One, the machine learning model doesn't have film as an entity type.
And on top of that, West Chesterton Fieldville is not coming out correct as GPE.
So our goal right now is to fix both of these problems with an entity ruler.
This would be useful if I were maybe doing some text analysis on fictional places referenced
in films.
So things like Narnia, maybe Middle Earth, West Chesterton Fieldville, these would all
be classified as kind of fictional places.
So let's go ahead and make a ruler to correct this problem.
So what we're going to do is first we're going to make a ruler by saying ruler is equal
to NLP dot add pipe.
And this is going to take one argument here, you're going to find out when we start working
with custom components that you can have a few different arguments here, especially
if you create your own custom components.
But for right now, we're working with the components that come standard with spacey.
There's about 18 of them.
One of them is the entity underscore ruler, all lowercase.
We're going to add that ruler into our NLP model.
And if we do NLP dot analyze underscore pipes and execute that, we can now look at our NER
model and see as we go down that the NER pipe is here and the entity ruler is now the exit,
the final pipe in our pipeline.
So we see that it has been successfully added.
Let's go ahead now and try to add patterns into that pipeline.
Patterns are the things that the spacey model is going to look for in the label that it's
going to assign when it finds something that meets that pattern.
This will always be a list of lists.
So let's go ahead and do this right now.
Sorry, a list of dictionaries.
So the first pattern that we're really looking for here is going to be a dictionary.
It's going to have one key of label, which is going to be equal to GPE and another label
of pattern, which is going to be equal to, in this case, we want to find West Chesterton
Fieldville.
Let me go ahead and just copy and paste it so I don't make a mistake here.
And what we want to do is we want our entity ruler to see West Chesterton Fieldville.
And when it sees it, assign the label of GPE.
So it's a geopolitical entity.
So it's a place.
So let's go ahead and execute that.
Great.
We've got the patterns.
Now comes time to load them into the ruler.
So we can say ruler.add underscore patterns.
This is going to take one argument.
It's going to be our list of patterns added in.
Cool.
Now let's create a new doc object.
We're going to call this doc to that's going to be equal to NLP.
We're going to pass in that same text.
We're going to say for int n doc to dot ints print off int dot text and end dot label.
And you're going to notice that nothing has changed.
So why has nothing changed?
We're still getting the same results.
And we've added the correct pattern in.
The answer lies into one key thing.
If we look back up here, we see that our entity ruler comes after our NER.
What does that mean?
Well, imagine how the pipeline works that I talked about a little while ago in this
video.
A pipeline works by different components, adding things to an object and making changes
to it, in this case, adding ints to it, and then making those things isolated from later
pipes from being able to overwrite them unless specified.
What this means is that when West Chesterton field bill goes through and is identified
by the NER pipe as a person, it can no longer be identified as anything else.
What this means is that you need to do one of two things give your ruler the ability
to overwrite the NER, or this is my personal preference, put it before the NER in the pipeline.
So let's go through and solve this common problem right now.
We're going to create a new NLP object called NLP to, which is going to be equal to spacey
dot load.
And again, we're going to load in the English core web SM's model and core web SM.
Great.
And again, we're going to do ruler dot NLP to add pipe entity ruler, and we're going
to make that an object too.
Now what we can do is we can say ruler dot add patterns, again, we're going to go through
all of these steps that we just went through, we're going to add in those patterns that
we created up above.
And now what we're going to do is we're going to actually do one thing a little different
than what we did.
What we're going to do is we're going to load this up again, and we're going to do an extra
keyword argument.
Now we can say either after or before here, we're going to say before NER, what this is
going to do is it's going to place our NER before our entity will ever for the NER component.
And now when we add our patterns in, we can now create a new doc object.
Doc is going to be equal to NLP to text, we're going to say for int and doc dot ints, print
off int dot text, and dot label.
Now we notice that it is correctly labeled as a GPE.
Why is this?
Well, let's take a look at our NLP to object, analyze pipes, and if we scroll down, we will
notice that our entity ruler now in the pipeline sits before the NER model.
In other words, we've given primacy to our custom entity ruler, so that it's going to
have the first shot at actually correctly identifying these things, but we've got another
problem here.
This is coming out as a person, it should be Mr. Deeds as the entire collective multi
word token, and that should be a new entity.
We can use the entity ruler to add in custom types of labels here.
So let's go ahead and do this same thing.
Let's go ahead and just copy and paste our patterns, and we're going to create one more
NLP object, we're going to call this NLP three is equal to spacey dot load in core web SM.
Great, we've got that loaded up.
We're going to do the same thing we did last time NLP three, or sorry, ruler is equal to
NLP dot add underscore pipe entity ruler, we're going to place it remember got to place it
before the NER pipe, NLP three, there we go.
And what we need to do now is we need to copy in these patterns, and we're going to add
in one more pattern.
Remember this can be a list here.
So this pattern, we're going to have a new label called film, and we're going to look
for the sequence Mr. Deeds.
And that's going to be our pattern that we want to add in to our ruler.
So we can do ruler dot add underscore patterns, and we're going to add in patterns, remember
that one keyword argument, or one argument is going to be the list itself.
And now we can create a new doc object, which is going to be equal to NLP three, I think
I called it, yeah, text, and we can say for int and doc dot ints, print off and dot text
and and dot label.
And if we execute this, we see now that not only have you gotten the entity ruler to correctly
identify West Chesterton Fieldville, we've also gotten the entity ruler to identify correctly,
Mr. Deeds as a film.
Now some of you might be realizing the problem here, this is actually a problem for machine
learning models.
And the reason for this is because Mr. Deeds in some instances could be the person and
Mr. Deeds in other instances could be the movie itself.
This is what we would call a toponym.
So spelled like this, and this is a common problem in natural language processing.
And it's actually one of the few problems or one of many problems really, that remain
a little bit unsolved toponym resolution, spelled like this, or TR is the resolution
of toponym.
So things that can have multiple labels that are dependent upon context.
Another example of toponym resolution is something like this, if you were to look at this word,
let's say, let's ignore Paris Hilton, let's ignore Paris from Greek mythology.
Let's say it's only going to ever be a GPE.
The word Paris could refer to Paris, France, Paris, Kentucky, or Paris, Texas.
Toponym resolution is also the ability to resolve problems like this, when in context
is Paris was kind of talking about Paris, France, when in context is it talking about
Kentucky, and when in context is it talking about Texas.
So that's something that you really want to think about when you're generating your
rules for an entity ruler, is this ever going to be a false positive?
And if the answer is that it's going to be a false positive half the time, or it's a
50-50 shot, then really consider incorporating that kind of an entity into a machine learning
model by giving it examples of both Mr. Deeds, in this case, as a film, and Mr. Deeds as
a person.
And learn with word embeddings when that context means it's a film and when that context means
it's a person.
That's just a little toy example.
What we're going to see moving forward, though, and we're going to do this with a matcher,
not with the entity ruler, is that spacey can do a lot of things.
You might be thinking to yourself, now I could easily just come up with a list and just check
and see whenever Mr. Deeds pops up and just inject that into the doc.ins.
I could do the same thing with West Chesterton Field Bill.
Why do I need an NLP framework to do this?
And the answer is going to come up in just a few minutes when we start realizing that
spacey can do a lot more than things like regex or things like just a basic gazetteer
check or a list check.
What you can do with spacey is you can have the pattern not just take a sequence of characters
and look for a match, but a sequence of linguistic features as well, that earlier pipes have
identified.
And I think it's best if we save that for just a second when we start talking about
the matcher, which is, in my opinion, one of the more robust things that you can do
with spacey and what sets spacey apart from things like regex or other fancier string
matching approaches.
Okay, we're now moving into chapter six of this book, and this is really kind of, in
my opinion, one of the most important areas in this entire video.
If you can master the techniques I'm going to show you for the next maybe 20 minutes
or so, maybe 30 minutes, you're going to be able to do a lot with spacey and you're really
going to see really kind of its true power.
A lot of the stuff that we talk about here in the matcher can also be implemented in
the entity ruler as well with a pattern.
The key difference between the entity ruler and the matcher is in how data the data is
kind of extracted.
So the matcher is going to store information a little differently.
It's going to store it as within the vocab of the NLP model.
It's going to store it as a unique identifier or a lexeme, spelt lex, eme, I'm going to talk
about that more in just a second.
And it's not going to store it in the doc ends.
So matchers don't put things in your doc.ends.
So when do you want to use a matcher over an entity ruler?
You want to use the entity ruler when the thing that you're trying to extract is something
that is important to have a label that corresponds to it within the entities that are coming
out.
So in my research, I use this for anything from like, let's say stocks, if I'm working
with finances, I'll use this for if I'm working with Holocaust data at the USHMM, where I'm
a postdoc, I'll try to add in camps and ghettos because those are all important annotated alongside
other entities.
I'll also work in things like ships, so the names of ships, streets, things like that.
When I use the the matcher, it's when I'm looking for something that is not necessarily
an entity type, but something that is a structure within the text that will help me extract
information.
And I think that'll make more sense as we go through and I show you kind of how to improve
examples going through it, we're kind of using the matcher as you would in the real world.
But remember, all the patterns that I show you can also be implemented in the entity
ruler.
And I'm also going to talk about when we get to chapter eight, how rejects can actually
be used to do similar things, but in a different way.
Essentially, when you want to use the matcher or the entity ruler over rejects is when linguistic
components, so the lemma of a word or the identifying if the word is a specific type
of an entity, that's when you're going to want to use the matcher over rejects.
And when you're going to use rejects is when you really have a complicated pattern that
you need to extract.
And that pattern is not dependent upon specific parts of speech, you're going to see with
that how that works as we kind of go through the rest of part two, but keep that in the
back of your mind.
So let's go ahead and take our work over to our blank Jupiter notebook again.
So what we're going to do is we're going to just set up with a basic example.
We need to import spacey.
And since we're working with the matcher, we also need to say from spacey dot matcher,
import matcher with a capital M, very important capital M.
Once we have this loaded up, we can start actually working with the matcher.
And we're going to be putting the matcher in a just the small English model.
And we're going to say NLP is equal to spacey dot load.
And you should be getting familiar with this in core web SM, the small English model.
Once we've got that loaded, and we do now, we can start actually working with the matcher.
So how do you create the matcher?
Well, the Pythonic way to do this and the weights in the documentation is to call the
object a matcher, that's going to be equal to matcher with a capital M. So we're calling
this class right here.
And now what we need to do is we need to pass in one argument.
This is going to be NLP dot vocab.
We're going to see that we can add in some extra features here in just a little bit.
I'm going to show you why you want to add in extra features at this stage, but we're
going to ignore that for right now.
What we're going to try to do is we're going to try to find email addresses within a text,
a very simple task that's really not that difficult to do.
We can do it with a very simple pattern because spacey has given us that ability.
So let's create a pattern.
And that's going to be equal to a list, which is going to contain a dictionary.
The first item in the dictionary, or the first key, is going to be the thing that you're
looking for.
So in this case, we have a bunch of different things that the matcher can look for.
And I'm going to be talking about all those in just a second.
But one of them is very handily, this label of like email.
So if the if the string or the sequence of tokens or the token is looking like an email,
and that's true, then that is what we want to extract, we want to extract everything that
looks like an email.
And to make sure that this occurs, we're going to say matcher dot add.
And then here, we're going to pass in two arguments, argument one is going to be the
think of it as a label that we want to assign to it.
And this is what's going to be added into the nlp dot vocab as a lexeme, which we'll
see in just a second.
And the next thing is a pattern.
And it's important here to note that this is a list.
The argument here takes a list of lists.
And because this is just one list right now, I'm making it into a list.
So each one of these different patterns would be a list within a list, essentially the let's
go ahead and execute that.
And now we're going to say doc is equal to nlp.
And I'm going to add in a text that I have in the textbook.
And this is my email address w mattingly at aol.com.
That might be a real email address.
I don't believe it is, it's definitely not mine.
So don't try and email it.
And then we're going to say matches is equal to matcher doc.
This is going to be how we find our matches.
We pass that doc object into our matcher class.
And now what we have is the ability to print off our matches.
And what we get is a list.
And this list is a set of tuples that will always have three indices.
So index zero is going to be this very long number.
What this is, is this is a lexeme, spelt like this Ali X EME, it's in the textbook.
And the next thing is the start token and the end token.
So you might be seeing the importance here already.
What we can do with this is we can actually go into the nlp vocab where this integer lies
and find what it corresponds to.
So this is where this is pretty cool.
Check this out.
So you print off nlp dot vocab.
So we're going into that vocab object.
We're going to index it matches zero.
So this is going to be the first index, so this tuple at this point.
And then we're going to grab index zero.
So now we've gone into this list.
We've gone to index zero, this first tuple, and now we're grabbing that first item there.
Now what we need to do is we need to say dot text, you need to do it right here.
And if you print this off, we get this email address, that label that we gave it up there
was added into the nlp vocab with this unique lexeme that allows for us to understand what
that number corresponds to within the nlp framework.
So this is a very simple example of how a matcher works and how you can use it to do
some pretty cool things.
But let's take a moment, let's pause and let's see what we can do with this matcher.
So if we go up into spacey's documentation on the matcher, we'll see that you got a couple
different attributes you can work with.
Now we've, we're going to be seeing this a little bit.
The orth, this is the exact verbatim of a token.
And we're also going to see text, the exact verbatim, text of a token.
What we also have is lower.
So what you can do here is you can use lower to say when the item is lowercase and it looks
like and then give some lowercase pattern.
This is going to be very useful for capturing things that might be at the start of a sentence.
For example, if you were to look for the penguin in the text, anywhere you saw the penguin.
If you used a pattern that was just lowercase, you wouldn't catch the penguin being at the
start of a sentence.
It would miss it because the T would be capitalized.
By using lower, you can ensure that your pattern that you're giving it is going to be looking
for any pattern that matches that when the text is lowercase.
If is going to be the, the length of your token text is alpha is ASCII is digit.
This is when your characters are either going to be alphabetical ASCII characters.
So the American standard coding initiative, I can't remember what it stands for, but it's
that, I think it's 128 bit thing that America came up with when they started in coding text.
It's now replaced with UTF eight and is digit is going to look for something if it is a
digit.
So think of each of these as a token.
So if the token is a digit, then that counts in the pattern is lower is upper is title.
These should be all self explanatory.
If it's lowercase, if it's uppercase, if it's a title, so capitalized.
And if you don't understand what all of these do right now, I'm going to be going through
and showing you in just a second, just giving you an overview of different things that can
be included within the, the matcher or the entity ruler here.
So what we can also do is find something that if the token is actually the start of a sentence,
if it's like a number, like a URL, like an email, you can extract it.
And here is the main part I want to talk about because this is where you're really going
to find spacey out shines any other string matching system out there.
So what you can do is you can use the tokens, part of speech tag, morphological analysis,
dependency label, lima and shape to actually make matches.
So not just matching a sequence of characters, but matching a sequence of linguistic features.
So think about this.
If you wanted to capture all instances of a proper noun followed by a verb, you would
not be able to do that with regex.
There's not a way to do it.
You can't give regex if this is a verb.
Regex is just a string matching framework.
It's not a framework for actually identifying linguistic features, using them and extracting
them.
So this is where we can leverage all the power of spaces earlier pipes, the tagger, the morphological
analysis, the depth, the lemma, et cetera.
We can actually use all those things that have gone through the pipeline and the matcher
can leverage those linguistic features and make some really cool, allow us to make really
cool patterns that can match really robust and complicated things.
And the final thing I'm going to talk about is right here, the OP.
This is the operator or quantifier and determines how often to match a token.
So there's a few different things you can use here.
There's the exclamation mark, negate the pattern, requiring it to match zero times.
So in this scenario, the sequence would never occur.
There's the question mark, make the pattern optional, allowing it to match zero or one
times require the pattern to match one or more times with the plus and the asterisk,
the thing on the shift eight, allow the pattern to match zero or more times.
There's other things as well that you can do to make this match or a bit more robust.
But for right now, let's jump into the basics and see how we can really kind of take these
and apply them in a real world question.
So what I'm going to do is I'm going to work with another data set or another piece of
data that I've grabbed off of Wikipedia.
And this is the Wikipedia article entry on Martin Luther King, Jr.
It's the opening opening few paragraphs, let's print it off and just take a quick look.
And this is what it looks like.
You can go through and read it.
We're not too concerned about what it says right now.
We're concerned about trying to extract a very specific set of patterns.
What we're interested in grabbing are all proper nouns.
That's the task ahead of us.
Somebody has asked us to take this text in, extract all the proper nouns for me, but we're
going to do a lot more and not just the proper nouns, but we want to get multi word tokens.
So we want to have Martin Luther King, Jr. extracted as one token, so one export.
So the other things that we want to have are these kind of structured in sequential order.
So find out where they appear and extract them based on their start token.
So let's go ahead and start trying to do some of these things right now.
Let's scroll down here.
Great.
So we need to create really a new NLP object now at this point.
So let's create a new one.
We're going to start working with the Ncore Web SM model.
If you're working with a different model, like the large or the transformer, you're
going to have more accurate results.
But for right now, we're just trying to do this quickly for demonstration purposes.
So again, just like before, we're creating that with NLP dot vocab.
And then we're going to create a pattern.
So this is the pattern that we're going to work with.
We want to find any occurrence of a POS part of speech that corresponds to proper noun.
That's the way in which POS labels proper nouns is prop in.
And we should be able to with that extract all proper nouns.
So we can say matcher dot add, and we're going to say proper noun.
And that's going to be our pattern.
And then what we can do just like before, we're going to create the doc object.
This is going to be NLP text.
And then we're going to say matches is equal to matcher doc.
So we're going to create the matches by passing that doc object into our matcher class.
And then we're going to print off the length of the matches.
So how many matches were found, and then we're going to say for match in matches.
And we're just going to grab the first 10 because I've done this and there's a lot
and you'll see why let's print off.
Let's print off in this case, match.
And then we're going to print off specifically what that text is.
Remember, the output is the lexine followed by the start token and the end token, which
means we can go into the doc object.
And we can set up something like this.
We can say match one, so index one, which is the start token and match two, which is
the end token.
And that'll allow us to actually index what these words are.
And when we do this, we can see all these printed out.
So this is the match, the lexine here, which is going to be proper down all the way down.
We've got the zero here, which corresponds to the start token, the end token.
And this is the the token that we extracted.
Martin, Luther, King, Junior, Michael, King, Junior, we've got a problem here, right?
So the problem should be pretty obvious right now.
And the problem is that we have grabbed all proper nouns, but these proper nouns are just
individual tokens.
We haven't grabbed the multi word tokens.
So how do we go about doing that?
Well, we can solve this problem by let's go ahead and just copy and paste all this from
here.
And we're going to make one small adjustment here.
We're going to change this to OP with a plus.
So what does that mean?
Well, let's pop back into our matcher under spacey and check it out.
So OP members, the operator or quantifier, we're going to use the plus symbol.
So it's going to look for a proper noun that occurs one or more times.
So in theory, right, this should allow us to grab multi word tokens.
It's going to look for a proper noun and grab as many as there are.
So anything that occurs one or more times, if we run this, though, we see a problem.
We've gotten Martin, we got Martin Luther, what we got Luther, what we got Martin Luther
King, Luther King, King Martin Luther King, Junior, what what is going on here?
Well, you might already have figured it out.
It has done exactly what we told it to do.
It's grabbed all sequence of tokens that were proper nouns that occurred one or more
times.
Just so happens some of these overlap.
So token that's doc zero to one, zero to two.
So you can see the problem here is it's grabbing all of these and any combination of them.
What we can do, though, is we can add an extra layer to this.
So let's again, copy what we've just done because it was, it was almost there.
It was good, but it wasn't great.
We're going to do one new thing here when we add in the patterns, we're going to pass
in the keyword argument, greedy, we're going to say longest capital, all capital letters
here.
And if we execute that, it's going to look for the longest token out of that mix, and
it's going to give that one, make that one the only token that it extracts.
We noticed that our length has changed from what was it up here, 175 to 61.
So this is much better.
However, we should have recognized right now, another problem.
What have we done wrong?
Well, what we've done wrong is these are all out of order.
In fact, what happens is when you do this, I don't have evidence to support this, but
I believe it's right.
What will always happen is the, the greedy longest will result in all of your tokens
being organized or all your matches being organized from longest to shortest.
So if we were to scroll down the list and look at maybe negative one, negative, let's
do negative 10 on, you'll see single word tokens.
And again, this is me just guessing, but I think based on what you've just seen, that's
a fairly good guess.
So let's go ahead and just kind of so we can see what the output is here.
So how would you go about organizing these sequentially?
Well, this is where really kind of a sort comes in handy when you can pass a lambda to
it.
I can copy all this again, because again, we almost had this right.
Here we're going to sort our matches though, we can say matches.sort, and this is going
to take a keyword argument of key, which is going to be equal to lamb, duh, and lamb
is going to allow us to actually iterate over all this and find any instance where X occurs.
And we're going to say to sort by X one.
So what this is, it's a list of tuples.
And what we're using lambda for is we're going to say sort this whole list of tuples out,
but sort it by the first index, in other words, sort it by the start token.
And when we execute that, we've got everything now coming out as we would expect and nor
these typos that exist.
We've got zero to four, six to nine.
So we actually are extracting these things in sequential order as they appear in our
text.
You can actually go through and sort the appearance of the, of the matcher.
But what if our, the person who kind of gave us this job, they were happy with this, but
they came back and said, okay, that's cool.
But what we're really interested in what we really want to know is every instance where
a proper noun of any length, grab the multi word token still, but we want to know anytime
that occurs after a verb.
So anytime this proper noun is followed by a verb.
So what we can do is we can add in, okay, okay, we can do this.
We're going to have a comma here.
So the same pattern is going to be a sequence now.
It's not just going to be one thing.
We're going to say token one needs to be a proper noun and grab as many of those tokens
as you can zero or one to more times.
And then after those are done comma, this is where the next thing has to occur POS.
So the part of speech needs to be a verb.
So the next thing that comes out needs to be a verb.
And we want that to be the case.
Well, when we do this, we can kind of go through and see the results of the first instance
of this, where a proper noun is proceeded by a verb comes in token 50 to 52 King advanced
258 director J Edgar Hoover considered.
Now we're able to use those linguistic features that make Spacey amazing and actually extract
some vital information.
So we've been able to figure out where in this text a a proper noun is proceeded by
a verb.
So you can already start to probably see the implications here.
And we can we can create very elaborate things with this.
We can use any of these as long of a sequence as you can imagine.
We're going to work with a different text and kind of demonstrate that it's a fun toy
example.
We've got a halfway cleaned copy of Alice in Wonderland stored as a Jason file.
I'm going to load it in right now.
And then I'm going to just grab the first sentence from the first chapter.
And what we have here is the first sentence.
So here's our scenario.
Somebody has asked us to grab all the quotation marks and try to identify the person described
or the person described the person who's doing the speaking or the thinking.
In other words, we want to be able to grab Alice thought.
Now I picked Alice in Wonderland because of the complexity of the text, not complexity
in the sense of the language used children's book, but complexity and the syntax.
These syntaxes highly inconsistent CS and not CS Lewis, Carol Lewis C Carol was highly
inconsistent in how we structured these kind of sequences of quotes.
And the other thing I chose to do as I left in one mistake here, and that is this non
standardized quotation mark.
So remember, when you need to do this, things need to match perfectly.
So we're going to replace this first things first is to create a cleaner text, or we do
text equals text dot replace, and we're going to replace the instance of I believe it's
that mark, but let's just copy and paste it in to make sure we're going to replace that
with a, with a single quotation mark, and we can print off text just to make sure that
was done correctly.
Cool.
Great.
It was it's now looking good.
Remember, whenever you're doing information extraction, standardize the texts as much as
possible.
Things like quotation marks will always throw off your data.
Now that we've got that, let's go ahead and start trying to create a fairly robust pattern
to try to grab all instances where there is a quotation mark, thought, something like
this, and then followed by another quotation mark.
So the first thing I'm going to try and do is I'm going to try to just capture all quotation
marks and a text.
So let's go through and try to figure out how to do that right now.
So we're going to copy in a lot of the same things that we used up above, but we're going
to make some modifications to it.
Let's go ahead and copy and paste all that we're going to completely change our pattern.
So let's get rid of this.
So what are we looking for?
Well, first of all, the first thing that's going to occur in this pattern is this quotation
mark.
So that's going to be a full text match, which is an or if you remember, and we're going
to have to use double quotation marks to add in that single quotation mark.
So that's what we grabbed first.
We're going to look for anything that is an or and the next thing that's going to occur
after that, I think this is good to probably do this now on a line by line basis.
So we can keep this straight.
So the next thing that's going to occur is we're looking for anything in between.
So anything that is an alpha character, we're going to just grab it all.
So is alpha and then we need to say true.
But within this, we need to specify how many times that occurs because if we say is true,
it's just going to look at the next token in this case and and then say that's the end.
That's it.
That's the pattern.
But we want to grab not just and but and what is the use of a everything.
So we need to grab not only that, but when you say OP, so our operator again.
And if you said plus, you would be right here.
We need to make sure that it's a plus sign, so it's grabbing everything.
Now in this scenario, this is a common construct is when you have a injection here in the middle
of the sentence.
So thought or said, and it's the character doing it.
This oftentimes got a comma right here.
So we need to add in that kind of a feature.
So there could be is punked.
There could be a punked here.
And we're going to say that that is equal to true.
But that might not always be the case.
There might not always be one there.
So we're going to say OP is equal to a star.
We go back.
We'll see why.
To our OP, the star allow the pattern to match zero or more time.
So in this scenario, the punctuation may or may not be there.
So that's the next thing that occurs.
Once we've got that, the last thing that we need to match is the exact same thing that
we had at the start is this or appear.
And that's our sequence.
So this is going to look for anything that starts with a quotation mark has a series
of alpha characters has a punctuation like a comma possibly, and then closes the quotation
marks.
If we execute this, we succeeded.
We got it.
We extracted both matches from that first sentence.
There are no other quotation marks in there.
But our task was not just to extract this information.
Our task was also to match who is the speaker.
Now we can do this in a few different ways and you're going to see why this is such a
complicated problem in just a second.
So let's go ahead and do this.
How can we make this better?
Well, we're going to have this occur twice.
But in the middle, we need to figure out when somebody is speaking.
So one of the things that we can do is we can make a list.
So let's make a list of limitized forms of our verbs.
So we're going to say, let's call this speak underscore limits.
This can be equal to a list.
And the first thing we're going to say is think, because we know that think is in there
and say this is the limitized form of thought and said.
So what we can do now is after that occurs, it's adding a new thing.
We're going to be able to now add in a new pattern that we're looking for.
And so not just the start of a quotation mark, not just the end of a quotation mark, but
also a sequence that'll be something like this.
So it's going to be a part of speech.
So it's going to be a verb that occurs first, right?
And that's going to be a verb.
But more importantly, it's going to be a lemma that is in what did I call you speak lemmas?
So let's break this down.
The next token needs to be a verb.
And it needs to have a limitized form that is contained within the speak lemmas list.
So if it's got that fantastic, let's execute this and see what happens.
We should only have one hit.
Cool.
We do.
So we've got that first hit.
And the second one hasn't appeared anymore because that second quotation mark wasn't
proceeded by a verb.
Let's go ahead and make some modifications that we can improve this a little bit.
Because we want to know not just what that person's doing.
We also need to know who the speaker is.
So let's grab it.
Let's figure out who that speaker is.
So we can use part of speech.
Again, another feature here.
We know that it's going to be a proper noun because oftentimes proper nouns are doing
the speaking.
Sometimes it might not be.
Sometimes it might be like the girl or the boy lowercase, but we're going to ignore those
situations for just right now.
So we're looking for a proper noun.
But remember proper nouns, as we saw just a second ago, could be multiple tokens.
So we're going to say OP plus.
So it could be a sequence of tokens.
Let's execute this.
Now we've captured Alice here as well.
So and is the use and what is the use of a book thought Alice.
Now we know who the speaker is, but this is a partial quotation.
This is not the whole thing.
We need to grab the other quote.
Oh, how will we ever do that?
Well, we've already solved that.
We can copy and paste all of this that we already have done right down here.
And now we've successfully extracted that entire quote.
So you might be thinking to yourself, yeah, we did it.
We can now extract quotation marks and we can even extract, extract, you know, any instance
where there's a quote and somebody speaking.
Not so fast.
We're going to try to iterate over this data.
So we're going to say for text in data, zero twos, we're going to iterate over the first
chapter.
And we're going to go ahead and let's let's do all of this.
Doc is going to be equal to that sort that out.
And then again, we're going to be printing out this information, the same stuff I did
before, just now it's going to be iterating over the whole chapter.
And if we let this run, we've got a serious, serious problem.
And it doesn't actually grab us anything.
Nothing has been grabbed successfully.
What is going on?
We've got a problem.
And that problem stems from the fact that our patterns and the problem is that we don't
have our our text correctly, we're being removing the quotation mark that was the problem up
above.
So we're going to add this bit of code in.
And we're going to be able to fix it.
So now when we execute this, we see that we've only grabbed one match.
Now you might be thinking to yourself, there's an issue here and there there is, let's go
ahead and print off the length of matches.
And we see that we've only grabbed one match.
And then we haven't grabbed anything else.
Well, what's the problem here?
Are there are there no other instances of quotation marks in the rest of the first chapter?
And the answer is no, there are.
There absolutely are other quotation marks and other paragraphs from the first chapter.
The problem is, is that our pattern is singular.
It's not multivariate.
We need to add in additional ways in which a text might be structured.
So let's go ahead and try and do this with some more patterns.
I'm going to go ahead and copy and paste these in from the textbook.
So you'll be able to actually see them at work.
And so what I've did, I've done is I've added in more patterns, pattern two and pattern
three allow for instances like this, well thought Alice.
So an instance where there's a punctuation, but there's no proceeding quotation after
this, and then which certainly said before an instance where there's a comma followed
by that.
So we've been able to capture more variants and more ways in which quotation marks might
exist followed by the speaker.
Now this is where being a domain expert comes into play.
You'd have to kind of look through and see the different ways that Louis C. Carroll
structures quotation marks and write out patterns for capturing them.
I'm not going to go through and try to capture everything from Alice in Wonderland because
that would take a good deal of time.
And it's not really in the best interest because it doesn't matter to me at all.
What I encourage you to do, if this is something interesting to you is try to apply it to your
own texts, different authors, structure quotation marks a little differently than what patterns
that I've gotten written here are a good starting point.
But I would encourage you to start playing around with them a little bit more.
And what you can do is when you actually have this match extracted, you know that the
instance of a proper noun that occurs between these quotation marks or after one is probably
going to be the person or thing that is doing the speaking or the thinking.
So that's kind of how the matcher works.
It allows for you to do these things, these robust type data extractions without relying
on entity ruler.
And remember, you can use a lot of these same things with an entity ruler as well.
But we don't want this in this case, we don't want things like this to be labeled as entities.
We want them to just be separate things that we can extract outside of the of the
ints dot doc dot ints.
That's going to be where we conclude our chapter on on the on the matcher.
In the next section of this video, we're going to be talking about custom components in
spacey, which allow for us to do some pretty cool things such as add in special functions
that allow for us to kind of do different custom shapes, permutations on our data with
components that don't exist like an entity ruler would be a component components that
don't exist within the spacey framework.
So add in custom things like an entity ruler that do very specific things to your data.
Hello, we're now moving into a more advanced aspect of the textbook specifically chapter
seven.
And that's working with custom components.
A good way to think about a custom component is something that you need to do to the doc
object or the doc container that spacey can't do off the shelf.
You want to modify it at some point in the pipeline.
So I'm going to use a basic toy example that demonstrates the power of this.
Let's look at this basic example that I've already loaded into memory.
It's two sentences that are in the doc object now.
And that's Britain is a place.
Mary is a doctor.
So let's do for int and doc dot ints print off int dot text and dot label.
And we see what we'd expect.
Britain is GPE a geopolitical entity.
Mary is a person.
That's fantastic.
But I've just been told by somebody higher up that they want the model to never ever
give anything as GPE or maybe they want any instance of GPE to be flagged as LOC.
So all the different locations all have LOC as a label or we just want to remove them
entirely.
So I'm going to work with that latter example.
We need to create a custom pipe that removes all instances of GPE from the doc dot
ints container.
So how do we do that?
Well, we need to use a custom component.
We can do this very easily in spacey by saying from spacey dot language import
language capital L very important there.
Capital L now that we've got that class loaded up.
Let's start working with this.
What we need to do first is we need to use a flag.
So the symbol and we need to say at language dot component.
And we need to give that component a name.
We're going to say in this case, let's say remove GPE.
And now we need to create a function to do this.
So we're going to call this remove GPE.
I always kind of keep these as the same.
That's my personal preference.
And this is going to take one, one, one thing.
That's going to be the doc object.
So the doc object, think about how it moves through the pipeline.
This component is another pipe and that pipeline.
It needs to receive the doc object and send off the doc object.
You could do a lot of other things.
It could print off entity found.
It could do really any number of things.
It could add stuff to the data coming out of the pipeline.
All we're concerned with right now is modifying the doc dot ints.
So we can do something like this.
We can say original ends is equal to a list of the doc dot ends.
So remember, we have to convert the ends from a generator into a list.
Now what we can do is we can say for int and doc dot ends, if the end not label.
So if that label is equal to GPE, then what we want to do is we want to just
we just want to remove it.
So let's say original ints.remove and we're going to remove the int.
Remember, it's now a list.
Sorry, I executed that too soon.
Remember, it's now a list.
So what we can do is we can go ahead now and convert those original
ends back into doc dot ends by saying doc dot ends equals original ends.
And if we've done things correctly, we can return the doc object and it will
have all of those things removed.
So this is what we would call a custom component.
Something that changes the doc object along the way in the pipeline, but
we need to add it to NLP.
So we can do NLP dot add pipe.
We want to make sure that it comes after the NER.
So we're just going to say, uh, add the pipe or move GPE corresponds
to the component name.
And now let's go ahead and NLP dot analyze pipes.
And you'll be able to see that it sits at the end of our pipeline right there.
Remove GPE.
Now comes time to see if it actually works.
So we're going to copy and paste our code from earlier up here.
Let's go ahead and copy this.
And now we're going to say for int and doc dot ends print off int dot text.
And dot label.
And we should see, as we would expect, just marry coming out.
Our pipeline has successfully worked.
Now, as we're going to see when we move into red checks, you can do a lot
of really, really cool things with custom components.
I'm going to kind of save the, the advanced features for, I think I've
got it scheduled for chapter here, chapter nine in our textbook.
This is just a very, very basic example of how you can introduce a custom
component to your spacey pipeline.
If you can do this, you can do a lot more.
You can maybe change a different entity.
So they have different labels.
You can make it where GPEs and locks all agree.
You can remove certain things.
You can have it print off place found person found.
You can do a lot.
So really the sky's the limit here, but a lot of the times you're going
to need to modify that doc object.
And this is how you do it with a custom pipe so that you don't have to write
a bunch of code for a user outside of that NLP object, the NLP object.
Once you save it to disk by doing something like NLP dot to disk data,
new and core web SM, it's going to actually be able to go to the disk
and be saved with everything.
But one thing that you should note is that the component that you have
here is not automatically saved with your data.
So in order for your component to actually be saved with your data,
you need to store that outside of this entire script.
You need to save it as a library that can be given to the model
when you go to package it.
That's beyond the scope of this video for right now.
In order for this to work in a different Jupyter notebook, if you were to try
to use this, this container, this component has to actually be in the script.
When it comes time to package your model, your pipeline and distribute it,
that's a different scenario.
And that scenario, you're going to make sure that you've got a special my
component dot pie file with this bit of code in there so that, so that spacing
knows how to handle your particular data.
It's now time to move on to chapter eight of this textbook.
And this is where a spacey gets really interesting.
You can start applying regular expressions into a spacey component
like an entity ruler or a custom component, as we're going to see in just
a moment with chapter nine.
I'm not going to spend a good deal of time talking about regular expressions.
I could spend five hours talking about regex and what all it can do.
In the textbook, I go over what you really need to know, which is what regular
expressions is, which is as a way to do a really robust string pattern matching.
I talk about the strengths of it, the weaknesses of it, its drawbacks,
how to implement it in Python and how to really work with regex.
But this is a video series on spacey.
What I want to talk about is how to use regex with spacey.
And so let's move over to a Jupiter notebook where we actually have this
code to execute and play around with.
If we look here, we have the same example that we saw before.
What my goal is is not to extract the whole phone number,
rather try to grab this sequence here.
And we do this with a regular expression pattern.
What this says is it tells it to look for a sequence of tokens or sequence
of characters like this.
It's going to be three digits followed by a dash followed by four digits.
And if I were to execute this whole code, nothing is printed out.
Does that mean that I failed to write good regex?
No, it does not at all.
It's failed for one very important reason.
And this is the whole reason why I have this chapter in here is that regex,
when it comes to pattern matching, pattern matching only really works
when it comes to regex for single tokens.
You can't use regex across multi-word tokens, at least as of spacey 3.1.
So what does that mean?
Well, it means that that dash right there in our phone number is causing
all kinds of problems.
If we move down to our second example, it's going to be the exact same pattern.
A little different.
Let me go ahead and move this over so you can see it a bit better.
It's going to be regex that looks like this, where we just look for a sequence
of five digits, we execute that, we find it just fine.
And the reason for that is because this does not have a dash.
So regex, if you're familiar with it, if you've worked with it, it's very powerful.
You can do a lot of cool things.
When you're going to use this in Python, if you're using just the standard
off the shelf components.
So the entity ruler, the matcher, you're going to be using this when
you want to match regex to a single token.
So think about this, if you're looking for a word that starts off with a capital
D, and you want to just grab all words that start with a capital D, that would
be an example of when you would want to use it in a standard off the shelf component.
But that's not all you can do in spacey.
You can use regex to actually capture multi word tokens.
So capture things like Mr.
Deeds.
So any instance of Mr.
Period Space Name, a sequence of proper nouns.
You can also use it to, but yet in order to do that, you have to actually
understand how to add in a custom component for it.
And we're going to be seeing that in just a second as we move on to chapter nine,
which is advanced regex.
If you're not familiar with regex at all, take a few minutes, read chapter eight.
I encourage you to do so because I go over in detail and I talk about how to
actually engage in regex and Python and its strengths and weaknesses.
What I want you to really focus on though, and get away from, get from all this
is how to do some really complex multi word token matching with regex.
Remember, you're going to want to use regular expressions when the pattern
matching that you want to do is unindependent of the, the lima, the POS,
or any of the linguistic features that space is going to use.
If you're working with linguistic features, you have to use the
spacey pattern, pattern matching things like the morph, the orth, the lima,
things like that.
But if your sequence of strings is not dependent on that, so you're looking
for any instance of, in this case, we're going to talk about in just a second,
a, a case where Paul is followed by a capitalized letter and then a word break.
Then you're going to want to use regular expressions because in this case,
this is independent of any linguistic features and regular expressions
allows for you to write much more robust patterns, much more quickly.
If you know how to use it well, and it allows for you to do much more
quick robust things within a custom component.
And that's going to be where we move to now.
Now that we know a little bit about regex and how it can be implemented in
Python, let's go ahead and also in spacey, let's go ahead and try and see
how we can get regex to actually find multi word tokens for us within spacey
using everything in the spacey framework.
So the first thing I'm going to do to kind of demonstrate all this is I'm going
to import regex.
This comes standard with Python and you can import it as RE just that way.
Import RE and that's going to import regex.
I'm going to work from the textbook and work with this sample text.
So this is Paul Newman was an American actor, but Paul Hollywood is a British TV
TV host.
The name Paul is quite common.
So it's going to be the text that we work with throughout this entire chapter.
Now a regex pattern that I could write to capture all instances of things like
Paul Newman and Paul Hollywood, which is what my goal is, could look something
like this, I could say or make an R string here and say Paul, and then I'm going
to grab everything that starts with a capital letter and then my grab
everything until a word break.
And that's going to be a pattern that I can use in regex with this formula
means is find any instance of Paul proceeded by a in this case, a capital
letter until the actual word break.
So grab the first name Paul and then what we can make a presumption is going
to be that individual's last name in the text, a simple example, but one
that will demonstrate our kind of purpose right now.
So how we can do this is we can create an object called matches and use regex
dot find it or we can pass in the pattern and we can pass in the text.
So what this is going to do is it's going to use regex to try to find this
pattern within this text.
And then what we can do is we can iterate over those matches.
So for match and matches, we can grab and print off the match and we have
something that looks like this.
What we're looking at here is what we would call it a regex match object.
It's got a couple of different components here.
It's got a span, which tells us the start character and the end character.
And then it has a match and what this match means is the actual text itself.
So the match here is Paul Newman and the match here is Paul Hollywood.
So we've been able to extract the two entities in the text that begin with
Paul and have a proper last name structured with a capital letter.
We grabbed everything up until the word break.
That's great.
That's going to be what you need to know kind of going forward because what
we're going to do now is we're going to implement this in a custom spacey pipe.
But first let's go through and write the code so that we can then easily kind
of create the pipe afterwards.
So what we need to do is we need to import spacey and we also need to say
from spacey dot tokens import span and we're going to be importing a couple
of different things as we move forward because we're going to see that we're
going to make a couple of mistakes intentionally.
I'm going to show you how to kind of address these common mistakes that might
surface in trying to do something like this.
So once we've imported those two things, we can start actually writing out our
code.
Again, we're going to stick with the exact same text and again, we're going
to stick with the exact same pattern that we've got stored in memory up above.
So what we need to do now is we need to create a blank spacey object or sorry,
a blank spacey pipeline that we can kind of put all this information into.
And for right now what we're going to do is we're just going to kind of go
through and look at these individual entities.
So again, we're going to create the doc object, which is going to be equal to
nlp text and this is not going to be necessary for right now, but I'm
establishing a kind of a consistent workflow for us and you're going to see
how we kind of take all this and implement it inside of a pipeline.
So we're going to say original ends is equal to list doc dot ends.
Now in this scenario, there's not going to be any entities because we don't
have an any R or an entity ruler in our blank spacey pipeline.
What we're going to do next is we're going to create something called an
nwt int and that's going to stand for multi word token entity.
You can name this whatever you like.
This is just what I kind of stick to and then we're going to do and this
is straight from the spacey documentation.
We're going to say for match an RE dot find it or the same thing
that we saw above pattern doc dot text.
So what this is going to do is it's going to take that doc object.
Look at it as raw text because remember the doc object is a container
that doesn't actually have raw text in it until you actually call the dot
text attribute and then our goal is for each of these things.
We're going to look and call in this span.
So we're going to say is start and the end is equal to match dot span.
So what we're doing here is we're going in and grabbing the span attribute
and we're grabbing these two components the start and the end.
But we have a problem.
These are character spans.
Remember the doc object works on a token level.
So we've got to kind of figure out a way to reverse engineer this almost
to actually get this into a spacey form.
Fortunately the doc object also has an attribute called character span.
So what we can do is we can say the span is equal to doc dot char span
start and end.
So what this is going to do is it's going to print off essentially for us.
Let's go ahead and do that.
It would print off for us where we worry to actually have an entity here.
It would print off for us as we can see Paul Newman and Paul Hollywood.
So what we need to do now is we need to get this span into our entities.
So what we can do is instead of printing things off we can say if span is not
none because in some instance instances this will be the case.
You're going to say NWT ends dot append.
You're going to append a tuple here span dot start span dot end span dot
text.
So this is going to be the start the end and the text itself.
And once we've done that we've managed to get our multi word tokens into
a list that looks like this.
Start and Paul Newman Paul Hollywood and notice that our span dot start is
aligning not with a character span.
Now it's rather aligning with a token span.
So what we've done is we've taken this character span here and been able to
find out where they start and end within the the token sequence.
So we have zero and two.
So Paul Newman one this was the zero index.
It goes up until the second index.
So it grabs index token zero and token one and we've done the same thing
with Paul Hollywood.
Now we've got that data.
We can actually start to inject these entities into our original entities.
So let's go through and do that right now.
So we can do once we've got these things appended to this list.
We can start injecting them into our original entities.
So we can say for end in MWT ends.
What we want to do is we want to say the start the end and the name is equal
to end because this is going to correspond to the tuple the start the
end and the entity text.
Now what we can do is we can say per end.
So this is going to be the individual end.
We're going to create a span object in spacey.
It's going to look like this.
So a capital S here.
Remember we imported it right up here.
This is where we're going to be working with the span class and this is
going to create for us a span object that we can now safely inject into
the spacey doc.ins list.
So we can say doc start and label and this is going to be the label that
we want to actually assign it and this is going to be person in this
case because these are all people we can do now as we can go through and
say doc we can inject this into the original ends.
Original ins dot append and we're going to append the per end which is
going to be this span object and finally what we can say is doc.ins is
equal to original ends kind of like what we saw just a few moments ago
and let's go ahead and print off.
We've got our entities right there or we to do this up here when we first
kind of create the doc object you'll see nothing an empty list but now
what we've been able to do is inject these into the doc object the doc.ins
attribute and we can say for end and doc.ins just like everything else
and dot text and dot label and because we converted it into a span we
were able to inject it into the entity attribute from the doc object kind
of natively so that spacey can actually understand it.
So what can we do with this well one of the things that we could do is
we can use the knowledge that we just acquired about custom components
and build a custom component around all of this.
So how might we do that well let's go through and try it out.
The first thing that we need to do is we need to import our language class so
if you remember from a few moments ago whenever you need to work with a
custom component you need to say from spacey dot language import language
with a capital L what we're going to do now is we're going to take the code
that we just wrote and we're going to try to convert that into an actual
custom pipe that can fit inside of our pipeline as kind of our own custom
entity ruler if you will.
So what we're going to do now is we're going to call this language dot
component and we're going to call this let's call this Paul NER something
not too not too clever but kind of very descriptive we're going to call
this Paul NER and this is going to take that single doc object because
remember this pipe needs to receive the doc object and do stuff to it.
So what we can do is we can take all this code that we just wrote.
From here down and paste it into our function and what we have is the
ability now to implement this as a custom pipe.
We don't need to do this because we don't want to print things off but
here we're going to return the doc object.
So we have now is a custom kind of entity ruler that uses regex across
multiple tokens.
If you want to use regex in spacey across multiple tokens as of spacey
3.1 this is the only way to implement this.
So now we can take this pipe and we can actually add it to a blank custom
model.
So let's make a new nlp calls nlp2 is equal to spacey dot blank and we're
going to create a blank English model nlp2 dot add pipe.
We're going to add in Paul NER.
And now we see that we've actually created that successfully.
So we have one pipe kind of sitting in all of this.
Now what we can do is we can go through and we need to probably add in our
pattern as well here just for good practice because this should be
stored somewhat adjacent.
I like to sometimes to keep it up here when I'm doing this but you can
also keep it kind of inside of the function itself.
Let's go ahead and just kind of save that and we're going to rerun this
cool.
Now what we can do is we can say doc to is equal to nlp2 we're going
to go over that exact same text and we're going to print off our doc
to dot ints and we've now managed to implement that as a custom
spacey pipe but we've got one big problem.
Let's say just hypothetically we wanted to also kind of work in really
a another kind of something into our actual pipeline.
We wanted this pipeline to sit on top of maybe an existing spacey model
and for whatever reason we don't want Paul Hollywood to have that title.
We wanted to have the title.
Maybe we want to just kind of keep Paul Hollywood as a person but we
also want to find maybe other cinema style entities.
So we're going to create another entity here instead of all this that's
going to be something like let's go ahead and make a new a new container
down here a new component down here.
We're going to just look for any instance of Hollywood and we're going
to call that the word the label of cinema.
So I want to demonstrate this because this is going to show you
something that you are going to encounter when you try to implement
this in the real world and I'm going to show you how to kind of
address the problem that you're going to encounter.
So if we had a component that looked like this now it's going to look
for just instance instances of Hollywood and let's call this Holly
Cinema NER and change this here as well.
What we can do now is go ahead and load that up into memories.
We've got this new component called Cinema NER and just like before
we're going to create an LP three now this is going to be spacey dot
load in core web.
SM and so what this is going to do is it's going to load up the spacey
small model and LP three dot add pipe and it's going to be the what did
I call this again the cinema NER and if we were to go through
and add that and create a new object called doc three make that
equal to an LP three text.
We're going to get this air and this is a common air and if you
Google it you'll eventually find the right answer.
I'm just going to give it to you right now.
So what this is telling you is that there are spans that overlap
that don't actually work because one of the spans for cinema is Hollywood
and the small model is extracting not only that Hollywood as a cinema
but it's also extracting Paul Hollywood as part of a longer token.
So what's happened here is we're trying to assign a span to two of
the same tokens and that doesn't work in spacey.
It'll break so what can you do?
Well a common method of solving this issue is to work with the filter
spans from the spacey dot util.
Let's go ahead and do this right now so you can say from spacey dot
util import filter spans.
What filter spans allows for you to do is to actually filter out all
of the the spans that are being identified.
So what we can do is we can say at this stage.
Before you get to the dock dot ends you can say filtered is equal
to filter spans original ends.
So what does this do?
Well what this does is it goes through and looks at all of the
different start and end sections from all of your entities.
And if there is an ever an instance where there is a an overlap
of tokens so 8 to 10 and 9 to 10.
Primacy and priority is going to be given to the longer token.
So what we can do is we can set this now to filtered and it helps
if you call it correctly filtered.
There we go.
We can set that to filtered instead of the original entities.
Go ahead and save that.
We're going to add this again and we're going to do doc 3 and
we're going to say for int and doc 3 dot ends print int dot
text and int dot label.
And if we've done this correctly we're not going to see the
cinema label come out at all because Paul Hollywood is a
longer token than just Hollywood.
So what we've done is we've set told spacey give the primacy
to the longer tokens and assign that label by filtering out
the tokens you can prevent that air from ever surfacing.
But this is a very common thing that you're going to have to
implement sometimes rejects really is the easiest way to
inject and do pattern matching in the entity.
Okay.
So here's the scenario that we have before us in order to make
this live this kind of live coding and applied spacey a
little bit more interesting imagine in this scenario we
have a client and the client is a stockbroker or somebody
who's interested in investing and what they want to be able
to do is look at news articles like those coming out of Reuters
and they want to find the news articles that are the most
relevant to what they need to actually search for and read
for the day.
So they want to find the ones that deal with their their
personal stocks their holdings or maybe their the specific
index that they're actually interested in.
So what this client wants is a way to use spacey to automatically
find all companies referenced within a text all stocks
referenced within a text and all indexes referenced within
the next text and maybe even some stock exchanges as well.
Now on the actual textbook if you go through to this chapter
which is number 10 you're going to find all the kind of
solutions laid out for you what I'm going to do throughout
the next 30 or 40 minutes is kind of walk through how I might
solve this problem at least on the surface.
This is going to be a rudimentary solution that demonstrates
the power of spacey and how you can apply it in a very short
period of time to do some pretty custom tasks such as financial
analysis with that structured data that you've extracted you
can then do any number of things what we're going to start off
with though is importing spacey and importing pandas as PD if
you're not familiar with pandas I've got a whole tutorial
series on that on my channel Python tutorials for digital
humanities even though it has digital humanities in the title
it's for kind of everyone but go through if you're not familiar
with pandas and check that out you're not really going to need
it for for this video here you're going to just need to
understand that I'm using pandas to access and grab the data
that I need from a couple CSV files or comma separated value
files that I have.
So the first thing that we need to do is we need to create
what's known as a pandas data frame and this is going to be
equal to PD dot read CSV and I actually have these stored in
the data sub folder in the repo you have free access to these
they're a little tiny data sets that I cultivated pretty
quickly they're not perfect but they're good enough for our
purposes and we're going to use the separator keyword argument
which is going to say to separate everything out by tab
because these are CSV files tab separated value files and we
have something that looks like this so what this stocks dot
CSV file is is it's all the symbols company names industry
and market caps for I think it's around five thousand seven
hundred different stocks five thousand eight hundred and seventy
nine and so what we're going to use this for is as a way to
start working into an entity ruler all these different symbols
and company names what we want to do is we want to use these
symbols to work into a model as a way to grab stocks that might
be referenced and you can already probably start to see a
problem with this capital a here we're going to get to that
in a little bit and we want to grab all the company names
so we can maybe create two different entity types from
this data set stock and company so let's go through and make
these into lists so they're a little bit more so let's go
through and make these into lists so they're a little bit
more manageable what we need to do is we need to create a list
of symbols and that's going to be equal to DF dot symbol dot
two list this is a great way to do it and pandas so you can
kind of easily convert all these different columns into
different lists that you can work with in Python so companies
is going to be equal to DF dot company and name I believe
the name was two list and just to demonstrate how this works
let's print off symbols we're going to print up to 10 and
you can kind of see we've managed to take these columns now
and kind of build them into a simple Python list so what can
we do with that well one of the things that we can do is we
can use that information to start cultivating an entity
ruler but remember we want more things than just one or two
kind of in our entity ruler we don't just want stocks and we
don't just want companies we also want things like indexes
we're going to get to that in just a second though for right
now let's try to work these two things into an entity ruler
how might we go about doing that well as you might expect
we're going to create a fairly simple entity ruler so we're
going to say is nlp is going to be equal to spacey dot blank
we don't need a lot of fancy features here we're just going
to have a blank model that's just going to hose host an
single entity ruler that's going to be equal to nlp dot add
underscore pipe and this is going to be entity ruler and now
what we need to do is we need to come up with a way to go
through all of these different symbols and add them in so we
can say for symbol and symbols we want to say patterns dot
append and we're going to make a an empty list of patterns
up here and what we're going to append is that dictionary that
you met when we talked about the entity ruler and I believe
it was chapter five yeah and what this is going to have
there are two things label which is going to correspond to
stock in this case and it's going to have a pattern and
that's going to correspond to the pattern of the symbol so
we're going to say symbol and what that lets us do is kind of
go through and easily create and add these patterns and and
we can do the same thing for company remember it's never a
good idea to copy and paste in your code I am simply doing
it for demonstration purposes right now this is not polished
code by any stretch of the imagination and what we can do
here now is we can do the same thing loop over the different
companies and add each company and so what this is doing is
it's creating a large list of different patterns that the
entity ruler will use to then go through and as we create the
a doc object over that sample Reuters text I just showed you
a second ago which we should probably just go ahead and
pull up right now I'm going to copy and paste it straight
from the textbook.
Let's go ahead and execute that cell and we're going to add
in this text here it is a little lengthy but it'll be all
right and what we're going to do now is we're going to iterate
over create a doc object to iterate over all of that.
And our goal here is going to be able to say for int and doc
dot ends we want to have extracted all of these different
entities so we can say print off and dot text and dot label
and let's see if we succeeded.
And we have to add in our patterns to our entity ruler so
remember we can do this by saying ruler dot add patterns.
Patterns there we go.
That's what this error actually means and now when we do it
we see that we've been able to extract Apple as a company
Apple as a company Nasdaq everything's looking pretty good
but I notice really quickly that I wasn't actually able to
extract Apple as a stock and I've also got another problem
I've extracted to the lowercase TWO as a stock as well why
have these two things are as a company. Well it turns out in
our data set we've got to TWO that is a company name that's
almost always going to be a false positive and we know that
that kind of thing might be better off worked into a machine
learning model for right now though we're going to work
under the presumption that anytime we encounter this kind
of obscure company TWO as a lowercase it's going to be a
false positive. I also have another problem I know for a
fact that Apple the stock is referenced within this text to
make it a little easier. Let's see it right here and notice
that it didn't find it to make this a little easier to
display. Let's go ahead and display what we're looking at
as the splacy render so what we can do is we can use that the
splacy render that we met a little bit ago in this video.
So in order to import this if you remember we need to say
from spacey import display see and that's going to allow us
to actually display our entities. Let's go ahead and put
this however on a different cell just so we don't have to
execute that every time and we're going to say at splacy render
and we're going to render the doc object with a style that's
equal to ENT and we can see that we've got our text now
popping out with our things labeled and you can see pretty
quickly where we've made some mistakes where we need to
incorporate some things into our entity ruler. So for example
if I'm scrolling through this is gray little ugly we can change
the colors that's beyond the scope of this video though but
let's keep on going down we notice that we have Apple dot
IO and yet this has been missed by our entity ruler. Why has
this been missed well. Spacey as a tokenizer is seeing this
as a single token so Apple dot Oh the letter Oh capital letter
Oh why is that well I didn't know about this but apparently
it does has to deal with kind of the way in which stock indices
are I think it's on the NASDAQ kind of structure things so
what can we do well we've got a couple different options here
I know that these go through all different letters from A to
Z so we can either work with the string library or we can do
is we can import a quick list that I've already written out
of all the different letters of the alphabet and iterate
through those with our ruler up here.
Let's go ahead and add these letters right there and we can
kind of iterate through those and whenever a stock kind of
pops out with that kind of symbol plus any occurrence where
it's got a period followed by a letter in those scenarios we
want that to be flagged as a stock as well so what we can do
is we can add in another thing right here add in another
pattern and this is now going to be symbol plus we're going
to add in F string right here a formatted string any occurrence
of L we can set up a loop to say for L and letters do this
and what this is going to allow us to do is to look for any
instance where there is a symbol followed by a period
followed by one of these capitalized letters that I just
copied and pasted in so if we do that we can execute that cell
and we can scroll down and we can now do the exact same thing
that we just did a second ago and actually display this
and now we're finding these stocks highlighted as stock so
we're successfully getting these stocks and extracting them
we've got a few different things that our client wants to
also extract though they don't want to just extract companies
and they don't want to just extract stock and they want to
also extract stock exchanges and indexes but we have one
other problem and go ahead and get rid of this as the display
mode and switch back to just our set of entities because
it's a little easier to read for this example we've got
another problem and we see we have a couple other stocks
popping out we now know that Kroger stock is here the n i
o dot n stock is in this text as well now we're starting to
see a greater degree of specificity for right now I'm
going to include two as a set of a stop technical term would
be like a stop or something that I don't want to be included
into the model so I'm going to make a list of stops and
we're just going to include two in that and we're going to
say for company and companies do all this if company not in
stops we want this to occur what this means now is that our
our pipeline while going through and having all of these
different things all these different rules it's also going
to have another rule that looks to see if there's a stop or
if this company name is this stop and if it is then we want
it to just kind of skip over and ignore it and if we go
through we notice that now we've successfully eliminated
this what we would presume to be a consistent false positive
something that's going to come up again and again as a false
positive great so we've been able to get this where it works
now pretty well what I also want to work into this model if
you remember though are things like indexes fortunately
I've also provided for us a list of all different indexes that
are available from I believe it's like everything like the
Dow Jones is about 13 or 14 of them let's go ahead and import
those up above and let's do that right here in this cell so
it kind of goes in sequential order that follows better with
the textbook to so it's a new data frame object this is going
to be equal to P a PD dot read CSV we're going to read in that
data file that I've given us and that's going to be the indexes
dot T SV with a separator that's equal to a tab let's see what
that looks like and this is what it looks like so all these
different indices now I know I'm going to have a problem right
out of the gate and that's going to be that sometimes you're
going to see things referenced as SNP 500 I don't know a lot
about finances but I know that you don't always see it as
SNP 500 index but I do think that these index symbols are
also going to be useful so like I did before I'm going to convert
these things into a list so it's a little easier for me to work
with in a for loop and I'm going to say indexes is equal to
DF2 dot index name so grabbing that column to list and index
symbols is equal to DF2 dot index symbol dot to list and
both of these are going to be different and they're both going
to have the same exact entity label which is going to be an
index and so let's go ahead and iterate over these and add them
in as well so I'm going to go ahead and do that right now
for indexes and indexes we want this label to be index we
want this to be index here so it's going to allow us to kind
of go through and grab all those and we want to do the same
thing with index symbols keep these a little separated here
index symbols and that allows for us to do that and let's go
ahead and without making any adjustments let's see let's see
how this does with these new patterns that we've added in
and because we've already got this text loaded into memory
I'm going to go ahead and put this right here doc is going to
be equal to nlp text for int and doc and print off and dot
text and dot label and we can kind of go through and we're
actually now able to extract some indexes and I believe when
I was looking at this text really quickly though I noticed
that there was one instance at least where we had not only
the index referenced but also a name like S&P 500 right here
S&P 500 notice that it isn't found because it doesn't have
the name index after it and notice also that none of our
our symbols are being found because they all seem to be
preceded by a dot so in this case a dot J a DJI and so that's
something else that I have to work into this model and the
list I gave the data set that's not there so I need to collect
a list of these different names and work those into an entity
ruler as well but for right now let's ignore that and focus
on including this S&P 500 so how can I get the S&P 500 in
there from the list I already gave it well what I can do is
I can say okay so under these indices not only do I want to
add that specific pattern let's go ahead and break these things
up into different words and so I'm going to have the words is
equal to index dot split and then I'm going to make a
presumption that the the first two words so the S&P 500 the
S&P 400 are sometimes going to be referenced by themselves so
what I want to do is I want to work that into the model as
well and I want to say we're going to say patterns dot
append copy this as well we can say something like dot join
words up until the second index and let's go ahead and work
that into our model in our patterns or pipeline and print
off our NLP again and you'll find that we've now been able to
capture things like S&P 500 that aren't proceeded by the word
index and we see that we in fact have S&P 500 is now popping
out time and again that's fantastic I'm pretty happy with
that now we're we're getting a deeper sense of what this
text is about without actually having to read it we know that
it's going to deal heavily with Apple and we know that it's
also going to tangentially deal with some of these other
things as well but I also want to include into this into this
pipeline the ability for the entity ruler to not just find
these things but I also wanted to be able to find different
stock exchanges so I've got a list I cultivated for different
stock exchanges which are things like NYSE things like that
so I can say DS3 is going to be equal to PD dot read CSV data
backslash stock exchanges dot TSV and then the separator is
going to be again a tab and let's take a look at what this
looks like.
Stanges there we go.
There we are and we have something that looks like this
a pretty a pretty large CSV file CSV file sorry that's got a
bunch of different rows the ones I'm most interested in well
there's a couple actually I'm interested in specifically the
Google Prefix and this description the description has
the actual name and the Prefix has this really nice abbreviation
that I've seen pop out a few different times such as Nasdaq
here if we keep on going down we would see different things
as well NYSE these are kind of different stock exchanges.
So let's pop back down here and let's go ahead and convert
those two things into individual lists as well so we're going
to say exchanges it's going to be equal to DF3 dot ISO Mike
dot to list and then I'm also going to grab the F3 dot sorry
Google I have to do this as a dictionary because it's the
way the data sets cultivated it's got a space in the middle
this is a common problem that you run into and then I also
want to know grab all of these exchanges as well so I'm going
to say also on top of that DF3 dot description to list so I'm
making a large list exchanges and I get this here because it
says Google Prefix isn't an actual thing and in fact it's
prefix with an I and now we actually are able to get all
these things extracted so what I want to do now is I want to
work all these different symbols and descriptions into into
the model as well or into the pipeline as well so I can say
for for E and exchanges I want to say patterns dot append
and I want to do a label that's going to be let's do stock
exchange and then the next thing I want to do is a pattern
and that's going to be equal to in this case E as we're going
to see this is not adequate enough we need to do a few
different things to really kind of work this out but it's going
to be a good enough to at least get started
and it's going to take it just a second
and the main thing that's happening right now are these
different for loops so if we keep on going down we now see
that we were able to extract the NYSE stock exchange so we've
not only been able to work into a pipeline in a very short
order maybe about 20 30 minutes we've been able to work
into a pipeline all of these different things that are coming
out we do however see a couple problems and this is where I'm
going to leave it though because you've got the basic
mechanics down now comes time for you being a domain expert
to work out and come up with rules to solve some of these
problems Nasdaq is not a company so there's a problem with
the data set or Nasdaq is listed as a company name and one of
the data sets we need to work that out where Nasdaq is never
referenced as a company we have the S&P and is now being
coming out correctly as S&P 500 there might be instances
where just S&P is referenced which I think in that context
would probably be the S&P 500 but nevertheless we've been
able to actually extract these things sometimes the Dow Jones
Industrial Average might just be referenced to Dow Jones so
this index might just be these first two words I know that's
a common occurrence we've also seen that we weren't able to
extract some of those things that were a period followed by
a symbol that referenced the actual index itself nevertheless
this is a really good starting point and you can see how just
in a few minutes you're able to generate this thing that can
extract information from unstructured text at the end of
the day like I said in the introduction to this entire
video that's one of the essential tasks of NLP designing
this and implementing it is pretty quick and easy perfecting
it is where the time really is to get this financial analysis
entity ruler working really well where it has almost no false
positives and almost never misses a true a true positive it
would take maybe a few more hours of just some kind of working
and eventually there are certain things you might find that
would work better in a machine learning model nevertheless
you can see the degree to which rules based approaches in
Spacey can really accomplish some pretty robust tasks with
minimal minimal amount of code so long as you have access to
or have already cultivated the data sets required.
Thank you so much for watching this video series on Spacey
an introduction to basic concepts of natural language
processing linguistic annotations in Spacey vectors pipelines
and kind of rules based Spacey you've enjoyed this video
please like and subscribe down below and if you've also found
this video useful consider joining me on my channel Python
tutorials for digital humanities if you have like this and
found this video useful I'm envisioning a second part to
this video where I go with the machine learning aspects of
Spacey if you're interested in that let me know in the
comments down below and I'll make a second video that
corresponds to this one.
Thank you for watching and have a great day.

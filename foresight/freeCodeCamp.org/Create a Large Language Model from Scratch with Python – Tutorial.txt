Learn how to build your own large language model from scratch.
This course goes into the data handling, math and transformers behind large language models.
Elliot Arledge created this course.
He will help you gain a deep understanding of how LLMs work and how they can be used in various applications.
So let's get started.
Welcome to Intro to Language Modeling.
In this course, you're going to learn a lot of crazy stuff.
Okay, I'm just going to give you a heads up.
It's going to be a lot of crazy stuff we learn here.
However, it will not be insanely hard.
I don't expect you have any any experience in calculus or linear algebra.
A lot of courses out there do assume that, but I will not.
We're going to build up from square one.
We're going to take baby steps when it comes to new fundamental concepts in math and machine learning.
And we're going to take a larger steps once things are fairly clear and they're sort of easy to figure out.
That way we don't take forever just taking baby steps through every little concept.
This course is inspired by Andre Karpathy's.
Building a GPT from scratch lecture.
So shout out to him.
And yeah, we don't assume you have any experience, maybe three months of Python experience.
Just so the syntax is sort of familiar and you can.
You're able to follow along that way, but no matter how smart you are, how quick you learn.
The willingness to put in the hours is the most important because this is material that you won't normally come across.
So as long as you're able to put in that constant effort, push through these lectures, even if it's hard, take a quick break, grab a snack, whatever you need to do, grab some water.
Water is very important.
And yeah, hopefully you can make it to the end of this.
You can do it.
Since it's free code camp, everything will be local computation, nothing in the realm of paid data sets or cloud computing.
We'll be scaling the data to about 45 gigabytes for the entire training data set.
So have 90 reserved so we can download the initial 45 and then convert it to an easier to work with 45.
So yeah, if you don't actually have 90 gigabytes reserved, that's totally fine.
You can just download a different data set and sort of follow the same data pipeline that I do in this video.
Through the course, you may see me switch between Mac OS and Windows.
The code still works all the same, both operating systems, and I'll be using a tool called SSH.
It's a server that I can connect from my MacBook to my Windows PC that I'm recording on right now, and that will allow me to execute, run, build, whatever, do anything coding related, command prompt related on my MacBook.
So I'll be able to do everything on there that I can my Windows computer, it'll just look a little bit different for the recording.
So why am I creating this course?
Well, like I said before, a lot of beginners, they don't have the fundamental knowledge like calculus linear algebra to help them get started or accelerate their learning in this space.
So I intend to build up from baby steps and then larger steps when things are fairly simple to work with.
And I'll use logic analogies and step by step examples to help concept conceptualize rather than just throw tons of formulae at you.
So with that being said, let's go ahead and jump in to the good stuff.
So in order to develop this project step by step, we're going to use something called Jupyter notebooks.
And you can sort of play with these in the Anaconda prompt or at least launch them from here.
So Anaconda prompt is just great for anything machine learning related.
So make sure to have this installed.
I will link a video in the description so that you can sort of set this up and install it step by step guide in there.
So we can do from this point is sort of just set up our project and initialize everything.
So I'm going to do is just head over into my directory that I want to be Python testing.
We're going to make a directory free code camp GPT course.
And then from this point, we're going to go and make a virtual environment.
So virtual environment, it will initially in your desktop, you will have just all of your Python libraries, all your dependencies there just floating around.
And what the virtual environment does is it sort of separates that.
So you have this isolated environment over here, and you can just play around with this however you want.
And it's completely separate so that won't really cross with all of the global libraries that you have all the ones that just affect the system.
When you're not in a virtual environment, if that makes sense.
So we're going to go ahead and set that up right now by using Python dash M.
And then we're going to go V and V for virtual V and V and then CUDA.
So the reason why we say CUDA here is because later when we try to accelerate our learning or the models learning, we're going to need to use GPUs.
GPUs are going to accelerate this a ton.
And basically CUDA is just that little feature in the GPU that lets us do that.
So we're going to make an environment called CUDA.
I'm going to go and press enter.
It's going to do that for us going to take a few seconds.
So now that's done, we can go ahead and do CUDA.
And we're just going to basically activate this environment so we can start developing in it.
I'm going to go backslash, we're going to go scripts, and then activate.
So now you can see it says CUDA base.
So we're in CUDA and then secondary base.
So it's going to prioritize CUDA.
So from this point, we can actually start installing some stuff, some libraries here.
So we can go pip three install Matt plot lib numpy.
We're going to use p y l m z a l z m a.
And then what are some other ones?
We're going to do IPY kernel.
This is for the actual Jupyter notebooks and being able to bring the CUDA virtual environment into those notebooks.
So that's why that's important.
And then just the actual Jupyter notebook feature.
So go and press enter.
Those are going to install.
That's going to take a few seconds to do.
So what might actually happen is you'll get a build error with p y l z m a, which is a compression algorithm.
And don't quote me on this, but I'm pretty sure it's based in C plus plus.
So you actually need some build tools for this.
And you can get that with visual studio build tools.
So what you're you might see, you might see a little error and basically go to that website.
You're going to get this right here.
So just go ahead and download build tools.
What's going to download here, you're going to click on that.
It's going to, it's going to set up.
And then you're going to go ahead and click continue.
And then at this point, you can go ahead and click modify if you see this here.
And then you might get to a little workloads section here.
So once you're at workloads, that's good.
What you're going to make sure is that you have these two checked off right here.
Just make sure that you have these two.
I'm not sure what desktop particularly does.
It might help, but it's just kind of good to have some of these build tools on your PC anyways, even for future projects.
So just get these two for now.
That'll be good.
And then you can click modify over here if you wanted to modify just like that.
And then you should be good to rerun that command.
So from this point, what we can actually do is we're going to install torch and we're actually going to do it by using pip install three install torch.
We're not going to do it like this.
What we're actually going to do is we're going to use a separate command and this is going to install CUDA with our torch.
So it's going to install the CUDA extension, which will allow us to utilize the GPU.
So it's just this command right here.
And if you want to find like a good command to use, what you can do is go to the pie torch docs, just go to go to get started.
And then you'll be able to see this right here.
So we have stable windows pip Python and CUDA 11.7 or 11.8.
So I just clicked on this.
And since we aren't going to be using torch vision or torch audio, I basically just did pip three install torch.
And then with this index URL for the CUDA 11.8.
So that's pretty much all we're doing there to install CUDA.
That's part of our torch.
So we can go ahead and click enter on this.
So great.
We've installed a lot of things, a lot of libraries, a lot of setup has been done already.
What I want to check now is just to make sure that our Python version is what we want.
So I've done version 3.10.9.
That's great.
If you're between 3.9, 3.10, 3.11, that's perfect.
So if you're in between those, it should be fine.
At this point, we can just jump right into our Jupyter Notebook.
So the command for that is just Jupyter Notebook.
It's about like that.
Click enter.
It's going to send us into here.
And I've created this little biogram.ipynb here in my VS Code.
So pretty much you need to actually type some stuff in it.
And you need to make sure that it has the ipynb extension or else it won't work.
So if it's just ipynb and doesn't have anything in it, I can't really read that file for some reason.
So just make sure you type some stuff in it.
Open that in VS Code.
Type, I don't know, a equals 3 or str equals banana.
I don't care.
At this point, let's go ahead and pop into here.
So this is what our notebook is going to look like.
And we're going to be working with this quite a bit throughout this course.
So what we're going to need to do next here is make sure that our virtual environment is actually inside of our notebook.
And make sure that we can interact with it from this kernel rather than just through the command prompt.
So we're going to go ahead and check here.
And I have a virtual environment here.
You may not, but all we're going to do is basically go into here.
We're going to end this.
And all we're going to do is we're going to go ahead and do Python dash M and then ipy kernel install.
User, you'll see why we're doing this in the second user name equals CUDA.
This is from the virtual environment we initialized before.
So that's the name of the virtual environment.
And then the display name, how it's actually going to look in the terminal is going to be display name.
We'll just call it CUDA GPT.
I don't know.
That sounds like a cool name.
And I'm going to press enter.
It's going to make this environment for us great installed.
Good.
So we can go and run our notebook again and we'll see if this changes.
So we can go ahead and pop into our bi-gram again, kernel, change kernel, boom, CUDA GPT.
Let's click that.
Sweet.
So now we can actually start doing more and just sort of experimenting with how the notebooks
work and actually how we can build up this bi-gram model and sort of learning how language
models work from scratch.
So let's go ahead and do that.
Now that we jump into this actual code here, what I want to do is delete all of these.
Good.
So now what I'm going to do is just get a small little data set, just very small for us to
work with that we can sort of try to make a bi-gram out of, something very small.
So what we can do is go to this website called Project Gutenberg and they basically just
have a bunch of free books that are licensed under Creative Commons.
So we can use all of these for free.
So let's use the Wizard of Oz.
Put it at the end of Wizard of Oz.
Great.
So what we're going to want to do is just click on plain text here.
Great.
So now we can go Ctrl S to save this and then we could just go Wizard of Oz, Wizard underscore
of underscore Oz.
Good.
So now what I'm going to do is we should probably drag this into, we should drag this into our
folder here.
I'm just going to pop that into there.
Good stuff.
Did that work?
Sweet.
So now we have our Wizard of Oz text in here, we can open that.
What we can do is start of this book.
Okay.
So we can go ahead and go down to when it starts.
Sweet.
So maybe we'll just cut it here.
That'd be a good place to start.
Just like that.
I'll put a few spaces.
Good.
So now we have this book.
We go to the bottom here just to get rid of some of this other licensing stuff, which
might get in the way with our predictions in the context of the entire book.
So let's just go down to when that starts.
End of the book.
Okay.
So we've gotten all that.
That is done.
Get rid of the illustration there.
Perfect.
So now we have this Wizard of Oz text that we can work with.
Let's close that up.
233 kilobytes.
Awesome.
Very small size.
We can work with this.
This is great.
So we have this wizard of Oz dot txt file.
And what are we going to do with that?
Well, we're going to try to train a transformer or at least a background language model on
this text.
So in order to do that, we need to sort of learn how to manage this text file, how to
open it, et cetera.
So we're going to go ahead and open this and do wizard of Oz.
Like that.
And we're going to open in read mode.
And then we're going to use the encoding utf 8 just like that.
So this is the file mode that you're going to open in.
There's read mode.
There's write mode.
There's read binary.
There's write binary.
And those are really the only ones we're going to be worrying about for this video.
The other ones you can look into in your spare time if you'd like to.
I've already seen using those four for now.
And then the encoding is just what type of character coding are we using?
That's pretty much it.
We can just open this as F short for file.
I'm going to go text equals f dot read.
I'm going to read this file stored in a string variable.
And then we can print some stuff about it.
So we can go print the length of this text.
Run that.
We get the length of the text.
We could print the first 200 characters of the text.
Sure.
So you have the first 200 characters.
Great.
So now we know how to, you know, just play with characters.
At least just see what the characters actually look like.
So now we can do a little bit more from this point, which is going to be encoders.
And before we get into that, what I'm going to do is put these into a little vocabulary
list that we can work with.
So all I'm going to do is I'm going to say we're going to make a charge variable.
So the charge is going to be all the charge or all the characters in this text piece.
So we're going to make a sorted set of text here, and we're going to just print out charge.
So look at that.
We have a giant array of all these characters.
So now we can, what we can do is we can use something called a tokenizer and a tokenizer
consists of an encoder and a decoder.
What an encoder does is it's actually going to convert each character or sorry, each element
of this array to an integer.
So maybe this would be a zero.
This would be a one, right?
So a new, a new line or an enter would be a zero, a space would be a one exclamation
mark would be a two, et cetera, right?
All the way to the length of them.
And then what we could do is we could even, we could even print the length of these characters.
So you can see how many there actually are.
So there's 81 characters in the entire, in the entire Wizard of Oz book.
So I've written some code here that is going to do that job for us, the job of tokenizers.
So what we do is we just use a little generator, some generator for loops here,
a generator for loops rather, and we make a little mapping from strings to integers
and integers to strings, given the vocabulary.
So we just enumerate through each of these.
We have one assignment, first element assigned to a one, second assigned to a two, et cetera, right?
That's basically all we're doing here.
And we have an encoder and a decoder.
So let's say we wanted to convert the string hello to integers.
So we go encode, and we could do hello, just like that.
And then we could go ahead and print this out.
Perfect.
Let's go ahead and run that.
Boom.
So now we have a conversion from characters to integers.
And then if we wanted to maybe convert this back, so decode it,
sort this in a little, maybe decoded hello equals that.
And then we could go or encoded rather encoded hello.
And then we could go decoded.
Hello is equal to we go decode and we can use the encoded hello.
So we're going to go ahead and encode this into integers.
And then we're going to decode the integers back to a character format.
So let's go ahead and print that out.
We're going to go ahead and print the decoded hello.
Perfect.
So now we get that.
So I'm going to fill you in on a little background information about these tokenizers.
So right now we're using the character level tokenizer, which takes basically each character
and converts it to an integer equivalent.
So we have a very small vocabulary and a very large amount of tokens to convert.
So if we have 40,000 individual characters, it means we have a small vocabulary to work
with, but a lot of characters to encode and decode, right?
If we have, if we work with maybe a word level tokenizer, that means we have a ton, like
every single word in the English language, I mean, if you're working with multiple languages,
this could be like, you know, a lot, very large amount of tokens.
So you're going to have like maybe millions or billions or trillions if you're, if you're
doing something weird.
But in that case, you're going to have a way smaller set to work with.
So you're going to have very large vocabulary, but a very small amount to encode and decode.
So if you have a subword tokenizer, that means you're going to be somewhere in between a character
level and a word level tokenizer, if that makes sense.
So in the context of language models, it's really important that we're efficient with our data
and just having a giant string might not work the best.
And we're going to be using a machine learning framework called pi torch or torch.
So I've imported this right here.
And pretty much what this is going to do is it's going to handle a lot of the math, a lot of the calculus for us as well.
A lot of the linear algebra, which involves a type of data structure called tensors.
So tensors are pretty much matrices.
If you're not familiar with those, that's fine.
We'll go over them more in the course.
But pretty much what we're going to do is we're going to just put everything inside of a tensor so that it's easier for pi torch to work with.
So I'm going to go ahead and delete these here.
And all we can do is just make our data element.
We could this is going to be the entire text data of the entire Wizard of Oz.
So we could go ahead and make this data equals and we're going to go torch tensor.
And then we're going to go and code.
We're going to put the text inside of that.
So we're going to go ahead and encode this text right here.
And we're going to make sure that we have the right data type, which is a torch dot long data type equals torch dot long.
This basically means we're just going to have this as a super long sequence of integers.
And yeah, let's go see what we can do with this torch tensor element right here.
So I've just written a little print statement where we can just print out the first 100 characters or 100 integers of this data.
So it's pretty much the same thing in terms of working with arrays.
It's just a different type of data structure in the context of pi torch sort of easier to work within that way.
Pi torch is just primarily revolved around tensors and modifying them, reshaping, changing dimensionality, multiplying, doing dot products, which that sounds like a lot.
But we're going to go over some of this stuff later in the course just about how to do all this math.
We're going to actually go over examples on how to multiply this matrix by this matrix, even if they're not the same shape and even dot prodding, dot producting, that kind of stuff.
So next I'm going to talk about is something called validation and training splits.
So why don't we just, you know, use the entire text document and only train on that entire text corpus?
Why don't we train on that?
Well, the reason we actually split into training and validation sets, I'm going to show you right here.
So we have this giant text corpus.
It's a super long text file.
Think of it as a, you know, an essay, but a lot of pages.
So this is our entire corpus and we make our training set, you know, 80% of it.
So maybe this much.
And then the other validation is this 20% right here.
Okay.
So if we were to just train on the entire thing, after a certain number of iterations, it would just memorize the entire text piece and it would be able to, you know, simply write it, just write it out.
It would have it in the entire thing memorized.
It wouldn't really get anything useful out of that.
You would only know this document.
But what the purpose of language modeling is, is to generate text that's like the training data.
And this is exactly why we put into splits.
So if we, if we run our training split right here, it's only going to know 80% of that entire corpus.
And it's only going to generate on that 80% instead of the entire thing.
And then we have our other 20%, which only knows 20% of the entire corpus.
So the reason why we do this is to make sure that the generations are unique and not an exact copy of the actual document.
We're trying to generate text that's like the document.
Like, for example, in Andre Carpathi's lecture, he trains on Shakespearean text, an entire piece of Shakespeare.
And the point is to generate Shakespearean like text, but not exactly what it looked like.
Not that exact, you know, 40,000 lines or like a few thousand lines of that entire corpus, right?
We're trying to generate text that's like it.
So that's the entire reason, or at least that's most of the reason why we use train and vowel splits.
So you might be wondering, you know, like, why is this even called the bi-gram language model?
I'm actually going to show you how that works right now.
So if we go back to our whiteboard here, I've drawn a little sketch.
So if we have this piece of content, the word hello, let's just say it, we don't have to encode it as any integers right now.
We're just working with characters.
Pretty much we have two, right?
So by means to the by prefix means two.
So we're going to, we're going to have a bi-gram.
So given maybe, I mean, there's nothing before an H in this content.
So we just assume that's the start of content, and then that's going to point to an H.
So H is the most likely to come after the start.
And then maybe given an H, we're going to have an E, then given an E, we're going to have an L, then given an L,
we're going to have another L, and then L leads to O, right?
So maybe there's going to be some probabilities associated with these.
So that's pretty much how it's how it's going to predict right now.
It's only going to consider the previous character to predict the next.
So we have given this one, we predict the next.
So there's two, which is why it's called bi-gram language model.
So I ignore my terrible writing here, but we're actually going to go into how we can train the bi-gram language model to do what we want,
how we can actually implement this into a neural network, an artificial neural network, and train it.
So we're going to get into something called block size, which is pretty much just taking a random snippet out of this entire text corpus here,
just a small snippet, and we're going to make some predictions and we're going to make some targets out of that.
So our block size is just a bunch of encoded characters or integers that we have predictions and targets.
So let's say we take a small little size of maybe block size of five, okay?
So we have this tiny little tensor of five integers and these are our predictions.
So given some context right here, we're going to be predicting these and then we have our targets, which would be offset by one.
So notice how here we have a five and then here the five is outside and then this 35 is outside here and now it's inside.
So all we're doing is just taking that block from the predictions and in order to get the targets, we just offset that by one.
So we're going to be accessing the same indices.
So at index zero, it's going to be five, index zero is going to be 67, right?
So 67 is following five in the bi-gram language model.
So that's pretty much all we do.
So let's just look at how much of a difference is that target away from or how much far is the prediction away from the target.
And then we can optimize for reducing that error.
So the most basic Python implementation of this in the character level tokenizers or the character level tokens rather would be just simply this right here.
We would take a little snippet random.
It would be pretty much just from the start or some whatever just some snippet all the way from the start of the snippet up to block size.
So five.
Ignore my terrible writing again.
And then this one would just be it would just be one up to block size or five plus one.
So we'll be up to six, right?
And that's that's pretty much all we do.
This is exactly what it's going to look like in the code.
So I've written some code here that does exactly what we just talked about in Python.
So I define this block size equal to eight just so you can kind of see what this looks like on a larger scale, a little bit larger.
And just what we wrote right there in the Jupyter notebook this position zero up to block up to block size and then offset by one.
So we make it position one up to block size plus one little offset there.
And we pretty much just wrote down here X as our predictions as and why as our targets, and then just a little for loop to show what the prediction and what the targets are.
So this is what this looks like in Python, right, we can do predictions.
But this isn't really scalable yet.
This is sequential right sequential.
It is another way of describing what the CPU does CPU can do a lot of complex operations very quickly.
That only happens sequentially it's this one and this task and this task and this task, right.
But with GPUs, you can do a little bit more simpler tasks, but very, very quickly, or in parallel.
So we can do a bunch of very small or not computationally complex computation, and a bunch of different little processors that aren't as good, but there's tons of them.
So pretty much what we can do is we can take each of these little blocks, and then we can stack them and push these to the GPU to scale our training a lot.
So I'm going to illustrate that for you right now.
So let's just say we have a block.
Okay, block looks like this.
And we have some we have some integers in between here.
So this is a block.
Okay.
Now, if we want to make multiple of these, we're just going to stack them.
So we're going to make another one.
Another one.
Another one.
So let's say we have four batches.
Okay.
Or sorry, four blocks.
So we have four different blocks that are just stacked on top of each other.
And we can represent this as a new hyper parameter called batch size.
This is going to tell us how many of these sequences can we actually process in parallel.
So the block size is the length of each sequence, and the batch size is how many of these are we actually doing at the same time.
So this is a really good way to scale language models.
And without these, you can't really expect any fast training or good performance at all.
So we just went over how we can actually get batches or rather how we can use batches to accelerate the training process.
And we can, it just takes one line to do this actually.
So all we have to do is call this little function here saying if CUDA dot torch dot CUDA is available, we'll just check if the GP was available based on your CUDA installation.
And if it's available, like it says it's available, we'll set the device to CUDA else CPU.
So we're going to go and print out the device here.
So that's going to run and we get CUDA.
So that means we can use the GPU for a lot of our processing here.
And while we're here, I'm actually going to move up this hyper parameter block size up to the top block size.
And then we're going to use batch size, which is how many blocks we're doing in parallel.
And we're just going to make this four for now.
So these are our two hyper parameters that are very, very important for training.
And you'll see that why these become much more important later when we scale up the data and use more complex mechanisms to train and learn the patterns of the language based on the text that we give it.
And if it doesn't work right away, if it's a new Jupyter notebook doesn't work right away, I'd recommend just hitting control C to cancel this hit it a few times might not work the first.
It'll shut down and you just go up Jupyter notebook again and then enter.
And then after this is done, you should be able to just restart that and it will work.
Hopefully.
There we go.
So I go in restart and clear outputs.
And we can run that.
See, we get boo.
So awesome.
Now, let's try to do some actual cool pie torch stop.
So we're going to go in and import torch here.
And then let's go ahead and try this random feature.
So you go random.
We'll do equals torch dot random.
And then let's say we go minus 100 to 100.
And then in brackets, we go six, just like that.
So if we want to print this out here, or we could just go random like that.
Run this block first.
Good.
And boom.
So we get a tensor type.
And all these numbers are we have we have six of them.
So 123456.
And they're between negative 100 and 100.
So we're going to have to keep this in mind right here when we're getting our random
batches from this giant text corpus.
So let's try out a new one.
Let's just try.
We can make we can make tensors.
We've done this before.
So you do tensor equals torch dot tensor.
So if you go 0.1, 1.2, here, I'll just copy and paste one right here.
So we do this.
And we can just do tensor and we'll get exactly this.
So we get a three by two matrix.
Now we're going to try a different one called zeros.
So zeros is just torch dot zeros.
And then inside of here, we could just do the dimensions or the shape of this.
So two by three, and then we could just do zeros.
And then go ahead and run that.
So we get a two by three of zeros.
And these are all floating point numbers, by the way.
Maybe we could try ones.
Now I know ones is pretty fun ones.
So we both torch dot ones.
It's pretty much the same as zeros.
We could just do like maybe three by four and then print that ones out.
So we have a three by four of ones.
Sweet.
So what if we do input equals torch dot empty.
We can make this two by three.
So these are interesting.
These are pretty much a bunch of very either very large or very small numbers.
I haven't particularly found a use case for this yet,
but just another feature that PyTorch has.
We have a range.
So we go arrange equals torch dot arrange.
I could do like five, for example, just do range.
So now we have a tensor just sorted zero or rather starting at zero up to four.
So five, just just like that.
Line space equals torch dot line line space.
Spelling is weird to three, 10, and then steps, for example, equals five.
So it makes sense in a second here, go run.
And we got a line space of steps equals five.
So we have five different ones, boom, boom, boom, boom, boom.
And we go all the way from three to 10.
So pretty much getting all of the constant increments from three all the way up to 10 over five steps.
So you're doing, you're basically adding the same amount every time.
So three plus 1.75 is 4.75 plus another 1.75 is 6.5 and then 8.25 and then 10, right?
So just over five steps, we want to find what that constant increment is.
So that's a pretty cool one.
And then we have, we'll do log space, which is interesting.
Log space equals torch dot log space.
And then we'll go start, start equals negative 10 and equals 10.
These are both start and end.
You can either put these here.
You can either put the start with them, start equals, or you don't have to.
It's honestly up to you.
And then we can put our steps again.
So steps equals every five.
Let's go ahead and run that.
Oops, need to put log space there.
So we get that.
So we start at one of the negative 10.
And then we just do this little increments here.
So it goes 10, negative five, zero plus five times.
Just over five steps.
So that's pretty cool.
What else do we have here?
So we have I, torch dot I.
I just have all these on my second screen here.
So a bunch of examples just written out and we're just kind of visualizing what these can do.
And maybe you might even have your own creative little sparks of thought that you're going to maybe find something else that you can use these for for your own personal projects or whatever you want to do.
So we're just kind of experimenting with these.
What we can do with the basics of pytorch and some of the very basic functions.
So first I will print this out here.
So we get pretty much just a diagonal line and it's in five.
So you get a five by five matrix and pretty much just reduced row each long form.
I don't know how to pronounce it, but that's pretty much what it looks like.
So pretty cool stuff.
Let's see what else we have.
We have empty like.
We have empty like torch dot empty like a and then we'll just say maybe make a equal to make it a torch dot empty.
And then we can go two by three and then data type torch dot int 64 64 bit integers.
And then let's see what happens here empty.
So that's pretty cool.
What else do we have?
Yes, we can do timing as well.
So I'm just going to erase all of these.
You can scroll back in the video just look and maybe experiment with these a little bit, try a little bit more than just what I've done with them, maybe modify them a little bit.
But yeah, I'm actually going to delete all of these here.
And then we can go ahead and do the device equals Cuda and we're going to go ahead and switch this over to the Cuda GPT environment.
Cuda if torch dot Cuda underscore is dot Cuda is available.
And then else you print out our device here and run this Cuda suite.
So we're going to try to do stuff with the GPU now compared to the CPU and really see how much of a difference Cuda or the GPU is going to make in comparison to the CPU when we change the shape and dimensionality.
We're just doing different experiments with a bunch of different tensors.
So in order to actually measure the difference between the GPU and the CPU, I just imported a library called time.
So this comes with the operating system or sorry with with Python.
You don't have to actually install this manually.
So basically what we do is we whenever we call time dot time and then parentheses, it will just take the current time snippet right now.
So start time will be like right now and then end time maybe three seconds later will be, you know, right now plus three seconds.
So if we subtract end time, start time will get a three second difference and that would be the total elapsed time.
And then this little number here, this four will be just how many decimal places we have.
So I can go ahead and run this here.
Time is not defined. Let's run that first.
It's going to take, you know, almost no time at all.
So we can actually increase this if we want to 10 and then run that again.
Again, it's, you know, we're making up pretty much a one by one matrix.
So just a just a zero.
So we're not really going to get anything significant from that.
But anyways, for for actually testing the difference between the GPU and the CPU, what we're going to worry about is that iterative process, the process of forward pass and back propagation through the network.
That's primarily what we're trying to optimize for actually pushing all these parameters and all these model weights to the GPU isn't really going to be the problem.
It'll take maybe a few seconds at most like maybe 30 seconds to do that.
And that's not going to be any time at all in the entire training process.
So what we want to do is just see, you know, which is better NumPy on the CPU or torch using CUDA on the GPU.
So I have some code for that right here.
So we're going to initialize a bunch of matrices here.
So our sorry, tensors, and we have just basically random ones.
So we have a 10,000 by 10,000, all random, all random floating point numbers.
And then we're going to push these to the GPU.
And we have two of these and then same thing for NumPy.
So in order to actually multiply matrices with PyTorch, we need to use this at symbol here.
So we multiply these and we get this new, we get this new random tensor and then we stop it and then we do the same thing over here, except we use NumPy.multiply.
So if I go ahead and run these, it's going to take a few seconds to initialize these and not even a few seconds.
And then we have, see, look at that.
So for the GPU, it took a little while to do that.
And then for the CPU, it didn't take as long.
So this is because there's the shape of these matrices are not really that big.
They're just two dimensional, right?
So it's see, this is something that the CPU can do very quickly because there's not that much to do.
But let's say we want to bump it up a notch.
So if we go to 100, 100, 100, and then maybe we'll throw in another 100 there.
Hopefully that works.
And then we can do, we'll just do the same thing.
So just paste this.
Now if we try to run this again, you'll see that the GPU actually took less than half the time that the CPU did.
And this is because there's, you know, a lot more going on here.
There's a lot more simple multiplication to do.
So the reason why this is so significant is because when we have, you know, millions or billions of parameters in our language model,
we're not going to be doing very complex operations between all these tensors.
They're going to be very similar to what we saw in here.
The dimensionality and shape is going to be very similar to what we're seeing right now.
You know, maybe three or four dimensions.
And it's going to be very easy for a GPU to do this.
They're not complex tasks that we need the CPU to do.
They're not very hard at all.
So when we give this task to parallel processing, it's going to be a ton quicker.
So you're going to see why this matters later in the course.
You're going to see this with some of the hyper parameters we're going to use,
which I'm not going to get into quite yet, but over the next little bit,
you're going to see why the GPU is going to matter a lot for increasing the efficiency of that iterative process.
So this is great.
Now you know a little bit more about why we use the GPU instead of the CPU for training efficiency.
So there's actually another term that we can use called a percentage percentage time.
I don't know if that's exactly how you're supposed to call it, but that's what it is.
And pretty much what it'll do is time how long it takes to execute a block.
So we can see here there's CPU times zero nanoseconds.
The end is for nano billionth of a second is a nanosecond and then wall time.
So CPU time is how long it takes to execute on the CPU.
The time that it's doing operations for and then the wall time would be how long it actually takes like in real time.
How long do you have to wait? Do you have to wait until it's finished?
So the only thing that the CPU CPU time doesn't include is waiting.
So in an entire process, there's going to be some operations and there's going to be some waiting.
Wall time is going to have both of those and CPU time is just the execution.
So let's go ahead and continue with some of the basic PyTorch functions.
So I've written some stuff down here.
So we're going to go over Torch.stack, Torch.multinomial, Torch.trill, Triu.
I don't think that's how you pronounce it, but we'll get into that more.
Transposing, linear, concatenating, and the softmax function.
So let's first start off here with the Torch.multinomial.
So this is essentially a probability distribution based on the index that you give it.
So we have probabilities here.
We say 0.1 and 0.9.
These numbers have to add up to one to make 100%.
100% is one, one whole.
So I have 10% and 90%.
This is an index zero.
So there's a 10% chance that we're going to get a zero and a 90% chance that we're going to get a one.
So if I go ahead and run these up here.
Give this a second to do its thing.
So you can see that in the end we have our numSample set to 10.
So it's going to give us 10 of these.
1, 2, 3, 4, 5, 6, 7, 9, 10.
And all of them are ones.
If we run it again, we make it slightly different results.
So now we have some zeros in there.
But the zeros have very low probability of happening.
As a matter of fact, exactly a 10% probability of happening.
So we're going to use this later in predicting what word is going to come next.
Let's move on to Torch.cat or short for Torch.concatenate.
So this will essentially concatenate two tensors into one.
So I initialize this tensor here, torch.tensor, 1, 2, 3, 4.
It's one dimensional.
And we have another tensor here that just contains five.
So if we concatenate 1, 2, 3, 4 and 5, then we get 1, 2, 3, 4, 5.
We just combine them together and this is what will come out in the end.
So we run that 1, 2, 3, 4, 5.
Perfect.
So we're going to actually use this when we're generating.
When we're generating text given a context.
So it's going to start from zero.
We're going to use our probability distribution to pick the first one.
And then based on the first one, we're going to, you know, we're going to predict the next character.
And then once we have predicted that, we're going to concatenate the new one with the ones that we've already predicted.
So we have this, maybe like 100 characters over here.
And then the next character that we're predicting is over here.
We just concatenate these.
And by the end, we will have all of the integers that we've predicted.
So next up, we have torch.trill.
And what this stands for, what the trail stands for is a triangle lower.
So it's going to be in a sort of a triangle formation like this diagonal.
It's going to be going from top left to bottom right.
And so you're going to see a little bit more why later in this course.
But this is important because when you're actually trying to predict integers or a next tokens in the sequence, you have, you only know what's in the current history.
We're trying to predict the future.
So giving the answers in the future isn't what we want to do at all.
So maybe we've just predicted one and the rest of them we haven't predicted yet.
So we set all these to zero.
And then we predicted another one.
And these are still zero.
So these are talking to each other in history.
And as and as our predictions add up, we have more and more history to look back to and less future, right?
Basically, the premise of this is just making sure we can't communicate with the answer.
We can't predict while knowing what the answer is just like when you write an exam, you can't use the answer sheet.
They don't give you the answer sheet.
So you have to know based on your history of knowledge, which answers to predict.
And that's all that's going on here.
And we have, I mean, you could probably guess this triangle upper.
So we have all the upper ones.
These are, you know, lower on the lower side and then these are on the upper side.
So same concept there.
And then we have a masked fill.
So this one's going to be very important later because in order to actually get to this point, all we do is we just exponentiate every element in here.
So if you exponentiate zero, if you exponentiate zero, it'll become one.
If you exponentiate negative infinity, it'll become zero.
All that's going on here is we're doing approximately 2.71.
And this is a constant that we use in the dot exp function.
And then we're putting this to whatever power is in that current slot.
So we have a zero here.
So 2.71 to the zero is equal to one 2.71 to the one is equal to 2.71.
And then 2.71 to the negative infinity is, of course, zero.
So that's pretty much how we get from this to this.
And we're just, we're simply just masking these over.
So that's great.
And I sort of showcase what the exp does.
And we're just using this one right here.
We're using this output and we're just plugging it into here.
So it'll go from negative infinity to zero and then zero to one.
So that's how we get from here to here.
Now we have transposing.
So transposing is when we sort of flip or swap the dimensions of a tensor.
So in this case, I initialize a torch dot zeros tensor with dimensions two by three by four.
And we can use the transpose function to essentially flip any dimensions that we want.
So what we're doing is we're looking at the zero with as it sounds weird to not say first dimension,
but we're pretty much swapping the zero with position with the second.
So zero, one, two, we're swapping this one with this one.
So the end result, like you would probably guess the shape of this is going to be 432 instead of 234.
So you kind of just take a look at this and see, you know, which ones are being flipped.
And those are the dimensions and that's the output.
So hopefully that makes sense.
Next up, we have torch dot stack.
And this is where we're actually going to go.
We're going to we're going to do more of this.
We're actually going to use torch dot stack stack very shortly here when we're getting our batches.
So remember before when I was talking about batch size and how we take a bunch of these blocks together and we just stack them giant,
a giant length of integers or tokens.
And all we're doing is we're just stacking them together in blocks or to make a batch.
So that's pretty much what we're going to end up doing.
And that's what torch dot stack does.
We can take something that's one dimensional and then we can stack it to make it two dimensional.
We can take something that's two dimensional and stack it a bunch of times to make it three dimensional.
Or we can say three dimensional, for example, we have a bunch of cubes and we stack those on top of each other.
Now it's four dimensional.
So hopefully that makes sense.
All we're doing is we're just passing in each tensor that we're going to stack in order.
So this is our little output here and that's pretty much all it is.
The next function that's going to be really important for our model and we're going to be using this the entire time from start to finish.
It's really important.
It's called the nn dot linear function.
So it is a pretty much a function of the nn dot module.
And this is really important because you're going to see later on nn dot module is it contains anything that has learnable parameters.
So when we do a transformation to something, when you apply a weight and a bias, in this case, it'll be false.
But pretty much when we apply a weight or a bias under nn dot module, it will learn those and it'll become better and better.
And it'll basically train based on how accurate those are and how close certain parameters bring it to the desired output.
So pretty much anything with nn dot linear is going to be very important and it's going to be learnable.
So we can see over here.
This is the tors.nn little site here on the docs.
So we have containers, a bunch of different layers like activations, layers, pretty much just layers.
That's all it is.
And so these are these are important.
We're going to, we're basically going to learn from these.
And you're going to see why we're going to use something called keys and values, keys, values and queers later on.
You'll see why those are important.
But if that doesn't make sense yet, help me, let me illustrate value for you right now.
So I drew this out here.
So if we look back at our examples, we have a, we make, we initialize a term.
We make, we initialize a tensor.
It's 10, 10 and 10.
What we're going to do is we're going to do a linear transformation.
This linear stands for linear transformation.
So pretty much we're just going to apply a weight and a bias through each of these layers here.
So we have an input and we have an output x is our input, y is our output.
And this is of size three and this is of size three.
So pretty much we just need to make sure that these are lining up.
And for more context, the nn.sequential is sort of built off nn.linear.
So if we go ahead and search that up right now, this will make sense in a second here.
This is also some good prerequisite knowledge in general for machine learning.
So let's see nn.sequential doesn't show it here, but pretty much.
If you have, let's say, two, you have two input neurons and maybe you have one output neuron.
Okay, you have a bunch of hidden layers in between here.
Let's say we have one, two, three, four, and then one, two, three.
So pretty much you need to make sure that the inputs aligns with this hidden layer.
This hidden layer aligns with this one and this one aligns with this one.
So you're going to have a transformation of two to four.
So two, four, and then this one's going to be four to three, four to three,
and then you're going to have a final one.
This is two to four right here, four to three here, and then this final one.
It's going to be three to one.
So you pretty much just need to make sure that these are lining up.
So we can see that we have two, four, and then this four is carried on from this output here.
And pretty much this will just make sure that our shapes are consistent.
And of course, if they aren't consistent, if the shapes don't work out, the math simply won't work.
So we need to make sure that our shapes are consistent.
If that didn't make sense, I know I'm not like super great at explaining architecture of neural nets,
but if you're really interested, you could use chatGPT, of course.
And that's a really good learning resource, chatGPT, going on to get up discussions, maybe,
or just looking at documentation.
And if you're not good at reading documentation, then you could take maybe some little keywords from here,
like a sequential container.
Well, what is a sequential container?
You can ask chatGPT those types of questions and just sort of a virtual engineer the documentation
and figure things out step by step.
It's really hard to know what you're doing if you don't know all of the math and all of the functions that are going on.
You don't need to memorize them.
But while you're working with them, it's important to understand what they're really doing behind the scenes,
especially if you want to make an efficient and popular working neural net.
So that's that.
And pretty much what's going to happen here with these linear layers is we're just going to simply transform
from one to the other input to output, no hidden layers.
And we're just going to be able to learn best parameters for doing that.
You're going to see why that's useful later.
Now we have the softmax function.
So that sounds scary.
And the softmax function isn't actually what it sounds like at all.
Let me illustrate that for you right now.
So let's go ahead and change the color here.
So let's say we have a array, we have a one, two, three, let's move will make them floating point numbers 2.0, 3.0, etc.
Right, floating points, whatever.
So pretty much if we put if we put this into the softmax function, what's going to happen is we're going to exponentiate each of these.
And we're going to divide them by the sum of all of these exponentiated.
So pretty much what's going to happen, let's say we exponentiate one.
So what that's going to do is it's going to do, this is what it's going to look like in code, it's going to go one dot exp.
And I think I talked about this up here.
This is exponentiating when we have 2.71 to the power of whatever number we're exponentiating.
So if we have this one, we're going to exponentiate that and that's going to give us, it's going to give us 2.71.
And we have this two here, and that's going to give us whatever, whatever two is exponentiated 2.71, power of two.
Okay, so we're going to get 7.34.
I'm going to get 7.34.
Gorg my writing, it's terrible.
2.71 to 3 cubed.
So 19.9.
So pretty much what's going to happen is we can rearrange this in a new array.
7.34 and 19.9.
So if we add all these up together, we add all these up together, we're going to get 2.71 plus this.
Let's do this math real quick.
I'm just going to walk you through this to help you understand what the softmax function is doing.
7.34 plus 19.9.
That's going to give us a total of 29.95.
Great.
29.95.
So all we do is we just divide each of these elements by the total.
So 2.71 divided by this is going to give us maybe x.
And we do 7.34 divided by this is going to give us y.
And then we have 19.9 divided by this is going to give us z.
So pretty much you're going to exponentiate all of these.
You're going to add them together to create a total.
And then you're going to divide each of those exponentiate elements by the exponentiated total.
So after that, this x right here is just, we're just going to wrap these again.
And all this softmax function is doing is it's converting this 1, 2, 3 to x, y, z.
That's all it's doing.
And yeah, it's not really crazy.
There's a weird formula for it.
Softmax, softmax function.
So if you're in Wikipedia, you're going to crap yourself because there's a lot of terms in here
and a lot of math that's above the high school level.
But yeah, like this formula here, I believe this is what it is.
Or standard unit, softmax function, there you go.
So pretty much this is what it does.
And there's your easy explanation of what it does.
So you're going to see why this is useful later, but it's just important to know what's going on
so that you won't lag behind later in the course when this background knowledge becomes important.
So if we go over a little example of that, of the softmax function in code, it looks like this right here.
So we import torsha and n dot functional as f, f short for functional.
And we pretty much just do f dot softmax and then plug in a tensor.
And what we want the dimension to be the output dimension.
So if we plug this into here and we print it out, we go and print it out.
It's going to take a second.
Torch is not defined. So let's run this from the top here.
Boom.
And let's try that again. Boom. There we go.
So if you took all those values, let's actually do this again from scratch.
So we do 2.71, 2.71 divided by 29.95.
We get 0.09, 0.09. Good.
And then if we do 7.34 divided by 29.95, we get 0.245.
So 0.245. Well, it's kind of close.
Really close actually. And then 66.52. So if we go, what was that last one there?
19.9. So we do 19.9 divided by 29.95.
66.4. So 66.5. It's pretty close.
Again, we're rounding, so it's not perfectly accurate.
As you can see, they're very close and for only having two decimal places, we did pretty good.
So that's just sort of illustrating what the softmax function does and what it looks like in code.
We have this sort of shape here. Zero dimensions means we just take, you know, it's just kind of a straight line.
It's just like that.
So now we're going to go over embeddings.
And I'm not actually, I don't have any code for this yet.
We're going to figure this out step by step with chat GPT, because I want to show you guys sort of the skills
and what it takes to reverse engineer an idea or function or just understand how something works in general in machine learning.
So if we pop in a chat GPT here, we say, what is an end dot embedding?
And then dots. Let me type in a non-bedding class in the PyTorch library.
Okay, actual language processing max maps each discrete input to a dense vector representation.
Okay, how does this work? Let's see.
So we have some vocab. So that's probably our vocabulary size.
I think we talked about that earlier, vocabulary size, how many characters, how many unique characters are actually in our data set.
That's the vocabulary size. And then some embedding dimension here, which is a hyper parameter.
So let's see. This doesn't quite make sense to me yet.
So maybe I want to learn what does this actually look like?
Can you explain this to a, maybe an eighth grader and provide a visualization?
Certainly. Okay.
Little secret codes that represent the meaning of the words. Okay, that helps.
So if we have cat, okay, so cat, cat's a word.
So maybe we want to know what it would look like on a character level.
What about on a character level instead of word level?
So it's probably going to look very similar.
We have this little vector here storing some information about whatever this is.
So a, it means this here. Okay, so as your point to, and this is really useful.
So we've pretty much just learned what embedding vectors does.
And if you haven't kept up with this, pretty much what they'll do is they'll store some vector of information about this character.
And we don't even know what each of these elements mean.
We don't know what they mean.
This could be maybe positivity or should be the start of a word or it could be any piece of information,
maybe something we can't even comprehend yet.
But the point is, if we actually give them vectors and we feed these into a network and learn because as we saw before,
nn.embedding right here is a part of the nn.module.
So these are learnable parameters, which is great.
So it's actually going to learn the importance of each letter,
and it's going to be able to produce some amazing results.
So in short, the embedding vectors are essentially a vector or a numerical representation of the sentiment of a letter.
In our case, it's character level, not subword, not word, it's character level.
So it's going to represent some meaning about those.
So that's what embedding vectors are.
Let's go figure out how they work in code.
We have this little character level embedding vector and it contains a list.
There's five elements in here, one, two, three, four, five, and it's by the vocab size.
So we have all of our vocabulary by the length of each embedding vector.
So this actually makes sense because our vocab size by the embedding dimension,
which is how much information is actually being stored in each of these characters.
So this now is very easy to understand.
I'm just going to copy this code from here and I'm going to paste it down here.
And let's just get rid of the torch because we already initialized that above.
So if we just run this, actually, let's turn that down to maybe a thousand characters.
Let's try that out.
And it's not defined.
We did not initialize it.
So let's go back down here and look at that.
So this dot shape is going to essentially show the shape of it this much by this much.
So it's four by a hundred.
And yeah, so we can we can work with these and we can store stuff about characters in them.
And you're going to see this in the next lecture, how we actually use embedding vectors.
So no need to worry if a lot of this doesn't make sense yet.
That's fine.
You're going to learn a little bit more about how we use these over the course.
You're going to get more confident with using them even in your own projects.
So don't don't stress about it too much right now.
Embeddings are pretty tricky at first to learn.
So don't worry about that too much.
But there are a few more things I want to go over just to get us prepared for some of the linear algebra and matrix multiplication in particular that we're going to be doing in neural networks.
So if we have, I remember before we pulled out this little sketch of this is actually called a multilayer perceptron, but people like to call it a neural network because it's easier to say.
But that's the architecture of this multilayer perceptron.
But pretty much what's happening is we have a little input here and we have a white matrix.
So white matrix is looks like this.
It's like this and we have some, we have some values in between X1, Y1 and maybe Z1.
So a bunch of weights and maybe biases to that we add to it.
So the tricky part is how do we actually multiply our input by this white matrix?
We're just doing one matrix times another.
Well, that's called matrix multiplication.
And I'm going to show you how to do that right now.
So first off, we have to learn something called dot products.
So dot products are actually pretty easy and you might have actually done them before.
So let's say we go ahead and take, we go ahead and take this right here we go.
One, two, three.
That's going to be what A is.
And then we have four, five, six.
So if we want to find the dot product between these two, all we have to do is simply take the index of both of these,
the first ones and the second ones and third ones, multiply them together and then add.
So we're going to go ahead and do one, multiply four, one times four, and then add it to two times five,
and then add it to three times six.
So one times four is four, two times five is ten, three times six is eighteen.
So we're going to go ahead and add these up, we get fourteen plus eighteen, I believe is thirty-two.
So the dot product of this is going to be thirty-two.
And that's pretty much how simple dot products are.
It's just taking each index of both of these arrays, multiplying them together and then adding all of these products up.
That's a dot product.
So we actually need dot products for matrix multiplication.
So let's go ahead and jump into that right now.
So I'm just going to create two matrices that are going to be pretty easy to work with.
So let's say we have A and have one matrix over here.
It's going to be one, two, three, four, five and six.
This is going to be equal to A and then B is going to be another matrix.
So we're going to have seven, eight, nine, ten, eleven, twelve.
Ignore my terrible writing.
Pretty much what we do is to multiply these together.
First we need to make sure that they can multiply together.
So we need to take a look at the amount of rows and columns at this half.
So this one right here is three rows, one, two, three.
Three rows and two columns.
So this is going to be a three by two matrix.
And this one has two rows and three columns.
So it's a two by three matrix.
So all we have to make sure that if we're multiplying A dot product with B,
and this is the PyTorch syntax for multiplying matrices,
if we're multiplying A by B, then we have to make sure the following is true.
So if we use three by two and then dot product with two times three,
we have to make sure that these two inner values are the same.
So two is equal to two, so we cross these out,
and then the ones that we have left over are three by three.
So the resulting matrix would be A three by three.
Or if you had like a three by four times A five by five by one,
that doesn't work because these values aren't the same.
So these two matrices couldn't multiply.
And sometimes you actually have to flip these to make them work.
So maybe we change this value here to A three.
We change this value to a three.
In this order, they do not multiply.
But if we switch them around, we have a three by five with A five by three,
sorry, five by three with A three by four.
So these two numbers are the same.
That works.
The resulting matrix is a five by four.
So that's how you make sure that two matrices are compatible.
So now to actually multiply these together,
what we're going to do, I'm going to make a new line here.
So we're going to rewrite these.
Now we don't have to rewrite them.
Let's just cross that out here.
So pretty much what we have to do is we have to take these two and dot product with these two.
And then once we're done that, we do the same with these and these, these and these.
So we start with the first, the first row in the A matrix.
And we iterate through all of the columns in the B matrix.
And then after we're done that, we just go to the next row in the A matrix and then et cetera, right?
So let's go ahead and do this right now.
That probably sounds confusing to start, but let me just illustrate this, how this sort of works right here.
So we have our one times, our one times seven plus two times 10.
So one times seven plus two times 10.
And this is equal to 27.
So that's the first dot product of one and two and seven and 10.
So what this is actually going to look like in our new matrix, I'm going to write this out here.
So this is our new matrix here.
This 27 is going to go right here.
Let's continue.
So next up, we're going to do one and two and then eight and 11.
So we're going to go one, one times eight plus two, or sorry, two and 11.
So one times eight is eight and then two times 11 is 22.
So our result here is 30 and 30 is just going to go right here.
So 27, 30, and you can see how this is going to work, right?
So in our first row of A, we're going to get the first row of this resulting matrix.
So let's go ahead and do the rest here.
So we have one and two and then nine and 12.
One times nine, two times 12.
One times nine is nine, two times 12 is 24.
So if we do, that's like 33, I believe.
So 33 and we can go ahead and write that here.
So now let's move on to the next.
We have three and four, three, three and four dot product with seven and 10.
So three will multiply seven.
And then we're going to go ahead and add that to four times 10.
Three times seven, three times seven is 21, and then four times 10 is 40.
So we're going to get 47.
So I'll put there so we can go in and write 47 right there.
And our next one is going to be three and four dot product with eight and 11.
So eight plus four times 11.
Perfect.
So we get three times eight is 24 and then plus 44.
So 24 plus 44, that's 68.
So we get 68 and we can go in and write that here.
So next up, we have three and four and nine and 12.
So three times nine is 27.
And then four times 12.
So let's just, let's just do that.
I'm not doing that in my head.
27 plus was four times 12.
So that's 48.
27 plus 48 gives us 75.
Let's go ahead and write our 75 here.
Then we can go ahead and slide down to this row since we're done, since we're done that.
And then we go five, five and six dot product was seven and 10.
So our result from this five times seven is 35 and then six times 10 is 60.
So we're going to get 95.
We can go in and write our 95 here.
And then five and six dot product with eight and 11.
So five times eight is 40 and then six times 11 is 66.
So we get 104.
And then the last one, so five and six dot product with nine and 12.
So five, five times nine is 45.
And then six times 12 is what six times 12, 72, I think.
So six times 12, 72.
Yeah.
So 45 plus 72, 117.
And that is how you do a three by two matrix and a two by three matrix multiplying them together.
So the result would be C equals that.
So as you can see, it takes a lot of steps that took actually quite a bit of time compared to a lot of the other stuff I've covered in this video so far.
So you can see how it's really important to get computers to do this for us and especially to scale this on a GPU.
So I'm going to keep emphasizing that point more and more is how the GPU is very important for scaling your training.
But pretty much that's how you do dot products and matrix multiplication.
So I actually realized I messed up a little bit on the math there.
So this 104, that's actually 106.
So I messed up there if you caught that.
Good job.
But pretty much this is what this looks like in three lines of code.
So all of this up here that we just covered all of this is in three lines.
So we initialize an A tensor and a B tensor.
Each one of these is a row.
Each one of these is a row and it'll pretty much multiply these together.
So this at symbol, this is a shorthand how you multiply two matrices in pytorch together.
Another way to do this is to use the torch dot matrix multiply function or math mall for short.
And then you can do A and B.
So these will print literally the same thing.
Look at that.
So I'm not too sure on the differences between them.
I use A at B for short.
But if you really want to know just, you know, take a look at the documentation or has to have CPT one of the two and should be able to get an answer from that.
But I'm going to move on to something that we want to watch out for, especially when we're doing our matrix multiplication in our networks.
So there's our network here if I go up.
Imagine we have, we have some matrix, some matrix A, and every element in this matrix is a floating point number.
So if it's like a one, it would be like one dot zero or something or just like a one dot.
That's what it would look like as a floating point number.
But if it were an integer, say B is full of ones with integers, it would just be a one.
There wouldn't be any decimal zero zero center, right?
It would just be one.
So in PyTorch, you cannot actually multiply integers and floating point numbers because they're not the same data type.
So I showcase this right here.
We have an int 64.
So type of it is an integer and a float 32, 64 and 32 don't mean anything.
All we have to know is an integer and floating point number.
So I've initialized a torch.randint, I covered above and set above here.
And maybe not.
Anyways, this pretty much does torch.randint is going the first parameter here is anything.
It's pretty much your range.
So I could do like zero to five, or I could just do like one.
So it'll do zero up to one, and then your shape of the matrix that it generates.
So I said it's a random int.
So that means it's going to generate a tensor with the data type integer 64.
So we have a three by two, and then I initialize another random key detail here.
We don't have the int suffix.
So this just generates floating point numbers.
And if we actually return the types of each of these.
So five print int 64 dot d type, and then float 32 dot d type.
Save that.
I'm going to comment this out for now.
We get a in 64 and float 32.
So if we just try to multiply these together, try to multiply these together.
Expected scalar type long above found float.
So long is pretty much when you have a sequence of integers.
And float is, of course, you have the decimal place.
So you can actually multiply this together.
So pretty much what you can do is cast the float method on this.
If you just do dot float, and then parentheses, and then run this, it'll actually work.
So you can cast integers to floats.
And then I think there's a way you can cast floats to integers, but it has some rounding in there.
So probably not the best for input and weights, matrix multiplication.
But yeah, pretty much if you're doing any way to matrix multiplication, it's going to be using floating point numbers because the weights will get extremely precise.
So you want to make sure that they have sort of room to float around.
So that's pretty much how you avoid that error.
Let's move on.
So congratulations.
You probably made it further than quite a few people already.
So congratulations on that.
That was one of the most comprehensive parts of this entire course.
Understanding the math is going on behind the scenes.
For some people, it's very hard to grasp if you're not very fluent with math.
But yeah, let's continue the biogram language model and let's pump out some code here.
So to recap, we're using CUDA to accelerate the training process.
We have two hyperparameters, block size for the length of integers, and batch for how many of those are running in parallel.
Two hyperparameters.
We open our text.
We make a vocabulary out of it.
We initialize our encoder and decoder.
We get our data encoding all this text, and then we get our train and bow splits.
And then this next function here, get batch.
So before I jump into this, go ahead and run this here.
So this is pretty much just taking the first little, I don't know, we have eight characters.
So it's taking the first eight characters and then index one all the way to index nine.
And we can pretty much use this to show what the current input is and then what the target would be.
So if we have 80, target is one, 80 and one, target is one, 80 and one, target is 28, et cetera, right?
So this is the premise of the biogram language model.
Given this character, we're going to predict the next.
It doesn't know anything else in the entire history.
It just knows what's before it or just knows what the current character is.
And based on that, we're going to predict the next one.
So we have this get batch function here, and this part right here is the most important piece of code.
This is going to work a little bit more later with our train and bow splits, making sure that, you know,
I'll try to explain this in a different way with our training bow splits.
So imagine you take a course, as you take a math course, okay?
And 90% of all your work is done just learning how the course works, learning all about the math.
So that's like 90% of data you get from it.
And then maybe another 10%.
Another 10% at the end is that final exam, which might have some questions you've never seen before.
So the point is in that first 90%, you're tested on based on what you know.
And then this other 10% is what you don't know.
And this pretty much means you can't memorize everything and then just start generating based on your memory.
You generate something that's alike or something that's close based on what you already know and the patterns you captured
in that 90% of the course.
So you can write your final exam successfully.
So that's pretty much what's going on here.
The training is the course, learning everything about it and then validation is validating the final exam.
So pretty much what we're doing here is initialize IX and that'll take a random manager between zero
and then length of the length of the entire text minus block size.
So if you get the index that's at length of data minus block size, you'll still get the characters up to the length of data.
So that's kind of how that works.
And if we print this out here, it'll just give us this right here.
So we get some random integers.
These are some random indices in the entire text that we can start generating from.
So print this out and then torch.stack.
We covered this before.
Pretty much what this does, it's going to stack them in batches.
This is the entire point of batches.
So that's what we do there.
We get X and then Y is just the same thing, but offset by one like this.
So that's what happens there.
And let's get into actually, I'm going to add something here.
This is going to be very important.
We're going to go X and Y is equal to model dot.
We're going to go X dot to device.
So notice how, no, we didn't do it up here.
Okay, we'll cover this later, but pretty much you're going to see what this does in a second here.
We return these and you can see that the device changed.
So now we're actually on CUDA.
And this is really good because these two pieces of data here, the inputs and the targets are no longer on the CPU.
They're no longer going to be processed sequentially, but rather in our batches in parallel.
So that's pretty much how you push any piece of data or parameters to the GPU is just dot to and then the device which you initialized here.
So now we can go ahead and actually initialize our neural net.
So what I'm going to do is I'm going to go back up here and we're going to import some more stuff.
So I'm going to import dot nn as nn and you're going to see why a lot of this is important in a second.
I'm going to explain this here.
I just want to get some code out first.
And down here we can initialize this.
So it's a class.
We're going to make it a by-gram language model subclass of nn.module.
And the reason why we do nn.module here is because it's going to take an nn.module.
I don't know how to explain this like amazingly, but pretty much when we use the nn.module functions in PyTorch and it's inside of a nn.module subclass, they're all learnable parameters.
So I'm going to go ahead and look at the documentation here so you can sort of understand this better.
We go to nn.
So pretty much all of these convolutional layers, recurrent layers, transformer, linear, like we looked at linear layers before.
So we have nn.linear.
So if we use nn.linear inside of this, that means that the nn.linear parameters are learnable.
So that weight matrix will be changed through gradient descent.
And actually, I think I should probably cover gradient descent right now.
So in case some of you don't know what it is, it's going to be really hard to understand exactly how we make the network better.
So I'm going to go ahead and set up a little graph for that right now.
So I'm going to be using a little tool called Desmos.
Desmos is actually great.
It acts as a graphing calculator.
So you can plug in formulas and move things around.
You sort of visualize how math functions work.
So I've written some functions out here that will basically calculate the derivative of a sine wave.
So if I move A around, you'll see that changes.
So before I get into what's really going on here, I need to first tell you what the loss actually is.
If you're not familiar with the loss, let's say we have 80 characters in our vocabulary.
And we have just started our model, no training at all, completely random weights.
And theoretically, there's going to be a one in 80 chance that we actually predict next token successfully.
So how we can measure the loss of this is by taking the negative log likelihood.
So the likelihood is one out of 80.
We take the log of that and then negative.
So if we plug this in here, we'll get 4.38.
So that's a terrible loss.
Obviously, that's one out of 80.
So it's like, you know, not even 2% chance.
So that's not great.
So pretty much the point is to minimize the loss, increase the prediction accuracy or minimize the loss.
And that's how we train our network.
So how does this actually work?
How does this actually work out in code, you ask?
So pretty much, let's say we have a loss here, okay?
Start off with a loss of 2, just arbitrary loss, whatever.
And what we're trying to do is decrease it.
So over time, it's going to become smaller and smaller if we move in this direction.
So how do we know if we're moving in the right direction?
Well, we take the derivative of what the current point is at right now,
and then we try moving it in a different direction.
So if we move it this way, sure, it'll go down.
That's great.
We can hit the local bottom over there, or we can move to this side.
And then we can see that the slope is increasing in a negative direction.
So we're going to keep adjusting the parameters in favor of this direction.
So that's pretty much what gradient descent is.
We're descending with the gradient.
So pretty self-explanatory.
That's what the loss function does.
And gradient descent is an optimizer.
So it's an optimizer for the network.
Optimizes our parameters, our weight, matrices, etc.
So these are some common optimizers that are used.
And this is just by going to torch.optim, short for optimizer.
And these are just a list of a bunch of optimizers that PyTorch provides.
So what we're going to be using is something called AdamW.
And what AdamW is, is it pretty much...
I'm just going to read off my little script here,
because I can't memorize every optimizer that exists.
So Adam, without Adam, just Adam, not AdamW,
Adam is a popular optimization algorithm that combines ideas of momentum.
And it uses a moving average of both the gradient and its squared value
to adapt the learning rate of each parameter.
And the learning rate is something that we should also go over.
So let's say I figure out I need to move in this direction.
I move, I take a step like that.
Okay, that's a very big step that I say,
okay, we need to keep moving in that direction.
So what happens is I go like this, and then I end up there.
And it's like, whoa, we're going up now, what happened?
So that's because you have a very high learning rate.
If you have a lower learning rate, what will happen is you'll start here.
It'll take little one-pixel steps or very, very small steps.
Okay, that's good. That's better. It's even better.
Keep going in this direction. This is great.
And you keep going down. You're like, okay, this is good.
We're descending. And it's starting to flatten out.
So we know that we're hitting a local bottom here.
And then we stop because it starts ascending again.
So that means this is our best set of parameters because of what that loss is
or what the derivative is of that particular point.
So pretty much this is what the learning rate is.
So you want to have a small learning rate so that you don't take too large steps
so that the parameters don't change dramatically and end up messing you up.
So you want to make them small enough so that you can still have efficient training.
You don't want to be moving in a millionth of one or something.
That would be ridiculous. You'd have to do so many iterations to even get this far.
So maybe you'd make it decently high but not too high that it'll go like that, right?
So that's what the learning rate is, just how fast it learns pretty much.
And yeah, so AtomW is a modification of the Atom Optimizer.
And it adds weight to K. So pretty much there's just some features that you add on to gradient descent
and then AtomW is the same thing except that has weight to K.
And what this pretty much means is it generalizes the parameters more.
So instead of having very high level of performance or very low level,
it takes a little generalized in between.
So the weight significance will actually shrink as it flans out.
So this will pretty much make sure that certain parameters in your network,
certain parameters in your weight matrices aren't affecting the output of this model drastically.
That could be in a positive or negative direction.
You can have insanely high performance from some lucky parameters in your weight matrices.
So pretty much the point is to minimize those, to decay those values.
That's what weight to K is, to prevent it from having that insane or super low performance.
That's what weight to K is.
So that's a little background on gradient descent and optimizers.
Let's go ahead and finish typing this out.
So next up, we actually, we need to initialize some things.
So we have our init self, of course, since it's a class, vocab size.
I want to make sure that's correct, vocabulary size.
I might actually shrink this just a vocab size because it sounds way easier to type out.
And vocab size, good.
So we're going to pump out some R code here.
And this is just assuming that you have some sort of a background in Python.
If not, it's all good.
Just understanding the premise of what's going on here.
So we're going to make something called an embedding table.
And I'm going to explain this to you in a second here, why the embedding table is really important.
Notice that we use the nn.
We use the nn module in this.
So that means this is going to be a learnable parameter, the init.embedding.
So we're going to make this vocab size by vocab size.
So let's say you have all eight characters here and you have all eight characters here.
I'm going to actually show you what this looks like in a second here and why this is really important.
So first off, we're going to finish typing out this background language model.
So we're going to define our forward pass here.
So the reason why we type this forward pass out, instead of just using what it offers by default,
is to let's say we have a specific use case for a model and we're not just using some tensors and we're not doing a simple task.
This is a really good practice because we want to actually know what's going on behind the scenes in our model.
We want to know exactly what's going on.
We want to know what transformations we're doing, how we're storing it, and just a lot of the behind the scenes information that's going to help us debug.
So I actually asked this, the chatGPT says, why is it important to write a forward pass function in PyTorch from scratch?
Well, like I said, understanding the process, what are all the transformations that are actually going on,
all the architecture that's going on in our forward pass, getting an input, running it through a network, and getting an output?
Our flexibility, debugging, like I said, debugging is going to bite you in the ass if you don't sort of follow these best practices
If you're using weird data and the default isn't really used to dealing with it, you're going to get bugs from that.
So you want to make sure that when you're actually going through your network, you're handling that data correctly and each transformation, it actually lines up.
So you can also print out at each step what's going on so you can see like, oh, this is not quite working out here.
Maybe we need to, you know, use a different function. Maybe this isn't the best one for the task, right?
So help you out with that, especially. And of course, customization, if you're building custom models, custom layers, right?
And optimization, of course. So that's pretty much why we write out the forward pass from scratch.
It's also just best practice. So it's never really a good idea to not write this.
But let's continue. So self, and it will do index and targets.
So we're going to jump into a new term here called logits. But before we do that, and I'm kind of all over the place here.
Before we do logits, I'm going to explain to you this embedding table here.
Paste that in.
Return logits. You're going to see why we return logits in a second here.
So this an end on embedding here is pretty much just a lookup table.
So what we're going to have, I'm actually going to pull up my notebook here.
So we have a giant sort of grid of what the predictions are going to look like.
It's going to look, can I drag it in here? No.
So go ahead and download this full screen. Boom.
This is my notion here, but pretty much this is what it looks like.
And I took this picture from Andrei Karpathy's lecture.
But what this is, is it has start tokens and end tokens.
So start is at the start of the block, and end tokens are at the end of the block.
And it's pretty much just predicting, it's showing sort of a probability distribution of what character comes next given one character.
So if we have, say, I don't know, an A, 6,640 times out of this entire distribution here.
So if we just add up all these, if we normalize them, and we get a little probability of this happening,
I don't know, if we add up all these together, I don't know what that is.
It's some crazy number, maybe 20,000 or something, something crazy.
Pretty much that percentage is the percentage of the end token coming after the character A.
And then same thing here, like if we do R, that's an RL or an RI, I don't know, I'm blind.
That's an RI. But pretty much we normalize these, which means, normalizing means you take how significant is that.
To that entire row. So this one's pretty significant in proportion to the others.
So this one's going to be a fairly high probability of coming next.
A lot of the times you're going to have an I coming after an R.
And that's pretty much what that is. That's the embedding table.
So that's why we make it vocab size by vocab size.
So that's a little background on what we're doing here.
So let's continue with the term logits.
So what exactly are the logits? You're probably asking that.
So let's actually go back to a little notebook I had over here.
So remember our softmax function, right? Our softmax right here.
So we exponentiated each of these values and then we normalized them.
Normalized. We took its contribution to the sum of everything. That's what normalizing is.
So you can think of logits as just a bunch of floating point numbers that are normalized, right?
So you have a total, I'll write this out.
So let's say we have, that's a terrible line. Let's draw a new one.
So let's say we have 2, 4, and 6. And we want to normalize these.
So take 2 out of the totals. What's the total? We have 6 plus 4 is 10 plus 2 is 12.
So 2 divided by 12. We take the percentage of that.
2 out of 12 is 0.16 something, okay?
So 0.16, we'll just do 1.167.
And then 4 out of 12 would be double that.
So 4 out of 12 would be 33, 33%.
And then 6 out of 12, that's 50. So 0.5.
So that's what these looks like normalized. And this is pretty much what the logits are, except it's more of a probability distribution.
So let's say we have, you know, a bunch of, a bunch of bigrams here, like, I don't know, a followed by b and then a followed by c and then a followed by d.
We know that from this distribution, a followed by d is most likely to come next.
So this is what the logits are. They're pretty much a probability distribution of what we want to predict.
So given that, let's hop back into here. We're going to mess around with these a little bit.
So we have this embedding table, and I already showed you what that looked like.
It looked like this right here. This is our embedding table.
So let's use something called, we're going to use a function called dot view.
So this is going to help us sort of reshape what our logits look like.
And I'm going to go over an example of what this looks like in a second here.
I'm just going to pump out some code. So we have our batch by our time.
So the time is, you can think of time as that sequence of integers.
That's the time dimension, right? You start from here.
Maybe through the generating process, we don't know what's here next.
We don't know what's on the, we don't know what the next token is.
So that's why we say it's time because there's some we don't know yet and there's some that we already do know.
That's what we call the time dimension.
And then channels would just be, how many different channels are, what's the vocabulary size?
Channels is the vocabulary size.
So we can make this the logits dot shape.
This is what logits going to return here is B by T by C.
That's the shape of it.
And then our targets do, actually, no, we won't do that yet.
We'll do do logits equals logits dot view.
And then we'll, this is very important, B by T.
So because we're particularly paying attention to the channels, the vocabulary, the batch and time,
they, I mean, they're not as important here.
So we can sort of blend these together.
And as long as the logits and the targets have the same batch and time, we should be all right.
So we're going to do B, B times T by C.
And then we can go to initialize our targets.
It's going to be targets dot view.
And it's going to be just a B by T.
And then we can make our loss, remember the loss function, right?
We do the functional of cross entropy, just a way of measuring the loss.
And we basically take where there's two parameters here.
So we have the logits and the targets.
So I'm going to go over exactly what's going on here in a second.
But first, you might be asking, what does this view mean?
What exactly does this do?
So I'm going to show you that right now.
There's some code here that initializes a random tensor of shape 2 by 3 by 5.
And so what I do is I pretty much unpack those, I unpack those dimensions by using a dot shape.
So shape takes the, you know, it takes the 2 by 3 by 5.
We get x equals 2, y equals 3, and z equals 5.
So then we can do dot view, and that'll pretty much make that tensor again with those dimensions.
So then we can just print that out afterwards.
We go, we could print out, I don't know, print x, y, z.
We have 2, 3, 5.
Print, print a dot shape.
And actually, I'll print out a dot shape right here first so you can see that this actually does line up.
A dot shape.
And then down here as well.
Same exact thing.
This also view does, basically allows us to unpack with the dot shape,
and then we can use view to put them back together into a tensor.
So you might be asking, why in this notebook did we, did we have to reshape these?
Why do we do that?
Well, the answer sort of falls into what the shape needs to be here with cross entropy.
What does it expect?
What does PyTorch expect the actual shape to be?
So I looked at the documentation here, and it pretty much says that we want either one dimension,
which is channels, or 2, which is n, which I believe n is also the batch.
So you have n, n different blocks or batches.
And then you have some other dimensions here.
So pretty much what it's expecting is a b by c by t instead of a b by t by c,
which is precisely what we get out of here.
It's the logits dot shape is b by t by c.
We want it in a b by c by t.
So pretty much what we're doing is we're just putting this into,
we're just making this one parameter by multiplying those.
That's what's going on here.
And then that means the second one is going to be c.
So you get like a b times t equals n, and then c, just the way that it expects it, right?
Just like that.
So that's pretty much what we're doing there.
And a lot of the times you might get errors from passing it into a functional function in PyTorch.
So it's important to pay attention to how PyTorch expects the shapes to be,
because you're going to get errors from that.
And I mean, it's not very hard to reshape them.
You just use the dot view and dot shape and you unpack them, reshape them together.
It's overall pretty simple for a beginner to intermediate level projects.
So it shouldn't really be a trouble there, but just watch out for that,
because it will come back and get you if you're not aware at some point.
So I've added a new function here called generate,
and this is pretty much going to generate tokens for us.
So we pass an index, which is the current index or the context,
and then we have max new tokens, and this is passed in through here.
So we have our context, we make it a single zero, just the next line character.
And then we generate based on that, and then our max new tokens, second parameter,
we just make it 500 second parameter.
So cool.
What do we do inside of here?
We have a little loop that pretty much it generates based on the range of the max new tokens.
So we're going to generate max new tokens, tokens, if that makes sense.
Pretty much what we do is we call forward pass based on the current state of the model parameters.
And I want to be explicit here and say self dot forward, rather than just self index,
it will call self dot forward when we do this.
But let's just be explicit and say self dot forward here.
So we get the logic and the loss from this.
We focus on the last time step.
That's the only one we care about diagram language model.
We only care about the single previous character, only one doesn't have context before.
And then we apply the softmax to get probability distribution.
And we already went over the softmax function before.
The reason why we use negative one here is because we're focusing on the last dimension.
And in case you aren't familiar with negative indexing, which is what this is here and same with here,
is imagine you have a little number line.
It starts at index zero, one, two, three, four, five, et cetera.
So if you go before zero, it's just going to loop to the very end of that array.
So when we call negative one, it's going to do the last element, negative two,
second last element, negative three, third last element, et cetera.
So that's pretty much all this is here.
And you can do this for anything in Python.
Negative indexing is quite common.
So that's what we do here.
We apply softmax to the last dimension.
And then we sample from the distribution.
So we already went over torch dot monomial, we get one sample.
And this is pretty much the next index or the next encoded character that we then use torch dot cat short for concatenate.
It concatenates the previous context or the previous tokens with the newly generated one.
And then we just combine them together.
So they're one thing.
And we do this on a B by T plus one.
And if that doesn't make sense, let me help you out here.
So we have this time dimension, let's say we have, you know, maybe just one element here.
So we have something in the zero position.
And then whenever we generate a token, we're going to take the information from the zero position.
And then we're going to add one to it.
So it becomes a B by T.
Since there was only one element, the length of that was one, it is now two.
Then we have this two, we make it three.
And then we have this three, we make it four.
So that's pretty much what this doing is just keep, just keep concatenating more tokens onto it.
And then we, you know, after this loop, we just return the index.
So this is all the generated tokens for max new tokens.
And that's pretty much what that does.
Model up to device here, this is just going to push our parameters to the GPU for more efficient training.
I'm not sure if this makes a huge difference right now because we're only doing background language modeling.
But yeah, it's handy to have this here.
And then, I mean, this is, this is pretty self explanatory here.
We generate based on a context.
This is the context, which is a single zero or a next line character.
We pass in our max new tokens.
And then we pretty much just decode this.
So that's how that works.
Let's move on to the optimizer and the training loop, the actual training process.
So I actually skipped something and probably left you a little bit confused.
But you might be asking, how the heck did we actually access the second out of out of three dimensions from this logits here?
Because the logits only returns two dimensions, right?
You have a B by T, or you have a B times T by C.
So how exactly does this work?
Well, when we call this forward pass, all we're passing in is the index here.
So that means targets defaults to none.
So because targets is none, the loss is none, and this code does not execute.
And it just uses this logits here, which is three dimensional.
So that's how that works.
And honestly, if you, if you're feeding in your inputs and your targets to the model, then you're obviously going to have your targets in there.
And that will make sure targets is not none.
So then you'll actually be executing this code and you'll have a two dimensional logits rather than a three dimensional logits.
So that's just a little clarification there, if that was confusing to anybody.
Another quick thing I want to cover before we jump into this training loop is this little tors dot long data type.
So tors dot long is the equivalent of int 64 or integer 64, which occupies 64 bits, or eight bytes.
So you can have different data types, you can have a float 16, you can have a float 32 float 64, I believe you can have an int 64 in 32 difference
between float and int is float has decimals, it's a floating point number.
And then integers just, just a single integer doesn't, it's not really anything more than that.
It can just be bigger based on the amount of bits that occupies.
So that's just a overview on tors dot long.
It's the exact same thing as in 64.
So that's that.
Now we have this, we have this training loop here.
So we define our optimizer.
And I already meant over optimizers previously, Adam W, which is Adam weight decay.
So we have weight decay in here.
And then all of our model parameters, and then our learning rates.
So I actually wrote to learning rate up here.
So I would add this and then just rerun this part of the code here if you're typing along.
So I have this learning rates, as well as max itters, which is how many iterations we're going to have in this training loop.
And the learning rate is special, because sometimes you're learning what will be too high.
And some said, sometimes it'll be too low.
So a lot of the times you'll have to experiment with your learning rate and see which one provides the best both performance and quality over time.
So with some learning rates, you'll get really quick advancements and then it'll like overshoot that little dip.
So you want to make sure that doesn't happen, but you also want to make sure the training process goes quickly.
You don't want to be waiting like, you know, an entire month for a background language model to train by having, you know, by having a number like that.
So that's a little overview on like, basically, we're just putting this this learning rate in here, that's where it belongs.
So now we have this training loop here, which is going to iterate over the max iterations.
We just give each iteration the term iter.
And I don't think we use this yet, but we will later for just reporting on the loss over time.
But what we do is we get, we get a batch with the train split specifically, we're just, again, we're just we're just training.
This is the training loop, we don't care about validation.
So we're going to call train on this, we're going to get some x inputs and some y targets.
So we go in and do a model dot forward here, we got our logits and our loss.
And then we're going to do our optimizer dot zero grad and I'll explain this in the second here.
It's a little bit confusing.
But again, we ever we have our loss dot backward and this in cases doesn't sound familiar in case you are not familiar with training loops.
I know I can go by this a little bit quickly.
But this is the standard training loop architecture for basic models.
And this is what it'll usually look like.
So you'll, you know, you'll get your data, get your inputs or outputs, whatever.
You'll do a forward pass.
You'll define some thing about the optimizer here.
In our case, it's your grad.
And then you'll have a loss dot backward, which is backward pass.
And the optimizer dot step, which lets gradient descent work its magic.
So back to optimizer dot zero grad.
So by default, PyTorch will accumulate the gradients over time via adding them.
And what we do by by putting a zero grad is we make sure that they do not add over time.
So the previous gradients do not affect the current one.
And the reason we don't want this is because previous gradients are from previous data.
And the data is, you know, kind of weird sometimes, sometimes it's biased.
And we don't want that determining, you know, how much like what our error is, right?
So we only want to decide, we only want to optimize based on the current gradient of our current data.
And this little parameter in here, we go set to none.
This pretty much means we're going to set, we're going to set the gradients instead of zero,
instead of zero gradient, we're going to set it to none.
And the reason why we set it to none is because none occupies a lot less space.
It just, yeah, just occupies a lot less space when you have a zero.
That's, that's probably an int 64 or something that's going to take up space.
And because, you know, we might have a lot of these accumulating that takes up space over time.
So we want to make sure that the set to none is true, at least for this case, sometimes you might not want to.
And that's pretty much what that does.
It will, if you do have zero grad on, commonly, the only reason you'll need it is for training large recurrent neural nets,
which need to understand previous context because they're recurrent.
I'm not going to dive into RNNs right now, but those are a big use case for not having zero grad gradient accumulation.
We'll simply take an average of all the accumulation steps and just averages the gradients together.
So you get a more effective, maybe block size, right?
You get more context that way.
And you can have the same batch size.
So just little neat tricks like that.
We'll talk about gradient accumulation more later in the course, but pretty much what's going on here.
We define an optimizer, Adam W.
We iterate over max editors.
We get a batch training split.
We do a forward pass, zero grad, backward pass, and then we get a step in the right direction.
So we're gradient descent works as magic.
And at the end, we could just print out the loss here.
So I've run this a few times.
And over time, I've gotten the loss of 2.55, which is okay.
And if we generate based on that loss, we get still pretty garbage tokens.
But then again, this is a background language model.
So actually, I might need to retrain this here.
It's not trained yet.
So what I'm actually going to do is run this, run this, run this, boom.
And then what I'll do, oh, it looks like we're printing out a lot of stuff here.
So that's coming from our get batch.
So I'll just comment that.
Or we can just delete it overall.
Cool.
And now if we run this again.
Give it a second.
Perfect.
So I don't know why it's still doing that.
If we run it again, let's see.
Where are we printing stuff?
No.
Ah, yes.
We have to run this again after changing it.
Silly me.
And of course, 10,000 steps is a lot.
So it takes a little while.
It takes a few seconds, which is actually quite quick.
So after the first one, we get a loss of 3.15.
We can generate from that.
And we get something that is less garbage.
You know, it has some next line characters.
It understands a little bit more to, you know,
space things out and whatnot.
So that's like slightly less garbage than before.
But yeah, this, this is pretty good.
So I lied.
There aren't actually any lectures previously where I talked about optimizers.
So might as well talk about it now.
So a bunch of common ones.
And honestly, you don't really need to know anything more than the common ones
because most of them are just built off of these.
So you have your mean squared error,
common loss function using regression, regression problems,
where it's like, you know, you have a bunch of data points,
find the best fit line, right?
That's a common regression problem.
Goals to predict a continuous output
and measures the average squared difference
between the predicted and actual values,
often used to train neural networks for regression tasks.
So cool.
That's the most basic one.
You can look into that more if you'd like,
but that's our most basic optimizer.
Gradient descent is a step up from that.
It's used to minimize the loss function in a model,
measures how well the model,
the gradient measures how well the model is able to predict
the target variable based on the input features.
So we have some input X,
we have some weights and biases maybe, WX plus B.
And all we're trying to do is make sure that the inputs
or make sure that we make the inputs become the desired outputs
and based on how far it is away from the desired outputs,
we can change the parameters of the model.
So we went over gradient descent recently or previously,
but that's pretty much what's going on here.
And momentum is just a little extension of gradient descent
that adds the momentum term.
So it helps smooth out the training
and allows it to continue moving in the right direction,
even if the gradient changes direction or varies in magnitude.
It's particularly useful for training deep neural nets.
So momentum is when you have, you know,
you consider some of the other gradients.
So you have something that's like maybe passed on from here
and then it might include a little bit of the current one.
So like 90%, like a good momentum coefficient
would be like 90% previous gradients
and then 10% of the current one.
So it kind of like lags behind and makes it converge sort of smoothly.
That makes sense.
Arm as prop, I've never used this,
but it's an algorithm that use the moving average of the squared gradient
to adapt learning rates of each parameter,
helps to avoid oscillations in the parameter updates
and can move and can improve convergence in some cases.
So you can look more into that if you'd like.
Adam, very popular,
combines the ideas of momentum and arm as prop.
He uses a moving average,
both the gradient and its squared value
to adapt learning rate of each parameter.
So often uses the default optimizer for deep learning models.
And in our case, when we continue to build this out,
it's going to be quite a deep net.
And Adam W is just a modification of the item optimizer
that adds weight decay to the parameter updates.
So helps to regularize and improve generalization performance.
Using this optimizer as it best suits the properties of the model
we'll train in this video.
So, of course, I'm reading off the script here.
There's no really other better way to say how these optimizers work.
But, yeah, if you want to look more into, you know,
concepts like momentum or weight decay
or, you know, oscillations and just some statistic stuff, you can.
But honestly, the only thing that really matters
is just knowing which optimizers are used for certain things.
So, like, what is the momentum used for?
What is Adam W great for?
What is MSC good for, right?
Just knowing what the differences and similarities are,
as well as when is the best case to use the optimizer.
So, yeah, you can find more information about that at torch.optim.
So when we develop language models,
something really important in language modeling,
data science, machine learning, at all,
is just being able to report a loss
or get an idea of how well our model is performing
over, you know, the first 1,000 iterations
and then the first 2,000 iterations
and 4,000 iterations, right?
So we want to get a general idea
of how our model is converging over time.
But we don't want to just print every single step of this.
That wouldn't make sense.
So what we actually could do is print every, you know,
200 iterations, 500.
We could print every 10,000 iterations
if you're running a crazy big language model if you wanted to.
And that's exactly what we're going to implement right here.
So, actually, this doesn't require an insane amount of Python syntax.
This is just, I'm actually just going to add it into our for loop here.
And what this is going to do is it's going to do what I just said,
is print every, you know, every certain number of iterations.
So we can add a new hyper parameter up here called eval-itters.
And I'm going to make this 250 just for,
just to make things sort of easy here.
And we're going to go ahead and add this in here.
So I'm going to go if-iter.
And we're going to do the module operator.
You can look more into this if you want later.
And we're going to do eval-itters equals equals zero.
What this is going to do is it's going to check if the current iteration
divided by, or sorry, if the remainder of the current iteration
divided by our eval-itters parameter,
if the remainder of that is zero, then we continue with it.
So hopefully that made sense.
If you want to, you could just ask GPT4
or GPT3.5, whatever you have, just this module operator,
and you should get a good general understanding of what it does.
Cool.
So all we can do now is we'll just say,
we'll just have a filler statement here.
We'll just do print, we've been f-string,
and then we'll go losses, losses, maybe that.
Or actually, I'm going to change this here.
We can go step-iter.
Add a little colon in there.
And then I'll go split.
Actually, I'll just go loss, and then losses like that.
And then we'll have some sort of put in here.
Something soon.
I don't know.
And all I've done is I've actually added a little function in here
behind the scenes.
You guys didn't see me do this yet.
But pretty much, I'm not going to go through the actual function itself,
but what is important is that you know this decorator right here.
This probably isn't very common to you.
This is torch.nograt.
And what this is going to do is it's going to make sure that
PyTorch doesn't use gradients at all in here.
That'll reduce computation.
It'll reduce memory usage.
It's just overall better for performance.
And because we're just reporting a loss,
we don't really need to do any optimizing or gradient computation here.
We're just getting losses.
We're feeding some stuff into the model.
We're getting a loss out of it, and we're going from there.
So that's pretty much what's happening with this torch.nograt.
And, you know, for things like, I don't know,
if you have other classes or other outside functions,
like, I mean, get batched by default isn't using this
because it doesn't have the model thing passed into it.
But estimate loss does have model pass into it right here.
So we just kind of want to make sure that it's not using any gradients.
We're going to reduce computation that way.
So anyways, if you want,
you can just take a quick readover of this,
and it should overall make sense.
Terms like .item.me are pretty common.
A lot of the other things here, like model, X and Y,
we get our logits and our loss.
This stuff should make sense.
It should be pretty straightforward.
And only two other things I want to touch on
is model.eval and model.train,
because you probably have not seen these yet.
So model.train essentially puts the model in the training mode.
The model learns from the data,
meaning the weights and biases,
if we have, well, sometimes you only have weights,
sometimes you, you know,
sometimes you have weights and biases, whatever it is,
those are updated during this phase.
And then some layers of the model,
like dropout and batch normalization,
which you may not be familiar with yet,
operate differently in training mode.
For example, dropout is active,
and what dropout does is this little hyperparameter
that we add up here.
It'll look like this.
Dropout would be like 0.2.
So pretty much what dropout does
is it's going to drop out random neurons in the network
so that we don't overfit.
And this is actually disabled in validation mode,
or eval mode.
So this will just help our model sort of learn better
when it has little, like, pieces of noise
and when things aren't in quite the right place
so that you don't have, you know, certain neurons in the network
taking priority and just making a lot of the heavy decisions.
We don't want that.
So dropout will just sort of help our model train better
by taking 20% of the neurons out, 0.2, at random.
And that's all dropout does.
So I'm just going to delete that for now.
And then, yeah, model that train.
Well, dropout is active during this phase,
during training, randomly turning off,
random neurons in the network.
And this is to prevent overfitting.
We went over overfitting earlier, I believe.
And as for evaluation mode,
evaluation mode is used when the model's being evaluated
or tested just like it sounds.
It's being trained.
What the other mode is being validated or tested.
And layers like dropout and batch normalization
behave differently in this mode.
Like dropout is turned off in the evaluation, right?
Because what we're actually doing
is we're using the entire network.
We want everything to be working sort of together.
And we want to actually see how well does it perform.
Training mode is when we're just, you know,
sampling, doing weird things to try to challenge the network
as we're training it.
And then evaluating or validation would be
when we just get the network in its optimal form
and we're trying to see how good of results it produces.
So that's what a val is.
And the reason we switched into a val here
is just because, well, we are testing the model.
We want to see, you know, how well it does
with any given set of data from a get batch.
And we don't actually need to train here.
If there was no training, this would not be here
because we would not be using any gradients.
So we would be using gradients if training was on.
Anyways, that's estimate loss for you.
This function is, you know, just generally good
to have a data science.
Your train and validation splits, whatnot.
And yeah, good for reporting.
You know how it is.
And we can go ahead and add this down here.
So there's something soon.
We'll go losses is equal to estimates loss.
And then we can go ahead and put a...
Yeah, we don't actually have to put anything in here.
Cool.
So now let's go ahead and run this.
Let me run from the start here.
Boom, boom, boom, boom, boom, boom.
Perfect.
I'm running for 10,000 iterations.
That's interesting.
Okay.
So, yes.
So what I'm going to do actually here
is you can see this loss part is weird.
So I'm actually going to change this up.
And I'm just going to switch it to...
We're going to go train loss.
And we're going to go losses.
And we're going to do the train split.
And then we're going to go over here
and just do the validation loss.
We can do validation or just val for short.
And I'm going to make it consistent here.
So we have a colon there, a colon here.
And then we just go losses and do val.
Cool.
So I'm going to reduce these max editors up here to only 1,000.
Run that.
Run this.
Oh, somebody did a match.
Okay.
Okay.
Okay.
Yes.
So what actually happened here was
when we were doing these little ticks,
what was happening is these were matching up with these.
And it was telling us, oh, you can't do that.
You can't start here and then end there
and have all this weird stuff.
Like, you can't do that.
So pretty much we just need to make sure that these are different.
So I'm going to do a double quote instead of single
and then double quote to finish it off.
And as you can see, this worked out here.
So I'll just run that again
so you guys can see what this looks like.
Okay.
Because we have, you know, a lot of decimal places.
So what we can actually do here is we can add in a little format
or a little decimal place reducer, if you call it,
just for, you know, so you can read it.
So it's not like some weird decimal number
and you're like, oh, does this eight matter?
Probably not.
Just like the first three digits, maybe.
So all we can do here is just add in,
I believe this is how it goes.
I don't think it's the other way.
We'll find out.
Some stuff in Python is extremely confusing to me.
But there we go.
So I got it right.
Go on and then period.
And as you can see, we have those digits reduced.
So I can actually put this down to 3F.
Wonderful.
So we have our train loss and our validation loss.
Great job you made it this far.
This is absolutely amazing.
This is insane.
You've gotten this far in the video.
We've covered all the basics, everything you need to know
about background language models, optimizers,
training loops, reporting losses.
I can't even name everything we've done because it's so much.
So congratulations that you made it this far.
You should go take a quick break.
Give yourself a pat on the back and get ready for the next part
here because it's going to be absolutely insane.
We're going to dig into literally state of the art language
models and how we can build them from scratch,
or at least how we can pre-train them.
And some of these terms are going to seem a little bit out
there, but I can ensure you by the end of this next section
here, you're going to have a pretty good understanding
about the state of language models right now.
So go take a quick break and I'll see you back in a little bit.
So there's something I'd like to clear up and actually sort of
lied to you a little bit, a little while back in this course
about what normalizing is.
So I recall we were talking about the softmax function
and normalizing vectors.
So the softmax is definitely a form of normalization,
but there are many forms.
There are not just a few or like there's not just one or two normalizations.
There are actually many of them and I have them on my second monitor here,
but I don't want to just dump that library of information on your head
because that's not how you learn.
So what we're going to do is we're going to plug this into GPT-4.
I'm going to say, can you list all the forms of normalizing in machine learning?
And how are they different from one another?
GPT-4 is a great tool.
If you don't already use it, I highly suggest you use it,
or even GPT-3.5, which is the free version.
But yeah, it's a great tool for just quickly learning anything
and then have it give you example practice questions with answers
so you can learn topics in literally minutes
that would take you several lectures to learn in a university course.
But anyways, there's a few here.
So min-max normalization, yep.
z-score, decimal scaling, mean normalization,
unit vector, or layer 2, robust scaling, power transformations.
Okay.
So yeah, and then softmax would be another one.
What about softmax?
It is in data type normalization,
but it's not typically using from normalizing input data.
It's commonly used in the output layer.
So softmax is a type of normalization,
but it's not used for normalizing input data.
And honestly, we've proved that here by actually producing some probabilities.
So this isn't something we used in our forward pass.
This is something we used in our generate function
to get a bunch of probabilities from our logits.
So this is, yeah, interesting.
It's good to just figure little things like these out for, you know,
just to be, put you on the edge a little bit more
for the future when it comes to engineering these kind of things.
All right, great.
So the next thing I want to touch on is activation functions.
And activation functions are extremely important
in offering new ways of changing our inputs that are not linear.
So, for example, if we were to have a bunch of linear layers,
a bunch of, let me erase this,
if we were to have a bunch of, you know,
nn.linears in a row,
what would actually happen is they would all just, you know,
they would all squeeze together
and essentially apply one transformation that sums up all of them kind of.
They all sort of multiply together and it gives us one transformation
that is kind of just a waste of computation
because let's say you have 100 of these nn.linear layers
and nothing else.
You're essentially going from inputs to outputs,
but you're doing 100 times the computation for just one multiplication.
That doesn't really make sense.
So what can we do to actually make these deep neural networks important
and what can we offer that's more than just linear transformations?
Well, that's where activation functions come in
and I'm going to go over these in a quick second here.
So let's go navigate over to the PyTorch docs.
So the three activation functions I'm going to cover
in this little part of the video are the relu, the sigmoid,
and the tanh activation functions.
So let's start off with the relu or rectified linear unit.
So we're going to use functional relu
and the reason why we're not just going to use torch.n
is because we're not doing any forward passes here.
I'm just going to add these into our,
I'm going to add these, let me clear this, clear this output.
That's fine.
I'm actually going to add these into here and there's no forward pass.
We're just going to simply run them through a function
and get an output just so we can see what it looks like.
So I've actually added this up here from torch.n
and import functional as capital F.
It's just kind of a common PyTorch practice, capital F.
And let's go ahead and start off with the relu here.
So we can go, I don't know, x equals torch.tensor
and then we'll make it a negative 0.05, for example.
And then we'll go dtype equals torch.flurp32
and we can go y equals f.relu of x.
And then we'll go ahead and print y.
It has no attribute relu.
Okay, let's try nn then.
Let's try nn and see if that works.
Okay, well that didn't work and that's fine
because we can simply take a look at this
and it'll help us understand.
We don't actually need to,
we don't need to write this out in code
as long as it sort of makes sense.
We don't need to write this in the forward pass, really.
You're not going to use it anywhere else.
So yeah, I'm not going to be too discouraged
that that does not work in the functional library.
But yeah, so pretty much what this does
is if a number is below,
if a number is 0 or below 0,
it will turn that number into 0.
And then if it's above 0, it'll stay the same.
So this graph sort of helps you visualize that.
There's a little function here.
That might make sense to some people.
I don't really care about the functions too much
as long as I can sort of visualize what the function means,
what it does, what are some applications it can be used.
That usually covers enough for like any function at all.
So that's the Relu function.
Pretty cool.
It simply offers a non-linearity to our linear networks.
So if you have 100 layers deep
and every, I don't know,
every second step you put a Relu,
that network is going to learn a lot more things.
It's going to learn a lot more linearity, non-linearity.
Then if you were to just have 100 layers
multiplying all into one transformation.
So that's what that is.
That's the Relu.
Now let's go over to Sigmoid.
So here we can actually use the functional library.
And all Sigmoid does is we go 1 over 1 plus
exponentiated of negative x.
So I'm going to add that here.
We could, yeah, why not do that?
Negative 0.05 float 32.
Sure.
We'll go f dot Sigmoid.
And then we'll just go x and then we'll print y.
Cool.
So we get a tensor 0.4875.
Interesting.
So this little negative 0.05 here
is essentially being plugged into this negative x.
So 1 over 1 plus 2.71 to the power of negative 0.05.
So it's essentially,
if we do 2.71, 2.71 to the power of negative 0.05,
we're just going to get positive.
So 1.05 and then 1 plus that.
So that's 2.05.
We just do 1 over that.
2.05.
So we get about 0.487.
And what do we get here?
0.4 at 7.
Cool.
So that's interesting.
And let's actually look, is there a graph here?
Let's look at the Sigmoid activation function.
Wikipedia.
Don't get too scared by this math here.
I don't like it either,
but I like the graphs they're cool to look at.
So this is pretty much what it's doing here.
So yeah, it's just a little curve.
Kind of looks like a, it's kind of just like a wave,
but it's cool looking.
That's what the Sigmoid function does.
It's used to just generalize over this line.
And yeah, Sigmoid function is pretty cool.
So now let's move on to the tanh.
The tanh function.
Google Bing is, or Microsoft Bing is giving me
a nice description of that.
Cool.
Perfect.
E to the negative x.
I like that.
So tanh is a little bit different.
There's a lot more exponentiating going on here.
So you have, I'll just say expo or exp of x
minus exp of negative x
divided by exp of x plus exp of negative x.
There's a lot of positives and negatives in here.
Positive, positive, negative, negative, negative, positive.
So that's interesting.
Let's go ahead and put this into code here.
So I'll go torch dot examples, or torch examples.
This is our file here.
And I'll just go tanh.
Cool.
So negative 0.05.
Cool.
What if we do a one?
What will that produce?
Oh, 0.76.
What if we do a 10?
1.0.
Interesting.
So this is sort of similar to the sigmoid,
except it's, you know,
it's actually asked to attach a BT what the difference is.
When would you use tanh over sigmoid?
Let's see here.
Sigmoid function and hyperbolic tangent or tanh function
are activations functions used in neural networks.
They have a similar s-shaped curve,
but have different ranges.
So sigmoid output values between a 0 and a 1
while tanh is between a negative 1 and a 1.
So if you're, you know,
if you're rating maybe the,
maybe if you're getting a probability distribution,
for example, you want it to be between 0 and 1,
meaning percentages or decimal places.
So like a 0.5 would be 50%, 0.87 would be 87%.
And that's what the sigmoid function does.
It's quite close to the softmax function, actually.
Except the softmax just, you know,
it prioritizes the bigger values
and puts the smaller values to our priority.
That's all the softmax says.
It's kind of a sigmoid on steroids.
And the tanh outputs between negative 1 and 1.
So, yeah, you could maybe even start theorycrafting
and thinking of some ways you could use
even the tanh function and sigmoid in different use cases.
So that's kind of a general overview on those.
So biogram language models are finished.
All of this we finished here is now done.
You're back from your break.
If you took one, if you didn't, that's fine too.
But pretty much we're going to dig into
the transformer architecture now.
And we're actually going to build it from scratch.
So there was recently a paper proposed
called the transformer model.
And this uses a mechanism called self-attention.
Self-attention is used in these multi-head attention,
little bricks here.
And there's a lot that happens.
So there's something I want to clarify
before we jump right into this architecture
and just dump a bunch of information
on your poor little brain right now.
But a lot of these networks, at first,
can be extremely confusing to beginners.
So I want to make it clear.
It's perfectly okay if you don't understand this at first.
I'm going to try to explain this in the best way possible.
Believe me, I've seen tons of videos
on people explaining the transformer architecture.
And all of them have been, to some degree,
a bit confusing to me as well.
So I'm going to try to clarify
all those little pieces of confusion.
Like what does that mean?
You didn't cover that piece.
I don't know what's going on here.
I'm going to cover all those little bits
and make sure that nothing is left behind.
So you're going to want to sit tight
and pay attention for this next part here.
So yeah, let's go ahead and dive into
just the general transformer architecture
and why it's important.
So in the transformer network,
you have a lot of computation going on.
You have some adding and normalizing.
You have some multi-hat attention.
You have some feed forward networks.
There's a lot going on here.
There's a lot of computation, a lot of multiplying,
there's a lot going on.
So the question I actually had at first was,
well, if you're just multiplying these inputs
by a bunch of different things along,
you should just end up with some random value at the end
that maybe doesn't really mean that much
of the initial input.
And that's actually correct.
For the first few iterations,
the model has absolutely no context
as to what's going on.
It is clueless.
It is going in random directions
and it's just trying to find the best way to converge.
So this is what machine learning and deep learning
is actually all about,
is having all these little parameters in,
you know, the adding and normalizing,
the feed forward networks, even multi-hat attention.
We're trying to optimize the parameters
for producing an output that is meaningful
that will actually help us produce
almost perfectly like English text.
And so this is the entire process of pre-training.
You send a bunch of inputs into a transformer
and you get some output probabilities
that you used to generate from.
And what attention does
is it sets little different scores
to, you know, each little token in a sentence.
For tokens you have character, subword,
and word-level tokens.
So you're pretty much just mapping
bits of attention to each of these,
as well as, you know,
what is the position also mean as well.
So you could have two words that are right next to each other,
but then if you don't actually, you know,
positionally encode them,
it doesn't really mean much,
because it's like, oh, these could be like 4,000 characters apart.
So that's why you need both
to put attention scores on these tokens
and to positionally encode them.
And that's what's happening here.
So what we do is we get to our inputs.
We got our inputs.
So, I mean, we went over this with
diagram language models.
We feed our X and Y,
so X would be our inputs,
Y would be our targets or outputs.
And what we're going to do
is give these little embeddings.
So I believe we went over embeddings a little while ago,
and pretty much what those mean
is it's going to have a little row
for each token on that table,
and that's going to store, you know,
some vector as to what that token means.
So let's say you had, like, you know,
the character E, for example,
the sentiment or the vector of the character E
is probably going to be vastly different
than the sentiment of Z, right?
Because E is a very common vowel,
and Z is one of the most uncommon,
if not the most uncommon letter in the English language.
So these embeddings are learned.
We have these both for our inputs and our outputs.
We give them positional encodings
like I was talking about,
and there's ways we can do that.
We can actually use learnable parameters
to assign these encodings.
A lot of these are learnable parameters, by the way,
and you'll see that as you, you know,
delve more and more into transformers.
But, yeah, so after we've given these inputs,
embeddings, and positional encodings,
and same thing with the outputs,
which are essentially just shifted right,
you have, you know, I up to block size for inputs,
and then I plus one up to block size plus one, right?
Or whatever little thing we employed here
in our background language models.
Quite what it was.
Or even if we did that at all.
No.
I'm just speaking gibberish right now,
but that's fine because it's going to make sense
in a little bit here.
So what I'm going to actually do
is I'm not going to read off of this right here
because this is really confusing.
So I'm going to switch over to a little,
like a little sketch that I drew out.
And this is pretty much the entire transformer
with a lot of other things considered
that this initial image does not really put into perspective.
So let's go ahead and jump into
sort of what's going on in here from the ground up.
So like I was talking about before,
we have some inputs and we have some outputs
which are shifted right,
and we give each of them some embedding vectors
and positional encodings.
So from here, let's say we have n layers.
This is going to make sense in a second.
n layers is set to four.
So the amount of layers we have is set to four.
So you can see we have an encoder, encoder.
Like we have four of these and we have four decoders.
So four is actually the amount of encoders
and decoders we have.
We always have the same amount of each.
So if we have, you know, ten layers,
that means we have ten encoders and ten decoders.
And pretty much what would happen
is after this input,
embedding and positional embedding,
we feed that into the first encoder layer
and then the next, and then next,
and then right as soon as we hit the last one,
we feed these into each of these decoders here,
each of these decoder layers.
So only the last encoder will feed into these decoders.
And pretty much these decoders will all run.
They'll all learn different things.
And then they'll turn what they learned.
They'll do, they'll apply a linear transformation
at the end of it.
This is not in the decoder function.
This is actually after the last decoder.
It'll apply a linear transformation
to pretty much sort of simplify
or give a summary of what it learned.
And then we apply a softmax on that new, you know, tensor
to get some probabilities to sample from,
like we talked about in the generate function
in our biogram.
And then once we get these probabilities,
we can then sample from them and generate tokens.
And that's kind of like the first little step here.
That's what's going on.
We have some encoders.
We have some decoders.
We do a transformation to summarize.
We have a softmax to get probabilities.
And then we generate based on those probabilities.
Cool.
Next up, in the encoder,
in each of these encoders, this is what it's going to look like.
So we have multi-hat attention,
which I'm going to dub into a second here.
So after this multi-hat attention,
we have a residual connection.
So in case you aren't familiar with residual connections,
I might have went over this before.
But pretty much what they do is
it's a little connector.
So I don't know.
Let's say you get some inputs X,
you have some inputs X down here,
and you put them into some sort of function here,
some sort of like feedforward network, whatever it is.
A feedforward network is essentially just a linear,
a RELU, and then a linear.
That's all feedforward network is right here.
Linear, really, really linear.
And all you do is you wrap those inputs
around so you don't actually put them
into that feedforward network.
You actually wrap them around,
and then you can add them to the output.
So you had some X values here,
go through the RELU, and then you had some wrap around.
And then right here, you simply add them together
and you normalize them using some encod layer norm,
which we're going to cover in a little bit.
And the reason why residual connections
are so useful in transformers
is because when you have a really deep neural network,
a lot of the information is actually forgotten
in the first steps.
So if you have your first view encoder layers
and your first view decoder layers,
a lot of the information here is going to be forgotten
because it's not being carried through.
The first steps of it aren't explicitly being carried through
and sort of skipped through the functions.
And yeah, you can sort of see how they would just be forgotten.
So residual connections are sort of just a cheat
for getting around that,
for not having deep neural networks forget things
from the beginning,
and having them all sort of work together to the same degree.
So residual connections are great that way.
And then, you know, at the end there,
you would add them together and then normalize.
And there's two different ways that you can do this add a norm.
There's add a norm and then norm and add.
So these are two different separate architectures
that you can do in transformers.
And both of these are sort of like meta architectures.
But pretty much pre-norm is the normalize then add,
and then post-norm is add then normalize.
So in this attention is all you need paper
proposed by a bunch of research scientists was
initially you want to add these,
you want to add these together and then normalize them.
So that is what we call the post-norm architecture.
And then pre-norm is just flip them around.
So I've actually done some testing with pre-norm and post-norm
and the original transformer paper
turned out to be quite actually a lot better,
at least for training very small language models.
If you're training bigger ones, it might be different,
but essentially we're just going to go by the rules that we use in here.
So add a norm.
We're not going to do norm and add.
Add a norm in this video specifically because it works better
and we just don't want to break any of the rules and go outside of it
because then that starts to get confusing.
And actually if you watch the Andre Carpathi lecture
on building GPTs from scratch,
he actually implemented it in the pre-norm way.
So normalize then add.
So yeah, based on my experience,
what I've done on my computer here is the post-norm architecture works quite better.
So that's why we're going to use it.
We're going to do add then normalize.
So then we essentially feed this into a feedforward network
which we covered earlier.
And then how did it go?
So we're encoder.
We do a residual connection from here to here
and then another residual connection from outside of our feedforward network.
So each time we're doing some other things like some, you know,
some computation blocks in here,
we're going to have a rest connection.
Same with our feedforward rest connection.
And then of course the output from here,
just when it exits,
it's going to feed into the next encoder block
if it's not the last encoder.
So this one is going to do all this.
It's going to feed into that one.
It's going to do the same thing.
Feed into this one.
Going to feed into that one.
And then the output of this is going to feed into each of these decoders,
all the same information.
And yeah, so that's a little bit scoped in as to what these encoders look like.
So now that you know what the encoder looks like,
what the feedforward looks like,
we're going to go into multi-head attention,
sort of the premise,
sort of the highlight of the transformer architecture
and why it's so important.
So multi-head attention,
we call it multi-head attention
because there are a bunch of these different heads
learning different semantic info
from a unique perspective.
So let's say you have 10 different people
looking at the same book.
If you have 10 different people,
let's say they're all reading the same Harry Potter book.
These different people,
they might have different cognitive abilities.
They might have different IQs.
They might have been raised in different ways.
So they might interpret things differently.
They might look at little things in that book
and their mind will,
they'll imagine different scenarios,
different environments from the book.
And essentially why this is so valuable
is because we don't just want to have one person,
just one perspective on this.
We want to have a bunch of different heads in parallel
looking at this same piece of data
because they're all going to capture different things about it.
And keep in mind each of these heads,
each of these heads in parallel,
these different perspectives,
they have different learnable parameters.
So they're not all the same one
looking at this piece of data.
They're actually,
they all have different learnable parameters.
So you have a bunch of these
at the same time learning different things
and that's why it's so powerful.
So this scale.product attention runs in parallel,
which means we can scale that to the GPU,
which is very useful.
It's good to touch on that.
Anything with the GPU that you can accelerate
is just an automatic win
because parallelism is great in machine learning.
Why not have parallelism, right?
If it's just going to be running the CPU, what's the point?
That's why we love GPUs.
Anyways, yeah.
So you're going to have these different,
you're going to have these things that are called keys,
queries and values.
I'll touch on those in a second here
because keys, queries and values
sort of point to self-attention,
which is literally the entire point of the transformer.
Transformer wouldn't really mean anything
without self-attention.
So I'll touch on those in a second here
and we'll actually delve deeper
as we hit this sort of block.
But yeah, you have these keys, queries and values.
They go into scale.product attention.
So a bunch of these running in parallel
and then you concatenate the results
from all these different heads running in parallel.
You have all these different people.
You concatenate all of them,
you generalize it,
and then you apply a transformation
to a linear transformation
to pretty much summarize that
and then do your add a norm,
then pay for a network.
So that's what's going on in multi-head attention.
You're just doing a bunch of self-attentions
in parallel, concatenating,
and then continuing on with this part.
So scale.product attention.
What is that?
So let's just start from the ground up here.
We'll just go from left to right.
So you have your keys, queries and values.
What do your keys do?
Well, a key is
let's just say you have a token and a sentence.
Okay?
So if you have
let me just
roll down here to a good example.
So
self-attention
uses
keys, queries and values.
Self-attention helps
identify
which of these tokens in a sentence
in any given sentence are more important
and how much attention
you should pay
to each of those characters or words, whatever you're using.
We'll just use words
to
make it easier to understand for the purpose of this video.
But
essentially imagine you have
these two sentences here.
So you have
let me bring out my little piece of text.
So you have
that didn't work.
So imagine you have
server, can I have the check?
And then you have
and you have
looks like I crashed the server.
So
I mean, both of these have
the word server in them, but they mean different things.
Server meaning like the waiter
or the waitress or whoever
is billing
you at the end of your restaurant visit.
And then looks like I crashed the server
is like, oh, there's actually a server running
in the cloud, not like a person
that's billing me, but an actual server.
That's maybe running a video game.
And
these are two different things. So what attention can do
is it can actually identify
which words would get attention here.
So it can say
server, can I have the check?
Can I have?
So it's maybe you're looking
for something you're looking for the check
and then server
is like, oh, well in this
in this particular sequence or in this
in the sentiment of this sentence here
server
is specifically tied to
this one meaning, maybe a human
someone at a restaurant
and then crash
the server
crash is going to get a very high attention
score because
you don't normally
crash a server at a restaurant
that doesn't particularly make sense.
So
if you have different words like this
what self-attention will do
is it will learn
which words in the sentence
are actually more important
and which words should
pay more attention to.
So that's really all that's going on here
and
the key
is essentially going to emit
a different
it's going to emit
a little tensor
here saying
what do I contain
and then query
is going to say
what am I looking for?
So what's going to happen
is if these, let's say
server, it's going to look for things like
check or crashed
so if it sees crashed
then that means the key and the query
are going to multiply
and it's going to get a very high attention score
but if you have something
like
it's like
there's literally almost any sentence
so that doesn't mean much.
We're not going to pay attention to those words
so that's going to get a very low attention score
and all attention
is you're just dot-producting
these vectors together.
So you get a key
and a query, you dot-product them
we already went over dot-products
in this course before
and then
this is a little bit of a confusing part
is you just scale
by one over the
square root
of the length of a row
in the keys or queries matrix
otherwise known as
DK.
So let's say we have
our key and our query
these are all going to be the same length by the way.
Let's say our keys
is
maybe our keys is going to be like
10 characters long
our keys are going to be 10 characters long as well
so it's going to do
one over the square root of 10
if that makes sense
and so
that's just
essentially a way of preventing
these dot-products
from exploding
we want to scale them because
as we have
as the length of it increases
so will the
ending dot-product
because there's more of these to multiply
so we pretty much just want to
scale it by using
an inverse square root
and that will just help us with
scaling make sure nothing explodes
in unnecessary ways
and then
the next little important part
is using tort.trill
which I imagine we went over in our examples here
trill
yeah
so
you can see that
it's a diagonal
it's a left triangular
matrix of ones
and these aren't going to be ones
in our self-attention here
in our tort.trill or masking
what this is going to be
is
the scores at each time step
the combination of scores
at each time step
so
if we've only gone
if we're only looking at the first
time step
we should not have access to the rest of things
or else that would be cheating
we shouldn't be allowed to look ahead
because we haven't actually produced these yet
we need to produce these before we can
put them into perspective
and put a weight on them
so we're going to set all these to zero
and then we go to the next time step
so now we've just generated this
one we haven't generated these yet
so we can't look at them
and then as we go more and more
as the time step increases
we know more and more context
about all of these tokens
so
that's all that's doing
mask attention is pretty much just saying
we don't want to look into the future
we want to only guess with what we currently know
in our current time step
and everything before it
you can't jump into the future
look at what happened in the past
and do stuff based on that
same thing applies to life
you can't really skip to the future and say
hey if you do this you're going to be a billionaire
no that would be cheating
you're not allowed to do that
you can only look at the mistakes you made
and say how can I become a billionaire
based on all these other mistakes that I made
how can I become as close to perfect as possible
which no one I can ever be perfect
but that's my little analogy for the day
so that's mask attention
pretty much just not letting us skip time steps
so that's fun
let's continue
two more little things I want to touch on before I jump forward here
so
these keys, queries and values
each of these are learned through a linear transformation
just an end dot linear
is applied
and that's how we get our keys, queries and values
so that's just a little
touching there if you're wondering how do we get those
it's just an end dot linear transformation
and then as for our
masking we don't actually apply this all the time
you might have seen right here
we have
multi-head attention
multi-head attention and then mask
multi-head attention
so this masked attention isn't used all the time
it's only used
actually one out of the three attentions
we have per layer
so
I'll give you a little bit more information
about that as we
progress more and more into the architecture
as we learn more about it
I'm not going to dive into that
quite yet though
so let's just continue on with what's going on
so we have a softmax
and why softmax important
well
I actually mentioned earlier
softmax is not commonly used
as a normalization method
but here we're actually using
softmax to normalize
so when you have all of these
when you have all of these
attention scores
essentially what the softmax is doing
is it's going to
exponentiate and normalize all of these
so
all of the attention scores that have scored
high like maybe 50 to
90% or whatever it is
those are going to take a massive effect
in that entire
attention
I guess tensor if you want to call it that
and that's important
it might not seem important
but it's essentially just giving the model
more confidence
as to which tokens matter more
so for example
if we just
did a normalization
we would
have words like server and crash
and then server and check
and then
you would just know
a decent amount about those
those would pay attention to a decent amount
because they multiply together quite well
but if you softmax those
then it's like
those are almost the only characters that matter
so it's looking at the context
of those two
and then we're sort of filling in
like we're learning about the rest of the sentence
based on just the
sentiment of those attention scores
because they're so high priority
because they multiply together
to such a high degree
we want to emphasize them
basically let the model learn more
about which words matter more together
so
that's pretty much just what the softmax does
it increases our confidence in
attention
and then a matrix multiply
we go back to our V here
and this is a value
so essentially what this is
is just a linear transformation
and we apply this on our
we apply this on our inputs
and
we have some value about
you know
what exactly those tokens are
and after we've gotten all of our attention
our softmax everything done
it's just going to multiply
the original values
by everything we've gotten so far
just so that you don't have any information
that's really lost or we don't have anything scrambled
just that we have like a general idea
of okay these are actually
all the tokens we have
and then these are
we found interesting the attention scores
so
we have an output which is a blend of input
vector values and attention placed on each token
and
that's pretty much what's happening in scaled dot
product attention in parallel
so we have a bunch of these that are just happening
at the same time
many of these happening at the same time
and yeah so
that's what attention is
that's what feedforward networks are
residual connections are
and yeah
and then so after this after we've
fed these into our decoders
we get an output
we apply linear transformation to summarize
softmax probabilities
and then we generate based on that
based on everything that we learned
and
actually what I didn't quite write a lot about
was the decoder
so what I'm actually going to talk about next
is something I didn't fill in yet
which is why
why the heck do we
use mass attention here
but not in these places so why the heck
do we have a multi attention here
all that attention here but mass attention here
so why is this
well the purpose of the encoder
is to pretty much learn
the present
past and future
and put that into a vector representation
for the decoder
that's what the encoder does
so it's okay if we look into the future
and understand tokens that way
because we're technically not cheating
we're just learning the different attention scores
and yeah we're just using that
to help us predict based on
what the sentence looks like
but not explicitly giving it away
just giving it an idea of
what to look for type of thing
and then
we use mass attention here because
well we don't want to look ahead
we want to look at the present and the past
and
later on
we're not giving anything explicit
here we're not giving anything yet
so we want to make some raw guesses
they're not going to be very good guesses at first
we want to make some raw guesses
and then later on
we can feed these
the added and normalized guesses
into
this next multi attention
which isn't masked
and then we can use
this max multi head attention
with the vector representation
given by the encoder
and then we can sort of do
more useful things with that
rather than just being forced to guess
raw attention scores
and then being judged for that
we can sort of introduce more
more and more elements
in this decoder block to help us learn more meaningful things
so
we start off with
making this
mass multi head attention
and then combining that
with
our
then afterwards we do a multi head attention
with the
vector representation from the encoder
and then we can make decisions on that
so that's kind of why that works
this way
if you don't think I explain it like amazingly
well you can totally just
ask GPT4
or GPT3.5
and get a pretty decent answer
but that's how that works
and
another thing I kind of wanted to point out here
is these linear transformations
that you see
I mean there's a lot of them
in the
scaled dot project attention
so you have your linears
for your value or key value
and key query and values
so
as well as the one up here
linears are great
for just expanding or shrinking
a bunch of important info
into something easier to work with
so if you have a bunch of
if you have a large vector containing a bunch
of info learned from this
scaled dot project attention
you can
you can sort of just compress
that into something more manageable
through a linear transformation
and it's essentially what's just happening here
with Softmax as well as
in our
scaled dot project attention here
for these linear transformations
from our inputs
to
quick keys, queries and values
that's all that's happening
if you want to read more about
linear transformations the importance of them
you can totally go out of your way to do that
but that's just sort of a brief summary
as to why they're important
just shrinking or expanding
so that's sort of a brief overview on how
transformers work
however in this
course we will not be building the transformer
architecture we'll be building
something called a GPT which you're probably familiar
with and GPT stands for
Generatively Pre-Trained Transformer
or Generative Pre-Trained Transformer
one of the two
and pretty much what this is
it's pretty close to the transformer
this architecture here except
it only adopts
the decoder blocks and it takes away
this multi-head attention here
so all we're doing is we're removing
the encoder
as well as what the encoder plugs into
so all we have left
is just some inputs
our max multi-head
attention
our post-norm architecture
and then
right after this we're not going to
a non-mass multi-head attention
but rather to a feed forward network
and then a post-norm
so that's all it is, it's just 1, 2, 3, 4
that's all it's going to look like
that's all the blocks are going to be
it is still important
to understand the transformer architecture itself
because you might need that in the future
and it is sort of a good practice in language
modeling to
have a grasp on and to understand
you know why we use mass multi-head
attention in the decoder and why we don't
use it in the encoder and stuff like that
so anyways
we're going to go ahead and build this
if you need to
look back if something wasn't quite clear
definitely skip back a few seconds
or a few minutes through the video and just
make sure you clarify everything up to this point
but yeah
I'm going to go over some more
math on the side here and just some other
little
little widgets we're going to need
for building the decoder
GPT architecture
so let's go ahead and do that
we're going to jump into
building the transformer rather than
building the GPT from scratch
what I want to do is linger on
self-attention for a little bit
or rather just the attention mechanism
and the matrix multiplication behind it
and why it works
so I'm going to use
whiteboard to illustrate this
so we're going to go ahead and draw out
a
we'll just use maybe a four token
sequence here of words
okay
so
we're going to highlight which words
are probably going to end up
correlating together
or the attention mechanism
is going to multiply them together
to a high amount based on what it learns
about those tokens this is what this is
so I'm going to help us illustrate that
and what the
GPT is going to see
sort of from the inside what it looks like from the inside
so
I'm going to go ahead and draw this out here
just make a table here
we'll give it
four of these
and draw a little line through the middle
my drawing might not be
perfect but it's definitely better
than on paper
so cool we have this
we have
my
I'm going to go here
dog
has
please
and then my
my dog
so I delete that
my dog has
please
cool
so to what degree
are these going to interact well my and my
I mean it doesn't really
give away that much it's only just the start
so maybe this will interact to
a low amount
and then you have my and dog
these might interact to a medium
amount because it's like your dog
so we might go
we might go medium
like that
and then my and has well that doesn't give away too much
so maybe that'll be low
and then my and please it's like oh
that doesn't really mean much my please that doesn't
really make sense maybe we'll
have it interact to a low amount
and then
these would be the same
thing so
my and dog so be medium
and then has and has
would be low
and then my and please would also be low
and then you have dog and dog
so these might interact to a low amount they're the same word
so we'll just
forget about that and then we have
a dog has
so these might interact to a medium amount
dog has the dog has
something
and then dog and please
these might interact to a high amount
because they're associating the dog
with something else meaning please
we have has
and dog these would interact to the same amount so
medium and then has and has
be
probably
to a low amount
and then
we could do low for
we could do what was it high
for this one as well please and dog
so these will interact
to a high amount
and then we have has and please
so
these could interact maybe a medium
amount
medium and then please and please which would be low
so what you get
I'll just highlight this in
I'll just highlight this in green here
so you get
all the medium
and high attention scores
you'd have your medium here
medium here
high medium
medium high
medium and medium
so you can see that these are sort of symmetrical
and this is what the attention map
will look like of course there's going to be some
scaling going on here based on the amount
of actual attention's
heads we have running in parallel
but that's besides the point
really what's going on here
is the network
is going to learn how to place
the right
attention scores because attention is simply
being used to generate tokens
that's that's how the
that's how the GPT works it's using attention
to generate tokens
so we can make
those sort of attention
scores how they're placed
we can make those learnable
through all of the like embeddings
like everything we have in the entire
network can make sure
that we place effective attention scores
and to make sure that they're measured properly
so
obviously I didn't quantify these very well
like not with floating point numbers
but this is sort of the premise
of how it works and how we want
the model to look at different tokens
and how they relate to one another
so that's what the
attention mechanism looks like under the hood
so this is what the actual
GPT or decoder only
transformer architecture looks like
and
so I'm just going to go through this step by step here
and then we can hopefully jump into some of the math
and code behind how this works
so we have
our inputs embeddings and positional
encodings we have only decoder
blocks and then some
linear transformation
and then pretty much just
we do some softmax
probability distribution
we sample from those and then we
start just generating some output
and then we compare those to our inputs
and see how off they were, optimized from that
in each of these
decoder blocks we have our all data
attention, res connections
feedforward network consists
of a linear, real linear
border and then
another res connection
in each of these multi-attentions
we have
multiple heads running in parallel
and each of these heads is going to take a
key, query and value
these are all learnable
linear transformations
and
we're going to basically dot product the key and query together
concatenate these results
and
do a little transformation to sort of
summarize it afterwards
and then what actually goes on in the
dot product attention is just the dot
product meaning of the key and query
the scaling to prevent
these values from exploding
to prevent the vanishing gradient problem
and then we have our
masking to make sure that
these, to make sure the model
isn't looking ahead and cheating
and then softmax matrix multiply
we output that and then
kind of fill in the blank there, so cool
this is a little bit
pretty much the
transform architecture a little bit dumb
down a little smaller
in complexity to actually understand but
that's kind of the premise of what's going on here
so still
implements a self-attention mechanism
so as you can see now
I am currently
on my macbook
M2 chip, I'm not going to
go into the specs of why it's important
but really quick, I'm just going to show you
how I SSH onto my other PC
so I go
SSH
just like that and then I type in my
ipv4 address
and then
I just
get a simple password
here, password that I've never had
is cool
so now I'm on my desktop computer
and this is the command prompt that I use for it
so awesome
I'm going to go ahead and go into the
free code camp
little directory I have
so cd desktop
cd python testing
and then here I'm actually going to activate
my CUDA virtual
environment
oops, not accelerate
I'm going to go CUDA
activate
cool and then I'm going to go
cd into free code camp
gbt course, awesome
so now, if I actually do
code on here like this to open up my
VS code, it doesn't do that
so there's another little way I have to do this
and you have to go into
VS code
go into a little remote explorer here
and then you can simply connect
so I'm just going to connect
to the current window
itself
there's an extension you need for this
called open SSH server, I think it's what it's called
and
it's simply the same password I used in the command prompt
I can type it correctly
awesome
so now it's SSH into my computer
upstairs
and I'm just going to open the little editor in here
nice, so you can see
that it looks just like that, that's wonderful
so now
I'm going to open this in a Jupyter notebook
actually
cd into desktop here
cd python
cd python testing
CUDA scripts
activate
cd free code camp
gbt course and then code
like that and it will open
perfect
how wonderful is that and I've already done
a little bit of this here but
we're going to
jump into exactly
how we can build up this transformer
or gbt architecture
in the code itself
so I'm going to
pop over to my Jupyter notebook in here
cool and now this little address
I'm going to paste that
into my
browser
awesome
so we have this gbt v1
Jupyter notebook
so what I've actually done is
I've done some importations here
so I've
imported all of these
python importations
all the hyper parameters that we used from before
I've imported the data loader
I've imported the tokenizer
the train and bell splits
they get batch function
estimate loss, just everything
that we're going to need and it's all in
neatly organized little code blocks
so awesome
now what?
well let's go ahead and continue here
with the actual
upgrading
from the very
top level so I remember
I actually showed
and you can skip back to this
I actually showed
the architecture of the gbt
sort of
lined out in I guess a little sketch
a little sketch that I did
and all we're going to do
is pretty much build up from the high level
the high high level general
architecture down to the technical stuff
down to the very root
dot product attention
that we're going to be doing here
so I'm going to go ahead and start off
with this
gbt language model which I just
renamed I replaced
bygram
with gbt here
so that's all we're doing and
we're going to add some
little code bits and
just walk through step by step
what we're doing so
let's do that so great
we're going to next we're going to talk about
these positional encodings
so I go back to the paper here
rather this architecture
we initially have our tokenize inputs
and then we give
we give them embedding
so token embeddings and then a positional
encoding so this positional
encoding going back to the attention paper is right here
so all it does
is every
even token index
we apply this function
and then every odd token index
we apply this function you don't really need to know
what it's doing other than
the fact that these are the different sine
and cosine functions that it uses
to apply positional encodings
to the tokenized inputs
so every
so on our first
index or whatever let's say we have hello world
okay there's five characters here
h will be index zero
so it'll get an even
encoding function
and then e will be odd
since it's index one so it'll get this one
and then l will get this the next l will get
this and then
or I don't know if I messed up that
order but essentially it just iterates
and it goes back and forth between
those applying these fixed functions
and the thing is with fixed functions
is that they don't actually
learn about the data at all
because they're fixed so another way we could
do this would be using
nn.embedding which is what we use
for the token
embedding so I'm going to go ahead
and implement this here in our
gbtv one script so I'm going to go
ahead and add on this line
self dot positional
self dot position embedding table
nn.embedding block size
so the block size is the length
or the sequence length
which in our case
it's going to be 8 so there's going to be 8 tokens
and
this means
we're going to have 8 different indices
and each one is going to be
of size nn.embed
and this is a new parameter I actually want to add here
so
nn.embed will not only be used
in positional embedding
but it will also be used in our
token embedding because when we actually
store
information about the tokens
we want that to be in a very large
vector so not necessarily
a probability distribution
or what we were using before in the
bi-gram language model but rather
a really large vector
or a list you could think about it
as a bunch of different
attributes that
are about a character so maybe
you know
A and E would be pretty close
but both vowels versus like
E and Z
would be very different because Z is not
a very common letter and E is the most common letter
in the alphabet so
we pretty much just want to have
vectors to differentiate
these tokens to place some
semantic meaning on them
and anyways
that's a little talk about what token embedding table
is going to do when we add n.embed
and then positional embedding table
is just the same thing
but instead of each character
having its own thing
each letter
index in the input is going to have its own embedding
so I can go and add this
up here
the n.embed
and we can just make this
maybe 384
so 384 is quite huge
and it's maybe a little too big
for your PC but we'll see in a second
so
what this is going to do is it's going to have a giant vector
it's going to be like
we could say like
embedding
embedding vector
and then it would be like this
and you would have
a bunch of different attributes so like 0.1
0.2
0.8
1.1
right? except
instead of 4 this is
384 elements long
and each of these
is just going to store a tiny little attribute
about that token
so
let's say we maybe had like a
two dimensional and we were using a word
so if we had
sad versus
happy
sad might be
sad might be
0.1
and then
0.8
or 0.8
whereas happy
sad would be
maybe the positivity
of what it's saying and then 0.8 would be
is it showing some sort of emotion
which is a lot right?
it's 80% emotion
and 0.1
of maybe positive sentiment
and then if we had
0.9
would be happy because it's happy
it's very good and then 0.8
is emotional because they're sort of the same
emotional level
but yeah so this is what our embedding vectors
are pretty much describing and
all this hyperparameter
is concerned with is how long
that vector actually is
so anyways
let's continue with the GPT
language model class so the next bit I like
to talk about is how many decoder
layers we have
so in here let's just say we have
four decoder layers
so we have four of these it's going to go through this one
and then this one and then this one
then this one this is all happening
sequentially so we could
actually make a little
sequential neural network with
four decoder layers
so I'm actually going to add this in
and then a little bit of extra code which I'll explain
in a second here so this
self
dot blocks is how many
decoder blocks we have running
sequentially or layers
blocks and layers can be used interchangeably in this
context
but yeah we have an end dot sequential
and this asterisk is pretty much saying
we're going to repeat
this right here
for how many
end layer is and end layer is another hyperparameter
we're going to add
we go end underscore layer
we go equals four
so end underscore layer equals four
that means it's going to make four of these
I guess blocks
or layers sequentially
it's going to make four of them
and this little block thing
we're going to build on top of this in a second here
we're going to make an actual block
class and I'm going to explain what that does
but for now
this is going to be some temporary code
as long as you understand that this is what
this is how we create our four layers
our four decoder layers
that's all you need to know for now
I'm going to move more into this block later
as for this
self dot layer norm final
this is the final layer norm
all this is going to do
is we're just simply going to add this
to the end of our network here
just simply at the end here
and all this is going to do
is just going to help the model converge better
layer norms are super useful
and yeah
so you'll see more how that works
I'll actually remove it later on
and we'll actually
compare and see
how good it actually does
and you can totally go out of your way
to experiment
with different normalizations
and see how well the layer norm
helps the model perform
or how well the loss
sort of converges over time
when you put the layer norm in different places
so
let's go back here
and now we have this
end here
which is the language
I believe this is the language modeling
head or something
again this is what Andrey Karpathy used
I'm assuming that means language modeling head
but pretty much
all we're doing is we're just
projecting
we're doing this final
transformation here
this final little linear layer here
from all of these sequential
decoder outputs
and we're just going to transform that
to
something that the softmax can work with
so we have our layer norm afterwards
to sort of normalize help the model converge
after all these
after all this computation
we're going to feed that into a linear layer
to make it I guess
softmax
workable so the softmax can work with it
and
yeah so we're just
simply projecting it from
an embed which is the vector length that we get
from our decoder
and
and this vocab size
so the vocab size is going to
essentially give up a little
probability distribution on each token that we have
or the vocabulary
so anyways
I'm going to make this back to normal
here and we're going to just
apply this
to the forward pass
so a little thing I wanted to add on
to
this positional embedding
or rather just the idea of
embeddings versus
the fixed definite function
of the
sinusoidal functions
and the cosine functions that we used here
these are both actually
used in practice
the reason I said we're going to use embeddings
is because we just want it to be more oriented
around our data
however in practice
sinusoidal encodings are used
in base transformer models
whereas learned embeddings what we're using
are used in variants like
GBT and we are building a
GBT so we're probably
going to find out a performance from learning about embeddings
and this is just
summing up the experts do
it's a little practice that experts do
when they're building transformer models
versus variants like GBTs
so that's just a little background on
why we're using
learnable embeddings
so now let's continue
with the forward pass here
so I'm going to paste in some more code
and
let me just make sure this is
formatted properly cool
so we have this
token embedding which is our token embedding
table
we take an IDX
token embedding here
then what we do with this positional embedding table
so we have this torch.arrange
we make sure this is on the CUDA device
the GPU device
so it's in parallel
and all this is going to do
is it's going to look at how long is T
and
let's say T is our block size
so T is going to be 8
so all it's going to do is give us 8 indices
it's going to be like 0, 1, 2, 3,
4, 5, 6, 7
8 of those
and we're essentially just going to give each of those
each of those indices
a different
a different
end embedding vector
for each of those indices
just a little lookup table
and that's what that is
so all we do now
is it's actually quite simple
and this is a very efficient way to do it
is you just add these two together
broadcasting rules
which you might want to look into
I'll actually search that up right now
torch
broadcasting semantics
pie torch
broadcasting
I cannot spell
broadcasting semantics
so
these are a little bit funky
when you look at them the first time
but pretty much these are just rules
about how you can do
arithmetic operations
and just operations in general
to tensors
so tensors are like you think of matrices
where it's like a 2x2
tensors can be the same thing
but they could be like a 2x2x2
or a 2x2x2x2x2
whatever dimension you want to have
there
and pretty much it's just rules about how you can
have two of those
weirdly
shaped tensors and do things
to them
so just some rules here
I would advise you familiarize yourself with these
even play around with it if you want
just for a few minutes
and just get an idea for
which, like just try to multiply
tensors together
and see which ones throw errors and which ones don't
so it's a good idea to understand how broadcasting
rules work
obviously this term
is a little fancy and it's like
that's like a crazy advanced term
not really
it's pretty much just
some rules about how you're
multiplying these really weirdly shaped tensors
so yeah
anyways
if we go back to here
we are allowed to broadcast these
we're allowed to actually add them together
so the positional embedding and the token embedding
we get X from this
B by T by C shape
so now
what we can do
with these is we can actually feed it
into the
GPT or I guess
sort of a transformer network if you want to say that
so we have these embeddings
and positional encodings
we add these together and then we feed them
into our sequential network
so how are we doing this
well we go self dot blocks which is up here
and we essentially just feed
an X which is literally
exactly what happens here
we have our tokenized inputs
we got our embeddings and our positional encodings
through learnable embeddings we add them together
and then we feed them into the network directly
so
that's all that's happening here
and that's how we're feeding an X
which is the output of these
then after
this is like way after
we've gotten through all of these
GPT layers or blocks
we do this final layer norm
and then this linear transformation
to get it to a
softmax
to get it to essentially probabilities
that we can feed into our softmax function
and then other than that
this forward pass is exactly the same
other than this little block of code here
so if this makes sense so far
that is absolutely amazing
let's continue I'm actually going to add
a little bit of
in practice
some little
weight initializations
that we should be using
in our language model
and in module subclass
so
I'm going to go over a little bit of math here
but this is just really important for practice
and to make sure that your model
does not fail in the training process
this is very important
it's going to be a little funky
on the conceptualizing
but bring out some pen and paper
and do some math with me
we've built up some of these
initial GPT language model architecture
and before we continue building
more of it and the other functions
some of the math stuff that's going on
the parallelization that's going on in the script
I want to show you some of the math
that we're going to use to initialize the weights
of the model to help it train
and converge better
so there's this new thing
that I want to introduce called standard deviation
and this is used in intermediate level mathematics
the symbol essentially looks like this
population standard deviation
so
n
the size
so it's just going to be an array
the length of the array
and then xi
we iterate over each value
so xf position 0
xf position 1
xf position 2
and then this u here is the mean
so
we iterate over each element
we're going to
subtract it by the mean
we're going to square that and then keep adding
all these squared results together
and then once we get the sum of that
we're going to
subtract or we're going to divide
this by the number of elements there are
and then once we get this result
we're going to square root that
so this symbol here
might also look a little bit unfamiliar
and
I'll illustrate this out for you
so we go to our whiteboard
and this e
looks like
looks like that
let's just say we were to put in
x
i like that
and our array
let's just say for instance
our array
is 0.1
0.2, 0.3
so what would the result of this be
well if we look at each element
iteratively add them together
so 0.1
plus 0.2 plus 0.3
well we get 0.6 from that
so this would essentially
be equal to
0.6
that's what that equals
we just add each of these up together
or we do whatever this is iteratively
whatever this element is
we iterate over
the number of elements we have in
the arbitrary array
or
vector or list or whatever you want to call it
and then we just
sort of look at what's going on here
and we can do some basic arithmetic stuff
so
let's walk through a few examples
just to illustrate to you
what the results look like
based on the inputs here
so I'm going to go back to my whiteboard
we're going to draw a little line here
just to separate this
so
I want to calculate the standard deviation
do standard deviation
of
and then we'll just make some random array
negative
0.38
negative 0.38
0.52
and then 2.48
cool
so we have this array this is three elements
so that means n
is going to be equal to three
let me drag this over here
so n is the number of elements
so n is going to be equal to three
our mean
well
our mean is just
we add all these up together and then we average them
so our mean
is going to be equal to
let's just say
0.38
plus 0.52
plus
2.48
and then divided by three
and the answer to this
I did the math ahead of time
is literally 0.873
repeated but we're just going to put 0.87
for simplicity's sake
cool so the mean of this
is 0.87 and n is equal to three
now we can start doing
some of the other math
so
we have this
O has a cool line
and we do
square root
one over
n which is equal to three
and then we
multiply this
by sigma
that's what this symbol is
that's sigma that's the name for it
and then we go
X
I
minus
and then our mean of
0.87
apologies for the sloppy writing
and then we square that
so let me drag this out
awesome
so let's just do this
step by step here
so the first one is going to be
0.38
0.
negative
0.38
and we're going to do minus the mean here
so minus 0.87
and I'm just going to wrap all this
in brackets so that we don't miss anything
wrap it in brackets
and then just square it and see what we get after
so I'm just going to write all these out
then we can do the calculations
so next up we have 0.52
minus 0.87
we'll square that
and then next up we have
2.48
minus 0.87
and then we square that as well
so awesome
what is the result of this
the result of
negative 0.38 minus
0.87
squared is
1.57
the result of
this line
is 0.12
again these are all approximations
they're not super spot on
we're just doing this to understand
what's going on here
just to overview the function not for precision
then the next one is going to be
2.59
and you can double check all these
calculations if you'd like
I have done these preemptively so
that is that
and now from here
what we have to do is add each of these together
so
1.57
plus 0.12
plus 2.59
divided by 3
is
1.57
plus 0.12
plus 2.59
all that divided by 3
is going to be equal to 1.42
keep in mind we also have to
square root this
so the square root of that
is going to be
1.19
approximately
we'll just add
this guy ahead of it
so that's what the
standard deviation of
this array is
negative 0.38
0.52, 2.48
standard deviation is
1.19
let's do another example
so let's say
we want to do the standard deviation
of
0.48
0.5
0.50
I guess 0.52
so there's a little pattern here
just goes up by 0.02 each time
and
you're going to see why this is
vastly different than the other example
so let's walk through this
so first of all we have N
N is equal to 3
cool
what does our mean
our mean
well if you do our mean our mean is 0.5
0.48 plus this
plus that
that's going to be 0.5
and
if you're good with numbers
you'll probably already be able to do this in your head
but that's okay if not
next up
we're going to do this in the formula
so
what do these iterations look like
so
0.
let's just do these in brackets
the old way
0.5
squared
the next one is
0.5
minus 0.5
squared which we already know is 0
and this one is 0.52
minus
0.5
squared so the result of 0.48
minus 0.5 squared
and what's right equals here
is going to be
approximately 0.02
squared
so that would be 0.004
like that
so I'll make this not actually overlap
0.004
and then this one
we obviously know would be 0
because 0.5 minus 0.5
that's 0 then you square 0
still the same thing
and then this one is
0.0004 as well
so
when we add these two together
we're going to get
0.0008
just like that
and then if we divide them by 3 or whatever
n is
then we end up getting
0.00026
repeating so I'll just write
266 like that
and so
all we have to do at this point
is do the
square root of this
and
we'll do
square root of 0.00026
approximately
and
that's going to be equal to about
0.0163
so
that is our
standard deviation
of both of these arrays here
so 0.048
and then 0.52
our standard deviation is
0.0163
so very small
and then we have
negative 0.38, 0.52
and 2.48
we get a standard deviation of 1.19
so you can see that these numbers are vastly different
one is like
one is literally
100 times greater than the other
so
the reason for this is because these
numbers are super
diverse
I guess another way
you could think of them is that
they stretch out very far from the
mean
this essentially means when you're initializing
your parameters
that if you have some outliers
then your network
is going to be funky
because it's
the learning process just messed up because you have outliers
and it's not just learning the right way
it's supposed to
whereas if you had
way too small of a standard deviation
from your initial parameters
like in here but maybe even smaller
so let's say they were all
0.5
then all of your neurons
would effectively be the same
and they would all learn the same pattern
so then you would have no learning done
so one would either be
you're learning a super super unstable
and you have outliers that are
just learning
very distinct things and not really
not really
not really letting other neurons
get opportunities to learn
or rather other parameters to learn
if you have a lot of diversity
you just have outliers and then if you have
no
diversity at all then
essentially nothing is learned and your network
is useless so all we want to do
is make sure that our standard deviation
is balanced and stable
so that the training process
can learn effective things
so each neuron can learn a little bit
so you can see here
this would probably be an okay standard deviation
if these were some parameters because
they're a little bit different than each other
they're not all like super super
close to the same
and yeah
so essentially what
this looks like in
code here is the following
so you don't actually need to
memorize what this does as it's
just used in practice
by professionals
but essentially what this does
is it initializes our weights
around certain standard deviations
so here we set it to 0.02
which is pretty much the same
as what we had in here
so
point
point
this one's a little bit off in the standard deviation
set here
but essentially
we're just making sure that our weights
are initialized properly
and you don't have to memorize this at all
it's just used in practice and it's going to help our training
converge better
so as long as you understand
that we can apply some initializations
on our weights
that's all that really matters, so cool
let's move on to the next part
of our GBT architecture
so awesome, we finished this GBT language
class, everything's pretty much done here
we did our knit
we did some weight initializations
and we did our forward pass, so awesome
that's all done, now let's move on to the next
which is the
block class
so what is block?
well, if we go back to this diagram
each of these decoder blocks is a block
so
we're pretty much just going to fill in this gap here
our GBT language model has these two
where we get our tokenized inputs
and then we do some transformations
and the softmax after
and essentially we're just filling
in this gap here and then we're going to build out
and just sort of branch out until it's
completely built
so let's go ahead and build these blocks here
what does this look like?
that's what this does
so we have our knit, we have a forward pass
as per usual
and knit
and a forward pass as seen
in the GBT language model class
which is going to look like this
forward and an init
so the init
is going to just initialize some things
it's going to initialize some transformations
and some things that we're going to do in the forward pass
that's all it's doing
so what do we do first?
well we have this new head size
parameter introduced
so head size is the number of features
that each head will be capturing
in our multi-head attention
so all the heads in parallel
features are each of them capturing
so we do that by dividing
n embed by n head
so n head is the number
of heads we have
and n embed is the number of features we have
where we're capturing
so 384 features divided by 4 heads
so each head is going to be capturing
96 features
hence head size
so
next up we have self.sa
which is just short for self-attention
we do a multi-head attention
we pass in our n head
and our head size and you'll see how these
parameters fit in later
once we build up this multi-head attention
class so cool
now we have a feed forward
which is as explained
just in the diagram here
our feed forward is just this
which we're actually going to build out next
and we have two layer norms
and these are just for the
post norm
pre norm architecture that we could implement here
in this case it's going to be
post norm just because
I found that it converges better for this
for this course and the data that we're using
and just the model parameters
and what not it just works better
so
also that is the original
architecture that we use in the
attention paper
so you might have seen that they do an add a norm
rather than a norm and add
anyways
we've initialized all of these
so we have head size, self attention
feed forward and then two layer norms
so in our forward pass
we do our self attention first
let's actually go back to here
so we do our self attention
then add a norm
then a feed forward and then add a norm again
so what does this look like
self attention, add a norm
feed forward, add a norm
cool
so we're doing an add so we're going
x plus the previous
answer which is adding them together
and then we're just applying a layer norm to this
so cool
if you want to look up more into what layer norm does
and everything and why it's so useful
you can totally go out of your way to do that
but
layer norm is essentially just going to
help smoothen out our features
here
so
and honestly there's not much else to that
we just return this final value here
and that's pretty much the output of our blocks
so
next up I'm going to add
a new little code block here
which is going to be
our feed forward
so let's go ahead and do that
so feed forward, it's just going to look exactly like this
it's actually quite simple
so all we do is we make an nn dot sequential
torch dot nn
we make this a sequential network of linear
linear, relu, and then linear
so
in our linear
we have to pay attention to the shapes here
so we have n embed
and then n embed times 4
and then the relu will just
essentially
what the relu will do is it looks like this
let me illustrate this for you guys
so
essentially you have this graph here
and
let's just make this a whole plane actually
so
all of these values
that are below 0
all these values that are below 0 on the x axis
and
equal to 0 will be changed
just to 0 like that so you have all these values
that look like this
and then everything that is above 0 just stays the same
so you essentially just have this
funny looking shape it's like straight
and then diagonal that's what the relu function does
it looks at a number
sees if it's equal to or less than 0
if that's true we give that number 0
and if it's not
then we just leave the number alone
so cool very cool
non-linearity function
you can read papers on that if you like
but
essentially the shape of this
just doesn't matter all we're doing is
we're just making sure that we're just converting
some values if they're equal to
or below 0 that's all this is doing
and then
we essentially are multiplying
this we're doing this
linear transformation times this one
so we have to make sure that these inner
we have to make sure that these
inner dimensions line up so 4 times
N embed and 4 times N embed
those are equal to each other so our output shape
should be N embed
by N embed cool
so now we have our dropout
and in case you don't know what dropout is
it pretty much just
makes a certain percentage
of our neurons just
dropout and become 0
this is used to prevent overfitting
and some other little details
that I'm sure you could
you could figure out through experimenting
so
all this actually looks like in a parameter form
is just
dropout
dropout equals
we'll just say 0.2 for the same
so 0.2 means
20%
or 0.2 is going to
yeah so 0.2
in percentage form is just going to dropout
20% of our
neurons turn them to 0 to prevent overfitting
that's what that's doing
so cool
we have our feedforward network we dropout after
to prevent overfitting and then we just
call it forward on this sequential network
so cool
feedforward pretty self-explanatory
we're going to add the
multi-head attention class
so we've built all these decoder blocks
we've built
inside of the decoder blocks we've built the feedforward
and our res connections
and now
all we have to do left in this block
is the multi-head attention
so it's going to look exactly like this here
we're going to ignore the keys and queers for now
and save this for dot product attention
so we're going to
essentially just make a bunch of these
multiple
heads
and we're going to concatenate results and do a linear
transformation so what does this look like in code
well let's go ahead
and add this here
all that attention cool
so multiple heads of attention in parallel
I explained this earlier so I'm not going to jump into
too much detail on that
but we have our knit
we have our forward
and what are we doing in here
so our self dot heads is just a module list
and
module list is kind of funky I'll dive into it
a little bit later
but essentially what we're doing is we're having
a bunch of these heads
essentially in parallel
for each head
so num heads let's say our num heads is
set to
our num heads
is set to
maybe four in this
block we do multi-head attention
we do n heads and then head size
so
and heads and then head size so num heads
essentially what it is so for the number of
heads that we have which is four
we're going to pretty much make one head
running in parallel
so four heads running in parallel is what this
does here
then we have this projection
which is essentially just going to
project the
head size
times the number of
heads to an embed
and you might ask well that's
weird because
num heads times this is
literally equal to an embedding
if you go back to the math
we did here
and the purpose of this is just to be
super hackable so that if you actually do want to
change these around it won't be throwing you dimensionality
errors so that's what we're doing
just a little projection
from our
whatever these values are
up to this
constant feature
length of an embed
so then we just follow that with a drop out
dropping out 20% of the
networks neurons
now let's go into this forward here
so forward
torch dot concatenate or torch dot cat
we do four h and self dot heads
so we're going to concatenate
each head together
along the last dimension
and the last dimension in this case
is the
b batch
by time
by we just say feature dimension or channel dimension
the channel dimension here is the
last one so we're going to
concatenate along this feature dimension
and let me just help you illustrate
what exactly this looks like
so
when we concatenate along these
we have this b by t
and then we'll just say
our features are going to be
h1 like
each of our heads here
another h1
h1 h1 and these are all just features
of head one and then our next
would be h2
h2 h2 h2
and then let's just say we have
a third head go h3
h3
h3 h3
h3 like that
so we have
maybe four features per head
and there's three heads
so essentially all we're doing
when we do this concatenate
is we're just concatenating these along the last
dimension so to convert
this like ugly list format
of just each head
features sequentially in order
which is like really hard
to process we're just concatenating these
so they're easier to process
so that's what that does
and then we just follow this with a dropout
self dot projection
and then just follow that with a
dropout so cool
if that didn't totally make
sense you can totally just plug this code into chat
gbt and
get a detailed explanation on how it works
if something wasn't particularly clear
but essentially that's the premise
you have your batch by time
batch by
sequence length
or time use interchangeably
and then you have your features which are all
just in this weird list format
of each feature just listed
after another
so cool
that's what multi head attention looks like
let's go ahead and implement dot product
attention or scale dot product attention
so a little something I'd like to cover before
we go into our next scaled
dot product attention was just this linear
transformation here
and you might think well what's the point if we're just
transforming
an embed to an embed right
we're to have the match like that
and
essentially what this does is it just adds in another
learnable parameter
for us so it has a weight
and a bias if we set bias
to false
like that then it wouldn't have a bias
but it does have
a bias so another just wx
plus b if you will a weight times x
plus a bias so it just adds
more learnable parameters to help our
network
learn more about this text
so cool I'm going to go ahead and add
in this last but not least
scale dot product attention
or head class so there's going to be
a bunch of these
heads hence
class head running in parallel
and inside of here we're going to do some
scale dot product attention
so there's a lot of code in here don't get
too overwhelmed by this but I'm going to walk
through this step by step so we have our
in it we have our forward
awesome
so what do we do in our
architecture here
so we have a key
a query and a value
the keys and the queries dot
product together they get scaled
by
one over the square root of
length of a row in the keys or queries
matrix so we'll just say maybe keys
for example
the row of keys
the length of a row in keys
and then we just do our
masking to make sure the network
does not look ahead and cheat
and then we do a softmax
and a matrix
multiply to
essentially add this
value weight on top of it
so cool
we do this
keep in mind this initialization
is not actually doing any calculations
but just rather initializing
linear transformations that we will do
in the forward pass
so this self dot key
is just going to
transform and embed to head size
bias false and then
I mean the rest of these are just the same
and embed to head size because each head
will have 96 features
rather than 384
so we kind of already went over that
but that's just what that's doing
cool that's just a linear transformation
that's happening to convert
from 384 to 96 features
then we have this
self dot register buffer
well what does this do you might ask
register buffer is essentially just going
to register
this no look ahead
masking in the model state
so instead of having to re-initialize
this every single head for every
single forward and backward pass
we're just going to add this to the model
state so it's going to save us a lot of
computation that way on our training
so our training times can be reduced just because
we're registering this
yeah
so it's just going to prevent some of that
overhead computation of having to redo
this over and over again
you could still do training without
this it would just take longer
so that's what that's doing
yeah
so now we have this drop-out
of course and then in our forward pass
let's
break this down step by step here
so we have a b by t by c
so batch by time
by channel is our shape
we just unpack those numbers
and then we have a key
which is just calling this
linear transformation here on an input
x
and then a query which is also
calling the same transformation but a different
learnable transformation on x as well
so what we get
is this instead of b by t by c
we get b by t by head size
hence this transformation
from 384 to 96
so that's what that is
that's how these turn out here
so now we can actually compute
the attention scores
so what do we do
we'll just say weights is our attention
weights are
I guess you could say that
we have our queries
dot product matrix multiply
with the
keys transposed
so
what does this actually look like
and I want to help you guys
sort of understand what transposing does here
so
let's go back to here
and draw out what this is going to look like
so
essentially what transposing is going to do
is
it is just going to make sure
let me draw this out
first
so let's say you had
I don't know
maybe
a
b
c
d
and you have a
b
c
and d cool let's draw some lines
to separate these
so
awesome so essentially what this does
is the transposing
puts it into this form
so if we didn't have
transposed then this would be in a different order
it wouldn't be a b c d
in both
from like top to bottom left to right type of thing
it would be in a different order
but essentially not allow us
to multiply them the same way
so when we do a by a
a times b
it's like sort of a direct
multiply if you will
I don't know if you remember times tables at all
from elementary school
but that's pretty much what it is
we're just setting up in a times table form
and we're computing attention scores that way
so
that's what that is
that's what this transposing is doing
all this does is it just flips
the second last dimension
with the last dimension
so
in our case our second last
is t and our last is head size
so it just swaps these two
so we get b by t by head size
and then b by head size by t
we dot product these together
also keeping in mind our scaling
here
which is taking this
we're just taking this scaling
one
over the square root of length
of a row in the keys
if we look at this here
now there's little analogy
I'd like to provide for this scaling
right here
so imagine in a room
with a group of people and you're trying to understand
the overall conversation
if everyone is talking at once
it might be challenging to keep track
of what's being said
it would be more manageable if you could focus on
time right?
so that's similar to how a multi head attention
in a transformer works
so each of these heads
divides the original problem
of understanding the entire conversation
i.e. the entire input sequence
into smaller more manageable
sub problems
each of these sub problems is a head
so the head size
is the number of these sub problems
now consider what happens when each person
talks louder or quieter
if someone speaks too loudly
or
the values and the vectors are very large
it might drown out the others
this could make it difficult to understand the conversation
because you're only hearing one voice
or most of one voice
to prevent this
we want to control how loud
or how quiet each person is talking
so we can hear everyone evenly
the dot product of the
query and key vectors
in the attention mechanism
we want to check how loud each of voices
if the vectors are very large
or high dimensional
or many people are talking
the dot product can be very large
to control this volume
by scaling down the dot product
using the square root of the head size
this scaling helps ensure that no single
voice is too dominant
allowing us to hear all the voices evenly
this is why we don't scale
by the number of heads
time steps
they don't directly affect how loud each voice is
so in sum
multi head attention allows us to focus on
different parts of the conversation
and scaling helps us to hear
all parts of the conversation evenly
allowing us to understand
the overall conversation better
so hopefully that helps you understand exactly
what this scaling is doing
so now let's go into the rest of this here
so we have this scaling applied
for our head size
our head size dimension
we're doing this
dot product matrix multiplication
here we get our B by T by T
and then what is this
masked fill doing
so let me help you illustrate this here
so mask fill
is essentially
we'll say block size
is 3 here alright
so we have
initially
like a 1
a 0.6
and then like a 0.4
then our next one is
yeah we'll just say all of these are the same
so essentially
in our first one
we want to mask out everything
except for the first time step
and then when we advance one
so let's just change this here back to 0
when we go on to the next time step
we want to expose the next piece
so 0.6 I believe it was
and then a 0 again
and then when we expose the next time step after that
we want to expose all of them
so just kind of what this means is
as we
as the time step advances
in this sort of I guess vertical
part
is every time this steps 1
we just want to expose one more token
or one more
and then we'll use sort of in like a staircase format
so
essentially what this mask fill is doing
is it's making this
T by T so block size by block size
and
for each of these values we're going to set them
to negative infinity
so for each value that's 0
we're going to make that the float value negative infinity
so it's going to look like this
negative infinity
negative infinity
just like that
so essentially what happens after this
is our softmax
is going to take these values
and it's going to exponentiate normalize them
we already went over the soft
softmax previously
but
essentially what this is going to do this
this last dimension here
concatenate
or not concatenate
rather apply the softmax along the last dimension
is it's going to do that
in this sort of horizontal here
so this last
this last T
it's like blocks
it's like block size by block size
so it's like we'll say
T1 and T2
each of these being like the block size
we're just going to do it to this last T2 here
and this horizontal is T2
so
hopefully that makes sense
and essentially
what this exponentiation is going to do
is it's going to turn these values to 0
and
this one is obviously going to remain a 1
and then
it's going to turn these
into 0
and it's going to probably sharpen this 1 here
so this 1 is going to be more significant
it's going to grow more than the 0.6
because we're exponentiating
and then same here so this 1 is going to be
very, very sharp
compared to 0.6
or 0.4
that's what the softmax does
essentially the point of the softmax function
is to
make the values stand out more
it's to make the model more confident
in highlighting attention scores
so when you have one value that's like very big
but not too big, not exploding
because of our scaling, right?
we want to keep a minor scaling
but when a value is big, when a score
or attention score is very big
we want the model to put a lot of focus on that
and to say this
the entire sentence or the entire thing of tokens
and we just want it to learn the most
from that
so essentially that's what softmax is doing
instead of just a normal
normalizing mechanism
it's just doing some exponentiation
to that to make the model more confident
in its predictions
so this will help us score better
in the long run if we just
highlight what tokens
and what attention scores are more important
in the sequence
and then after this
softmax here
we just apply a simple dropout
on this way variable
this new
calculated way
scale.product.attention
masked
and then softmaxed
we apply a dropout on that
and then we perform our final
weighted aggregation
so this v
multiplied by the output of the softmax
cool
so we get this v
self.value of x so we just multiply that
a little pointer
I wanted to add
to this
module list
module list here
and then our
go
yes our sequential network here
so we have this
sequential number of blocks here
for n layers
and we have our module
list so what really is the difference here
well
module list is not the same as n and not sequential
in terms of the
asterisk usage that we see
in the language model class
module list doesn't run one layer
or head after another
but rather each is
isolated and gets its own unique perspective
sequential processing
is where one block depends on another
to synchronously complete
so that means we're waiting on one
to finish before we move on to the next
so they're not completing asynchronously
or in parallel
so the multiple heads in a transformer model
operate independently
and their computations can be processed
in parallel however this parallel
parallelism isn't due
to the module list that stores
the heads
instead
it's because of how
the computation are structured
to take advantage of the GPU's capabilities
for simultaneous
computation
and this is also how the deep learning
framework PyTorch
interfaces with the GPU
so this isn't particularly something we have to worry
about too much but
you could supposedly think
that these are sort of running in parallel
yeah
so if you want to get into hardware
then that's like your whole realm there
but this is PyTorch, this is software
not hardware at all
I don't expect you have to have any hardware
knowledge about GPU, CPU
anything like that
anyways that's just kind of a background
of what's going on there
so cool
so let's actually go over what is going on
from the ground up here
so we have this
GPT language model
we got our token embeddings, positional embeddings
we have these sequential blocks
initialize our weights
for each of these blocks
we have a
this class block
so we get a head size parameter
which is n embedded of 384
divided by n heads which is 4
so we get 96 from that
that's the number of features we're capturing
self-attention
we do a feed forward to layer norms
self-attention, layer norm
feed forward
layer norm
in the post norm architecture
then we do a feed forward
just a linear
followed by a relu followed by a linear
and then dropping that out
and then we have our multi-head attention
which just sort of structured
these attention heads
running in parallel and then concatenates the results
and then for each of these heads
we have our keys, queries
and values
we register a model state
to prevent overhead computation
excessively
then we just
do our scale dot product attention
in this line, we do our mast field
to prevent look ahead
we do our softmax to make our values
sharper and to make some of them stand out
and then
we do a drop out finally
on that and just some weighted aggregation
we do our weights
this final
weight variable
multiplied by our
weighted value
from this
initially this linear transformation
so cool, that's what's happening
step by step
in this GBT architecture
amazing
give yourself a good pat on the back
go grab some coffee, do whatever you need to do
even get some sleep
and get ready for the next section
so there's actually another hyper parameter
I forgot to add
which is n layer
and n layer
is essentially equal to 4
n layer is essentially equal to
the number
of decoder blocks
we have
so instead of n block we just say n layers
doesn't really matter what it's called
but that's what it means
and then number of heads is how many heads
we have running theoretically in parallel
and then n embed
is the number of total
dimensions we want to capture
from all the heads concatenated together
type of thing, we already went over that, so cool
hyper parameters
block size, sequence length
batch sizes, how many
of these do we want at the same time
max itters is just training
how many iterations we want to do
learning rate is
what we covered that in
actually the Desmos calculator that I showed
a little while back
just showing how
we update the model weights based on the derivative
of the loss function
and then
validators
which was just reporting the loss
and then lastly the
dropout which is dropping out
0.2 or 20%
of the total neurons
so awesome
that's pretty cool, let's go ahead and jump into
some data stuff
I'm going to pull out a paper here
so let's just make sure everything works here
and then we're actually going to download our data
so I want to try to run some iterations
and
just make sure that our, actually I made some changes
pretty much this was
weird and didn't work so I just changed
this around to
making our characters empty
opening this text file
opening it
storing it in a variable
format
and then just making our vocab
this sorted list
set of our text
and then just making the vocab
size the length of that
so let's go ahead and actually run
this through, I did change the block size
to 64 batch size 128
some other hype parameters here
so
honestly the block size and batch size will depend
on your
computational resources
so
just experiment with these
I'm just going to try these out first
just to show you guys what this looks like
okay
so it looks like we're getting idx is not defined
where could that be
okay yep
so this is
we could just change that
it's just saying idx is not defined
we're using index here idx there so that should work now
and we're getting local variable
t reference before assignment
okay so
we have some
we have t here and then we initialize
t there so let's just bring up
up to there cool
now let's try and run this
oh shape is invalid
for input size of
okay let's see what we got
it turns out we don't actually need
two token embedding tables a little bit of a selling
mistake but we don't need two of those
so I'll just delete that
and then
what I'm going to do is go ahead and run this
again let's see a new error
local variable t reference before assignment
okay so our
t is referenced here
and well how can we initialize this
what we can do is we could take this index
here of shape
b by t because it goes b by t
plus 1 etc and just keeps growing
so we could actually unpack that
so we could go b
b and
t is going to be index
that shape just unpack that
so cool
so now we're going to run this training
loop and
it looks like it's working so far
so that's amazing
super cool
step 0 train last
4.4 that's actually a pretty good training
loss overall so
we'll come back after this is done
I've set it to train
for
3,000 iterations printing every 500 iterations
so we'll just see
the loss six times over this entire
training process
or we should
I don't know why it's going to 100
eval itters
eval itters
estimate loss is
okay so we don't actually need
eval interval
we'll just make this
sure why not 100
we'll keep that
and it's just going to keep going here
we'll see our loss over time
it's going to get smaller so
I'll come back when that's done
as for the data we're going to be using
the open web text corpus
and
let's just go down here
so this is a paper called
survey
survey of large language models
so I'll just go back to open
web
text
where that is
up
it's just fine
so open web text
this is consisted of a bunch of reddit links
or just reddit upvotes
so if you go and reddit and you see
a bunch of those
posts that are highly upvoted
or downvoted
they're pretty much those
pieces of text are valuable
and they contain things that we can train them
so
pretty much web text is
just a corpus of all these upvoted links
but it's not publicly available
so somebody created an open source version
called
open web text
hence open
and it's pretty much as an open version of this
so we're going to download that
for a here like common crawl which is
really really big so like
petabyte scale data volume
you have a bunch of books
so this is a good paper to read over
it's just called
a survey
of large language models you can search this up
and it'll come up you can just download the pdf for
so this is a really nice paper
read over that if you'd like
but anyways
this is a download link for this open web text corpus
so just go to this link
I have it in the github repo
and you just go to download
and it'll bring you to this drive
so you can go in and right click this
and just hit download
it'll say 12 gigabytes exceeds
maximum file size that it can scan so it's like
this might have a virus
don't worry it doesn't have a virus this is actually
created by a researcher so
not really bad people are in charge of
creating text corpora
so go in and download anyway
I've actually already downloaded this
so
yeah I'll come back
when our training
is actually done here
so I'm actually going to stop here iteration
2000 because
we're not actually getting that much amazing progress
and the reason for this is because
our hyper parameters
so batch size and block size
I mean these are okay
but we might want to change up as our learning rate
so some combinations of learning rates that are really useful
is like
3e to the negative 3
you go 3e to the
negative 4
you go 1e to the negative 3
1e
1e to the negative 4
so these are all learning rates that I like to play around with
these are just sort of common ones
it's up to you if you want to use them or not but
what I might do actually
is just downgrade to 3e to the negative 4
and we'll retest it
as well I'm going to bump up the
the number of heads
and the number of layers
so that we can capture more
complex relationships in the text
thus having it learn more
so I'm going to change each of these
to 8
go 8
actually
kernel will go
restart
now we'll just run this from the top
and
and we'll run that
cool
so let's see
what we actually start off with and what our loss looks like over time
cool
so we got step 1 4.5 about the same as last time
it's like 0.2 off
or something so it's pretty close
let's see the next iteration here
that's wonderful
so before we were getting like 3.1 ish
or something around that range 3.15
now we're getting 2.2
so you can see that
as we change hyper parameters
we can actually see a significant change
in our loss
this is amazing
this is just to sort of prove how cool hyper
parameters are and what they do for you
so
let's start
changing around some data stuff
this right here is the Wizard of Oz text
just a simple text file
it's the size isn't
super large
so we can actually open it all into ram at once
but
if we were to use the open web text
we cannot actually read
you know 45 gigabytes of
utfa text in ram at once
just can't do that unless you have like maybe
64 or 128 gigabytes
of ram this is really just
not feasible at all
so
we're going to do some data pre-processing
here some data cleaning
and then just a way to simply load
data into the
GPT so let's go ahead and do that
so the model has actually gotten really good at predicting
the next token as you can see
the train loss here is 1.01
so let's actually
find
what the prediction accuracy of that is
so I might just go into GPT-4
here
and
just ask it
what is
the prediction
accuracy
of
loss 1.01
the loss value
comes with a loss function during the pre-process
okay so let's
see
cross entropy loss
doesn't mean the model is 99% accurate
okay
so
that pretty much means that the model is really accurate
but I want to find a value here
so
if the
we'll go to Wolfram alpha
and just we'll just guess some values here
so negative ln
of let's say
0.9
okay so probably not that
0.3
0.2
0.4
0.35
yep so the model
has about a 35% chance
of guessing the next token as of right now
so that's actually pretty good
so 1 in every 3 tokens
are spot on
so that is wonderful
this is converging even more
we're getting 0.89 so now it's getting like
every
40% are being guessed properly
our validation is not doing
amazing though
but we'll linger on that a little bit here
and you'll see sort of how this changes
as we scale our data
but
so I've installed this
webtext.tar file
tar file is interesting
so in order to actually extract these
you simply just
right click on them
you go extract to
and then it'll just make a new file here
so it'll process this
you have to make sure you have WinRAR or else this might not work
to the fullest extent
and yeah
so we'll just wait for this to finish up here
we should end up with something that looks like this
so open webtext
and inside of here
you have a bunch of xz files
cool so there's actually 20,000
of these so we're gonna have to do a lot of
there's definitely gonna be some for loops in here for sure
so
let's just handle this
step by step in this data
extract file
so first off
we're gonna need to import some python modules
we're gonna use OS for interacting with the operating system
LZMA
for handling
xz files which are a type of compressed file
like 7zip for example
and then
TQDM for displaying a progress bar
so you see a progress bar left to right
in the terminal
and that's pretty much gonna show us how quick we are
at executing the script
so next up
we're gonna define a function
called xz files in dir
it takes a directory as an input
returns a list of all of the xz file names
in that directory
it's gonna use os.listdir
to get all the file names
and os
path as file
to check if each one is a file
and not a directory or
symbolic link
if a file name ends with .xz
and it's a file
it'll be added to the list
so we just have a bunch of these files
each element
is just the title of each file in there
so that's pretty much what that does
and next up here
we'll set up some variables
folder path
it's just gonna be where our xz files are located
so I'm actually gonna change this here
because that's an incorrect file path
but
yes
just like that
you have to make sure that these
slashes are actually forward slashes
or else you might get bytecode errors
so when it actually tries to read the string
it doesn't think that
these are separated
the backward slashes do weird things
so you could either do
a one forward slash
or two backward slashes
that should work
just make sure you get forward slashes
and you should be good
so folder path is where all these files are located
all these xz files are located as you saw
output file
is the pattern for output file names
in case we want to have more than one of them
so if you want to have 200 output files
instead of one then it'll just be like
output 0, output 1, output 2 etc
and then vocab file is where we want to save
our vocabulary
keep in mind in this giant corpus
you can't push it on to ram at once
so what we're gonna do is as we're
reading these little compressed files
20,000 of them
we're gonna take all of the new characters
from them and just push them into some vocab file
containing all of the different
characters that we have
so that way we can handle this later
and just pretty much sort it into some
list containing all of our vocabulary
split files
how many files do we want to split this into
so pretty much this
it ties back to output file
and just these curly braces here
how many do we want to have
if we want to have more than one then we would
this would take effect
so cool
now we'll use our
x files in dir
to get a list of file names and store them in this variable
we'll count
the number of
total xd files
simply the length of our file names
now in here
we'll calculate the number of files
to process for each output file
if the user is requested
more than one output file
request more than one output file
this is the total number of
files divided by the number
output files rounded down
so
if the user only wants one
output file max count is the same as total files
and
that's how that works
so
next up we'll just create a
set to store a vocabulary when we
start appending these new characters into it
a set is a
collection of unique items in case you did not know
entirely what a set was
now
this is where it gets interesting
we're ready to process our
.xz files
for each output file we'll process
max count files
for each file we'll open it
read its contents
and write the contents to the current output file
and then add any unique characters to our vocabulary
set
after processing max count files
remove them from our list of files
and then finally
we'll write all of our vocabulary to this file
so
we pretty much just open
we just
write all of these characters in the vocab
to this
vocab file which is here vocab.txt
so awesome
now
honestly we could just go ahead
and run this
so let's go ahead and go in here
I'm going to go cls to clear that
we'll go python
data extract
.py
let's see this work it's magic
how many files would you like to split this into
we'll go one
then we get a progress bar
20,000 files and we'll just let that load
I'll come back to you in
about 30 minutes to check up on this
okay so there's not a little
one thing we want to consider for
and
it's actually quite important is our splits
for train and file splits
it would be really inefficient
to just get blocks and then creating
train and file splits as we go
every new batch we get
so in turn
what we might be better off doing
is just creating an
output train file and an output file file
so just two of them instead of one
train is 90% of our data
file is 10% of our data
if that makes sense
so pretty much what I did
is I got the output line for how many
files do you want
so you can see I got quite a bit of files
produced here
by not doing that correctly
so don't do that
and
yeah
essentially we're just
we're pretty much just doing that
so we're processing some training files
we're separating 90%
of the names on the left side
and then 10% of the names
we're just separating those in the two different
arrays, file names
and then we're just processing each of those
arrays based on the file names
so I took away that little bit
that was asking
how many files per
split do you want
so I took that away
and this is effectively the same code
just a little bit of tweaks
and yeah
so I'm going to go ahead and run this
data extract
cool
so we got an output train
and then after this it's going to do
the output validation set
so I'll come back after this is done
so awesome I have just downloaded
both or I've both
got both these splits
output train and val train so just to
confirm that they're
actually the right size got 38.9
and then 4.27
so if we do this divided by
9 so about 30
8.9 divided by 9
4.32 and it's
pretty close to 4.27 so
we can confirm that these are pretty much
the
length that we expect them to be
so awesome we have this vocab.txt
file wonderful so now
we have to focus on is
getting this into
our batches so when we call
our get batch function actually
cd out of this open this in
a Jupyter notebook
copy my desktop
paste it over here
and perfect
so
this open when web text folder with
these files awesome
and our GPTV
one
so
this get batch function
is going to have to
change also these
are going to have to change as well
and this one too these are probably
not going to be here
but pretty much
let's go ahead and first of all
get this vocab.txt
in so what I'm going to do
I'm just going to go
we're going to go
open web text slash
vocab.txt
cool so that's our vocab
right there text read
vocab size the length of that nice
so that's what our vocab is
and then
what we're going to do next
is change this get batch function
around
so first of all I'm going to go ahead
and get rid of this here
and then
I've actually produced
some code specifically for
this so I'm just going to go back
to my
I'm just going to find
this folder
okay so I've
actually produced some
code here
I produced this off camera
but
pretty much what this is going to do
it's going to let us call a split
okay so we have our get batch
function all of this down here is the
same as our GPTV
one file and then
this data is
just going to get a random chunk of text
with giant block of text
and the way
that we get it is actually pretty interesting
so the way that we get this text is
something called memory mapping
so memory mapping is a way
to look at disk files
or to open them and look at pieces of them
without opening the entire thing at once
so memory mapping
I'm not a hardware guy so I can't
really talk about that
memory mapping is pretty
cool and allows us to look at little
chunks at a time in very large text files
so that's essentially what we're doing here
we're passing this split
split
file name is equal to train split
this is just an example text file
if the split is equal to train then this
is our file name else
file split and then we're going to
open this file name in binary mode
this has to be in binary mode
it's also a lot more efficient in binary
mode and then
we're going to open this with a mem map
so I don't expect you to memorize all the mem map syntax
you can look at the docs if you would like
but I'm just going to explain
logically what's happening
so we're going to open this
with the mem map library
and we're going to open this as
mm so
the file size is literally
just the length of it so determining
the file size and
all we're doing from this point is we're just finding
a position so we're using the random library
and we're finding
a position between
0
and the file size
minus block size times batch size
so pretty much we have this
giant text
file we could either
what we want to do is we want to start
from 0 and go up to like
just before the end because if we
actually sample
that last piece then it's still
going to have some wiggle room to
reach further into the file
if we just made it from like
the first
the very start of the file to the very end
then it would want to do
is it would want to look past the end
because it would want to look at more tokens from that
and then we would just get errors
because you can't read more than
the file size if that makes sense
so that's why I'm just making this little threshold here
and
yeah so that's what that does
that's the starting position could be a random
number between the start and
a little bit a little margin from the end
here so
next up we have
this seek function so seek is going to
go to the start position and then
block is going to
read we're going to
go up to the start position it's going to
seek up to there that's where it's going to start
it's going to go up to it and then the read
function is going to
find a block of text that is
block size times batch size so it's
going to find a little snippet
of text in there at the starting
position and it's going to be of size
it's going to have this the same amount of
I guess bytes as
block size time times batch size
then all that minus one
just so that it fits into this start position
we don't get errors here that's why I put the minus one
but
yeah so we'll get a pretty
we'll get a pretty decent
text amount I guess you could say
it's going to be enough to work with you could
you could of course increases if you
wanted to you could do like
times eight if you wanted
times eight and then times eight up here but
we're not going to do that
based on my experience this is performed pretty well
so we're going to stick with this method here
and then
we just decode this
bit of text the reason we decode it is it's
it's because it's
we read it in binary form
so once we have this block of
text we actually have to decode this to
UFA format or UTF
format and then any like
bytecode errors we get we're just going to ignore
that this is something you learn
through practice is when you start dealing
with like really weird data or if it has
like corruptions in it you'll get errors
so all you want to do is all
this does is it pretty much says
okay we're just going to ignore this
bit of text and we're just going to sample
everything around it and not include that
part and plus since we're doing so many
iterations it won't actually interfere
that much so we should
be all right and then for this replace
go function here I was noticing
I got errors about this slash R
so all this does is it just replaces that
with an empty string and then finally
we have all this
we have all this decoded data
so all we're going to do is just encode
this into the
tokenized form so it's all in
it's all in the tokenized form
integers or torch.longs
data type
and we just that's what our data is
instead of a bunch of characters it's just a bunch
of numbers and then we
return that into our get batch
and this is what our data is
so that's pretty cool
we can get either train or a valve
split and
that's sort of what it looks like in practice
that's how we sample from
very large text files at a smaller
scale bit by bit so
let's go ahead and implement this here
and go grab this entire
thing
and pop over to here
we're just going to replace that
so
get random chunk, get batch
cool
so now we can actually go ahead and
perhaps run this
actually before we run this there's a little something we need to
add in here
so I have this
train split.txt and a valve split.txt
so I actually need to
change these
so let's go rename we'll go
train split.txt
and then
a valve split.txt
cool
and then we could just go
open web text
forward slash
and then same thing for here
cool let's go ahead
and run this now
and we're getting errors
mem map is not defined
so that's another thing we need to probably
add in then
so I'm actually just going to
stop this process from running here
we're going to go pip
install
mem map
mem map is not defined
we don't actually need to install this
by default comes with the operating system
so
what we actually need to do
is
just close this
gptv1
awesome
everything is good
nothing is broken
so what I actually need to do up here
is import this
so I need to go
import mem map
just like that
and
should be good to start running this script
name random is not defined
again another importation we have to make
import
import
random
and we should start seeing some
progress going here so once we see the first iteration
I'm going to stop it come back
at the last iteration and
then we'll start adding some little bits and pieces
onto our script here to make it better
so we're already about 600 iterations
in and you can see how the training loss
is actually done really well so far
it's gone from 10.5 drop all the way to
2.38
and
we can actually see that
we might be able to actually get a
val loss that is lower than the
train because keep in mind
in train mode
the dropout takes effect but in val
in eval mode
let me just scroll up to this here
yes
so model about eval what this does
is it turns off the dropout
so
we don't lose any of the neurons
and they're all sort of showing the same
features and giving all the information that they're supposed
to because they're all active but in train mode
20% of them are off so
once you actually see
in eval mode it does better
that means
that the network has started to
form a sense
of completeness in its learning
so it's just adjusting things a little bit
once it hits that point
and we might see this happen
momentarily but this is
really good progress so far a loss of
1.8 is amazing
so
in the meantime
I'm just going to add some little tweaks
here and there to improve this script
so I've actually stopped the iteration process
but we've gotten to 700 steps and we can already
see that val loss
is becoming a less than train loss
which is showing that the model is actually converging
and doing very well
so this architecture is amazing
we've pretty much covered
every
architectural, math, pie torch part
that this script has to offer
the only thing I want to add
actually a few things I want to add
one of them being torch.load
and torch.save
so one thing that's going to be really important
when you start to scale up
your iterations
is you don't just want to run a script
that executes a training loop
with an architecture and
that's it. You won't have some way to
store those learning parameters
so that's what torch.load and torch.save does
save some file
right and
you can pretty much
you could put it into like a serialized
format when you
save it you take your initial
architecture in our case it would actually
be the GPT language model so you would
save this because it contains
everything all these other classes
as well they're all inside of GPT
language model would save that architecture
and you essentially
serialize it into some pickled file
that would have
the file extension .pkl
so
essentially
instead of using torch we're just going to use
a library called pickle because
they're essentially the same thing
pickle is a little bit easier
to use or at least a little bit easier to understand
there's less to it
pickle will only work
on one GPU
so if you have like 8 GPUs at the same time
you're going to want to learn a little bit more
about hardware stuff and
some PyTorch docs but
pretty much
if we want to
save this after training
what we're going to do is we're going to use
a little library called pickle and this comes
pre-installed with windows
import pickle
okay so what we want to do is
implement this after the training loop
after all these parameters have been updated
and learned to the fullest extent
so after this training loop
we're simply going to open
we're going to do with open
and we could just go
model 01 like that
and then
just that .pkl is the file extension
for it
and then since we're writing to it we're going to go
write binary
F
and then in order to actually save this
we just go pickle.dump
and then we can use
model and then
just F like that
so
if I start recording this
it's going to make
if I start recording this training process
it's going to make my clip
like so
I'm going to come back to this after we've done
let's just say about
100 iterations
we're going to do 100 editors
and I'm going to come back and
show you guys
what the model file looks like
what I actually did is I changed some of the model
hyper parameters because
it was taking way too long
to perform what we wanted it to so I changed
and head to one and layer to one
and I had half batch size
all the way down from 64 to 32
so what I'm actually going to add here is just
to make sure I like to print this out at the beginning
of this
make sure that the device is CUDA
let's go back down
so it did in fact train the model
so we got all this done
and yeah
so I don't know why I did 2.54
whatever that
that was just the entire loss
so
model saved awesome
what does this actually look like here
so this model.pkl
106 megabytes isn't that wonderful
so this is our model file this is what they look like
it's just a serialized
pretty much the entire architecture
all the parameters of the model the state
everything that it contains
and we just compress that
into a little pkl file take that out
decompress it and then just use it again
with all those same parameters so
awesome
and all this really took was
we just open
as this
we do a pickle.dump
to make sure that actually save I just like to add
a little print statement there cool
so next
what I'd like to add is a little
wait for us to
instead of just doing all of our training at once
and then saving the model being able to
train multiple times
so I'm gonna go up here
to our
GPT language model here
and
let's just see
what I'm gonna do
with open
and we're gonna go
model 01
pkl
and we're gonna go read binary
so actually gonna read it we're gonna
load this into
our script here
so
we're gonna go as f
and then
I believe it's pickle.load
you just go yeah
model equals
pickle.load and then we'll just
essentially dump that
right in there
go print
loading
model
parameters
dot dot dot
and then
just put f in there
and then once it is loaded
we'll do print
loaded
successfully
cool
so I'm actually gonna try this out now
go
do that
boom
and boom
okay
so
loading model parameters loaded successfully
and we'll actually see this
start to work on its own now
so
is it going to begin or is it not going to begin
let's run that
okay perfect
so now we should take the loss
that we had before which was about
2.54 I believe
something around those, something along those lines
you can see that our training process
is greatly accelerated
so we had 100
now it's just gonna do an estimate loss
cool
and we're almost done
1.96 awesome
and the model saved
so essentially what we can do with this
is we can now
save models
and then we can load them and then iterate further
so if you wanted to
you could create a super cool
GPT language model
script here and
you could essentially give it like 10,000 or 20,000
iterations to run overnight
you'd be able to save it
and then import that into say a chat bot
if you want
so that's pretty cool and that's just kind of
a good thing
good little, it's kind of
essential for language modeling because
what's the point
in having a machine learning model if you can't
actually use it and deploy it
so you need to save for this stuff to work
alright
now let's move on to
a little something in this task manager
here which I'd like to go over
so this shared GPU memory here
and this dedicated GPU memory
so dedicated
means how much
VRAM, video RAM
does your GPU actually have
on the card
so on the card it's going to be very quick memory
because it doesn't have to
the electrons don't have to travel as quickly
that's kind of the logic of it
the electrons don't have to travel
they don't have to travel as far
because
the little RAM chip is right there
so
dedicated GPU memory is a lot faster
shared GPU memory
is essentially if this gets overloaded
it'll use some of the RAM on your
computer instead
so this will typically be about half of your
computer's RAM
I have 32 gigabytes of RAM on my computer
so 16.0 makes sense
half 32
and yeah
so you want to make sure you're only using dedicated
GPU memory
having your shared GPU memory go up
is not usually a good thing
a little bit is fine
but
dedicated GPU memory is the fastest
and you want everything to stick on there
just try to make sure all of your parameters
sort of fit around this
whatever your max capacity is
maybe it's 4, maybe it's 8
maybe it's 48
who knows
and a good way to figure out
what you can use on your GPU
without it getting memory errors
or using shared memory
is to actually play around
with
these parameters up here
so
block size and batch size
actually let me switch those around
these are not supposed to be in that order
but
all good
we'll make our batch size
64
that's 128
okay
so
batch size and block size
are very big contributors to how much memory you're going to use
learning rate is not
max iterations is not
evaluators is not
but these three will
the amount of features that you store
the amount of heads you have running in parallel
and then also
layers so
some of these will not
affect you as much because they're more
sort of restrained to computation
how quickly you can do operations if something is sequential
so N layer won't strain you
as much as something like batch and block size
but
those are just good little things to
sort of tweak and play around with
so I found the optimal
sort of set of
hyper parameters for my PC
that happens to be
8, 8, 3, 8, 4
learning rates is the same
and then 64, 128 for this
so that happened to be the optimal
hyper parameters for my computer
it'll probably be different for yours
if you don't have 8 gigabytes of RAM on your GPU
so anyways
that's a little something you have to pay attention to
to make sure you don't run out of errors
and a technique you can use
which I'm not actually going to show you in this course
but it's quite useful is something called auto tuning
and what auto tuning does
is it pretty much runs
a bunch of these
a bunch of models with different
sets of hyper parameters
so to run like batch size 64
batch size 32, batch size 16
batch size maybe 256
we'll be like okay which ones are throwing errors and which ones aren't
so what it'll do
if you properly
if you properly set up an auto tuning script
is
you will be able to find
the most optimal
set of parameters for your computer
most optimal set of hyper parameters
that is possible
so auto tuning is cool
you can definitely look more into that
there's tons of research on it
and yeah so
auto tuning is cool let's dig into the next part
the next little trick we use in practice
especially by machine learning engineers
it's a little something called arguments
so you pass an argument into
not necessarily a function but into the command line
so this is what it'll look like
this is just a basic example
of what arg parsing will look like
so just go
python, arg parsing
because that's a script's name
I go dash
llms because that's what it says
right here this is what the argument is
and then we can just pass in a string
say hello
the provided
whatever is hello
cool you can add little arguments to this
and I'm even going to change this around
I could say
batch size
and then
let's go like that
batch
batch size
please
provide
a batch size
I can do the same thing again
and see it says
following arguments required are batch size
so that obviously didn't work
and if we actually tried the correct way
our parsing.py then we go
dash, batch size
we can make it 32
oops
that's because it's not a string
so
what we need to actually do
is it's bs somewhere
okay
so
args
parse args
so we need to change this
to bs like that
let me go batch size
batch size is 32
okay
so even I'm a little bit new to arguments as well
but
this is something that comes in very handy
when you're trying to know each time
you're trying to change some parameters
if you add
new gpu or whatever and you're like oh I want to double my batch size
it's like sure you can easily do that
so a lot of the times
it won't just have one but you'll have like
many meaning like maybe a dozen
or so of these
of these little arguments
so that is what this looks like
and
we're going to go ahead and implement this
into our little script here
so
I'm just going to
pop over to gpt1
I'm going to pull this up on my
second monitor here
and
in terms of these
I'm just going to start off
making a
importation
arg
arg parser
or arg parse rather
that's what it's called
and then we go
parser is equal to
I'll just
copy and paste this entire thing
and why not
cool
okay
so
we get a batch size
or something
and then
we'll add in the second part here
so
args parse the arguments
here
and the little scope
of
batch size like that
our batch size is equal to
whatever that was
and we'll just go args
dot
args dot batch size so cool
we're going to run this
and
not defined
so I got a little not defined thing here
and pretty much
all I missed was that
we're doing this so essentially
this
should be equal to this right here
so I'm just going to go ahead and copy that
and
boot parse args
except
we don't have a parse args function
so
what do we need to do instead
well it
actually that might just work on it so let's try it out
okay so it looks like
it's actually expecting some input here
in code so
that's probably working
and if we
ported this into a script
then it would simply ask us for some input
so I believe we're doing this correctly
let's go ahead
and actually switch over
and pour all of this into some code
so I'm going to make
a training file
and a chat file
the training file is going to be all of our parameters
whatever all of our architecture
and then the actual training loop itself
we're going to have some arguments in there
and then the chat bot is going to be
pretty much just a question-answer
thing that just reproduces text
so it'll just be like prompt, completion
type of thing and
yeah so let's go ahead and implement that here
so in our
GPT course
here I'm going to go
training.py
and we're going to go
chatbot.py
just like that
so in training
let's go ahead and drag everything in here
I'm just going to
move this over to the second screen
and just copy and paste
everything in order here
so next up we have our
characters
and then we have our
tokenizer
and then our
getRandomChunk and getBatches
suite
our estimateLoss function
and then this giant piece
of code
containing
most of the architecture we built up
we're just going to add that in there
we're not getting any warnings
and then the training loop
and the optimizer
awesome
then after this
we would simply have this context
but the point of this is that we want to have this in our
chatbot script
so what I'm going to do
is in this training.py
I'm going to keep
all of these the same I'm going to keep this entire thing
the same
get rid of this little block of code
and we're going to go into
the chatbot
here so loadingMile
parameters good we want to load some in
train some more and then dump it
chatbot is not going to dump anything
it's just going to save so I'm going to take
all of our training here
and instead of dumping
take that away we'll also take
away the training
loop as well
I don't believe we have anything
else to actually bring in
we don't need our getBatch
we do not need our getRandomChunks
so awesome
we're just importing these parameters
by default like that
awesome
so from this point
we have imported
we've imported our model
cool so let's go ahead
and port in our little
chatbot here
this little end piece
which is going to allow us to
essentially chat with the model
this is what it looks like a little wild loop
we have a prompt we just input
something
prompt next line that should be fairly self explanatory
and we have this tensor
we're going to encode this prompt into a bunch
of integers or torch.long data types
on the GPU
devices CUDA
and then after
after we've actually generated these
so model.generate
we're going to unsqueeze these
remember it's a torch.tensor
so it's going to be in the matrices form
so it's going to look like this
it's going to look like this or whatever
that's essentially what the shape is
so all we're doing when we unsqueeze it
is we're just taking away this wrapping
around it
so awesome
we're just going to do some
tokens for example 150 here
and then to a list format
and then we can just print these out
as January characters
awesome so we're just going to ask this prompt
and then do some compute give us a completion
so on and so forth
so that's what this is doing here
and another thing I wanted to point out
is actually when we load these
parameters in
at least on training
it's going to initially give us errors
from we're going to get errors from that
because the model will just not be
anything and we won't be able to import stuff
so that's going to give you errors first of all
another thing you want to pay attention to
is to make sure that when you've actually trained
this initial model that it matches
all of the architectural
stuff and the hyper parameters
that you used
that when you're using to load up again
so
when you're running your forward pass and whatnot
you just want to make sure that this architecture
sort of lines up with it
just so that you don't get any architectural errors
those can be really confusing to debug
so yeah
and the way we can do this is actually just
commenting it out here
awesome, we're able to save load models
and
we're able to use a little loop
to create a sort of
chat-up that's not really helpful
because we haven't trained it
an insane amount on
data that actually is useful
so another little detail that's very important
is to actually
make sure that you have nn-module in all
of these classes and subclasses
nn.module basically works
as a tracker for all of your
parameters it makes
make sure that all of your
nn extensions run correctly
and just overall a cornerstone
for PyTorch like you need it
so make sure you have nn-module in all of these classes
I know that
block sort of comes out of GPT
language model and so on and so forth
but just all of these
classes with nn
or any learnable parameters
you will need it in it's overall just
a good practice to have nn-module in all
of your classes overall
just to sort of avoid those errors
so cool
I didn't explicitly go over that
at the beginning but that's just a heads up
you always want to make sure nn-module is inside of these
so cool
now
something I'd like to highlight
is a little error that we get
we try to generate when we have max new
tokens above block size so let me show you
that right now
you just go python, chat bot
and then batch size 32
so we could say
we could say hello
for example
okay so it's going to give us
some errors here and what exactly
does this error mean
well when we try to
generate 150 new
tokens what it's doing
is it's taking the previous
you know
H-E-L-L-O
exclamation mark 6 tokens
and it's pretty much adding up 150
on top of that so we have
156 tokens
that we're now trying to fit inside of block size
which in our case is
128
so of course
156 does not fit
into 128 and that's
why we get some errors here
so
all we have to do is make sure
that
we essentially
what we could do is make sure that max new tokens
is small enough and then be sort of
paying attention when we make prompts
or
we could actually make a little
cropping
cropping tool here so what this will do
is it will pretty much crop
through the last block size tokens
and
this is super useful because it
pretty much doesn't make us have to pay
attention to max new tokens all the time
and it just essentially
crops it around that 128 limit
so
I'm going to go ahead and replace index here
with index con or index condition
and
we go ahead and run this again
so I could say hello
and we get a successful
completion awesome
we can keep asking new prompts like this
right
and awesome so
yeah we're not really getting any of these
dimensionality like
architecture fitting type errors if you want to call them
if you want to make it super fancy that way
but yeah
not really that much else to do
yeah there's a few points I want to go over
including fine tuning
so I'm going to go over a little
illustrative example as to what
fine tuning actually looks like in practice
so in pre-training
which is what this course is based off of
in pre-training you have this
giant text corpus right you have this
giant corpus here
some text in it
and essentially
what you do is you take out little snippets
these are called
blocks or batches
or chunks you could say you take out little batches
of these you sample
random little blocks and you take multiple batches
of them and
you essentially have this
let's just say
H E L L O
and maybe the next
predict maybe the outputs
or the targets rather
or
the L L O
exclamation mark
so it's just shifted over by one
and so given this
sequence of characters
you want to predict this which is just
the input shifted by one
that's what pre-training is
and keep in mind that these are the same size
this is one, two,
three, four, and five
same thing here these are both
five characters long
fine tuning however is not completely the same
so I could have
hello
and then maybe like a question mark
and it would respond
you know
the model might respond
L R U
maybe that's just a
a response that it gives us
we can obviously see that hello does not have the same amount of characters
with the same amount of indices
as how are you
so
this is essentially the difference between
fine tuning and pre-training
with fine tuning you just have to add a little bit of
different things in your generate function
to compensate for not having
the same
amount of indices in your inputs
and targets and rather just
generate until you receive an end token
so
what they don't explicitly say here is at the
end of this question
there's actually a little end token which we usually
do
looks like this
like that
or
like this
these are end tokens and then you typically
have the same for start tokens like an s
or
start
like that, pretty simple
and essentially you would just append them
and
a start token
the start token doesn't matter as much
as we essentially just are looking at
what this does and then
we start generating the start doesn't really
matter because
we don't really need to know when to start generating
it just happens but the end token is
important because we don't want to just generate
an infinite number of tokens
because these aren't the same size
it could theoretically generate a really
really long completion
so all we want to make sure
is that it's not generating an infinite amount of tokens
consuming an infinite amount of computation
and just to prevent that loop
so that's why we append this end token
to the end here
we have this little end bit
and
essentially once this end token is sampled
you would end the generation
simple as that
and we don't actually
sample from the token itself
but rather the actual
the
I guess you could say index
or the miracle value
the encoded version of end
which
is usually just going to be the length of your vocab
size
plus one
so if your vocab size in our case
is like maybe 32,000
your end token would be at index
32,001
so that way when you sample
when you sample an end token
when you sample that
32,001 token
you actually just end the sequence
and of course when you train
your model you're always
appending this end token to the end
so you get your initial inputs
and then inside of either your
training data
or when you actually are processing it
and feeding it into that transformer
you have some sort of function that's just appending
that little
32,001 token index
to it
so that's pretty much what fine tuning is
it comes up fine tuning
and the whole process of creating
these giant language models
is to of course help people
and there's no better way to do that
than to
literally have all the information
that humans have ever known meaning like common crawl
open web text or Wikipedia
and even research papers
pre-training on all of that
so just doing again the same size
and then shift over for targets
and then after you've iterated on that
many many times you switch over to fine tuning
where you have these
specifically picked out
prompt and completion pairs
and you just train on those for a really long time
until you are satisfied
with your result
and yeah that's what language modeling is
there are a few key pointers I want to leave you with
before you head on your way to
research and development and machine learning
so first things first
there's a little something called
efficiency testing
or just finding out how quickly
certain operations takes
we'll just call this
efficiency testing and I'll show you
exactly how to do this right here
efficiency
yeah
I don't know if I spelled that correctly
I don't know what it's doing now
anyways
we'll just pop into code here
and
essentially
we'll just do
I don't know
I'm testing
import time
and
essentially
all we're going to do is just
time how long operations take
so
in here you can go
you can go start time
equals time dot time
and essentially what this function does
is it just takes a look at the current time right now
the current like millisecond
very precise
and we can do some little
operation like
I don't know 4
I in range
we'll just go
10,000
go
print
I
print I times 2
and then we can just end the time here
so go end time
equals time dot time again
calling the current time so we're doing
right now versus back then
and that little difference is how long it took to execute
so all we can do
is just do we can say total time
we can say total time equals
end time
minus start time
and we'll just go print
end time
or
I'm taking
let's go total
total time like that
just execute this
Python
time testing
cool
time taken 1.32 seconds
so you can essentially time every single operation
you do with this method
and you can see even in your
I encourage you to actually try this out
I'm not going to but I encourage you to try out
how long the model actually takes
to do certain things like how long does it take
to load a model how does it take to save a model
how long does it take to estimate the loss
right
play around with hyperparameters see how long things take
and maybe you'll figure out something new who knows
but this is a little something we use
to pretty much test how long something
takes how efficient it is
and then to also see if
it's worth investigating a new way of approaching
something in case it takes
ridiculous amount of time
so that's time testing
and efficiency testing for you
the next little bit I want to cover
is the history
I'm not going to go over the entire history
of AI and LLMs
but essentially
we originated with something called RNNs
okay RNNs are called
recurrent neural networks
and they're really inefficient
at least for scaled
AI systems so RNNs
are a little essentially think of it as a little loop
keeps learning and learning
and this is sequential right
it does this and then this and then this
has to wait for each completion
synchronous you can't have multiple of them at once
because they're complex
GPUs cannot run complex things
they're only designed for just
matrix multiplication and very simple
math like that
so RNNs are essentially
a little bit dumber than transformers
and they
are run on the CPU
so RNNs was where we last sort of stopped at
and what I encourage you to do
is look into more of the language
modeling and AI
history and research that has led up to this
point so you can have an idea
as to how researchers
have been able to quickly innovate
given
all these historical innovations
so you have like all these things leading up to the transformer
well how did they all
philosophize
up to that point
and yeah it's just
something good to sort of be confident
in is innovating
as both a researcher
and engineer and a
business person
so cool
RNNs were where we sort of
finished off and now it's transformers and GPTs
that's the current state of AI
next up I
would like to go over something called
quantization
so quantization is essentially
a way to reduce the memory
usage by your parameters
so there's actually a paper here
called QLaura Efficient Fine
Tuning of Quantized
LLMs so
all this does in simple
form is pretty much instead of
using 32 bit floating
point numbers it goes not only
to 16 bit of half precision
but all the way down to 4
so what this actually
looks like is in binary code
or in bytecode
it will look
here there's some array
of numbers
that it uses
okay I can't find it
but pretty much what it is
it is a bunch of
it's a bunch of floating point numbers
and they're all between
negative one and one
and there are 16 of them
if you have a 4 bit number
that means it can hold 16 different
values 0 through 15
which is 16 values
and all you pretty much do is you have this
array of floating point numbers
you use the bytecode of
that 4 bit
number to look up the index
in that array and that is your weight
that is the weight they use
in your model
so this way instead of using 32 bit
having these super long numbers
that are super precise
you can have super precise numbers
that are just generally good parameters
to have that just perform
decently
they're just sort of well spread out
and experimented on and they just
happen to work and you have 16 of them
instead of a lot
so that's
another cool little thing that's going on right now
is 4 bit quantizations
it's a little bit harder
to implement
I would encourage you to experiment with half precision
meaning 16 bit
floating point numbers
so that means it occupies
16 on and off switches
or capacitors on your GPU
and
so quantization is cool to
sort of scale down the memory
so that way you can scale up all of your hyper parameters
and have a more complex model
with these
yeah just essentially to have bigger models
with less space
take it up
so that is
quantization
and this is the paper for it
it's a little link you can search out if you want to get
more familiar with this see
sort of performance standards and what not
the next thing I'd like to cover
is gradient accumulation
so you might have heard of this you might not have heard of this
gradient accumulation
will
what gradient accumulation does
is it will accumulate
gradients
over say we just set a variable
x so every x iterations
it'll just accumulate those
iterations, average them
and what this allows you to do
is instead of
updating each iteration
you're updating every x iterations
so that allows you to fit
more parameters and more info
or generalization into this one piece
so that way when you
update your parameters
it's able to generalize more
over maybe a higher batch size
or a higher block size
so when you distribute this
over many
iterations and average them
you can fit more into each iteration
because it's sort of calculating
all of them combined
so yeah that's a cool little trick
you can use if
your GPU maybe isn't
as big if it doesn't have as much
VRAM on it
so gradient accumulation is wonderful
and it's used lots in practice
the final thing I'd like to leave
you guys off with is something called
hugging face and you've probably
heard a lot about this so far
but let me just guide you through
and show you how absolutely explosive
hugging face is
for machine learning so you have
a bunch of models, data sets
spaces, docs, etc
and
let's go to models for example
so let's just showcase how cool this is
you have multimodal AIs which could be
like
image and text or video
etc you have multiple different modes
so it's not just text or not just video
it's many different ones at the same time
so you have multimodal models
you have computer vision
you have natural language processing
and we're actually doing natural language
processing in this course
we have audio, a tabular
and reinforcement learning
so this is really cool
and you can actually just download these models
and host them on your own computer
that is really cool
you also have data sets which are even cooler
and these are pretty much
just really high quality data sets
of prompt and answer completions
at least for our purpose
if you want to use those
so you have
question answering
or conversational
work data set for example
as 9000 downloads
500 likes
it has a bunch of
IDs, system prompts
so you're an AI assistant or whatever
and then you have the cool stuff which is
you'll be given a definition of a task first
and some input of the task etc
and then the response it's like oh
we just gave it an input and asked it to answer
in a format and actually did that
correctly so
you could pretty much train these
on a bunch of
prompts that you would be able to feed into GPT-4
and try to make your model perform that way
and this actually has
4.23 million rows
in the training split which is amazing
so
data sets are wonderful
and you can find the best ones
at least the best fine tuning data sets on OpenORCA
really good
as for pre-training
I believe I mentioned this earlier
in this survey of large language models
that we just
put down through Reddit links
yep so you could use like OpenWebText
you could use CommonCrawl
you could use Books
you could use Wikipedia
these are all pre-training data sources
so yeah
hopefully that leaves you with a better understanding
on how to create GPTs, transformers
and
pretty good large language models from scratch
with your own data that you scraped
or that you downloaded
and yeah
that's it, thanks for watching
so you've learned a ton in this course
about language modeling
how to use data, how to create architecture
from scratch
maybe even how to look at research papers
so if you really enjoy this content
I would encourage you to maybe subscribe
and like on my YouTube channel
which is in the description
I make many videos about AI
and computer science in general
so
you could totally feel free to subscribe there
if you don't want to subscribe, that's fine
you could always unsubscribe later if you want to
it's completely free
but yeah, also have a GitHub repo in the description
for all the code that we used
not the data because it's way too big
but
all of the code and the Wizard of Oz
Text file
so that's all in the GitHub repo in the description
thanks for watching

Welcome back everybody. We're really glad to see you all here today and to those people online as well
I'm still Jillian Hadfield
And still director and chair nobody's taken me out yet of the Schwartz-Riesman Institute
Looking really forward to these sessions today
new questions and challenges and
Lots of lots of wonderful lots of wonderful sessions
Let's see is there anything else you want to make sure I say before we get started
No, okay, I think we're gonna get we're just gonna move into our first session which is which I think
Yeah, I'll stay up here to do the intro. So this first session is on the reward hypothesis which we got some mention of
yesterday and
And
Will we're interested in this I think rich will give us who is the
the source of the word hypothesis
posited 20 years ago that
Maybe rich you're gonna say this but let me for just summarize it here for that all we mean by goals and purposes can be well thought of as
maximization of the expected value of the cumulative sum of a scalar
Receive scalar signal or reward
So the question that we're gonna be discussing is then is this a good model of
Reinforcement learning is a good model for understanding human behavior and values. How far can it go?
Can it guide normative decision-making for individuals and groups?
and
Totally delighted to have with us on this on this panel
Rich Sutton who I neglected to actually
Introduce yesterday when we started our session with blaze. So my apologies for that because he is
world-class
research
Researcher and reinforcement learning as well as chief scientific advisor fellow in Canada CIFAR AI chair at Amy the Alberta machine intelligence Institute
He's a professor of computing science at the University of Alberta and a distinguished research scientist at DeepMind
He's been named a fellow of the Royal Society of Canada the Association for the Advancement of Artificial Intelligence and the Canadian Artificial Intelligence
Association where he received a lifetime
Achievement award in 2018 and as I mentioned yesterday, we're also very delighted
He's on our advisory board at Schwartz-Reisman and joining rich today is Julia Haas
Senior research scientist in the ethics research team at DeepMind
She was previously an assistant professor in philosophy and neuroscience at Rhodes College and an affiliated researcher with
and use humanizing machine intelligence grand challenge her research is in the philosophy of
Cognitive science and neuroscience. She works on the nature evaluation and its roles and theories of the mind
Your current work includes investigating the possibility of meaningful moral artificial
Intelligence and I will also mention that Julia is this is a return trip to absolutely interdisciplinary for Julia
Which we're also very grateful. It's wonderful to have people saying yes to those invitations
But our previous one of course was online, so it's wonderful to have you here in person Julia
And so rich, I think you're gonna get us started
Thanks very much
Thank you, Jillian
Good to see y'all again. Let's try to have some more fun today
My topic is
Roughly the reward hypothesis, but the reward and other related
Reductionist hypotheses because the reward hypothesis is pretty reductionists
And I do have a slide just stating all so you can all see it
All of what we mean by goals and purposes can be well thought of as the maximization
With expected value of the cumulative sum of a received a scalar signal called reward
so
That's kind of a long
sentence
Sounds like it's got lots of little bits that
that are
intricate
but
But really it just says that maybe the goals of
Whenever you want to talk about goals, maybe you can just talk about maximizing a single number and
That is pretty reductionist and
I'll be talking about that a little bit, but I don't want to start there
I want to start by going back a bit. So this is my outline. I want to talk about intelligence and
To what degree it's intrinsically tied up with the notion of a goal
Which of course is the reward hypothesis is about
And then we'll step into the reward hypothesis fully
And then I also want to talk about you know
What goes on inside because the reward hypothesis is not about what goes on inside the reward hypothesis like how you how?
What's an appropriate framing of the problem?
What's the appropriate framing of of a goal or a purpose?
And it's not about how that purpose is achieved
Okay, but that's what the agent the agent is the intelligent agent like we are the agents and
the robots are the agents and what are the essential for
What are they literally essential components, you know, that's
That's something I want to talk about that. So and then and then some other hypotheses the value function hypothesis is really
important to understand the implications of the reward hypothesis and then I'll have just really just want to slide we're
Prepare us to start to think about how this might have implications for thinking about ethics
Okay, so let's start with intelligence and
Here's some
quotes
William James was the like the original psychologist
His textbook was in 1890 and he spoke about
Well, he didn't spoke about intelligence he spoke about mind
So the hallmark of mind is attaining consistent ends by variable means
What do you think about that? That's really talking about a goal right consistent ends from variable means something is varying its means in order to
Achieve those consistent ends. That's really saying the hallmark of mind is
goal-seeking
And then there's the field of artificial intelligence which is
Famous for not defining what intelligence is or what artificial intelligence is
Yeah, for so long it just refused to do it and you know, I think that's not okay
You have to everyone have a well-defined field. You have to have be clear about what your objectives are and what your subject is
John McCarthy is the fellow who
Invented the term artificial intelligence. He of course is one of the founding fathers
But many years later
He wrote down a specific
Definition that's the one here that I that I like as the computational part of the ability to achieve goals
The computational part the ability to achieve goals. I think that's a really interesting definition
It's not the only definition
Often intelligence is taken to be like mimicking people as in
AI
Seeks to reproduce behavior that we would call intelligent if it was done by people
The classic Turing test is focusing on behaving like a person
supervised learning the task is often to label
Pictures say the same as a person would label those pictures and then the large language models, of course the large language models are all about
mimicking
Text generation by people
Okay, so this is this is a major thing
You notice aren't really any goals involved in mimicking people except, you know
You could say mimicking people as a goal
But really that's not what what I think goals are about goals are about affecting some change in the world
Observing something and then you're being satisfied that you're mimicking it is not about a change in the world
But you know think of all those systems
That are mimicking there. They're not seeking to change their input at all. They're just seeking to mimic it
Okay, anyway, so there's two definitions
one is that intelligence is has to do with mimicking people and
The other one is achieving goals and we might ask
which is better and
I want you to think about that and so I don't want to think about it with a little bit of sophistication
Maybe this is kind of like yesterday, you know, I don't want us to jump to an answer because I want to think about what it means
for there to be an answer and
You don't really have any slides on this, but I do have this slide
That just reminds us that you know a word like intelligence when you look in the dictionary you will find multiple
definitions
The second one being a military intelligence like spies and stuff
So every word is like this and that's the way language is and it's it's good
it's good that there are multiple definitions and multiple meanings for words and
There so there isn't a sense that one is right or wrong. There's
There's a sense in which they're useful for or not useful for particular purposes
So it's our we have there's a free choice when you define a term. It's a totally free choice
There's no right and there's no wrong in the in the definition of a word
but there is
Consequences right because you're you're choosing how to use it
And so then you have to use it that way and you can decide whether for your purpose
That's a useful way to define the term
So the to the extent there is a right or wrong is just it's not it's it's a sense whether they're useful
suited to purpose
so
Now let's go on and try to
Assess the different meanings
and
I want to do that by reference to this quote from Ray Kurzweil
He says that intelligence is the most powerful phenomenon in the universe
So that's not a definition obviously it's it's like a property of what intelligence means something and then and then it's claiming that it's powerful
But
Just take it on his face for a moment
Could intelligence be the most powerful phenomenon in the universe? I mean, what about you know black holes and supernova supernova very powerful
But I think Ray means this literally and I think it's not crazy to mean this literally like you know
supernova
Are big but isn't it it's
Isn't it also plausible that
Well supernova have been around for billions of years. They've had billions of years to develop intelligence has been around for
Few hundreds of thousands of years
If you give give intelligence, you know a billion years
Is it is it doesn't seem almost likely that?
Intelligence if it if it still exists would would be doing things like moving the stars around moving the planets
It would be a powerful phenomenon in the universe at that scale. I
think it's
It sort of expresses the ambition of what we want intelligence to be
Okay, so
So when when I suggested to you that it might
Become a big deal and in a universal galaxy level
What what I was thinking was could the fact that there are
Agents in the world that have goals
And it's best to think of them that way
Could it be that powerful phenomenon eventually and that's what I want to say yes to
For the other meaning could the ability to mimic people be such a powerful phenomenon
I'm I think I think the answer is just no
People are the powerful thing and so mimicking the thing that mimic will gain some power from the people
But the the ability to mimic people is not powerful in this sense
so
That's my first conclusion that the powerful part of intelligence is not the ability to mimic people but the ability to achieve goals and
so
That's a point in favor of using the word in this way the ability to achieve goals
Okay, now
Let's look a little bit deeper into McCarthy's definition
You define as the computational part of the ability to achieve goals
So that computational part is meant to rule out like you I can achieve goals because I'm stronger as I'm faster
And I have better sensors
These would make you better able to achieve many goals, but it would not be because of your computations
So we don't consider that to be intelligence
There are at least that definition doesn't
So moving that up a little bit to give myself give myself a little more room
I think similarly
You could achieve goals better if you're given knowledge about
The world or the domain you were working in you know that would enable you to perform better to achieve goals better
But it's not again. It's not because of your computations
It's not your intelligence is because of the computations of where we gave you that knowledge gave you that help
so I
guess I have a conclusion out of that that I
Want to say the intelligence is the computational and domain independent part of the ability to achieve goals
maybe that's a refinement or a
narrowing of McCarthy's definition, but I think it's in the spirit of
Having a powerful phenomenon of Intel for intelligence being a powerful phenomenon
Okay, so just to summarize that
Mimicry and domain knowledge are not the powerful part of intelligence
Mimicry is getting goal directed behavior without the goals or the processes that compute the behavior from the goals
Injecting domain knowledge is a way of getting gold directed behavior without the processes for obtaining the domain knowledge
So this is sort of the way I understand
The limitations of large language models. They are the abilities
But they were given to them and they don't have the ability to
To develop that knowledge and that behavior themselves
So they're they're both incomplete can't stand on their own
These shortcuts don't have the power of intelligence. They can be very useful
but but
That shouldn't make them intelligence
Using in the word in that way would weaken the search for an understanding of intelligence. That's powerful in Kurzweil sense
Let me just say another another thing on that which is that
intelligence the word intelligence AI AI in particular in today's world has cash a
like
Actually, it's not even just today's world. It's always been this way
but
What I mean is like for example, I went to the grocery store the drugstore the other day and I saw the aisle full of
electric toothbrushes and
You go to the electric toothbrush section today and they will say this toothbrush has AI in it. I
Mean literally they will say that
It has cash a to have AI to have intelligence is as viewed as a very positive thing
And so everyone wants to be that way
I hear that LG has a has a clothes washer that's has AI in it
In the old days they used to make
computer terminals and
They would call the terminals intelligent terminals and all they would do they were only intelligent because they could use multiple fonts
You know, so this has always been the case
And so there's a tendency for everything to become AI
More's law or the generalized Moore's law of increasing
Computation per dollar that's gone on for a hundred years or more
That is all about increasing computation. It's not about increasing not not it's not necessarily about increasing AI
So there's a big there's a tendency to conflate the two trends that AI is becoming more important and
Computation is becoming more plentiful. So all everywhere there's computation
And that's that's what we're seeing with this
Stretching of the term
intelligence to cover everything
That just uses computation. I don't think that's a useful direction to stretch the term
Okay, I think now I'm ready for a joke and so I'm gonna I think I'm ready now to present my present my my joke
Or my cartoon. This is my cartoon. You may have seen it
To think that this all began with letting autocomplete finish our sentences
So this I found this in the New York. I hope I'm not violating
important copyright things anyway
I'd like I really like this cartoon because it's
It's exaggerated. It's making both fun making fun of
Both the positive and the negative hype that's around AI
The the positive the positive hype is that autocomplete finishing our senses will lead to you know, the robots
being super powerful and taking over and
Guess the negative hype is that you know if if
Computers become powerful if we should robots become powerful they will subject us all to slavery as in as in this picture
Okay, so humor is the best way to purify or
our thoughts and and
Think about controversial things
Okay, so now I'm ready to talk more specifically about the reward hypothesis, but I hope you
You've gotten the point the point, you know the word hypothesis about how we talk about purposes and goals and
Intelligence is is really centrally about goals and purposes
so if this is true if it's true that all goals and purposes can be thought of as as
Maximizing a single number in this way, then it means that all of intelligence can be thought of as maximizing a simple number
So this is one the multiple ways to develop this hypothesis one is to do it mathematically and formally and and
Like you know, what's this? What does it mean to have a have a goal?
It's an ordering on possible outcomes and what are they all possible different ways of doing an ordering and you're looking at this
Carefully as a mathematical formal statement
Like one might in do in economics or the theory of decision-making
So that's that's the work this idea has been developed in that direction
I'm and I'm referring to the work in the corner
It's more recent work by Michael bowling and John Martin David
Abel and Will Dabney where they formally assess it
to the extent that it is
Complete and includes anything else you might propose. So what are the other things that are ruled out? You may be thinking, you know
What's ruled out would be things like considerations of risk in a special way
consideration of multiple objectives
You see it's all about the expected value rather than the distribution of possibilities
Okay, now another another hypothesis that's been around
The reward is enough hypothesis
That there's intelligence and all of its associated abilities can be understood as subserving the maximization of reward
So this is very similar. It's almost like the combination of the reward hypothesis and the McCarthy's definition of intelligence
But notice all these things are about the goals are about the the problem the problem
They're not at all talking about solution methods. They're just saying
Reward might be enough to motivate and drive the achieving the achievement of the associated abilities, whatever they may be
You can understand them as as being driven or subserving the maximization of reward
Okay, so now I want to
Is reward enough is a single number enough? It doesn't it's it's it's it doesn't seem like enough. I just wanted to
Bring this on the table. I'm sure you felt that it seems too small a
single number
coming from outside
the agent
You know people seem to choose their own goals. I mean, that's one of the biggest things we see for ourselves
We define ourselves by the goals we set out to achieve
Reward just seems too small too reductive. It's definitely reductionist hypothesis. It's and as reduct being reductive
It's kind of demeaning
Surely our goals are grander than maximizing, you know pleasure and pain or some number
You know, we have things like raising a family saving the planet making the world a better place
contributing to human knowledge
Not just these small things so
So that's really the tension. I think around the reward hypothesis that it seems like it seems too small and yet it seems
We keep being driven back to it as we try to get formal as we try to be clear about goals
we're
We're driven back to it because it's clear
because it's
experiential
Kind of grounds things it gives us a well-defined place to proceed
so now a few slides
exemplifying how
Even though we're uneasy with this idea we keep coming back to it we what is we I mean, it's an interdisciplinary idea
This is the modern view about
About how our brains work is that there is there is a measure of
Pleasure and pain and then
There are calculations. I'll get into a little bit later
But it's totally consistent with this and not just we it's like all animals have like a dopamine center
and
This is a good it has been true that a good way to think about them has been in terms of a scalar outcome
Not entirely
Okay, so let me go through some facts
This is some of these slides a little more detailed than we need
But I just want to refer that within AI which is also uneasy with the idea of reward
It's it's become more comfortable with it over time
So the very earliest AI systems were all formulated their goals as
Attain the state of the world
Okay, which is which is very different from
Maximizes number coming into your into your mind
It's as if we can access directly the states of the world which which we cannot of course we have to infer them from our
from our sensations
but
And and I'm saying it's even true the latest version of the standard AI textbook
Russell Norvig it still talks about goals primarily in terms of states of the world and not in terms of experience not in terms of reward
But it also has chapters on reinforcement learning and those those all use reward and
With the rise of machine learning within AI the reward formulation has becoming more and more standard as in planning and mark-up decision processes
and
We can look at Yanlacoon Yanlacoon who was who's sort of an anti
Reinforcement learning person he now admits that if the if the mind is a cake
This is his metaphor
Is that if the mind is a cake then reinforcement learning is a tiny part of it's like the cherry on top
but he thinks the reinforcement learning acknowledges that reinforced learning is necessary because goals are necessary and
Either the substance of the of the cake is maybe doing prediction or unsupervised learning and so it those parts are not
Goal oriented. They're just trying to predict and understand the world
And I shouldn't say just they're trying to predict and they're trying to understand the world and model the world those don't
Don't have goals in them. They're just
I said it again just gotta watch out for that little word just they are
Understanding the the truth of the world as separate from
Trying to direct that in any particular way
Okay, but anyway, he's got the cherry on the top and to me. It's a cherry on the top of the cake. So it's pretty important
and
Here's another one from classic AI artificial intelligence
There's this these cognitive architectures since the 80s
They're very symbolic and using production rules and since like 2008 and they've included reward as part of it as as a as a basis for
The the goals of the system
So single numbers have becoming more prominent in AI as a formulation of the goal
now this
now let's talk more in a more interdisciplinary sense and
I recently wrote a little paper where I just sort of said, oh, you know
There's a lot of commonality between many many fields thinking about
Agenthood and goals and purposes in mind, you know in psychology control theory AI economics neuroscience operations research at least these six
You can find basically the elements
In these two pictures the first picture is the agent interacting with the world or the environment and
So this is the basic picture of a of a decision maker. He
Sees information from the world his observations and he picks actions and then he gets back a
Scalar measure of performance that he with the objective is to maximize it. That's sort of accepting the reward framework
You will find this
Of course these ideas very basic, but even when you look inside the agent now
So I'm gonna look inside the agent how the problem is solved you can find some of the common elements and
So there are four of them
But let's talk first about the core two
So I'm gonna block some of these out. Yeah, the core two is the perception and
The policy the reactive policy so perception
Takes in observation so observations are like, you know, your sensory input and
From that you construct a sense of where you are. That's that's the state representation
And then you choose what to do based upon the state representation
The difference between the two boxes is that the policy is
Memoryless right you can't use old states to decide what you're gonna do now the whole point of a stage as it says
This is where I am now this I should decide what to do based on where I am now
it's
And it's the job of perception to construct that sense of where you are now
So perception is recurrent it involves the last thing you did the last observation and the the previous state
Now
This is a complete behaving system
Observation comes in flows through perception and the policy produced in action
That's all you need to be a complete behaving system and you notice reward is kind of hanging out there
It's not really doing anything and that's because
This is a complete behaving system, but not a system that's made up for changing what that system does
So let me dig do one more dig at large language models. They are this part, you know
they are a way of
transforming the sequence of words into a
Memory of where you are and then deciding which word to output
But it's not a way of changing that the there isn't a once your large language model is in place is running in the world
It can't change. It's it's just these two parts now
You can change in the sense that perception can accumulate your a
More refined state as you get more observations, but you can't get a change in the policy for example
or or a change in the
Function that is implemented as perception
Maybe I'll dwell on this a little bit more and then on the
interdisciplinary aspect
So we see just remember that all these different fields have used different names like if you're in psychology you talk about
so the action you talk about the response and
in control theory you would talk about the
the control instead of the policy they talk about the
control law
instead of instead of
Perception in in control theory they talk about a state estimation
box
In psychology the observation would be a stimulus and the reward might be reward
But actually the subtleties of the
Connotations of the word are somewhat different in psychology
So
I think it's it's kind of interesting to try to find a relatively neutral language that can
Apply to all these all these different fields
Okay, so
I'm unblocked the transition model the transition model is supposed to be the model of the world
It's gonna have your domain major place where domain knowledge is
Okay, I'm not gonna talk about that today
So let's get rid of that. Anyway, but it would do planning. That's how you do planning
We're not gonna talk about that
But I do want to talk today as part of talking about the reward hypothesis about the
The other box the value function box is the value function is the major source of
Learning input of changes in the policy
And this notion of a value function is also common to all these fields
Hey, I through reinforcement learning has value functions
psychology has has
Reinforcers and secondary reinforcers and and it's built up in the same way
and
There are issues of proxy objective functions and control theory that are all about
Constructing value functions. So what is this value function thing? Let's talk about that
Oh, but I couldn't I couldn't resist I want I got in this x1 extra slide where I'm just
It does remind me to say to say one important thing that even though there's only one objective
the reward to maximize reward the agent can well as as as
Means as
Solution strategies it might have it might pose some problems for itself other problems for itself
Like you might learn how to how to walk or how to drink a glass or how to navigate to the monk school
You learn all those things that they're not your main task
But it might if you learn how to do those things it may be useful for solving the main task. So you you
My theory of of mind and the next step of that theory would be that we
We pose some problems for ourselves and solve those and then use those skills that we develop working on the sub problems
In order to solve the real problem, which is to maximize reward
Okay, so value functions. We're getting almost done
value functions
So reward and value reward defines what's good. So we seek a policy that maximizes reward
That's done defining the problem
But reward is often delayed and that makes it hard to learn a policy using reward
And so instead of working directly with reward value functions map states to predictions of the future reward
if you have prediction of the future that includes the
Delay and if you can bring that into the into the present if you can predict now what the future rewards will be that
enables you to
Eliminate the delay and makes finding a good policy much easier
Maybe that's intuitive you think about I always think about playing chess
You know the reward is checkmating your opponent
And so then you get maybe a plus one for checkmating a minus one for losing is zero for drawings
so that is
and of course
Formally you get a reward all during the game, but they're all zero and you only get an interesting reward at the end and
So there's a delay between any good move or poor move
You might make early in the game and and whether you won or you lost and so of course what we would all do is we would
Learn to predict. Am I going to be checkmated in this game?
Okay, so that's a prediction of the future and it's often called an evaluation function or a value function
It's a prediction of the reward. So I think it's it's
Well, this is the the value function hypothesis is that
Forming such value. I guess that's that's that's right on my slide here
The value function hypothesis is that all efficient methods for solving
Sequential decision problems, which is what just means decision problems over and over again through time
They learn or compute
Value functions as an intermediate solution step
So when you're gonna play chess you learn the sense of am I when you're losing and then you know when you make a bad move and now
You thought you were winning and now you're losing you see then no
That's the critical part
Where you made the mistake and you can use that to assign credit to your policy and change your policy in a much better way
Okay, so that's all sounds awfully technical
Let's go back to philosophy go back to Plato, you know
You can find in Plato talking about good and evil and pleasure and pain and he will say things that are a lot like this
These are some quotes even enjoying yourself. You call evil whenever it leads to the loss of a pleasure greater than its own
Or it lays up pains lays up pains that outweigh its pleasures
Isn't it the same when we turn back to pain to suffer pain you call good
When it either rids us of greater pains than its own or leads to pleasures that outweigh them
So what does he say he's saying good and evil are about the sum of upcoming reward?
Which is what we try to predict with a value function
So basically it's all hedonism. It's all hedonism, but value functions make it hedonism with foresight
Okay, that's the story the main story of reward and value how we use them in decision-making to change our policies
But I want to try to do one more thing
I get you understand the way it works in a little bit more
I don't know if this is gonna work, but let me try I call it the mystery of expectation and reinforcement
I call it the mystery of expectation and reinforcement now
Superficially those are new topics. I haven't talked about expectation and haven't talked about reinforcement
But by expectation I mean like the prediction of reward that the value function is the value function learns predictions i.e
expectations of upcoming reward and
Reinforcement reinforcement
Is not reward reinforcement is the
Moment-by-moment signal that reinforces the behavior at that time
So it's really your sense of did I do something good or did I do something bad is should I stamp what in should I stamp in what?
I did or not so reinforcement is not reward because there are many cases
And we just alluded to some of them where you get say a positive reward, but you
View it negatively
And so expectations play a role there so expectations
Contribute negatively to reinforcement in the sense like you know I went to the movie
I thought it'd be a really good movie and I went there. I was just kind of so so so actually I feel bad about it
so so expectations are
Negatively to the reinforcement movie was just so so but because you expected it to be good
That gave you a bad feeling about it a bad negative reinforcement
But expectations also contribute positively to reinforcement like when you
Expect to get reward that that is a reinforcing event if you if you I don't know are given a million dollars
That's a good thing, okay, but it's actually not really reward reward becomes because you expect to have a million dollars and be able to use it
you know to buy
Good food or anything you might want so that would be the reward and the expectations are also positive
And so that's the mystery. How can expectations contribute both positively and negatively it is in just a common-sense way and
the answer is over time and
So just consider this simple case of evolving through three states states one two and three and
This is like your life some life moment by moment
so three moments two transitions and I'll just take it that your expectations your predictions your values of the three states are
zero ten and zero and then
The rewards on the when you experience these three states are zero and eight
Okay, so now I want to ask you the question or ask you to ask this question of yourself
How do you feel about the first transition?
So what's the reinforcing effect is it positive?
It's negative like is it good is it bad as you went from state one to state two and
It's I want you to give me a number because it's all coming down to numbers
So what number would you give to that transition from state one to state two
Expectation was zero now. It's ten the reward was zero. Do you feel good? You feel bad?
Anybody
John's giving me a thumbs up she was giving me a thumbs up
Yeah, I think we're gonna say this is like a plus ten experience because
You weren't expecting anything and you didn't get anything right away
But you now you now you expect to get ten in the future. That's like the getting the million dollars
And now the second transition
What do you expect?
Expecting ten you got eight and now you don't expect anything in the future
That's gonna be like minus two
Okay, so that all makes sense. It's very intuitive. Okay, so if you just
Make a formula out of what you you've just understood is as obviously what should happen
You're gonna end up with temporal difference learning the fundamental algorithm of of
Reinforcement learning. Okay, so the the thing you just formed in your head was that the reinforcement is sure it's the reward
But beyond that it's the change in your expectation from one time step to the next
As has the expectation gone up as it gone down
And so it's the sum of the reward and the change in the expectation because it's a change
It's a temporal difference temporal difference just means time difference, which is just change
It's the change in your expectation contributes to the reinforcement. And so this is the
temporal difference error and
also called the reward prediction error and
I need to finish up, but I did want to mention since it's such an interdisciplinary
Conference that this theory of brain systems as
Following TD learning is one of the most
Important interactions ever between engineering science as a neuroscience is a big big thing around the turn of the century
And finally my last slide is on ethics
So what we've talked about rewards are a good way to think about the ultimate goal and value functions
Which are predictions of reward are a good way to think about how that goal is achieved. All this is neat and complete
It's a good theory of a single agents decision making
But it's it doesn't have what we would expect what we might hope to get from a theory of ethics is that there would be some
universality to it there would be a
Reason to for different agents to have similar values, you know, that's what that's that's what I understand ethics to be about
It's when we reach for some
universality rather than just an individual and what we have so far doesn't have any
element of that and I'm gonna now admit that I have no
no sense how
Any universality could occur and so I want to propose that maybe there is no universality there's a need to be universal
Sality for there to be ethics. Maybe we just
Many of us have there is an aspect of what we do of commonality between many people and
That that's what ethics is about. It's so this is a reductionist hypothesis again
that maybe ethics is just values that are happened to be held in common by many agents and
so even though
Even things that are held in common by many agents won't be held in common by all agents
The lion and the gazelle will have different values
So that's really all I wanted to say and I thought it would be a good basis for starting a discussion of ethics
Thank you very much
You're up
Jinx
Thanks, Rich, I'm really glad we got a chance to do this and thanks to Jillian for the same reason
So I'm gonna start with just a little bit of housekeeping disciplinary housekeeping just to make sure we're all on the same page
So like any paradigm
Reinforcement learning is going to have certain technical and foundational commitments
and then
Loss of different versions of those frameworks are going to add certain assumptions and relax others
And so just before I get started
I want to make clear two assumptions that I'm committing to because they will inform kind of what I say next
I'm gonna call this RL star or sometimes they can also be called RLDM like reinforcement learning and decision-making I
Think it also make clear maybe where where Rich and I agree and sometimes might disagree
So the first assumption
In my kind of particular
Version of reinforcement learning is that there is in fact something really special
About reinforcement learning when it comes to understanding the mind, right? And so I think this is an assumption we share we both subscribe to this assumption
Peter Diane who just got mentioned and put up as like a little plastic bobble head and yell these
Put forward something like the following claim so reinforcement learning algorithms such as the temporal defense learning
Rule apparent to be directly instantiated in neural mechanisms such as the phasic activity of dopamine neurons
That reinforcement learning appears to be so transparently embedded has made it possible
To use it in a much more immediate way to make hypotheses about and
Retradictive and predictive interpretations of a wealth of behavioral and neural data collected in a huge range of paradigms and systems, right?
So there's this
Is the sound coming out already?
Yeah, I've been wondering that too. So thanks, Rich
There's a star beside instantiated for the philosophers of science in the room. Just relax about that
It doesn't matter we can just say look reinforcement learning is shockingly surprisingly good about capturing something of the mind
We don't have to commit to instantiate. It's one of those weasel words we talked about yesterday
It's a quote from them, but that's just not what we don't want to talk about here
There's something about reinforcement learning
That is extremely powerful when it comes to understanding the mind whether it's instantiated or not
So that's the first commitment that I'm making here the second commitment where we may disagree although
sudden
June 21st 2023 suggest maybe that's not the case is that
Mines like ours so personally assigned subjective rewards somehow so there's a big debate about this question of where do rewards come from?
right and the kind of classic view is that reward comes externally and
That is an important tenet of some frameworks of reinforcement learning
But I'm gonna go ahead and say and this is kind of an agnostic claim that the mind somehow
Internalizes reward there are good competing theories for how that takes place
Such as by Chris and refilled it doesn't matter. I just want to say that the mind somehow
Internalizes reward in ways that are important
So that's gonna be a commitment where we perhaps depart from one another
But it's gonna inform kind of how we want to go forward about this
Okay, so that's a bit of ground-clearing
What I want to use my time
For today and kind of as a basis or a backdrop for the discussion
Let's make three claims about kind of this version of reinforcement learning and what it means
For understanding the mind and so the first question that we have right is you know
Is the reward hypothesis a good model for understanding the mind?
And
As rich has pointed out, you know, it's been kind of transformative in the decision sciences and all those schools that we talked about
Cognitive neuroscience computational neuroscience psychology economics increasingly. I think in philosophy
It's like making inroads in philosophy
But I would actually say the first kind of claim that I want to make is that it's still underappreciated
In how much it should and can transform my understanding of what the mind is fundamentally in the business of doing
So this is kind of the cartoon dialectic slide, right?
I think when we think of the mind we have historically but even in
Recent kind of cognitive scientific history. So we've seen this in Joel's slide in Blaise's slide
Yesterday is you know, when we think of what the mind is fundamentally in the business of doing
As how Glenn put it, you know, it's about thinking. It's what intellect. It's it's fundamentally kind of an epistemic machine
It's what the mind is a computer
And one of the ways that we can put that is that the mind is in the business of performing computations
over representations of descriptive matter of fact, which is just to say the mind is in the business of making better and worse kind of
Assessments of what's going on out there in the world. Is it raining? You know, what day of the week is it?
It's about knowing the world so that we can move around in it
In various ways and so more or less factive representations of the world out there
And I hope Blaise's ears are ringing right now because he basically went up yesterday and said, you know
Maximum entropy first in all these people, you know, a new way of putting these kind of very epistemic
Conceptions of mind is that it's all about prediction
You know, the mind is a prediction machine and all we do is make predictions about what's out there in the world
Then we update them. So but again, it's it's essentially about knowing in some lowercase case
Sense of what the mind is all about
And I really disagree with that
I think the thing that we learn from the reward hypothesis and kind of the reinforcement learning framework that follows from it is
That the mind not only performs computations over
Representations of descriptive matters of fact, but the mind also
Fundamentally performs computations over those representations as better or worse
So we are continually
Representing the world as we move around in it, you know what this room looks like
What day of the week it is whether it's raining or not
But as part and parcel of that process, we are also continually representing all of these states as
Better or worse with respect to our goals
So what we're doing is we're laying over these fabrics kind of of the states and assigning
Value in the way that rich just described and so we're not just seeing whether it's raining, but oh, it's raining
Here's what that means for me or oh, it's raining. I live in London, of course, it's raining, right?
These things can matter more or less, but that we're not just experiencing the world. We're
Continually evaluating and so I think one of the things that the report hypothesis and
Tells us is that the mind is not just a thinking machine. It's a valuation machine. It's continually
evaluating
Now one of the examples that I like to use for this is kind of the thin end of the wedge. It's very narrow case
But it's the phenomenon of vernacular robbery. So
vernacular rivalry occurs when you place some kind of division between the left eye and the right eye
And you show one stimulus to one eye and another stimulus to the other eye. So in this case borrowed from
Feldman Barrett's lab is one eye sees one eye is presented
I should say with a house and one side is
presented with a face and what you might expect
The participants to see is you know house face
But the reason vernacular rivalry is interesting is because what they actually perceive what they actually experience is an alternation
They see a house they see a face they see a house and they see a face and
obviously for you know, the view that the mind is
Engaging with the world that the mind is computer that we're making prediction machines
This is a very interesting case because we know for a fact that we are not perceiving
What we are essentially seeing right the experience
Strictly departs from
what we know is being presented and
This was a very important test case for predictive processing and first in like folks because they gave a very elegant
Explanation of what's going on?
So they basically gave a Bayesian interpretation of predictive of vernacular rivalry and said that roughly what happens is that we have very low
Priors for seeing a house face in the natural world
And so what the mind does is it says well it can't be a house face
So it must be a ha it must be a house
But then the prediction error comes back up and says normal
You're only accounting for half of what you're experiencing here
So it must be the other stimulus and it switches and so it's saying okay now it's a face
Again a strong prediction error and an ultrace
And what you have here is you know perceptual dominance
So you might see the face first or you might see the face for longer or it might alternate more
Okay, the mind is a thinking machine. It's epistemic great
We have a very you know epistemically normatively informed explanation of what's going on there
The reason I like to use vernacular rivalry as a case for the evaluative mind
Is that when you subscribe to something like the reward hypothesis as I did?
Let's say in 2017 when I was presented with this is that I thought you know, I bet you reward will modulate
This phenomenon so it allows you to make predictions even from the armchair and I bet you reward will
Modulate the experience vernacular rivalry and that is in fact what you find so if you
Reward a certain stimulus like the face so you say hey, I'm gonna give you a penny every time the face appears
We have perceptual dominance
Of the face but it gets better than that you can also reward just the percept so people don't cheat basically and
Every time they report seeing a face you can do like a pitching
And again the face will be more dominant. You'll have perceptual dominance
First in folks when you email them
They're like yeah, but it could just be information right the reward is an added piece of information
So you can put that into kind of your Bayesian function and that can still explain
Why this perceptual dominance is occurring, but what you can do is you can have a
Punished percept so every time they see a house you can make a little sound that says
You know like you're sad
And if it's just information you should have perceptual dominance of the house
That's not what you find you actually still have perceptual dominance
Of the non-punished percept of the face so really what this tells you and this might glamorize the finding a bit
You want me to like
Animate less. Yeah, no, I know I know I know it's okay
I'll try
Yes, sorry
I will hold on to the podium
This is a small finding but it tells you basically that to some very limited non-exciting degree you see you perceive
What it is in your interest with respect to your goals to perceive right again, this is a very thin case is very narrow case
But it tells you that you don't just
Perceive the world we perceive the world conditional on our goals
Which in this case is to make a little bit of funny in these studies, right?
Binocular rivalry is a tiny case, but if you look at all the research
That kind of riches alluded to over the course of the last several decades
We find that every stage of mental processing from sensation
And computation to action at every level of description and mental processing from the sub personal to the personal
It's conditional on these attributions of reward and value
So we sense we perceive and we attend to the features of our environment
conditional on reward and value we remember and remember to remember remember
prospectively conditional on
the attribution of
Reward and value our cognitive control our ability to decide choose
Plan our future actions in each case you will find a body of evidence that's just it's just like in binocular rivalry
It is conditional on the attribution of reward so
We have this kind of classic sorry picture of the mind as a thinking machine
I think that's part of the story, but the mind is also
continually in the business of evaluating all of these parts of the so-called kind of cognitive sandwich
So I want to make some kind of caveats here. First of all, it's an additive thesis
What I want to say is that the mind
computes over these representations of matter-of-fact and it continually evaluates them now for some people that might seem kind of weak sauce
Particularly, I think philosophers sometimes want to come out swinging and say, you know, this is everything
I call that the stronger thesis the stronger thesis something like what the mind is fundamentally in the business of doing is
Evaluating things is better or worse, right? That's that's mainly what the mind is about. I don't want to go that far
We can talk about that in the discussion of why not I want to say that the mind is fundamentally in the business of doing both
So I'm making an additive claim. I don't subscribe to the stronger thesis
Although I do think it's kind of fun and it it does kind of really drive a lot of nice research
And I also don't think this is exactly the same thing as saying reward is enough
My claim is about the nature of the mind and what the mind is in the business of doing
I take reward is enough on one interpretation to be saying
What's needed in order to produce intelligent behavior? Those are obviously cousins as positions
But I think reward is enough again
Make some stronger claims than than than what I would subscribe to but that's definitely something that we can talk to you
But I think we need to start thinking of the mind as evaluative in these ways not just because that's what we're doing
But also if we want to design artificial intelligence, that is a very different picture of what we're trying to build
And we'll also go towards addressing some of the kinds of challenges that we heard about yesterday
So the first question was does the reward hypothesis provide a good model for understanding the mind?
Yeah, I really think it does and the answer to that is yes
And I think it changes our understanding of what the mind is kind of in the business of doing
The second question is you know, how far does the reward hypothesis go as you might imagine I'm gonna say something like pretty far
So I gave you a list of kind of all the the kinds of
Processes that reward and value are are implicated in and you can kind of see that a lot of these are sort of you know
So called low-level cognitive capacities
But all the way kind of extending into sort of high level cognitive capacities that people might be be interested in and so
I don't want to go into that too much, but I want to focus on what you might think as one of the kind of
Crown jewels of the human mind, right, which is our ability to have moral experiences
If there's anything that we might want to think is special, but how the mind works
One would be language
I can talk about that after yesterday
But another one might be our ability to have moral experiences, so when I say moral cognition
I mean something quite straight forward
It's the capacity to create and respond to situations of moral significance. So let me give you an example. It's nothing fancy
You know my sister Barbara
Goes to buy a coffee at a coffee shop and she can make the following decision
She can buy the more expensive fair-trade coffee or she can buy the cheaper
Commercially sourced coffee, right and she can stand in line. She can make that decision and she
Realistically probably past the cheaper commercially sourced coffee. Sorry Barbara
But that is an example of moral cognition. She's not making a decision about a trolley problem or anything like that
These are the everyday routine moral experiences that we have that we make like this and that's the kind of thing that I have in mind
When I'm talking about moral cognition
So again, they're kind of quite
well
established received views on kind of how our moral cognition
Works what the mechanisms behind that are and so
Again, this this isn't actually even too much of a cartoon. I think we have kind of these rationalist inferentialist
Views where the idea is that Barbara
Does or doesn't buy the fair-trade coffee based on the belief or set of beliefs that something is the right thing to do
And then it will kind of follow from you know her knowledge
about a particular situation
On the other end of that dichotomy
There's this idea that Barbara might do or not do the right thing
Because of her moral emotions
She might feel empathy towards the workers or not
So an agent has the fitting moral emotion or emotions that something is the right thing to do
And then despite the kind of distribution of the literature actually I think a lot of mainstream views
Are hybrid views some combination of the views that you know certain things that you have certain kinds of beliefs
You have the relevant beliefs. You also have the accompanying emotions that might motivate that behavior
And we should definitely talk about that Plato slide, but without going all the way back to you Plato
I think these have been kind of the main
Alternatives in understanding moral psychology and understanding our moral cognition. I
Think the reward hypothesis suggests that there's a kind of a new player on the scene
And that moral cognition is in fact constituted by the sub personal attribution of goal and context
Dependent subjective reward value. So again that familiar phrase that we see
What we need to see in binocular rivalry is actually one important ingredient that mechanism is also driving our experiences of saying something like
That's just wrong. That's morally disgusting. I really ought to be doing this thing
Again in the descriptive cases, but what I am suggesting is that we actually recruit the same reward mechanisms
In our moral cognitive experiences now I'll flush that out again because that's a bit strong and a bit strong stuff
But what I want to suggest is that
There is quite a lot of flattening going on here, right? So in fact
When you are experiencing
Something as right or wrong you are attributing reward and value or laying that fabric in just the same way in a moral
Context with moral determinants like something like fairness something like honesty as you are when you are just choosing between
You know left and right to your coffee
So that's to say that if I'm choosing between to your coffee or coffee and fair-trade coffee
I'm actually recruiting some of the same mechanisms now again
There will be caveats that come to kind of flesh out this picture
But this is an important flattening of our understanding of what's going on in our moral psychology in our moral cognitive experiences
And you might think okay, but I really feel strongly about certain
You know moral propositions, and I don't feel particularly strongly
about to your coffee or something like that and
Sort of the second thesis of this view is that reward and value the strength of the reward and value are actually the source of our
Moral motivational force so through our evolutionary history and through our upbringing
certain things are
reinforced
To be extremely important to be absolute no-knows or absolute musts
And that these are the sources of why we sometimes feel that something is right or wrong and why we experience things
Something we really ought to do again. This is not a normative on but it's the feeling of I oh
I really should call my grandmother or something like that
It's because the driving mechanism here is again reward and the value not my emotions not my
Knowledge though again, I'll build a kind of more complete puzzle there
And so this is kind of one of the things that fundamentally differentiates between something like our social and our moral values
Right, we know we shouldn't wear white after Labor Day
Or we did know that once when I was like five
That has some force, but it has much different force from you know
It's wrong to lie and that is I think a function of the strength of the value that has been
Attributed to that over the course of our lifetime experiences our
Education's our philosophical conversations and things like that
So it's a source of the strength of the moral motivational force
Now this is pretty quick and again, I'm gonna flush out the picture in a second, but I think
This kind of you starts to account for features of our moral psychology that the kind of rationalist
Inferentialist views and the sentimentalist views don't account for so it doesn't it it accounts for something like the multidimensional nature
It's hard to explain the Barbara fair-trade coffee case with just the emotions, right?
There's lots of features that need to be weighed off there that a kind of complex
Waiting of our values allows us to explain explains why sometimes she buys fair-trade coffee and sometimes she doesn't
Why we have cross cultural values, so we have themes in our values
So all communities, you know value honesty, but what it means to be honest or what it means to murder or what counts as
incest is going to vary in us because we share some of these
Mechanisms, but how we tune them up is going to really depend on the community and the culture same with
moral learning
It's going to
Allow us to understand kind of what counts as morally exceptional, right where people stand up against all kinds of
opposition and costs and
But we're going to kind of place a very high value on the certain principle and stand by that principle
Through that strong moral motivational force
and I think kind of one of the
The more complex but also more compelling kind of pieces of evidence for a view like this is
What happens when we have failures in our moral cognition make errors?
We have dysfunctions and sometimes we have pathologies in our moral cognition
And one of the interesting things is that these seem to rise and fall
With dysfunctions in our reward systems in our reward mechanisms in ways that we wouldn't necessarily expect to see otherwise, so I
think
I do want to specify that I think it's a part of the puzzle again
It's going to be an inclusive view in the same way that the evaluative mind is an inclusive view
That's not to say that we don't have any place for the emotions
In our moral cognitive experiences. It doesn't mean that we don't reason and read philosophy and things like this
But when you're standing in the coffee shop and you're trying to decide whether you're making
Whether you're going to buy the fair-trade coffee or not
What you're recruiting there are your valuation systems and your life-long experiences
Of what you value and what we attribute our values over now
I think one key open question here is what do we attribute those rewards and values over is it to states?
Is it to state action pairs? Is it to abstract ideas?
Is it possible that philosophers for example have learned to attribute a high amount of value just to the concept of
Justice and the existence of justice in the world, right? I think these are important questions of how exactly it plays out
But I think and here I will take a dig actually at that Plato slide
It's not about pleasure and pain actually
I think pleasure so I follow Kent barrage here and distinguishing between pleasure and pain and
Reward and punishment or reward and and disappointment and I think our moral cognitive theories have been based on what we can introspect
Oh, I think this oh, I feel this and we don't necessarily have introspective access to our reward mechanisms
And so I think actually in a way
It's a disservice to try and tie it back to hedonism because it's rewarded something different here that hasn't been on the scene
But I think is a driving factor in this much more complex architecture of our of our moral cognition
So how far does it go? It goes pretty far
It goes to certain things that I think we thought, you know, we're a little bit untouchable in our human experience
And again, I'm just touch on this and we can talk about it in the discussion, but I think it really informs
How we understand
Moral artificial intelligence because this is actually quite tractable. This is actually quite quantifiable
We can actually go much further in understanding what works and what doesn't work in our moral cognition
Because we have this nice
Quantifiable computational theory and we can go much farther and understanding our moral cognitive experiences
And in designing those right so morality on a sore architecture is gonna suck
But it doesn't need to suck because that's actually not how it works in us either
And so I think having a handle on this opens some pretty interesting avenues even in the realm of artificial intelligence
So lastly, and this will be the fastest part
But the third question should it should this understanding then guide our normative decision-making and here
It'll definitely depart have from from rich. I think my answer straightforward Lee
That got formatted out of existence. Sorry
Is no so I take guide to be pretty strong
Guide means I should then use this to govern my normative decision-making
And the analogy that I often draw here is that moral cognition should be understood in analogy to folk physics
So, you know the Wiley Coyote case where Wiley runs off the cliff and hangs out there for a second and then falls right or
Other cases might be Muller liar illusions or putting a straw in a cup and things like that
We have understandings of physics that are produced by our sensations and our perceptions that we know to be incorrect and
If we rely on them, we will not land on the moon and we will
You know not make other scientific achievements, and I think we have to understand moral cognition as the equivalent of folk physics
It's full of errors now does that mean that the reward hypothesis is not useful
No, because I actually think it's really important to understand
Where our moral cognition reliably and in fact systemically falls down, right?
So this is have basically reduced to a bumper sticker. I used to walk past
Basically every day when I was doing my PhD, which says don't believe everything you think, right? It's you've all seen it
It's right, but actually that's basically exactly what this is is we have very strong moral experiences hatred judgment
empathy
But we shouldn't believe all of those right but what the reward hypothesis and what reinforcement learning along with many other
Contributions allow us to do is understand the mechanisms that generate those things that we experience is right or wrong
And I do think that's powerful. I wouldn't use it to guide normative decision-making
I wouldn't say okay. Well that I guess that's what we're doing
But I think we can use it to inform it in the following way
So one of the things that I have suggested is that we should have a kind of fault line approach
So when I say fault line, I mean something like tectonic plate movement, right?
If you understand the movement of tectonic plates
You can make nice predictions about where earthquakes and volcanoes are going to occur where things are gonna kind of continually erupt
And I think if you can understand the contours of our moral cognitive decision-making
You can also start to see sort of the fault lines
We're not just gonna make token errors in our moral cognitive judgments
We're gonna make type errors in our moral cognitive judgments
and I think we can use that to inform then
The kinds of things because I think in in sort of everyday life
We really do take our moral cognitive experiences at face value with one another and I think that's a mistake
Whereas if we have a kind of causal understanding of what generally sees and where they might depart
Then that can be used as a kind of type remediation
Of what's going on not necessarily in a policy sense or anything like that
But I certainly think you know, it's useful knowledge to have it can inform us for the better
I think it's good to know kind of the contours of the mistakes that you're making
But I wouldn't say guide because of the way these things are generated
So I think that might be a place where we depart
So yeah, I think it's pretty productive
Thanks, Julia and and rich that's just terrific. I'm so glad I got you both on to this conversation
And and I I have a couple of questions to get us started, but also
Ready to sort of follow with different directions. You'd like to head
I heard two things perhaps that it might be worth exploring one was pleasure and pain
And rich I don't know if you want to respond to that one and the second one
Oh, which I'll come back to is is the guiding normative decision-making, but let's take on pleasure and pain
Is that an important part of how you're thinking about this to me the pleasure and pain are just examples of what reward might be
Well, I think it was the play-doh slide. Yeah. Yeah. Yeah. Yeah. Yeah, he was clearly talking about pleasure and pain
Reward could be pleasure payment. It's undoubtedly more complicated than that
And
I agree with you that we don't know what our word signal is we we don't have
introspection
introspective access to it
And and more than that
What we do have an introspective access to it is the
I
Want to call it TD error remember it's the thing I was talking about is reinforcement
It's which is which is this measure of how do we feel about how well things are going this as you're saying the
Constant sense of our things getting better or worse. We evaluating all the time
So we have access to our evaluations and we we have very
Preeminent access or prominent access to our things getting better or worse
But things getting better and worse
if you remember it's the reward plus the change in your evaluation function and
So these two things the reward and the change in your evaluation these two are mixed together
And so when you feel good or feel bad, you don't know if it's you don't really know if it's because the reward was high
Or the change in evaluation was high
and so
So that is a confusion that we all have we don't know
Once so once an evaluation a value function becomes very well established then, you know, it feels
Bad when our values are violated just as if we had you know a form of pain or a bad thing directly. So
This is this is a major phenomenon, you know in terms of our
subjective experience that it's hard to tell
Difficulties in a reward and a change of value
Doing anything
Yeah, I would say that and again this is
Following Kent Barrage. I think pleasure and pain are sick, you know are evolved biological signals of
reward and value and
As signals, they're pretty good, which means they go together a lot of the time
I think one of the cases that the barrage talks about is the case of for example a digger
This might be David Reddish talking about cases of addiction to heroin which where the signals
and the reward and disappointment come apart so
You very clearly start to have not pleasurable experiences
When you will become a heroin addict, but the reward is so strong
That you continue to do it right heroin is basically ruining your life
But because it's hijacked the reward mechanisms and you continue. So these are kind of edge cases
but I mean, I think one of the powers of
Like the reward hypothesis, but also the scientific process and that discovery that you're talking about with Peter Diane and others
Is that it really gets at an important part?
Of of our kind of mechanistic composition that we don't have direct
introspective access to you
But where there are departures between what a hedonistic or pleasure and pain-based view would predict about us?
And what the reward view would predict about us and and clearly the reward view makes better predictions is better supported by the evidence
and so
It's kind of funny saying this to you
But I would say like there's no reason to lean into the pleasure and pain because like reward actually provides a
Better model in some cases, even though they you know, they often walk together
So you're thinking it's important to keep the distinction
I mean the reward
way of talking is more abstract and
And
If it was equated with pain
Pain and pleasure then it becomes more specific and
And that they're there for falsifiable
Which is good
Yeah, so I'm totally on board of course with it's a general thing it doesn't have to be pain and pleasure and there may be cases I
mean, I think I
Guess that totally makes sense to me like there may be pain and then maybe you could
Change so that the pain is less important to you or the pleasure is less important. Yeah, it's just more general
The reward is more general
I think another example might be the emotions where again we have introspective access to our emotions
And in that very hastily shown chart, right? I think there's incredibly close coupling
Between our affective responses fear anger and so on and the valuation component
But you might ask yourself, you know, how do you know I lost that between the the emotions and and the
Valuations, yeah, and so if you are you have a component theory of affect or emotion
You say that there's an appraisal component of the emotion, right?
What triggers certain emotions? Well on my view would be something like the valuation system the reward system
But I think you lose a lot if you only look at the introspective component. You say it's all emotion
Emotion is important
But actually the emotions are kicked off by the valuation or mechanism by the valuation assessment that then triggers these
ballistic responses like empathy or fear or anger or so on so I think again, they're they're very close
I don't know if bad fellows is the right word like these things really run together
But I do think that you come up with cases that reward accounts for and the emotions do not and that there are cases where
reward explains the phenomenon and pleasure and pain do not and so I
Do you think it's you know, they're obviously very closely related but something is lost by blurring between those two
Maybe I
Of course economists of which I am one
Have thought of and debated about these questions a lot because we represent a utility function
Most of the modeling is done with the idea that you can take all decision-making and put it into a
Continuous function, but I want to press a little bit because I hear in Julia's framework
Again with this concept this broader concept of valuation to say like your brain may actually have many sources
many valuations schemes that aren't necessarily
Representable as a continuous
Scaler reward and so I wouldn't want to push on the scalar part because I think that's a difference between
What you've presented
and
That's the economics agrees with the scalar
Yeah, you do. Yeah, I do. All right. I'll be the disagreeer. Yeah
So there's a nice paper by
I'll come up with the name in a second
Which just basically suggests that you can have these vectors that reduced to the scalar
and so that there are kind of mathematical ways of
Taking that kind of consideration and still having it reduced to scalar and you can say like whoof the theory holds that it's scalar
And so I think I'm probably less invested in this part of of it
But that would be the way that I would appeal to it is that you can accommodate that kind of intuition and still preserve the scalar feature of
The theory, but you might want to defend it like more
The original work arguing that you can reduce the vector to the scalar is in second is in economics
I
Morgan Stern and
Somebody rather you know, well, yes, it's hope and see there's a debate in economics about and and but by far the dominant
Neoclassical economics is that you can you can do that. Yeah, but there's also a different project in economics, which is
We're trying to build models that will predict the way
humans and
Entities composed of humans like markets will behave and so then when you say it's enough
You're saying oh, it's good enough to be predictive
And so one of the things I think about is how much are we taking that and now saying well, this is the way
Valuation works in
Human societies in the human and in human societies and I guess I want to put a wedge in there to say
You know, it could be that I could predict pretty well even if values are incommensurate and can't be reduced to a trade-off
Nonetheless at the end of the day. I'm just predicting are you gonna choose this or this so I could have a representation
That worked for that predictive exercise
so
All bets are off when you have groups
you know the the the basic theory is the theory of an individual and and
Yet and yet we want to move to say something interesting about
About groups or to ask if it's possible to say anything
normative
or
Yeah, can we say something normative normative, so what does this mean this mean I
See I might any definition I'll give will be reductive again, so
So how can we walk up to this so one way to walk up to it is the way Julia did which saying well, maybe these
The moral decisions
Are are not that different from the ordinary decision-making we do it every day
And to that to me that makes sense
And so I mean I want to push and say that for the individual we can think of it all as one system
Now to make it a little bit more acceptable to you that it might be that way
You want sure you remember that the the value judgments as well as the policy
But let's the value judgments are compiled. They are they are they may they may be due to learning or reasoning
but in the end they are they are they are like
they're
Hard-coded and they're automatic and they're intuitive. They're not the reasons for them are no longer present
that's so I see the reasons you might have a very firm feeling that that
Children should have a loving parent and
You might not have
Any more have the reasons for that belief you just believe it really strongly and and maybe maybe there was a reason for it
Maybe you had experience or you've seen people or you've done some reasoning process and you came up with a reason
But but now you just know you have this belief and so this is why we tend to think of values as core things
And and more emotional
Because we we don't have the reasons now it doesn't mean we lost the reasons
You know, maybe the reasons who are built built into us by evolution to me
It doesn't really matter. You have ended up with a judgment about the the the goodness or badness of certain things and and
Nevertheless a good way to think about them maybe as a prediction of the likely rewards
For being in that state or acting in that way
And to me that that's a really exciting
Apothos that you might be able to think about of all the judgments that people may including their moral ones in terms of a single framework
So I want to separate that is that impressive and I feel you from your talk that you were like
trying to lead lead us that way and
So I want to separate that there are two big things to talk about is that makes sense. Okay, and then
If that makes sense
What about when you have multiple agents and and then and we asked the question of universality and normative judgments
So are we at the?
Yeah, are we ready to have we have we done accepted the first thing and are we ready to do the second thing?
Are we still unsure and I'm I'm gonna ask you, you know, everyone, you know
Anyway, these these these two steps
Are we ready?
So can I just jump in on that real quick because I think there's a step between is that okay? Yeah, no, that's great
And then I'll go to question. Um, so I like this idea of reasons are no longer present
So on the slide and have time to talk about it, but I had this case of moral dumbfounding. So moral dumbfounding is when
You give people scenarios
like, you know
You know Jane and and John are brother and sister and and they would like to have sex
Just the one time with protection. So, you know, there's no possibility of any of the genetic consequences
of, you know
incest and so on and so forth and you ask people like, you know, is this morally acceptable and you know
and
You ask them and they'll give you kinds of reasons, but they they can't really so it's this dumbfounding case where you're like
Yeah, okay, there should be everything fine with this. The other cases are like having sex with the chicken
Anyway, there's some stuff going on in the lab there that came up these examples
But there's strong cases we have these the responses and I really like this way of putting the reasons are no longer present
Right, the reasons are good reasons
But they're no longer present in this case because if if John and Jane or whatever I said use contraception then, you know
This this evolved reason for why we're opposed morally opposed to incest is no longer there
But that's actually a really important
Junction, right? That suggests that those intuitions
Maybe over the course of evolution are a good
signal of reward or not, but that there are departures from that
And that's where the normative
Story comes in so we have these descriptive experiences like no, but the reasoning and the moral philosophy or the ethics or
Policymaking or all these normative disciplines law economics come in because we tell ourselves, okay
But shunning the outgroup may no longer be doing us a service because we live in a global cosmopolitan society where
We all want to get along
And who's getting along there is all of a sudden a much bigger group
And so I think you can have a part of that first story which is radical in and of itself
But not necessarily follow it all the way to step two
Because sometimes we believe things that are just
Silly like you shouldn't have sex with a chicken
So I I'm just I'm gonna I'm gonna just throw out there and then I'll come to the audience
and I may want to put Joel and will on the spot on this one as well, but
You know the the the idea of thinking about the values or as or the as
Intuitions as
Some kind of thing that is just there
We don't know where it came from it accumulated from arts as opposed to which is what I really like about your framework Julia is
It's a very active process and actually so I make the claim in the work that I'm doing thinking about normative systems
And I have a very very reductive definition of normativity
It's what the group labels and we just have a labeling scheme
This group says this is okay, and this is not okay, and we've you know coordinated a cognitive structures and institutions
And so on to to produce that but it's adaptive it changes
We get angry about rule violations
But you can go to a different environment where the rule is different and you used to get mad when people came into the building with short
shorts on and now you don't
Right, and so I think it's it's it's very much a
Processing of the information from the group
So it's a group thing
It's a pick a particular group all groups don't have to agree
Even a single group may change its mind over time
Yes, and that that's actually a complex system to understand how that how that functions
And the silly rules about chickens for example or something that thought a fair bit about
But I want to make sure we get because I know that there's lots of provocations for our audience here to to contribute on so let me
Let me open it up here. I'm gonna go to Jennifer first
Yes
Yes, and do wait for the mic
So so this is sort of following up on Julia's claim that minds
Subpersonally a sign reward, and I don't disagree with that. I think absolutely we do that
I'm curious about whether we also personally do this and whether it's not an expression of our
Autonomy as rational agents capable of setting goals that we do this
This is also my reservation about the sort of McCarthy definition of intelligence as
You know being about the computational part of achieving goals surely we can apply our rational intelligence to setting
Goals, I mean we don't have that much flexibility with respect to the sort of homeostatic needs that are
Encoded in us as biological creatures
But but beyond that we're on a long leash from evolution and we can surely in
Rational conversations with each other work towards finding
Goals that stand the test of
You know
rational
scrutiny and
And and and so that kind of that kind of expression of our freedom
Intellectually seems to me to be a very very important application of human intelligence
We're not just slaves of our reward functions, you know just looking for ways to maximize
You know, whatever this damn thing is in us
We get to say what we're pursuing and that seems a way in which you know our intelligence is
Caught into play
Isn't there a famous philosopher who said who said that our reason is a slave to the passions
David Hume
Reason is and ought only to be a slave to the passions
Yeah, but that's you. Yeah
And he's obviously wrong about that so so I'm very much on this. I'm very much on the side of the manual count here that we
That that what is what is right is what is universalizable?
there are so so so you know the the the maximum underpinning your conduct should be something that
That is universalizable that could be could be a law for all
There's there's an important contribution of the inclinations in here, right?
So we have certain natural inclinations that we have to satisfy to continue our existence as the kinds of biological creatures
That we are and we may have to discover things about people's inclinations empirically through interactions with each other
but
But the question of what we're inclined to do is
Sabrable from the question of what we ought to do we can criticize our inclinations
I could find myself inclined to do stuff that that I think is wrong actually even just kind of at the level of biological fitness, right?
there's you know various
appetites that I have towards you know sweet and salty foods and so on and I can I can appreciate
rationally the reasons for which I have those inclinations and
Actually reject them as you know in the current environment not contributing to my fitness. I can overrule them
And that's not something that you know reckons can do
That's why we're more intelligent than they are
My sister literally had four raccoons in her garage this morning, so
Okay, so obviously like a big question, so let me kind of try and step through that
So we were talking about this a little bit yesterday, so I would say that I'm a motivational involuntary
So I'll give the example of coffee, but cigarettes might be a good example for other people
Over the course of my experience. I have acquired a very strong
liking or a cup of coffee and
That guides what I do, but again coffee is kind of a trivial case. Oh and Jennifer
I definitely put an asterisk there just for you because I was like
So in my defense
Now I can't just reach in and say I'm not going to need that cup of coffee tomorrow
I'm not going to need that cigarette and I I can't say either when you scale it up to some of my moral responses
Maybe less savory moral responses. I'm just not going to judge that way or I'm going to do that thing
So I don't think we can overrule them in the that that sense
And I think that's basically I'm an early modern nationalist pre-con that I think that they're they're thick causal mechanisms
Of course, we're not raccoons
And we do have reasons we can make
Outs as groups and and to the the group
You know what what is normative is for group? I think one way to kind of start picking at that is that some groups are just better
They have better principles and better norms that work better for the group than others
So that should be a kind of clue that
It's not you know that there there might be a kind of normative dimensions like okay
These institutions that in fact work better towards the goals of us getting along together
Close-eyed bar. So what do we do when we have this involuntarism?
but we also come equipped with the raccoon plus of
Knowing that there is a better way. I think we have relatively indirect paths
To let's say diminishing my reward for coffee. So what I can do is I can
You know drink a glass of salty water every day that I have a cup of coffee and
Indirectly ratchet down
The value that I place on coffee or the value that I place on the smell of a cup of coffee and so on and so forth
So it's kind of an intermediate view where it's like I think we can leverage our reason
But it's by no means as easy as some of us would want
To hope that it is and I think it's important to emphasize this end of the debate
I mean again, it's always a question of emphasis, but it's important to emphasize the difficulty because I think if we don't
We think we could just pull the lever and I think that leads to all kinds of
disasters basically
So I want to
React as well
I'm really glad you asked this question because I think it's really the heart of the whole thing
It's you know, is it demeaning to think that there we have there they have something that's a signal coming in
I'm gonna call it pleasure and pain because it
It goes with it with the demeaning aspect to it. We are a slave to our pleasure and pain and it makes us seem small
And like we don't have choices and these are the most important things in our lives
What are what we're trying to do with our lives and that we the idea we don't have
Control of them is really annoying
But I think it absolutely is true and it's true in kind of a definitional sense because
When you say you have a reason for something you mean I do this because I'm trying to do this and
Obviously throughout our lives. We have reasons like that. Why I'm going to work to earn money. I'm trying to earn money. So
Raise a family. I'm trying to raise a family because I enjoy certain activities and I want to see things happening
We all we have many many reasons and we're used to that
But we also should acknowledge that eventually it has to stop, you know, you have reasons for things
But eventually there's some final thing that doesn't have any reason for it or at least that's
Maybe the standard way to think about things in science. I don't know, you know
It's the old thing about the odds and the is is you can't derive
By learning about the world you cannot really derive what you should want
there's there is
There is one standard philosophical view. I can't do it justice
But but it's this the view that that you can't derive an ought from it is and that so you have to have some ultimate
Oughts that are given and those ultimate odds are the things that we can't choose. So there must be something that we can't choose
And that's what the reward is meant to be. It's the one thing that we can't choose comes from
outside
And it's an outside just means we can't choose it
And so since that has to be true, I think even though it feels
demeaning
That we should get used to it and stop feeling that it makes it demeaning
We have something that we ultimately want and and so we're ultimately and it's individual in some sense
selfish
And I think
That if you embrace that it's kind of liberating, you know different people are different. They want different things
It's okay for them to pursue different things
Doesn't make us bad people
I'm going to jump to another question because I want to make sure we get a few more people in here Sheila
I
Thank you so much to all three of you for for such an engaging
discussion, so I was I
Guess I wanted to start by just
Harking back to a conversation that that rich and I had yesterday. I think I and in defense of John McCarthy
I think that McCarthy in 1955 defined a notion of artificial intelligence and and that that intelligence is is a vague term
Just like our conversation yesterday about about baldness, you know
If I start pulling hairs out of my head at what point do I recognize that it that I'm bald
Whereas with when I put that hair back in I'm when you call me this way. Yeah
Anyways, I think there are lots of terms that are vague and I think that people define them purposefully
And I think that intelligence is one of those so
But what I but I wanted to I was really struck by by Julia's
Discussion of binocular rivalry because I feel like I'm having binocular rivalry, you know between the two of you
And and that we're all just trying to make sense of this this complicated idea
And and we and again to a point that Julia made you know
What we see the way that we interpret the way that we make sense of the world is indeed informed by by some sort of
Bias we have or it's sorry. That's not the word that you use but some sort of expectation of what we want to achieve
So so what I wanted to just to
To to make sense of myself is
Where in your conceptualizations of things you you absolutely disagree or where you actually where we're some of the
Disagreements we see and even to Gillian's question about the reward hypothesis is just about what we actually
Decide to highlight in our formalization or our understanding of things like so
Rich even when I was looking at your your decision-making box on the right, which is one that and one of your slides
Which is one that I'm very familiar with, you know, I thought where's memory? Where's long-term memory everything's been compiled
You know, we're from classical
Decision-making where's the notion of a you know of a of a that predictive mechanism is actually being not sort of compiled into the value
function or everything being compiled into
Scalar reward to Julia's response to to to Gillian and and so I guess I want to push back on on saying
What are the elements that are important? Where's long-term memory?
Where is where is this notion some of the the elements that actually contribute to a reward eventually being compiled into a scalar reward?
And and is that diagram for you on the right?
really representing everything that that it needs and then I guess
Point and and and again, I thought about things again that we think about in the context of multi-agent systems about about
Goal-seeking, you know the ability to create goals about
Commitment about intention all of those elements that we often use to make sense of the world that that are not part of your diagrams
Probably we can they're compiled in somewhere in there
But I think there's utility in including them
So what what do we have are there things in your formalisms that are just compiled away that are important and then?
I guess the other question was where do we fundamentally or where do you in your binocular rivalry?
fundamentally
Or my my sense of you're the binocular of rivalry that I'm having
fundamentally disagree and and and I was and maybe this is a
Conversation to take offline and for afterwards
But one of the things I was intrigued about is this you know what what I perceive is our perhaps our ability to be able to reflect on our
Art to have an awareness of our reward to be able to reflect on our reward as as as we do
Representation other representations and whether that's an element that that that that is of utility and explaining some of what we see is
Understands the mind. So that's it. Well, those are a lot of things
Julie is writing down her thoughts
I'm not writing so I have to go first because
I'll forget
There are many things I will I will forget some remind me if you get them wrong
First memory long-term memory
There were four boxes all four boxes learn and so maybe they're four boxes are for
Maybe they're for neural networks. You that'd be a fine way to think about it
And then they all have weights and those weights are all adjusted through
Learning and planning both so that they would all have long-term memory. Memory is throughout
So I want us
We can get a sense of freedom. We get a sense of intention
We can set a sense of choosing goals because I do think it's a really major
Psychological thing that we choose goals. We choose except they're not they're not they're not reward goals
So maybe they shouldn't be called goals. They are sub-goals. They are
strategies
that
That you choose as part of your solution of the problem the problem is fixed the problem is reward
The problem is is it's the reward is not a compilation of something. It is the it is the thing that is unchanged
It is not a compilation of something
other things are compiled like by
Learning what gets rewarding and what's not rewarding you decide what to do by learning what's rewarding. What's not rewarding you?
You compile you compile into the policy you compile into the value function
You even compile into the model and you compile into the perceptual process where you're figuring out
Oh, this is a good way to think about the state that I'm in or this is a bad way to think about the state that I'm in
all those are learned and in that sense compiled and
If you want to look at reasoning reasoning is in the model of the world we learned that you know this causes that and
that that that
Reasoning that that knowledge of the world is in multiple levels low levels low level physics and high level decisions like like oh
I'll go to the conference or I'll take this job
But there's no compiling in the reward signal or you know if there's any compiling it would be evolution compiled its goals into our
Reward, but for us
Reward is just a primitive thing and it's never never changed
And yet we absolutely it's appropriate to have the sense that we choose our sub-goals
And that's the most important thing we make I'm gonna I'm going to choose to study
Economics or
Or our philosophy
Or I'm going to choose to devote my life to understanding how the mind works. Okay, those are and I'm thinking
I'm not thinking about this, but
Maybe if I devote my life to figuring out how the mind works, I will enjoy myself better. I think that's what the way it goes
I will forget more reward
So I think yeah, there's also a multi-part and I feel like Joel basically asked you Sheila to ask this question
So in terms of the question of you know emphasis and binocular rivalry, you know
I think that's just the nature of scientific and philosophical theorizing, right?
It's a question of emphasis in a lot of cases
So depending on how we configure our audience
We would either be like aggressive opponents or allies depending on who we're engaging with and the point that you're trying to get across
There's just a sociological feature of that
But I think where things really start to come apart is what predictions
Do your commitments generate and what implications follow from your view?
And that's where you also start to see some departures between us
So just examples from the kind of conversation that we've had so far that I'm thinking about is okay
I think we actually really disagree on this pleasure and pain thing like we're like oh
No, we really disagree about that and it will generate different predictions
And it has very different implications for how we understand certain things about the nature of human beings and and how we kind of follow from that
I think we disagree on
This normative question of what then we should take away from these experiences of values that we have
It's funny because I wrote down liberating to but I mean it in a very different way
It's liberating in the sense that if you recognize your limitations, that's very liberating
So you accept the flaws in your system
That's actually very empowering to the way that I was mentioning with the coffee because then I can use I can leverage that to become better
and
again, so you'll see different predictions there about what works in terms of
Self-regulation and there will just be departures and in the kinds of commitments they have and then I would say in terms of
What we're striving for with artificial intelligence and how we should recognize them for example as people that would be another place where
You would actually see our commitments that look very similar in some places
separate because of
the implications that follow from that so I think
But it but it really is like there's partly a sociological phenomenon of who we're engaging with and who we're trying to persuade
Where in some cases we're a united front because I think the reward hypothesis
Is in a minority or can be in a minority despite how powerful it is
But then you know between one another we would see see big differences or something like that
Thank you, and this may be our last question we'll see yeah
This was a really fascinating talk I
Don't have a specific question in particular, but I do want to ask in the
Interests of internalizing reward and self-regulation. I think those things you brought up
If it's not beyond the scope of your research, I wanted to ask about reward hacking and
The idea of good-hearted law which is when the measure becomes a target it seems to be a good measure
I'm really interested in hearing if you have any thoughts on that
I don't know if I heard everything
But reward hacking definitely happens to people. Maybe that's what drug abuse is about
I'm not sure I understood everything you're asking I
Can repeat if
Yeah, so I was asking about reward hacking which is something you answered but also good-hearted law which is
When the measure becomes a target it ceases to be a good measure
If you're familiar with that law, I wanted to ask in the interest of self-regulation as something Julia mentioned
If there are thoughts on that
I guess good-hearted law doesn't apply because that's sort of a community thing
You know it gets the measure gets manipulated by actors
But if it's your own measure inside your head
It doesn't get get lost in its importance in that way
Self-regulation
I
Mean it's super important to see
Binocular rivalry the two different views. It's what that's the most important thing that's going on here that you know like oh
It's it's ugly to think that we're just doing one number of pain and pleasure or whatever it is rather than
The grand reviews we have ourselves
But they're both true
We have to self-regulate we have to decide what what is our defining goal
And it's it's we we have to decide but I've just said we don't have to decide because it's given and
It's in it and it's in us, but we don't know what it is and besides
It's it's there's many senses in which the values are more important than the rewards
I mean the rewards are the ultimate determination
But the values are what the consequences of that that come about in interaction with the world
In interaction with the world, I figure out that the thing I need to do in order to get the most reward I
Have to figure that out
The fact that the rewards are fixed doesn't help me
Doesn't inform me about what what I should do in the world to decide what to do in the world and what I should
Say to myself and say to others is my goal
That will be all about value and values come by
Through the complex interaction with the world and what is what is actually going to make me happier in the long run?
You know the rewards can't tell you that the words just tell you when you get there
I'll tell you if you're happy or not
They can't tell you what you should do and they can't tell you so you have to decide
You know which which career to have and and which person to be involved in and all those
Subtle hard decisions. It doesn't the fact that it's fixed doesn't help me make those decisions. You still have to do those
I
Think I probably also will and I'm looking at the clock will also only address part of your question
but um, I do think reward hacking occurs and people I think addiction is kind of the edge case of that
But I'm wondering whether you would also find there's a philosophical literature
Maybe just one philosophical author
On this idea of value capture and so I think one of the examples given in the phenomenon value capture is
people wear like a Fitbit
And the Fitbit tracks the number of steps that you take and obviously you need to take steps in order to maintain your health or like
whatever the man tries to tell you and
I'm just kidding. It's probably important and
But people become obsessed
With getting their steps. They're like they if they go for a walk and they don't have their Fitbit
It didn't count and that's an example of value capture. That's obviously like a very
Sort of relatively cute example of value capture, but I think this is something that happens like
Professionally, right? It happens like if we have certain goals like, you know, the pursuit of knowledge or contributing to something, you know
But we value capture in in publications or whatever, you know, lots of cases across not just an academic phenomenon
This is something that happens
Across different aspects of human life. I don't know if that's exactly what you're getting at but
In terms of self-regulation, I think understanding again the understanding is extremely powerful because if you recognize what's happening
You can regulate around that. So I think just based on our conversation a little bit yesterday based on your interest
I think I wonder I was wondering whether value capture might be something that you're also interested in looking at
Which is just a kind of nuanced version of how that happens in everyday life, I think
Thank you. Well, this is we are we are at two hours folks. So we're gonna we're gonna call him here
But we have a 30 minute a
30-minute coffee break now
So I'll invite you to to join us and I'm actually gonna ask that you let our speakers move to the coffee room before
Asking them questions so they don't get trapped here, which I've started to notice and do come back with us at 11 30
We have Abby Goldfarb Daniel Daniel Rock and Frank British on in machine learning in the workplace, which will be as as riveting as this one
Thanks so much also
You

All right, well, Noam Chomsky, I think it's time for me to turn this over to you.
I really want to give you a great big welcome from OliU of A and appreciate your coming today
and presenting to our organization and we're truly an honor and a privilege to have you
with us and I know that a number of emails have gone out and I don't think any introduction
is honestly needed because but can you hear me thank you for joining us and I'll let you
take it over.
Okay, thank you very much. I'd like to have a few words about the maker of language,
its acquisition, origins, its use and so far as we understand these matters today and I'd like to
approach this within the broader context of the inquiry into language and mind which are
the distinctive properties of the human species. Well, good place to start is at the beginning of
their historical records, classical Greece, the Delphic Oracle, which produced the maxim,
know thyself. Well, two word aphorism is open to many interpretations. The intended
interpretation was individual, you the individual should know yourself and unexamined life is not
worth living, the platonic expansion of this, but it makes good sense to understand the maxim of
the oracle collectively, know ourselves. The reason is that we're pretty much similar. It's a
very young species. Humans have only been around for two to 300,000 years. That's early in eye
blink and evolutionary time. It's been mentioned by evolutionary biologists, partly as a joke,
partly real, that if you see two squirrels in a tree outside your window, they very likely are
more diverse genetically than all of the humans in the world. Humans have arisen within a very
narrow window of evolutionary time and they have unique properties. One of these properties is
human language. There's no analog, no similar properties anywhere in the organic world.
Furthermore, it's a species property. It's not only unique to humans, but uniform among humans.
It means that any infant can acquire any language with equal facility, as far as we know. So it's
a species property, virtually definitive of humans. There is another one thought, at least thought in
any form that we can grasp and understand. The kind of thought that made it possible to produce the
maxim of the oracle, to interpret it, to reflect on its significance, myriad other mental activities
that occupy us all day, even in sleep, just again a common human property, totally unique to humans.
Well, when you have two unique species properties, it makes good sense to ask what relation holds
between them. The simplest relation would be simply identity. Language constructs thought.
Thought is what is constructed by language and that is the conclusion that was reached
2,500 years ago, classical India, classical Greece, and it's a tradition that runs through millennia.
Late 19th century, a great American linguist, William Whitney, simply defined language as
audible thought, basically thought, which we happen to externalize through the mouth,
pick up through the ear. We now know that that's too narrow. You can use any other sensory motor
system. So if I don't know if this is being translated into sign, but if it is, you would be
seeing the same thing in sign language, which works very much the same as spoken language,
in the way it's acquired structure, the way it's used down to small details,
even the way it's norally stored. Well, you can think of the language as being
kind of like a program that's in your laptop. So suppose you have a program in your laptop,
calculator, or say, multiplying numbers. The program works by itself. You can attach it to a
printer, could be a printer that uses colors, dark letters, and one font, another font. The program
doesn't care. It can be hooked up to, sorry, try to be louder. It can be hooked up, is this any better?
Can be hooked up to any printer that you want. The program stays the same. And language seems
very much like that. There's an internal system constructing thoughts. It can be hooked up to
one sensory motor system or another to externalize it. But the program seemed to be the same.
Well, let me make a side comment here on the history and then return to the
mainstream of the discussion. What I've just been describing is basically the long millennial,
long tradition of the study of language in mind that goes right into the 20th century.
Early 20th century had changed. The general view in the early 20th century still largely surviving
is that language is not primarily an instrument, the means of thought, but rather it's a system
of communication. That's sort of the mantra today in philosophy of language, much of linguistics,
cognitive science. This, I think, is a result of the rise of behaviorism in the 20th century,
which drove scientists, philosophers, others to look at what's visible, what's apparent,
behavior, to keep away from hidden things like what's going on in your mind. That was a very
sharp departure from the scientific tradition. It's very anti-scientific in my view. It became
quite dominant, especially in the first half of the 20th century. There's also a kind of a
primitive version of Darwinism, which does have its roots in Darwin's writings,
the idea that all changes through evolution must be small changes, which operate through
natural selection and over time may give large-scale differences. It's now known that that's
completely false, but the residue of that does lead you to the belief, the illusion in my view,
that language somehow must have emerged from simpler systems, going back to
animal communication. The evidence is strongly against that, and there's nothing in
evolutionary biology that leads to such conclusions. Actually, this is,
notice that this behaviorist turn is a turn away from the effort to understand and explain
in effort to describe basically what you see in Silicon Valley linguistics, the kind that makes
the headlines about the things that it's claimed that machines can do. Associated with this was
conception of how language is acquired, dominant conception, philosophy of language,
linguistics, language is acquired through training and instituting habits,
if there's anything new that's said by analogy, all completely wrong, completely reshuted,
still very common. Well, let's go back to the tradition, which I think was on the right course.
The study of language took a large step forward with the 17th century scientific revolution.
The primary lesson of the scientific revolution was that you should be puzzled by simple things
that you take for granted. So, a heavy ball of lid plainly falls faster than a light one.
Except that it doesn't, as Galileo showed with elegant thought experiments,
take a sailboat moving through the sea. I suppose there's a lid ball on the top of the mast.
If it falls, it should fall behind the mast. Of course, the sailboat is moving forward, except
that it doesn't fall to the base of the mast. Again, Galileo showed this with thought experiments.
If he'd done an actual experiment, the results would have been all over the place. But as in
much of science, then and since, the idea was to construct ideal situations, often by thought,
sometimes by careful experiment, if you can, that bring out the hidden reality between behind the
phenomena that you see, which are always complex. Many variables can't be studied. That's why
scientists do experiments. Well, if you go back to scholastic science in the 16th century,
what Galileo and his contemporaries were struggling against, scholastic science had answers to
everything. If a ball falls to the ground and steam rises, it's because they're going to their
natural place. If two objects attract or repel each other, it's because they have sympathies
and antipathies. If you perceive a triangle, it's because the form of the triangle
passes through the air and implants itself in your brain. So there was an answer to everything.
All the answers were wrong. They were based on what modern science called occult qualities,
invented concepts that don't exist, and the willingness to be puzzled by things opened the
door to the new science, the science of the last several hundred years. The scholastic science
had answers to everything. The new science had no answers to anything. That's how serious inquiry
begins. And the same has been true of the study of language in the last 60 or 70 years,
breaking with the behaviorist, structuralist tradition. Well, let's take a look at language.
There are simple things that we take for granted. We talk to each other. We understand each other.
What could be simpler than that? Well, to the new scientists, Galileo and his contemporaries,
this seemed an extraordinary puzzle. Amazing. How can it be that with just
a few symbols we can produce, we can construct in our minds infinitely many thoughts,
never before expressed, never in the history of the language, and we can even
convey to others who have no access to our minds, we can convey to them the innermost
working of our minds. They regarded this as an amazing phenomenon. In fact, Galileo himself
regarded the alphabet as the most spectacular of human inventions because it facilitated this
astonishing quality. And it is indeed a remarkable fact. It's been repeated by others. One famous case
is Gottlob Frege a couple of centuries later, the founder of modern logic and logical philosophy
mid-19th century. He wrote that, I'm quoting him, he found it astonishing what language
accomplishes. With a few syllables, it expresses a countless number of thoughts and even for a
thought grasped for the first time by a human, it provides a clothing in which it can be recognized
by another to whom it is entirely new. It's essentially Galileo's insight. And it raises
very deep questions. How can this possibly be the case? This initiated a rich study of what
centuries study of what was called universal and rational grammar. Universal because it tried to
cover all languages. Rational because it was interested in explanation and not description.
Very different from the structural linguistics and philosophy of language of the first half
of the 20th century, which described itself as a taxonomic science. We have procedures of analysis
you can apply them to data that you come up with an organization of the data. It's basically today's
Silicon Valley. That's doesn't care about understanding, doesn't care about explanation,
just organization of data. A radical change from traditional science, including the science of
language. Again, the old tradition was picked up, revived around mid 20th century. Well,
it does raise the problems for Descartes in the 17th century. It was the basis for his establishment
of the classical mind body problem. Descartes accepted the basic idea of the new science, Galileo,
Royal Society in England, later Newton, others that the world is basically a machine,
a complex machine, the kind of machine that could be built in principle by a skilled artisan,
and that in their theology in fact was created by a super skilled artisan. Machine means something
built out of gears and levers and pulleys and so on. That's the world. But Descartes recognized
that there are certain things that didn't seem to fall within machine science, what was called at
the time mechanical philosophy. Philosophy meant science. So there were things that didn't fall
within it. One of the most striking ones Descartes felt correctly is the creative aspect of language
use. What amazed Galileo, his contemporaries centuries later, Freda, how is this possible
for a machine? It is impossible for a machine. And that led Descartes, a sensible scientist,
to postulate a new principle, a thinking principle, mind. That's the mind body problem.
Next task for Descartes and other sciences was to see how they're unified. Well, I quickly
ran into, sorry, somebody's at the door, my dogs are unhappy about it.
This collapsed pretty quickly. Isaac Newton's great discovery was that the world is not a machine.
There are no machines. There are no bodies in the sense, any sense of body that we have.
The world is just not intelligible to us. That was the criterion of intelligibility.
John Locke immediately recognized that, as he put it within his theological framework,
just as God has attributed to whatever exists, properties that we cannot conceive,
like interaction without contact. So God may have super added to whatever there is,
the property of thought. Thought is just a property of organized matter, whatever matter,
turns out to be. All of this was worked out extensively through the 18th century,
then forgotten, totally forgotten, revived in the mid 20th century. It's now become
a standard idea after a long period of neglect. Well, that leaves us with serious mysteries.
How is all of this possible? And the answer to that question is, we don't know. We do not know
how it is possible to answer Galileo, how it is possible for or frigate with a small number of
symbols to construct new thoughts, new to us, maybe new in history, to somehow transmit them
to others in such a way that they can interpret the inner workings of our minds. A lot of this is
just unintelligible to us. Well, there are parts of it that we can understand.
Here, we have to make a distinction, which was actually discussed by Aristotle,
but then forgotten until the 20th century. The distinction is between possession of knowledge
and use of knowledge. It's a crucial distinction in modern technical terminology. For those of
you familiar with that, that's the distinction between what's called competence and performance,
what we know and what we use. The internal language provides us with a store of an infinite
number of possible thoughts. That's our possession of knowledge. Our use of knowledge is perception
and production. When you perceive the noises that I'm making, you are accessing your internal store
of knowledge to provide an interpretation for these noises that you're hearing. When I produce a
sentence, I'm carrying out two acts. The first act is selecting a thought from the store of
possible thoughts. The second act is implementing it using the various mechanisms of my mind,
my articulatory system to externalize that internal thought. The second part of it,
we can study and understand. The first part of it, selecting the thought, is completely
beyond our understanding. We have no idea how this takes place. It's part of the general mystery of
voluntary action, which is basically to us an impenetrable mystery. Well, just keeping to what
we can hope to understand, the system of language that constructs thoughts, the formation of the
internal language, keeping to that, we can discover some quite remarkable properties.
Following the tradition of early modern science, let's take simple cases. Take the sentence,
the bombing of the cities is a crime. Notice it's is a crime, not are a crime. Actually,
that's puzzling when you think about it. Why doesn't the verb is agree with the closest
noun phrase? The closest noun phrase is cities. So why don't we say the bombing of the cities
are a crime? Okay, let's take the bombings of the city is a crime, or a crime, the bombings of the
city are a crime. Why don't we say the bombings of the city is a crime? Again, just using the
simple problem for property of adjacency. When you look at it, you see that what humans are doing
is ignoring the simplest possible computational procedure, adjacency, and instead using a complex
procedure, finding, not looking just at the linear order of words, but finding a structure,
and then finding the central element within that structure. It turns out to be pretty complicated
computation. But that's what we do reflexively. And in fact, that's what every infant does.
You can show by experiment that this is what infants do down to before the age of two 30 months
of age can't obviously can. And this is true of all constructions in all languages.
Just to give another example, a little more complicated, take the sentence,
the man who fixed the car carefully packed his tools. The man who fixed the car carefully
packed his tools. Notice it's ambiguous. He could fix the car carefully, or he could carefully
pack his tools. Now put the adverb carefully in the front of the sentence. Carefully, the man who
fixed the car packed his tools. Unambiguous. It means carefully packed his tools. And it's
unambiguous in a puzzling way. Carefully looks for a verb, but it doesn't look for the closest
verb. It looks for the more remote one, which if you build up the structure is the structurally
closest one. So what you're doing is what everyone is doing reflexively is ignoring
the linear order of words and attending only to structures that you never hear and that your
mind creates. Give one last example, take the sentence, the man who saw Bill is young.
Suppose I didn't hear the word Bill clearly. I say, I want to ask who it is. So I say, who is the man
who saw young? Jibberish. Can't say that. Fine thought. You can't say it. You have to, you can only
say you can't use the simplest computational procedure and find the closest occurrence of who
and put it in the front. Splock. Well, this is universal. As I say, children of 30 months old
know it, all constructions, all languages. What it means is the infant, all infants,
are ignoring 100% of what they hear, which is words in linear order. They're ignoring
very simple computations on linear order and they are reflexively paying attention to something
they never hear that their minds create and using computations on those abstract structures.
It's a pretty remarkable fact when you think about it and that is the kind of thing we discover
as soon as you try to answer the Galilean challenge. This property is called structure
dependence in the technical literature. Pretty much the same as true of learning of words.
There is a myth, standard myth, that the way children learn words is they see a cat. Their
mother says cat. They make an association between the thing they see and the sound your mother produced
and this is repeated over and over again in many circumstances, different situations. Finally,
the kid figures out that the sound cat goes along with that feline object. Totally false.
Nothing like that happens. There's a very careful experimentation on this by now.
Primary research and this is wonderful scientist, Lila Gleitman, who old friend passed away a couple
of weeks ago. What Lila Gleitman showed in her experiments is that children pick up words and
the meaning of words on one or two presentations. At the peak period of language learning,
around two years old, children are picking up words at about one every waking hour,
which means that they hear them once, maybe twice. They usually don't pay attention to
their mothers, just whatever is going on in the circumstances and they know the meanings of the
words. The meaning of the word, when you look at it, is very complex. No time to go into this,
but when you really look at the meanings of words, you find out that they are quite rich.
Well, actually, let me mention a few examples from classical Greece where this matter was discussed.
So, take Aristotle again. He took us as an example, the word house. He said,
what is a house? What's the meaning of the word house? And he said, well, it has two components.
One is what he called matter. It's the bricks, the timbers, the physical things that the
house is made up of. The other is what he called form, an idea in the mind.
The characteristic way the object is used, the thought in the mind of the architect who
constructed the way we conceive of it. So, something could look exactly like a house,
physically, but it could be a library, if that's the way it's used. It could be a garage,
could be a stable, could actually be a paperweight for a giant, could be whatever
we conceive of it, whatever the architect has in mind. That is a consequence, and that's quite correct.
That means that a physicist looking at the object couldn't know the meaning of the word.
That means that the child, when it's learning the meaning of the word, is using all of this
intricate understanding in the mind, none of it presented as evidence. Furthermore, that's true
of every word in the language. Let's take an earlier example, pre-socratic, heraclitus,
ask a rather profound question. How can you cross the same river twice?
The second time you cross it, it's a physically different object. In fact, it could be radically
different. It could be flowing in the opposite direction. It could be not water, but arsenic
from an upstream plant. It could have been divided into several trihitoris. It could be like the
river I pass on the way to work here in Tucson, where I live, the Rolito River, which never has
any water in it. It's a dry bed, but it's the Rolito River, because every once in a while you
get a trickle of water through it. Historically, it was a river. It's a river. It's all the way
we conceive it. You could take the Rolito River, start using it for commuting to Tucson. It would
be a highway, not the river. Physically, it hasn't changed at all. That's, again, true of all the
words in our vocabulary. One special case of it is called polysomy. I suppose I say
the book is harder to understand than it is to burn. What's the book? It's simultaneously
abstract, because you can understand it and concrete, because you can burn it. The world
doesn't contain objects that are both concrete and abstract, but our mind constructs them.
And that's what the words of language are. And these are learned virtually without experience.
Another, this one is a real mystery. We don't know how that's done, but it's a fact that has to be
explained. Well, these are among the amazing things that you discover as soon as you begin
to look at the facts closely. One conclusion we have to draw is that language, what we call
language, has two completely separate components. One of them is an internal language that constructs
thoughts, has no order, no linear order, just abstract structures. The second part is a system
that externalizes it into one or another sensory motor, medium, usually sound, could be sign,
could even be touch. Notice that that second component is an amalgam of language and sensory
motor systems. It's not pure language. Sensory motor systems have nothing to do with language.
They were in place long before language emerged. They have not changed since language was
emerged in the evolutionary record. So there's something extraneous to language,
kind of like the printer that your laptop could be attached to. Well, and that of course has
linear order and does use adjacency and other things. Well, when you try to find explanations
for language, you, real explanations, you face a kind of conundrum. A trial learns language with
virtually no evidence. Actual statistical studies of the data available to children
show that it's very sparse, very little. And as I mentioned, they know things for which they have
no evidence at all, like structure dependence. So that makes it seem as though what's innate,
internal to the mind, must be very rich in order to carry the child from extremely limited data
to very rich possession of knowledge. So on the one hand, the learning, the internal structure
must be very rich. But then there's another problem. This system had to evolve. Well,
it evolved in a very brief period of time. We now know from genomic analysis that human beings began
to separate at least 150,000 years ago. And they all have the same faculty of language,
same possession of knowledge, of language, same possession of knowledge. So it was in place
before 150,000 years ago. Well, that's not long after humans emerged. As I said, humans emerge
maybe 200,000 to 300,000 years ago. That's a very narrow window in evolutionary time.
Click up an eye. Before humans emerged, there's no evidence in the archaeological record
for any kind of symbolic activity. After humans emerged pretty soon in evolutionary time,
you had very rich symbolic activity. Ordinarily assume plausibly that this is associated with the
emergence of language. Well, what that seems to mean is that the basic
faculty of language must be quite simple since it emerged so quickly in evolutionary time.
But now we have a conundrum looking at acquisition. Looks like the internal innate system must be
very rich. Looking at evolution seems like it must be very simple. I should say the first problem
acquisition is sometimes called Plato's problem. Plato did raise the question,
how can we know so much with so little evidence? Didn't really have an answer.
The second question is sometimes called Darwin's problem. How could we evolve such a system
so quickly? So it must be both very complex and very simple. There's a third problem,
whatever the faculty of language is, it has to be universal. It has to cover all languages,
because an infant has equal access to all of them. Well, it's this collection of problems that has
guided the inquiry into language for the last 60 or 70 years since the abandonment of the
behaviorist structural restrictions and the return to what had been the tradition.
And it seems that finally in the last few years we are getting to the point where we may have
answers to these questions. So that, if it were to the extent it works out, is actually a new era
in the long millennial long study of language. The first time when we can reach genuine explanations,
satisfying, answering Plato's problem, Darwin's problem, and the universality of the theory.
So let's look first at the universality. It seems recent research is increasingly
attending towards the conclusion that the diversity of language is in the externalization
and superficial things like choice of lexical, the way you, the sound you associate with a lexical
item. So it could be, you know, Cat in English, Cha in French, something else in some other language.
But that's superficial. That, of course, you can learn easily. But the concept, the
meaning of the word that's already inside. So one aspect of variety is just
association of sound and symbol. That's easy. There are others. There are many, the variety of
languages looks on the surface, very large. So there are some languages. English happens to be a
what's called a highly analytic language. Lots of simple words from one after the other to make
a sentence. There are other languages at the opposite extreme. A sentence could be one word.
And other words could be scattered around freely. Well, it's by now been learned over recent years
that these apparently radically different languages are almost identical
at the deeper level. They have the same internals, very much the same, maybe exactly the same
internal structure, the internal language, the one that generates thoughts looks as if it's
pretty much the same, maybe exactly the same, no matter how different the superficial appearance.
I think that's not too surprising. The internal language can't be learned. We have absolutely
almost no evidence for it. It's true of the meaning of words, true of things like structure,
dependence, other major properties of language. So it makes sense for it to be
identical among people or close to that. On the other hand, the externalization is not strictly
language. Remember, it's an amalgam of language and sensory motor systems,
which are totally divorced from language. Well, connecting these two things is
fairly complex process. Two things that have no relation to one another can be done in many
different ways. It's basically solving a hard cognitive problem. Different communities have
solved the problem in different ways. So it looks complex. It's a complex problem.
The internal language looks quite simple because it's based on very simple computational procedures.
So on the one hand, we solve the problem of
Darwin's problem. We have a very simple system. Solve the problem of
Plato's problem because the learning is all in the superficial part. The internal part just
isn't learned. It's just there, like structure, dependence, the ways of interpreting those sentences
that I gave. And the variety is where you expect it to be in the complex problem of
relating the internal language that produces thought to the external system that
translates, that hands it over to the printer that allows other people to read and see and
interpret it. To the extent that that can work out, you have genuine explanations.
Well, that carries us on to the next chapter. How has it worked out?
I'm going to give the barest hint of that here. So let's take the major principle of language,
structure, dependence. Pretty remarkable principle. I wasn't even noticed until
fairly recently in all the history of language. Again, the fact that all operations of the
internal language, not externalization, all operations of the internal language that generates
thought ignore linear order. There's no linear order there at all. Just abstract structures
created by the mind and somatic interpretations, like the ones for river, house, book, every word
you can think of, which are constructed by the mind. So let's take structure, dependence. How
can we account for this? Well, we have to show that when language evolved, it mother nature
hit upon the simplest possible combinatorial operation. The basic property of language,
all languages, is that each language is what you know, what's the knowledge possessed is
an infinite array of sentences, which are structured expressions, hierarchy of structures,
and can somehow be assigned an interpretation as a thought, and by the externalization system
mapped on to the printer sensory motor organs. That's the basic property.
It's property called recursive generation. You have to generate an infinite array of expressions.
Well, the understanding of how to do that developed really in the early part of the 20th
century, as the theory of computation was developed by leading mathematicians, Kurt Gertl,
Alan Turing, Alonzo Church, Emil Post, established the modern theory of computation,
which shows clearly how a finite object like your laptop or your brain can store within it programs
that can yield an infinite array of objects, expressions in our case, and do it in a way
which could be hooked up to a printer. That allows us to ask what are the operations that
mother nature would have hit upon as soon as some accident took place in the evolutionary record
a couple hundred thousand years ago, and the property of recursive generation somehow appeared.
What's the simplest way to deal with it? Well, the simplest computation happens to be
what's called binary set formation, forming a set out of two elements, not changing either of them.
You repeat that over and over again, you get hierarchic structures indefinitely.
So a plausible evolutionary scenario is some accident took place, maybe some small mutation
out of it came. A process of recursive generation mother nature then picked the simplest one,
binary set formation, what's called merge in the recent literature,
and then everything flowed from that. Structure dependence flows from it immediately.
Many other properties do as well, where you can show that that's the case, you have a genuine
explanation. Notice incidentally that this is the way evolution works generally.
Evolution goes through all evolution, goes through three, I see a half of the question, but I can't
read it. Okay, could you just hold it for a second, we'll come later. The way evolution works is
three steps. First, some accident takes place, could be a mutation, cosmic ray passed by and
changed the DNA slightly, an accident. It could be what's called symbiosis. So if you go back
a couple hundred million years, it turned out that a bacterium, there were only bacteria and
similar microorganisms at the time, one bacterium by accident swallowed another microorganism
that led to the development of what are called eukaryotic cells, complex cells,
the basis for complex life, our life for example. The third step is winnowing of the various
things constructed in the second step, which ones have better reproductive capacity, that's
natural selection, those are the ones that survive. So it seems to be the way language
developed. First came some accident, maybe a mutation, which yielded recursive generation,
then other nature comes along, tries to find the most elegant solution to the problem of
how to deal with these new systems. In one case it was eukaryotic cells, in our case it was
a generation by merge. The winning stage apparently never came for language,
there's no winnowing stage or what we stayed the same, maybe because time just
hasn't been long enough for natural selection to operate, maybe it's because at the second
elegant stage the system was so closely integrated that you just can't change it or it falls
apart. That would be a very interesting discovery and research is tending in the direction of
showing that it's correct. Well time is running out, let me stop here, we're now reaching what would
be the next chapter. How does all this work out in detail? But I think maybe if this succeeded you
have a general picture of how we can hope to meet the Galilean challenge, the basic challenge in the
history of the study of language in mind and do it in a way which can give genuine explanations
for some quite remarkable properties of language and thought while leaving identifying
other properties as a total mystery which we simply cannot comprehend. That seems to me
pretty much where we stand today, I'll stop there. Thank you Noam, can you hear me okay?
Great, so we'll have actually asked people to raise their hands for questions, I think it
will be easier to moderate them but we did have a couple come in through chat and maybe
we can start there. Leslie Bailey if you want to unmute yourself and ask your question go ahead.
Oh great, okay I'm afraid this is a really really stupid question Noam, I'm sorry. If possible
please restate the difference between the behavioral structural approach to language and
what replaced it and how that transition occurred. I mean I know it's very complicated but if you
could just say it in two sentences. It's quite interesting, let's take a look at how the
structural linguists identified their own theory, their words. Structural linguistics
was identified by mid 20th century, a consensus had been reached among the at that time
fairly small number of structural linguists, it was a very small field then, a huge field now.
The small, they reached the consensus that structural linguistics was what they called
a taxonomic science. There were procedures of analysis, I was actually a student in the late
40s studying this. You studied the procedures of analysis, here's what you do when you have an
informant from a say Native American community, here are the procedures you use, of all you have
as a text, here's the procedures of analysis you use, those procedures identify units,
you organize the units, you're done. It's a taxonomic science, fields over.
Okay so it's descriptive. Let me continue. There was also a theory of learning. It was
stated explicitly by Leonard Bloomfield, the leading linguist, American linguist of the
early 20th century. Language in his words is a matter of training and habit. If there's anything
new produced, it's by analogy. Same view in philosophy of language. Take the major figures,
W.E. Quine, Ludwig Wittgenstein, essentially the same. Language is a matter of training,
habit, your Quine, Schenarian, operant conditioning, anything new is analogy.
The new system that developed starting around the 20th century was essentially a return to the
old tradition. We want to find explanations for linguistic phenomena. We want to find out why
they work this way, not the other way. Why do we say the bombing of the cities is a crime, not
or a crime? Essentially the 17th century questions. As soon as you started on that, you had the same
transition that happened in the 17th century from knowing everything to knowing nothing.
As soon as you started asking these questions, turned out you had no answer. Well, I should say
that the entire tradition was totally unknown, completely forgotten, wasn't rediscovered until
the 1960s after much of the ground had already been laid. The structuralist behaviors
era essentially wiped out the history. But the new system that I've been discussing is
essentially picking up from what the tradition was. I don't know if that's
clear enough to answer the question. No, that's great. Thank you so much.
All right, so our next question is from David Dalton. David, I just unmuted you, so you should
be able to ask your question. Listen to our discussion here, and I ask, is not our consideration
a little bit Anglo-centric in that English, yes, it has an alphabet of 26 characters,
but if you look at Mandarin, it has an alphabet of 40,000 characters, only 2,000 of which are
stand today. Not to mention that, Mandarin is a tonal language, whereas English is more of a
literal language. And I don't know Mandarin myself, but I just wonder how is it that we can, with such
broad brush strokes, compare all of language in this discussion, I would seem that there would be
quite a bit of difference in the semantics of Mandarin versus English or say German or French.
Well, I think the comment about the alphabet that I made was Galileo. He was, of course,
thinking of Latin and Greek, but the alphabet is a very superficial matter. It's like the printer.
You can use all sorts of alphabets. You can use the Mandarin style, the Korean style,
hieroglyphs, just as you can use any printer. It turns out that the internal system for Mandarin,
for English, for Youkuts, for Paraguay, for Tagalog, the internal systems are very much the same,
maybe identical. Lots of languages are tonal languages, but they function essentially. That's
just a different printer. And I mentioned just one example when I was talking. Languages like
Youkuts is an example where a single word includes the whole sentence. It was thought for a long time
that those languages must be very different. The more we discover, the more we find that
at the internal level, they're the same. The internal means for constructing expressions
and the semantics is either identical or very close to it. It doesn't look like that on the
surface. You're quite right. On the surface, they look wildly different. And it was assumed,
if you go back 50 years, it was assumed that languages can differ in just about every possible
way. And when you study a particular language, you can't use any presuppositions from another
language. That was sometimes called the Boazian principle. That's what I learned when I was a
student. It seems to be completely wrong. It seems that at the core, all languages are either
identical or very close to it. The systems that construct internal expressions in the mind,
expressions of thought don't seem to differ. The externalization looks very different.
They seem arranged differently, different sounds, and so on. But all of that seems superficial.
To go back to the analogy of the laptop and the printer, what's the internal program in the laptop
seems pretty close to common for all languages that we know, including very diverse ones.
The printer can be very different. And the alphabet isn't even the printer. It's not the
sounds. It's the representation of the sounds. So it's even more remote from language.
Our next question is from Fran Manley. Fran, if you want to unmute yourself or I've
unmuted you actually, Fran, you're good to go. Yeah, I got a two-part question, professor.
One, is there any evidence as to whether or not other species, for example, like dogs,
have a common language which allows them to communicate even with dogs they don't know?
And my second question, if I may, is on children, I've been led to believe that
children don't use a word unless they've heard it. But I got the impression from what you've said
that's not necessarily so. They don't have to hear a word to learn it. Okay, thank you.
Well, on the second one, children have to hear a word in order to not,
not to, depends on what you mean by learn the word. They know the meaning of the word
before they hear what the sounds are. So if you're a Chinese child, a Pottawatomie child,
French child, whatever it may be, you have the meaning in your head. You don't know what sound
is going to be associated with it. That you have to hear. And Lauderdale Gleitman and
her work and others have shown that takes almost no, no, no experience.
Couple here at a couple of times and you say, okay, that's the internal word that I know with
all of its complex meaning. You do have to hear something to know whether it's pronounced cat
or some other way. Okay, how about the animals? Do they have one language, form of language, other
species? Do they have more? Do they have, do like, do animals or do other species have their own
communication? Yeah. So animals, let's say dogs, I happen to have two of them on the desk at the
moment. You heard a couple of them before. Well, I've looked at them pretty carefully.
They were, they have a number of ideas in their heads, maybe a dozen or so.
Sometimes they can do moderately complicated things. Like, I'll give you my favorite example.
There's a big dog and a small dog. If the big dog wants to be taken, I hate to say the words
because unfortunately, she'll hear them, but wants to be extricated to the outdoors,
power phrase, which she doesn't understand. When she wants that, she has a trick to do it.
She takes something away from the small dog and the small dog comes over to us to complain.
And we make the big dog, give it, say a toy, give it back to the small dog, and then she
runs right to the door and expects us to extricate her. Okay. So it's more of a behavior than a
language, right? It's a small number of things. It's an entirely different from human language in
every respect. Every animal that's known, including the closest to us, apes, monkeys,
or have symbols, like take a vervet monkey, which has been studied carefully, has maybe a dozen
calls. Like, if the leaves are fluttering in a tree, the vervet monkey will come out with a sound
which makes all the other monkeys run away and hide. We interpret that as a warning sound.
Maybe there's an eagle in the tree or something. I don't know what the monkey's thinking. But
every, or can make another symbol, which is, we interpret as meaning, I'm hungry. Okay.
The point is that each symbol is keyed explicitly to a physically identifiable entity.
The motion of leaves in a tree, hormonal changes, something like that. Okay. Thank you.
And none of the, none of the human symbols are like that. That's Aristotle's point about house.
So they're radically different. Furthermore, there's no combinatorial structure in the
animal systems. It's just a symbol. Now, there have been major efforts to try to train chimpanzees
like infants. The main study you may have seen, it is called the NIMM study, named after me, actually,
by a number of very fine cognitive scientists, some of them former students. Now, one of them
a colleague and friend, they made an extensive effort to raise the chimpanzee NIMM from infancy
pretty much the way you raise an infant, extensive efforts to get NIMM to pick up something like
language. They used sign because the articulatory system for the chimpanzee doesn't work very well.
So train it like an infant would learn sign. At first, they thought they were getting somewhere,
but when they looked closely, it turned out it was nothing. It's just beyond the capacity
of a highly intelligent ape to do anything like what a one- and two-year-old infant does.
They're just built differently. There are things that other organisms do that we can't do,
like I live in the desert. In the back of my house, there are desert ants who have cognitive
capacities that I can't come close to. They can navigate in ways which I can't possibly do,
maybe humans can duplicate it with complicated instruments, and they have a brain about the size
of a miniscule brain. Organisms are just different. Our difference is we have the capacity for language.
It doesn't have any counterpart anywhere. No.
Our next question is from Les. I've unmuted you, Les, so go ahead and ask.
Yeah, this question is maybe more philosophical, but the idea about thought and expression,
can you have thoughts before expression internally, or do they rise simultaneously and cannot exist independently?
Well, these are things where we can't introspect because it's all beyond the level of
introspection, and there's no external scientific evidence. But the simplest theory,
the simplest assumption, the one that would have to be disproved, is that they're just the same thing.
The expression is the thought. There's an internal system which has provided we all have
the knowledge we possess, includes an infinite array of these thoughts formulated in the linguistic
system. When I produce a sentence, I pick out one of them somehow, how unknown, then I implement it.
So there may be no distinction between thought and expression, no first or second.
Just that's what the thoughts are. There's a repository of thoughts in linguistic form,
and we can then implement them. And on the receptive side, you can reconstruct them.
Maybe not the same way, maybe even in a different language, but something very much like them.
Our next question is Joe Keller. Go ahead, Joe. I've unmuted you.
Thank you, Scott. Professor, as the newly conceived human begins to develop her that
inborn language structure, will her external experiences add to or enhance that structure in
any way? Well, here we ought to pay attention to the way built-in structures work generally
to take your visual system. The visual system does quite complicated things,
very, there's time to talk about them, but very rich things. If your visual system is not
stimulated in the early weeks of life, it'll degenerate. We know this from studies with
other animals, cats, monkeys, and so on, in basis that we live about the same visual system.
It's shown that unless a kitten is presented with pattern stimulation
in the first couple of weeks of life, it'll be blind. That's the way
and all instinctive behavior works like that. It has to be triggered by something,
and then it is partially shaped, partially shaped by the way, by the stimulation.
So it takes kittens again. If kittens are presented in the early weeks of life only with
horizontal lines, then as a mature cat, it'll be able to perceive horizontal lines,
not vertical lines. That's a little bit like different learning, different languages.
You can shape the internal system marginally. You can't train a kitten to recognize
curves, for example, in infancy. It has to be lines and different orientations.
Lots of things can't get it to do, and that's the way instinctive behavior works quite generally.
It seems to be the same with language as far as we know. The system is built in, the internal system
has to be triggered in order to get started, to develop, then pretty much develops on its own
course. There are some respects in which experience outside data shapes the system.
One is the trivial one of how do you pronounce this? Others are, are you going to be like
English, an analytic language with a lot of separate words that form a sentence?
Or are you going to be like, let's say, a language where you have one huge word that
includes everything? Well, that requires some stimulation. A large part of recent research,
very recent research, is to try to sort out which parts are fixed and fundamental,
which parts are provided by the shaping expective experience. It's turned out pretty surprising.
So 40 or 50 years ago, the greatest anthropological linguists, friends of all of us here,
Ken Hale, former teacher's friend, one of the leading anthropological and
formal linguists of the past generation, he firmly believed that languages differed in
what's called a parameter, an option of choices. Some had flat structure with no hierarchy,
some like English had hierarchy. And it looked, that's the way the evidence looked. Over time,
actually Ken, Ken Hale and his students, Julie Legge, were able to show that even the most extreme
cases of languages that looked like completely free word order or flat no structure, even they
had the same internal structure as the hierarchical languages. These are real discoveries, major
discoveries of the past couple of decades. And we don't know how far it'll go, but I think it's
a reasonable guess by now that it'll probably go to showing that the internal system that generates
thought is either uniform or very close to it, and that the apparent variety and complexity
and mutability, the change from generation to generation, all of that's probably in the
externalization and the trivial lexical properties, like do you pronounce it cat or some other way.
Our next question is Miriam Burt. Miriam, you are unmuted, go ahead.
Thank you. I think my question, Professor, probably follows on pretty well to the recent one,
the one you just responded to. You said earlier that around two years to 30 months is this riot,
this wonderful time when, you know, they're learning a new word or a new every waking hours.
But I wonder if a child, if she's deprived of both that visual stimulation and the language,
you know, the auditory to hear the words from, because she's in an orphanage or, you know, in a
very poverty state, if you will, you know, for external stimuli. Will she be able to catch up
if she is subsequently, you know, exposed to this, this stimuli, you know, internally and
externally, I mean, visually and auditorially, or is she always going to be a little behind
and not quite up to what she could be. Thank you. That's a very, it's a very difficult question,
because we cannot do experiments. You don't do experiments with humans. Rightly or wrongly,
we do do them with cats and monkeys. That's how we know about the visual system, which is pretty
much shared with us. Can't do it with any other organism, because no other organism with anything
remotely like language. So we just have to look at the experiences that we see. And they're,
they're very interesting. You learn a lot from them. So there's a very fine neuroscientist
named Helen Neville, Oregon State University, who done fine work on the brain and language.
Actually, about 30 years ago, she got interested in pretty much this question.
She took children who are what are called low SES, socioeconomic status, and who seemed to be
problems in school. They weren't learning. They couldn't speak to all kind of behavioral and
other problems. And she began to study them carefully. She noticed some interesting things.
Their parents don't talk to them. I mean, they say, get out or go outside or leave me alone.
And not because they're bad people, they're just people who are trying to survive on two jobs in
miserable circumstances. Parents never read them stories. You know, they've never had that experience.
So she tried a simple experiment, trying to bring the parents and the children together
and encourage them just in some free time to interact, like have a mother read a story to
the children, things like that. Turned out they enjoyed it. They did more of it. They started
having much higher cognitive achievements. Weren't problems in schools and so on.
Well, that's a partial answer to your question. I'll mention another real experiment,
real event. It's a very fine, another cognitive neuroscientist to come and Massimo know very well.
He lives in Massachusetts. He began working with children who were such extreme behavior problems
that they were basically excluded from school. They couldn't stay in a classroom, had to be
excluded. He started working with them, looking at them carefully. He immediately found out
that these were kids who came from homes where they didn't eat breakfast. They didn't eat breakfast.
They got on a school bus. The bus roamed around for an hour. They finally got to the school.
They were out of their heads. They were sent to maybe an arithmetic class.
Here they couldn't pay any attention. They ran around the room and so on.
Well, he started doing something. For one thing, when they came to school, the first thing he did
was give them candy because their glucose levels were low. They hadn't eaten. Then,
instead of putting them right in an arithmetic class, he let them stay outside and run around
for half an hour. Then they brought them into school. Other things like that. By now, these kids
are performing considerably better than the average in the public school system.
Well, I think that's the kind of evidence that we have about your question. It's a humanly,
very significant question. We can't do direct experiments on it. We're not going to deprive
children and see what happens. You take the natural experiments and see what you can find out.
I think the basic answer is that if the deprivation is not too severe,
the child can probably recover normal cognitive capacity. If the deprivation is extremely
severe and there are such cases, the child is simply deprived of external contact,
maybe a sociopathic father, something like that, then they may never recover.
But it's hard to know what's caused. There's a famous case, Jeanne, which you may have read
about, but it's hard to know what the lack of recovery is from because any kid like that is
going to be psychotic. If you're brought up, tied to a chair till you're 12 years old with somebody
throwing food at you, you're going to be psychotic if you can even survive. So how much is that
interfering with what's happening? Well, we don't know and we don't want to do the experiments,
which will answer it. We've got about five minutes left. Our next question is from Linda Green.
Linda, you are unmuted. Go ahead. My question has to do with brain research.
What contemporary brain research is shedding light on linguistics?
Well, it's difficult because brain research altogether is very difficult and brain research
on humans is particularly difficult because we do not allow invasive experiments.
With a cat, we allow ourselves to stick an electrode into a neuron and see what it's doing.
You don't do that with humans. So it's all kind of evidence you can pick up by external
imaging and so on. And you can learn a great deal. In fact, the most important discovery
about this actually has to do with what I call structure dependence, the fact that
the computational operations of language make no use of 100% of what we hear, words in linear order.
They only deal with abstract structures in the mind.
They're very ingenious experiments initiated by a friend and colleague of ours,
Andrea Moro, Italian linguist, neuroscientist. He and his colleagues devised experiments on
the following paradigm. They took subjects, two groups of subjects who happened to be
speakers of German and presented them with invented systems. One of them modeled on
Japanese language that they'd never heard. Another, using simple rules which violated
structure dependency. So a rule that says, if you want to negate a sentence, take the negative
particle, say not, and make it the third word of the sentence. Very trivial computation.
They asked how subjects responded to this. Well, it turned out that when they were given a language
modeled on a natural language, the areas of the brain that are mostly involved in language
processing and use all fired up the way you'd expect them to. When they were given the system,
the artificial system with simple computations on linear order, they just got diffuse activity
in the brain, meaning the brain is treating it as a puzzle, not a language. That's quite an
interesting result, certain replicated, there's different variants of it and so on. But that's
the kind of thing that can be done. There are other, there is other experimental work in
neuro linguistics which shows a number of things. All right, we probably have one quick
question, Ralph Losty, if you'd like to, I've got you on muted Ralph, if you'd like to ask your
question. Yes, I just read recently about the multi-language brain. And it looks like people
with multi-language capabilities are actually perform worse than mono-language brain. And how
could that be when you said the languages are the universe? So the thoughts of being universal
and they should express, okay. Well, first, I don't know what you saw, but I don't believe it.
Every child is easily capable of learning many languages in infancy. It's very common in many
cultures for a child to know three or four languages. They don't even know their different
languages. This is the way you talk to your mother, this is the way you talk to your grandfather,
this is the way you talk to the kids in the street. Finally, at some age, maybe four or five,
they may realize, hey, I'm speaking different languages. But it's just normal. As far as we
know, every child can do that. There's no known difference between, there's no even any way of
studying it, because all children have this multi-language capacity. But I'd be very skeptical
about the source you looked at, frankly. Thank you so much. Well, I think that's all the time we
have today that Professor Chomsky, we really appreciate having you present to us. And I'm
grateful to know as I, when I was a child, and I probably begged for a candy for breakfast that
it wasn't a bad thing. But no, I, we really appreciate you being here. And again, thank the
Department of Linguistics at the University of Arizona for hosting this session and course,
and appreciate it so much. Do you have any other parting words for us, sir?
I hope some of you will be interested enough to look into the meat of the subject,
how it's actually done next chapter. We did have a few people inquiring about
the publications and things that would be of good service to kind of get a general
understanding of this area of study. What are your recommendations?
Well, I have a book called What Kind of Creatures Are We, which goes into many of these questions.
But there's a lot more work. They're very, I should say, they're excellent general introductions
to language. There's a book by Charles Yang called The Infinite Gift, which
focuses a lot on acquisition, but brings up these things. There's a recent book by
David Adger, A-D-G-E-R. I think it's called something like The Infinite Mind or something
like that, which is an excellent introductory discussion at a high level to many of us.
There's a very fine general introduction to many aspects of language by an outstanding
linguist, Ian Roberts. It's called The Wonders of Language, which goes into these and many other
things. There's a lot of very high-level introductory literature for syntax, the kind
of thing I'm talking about. Our colleague here, Andrew Carney, has a book simply called Syntax,
which goes into many of these things in a very lucid way, but in considerable detail.
Well, thank you again so much. We really appreciate it. I can't unmute everybody apparently.
Thank you. Thank you. Thank you so much. Absolutely outstanding. Thank you so much.
Thank you, Professor. Thank you. Thank you. Thank you, fabulous. Thank you.
Thank you.

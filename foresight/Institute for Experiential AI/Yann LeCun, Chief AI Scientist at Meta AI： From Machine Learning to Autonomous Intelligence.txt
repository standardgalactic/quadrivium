Okay, great, welcome everyone and thank you for joining us.
This is the last distinguished lecture series for the Institute of Expansion AI for the
academic year.
We'll resume again in September with a full program for the year.
As you also know, in parallel every two weeks we run the Expeditions in Expansion AI series,
which is designed to feature a lot of our Northeastern University experts and faculty
and so forth.
In two weeks, definitely join us for a talk by Silvio Amir on a super interesting topic
who's at the Curie College.
My name is Osama Fayad, I'm the Executive Director of the Institute for Experiential
AI and also Professor of the Practice in the Curie College for Computer Sciences and it
is my pleasure today to introduce Yan Lokhan.
Yan is a very well-known name in the field.
I've known him for many years, I think at one point in my life I interviewed at Bell
Labs or AT&T Labs and that's when he was there.
He is VP and Chief AI Scientist at Meta, also known as Facebook, and Silver Professor at
NYU affiliated with the Kauan Institute of Mathematical Sciences and the Center for Data
Science, which he actually founded.
He was the founding director of FAIR, I learned this morning that FAIR used to stand for Facebook
AI Research, now it's changed to MetaFair for Fundamental AI Research and of course
he founded the NYU Center for Data Science, received an engineering diploma from SEA in
Paris and a PhD from the Saoubon University.
After a postdoc in Toronto, he joined AT&T Bell Labs, which got renamed to AT&T Labs
in 1996 as Head of Image Processing Research.
He joined NYU as Professor in 2003 and Meta or Facebook in 2013.
He is the recipient of the 2018 ACM Touring Award, along with Jeffrey Hinton and Yashua
Benjio, and for those of you who don't know the Touring Award, it's essentially the equivalent
of the Nobel Prize for Computer Science, the toughest award to get from the ACM.
The award was for conceptual and engineering breakthroughs that have made deep neural networks
a critical component of computing.
He is a member of the National Academy of Sciences and the National Academy of Engineering,
amongst many others.
His interests include AI, machine learning, computer perception, robotics and computational
neuroscience, and I'm sure you're all eager to hear from Jan on what's been happening
with generative AI and what all the buzz is about, hopefully we'll get into the technical
details and immediately following his talk, we will do a fireside chat where I will try
to ask him some tough questions.
And then we will also get questions from the audience.
By the way, we did get online questions from the audience.
We got 150 questions, so there's no way we're going to walk you through all of those.
So we'll see how much time allows us to answer.
Thank you and please join me in welcoming Jan to Northeastern University.
Thank you, Samap.
A real pleasure to be here and thanks for coming here so numerous or for listening in online.
So I'm going to talk a bit about the state of the art in AI but also about the next step
because I'm always interested in the next step and how we can make machines more intelligent.
And we need to figure out how to get machines that cannot just learn but also can reason
and plan and current AI really does not allow current systems to do this.
So I'll try to kind of sketch a potential pathway towards such systems.
I can't say that we built it completely but we built some components and I go through this.
So AI is in the news, everybody is playing with it at the moment.
It's pretty amazing how it works.
There's a lot of success.
It's been very widely deployed very much in many applications that are behind the curtain
but in some of them much more visible.
So LLMs have the advantage of being visible but for the last 10 years or so there's massive
use of AI and the latest development of AI for such thing as ranking for search engine
and social networks or for content moderation, things like that.
But overall machine learning requires a lot of data and the machines that we have are
somewhat brittle, specialized.
They don't have human-level intelligence despite what we may be led to believe.
So in short, machine learning sucks at least compared to humans and animals.
We've been using supervised learning which really was the workhorse of machine learning
and AI systems until very recently.
Reinforcement learning is insanely inefficient but it works really well for games but not
many other things.
So one thing that has taken over the AI world in the last few years is something called self-supervised
learning which I will talk about at length.
But current AI systems are specialized and brittle.
They make stupid mistakes.
They don't really reason and plan with a few exceptions for a game playing for example.
Compared to humans and animals, they can learn new tasks extremely quickly, understand
how the world works, can reason and plan have some level of common sense.
Machines still don't have common sense.
So how do we get machines to reason and plan like animals and humans learn as fast as animals
and humans?
And we'll need machines that can understand how the world works, can predict the consequences
of their actions, can perform change of reasoning with unlimited number of steps, can plan complex
tasks by decomposing them into simpler tasks.
So let's start with this idea of self-supervised learning.
It's really taken over the world.
Every sort of top machine learning system today uses some form of self-supervised learning
as a first step to pre-train the system.
And it's used everywhere.
What does it consist of?
It's really the idea that instead of having, of training a system with an input and an
output, which is the case in supervised learning, or with an input and a reward, which is the
case for reinforcement learning, you train the system to basically model its input.
You don't train it for any particular task other than capture the dependency between
different parts of its input.
So one thing you might do is, for example, take a piece of video, a piece of text, show
a piece of the video to the system and ask it to predict the missing piece, like the
continuation of that video.
And after a while, you reveal the rest of the video and you adjust the system so that
it does a better job at predicting.
So prediction really is kind of the essence of intelligence.
And to some extent, by training a system to predict, it doesn't have to be predicting
the future.
It could be predicting the past or the left from the right.
You're training the system to represent data, essentially.
And that's been nothing short of astonishingly successful in the domain of natural language
understanding.
So every type performing NLP system today is pre-trained the following way, or with some
form of the following way, which is a special case of an old idea called denoising autoencoder.
And the idea is that you take a piece of text, sequence of words from a corpus.
Typically it would be a few hundred or a few thousand words long.
Those words immediately get turned into vectors, but let me not talk about this for just now.
So the first thing you do is you corrupt this text.
You remove some of the words and replace them by blank markers, or you substitute them for
another word.
And then you train some gigantic neural net to predict the words that are missing.
In the process of doing so, the system has to basically develop some sort of understanding
of the text, because if you want to be able to predict what word comes here, you have
to understand the role of the word in the sentence, the type of word that comes here,
and the whole meaning of the sentence.
So the system basically learns to represent text.
And the amazing thing is that just by doing this, you can train a system to represent
the meaning of text in pretty much any language, as long as you have data.
With a single system, you can have a system that represents the meaning of a piece of
text in any language.
So pretty cool.
You can use this to build translation systems, systems that detect hate speech on social
networks or figure out what something talks about.
The way you do this is that you chop off the last few layers of that gigantic neural
net, and you use the representation, the internal representation, learned by the system as input
to a subsequent downstream task that you train supervised, like, say, translation.
And it's really astonishing how well this works.
So from this to a generative AI system, there's a small step, particularly for text generation.
Text generation is a completely different thing, which I'm not going to talk about,
but although some systems use the same technique.
So what is a generative text generation system, a large language model?
It's a system of the type I just described, except that when you train it, you don't remove
random words in the text that you show at the input, you only remove the last one.
So you train the system to predict the last word in a sequence of words.
So show a sequence of words, and then show the last word, and train some gigantic neural
net, perhaps with billions or hundreds of billions of parameters, to predict the next
word.
And you have to train this on trillions of text snippets, typically one to two trillion
for the biggest models.
Once you have that system, you can use it to generate text using what's called autoregressive
prediction, which is a very classical thing to do in single processing.
So you take a piece of text called a prompt, you enter it into the system, you have it
predict the next word, and then you shift that word into the input.
So now it becomes part of the input to the system, and now you can predict the next word,
shift it in, predict the third word, shift it in.
That's autoregressive prediction.
And that's how all the bigger alarms that everybody has played with work.
That's how they've been trained.
That's how they generate text.
So those alarms are kind of amazing in terms of the performance that they produce.
So again, they're trained on something like one to two trillion tokens.
A token is like a word or a subword unit.
And there's a whole bunch of those models, most of which you probably haven't heard of,
but there's a few that have become household names.
So we've heard of chatGBT and GPT-4 from OpenAI, which are kind of usable, barred from Google,
and derivative of chatGBT and GPT-4 from Microsoft, married with Bing.
But there's a long history of those things that goes back several years, some from Fair,
Lunderbot, and Galactica.
Galactica was trained on the scientific literature and is designed to help scientists write papers.
And a more recent one, called LAMA, which is the code is open source.
The model, you can get it on request if you are using it for research purpose.
And it's the same level of performance as things like chatGBT, but it's not fine-tuned.
You have to fine-tune it for application.
And in fact, people have done this, so Alpaca is a model which basically is a fine-tuned
version of LAMA that was built by people at Stanford for answering questions and things
like that, instruction.
So they're pretty amazing, they surprised a lot of people in how well they work, but
they make a lot of factual errors, logical errors, inconsistencies, limited reasoning
abilities, things like that.
And they are easy to, they're pretty gullible.
So you tell them, what is 2 plus 2, and the system will say 4, and you say, no, actually
2 plus 2 equals 5.
Oh yeah, you're right, I made a mistake.
So they kind of, they predict answers that would sound like someone could produce these
answers, but the details might be wrong.
So you can't really use them for factual answers, but you can use them certainly for
writing aids.
And particularly, it works really well for text or for standard sort of templatized text
that you need to write, like I don't know, there's a bunch of professors here that have
to spend quite a bit of time writing recommendation letters for students, very useful for that.
And very useful for code generation.
So the software industry is probably going to be revolutionized by such tools.
So this is an example of code generated from a prompt by the Lama 65 billion model, the
open source one.
So, you know, ask it, you know, find real roots of AX squared plus BX plus C, and the
thing just writes a function in Python or whatever, whatever you want, or Reg X or whatever,
who remembers the syntax of Reg X?
Like.
You can have it, you know, hallucinate text that might sound plausible or completely implausible
like this.
Did you know that Yanukun dropped a rap album last year?
We listened to it and here is what we thought.
And the thing writes a review of my alleged rap album.
I'm not much of a rap person, I'm more of a jazz person, so when my colleagues showed
this to me, I told them, like, can you do the same for a jazz album that would be kind
of more appropriate?
I mean, I'm a terrible performer, but, and I said, yeah, we tried already, but it didn't
work very well because there's not enough training data on the web of reviews of jazz
albums.
I found that incredibly sad, I cried.
So you need a lot of data to train those things, right?
In fact, the amount of data, like something like 1.5 trillion tokens that Lama is trained
on, it would take about 22,000 years for a human reading eight hours a day at every
speed to read the whole material.
So obviously those things can accumulate a lot of knowledge, at least approximately.
So yeah, writing assistants, code generation, first draft of a lot of stuff, they're not
good for producing factual and consistent answers, at least not yet.
So a lot of LLMs are being augmented or extended so that they can use tools like calculators
or database engines or whatever to search for information and then refer to the source.
They're not good at all for reasoning, planning, or even for arithmetic.
So but we are easily fooled by their language fluency into thinking that they are intelligent.
They're not that intelligent.
And they really have no understanding of the physical world because they're trained with
text.
And there's another flaw, which is a huge problem.
It's the fact that if you imagine that there is the set of all possible answers represented
by this sphere, disk, which is really a tree, right?
Every token you add put, you have a certain number of options for what the token should
be, what the word is.
So it's a tree of all possible answers.
Within this tree, there is a small subtree that corresponds to correct answers for the
question being asked.
And imagine that there is a probability E for any token that is produced by the system
to be outside, to take you outside that tree of correct answers.
Once you go outside that tree, you can't come back because it's a tree.
So let's imagine that the probability per token is E. So the probability that a sequence
of N tokens would be correct is 1 minus E to the power N, making the assumption that
the errors are independent, which of course they're not, but that's kind of a crude assumption.
And so the problem with this is that it's an exponentially divergent process, this
autoregressive prediction, errors accumulate.
And if you produce too many tokens, the thing will sort of diverge away from the set of
correct answers, exponentially.
And that's not fixable with the current architecture.
You can fine tune those systems a lot to reduce E, but you're not going to make it
go away.
So I have a bold prediction, which is that the shelf life of autoregressive LLM is very
short.
My prediction is that five years from now, nobody in their right mind would use them.
So enjoy it while it lasts.
They'll be replaced by things that are better.
And I'll hint about directions to kind of perhaps fix up those problems.
So this is a paper that Jake Browning, who's a philosopher, and I published in the Noema
magazine, which is a philosophy magazine, about the fact that a system that is purely
trained from text, from language, cannot possibly attain human level intelligence because much
of what humans know is actually derived from experience of the physical world.
This is true for a lot of human knowledge, but it's true certainly for almost the totality
of animal knowledge.
It's all about the world is no linguistic related, no language related.
So linguistic abilities and fluency are not related to the ability to think.
Those are two different things.
There are some criticisms of autoregressive LLMs from people coming from the cognitive
science realm who say like, this is not at all the way the human mind works.
There is essential missing pieces.
Other criticism for people who come from sort of more classical AI, pre-deep learning,
they say like, you know, AI systems are supposed to be able to plan and reason, and those LLMs
can do it.
Or at least not, you know, they can do it maybe in very sort of primitive forms.
Perhaps they can plan things in situations that correspond to a template that they've
been trained on, but they're not so innovative.
So we should ask, how is it that humans and animals can run so quickly?
And I've been using this diagram for quite a while now, several, many years from Emmanuel
Dupu, who's a cognitive scientist in Paris.
And we tried to sort of make a chart of at what age babies learn basic concepts about
the world, so things like distinguishing between animate objects and inanimate objects, learning
the notion of object permanence, the fact that when an object is hidden behind another
one, it still exists.
Notion of rigidity, solidity, things like natural categories, babies don't need to
know the name of an object to actually know that there are different categories of objects
around four months or so.
And then it takes about nine months for babies to really understand that sort of intuitive
physics that objects that are not supported will fall, that, you know, objects have a
momentum, weight, friction, you know, knowing that if I push on this object, you know, light
objects like this, they're going to move, but if I push on an object that's heavier,
it's not going to move unless I push harder.
So things like that.
So if you show a six-month-old baby or a five-month-old baby, the scenario here on the left where
you have a little car on the platform, you push the car off the platform, it appears
to float in the air, a five-month-old baby will pay attention, a 10-month-old baby will
go like this because she understood that by then that objects are not supported or supposed
to fall and this object appears to be floating in the air.
So we can determine that her mental model of the world is being violated, okay?
That's how this chart was built.
So we accumulate as babies an enormous amount of background knowledge about how the world
works, mostly by observation, a little bit by interaction, when we start being able to
kind of grab things, but in the first few months it's mostly just observation.
So we don't know how to reproduce this with this type of learning with machines.
Once we accumulate all this background knowledge, you know, in a number of years, learning a
new task like driving is very fast.
So any teenager can learn to drive in about 20 hours of practice, mostly without causing
any accident.
So the teenager doesn't have to run off a cliff to figure out that the car, that nothing
good is going to happen if you run off a cliff.
The mental model of the world is already there, okay?
We still won't have level five salivating cars.
So obviously we're missing something pretty big.
Any 10-year-old can clear up the dinner table and fill up the dishwasher.
We're nowhere near having robots that can do this and it's not because of mechanical
design, it's because we don't know how to build the minds behind it.
So we're missing something big, right?
The past towards human-level AI is not just making LLMs bigger, that's just not going
to get us there.
It's been a common recurring error by AI scientists and engineers over the last six decades to
imagine that the one thing that they just discovered was the solution to human-level
AI, only to discover a few years later that no, there was actually a big obstacle, another
obstacle they had to clear.
It's a recurring history story in AI.
So common sense will probably emerge from the ability of machines to learn how the world
works by observation, the way babies and animals do it.
So I see three challenges for AI research over the next decade also, learning representations
of the world and predictive models of the world, I'll say why in a minute, and self-supervised
learning is going to be the key component of that, learning to reason.
So psychologists talk about system one and system two.
System one is the type of control that our brains use to kind of react to something without
really having to think about it, like subconscious action.
So if you're an experienced driver, you don't have to think about driving, you can just drive
and you can talk to someone at the same time and barely pay attention.
So that's system one.
But then when you are learning to drive, you pay attention to absolutely everything.
You use your entire focus, consciousness, attention to drive and that's system two.
And then the last thing is learning to plan complex action sequences, decomposing them
into simpler ones.
So I wrote a sort of vision paper about a year ago, which I posted on open review for
comments, so you're welcome to comment on it.
I give a bunch of technical talks about it, one of the earliest one was at Berkeley, but
you are having a more recent version of it right now, so you don't need to look at that
one.
And it's based on what's called a cognitive architecture.
So basically how can we sort of design a system with different modules so that those modules
may implement all the properties that I was telling you about so systems can perceive,
reason, predict, in particular predict the consequences of their actions and then plan
a sequence of actions to optimize, to satisfy a particular objective.
So the main components of the system is the key component, I would say, is the world model
and the world model is what allows the system to predict ahead, imagine what's gonna happen.
This is to some extent what current AI systems don't really have.
Perception system basically gets an estimate of the state of the world and initializes
the world model with it.
The cost here is a really important module, so basically the entire purpose of the agent
is to minimize this cost.
So the cost is something that uses a measurement of the state of the agent, particularly the
prediction from the world model, and predicts whether an act is going to be good or bad.
And the entire purpose of the agent here is to figure out a sequence of actions, so this
is taking place in the actor, figure out a sequence of actions such that when I predict
what's gonna happen as a consequence of those actions using my world model, my objective,
my cost function will be minimized.
So if my cost function is, so the cost function is basically our measures of discomfort of
the agent.
Biological brains have things like that in the visual language, so this is the thing
that tells you when you're hungry, for example, or you're hurting.
So nature tells you you're hungry, nature doesn't tell you how to feed, you have to
figure that out by yourself, perhaps using your world model and your planning abilities.
So this is the same thing here, imagine this is a robot and the robot battery are kind
of starting to get drained, so there's a cost function here that says be careful, you're
running out of power.
And so the system, according to this world model, would say, well, I can recharge my
battery by plugging myself into a socket.
So it figures out the sequence of actions to plug itself into a socket and that will
eventually minimize the cost function that just appeared.
So in fact, there's two ways to operate that system one is the kind of system one where
the system makes an estimate of the state of the world, run this to a perception system
called an encoder here, produces an estimate of the state of the world called S0 and that
runs into a neural net called a policy network that just produces an action and the action
goes into the world.
LLNs are like this, they are system one, you give them a pump, that's X, they produce
an action, that's the token they predict, that goes back into the world and the world
is very simplistic here, it's just you shift in the input.
So no reasoning necessary, here is system two.
So you use the same system here and this is a sort of time-enrolled version of the system.
So we have the world model, the world model is this green module and the different instances
of that green module are the state of the system at different time steps, so think of
it as like a recurrent net that you unfolded, so it's really the same module at different
time steps.
What the world model is supposed to be able to predict is given the representation of
the state of the world at time t and given an action that I'm imagining taking, what
is going to be the predicted state of the world at time t plus one.
So I can imagine a sequence of actions that I might take, imagine the effect on the world
using my world model and then I can plug the state of the world over this trajectory
through my cost and measure whether my cost is going to be minimized by this action sequence,
my objectives.
So what I should do is run some sort of optimization procedure that will try to search for a sequence
of actions that minimizes the cost given the prediction given to me by the produced by
the world model.
This type of planning is very classical in optimal control.
It's called model predictive control.
In classical optimal control, the model is not learned usually, it's handcrafted.
Here we are thinking about a situation where the world model is learned by, for example,
watching the world go by, by video, but also by observing actions being taken in the world
and seeing the effect.
So to get a good accurate model here, I'm going to have to observe the state of the
world, observe, like, take an action and observe the effect or observe someone else
take an action and observe the effect.
Let me skip this for now.
Ultimately what we want is a hierarchical version of this because if you want the system
to be able to plan complex actions, we can't plan it at the lowest level.
So for example, if I want to plan to go from here to New York City, I would have to basically
plan every millisecond exactly what muscle actions I should take, okay?
And it's impossible, right?
You can't plan an entire trip from here to New York City millisecond by millisecond,
partly because you don't have a perfect model of the environment, like you don't know if,
when you're going to walk up the room here, whether someone is going to be on the way,
in the way and you're going to have to go around.
So you can't completely plan in advance, right?
So what we do is we plan hierarchically, say like, okay, I want to go to New York City,
so the cost function at the top here measures my distance to New York City.
And the first thing I have to do is go to the airport and catch a train or go to the train
station and catch a train or go to the airport catch a plane.
So the top predictors are predictors at a high level that says, oh, okay,
if I catch a taxi, it might take me to the airport.
If I catch, or to the train station, then if I catch a train, it'll take me to New York City.
Okay, so you have those two hidden actions, those Z variables here.
And they define a cost function for the next level down.
So if the first action is I'm taking a taxi to the train station,
the lower level is how do I catch a taxi here?
I go down in the street and hail the taxi.
No, this is Boston.
I need to call it Uber or something.
Okay, so I go on the street and I call it Uber.
How do I go in the street?
There's going to be lower levels.
I have to get out of this building.
How do we get out of this building?
I have to walk through the door.
How do I work through the door?
I have to put one leg in front of the other over obstacles.
And all the way down to millisecond.
Also control for a short period, which is replanned as we go.
Okay, no AI systems today can do any of this.
This is completely virgin territory.
Okay, there's a lot of people who've worked on hierarchical planning,
but in situations where the representations at every level are hardwired,
they're known in advance.
They're predetermined.
It's sort of like the equivalent of a vision system where the features
at every level are hardwired or designed by hand.
There's no system today.
They can learn hierarchical representations for action plans.
So that's a big challenge.
The cost function, so here's what's important here.
A lot of people today are talking about the fact that AI systems are difficult
to control and that's terrible, maybe toxic, various things.
The system I describe cannot produce outputs that do not minimize the objectives.
And so if you have terms in the objective that guarantee certain conditions,
that system will have no choice but obeying those conditions.
Okay, so having a system that is designed like this,
that whose output is produced by minimizing a set of objectives,
according to a model, will basically help guarantee the safety of that system.
Because you can hardwire intrinsic objectives on the left here
that basically guarantee the safety.
And the system cannot escape the satisfaction of those constraints.
So let me take a very simple example.
Let's say someone figures out how to build a domestic robot they can cook.
This robot will have to be able to kind of handle a kitchen knife.
And you might put a cost function that says, don't flail your arm
if you have a kitchen knife in your arm and there is people around.
Okay, because it's dangerous.
So you can imagine putting a lot of kind of safety conditions in those systems
to make them steerable.
So I don't think the problem of making AI systems safe is such a huge problem
that some people who are very vocal are seeing it is that AI is going to kill us all.
It's not going to kill us all.
We would have to screw up really badly for that to happen.
Okay, now here's the thing.
How do we build the world model?
And that's basically the biggest challenge that we have at the moment.
How do we build a system that can predict what's going to happen in the world?
For example, by training itself to predict videos.
Now the problem with predicting videos is that the world is not entirely predictable.
It may not be deterministic, but even if it were deterministic,
it wouldn't be completely predictable.
So in fact, here is an example here.
If you take a video, this is a top-down video of a highway that looks like cars
driving around just following the blue car.
And you train a neural net to predict what's going to happen in the video
after the first few frames.
It produces blurry, it makes blurry prediction because it can't predict if
the car that's behind you is going to accelerate or break or change lane or whatever.
So it makes an average of all the possible future and that's a blurry image.
Same with, this is an old paper where we attempted to do video prediction using
neural nets and the predictions are blurry because there's too many things
that can plausibly happen and the system can only predict one thing.
So it predicts the average.
So that's no good.
The solution to this is what I call a joint evading predictive architecture.
And this is really the most important slide of the talk.
So the normal way to make predictions is through a generative model.
What's a generative model?
It's a model where you have a bunch of variables you observe,
let's say the initial segment of a video.
You run it through an encoder and through a predictor and the predictor predicts
y, which is, let's say, the continuation of that video.
And you have some cost function that measures the discrepancy divergence
between the predicted y and the actual y you observe.
This is when you train your world model.
It could be that the predictor has an action variable that comes in,
but in this example there isn't.
So examples of this are things like variational auto encoders,
mass auto encoders, or denoising auto encoders, which is a more general concept.
And so basically all NLP systems, including LMS, are of this type,
the generative models.
But here is the thing.
You don't want to be predicting every detail about the world.
Here you have to predict every single detail about the world.
So it's easy if it's text, because text is discrete.
So predicting the next word, I cannot predict the next word from a text.
But I can predict within 10 possible words some probability distribution of
the, over all the words in the dictionary of which word comes next, right?
They can represent distributions over discrete variables.
I cannot do this over the set of all possible video frames.
I cannot usefully represent a distribution over the set of all possible video frames.
So I can't use the same trick for video that is used for language.
The reason why we have LMS that works so well is because text is easy.
Language is simple.
We only popped up in the last few hundred thousand years anyway, so
it can be that complicated.
And it's also processed in the brain by two tiny areas called the Vernike area
for understanding and the Borke area for production.
What about the rest of the brain?
The prefrontal cortex, that's where we think, okay?
That's not part of LMS, the LMS are perhaps good models of Vernike and Borke,
but that's it.
Okay, so what I'm proposing here is to replace this generative architecture by
a joint embedding architecture and the essential characteristic of it is that
the variable that you want to capture the dependency of with respect to X goes
itself through an encoder and the encoder eliminates the relevant information
that is not useful for anything.
Okay, so for example, if I had a video of this, if I was shooting a video of
the room here and then panning the camera and asking a system to predict
what's the rest of the room, it would probably predict that the rest of
the room looks like the initial part that there'd be a lot of people in
different seats, but it couldn't predict your age, gender, hairstyle, clothing,
or the texture, precise texture of the floor or things like that, right?
So there's details that cannot possibly be predicted and one way to avoid
predicting them is to basically eliminate that information from the variable to be
predicted through an encoder.
So that's a joint embedding architecture or predictive architecture because it has
a predictor.
Now there's an issue with this thing, which is that if you train a system with,
let's say, a piece of video and the following piece of video and you just train
it to minimize the prediction error, you train the whole thing, it collapses.
It collapses, basically the encoders ignore the inputs, they produce constant
vectors for SX and SY and the predictor just needs to map SX to SY and it's a
constant, so it's super easy, okay, bad.
So the question now is how do we prevent this from happening?
How do we prevent it collapse?
It doesn't happen with generative models because they can't collapse.
So there are three flavors of those joint embedding architectures, a simple one
where you're basically trying to make the two representation of SX and SY identical.
So for example, X and Y are two different views of the same scene and you want SX
to represent the content of the scene, so it doesn't matter where you look it from.
You just want to make the representations equal.
When the encoders are identical, this is called a Syme's network.
This is another idea that goes back to the early 90s.
You have deterministic joint embedding architectures and then you have joint
predictive architectures that may be non-deterministic where the predictor
function has a latent variable that could be drawn from a distribution or taken
in a set that would allow that system to make multiple predictions if necessary.
Now, we have to ask ourselves the question of how do we train those things?
And I'm going to use a symbolism here where that I've used the rectangles
and squares of cost functions, energy terms, the circles of variables,
observed or not, and those symbols here are deterministic functions.
Imagine a neural net, okay, trainable.
We may have to hardwire some cost functions in the system to have it,
to drive it to focus on aspects of the input that are important.
So that's the purpose of that C cost function at the top.
Okay, but to explain how to train those things,
I'm going to have to explain a little bit what energy based models is about because
the classical kind of probabilistic modeling in machine learning kind of goes
at the window when we use the joint embedding architectures.
So what's an energy based model?
Energy based model is a learning system that captures the dependency between
two sets of variable x and y through an energy function that is supposed to take
low values, low energies around data, training samples.
So imagine those black dots are training samples.
You want that energy function f of x, y to take low values around the training
samples and then higher values outside.
And that system will capture the dependencies between x and y.
If I give you a value of x and I ask you what can be the possible values for y,
you're going to tell me, well, it's either this or that or maybe that other thing at
the top.
Okay, so it's not a mapping from x to y, it's an implicit function.
And by figuring out what value of y minimizes the energy function,
you can do inference.
You can infer y, possibly, but you don't necessarily have to do that.
So that's energy based model.
It's kind of a weaker form of modeling than probabilistic modeling.
And so now the learning problem becomes how do you train this energy function,
which is going to be some big neural net, so that the energy takes low value around
the training samples and high values outside.
If you're not careful, you're going to get a collapse so that the same type of
collapse I was telling you about before, if you just pull down the energy of the
training samples, minimize the prediction error in this joint invading architecture,
you're going to get zero energy for everything.
It's not a good way to capture the dependencies.
You have two classes of methods, contrastive methods.
So contrastive methods consist in generating those green points,
which are outside the region of data, and then push the energy up while you push
down on the energy of the data points.
Okay, so that's going to create a groove in the energy surface, and
the system will have captured the dependency between x and y.
But there's an alternative here, which is regularized methods,
where the point of those methods is to minimize the volume of space that can take
low energy, so that when you push down on the energy of data points,
the rest of the space takes higher energy because there is only a small amount of,
a small region of low energy to go around.
So those are the two classes of methods.
Every method you ever heard of in machine learning can be viewed as one of those two.
Most probabilistic methods actually belong to the contrastive category.
Anything that uses Monte Carlo sampling, for example, is contrastive.
And then things like sparse coding and k-means and
things like that are more on the regularized method side of things.
Okay, so I'm asking you to do four things.
Abandoned generative models in favor of the joint embedding architectures, right?
So generative models are the most popular thing at the moment.
Forget about it, at least if you're interested in getting to the next step in AI.
Abandoned probabilistic models, because if you have those joint embedding
architectures, you cannot actually use it to derive a pure y given x.
The only thing you can use is sort of energy-based view.
Abandoned contrastive methods in favor of those regularized methods,
which I'll talk about a bit more.
And then something I've said for many years now, abandoned reinforcement
modeling because it's too inefficient.
So those are some of the pillars of machine learning.
And I realize this is not a very popular opinion here, but okay.
So what about those regularized methods?
I'm just going to give you one example.
There's a whole bunch of them.
There's like a dozen of them, but I'm just going to give you one called Vicreg.
And the basic idea of it is to prevent those representations from collapsing.
We're going to use a criterion that attempts to maximize the information
content that comes out of those representations.
Okay, so we're going to measure the information content in some way, and
then maximize the information content or minimize the negative information content.
We're going to do this for both SX and SY.
We're also going to minimize the prediction error.
And if we have a latent variable, we're going to have to minimize
the information content of that latent variable.
I can't explain why, because it would take too long.
But you have to do that also to prevent another type of collapse.
I'm going to focus on how you do that.
So the sad news is we don't have good ways to measure information content, or
we don't have any good ways to estimate lower bounds on information content,
so that if we push up on this lower bound, the information content will go up.
We only have upper bounds for information content.
So we're going to do a very stupid thing, which is push up on the upper bound of
information content, and hope the actual information content will follow.
And it works.
So, there's a simple way to prevent the encoder from completely collapsing.
Which is to insist that every variable in SX, SX is a vector.
And you insist that every variable, as measured over a batch,
has a standard deviation that is at least one.
Okay, so this is the cost that you see at the top here.
Measure the standard deviation of each component of SX, and
put it in a hinge loss so that the standard deviation is at least one.
So that prevents the system from completely collapsing.
But it can still cheat by making all the components of SX equal or correlated.
So the second term says I want to minimize the off diagonal terms of
the covariance matrix of those vectors measured over a batch, right?
So I want pairs of variables to be uncorrelated.
So basically, the collection of those two criterion says,
if I measure the covariance matrix of those vectors, SX and SY, coming out over
a batch, I want the covariance matrix to be as close to the identity as possible.
There's a number of different methods that have been proposed to,
that are kind of similar to this, Barlow-Twins.
So this one is called Vic-Rag from my group at Meta in collaboration with Jean-Ponce.
And then variations of it, but like similar methods from Berkeley in
the E-Mise group at Berkeley called NCR squared.
Yeah, maybe one minute.
Yeah, so this works really well and
I'm going to not bore you with tables of results that show you how well it works.
Only to mention something else, which is another method to do this kind of
self supervised running which is closer to this JEPA architecture called IJEPA.
So this is for learning features for images without having to do the documentation.
But basically it's for masking and this works amazingly well, it's very fast.
It's a new method, paper is on archive.
I don't have time to explain how it works, but basically you run an image
through two encoders, one is the full image and
the other one is sort of a masked image, partially masked image.
You run them through the same encoder or very similar encoder and
you try to predict or to predict the full feature representation of the full image
from the representation obtained from the partial image.
And just doing this produces really good features for images.
You get really good performance on object recognition in images and stuff like that.
Again, tables that show you that's true.
But I'm coming to the end, so the reason for
training those JEPA is to build world models.
So architectures are this type.
So this is a JEPA, but it's also a world model.
That, given an observation about the state of the world,
is going to be able to enter an action or imagined action in latent variables.
It's going to predict what's going to happen next in the world.
And once the time passes by, we're going to observe what happens and
then perhaps adjust our system to train.
But we want to use a hierarchical version of this where we can have a higher
level, higher abstraction, higher level of abstraction representation that will
allow us to make predictions further in the future.
Okay, I can't tell you the details of how I'm going to get to the train station,
but I know I'm going to have to be at the train station around 4 PM.
Okay, so that's the high level.
And we have early experiments with sort of various complicated neural net
architectures which I'm not going to detail to train from video,
try to predict basically what's going to happen in the video using warping and
stuff like that and it works really well.
But in the end, what we'll have is a hierarchical system from which we can do
a hierarchical planning and then we'll have been trained to predict what's
going to happen in the world as a consequence of actions or
latent variables that we can observe, that we can infer.
And those systems will be able to plan and reason and
will be controllable because the behavior is entirely controlled by the cost
functions we ask you to minimize.
And so much more controllable than current LNMs and that's pretty much the end.
So cell supervised learning is really the ticket.
Handling and certainty can be done with this energy-based model method and
using the joint embedding architecture that allows us to avoid predicting all
the details that are irrelevant about the world.
Learning world models from observation and interaction and
then reasoning and planning is done by basically gradient-based minimization
with respect to actions.
And that's it, thank you very much.
Thank you, John, for the great talk.
Now we'll have the second part, which is the Fireside Chat between John and
Osama, so please.
Thank you very much, John.
It was actually truly inspirational because it is definitely different
than your typical machine learning talk, so I enjoyed that.
Well, to you to throw away all the basic pillars of machine learning, so yes.
So I've collected a bunch of questions, some coming from the audience,
some coming from our institute and our faculty.
And we'll try to go through them in 20 minutes or whatever we can cover.
Normally, I would commit to answering every question on social media, but
because we got 150 questions, I'm afraid to commit my time or yours to this at
this point, but we'll try our best.
So I'll start with my first question.
It's been a long-standing wisdom in statistical inference and
probabilistic reasoning that when the number of parameters of a model gets
large enough, you kind of lose your ability to generalize and
you start just memorizing data, and we all know that that's no good.
That's just too detailed, the bias variance trade off.
But somehow, deep learning seems to have broken through this barrier.
When we went from regular neural nets to the deep nets, and
is there an intuition or understanding today as to why this is working in
LLMs with hundreds of billions and now trillions of parameters.
Right, well, the fact that it is working,
that you can train a ridiculously over-sized neural net, and
it will still work reasonably and generalize is dumb-founding.
So much that it contradicts every single thing that has been written in every
statistical textbook.
That you should never have more parameters than you have training samples, right?
If you're fitting a polynomial or something like this.
But we knew experimentally, even in the late 80s and early 90s,
that you could make those neural nets pretty big.
And even if you didn't have a huge amount of training data,
it would still work pretty well.
There was just no theoretical explanation.
So the theorists told us, you're wrong, you're stupid.
This cannot possibly work, so I'm not gonna believe your results.
And that's in part what made it very difficult to get neural nets
accepted in the late 90s to the 2000s.
But it turns out there is a phenomenon that has since been named
double descent, which is that if you increase the number of parameters in
a model for a constant size training set, your training error,
of course, is gonna go down, right, to zero, probably.
But your test error is first gonna go down, go through a minimum, and
then go up when you start having parameters,
a number of parameters that is commensurate with the number of samples that you have.
Okay, so that's when the model starts to be over parameterized, and it goes up.
But here is the thing, if you keep going up, if you keep making the model more
complex, the tester will go down again.
It will go through a maximum and then go down again.
That's called the double descent phenomenon, nowadays.
And it will do this if you regularize the parameters somehow.
You don't necessarily need to regularize explicitly because neural nets have some
sort of implicit regularization in them.
But you see this phenomenon, even works if you fit a polynomial, right?
Fit a 10 degree polynomial with 11 data points.
And your fit will be horrible, right?
Because the polynomial has to go to every single point and
it's gonna go wild in between.
But if you increase the degree of the polynomial to something like 20 or
30, and you regularize the coefficient, your error goes down again,
your test error goes down again, the fitted polynomial goes through every point.
But it's less irregular than with just degree 10.
So this existed all along, it's just that people didn't realize it was a thing,
or at least people who were not practitioners of neural nets who
had realized this was a thing.
So do we have any explanation why this is a thing?
So there's a lot of conjectures, there is some theoretical work.
Some people claim it's about the dynamics of gradient descent.
There is some sort of implicit self regularization in neural nets that occurs.
Whereby the system kind of recruits just a number of virtual
parameters that it needs somehow.
Some say it's regularization due to stochastic gradient.
So stochastic gradient descent, which is noisy.
And so perhaps that forces the system to find robust minima in
the objective, in the loss, that generalize better.
It's not clear, there's a bunch of different things.
Yeah, definitely one of the mysteries that keep us interested.
This question comes from Raman Chandrasekharan or
Chandra, who's one of our senior research scientists in Seattle.
How long before LLM, and maybe, I don't know, models in general,
can genuinely start saying, I don't know the answer to this question.
As opposed to attempting to guess the right autocomplete anyway,
because that's what it's programmed to do.
Yeah, so I don't think current LLMs can really do this at the moment.
I think it's probably possible with architectures, the type that I show.
Because if there are no good minima to the objective that the system is
attempting to minimize to produce it, it's output, it's gonna say, well,
I found this thing, it seems to be minimizing this objective, but not very well.
So probably it's not the right answer you were looking for.
Or by the shape of the minimum, of this energy minimum, perhaps, you could say,
like if it's really a sharp minimum, then that's the one answer that
corresponds to the question.
If it's kind of a shadow minimum, maybe there are multiple answers that are possible.
So you might be able to attribute, to map energy levels to,
of different answers to a confidence level.
To, this is a question from me, I guess.
Two aspects of critical importance to,
let's say, GPT or large language models that are not talked about a lot by
the companies who do them are data curation.
Getting that clean data, that balanced data, that representative data,
which by the way, counter to popular belief, open AI spent a lot of its money
on curating just that right corpus so that they can do the training reliably.
And the second part, which is something we're big believers in at the Institute for
experiential AI, experiential AI stands for AI with the human in the loop.
Having that human intervention through relevance feedback,
that we know now open AI is doing and has been doing.
And some of the queries are actually taken over by humans at some point when
they make enough errors to come back.
But the good thing is they learn from them and we think that's a great practice.
Why do you think the companies don't want to talk about the importance of the data
and the importance of the human in the loop?
I don't know if they don't want to talk about it.
I mean, it's clearly very expensive to create data to produce a good LLM.
But in my opinion, it's doomed to failure in the long run for two reasons.
The first one is the curation requires going through this enormous amount of data that
you want to train the system on.
And any data you eliminate, it's less training data for your model.
But the second thing is even with human feedback,
human feedback that rate different answers or fine tune the system for
certain question and answers, sort of manually curated.
If you want those systems ultimately to be the repository of all human knowledge,
the dimension of that space of all human knowledge is enormous.
And you're not going to do it by paying a few thousand people in Kenya or
India rating answers.
You're going to have to do it with millions of volunteers that find
the system for all possible questions that might possibly be asked.
And those volunteers will have to be vetted in the way Wikipedia is being done, right?
So think of LLMs in the long run as a version of Wikipedia plus your favorite
newspapers plus the scientific literature plus everything, but you can talk to it.
You don't have to read articles, you can just talk to it.
And so if it's supposed to become the repository of all human knowledge,
the thing it's been trained to do will have to be curated by
quite sourcing the way Wikipedia is to cover all the possible things that
may be covered.
This is a very strong argument for having open source based models for LLMs.
So in my opinion, the future is inevitably going to be that you're going to
have a small number of open source based LLMs that are not trained for
any particular application, they're trained on enormous amounts of data
that requires a lot of money.
So you're not going to have 25 of them, you're going to have two or three.
And then actual applications are going to be built on top of it by
finding those systems for particular vertical applications.
That's the future.
Sadly, in the industry, there are people who are lobbying governments to
actually make the open sourcing of large scale LLM illegal.
What they're worried about is potential misuse of LLMs by bad actors,
potential users.
So some people in the US, for example, are worried, oh, if we open source our LLMs,
you know China and North Korea and Iran will put their hands on it and that's
going to be bad.
And then some people are worried that the real powerful LLMs are going to be
super intelligent and destroy humanity, which I think is preposterous,
even though some of my friends that I respect actually believe this.
So I think it would be really, really bad if those lobbying attempts succeed.
I'm very much in favor of a future with open based models.
And there's going to be bad actors, but there's going to be countermeasures
against them.
It's going to be, you know, or powerful good AI cop against their nefarious AI, essentially.
So let's shift to this trend.
And this is, I've merged a question from Jimmy Shanahan from our AI solutions hub
with questions from Tomo Lasovic and Ken Church at EAI.
The trend nowadays seems to be heading towards bigger is better, more compute, more parameters.
There's been some studies even suggesting that by open AI themselves that they're moving at a
pace faster than Moore's law, even though now they seem to be normalizing towards it,
although Moore's law itself is slowing down.
So the real question here is how long can this go on?
And will we ask them, what do you think?
I know that we may not have the final answer here, but it seems crazy.
Like all you have to do is wait a few weeks and you hear about the next big model.
Well, so actually in the last few months, you've seen a decrease in the size.
So Lama, for example, the 13 billion version of Lama in terms of raw performance on standard
benchmarks is actually better than GPT-3, which has 175 billion parameters.
And so it's not clear that bigger is better.
With the architecture I propose, I think you can get away with smaller systems that
perform at least as well.
The reason being that when you train in a current autoregressive LLM,
you have to train it to not just accumulate knowledge, not just predict the next word,
but also solve a lot of problems.
So basically, know how to produce the right answer when you specify the question in the
prompt.
And so everything is wrapped into the weights of that single model.
Whereas in the model I propose here, the architecture I propose,
the word model is just a word model.
The task is specified by the objective function, which may include the prompt.
So it may include the representation of the prompt.
And so you're separating different things.
You're separating the inference procedure that produces the output from the word model,
the sort of the mental model of the world that the system uses,
from the task itself, which is specified by the objective.
And you can probably get away with smaller networks for the same performance.
But yes, I mean, there were a few years ago models by Google that had like a trillion
parameters.
There were basically multiple models that were stuck together with some sort of
gating.
Yeah, between them, they've kind of backpedaled on this a little bit.
If you want the system to be practical, like to be used by everyone,
you can't make them like a trillion parameters.
Right now, it'd be just too expensive.
So you have to minimize that size.
Now you can run things like Lama 7 billion on a Mac.
You know, you can run on a laptop.
You can't train it on a laptop, but you can run it.
Yes.
So clearly, you believe you're advocating for a different view of what the machine learning
and AI community should be doing as opposed to what they are doing today.
That's the story of my career.
Yes.
And this question is coming from Ken Church.
A former colleague from AT&T.
From AT&T.
He is at the Institute for AI in Silicon Valley.
Do you believe?
Well, I guess the question is, how long do you think it will take to pivot the field from
where it is to where you would like it to be?
Well, last time I tried, it took 15 years.
If not more, actually, depending on how you count, it might have been 20.
So I don't know.
I think I see a phenomenon in kind of this is a sociology of science question.
When there is something that seems to work, everybody gets excited about it.
And it's a fashion trend type phenomenon where every paper written is about this trend.
I saw this in computer vision back in the early to mid 2000.
Everybody was working on boosting.
That was the thing, you had to work on boosting for computer vision.
And then someone in 2006 and 2005 came up with a different way of doing vision using dense
features like sift and stuff like that using unsupervised running for a middle layer and then
an SVM on top.
All of a sudden, everybody was doing this.
And then starting in 2013, everybody started using convolutional nets.
That came from results.
So now we are in a phase where everybody is focused on LLMs.
And if you don't work on LLMs, nobody wants to talk to you.
But it will change.
So you think it's 15 years?
No, I think it's more like five.
Like I made that prediction that autoregressive LLMs will probably...
Five years, that's true, yeah, they're doomed.
Yeah, I mean, I might be wrong, obviously.
We will hold you to that.
I'll come back and revisit in five years.
Maybe it's a wishful thinking, self-fulfilling prophecy perhaps.
A question for something different here from Sam Scarpino,
director of AI and Life Sciences at the Institute for Experiential AI.
What are the biggest gaps on the education side for graduates of higher education in AI
and in particular the new directions AI is taking?
What do you think is missing?
So I think what's missing...
So it depends which major you're following.
Most computer science curricula in the US are very weak in mathematics.
The requirements for mathematics in a typical CS degree,
the minimum requirement is very, very small, right?
It's one course in discrete math and perhaps in algebra if you're lucky.
Maybe a probability if you are courageous.
But what about optimization?
That would be something that would be very useful.
And then there is courses in physics because the mathematics of inference
and variational autoencoder and stuff like that, graphical models, etc.
The mathematics of this is from statistical physics.
And so if you have a choice between taking your course in, I don't know,
mobile app programming or quantum mechanics, take quantum mechanics.
I'm not kidding.
This is a question that came from the audience and a few of the people at the Institute.
Your thoughts on the current, you know, these recent congressional hearings where
certainly seems like much of the testimony by some Altman was understandably self-serving.
You know, they need to be allowed to compete and have their way of working protected.
At the same time, he's encouraging the rest of the community to be open source.
What would you have said to Congress?
Have you been on those hearings?
I was not invited.
I was not invited to the White House either before that.
So what I would have recommended is that if you want a vibrant ecosystem on top of current AI technology,
you need to have sort of open source based models on top of which an industry can be built.
And that industry will build vertical applications for particular domains
on top of a base model.
You don't want to have 25 companies selling 25 different base models
and keep them closed source.
If you want an industry to be built on top of it, the infrastructure has to be open.
Because that's the only way to really sort of know what you're doing, essentially.
And to have some control about your future, right?
You can't just go like this and pray that.
Unix versus Windows.
Right, so if you go back to the history of the Internet, there was a similar story where
back in 1992 when the built Internet Al Gore started to figure out like what,
you know, how do we build the information superhighway?
They went to see, you know, the big communication companies like AT&T and AT&T told them,
oh, you know, leave it to us.
We'll build the stuff.
It's going to be, you know, ATM and ISD enter the home and blah, blah, blah.
It'll be wonderful and you'll have to pay, you know, $5 per hour.
And Al Gore said no.
He said we're going to make the, what was an ARPANET that became the Internet,
basically available to the public and delocalized and, you know, self,
basically open in terms of standard and no company is going to control it.
And that was a really, really good idea.
We can thank Al Gore for this.
The world can thank Al Gore, not just the U.S.
He did invent the Internet.
And then a similar story happened several years later when people started to realize
that you could use, you know, graphic browsers like Mosaic and Netscape and stuff like that,
right, when the World Wide Web became popular.
So there was a war between Sun Microsystems and Microsoft.
Sun Microsystems said, oh, we're going to sell you servers running Solaris,
the version of Unix, with, you know, our web server infrastructure and Java.
And you're going to be able to build, like, anything you want.
Microsoft said no, it's going to be Windows NT with the IIT web server and the ASP
website, you know, server-side protocol framework, whatever.
They both lost.
Sun Microsystem went bankrupt, was sold for parts to Oracle, and Microsoft essentially
exited the market.
One was Linux and Apache, open source.
And the reason is because it's such an essential basic infrastructure that it has to be open.
It progresses faster if it's open, and it's more reliable, it's more secure.
I mean, there's all the advantages.
And, you know, it's easier for startups to build on top of it.
So in the future, we're going to see AI systems as basic infrastructure.
All of our interactions ten years from now with the digital world will be through
an intelligent virtual agent that will be with us all the time.
It's like every one of us will have a staff of intelligent people working for us.
Okay?
We shouldn't be threatened by the fact that those things will be smarter than us.
Like everybody that, you know, is working with me at fair is smarter than me.
So I don't feel threatened by that.
You're not a very good manager if you're threatened by people who are smarter than you.
So your purpose actually should be to hire people, only people who are smarter than you.
But anyway, so we're going to have those intelligent systems that are going to be
under control that are going to help us, you know, daily lives.
And we need those systems to be open because if it's kind of a closed system controlled by
some company in California, it's going to be able to control our entire
knowledge and data diet.
And that's just too dangerous.
And it's not necessary.
It's necessary for a search engine or a social network because it has to be centralized for
various reasons.
But for an agent like this, it could run on your local device.
It could run on your laptop.
You don't have to talk necessarily with big servers in California.
You don't want to give all your, you know, deepest secrets to that.
So it's going to have to be an open fact form for that reason.
If nothing else, governments around the world are going to insist that this is the case.
So that's why I would tell Congress, make it so that, like, don't ban open source
LLMs.
They're not going to destroy humanity.
Yeah, they're going to be bad actors, but you know, you can have countermeasures
and make it open.
It's the only way to make it safe.
I'll ask, we'll make this a quick question with a quick answer.
And then I know we have some questions live, so we'll switch to those.
In a way, you kind of answered this question when you said LLMs are doomed.
But if LLMs were to become perfect, at least in language, would that ever give us insight
into how language and natural language understanding works?
The language model today is distributed over these billions of parameters.
And do you think we'll ever have an understandable LLM, like, for example, we use PCA to understand
what regression is doing?
Or is that hopeless?
At some point, I think it's going to be right to be hopeless.
I mean, we'll probably learn a lot about, you know, how the systems represent data
and, like, how they manipulate it and stuff like that.
So this is not opaque, right?
We can completely kind of, there's complete visibility on how the systems operate.
Now, the question is understanding really how the decisions are being made.
So I'm actually not particularly interested in those questions, like, you know, as long as
they work properly.
The same way, I'm not particularly interested in figuring out exactly how the brain works.
I'm more interested in figuring out how the brain builds itself so that it works, right?
So I'm more interested in learning than in studying the result of learning, if you want.
So it's the same for those systems.
I'm more interested in how you get them to learn what you want and how to solve the problem
in the end is kind of considerably less interesting, in my opinion.
But at some point, they're going to be, you know, super intelligent,
repulsory of all human knowledge.
You know, it's going to be too big for us to kind of comprehend at a deep level.
Fair.
And by the way, I failed to acknowledge that question came from Walid Saba, who's one of our
senior research scientists at the EAI up in Portland, Maine.
The next question I'll use, and then I'll switch over to audience questions,
comes from Gene Tunic, the director of AI Plus Health at the Institute for Experiential AI.
You believe that deep learning can eventually lead to human-like understanding,
and you have said that self-supervised learning from unlabeled data
can be a powerful tool, although it seems like in human learning, as I was watching your examples,
for example, a lot of that data is, in a way, supervised or tied to some kind of reinforcement
feedback around what to expect, is it good, is it bad, etc. So how, where do you draw that line
between, you know, can we really truly go towards unsupervised, or there's a huge dependence on
supervised and on those labels to get it right? Because the world is, in a way, is telling us
indirectly through supervision. So self-supervised learning, I mean, the reason it's called self-supervised
is that deep down it's actually supervised learning. It's just supervised learning where
the supervision signal is the input itself, right? So in a way, that's kind of, you know, a kind of
simple answer to that question. It's still supervised learning in the end, but with
particular architectures to handle uncertainty and dimensionality and things like that. Regarding
reinforcement learning, there is a point at which you need some form of reinforcement learning,
and you need it in two situations, or at least techniques that have been developed in the context
of reinforcement learning. The first situation is if the objective function that is optimized by
your system does not reflect the ultimate objective function, you actually want to optimize. So for
example, you're learning to ride a bike, your objective function is the, you know, time to the
next fall or something, or the inverse time to next fall, you want to minimize that, right?
But you don't know how to compute this from the internal state of your system. And so you need
to train an objective function to approximate this real cost, which in the context of reinforcement
learning is called a critic. So that's when you need one of those things. The other situation
where you need it is when your world model is not accurate because it's not been trained in all
corners of the state space, and you happen to be in a part of the state space that it wasn't trained
on. Your world model is going to be bad, and your predictions are going to be bad, your planning is
going to be bad. So to prevent this, you need to train your world model using things that are
called curiosity or exploration. And that's another concept that comes from reinforcement
learning. So don't completely abandon reinforcement learning, but minimize its use.
As we switch over to the live questions, let me, I can't help but ask you this question. It comes
from several anonymous people as well as Ken Church, your former colleague. Did you actually say
the revolution will not be supervised? I did, yeah. Okay. But actually, I stole it
from Adi Asha Efros from Berkeley. He had a magnificent slide that was a picture of a wall
painting in Chile someplace, which was one of those kind of revolutionary thing. And I took
that picture and overlaid on it. The revolution will not be supervised. Yes. Okay. So I stole
that from him. I deserve no credit. Shall we switch over to a question from the audience?
Yeah. So first question from Glenn Jenkinson is, what two questions about AI do you wish you
would ask more often? Two questions. I don't know. I get asked a lot of questions. I can't
imagine a question have not been asked. That's relevant. I mean, I think the important questions
are the ones that I'm asking myself. And I wish other people would sort of frame the
problems in the same way. So big question. How is it that any teenager can learn to
drive a car in 20 hours? And we still don't have level five autonomous driving?
That was the first question. So second question is, what are we missing?
That's the answer I want. Joe. Next question from Juan Leylanda. Do you think quantum computing
will have a significant role in the future of AI? No.
Or at least not any time soon. But the time this happens, I probably won't be alive anymore.
So I'm not taking a big risk. No, I don't think so. I mean, there's precious few
situations today where quantum computing could be useful. There's no situation where it actually
is useful because the quantum computers are not big enough at the moment. So it's a huge bet.
I think scientifically it's fascinating. I'm really fascinated by quantum computing at the
conceptual level. I have one or two papers with Seth Lloyd on connections between neural
nets and quantum computing. I think it's a very interesting topic, but I don't think it has any
practical value in the short term. Joe. One last question from Anton Dabura.
To what extent do you see ML models being used for problems that we already have pretty good
algorithms to solve, such as sorting shortest path, linear integer programming, and so on?
How would you characterize the boundary, if any? So there's a lot of problems that we can currently
solve that are NP-complete or NP-hard, and so we can solve them within limits. What we need very
often are approximate algorithms, so methods that give us approximate solutions to complex problems
that, in theory, are NP-hard, NP-complete, whatever, but if you reduce yourself to accepting
approximate solutions, might become solvable. So I think there is a lot to be said for ML methods
that do something that has become to be known as amortized inference. So amortized inference is
this idea that you might have a problem that is formulated as an optimization problem. Every
computing problem can be formulated as an optimization problem. And what you might be able to do is
solve that problem in certain cases, give a solution, and now what you do with this is that you
train in your net of some kind to predict, to approximate the solution to that optimization
problem from the specification of the problem, from the inputs. So that system will not be able to
completely solve the problem in those situations, but for the type of problem that you train it on,
it's going to be able to give you an approximate solution really quickly. Amortized inference.
There is a tutorial on this that was written and given at a recent conference by one of my
colleagues at fair called Brendan Amos, AMOS, very interesting concept.
I will close my questions with one last question, then we'll take a real live one and call it the
end. I have to use this. It comes from one of our faculty who wanted to remain anonymous,
I don't know why, but given the big excitement around LLMs and not without a reason,
what are some of the research directions that are possible to tackle for non-Google slash
Facebook type sized institutions that are under studies? Space for foundational research,
big open questions in need of creative solutions. Thus, if you were a young investigator today,
like a starting assistant professor, what would you do in this environment?
I mean, that's a problem I have to face when I have PhD students at NYU that don't have access to
16,000 GPUs, unlike people at fair. So I think a lot of most good ideas still come from academia,
so you're not going to be Google or Meta or Microsoft on beating the record on
translation or something like that. You don't want to do this in universities.
But coming up with new ideas, for example, the problem I mentioned of how do you do
hierarchical planning? How do you train a system to figure out how to represent the world and
action spaces so that you can do hierarchical planning? It's completely unsolved. You can do
this with toy problems. If you have any idea of how you might approach that problem on toy
problems, you don't have to have tons of GPUs for that. You will have an idea that might have a
huge impact. So if you have a good architecture that you can show, can learn some simple world
model from video, it's the same. You don't have to train on all of YouTube. You can train on
artificial environments and stuff like that and demonstrate that it works. It doesn't have to be
large scale. So this is the kind of stuff you want to do. And then there is a new domain which is
building on top of open source base models. So unfortunately, right now, the best base models,
LLMs are the LAMA class of models from $7 billion to $65 billion. They're not usable for commercial
use. They are distributed with a license for non-commercial use, only for research, which you
can of course use in the university. And there's a lot of work to be done to figure out how to
make those things safe, factual, etc. And you can work from those base models. You don't have to
retrain them from scratch. So you don't need to have roomful, rooms full of GPUs.
We'll try for one last question, maybe two. Go ahead, please, with your question.
Hi. So my question really dwells from the side of, or we'd love to hear your thoughts,
on impact and control of these large language models or any of these models, the fancy models
that you showed with billions of trillions of parameters. So the impact side is, do you really
give or how much thought do you give to the impact that would have on the community or on the people
in general, based on what that model does? And control is, once that model is out there,
how do I make sure that it doesn't do a certain things it's not supposed to do with regular,
the way people used to use internet before those models. It used to be very controlled
environment where you could have, in a way, regulate those environments. But now with models,
it's getting increasingly difficult and a slow process to have or do not have certain things in
those models. Okay. So there is a long view, a very positive one, which is imagine that all of us
have those assistants with superhuman intelligence. So it's like every one of us has kind of a staff
of people working for us, but like super people working for us. This is going to create a new
renaissance for humanity. It's going to increase humanity's intelligence, however you want to
measure it. That has to be intrinsically good. It's been the case in the past that anytime a new
medium was invented or a new way of communication was invented, like the printing press.
Humanity kind of went to the next step. The printing press let
the dissemination of philosophy, science, secularism, democracy, all that stuff. The US
would not exist without the French philosophers of the 18th century. So neither would the French
revolution. So I think same for the internet, that gave people instant access to an enormous
wealth of knowledge. Also disinformation, but okay, I mean, we have to have countermeasures for
every technology can be used for good and bad. We need to have countermeasures for the worst
aspects. But ultimately, I think we need widest possible access to those AI systems by everyone.
Now, how do we make sure those systems don't lie to us? How do we make sure that the information
they give us is not under the control of someone that has nefarious purpose, you know, things like
that, which is I think a good reason for them to be open as I stated earlier. But I think it's a
bright future for humanity, you know, contrary to some people who tell young people don't expect
to live long, which is nuts. I think it's a very bright future. I know you've been waiting for the
next question, but we are five minutes over our time limit. And I know we have to grab a bite and
deliver you to the train station on time, according to the hierarchical plan. So with that, please
join me in thanking Jan for an amazing session today. Thank you.

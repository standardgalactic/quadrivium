Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér.
Today we are going to look at NVIDIA's incredible new AI that can create images and more.
Now, wait a second, stop right there.
Every Fellow Scholar knows that today there are plenty of text-to-image AIs out there where
in goes a piece of text and out comes an image.
They come in all kinds of flavors these days.
So, our question today is, why publish this paper?
Do we really need more of these?
Well, this new paper is called StyleGAN T. Keep your eyes on this part, because this
means that this is a GAN-based technique.
A GAN is a Generative Adversarial Network.
This roughly means that we have not one, but two neural networks competing against each
other and as they compete, they get better together.
Okay, that all sounds great, but I am still not convinced.
What does this give us?
Why would we even use this?
Well, there are two excellent reasons.
Reason number one, GANs are excellent at latent space interpolation.
What does that mean?
It means that we can generate these interesting 2D spaces through a point on this plane, which
in this case corresponds to a font, and the points nearby hide other fonts that are similar
to this one.
So, as we start exploring nearby, we get a beautiful smooth morphing animation between
these fonts.
In our earlier paper, we did something similar with photorealistic material models so artists
can find, or even better, a just material so that it fits their virtual worlds best.
Hmm, so this new technique supposedly can do proper latent space exploration for not
fonts and not materials, but for text to image.
Supposedly.
Now, let's see together if it is true in practice here too.
Here is a previous technique, the crowd favorite, stable diffusion.
This can make an interesting video, but as you see, the results are quite jumpy.
It doesn't feel like one result morphs into the next one.
And now, let's see the new technique.
Oh yes, now that's what I'm talking about.
With this, we can get more continuous results and can explore these latent spaces as much
as we desire, and that is going to be super useful.
You see, what we can do with this is that we write a prompt, for instance, a corgis
head depicted as an explosion of a nebula.
And we don't just get an image anymore, no no, due to its amazing interpolation capabilities,
we get an opportunity to not only witness the birth of the universe, but to choose the
good boy that we find to be the most adorable.
I choose this one, right before it morphs into a cat.
Yes, this one will do.
Which one is your favorite?
Let me know in the comments below.
So its latent space exploration capability is not only an afterthought here, it is one
of the new technique's key features.
Now, remember, I mentioned that this is reason number one of why we should use it.
So, what is reason number two?
Well, two, it is fast, real fast, but to know how fast exactly, let's pop the hood and
have a look.
Now, hold on to your paper's fellow scholars, and what, 0.1 seconds per image, is that really
possible?
Wow, these animations can be made practically in real time.
The age of real time AI image and even video synthesis is here.
My goodness, it did not take decades, and it didn't even take years.
Less than a year after OpenAI's Dolly 2, which asked for approximately 10 to 15 seconds
per image, we are here.
Real time.
I can't believe it.
Wow, this is truly incredible.
However, not even this technique is perfect.
Let's see a failure case.
This is a sign that says deep learning.
Come on, this one again.
Remember our moment with Dolly 2, it had the same issue.
There are techniques out there that do much better on text, for instance, image and video
is better for this, however, it is not nearly as fast as this one.
Yes, that one is about a hundred times slower per image.
So, the perfect text-to-image AI still doesn't exist, every technique offers its own little
trade-off, but man, are they all getting better and better at an insane pace?
Amazing new papers are popping up every week.
So, what do you think?
What would you use this for?
Let me know in the comments below.
This episode is brought to you by AnyScale.
The company behind Ray, the fastest-growing open source framework for scalable AI and
scalable Python.
Thousands of organizations use Ray, including OpenAI, Uber, Amazon, Spotify, Netflix and
more.
Ray lets developers iterate faster by providing common infrastructure for scaling data ingest
and preprocessing, machine learning training, deep learning, hyperparameter tuning, model
serving and more, all while integrating seamlessly with the rest of the machine learning ecosystem.
AnyScale is a fully managed Ray platform that allows teams to bring products to market faster
by eliminating the need to manage infrastructure and by enabling no AI capabilities.
Ray and AnyScale can do recommendation systems, time series forecasting, document understanding,
image processing, industrial automation and more.
Go to AnyScale.com slash papers and try it out today.
Our thanks to AnyScale for helping us make better videos for you.
Thanks for watching and for your generous support, and I'll see you next time.

Okay. All right. So what I'm going to be talking about is how what natural language is, I think, a cognitive computation rather than a, say, purely linguistic one, which is probably why it's relevant to you guys.
And everybody, I think, would agree that it has to do with symbols, although I find in linguistics, you could find somebody who'll argue about anything.
And so whether we have internal representations, for example, at all. And then what I'm going to point out is that these symbols have been data compressed, and that's why language looks as complicated as it does.
Frankly, if it weren't for that, probably everything I'm going to say could have been discovered by Aristotle. And it's just that I think nobody noticed these things.
And then a implication of this would be, we'll get to the end and it allows computers, I think, to be to do cognition. And I'm a little worried that AI is going to stumble across this accident.
So first, I'd like to do two warm up exercises, very brief. One, I'm going to say a sentence and ask yourself, are you having any trouble at all with this? So he saw the saw that I saw.
Anybody have trouble with it? I think it's slid right down, right? So that's actually a little bit remarkable.
Now, the second warm up question, we're going to take warm up exercise, we're going to take 10 seconds. And I want you to think, what are the parts of a chair?
Okay, now, did anybody have in their list, glue, nuts, bolts or screws?
Okay, this will become relevant later. But I want to want you to do it before you get biased towards thinking about things like that.
So this is all part of what I think of as the Descartes-Cahal problem. You're starting out here, somehow you end up with this.
And I'm not going to talk about how Shakespeare could possibly do this. I'm going to talk about how he could tell you these words and you understand what he's thinking.
But in the course of that, we're going to be getting back to how does Shakespeare do this in the first place?
So as far as understanding language, I think it's a much more amazing algorithm, whatever it is we do, than people really appreciate.
And so I want to start off a little bit slowly and point out a few things.
One is, I think it's a computation executed by neurons, most of us biologists would agree on that.
And what you're doing is one person has this three-dimensional, instantaneous, simultaneous picture in his head about something about the way the world's going around.
But we're going to transfer that 3D picture to somebody else using a linear string of symbols that are ambiguous, because words usually all have multiple meanings.
And their arbitrary symbols, like the word cat has C-A-T, has no relation to a cat with two ears and a tail whatsoever.
And the only relationship between these symbols is adjacency and succession.
And somehow that doesn't trigger a bundle of associations in the listener's brain.
It triggers, it assembles the coherent picture of the world, the one that the speaker had.
So how do you pull that off?
Well, the solution in the 20th century, starting with Bar-Hell-L, actually, and also Chomsky, was that language has three main points, and then the rest of the talk is going to be replacing each of these with something else.
First, that language processes arbitrary symbols, unrelated to the thing they're talking about, and it processes them using syntactic rules that don't have anything to do with the meaning of those words.
Second, the function, the language structure is dyadic, function argument like a mathematics.
So there is a subject and a complement in a sentence.
And the complement has a verb and an object.
So each of these two parts, and it goes on in steadily more detail.
And third, that the language processes is determined completely by the words that are sitting on the page.
And I'm going to argue that none of these is true.
And once you see through it, then it becomes simple and regular.
And in fact, before the 20th century, there was a history that meaning did matter.
There was a history that language structure had made of three parts, not two parts.
It went by the wayside with Chomsky.
And there are a lot of clever things that come about from transformational grammar.
But I think the active effort in linguistic to suppress any other thinking has led to the problem.
So, first point, I'd say that language symbols are grounded, not arbitrary.
Grounded meaning that the symbol itself has some physical resemblance to the thing that it's talking about or that it's representing.
But I'll get to that part in a minute.
So an arbitrary symbol, so we have a sentence here.
And the standard view, but here's the arbitrary symbol cats, and it points to a thing out in the real world of cats.
My points to me and the dog points to the dog.
And there's something that we're pointing to with the word chases.
So that's thing out in the real world of the reference.
So there are a number of attractions to this way of looking at things.
So symbolic computations require a few things.
They require variables.
So the word dog can stand for any number of dogs.
It can get instantiated by different dogs.
There's a directional structure rather than just being an aggregate of stuff or a clustering.
There's some hierarchies, you know, like my dog is a unit that's a subunit of this whole thing.
And these symbols remain particular.
It's not like red and white giving pink.
The dog stays the dog and the cat stays the cat, which is a non trivial property of language.
And the thing that Chomsky introduced is basically the idea that you want somewhere or somehow that the dog's chasing the cat.
So somehow cats has to get here.
Oh, can you guys see my pointer?
Does that work? Okay, good. Thanks.
And so Chomsky do attention to that problem and solve that problem.
And actually much of what I'm going to say will wind up deriving a lot of the early transformational grammar,
but from a cognitive basis rather than a linguistic basis and with a whole lot less ad hoc stuff.
Now, there's a problem though, which is, well, actually, let me point out the problem.
There's nothing in these words that says how to assemble this picture.
So you've got these reference with how do you assemble it?
It gets a little better if you are willing to agree that we have in our heads internal representation, which are these little thought balloons here.
And that, which by the way is also something that people argue, you know, there's a whole school of thought says, oh, no, we don't have internal representations.
But still there's nothing here that says how to assemble these internal representations either.
So what happens if we take seriously the grounding that each one of these symbols is grounded somehow and something in the real world.
In other words, physically represents it, whatever it is that's in our heads that represents a dog.
Then you're in a little better shape.
Oh, well, this is what I was saying is that if you don't assemble them, you just get this cluster of images.
Oh, and this word ambiguity is significant.
So 12 word sentence people have shown that can have between 10 to the fifth and 10 to the 28 potential parses.
There's only 10 to the 60th atoms in the universe last time I saw a number.
So that's a problem that's generally not addressed in linguistics.
And, you know, in the 70s, 80s, 90s, the link places like IBM were trying to write programs that would use Chomsky and linguistics process language and they failed.
And the famous joke is that one of the managers at IBM said that every time he fires a linguist the recognition rate goes up.
So while he's definitely on to something, it's not the solution to the problem.
And certainly no hand is towards going on biological.
Now, if you take seriously the ground symbols, then you're doing computations on these representations.
And that allows you to do two things that hard at pointed out a long time ago are critical for cognition.
You have to be able to compare two representations and distinguish between them.
Or you compare them and look for similarities.
Let's you do discrimination on the one hand, let's you do category categorization and generalization on the other.
So that you have this idea of dog independent of any particular dog.
Without that, you can't do cognition.
There's nothing about the word cats down here that I can compute with the word dog.
That's going to let me tell you whether they have the same number of years can't be done the information not there.
But it is up here if you have grounded symbols.
So these are sometimes called icons.
The classical one would be say a pedestrian crossing sign.
It looks a little bit like a pedestrian crossing the street.
Okay, so there's some correspondence and now you can do computations on them.
So, oh, there is a problem, which is that we still remember we still want to not just point to these individual representations.
We want to assemble them into higher things.
So you want to assemble these two into my dog and you want to assemble the whole thing into something about cats and dogs chasing.
And you also have to get cats over here somehow.
So you've got to do a computation.
But if the representations are grounded, there's no guarantee that your computations on them are still going to be grounded and therefore give you a valid result.
So I would say the first thing I'm going to add is that, okay, whatever the language rules are, they also have to be grounded.
And so that grounded this is preserved when you're doing computations on the individual grounded symbols.
So the solution to that I would say got two parts once the grounded rule hypothesis, the speaker and the listener have operations on the arbitrary symbols of words.
That are grounded in the external world so that they're manipulating those words in the same way that the representations would be manipulated.
Or if you prefer, they actually are manipulating your representations, which is the more radical proposal, but that's, and I think that's what's going on, but I'm not going to demand that just yet.
So then you can see how you could ground an object like a dog by pointing to it. How do you ground a rule?
So then my second addition is what I would call the exotopic rule.
So biology, the tissue mimics stuff in the outside world.
So I think you guys probably know that the arrangement of pitches of notes or sound frequencies in the cochlea is in order of the frequency and the representation in your brain spatially is in that same order.
And you have the same sort of thing for the representation of your hand in your arm. And so those are called like tonotopic and somatotopic and so forth.
And you can see the same thing in genetics. You guys will appreciate this that the homeobox genes are expressed last time I checked in the same order that they are arranged in the chromosome.
And so biology has this habit of mimicking in the biology, whatever is going outside.
And so what I'm going to propose is, first of all, the way we see the world is there are things out there, there are relations in between them.
And so you have two entities and a relation between them. And so I'm going to propose, first of all, that that's the structure of the world, at least as the way people see it.
We don't see it as Hamiltonians. And second, we're going to, I'm going to propose that, okay, there's something in our biological tissue or in these rules for manipulating representations that mimics this entity relation entity structure.
So that's the exotopic rule hypothesis. So the operands are going to be these icons, these grounded symbols or hierarchies of them.
And the operations are going to build these hierarchies using only those three things and the result of doing it is still going to be one of those three things.
So then the question is, okay, so how far can you get with this.
So, let's start doing some data. So I assembled about 1000 different sentence fragments and sentences, not linguistics tends to use the Wall Street Journal because usually they were trying to sell to Wall Street people.
But basically newspapers are not the way we really speak. And so I took a variety of, for example, dictionaries intended for foreign speakers tend to have example sentences in them that are.
Real world sentences.
There are that took some books that are written by clear writers and so forth.
So let's take three of these. I'm going to go through it slowly semi obvious but I'm going to go through it for it slowly because once you get over this slide if you're okay with me and if you have objections here's the time to ask.
And you'll see what I'm going through if you get through this and the whole rest of the talk is working through the details.
So the word string, and these are all taken from this these corpuses of senses, experts predict improvements.
So the meaning of that is roughly experts predict improvements. So this is this capital is a standard kind of abbreviation in the field meaning intended to mean that here's the meaning of the word.
And I'm going to notate this parentheses around the entities.
And then the relations don't get parentheses. So you see here we have an entity a relation and an added.
Now, the green grassland.
Well, the is pretty much what it means is that I'm going to tell you about something I already told you about, as opposed to our, where I'm going to tell you something new.
And green is pretty much like green us, you know, yes, representation of color, whatever that is.
And grassland is your representation of grassland.
But what I'm also saying is that the green is a component over a property of the grassland.
Okay.
So I'm going to abbreviate that with this symbol here.
You can think of it as a grassland and folding this component, this property of greenness.
And this green grassland is part of the things I already told you about.
So the has the component green grassland.
So it's a similar symbol but facing the other direction.
So the history to this symbol, which turns out to be wrong, but it's a nice symbol anyway, because it conveys this unfoldingness.
But we don't speak these things doing some in Chinese sometimes in some situations they do, but in English it out.
So we leave this out.
Now here's another one the person who threw it.
So person is, you know, some variable for a person to be instantiated, some particular person who is, you know, sort of referring to something.
But again, arbitrary through we have a representation of throwing it refers to some item that I've already talked about some known item.
The meaning here is also that this person is a component of this activity of somebody through something.
So gets another one of these things.
So again, we don't speak it though.
Okay, so if you're with me so far, then this is why I asked you about the parts of the chair.
We never think about these things we don't speak, just sort of all subconscious.
So this is data compression, which is, you know, if you're familiar with computers you know about data compression if you're a linguist you don't know about it and you don't want to know about it oddly, I find.
And so what this is saying is that when we speak in English, we're doing data compression.
Here's a few more examples.
And then I'll show you, well, I guess this is data.
He met me there.
Well, we don't say at, but that's kind of what you need in order to understand the sense.
They raise prices 8%, well, by 8%.
There's more of these.
He found her asleep.
Well, what he found was the condition of her having the property of being asleep.
And you can go on with this.
Well, indirect objects are important.
He gave mama the, he gave the situation of mama owning the stress.
This one, a linguist debate about, but there actually is a history pointed out to me by Steven Pinker, Steven Pinker actually, that there's strong arguments that ought to be treated this way.
And then he gave mama a function that has two arguments and so forth, which is the standard linguistics way.
So, and then one other thing I'll point out is that jaren's are really little miniature senses.
So cooking, well, it means that something cooks something.
And you just haven't said who the subject is and you haven't said who the object is.
But it's an E, well, E, so that's an E, you know, I can use the word that to talk about it.
It's an entity. Awesome.
Oh, I'll do one more.
Why do we have the word to in infinitives?
What's the point?
Well, if you, what's really going on here, Casey wants Casey to change to the situation of Casey throwing the ball.
Pro is a standard linguistics thing, meaning an unspoken subject.
So they're onto this idea that some things are not spoken.
But my point is that this is common. You see it everywhere.
Oh, the horse that raised past the barn fell. That's this classic garden path sense.
Because if you just say the horse raised past the barn, you're expecting it to be a past tense verb and you've got this picture of a horse racing past the barn.
Then you throw in the verb fell and now you're confused and you have to go read the sentence.
And what you left out was that was the horse that was raised past the barn.
Okay. So how common is this?
It's about 25% of English sentences are not spoken.
So these are various sources like the Longman's dictionary contemporary Englishes were non English speakers.
The Oxford dictionary, the English language has a whole section on examples, which are quite nice.
The Penn Tree Bank, which is a standard corpus linguistics corpus annotated for parts speech and so forth has lots of sentences and there are various other things.
And pretty consistently, it's about 25% of the things that are not spoken.
One of them are the system component relations, but sometimes the adverbs and so forth.
Now, once we agree to that so and so no objection so far. Great.
So what's the is there a pattern to this.
Well, so now the entities are in green the relations are in violet.
And it looks like continuously you're alternating entity relation entity relation.
Well, um, okay, that's nice. So things are beginning to look a little less complex than language is supposed to be.
Now, you can guide this or force the structure.
If you say that there's something like a reading frame so you guys know genetics linguists don't so you'll get it immediately.
So like a reading frame in genetics, you've got this pattern just waiting for the words to be dropped in.
So entity relation entity relation, etc, etc, etc.
So if we take the sentence like he saw the saw, I saw.
So we're going to drop the entity into this box.
Well, then the next box over is got to be is expecting a relation so it's going to take the verb sense of saw.
Then the next box is expecting an entity.
The goes in there just fine. Part of the dictionary definition of the is going to be that it's talking about something that's component something that you've already talked about.
So this are composed of comes along from the dictionary.
So now you're in this box.
So you get saw again but now it's expecting an entity.
And so this helps tremendously with the disambiguation of words and is a clue as to why we can get away with ambiguous words.
I'll show you some actual data on this point later.
You can keep on going.
He saw the saw Clive saw what Clive.
So you're here, but Clive is a proper now.
So it's got to go here.
Well, so now you got an empty space.
So now that's telling you that okay time to do some data decompression.
And we've got a table, which is not all that complicated, where you look up in the table what to insert here, given that you've got an entity here and an entity here and it just depends on things like whether these are proper nouns whether it's accountable now.
You know, it's basically, you know, it's a table to look up table you filled in.
If you had lots of computing time I'm sure one could optimize that table.
But I'll show you later how well this works. And so then now you're here and then now you've got a verb again.
And then you get to the period which is basically relation between sentences.
Okay, you got another empty space.
Okay, well, so you left an empty space here.
Well, actually, that's good, because you really want this saw over here to wind up over here, just like in transformational grammar, because Clive saw that saw.
And so I'll talk later about how to do that.
But you can see that this sequencing template is on simplifies everything.
Now I want to make two points.
So it simplifies the disambiguation data question.
But if the word is ambiguous, the dictionary alone and the word alone can't help you with it.
And I tell you where to place the word in this series of boxes.
So this template has to be exogenous to the sentence.
It's not contained in the words in the page.
And I think there's no other place for it to be than up here.
And I think this is the thing that humans have that lets us process language.
The whole literature in linguistics arguing that some of this structure comes from a previous ability to synchronize muscles in order to throw things.
And I think that's probably on the right track.
You already had some kind of sequencing track.
And now you've learned to put words into it.
The other thing is that if you're inserting gaps like this, well, that also can't be part of the words on the page.
It has to come from an exogenous sequencing track.
And for those of you who are, you know, like Dr. Christon, you think about Bayesian sentence processing.
I would say that, okay, here's the backbone.
Yes, you can add sophistication onto it.
And I'll show you later how much sophistication you...
How much you get from this alone and how much additional sophistication you need.
Okay.
Now, so far we just got a string of e's, r's, e's, r's, and so on.
End of due relation sentence.
Now we've got to build them up.
So now I'll tell you how to do that.
So this is the next thing.
So we're on number four.
The first three are me saying just the three things linguists are doing in the 20th century are wrong.
And now the next set of things are going to be things we can do now that they can't.
So how do you build this stuff?
Well, so here's another sentence just from the corpus.
Saracens built a network of highways to serve blah, blah, blah, blah.
So Saracens built a network.
So this all goes as I've been telling you.
Now you get to all.
What you want to happen is a network to be an entity now.
And so we'll say for the moment that some place around the word of there's an operator that's building things.
And what it's going to do is going to build a network.
Now, it could do it again and build Saracens build a network.
Oh, and if the next word were bricks, that would probably be the right thing to do because you'd have always an advert.
But in this case, it's not the right thing to do.
So you've got a preposition here, and you're just going to build this.
It's all called prepositional attachment, and it's largely an unsolved problem in logistics.
And for us, what we do in the computer program, I'll show you later, is we do it both ways.
And what will happen if you guess wrong sooner or later, that parse dies because there'll be some other conflict that it creates.
Now we keep on going of highways, you get to two.
And now, if we take the adverbial sense, you would say, okay, now the adverb is going to build this whole thing.
And then it does get to do it again.
So you've got, you built a network of highway.
And now it does it again.
And you get certain build a network of highways.
Why?
Well, to advance serve the practical needs of commerce and to do more parsing.
Now, if it had been, they build a network of highways to nowhere.
This would be a preposition.
You wouldn't be, you would build highways to nowhere.
Well, two wouldn't down here would someplace.
And so you would not be doing this.
That's another fork you could take.
But in the present case situation is this one.
Okay, great.
So actually all these operators are sitting right at the same place as the relations.
What, what are the rules for these building operators?
Well, turns out that the building operators are the relations.
They're not just sitting there.
They are the relations.
And we sat down to figure out what those relations.
What those rules are.
Because we figured, okay, this is going to be complicated.
You know, in such and such a circumstance that can only build this far and other
circumstances that can build twice in a row.
Or maybe it only builds from here to here and somebody else builds from there.
Turns out it wasn't complicated.
And so those of you who are engineers or computer guys, or even if you remember
algebra operator precedence hierarchies, you know, you multiply first and then
you do addition and subtraction or you do the exponents first or then you do the
multiplying and dividing and then you do the addition subtraction.
So for given operator somewhere in this precedence hierarchy.
Something just turns out.
I was amazed that you can just write an operator precedence hierarchy.
So that's just another table.
And you can sort of see things like this guy who comes after the law are pretty
low down.
You almost always build over him into something bigger.
Whereas things like periods and question marks.
Well, you don't.
And then other guys are in between.
So.
Then the next thing I'll show you is these Chomsky and.
I had a clock.
I don't know where I put it.
So the transformations.
I did what I never do.
I put it in my pocket because I thought that would be a good idea.
Okay.
Now, so can you do the Chomsky and transformations?
Well, they have a series of rules for how to get cats into here.
They're pretty good rule, but they're a little bit ad hoc sometimes.
But here there's this next amazing thing on the basis of two words flanking.
An empty space when we did the decompression.
Remember, we inserted these various relations.
Well, it turns out that for some reason that I really don't understand.
These same relationships also act at a higher level.
So we can now say cats.
Oh, okay.
We said that they are a component of this whole thing.
Well, they are a component of that.
So cats go into that.
Well, that is a component of this whole thing.
And guess what?
There's even empty space for it.
And so that's called successive, what is it?
I forget what it is.
Anyway, it's standard linguistics.
The point is you do have to do these things in steps.
And here all the algebraic instructions have already been inserted for you.
So poof, all you have to do is the computation.
And similar things.
Well, I won't go into that.
Similar rules can assign, well, actually I may have mentioned it.
Yeah.
So where you start and where you end are governed by some rules.
And it's called C command in linguistics.
There's a complete analog here.
You just have to rewrite it in terms of entities and relations.
And same kinds of rules for deciding on the relationship between pronouns and reference.
If you violate those rules, the sentence isn't grammatical.
So that's how you tell a grammatical sentence from an ungrammatical sentence.
Now, this only English.
So here's Lakota, which is the Sioux language.
So where we would say John found that letter under the bed.
In Sioux, it is John letter that bed the under found.
Okay.
That can be written as EE rel.
There's a phenomenon in linguistics where you can classify languages according to different structures.
And this, you know, E rel E happens to be one set of languages.
E rel is another.
And this is essentially like reverse Polish and, you know, mathematics or computer programming.
But you can do it.
And it's the same general idea.
So let me show you a little bit of what happens with actual parsing.
So Steve Senth, who was at Yale at the time, he's now at Marine Biological Laboratory in Woods Hall.
He's the guy who wrote the original voxel view program that processes images, three dimensional images in terms of boxes that are pixels.
So he wrote a program that's only eight megabytes.
Half of that is the dictionary.
So it's only four megabytes of processor, as opposed to these large language models, which are terabytes of RAM.
So here's just an example that's sort of a hard one.
You probably have to read it twice.
The man who knew the man who Aaron knew knew Clive called.
And there's actually two meanings that you can take out of that.
The man who Aaron knew already, that person knew Clive called.
Or the man who Aaron knew that person knew Clive called.
And this program spits that out.
It also is able to pull out of this who did what to who.
So in this first one, the man knew something.
Clive called somebody and Aaron knew the man.
Whereas this one is different.
The man called somebody Aaron knew something and the man knew Clive.
And you may be familiar with pretty print, which is supposed to be a way of showing hierarchical structure.
Steve came up with this other notation, which I think is much clearer.
If you just look at this, you can say, well, okay, somebody knew something.
Well, what did that person know?
He knew the Clive called somebody.
Well, over here, who knew something?
Well, oh, these guys are all just variations on those left and right component of symbol.
So the man, if you look down at this level, the man is a component of something.
It's a component of who Aaron knew.
Who is a component of Aaron knew?
Because Aaron knew this guy who.
And the man is a component of all that and it's a component of the who, et cetera.
So it's all built in there.
I can, you know, run a program in real time for you if you want.
It's probably simple.
It's just to finish the talk and we can play with that later if you want to run any sentences.
So, um, oh, and there's also a cost function.
That's what this is.
So that if you have to start skipping boxes, you can do it, but it costs you.
And so that's a way of telling whether you're doing something that's legal, but unlikely.
And sooner or later, those things wind up having a huge cost.
And you know, that's not right.
Um, now it doesn't work.
So I'm guessing you guys don't worry about linguistics much, but, um, there's some standard metrics.
Like recall precision for this thing or 97% coverage and consistency of 77%.
Basically, consistency is how many are correct parses do you get?
Well, let me just say that when people do this, they use things like the pen tree bank corpus,
which doesn't even bother with compound nominals like doghouse or prepositional attachments.
You know, like I was showing you the two different ways of doing off.
It doesn't even annotate for that.
I've annotated for that.
And it's still 77%.
Um, if I didn't annotate for that, probably 97%.
And then there's across brackets, which has to do with, well,
how many subunits or is the computer trying to get you to say overlap each other when they shouldn't.
And it's rather small.
And it only happens in really long sentences where people get confused.
So show you some data.
So I told you there were lots of, because of word ambiguity and, um, the resulting parse ambiguity because parses build things differently than entities.
There's a lot of alternative parses.
So on the X axis, you have the number of words.
So let's take out 15 word sentence.
And so the sentences I tried, they have about 10 to the seventh possible parses.
Oh, I forgot to show you.
Back here.
It's saying that for the man who Aaron knew new collide called it's saying that there are 3,800 potential parses and it's pulled out to
both of them are, um, you know, correct.
So back here.
So you've got 10 to the seventh potential parses and you've dropped that down to about.
Let's see.
10 to the so one.
Yeah, so this is dropped it down to about 10 parses here.
Um, so that's pretty impressive.
So what you're doing is removing with just this sequencing track and these cognitive rules.
You're removing orders of magnitude of the ambiguity in the words and the alternative ways of parsing the sense and understanding the sense.
So now you're only down to here.
And so that any knowledge you have to have about, well, what's the guy probably trying to say or any of the Bayesian stuff only has to solve another like 10 fold of this.
This ratio here turns out to be a reduction of 2.74 fold per word.
So three fold.
But that three, if you start raising it to the 15th power, it gets to be a big number.
Then as I stared at that, I was thinking, Oh, 2.74 that number looks kind of familiar.
And then I realized that this reduction here is one over e to the 1.008.
In other words, it's one over e.
And so the interpretation of that, there's a kind of like a target theory interpretation of that statistics.
You're trying to hit a target with radiation or something. Basically, there are enough constraints that in this, in the cognitive rules, I told you about that you are just going to so castically eliminate sentences that are wrong.
And so you're reducing things down to one over e.
And the, then what's left, and that is what happens when you have independent events, though it was treating words as more or less independent.
And the collisions are coming from the sequencing track rules.
Then everything else you need to get from this 10 down to a single interpretation is either the meaning or things like us.
Subject verb case agreement or subject for number agreement or cases, things like that that are relationships between a couple of words are not independent events.
So that's what the rest of linguistics is doing is getting you from here down to here.
Now, I hope at this point I've convinced you that, gee, language is maybe a whole lot more regular than we thought, and, and having a regular algorithm can process way more of it than we thought.
You could then wonder, oh, and that what is processing is the, or building, are these representations rather than words per se.
And then now you begin. So let's wonder about AI.
And what have large language models discovered, all they would have to do is discover or teach themselves to make a sequencing track like this, and a couple of rules, and a sequencing track, what is that it's just a series of probability.
Well, if I'm, if I am an entity word now, the probability is 1.0 that the word that the next word is going to be a relation and the probability of that is blah, blah.
So, you know, it's sort of Bayesian, but it's, you know, as somebody told me, well, rules are just probabilities of 1.0.
So a statistical learning machine could come up with this track.
And then what are the implications of that? Well, let me show you one more thing about how this program works. So we're trying to build a sentence, build the parts into higher and higher hierarchies.
So what we're doing, I'm going to show you that basically we have a shift register for grounded symbols.
So you have cats, cats that because the dictionary definition of that is that has these two component things because cat is going to go into that, and that is going to go into something else.
You've got my dogs, blah, blah, blah, you keep building here and now Chase is a relation.
These guys don't build much.
Chase's will build things. And what you want it to do is build my dog.
And this is, it does the shift register thing. It's going to build, put by this relate the composed of a dog, all in the same box, because that grouping is now an entity.
And it shifts these guys over to.
And then keeps going. It gets to R.
R is going to build. And now it's going to shift the parentheses and chases all into this box and put a new set of parentheses around it.
Oh, these things are all just parentheses are just different shape parentheses to make it easy to follow.
And then we're going to keep on going because our goes all the way to the beginning of the sentence that's going to shift it over again, lump all these things together, shift over again lump these things all together.
And then all you need biologically is instead of parentheses you need something that is going to group.
These subgroups into a bigger group without just averaging them like red and white into pink.
Well, if then you think about whether representations could be doing this, whether your brain could be doing this on representations, in addition to just doing it on words.
These words are really just representations to there's no words in your head. There's just like neural spikes. Right.
So these guys so you've already got representations getting dropped in here.
What happens. So it doesn't seem to me a very big step to be dropping these representations into the same sequencing track and doing the processing.
So if a large language model happened to event the sequencing time, it could also support processing of sensory representations instead of word representations.
And I've just shown you that the rules are the same.
And then if that then let's assemble complex representations.
And it does so in a way that you can now define a thing, which is the one remaining part, which I have an idea about how I could do it.
Because what we think about our thing.
But it can now retains all the information to do discrimination between them and do generalizations so that fulfills harness definition of what it takes to do cognition.
So I'm quite concerned that his large language model model are accidentally going to stumble across a way to be doing cognition.
So that is, I believe it. Yep.
And open to any questions, or if you want to see the parts or an action that's fine.
And I will.

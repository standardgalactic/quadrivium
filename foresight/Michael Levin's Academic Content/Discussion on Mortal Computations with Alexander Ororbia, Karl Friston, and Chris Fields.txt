No, that's great.
Okay, so maybe we could start.
I think, Alex, if you could start
and just give a few more words on this,
the whole notion of mortal computation
and specifically the issue of programmability
and morphology and how we can sort of distinguish
when we're looking at a system,
how can we distinguish what aspects
of mortal computation we're looking at.
Anything along those lines,
I think would be very useful for us.
Maybe we could start there.
Yeah, so I did, by the way, read last night
the paper you shared with me and Carl and Chris
at one point and some of your notions in there.
First of all, a lot of it sounds like parts
of mortal computation, just maybe you're not using
the word mortal computation phrase.
So in terms of the morphology part,
so the argument that Carl and I make in that paper
is just that the structure, right,
is critically important to the actual system itself.
So you don't want to devour,
there's a lot of this idea of the amorphic formulation
of computational models.
So when you take, and actually Carl and Chris had a paper
that predated the mortal computation paper,
but we developed that metaphor further,
you can draw a dot and arrow diagram of a neural network
and that endows it with this pedagogical morphology,
but really at the end of the day, it doesn't matter.
It's just a pile of linear algebra
that will do its calculations.
And then we have to go through
the von Neumann computing architecture
to transmit the weight, memory and all that.
And then that's where this great thermodynamic cost.
So living systems, I know as you know very well, Michael,
but in general are inherently morphic.
And actually this is the part where we didn't have it
in the paper, but your paper used the word polycomputing,
which is the idea that a substance
can compute many different things simultaneously
is like one way to look at that word.
And the idea is that it's all about the substrate.
And so mortal computation then just says,
if we're going to think about artificial
general intelligence or machine intelligence,
we're sort of going about it in the wrong,
potentially wrong direction by having this divorce
of the computational architecture and construct
separate from the morphology or the substrate too,
that this is going to be enacted on
because living systems, if you change the morphology,
you change the properties of the system,
you change also what it can compute, what it can do.
Now you talk about the liquid kind of brain
or the idea that we have this self over time,
which doesn't actually sit at odds with mortal computation.
So the idea is that you're constantly through change,
you're persisting, you're almost like I want to persist,
but I understand that my system
is going through like auto pieces.
So mortal computation sort of absorbs that
and tries to say we should be designing systems
from that perspective.
So just so I don't keep rambling,
you were talking about programmability.
And I think that was an interesting part
that we didn't really get to chat a whole lot about
and it wasn't 100% clear what was meant
by programming the morphology or the system
because actually after reading your paper,
I think that sort of gave the answer as to how,
because I said originally, oh well,
I'm thinking that the morphology or the substrate
dictates strictly what you can and can't do.
Yes, it will change and it will repair
and go through damage or do things like self-replication.
But I wasn't thinking about the human designer,
he would say if we are designing a chimeric system
or something new, manipulating that morphology so easily,
it's more like, well, we're gonna be looking
at computational simulations of that
and that would be programmable.
And then that's where I was talking about,
oh, we could do like a software simulation
of Anthrobots or something, or Xenobots of that form.
And then you could sort of set the properties
of the environment in the system roughly
according to something you wanna look at
and then simulate it and see what it does.
But then your paper sort of essentially,
in your work as an example of you can directly program
like the genetic aspects of a system,
or you can kind of manipulate the bioelectrical chemicals.
You had an example of like a tadpole or a froglet
where if we apply the right electrical stimulation,
you can get it to grow a tail or get it to grow a leg
and there's some properties there.
So I think that is where the programmability is.
And then that would be more,
you would have even actual experience
programming the morphologies.
Whereas I was thinking more from the perspective
of neural networks where we have the structure
and we want that to now be at the very top
of what Karl and I call mills.
And that you remember in the paper,
there's the mortal inference learning and selection.
And at the very, very top is structure.
And that would be something that I think is underexplored
in the area about, oh, maybe we have neurogenesis
and naphyogenesis and then the system sort of has to use
that as another aspect of how it evolves over time.
But that was thought of,
or at least the way I was writing it originally,
it's sort of doing that in its own way.
You're not really involved in saying,
oh, I'm gonna help you do model selection,
but maybe if you encode priors,
that was the other aspect I had.
If you were encoding certain constraints,
you say, well, I just, I'm gonna play,
I'm gonna skip ahead for what evolution
would naturally walk, randomly walk you to and say,
these types of structures are invalid.
That would be like programming,
maybe structure from that perspective.
I don't know if this is making sense,
but I think it's a tricky topic
because I wasn't entirely sure what exactly two
was meant by programmability
because you have experience actually doing that.
So the answer is yes, it can be done from your perspective
and we would just be translating it to chimeric systems
or artificial systems
rather than it only just being like biological material.
I hope that makes sense.
Yeah, yeah, I mean, the kind of programmability
I had in mind, so just as an example,
we have these flatworms, these planaria,
and you can chop them into pieces
and every piece regrows a complete,
perfectly patterned little worm.
And you could ask the question,
how does it know how many heads to make?
And so it turns out that there's an electrical pattern
that sort of a body-wide electrical pattern
that dictates the number and the location of the head.
And the amazing thing about that substrate
is that if you change that pattern,
the tissue will hold it.
So we can change the pattern to say no,
two heads instead of one, and it holds.
And those worms in perpetuity forevermore,
despite their completely normal genetics,
will continue to regenerate as two-headed worms.
So it's a very minimal example of reprogrammability
because we don't have anything like complete control yet,
I think in the future, maybe we will,
but at the moment we don't.
But it is an example where the hardware
in an important sense, so the genetics are normal,
all this, there are no weird nanomaterials,
there's no genomic editing,
there are no synthetic biology circuits,
it's stock hardware, but because of this experience,
this physiological experience that it's had,
it now has a different pattern that it uses
as the sort of target morphology
of what it's going to do if it gets cut.
So that's the kind of thing,
the kind of plasticity that it has where
the material is basically the same,
but it has really a memory of a past event
and that memory guides how it behaves
in anatomical space in the future.
So that's the kind of thing I wanted to sort of explore
with respect to this framework.
And I don't think what you described is at odds
with what you would do with a mortal computer.
I mean, the idea is that is where the programmability
comes into play, you're kind of encoding that
and then seeing how the morphology
and the system evolve over time.
If you want the two-headed worm example,
I don't see any reason why that wouldn't translate
to artificial systems or maybe non-biological systems
if we are able to formulate what that morphology looks like.
I think the key is setting that up
and that's what Carl and I have at the very end of the paper
we talk about, well, for example, someone like me,
a computational neuroscientist, computer scientist,
I don't have access to Xenobots
or the biological material that I'd love to,
or organoids, right?
I really find those fascinating,
but maybe we could simulate them
and that's what we have at the appendix, actually,
at the already long paper, digital morphology.
Maybe that could be a way to sort of bridge the gap
between the computational researchers
and researchers like yourself.
Oh, can we come up with benchmarks or system setups
that I could play with the properties,
mathematical models, maybe,
of these biological morphologies
and then kind of do investigation, right,
without the costs and the barriers to entry
to working with biological material
or some of the things I don't have.
So I don't see anything at odds.
I don't know if Carl would wanna add anything
that I might be missing or not getting.
No, that was very fluent.
I think you've covered everything there.
If I can just jump in for a minute.
It seems to me that this dimension of programmability
could also be expressed as the dimension from uniqueness,
which mortal computers, as I understand it,
have more of than my laptop,
to replicability or exact replicability, copyability.
And to the extent that a system is really unique,
you can't program it.
And in part, just because you don't have two copies of it,
so you can't test what you're doing in any way.
You can't test reproducibility
if you're working with a system that's completely unique.
And biological systems are somewhere in the middle, right?
Laptops are intended to be way out
on the extreme replicability end.
And so because a laptop is completely replicable,
it's a completely generic entity,
it's almost a Turing machine, right?
It's almost just an abstraction.
And so when we're programming,
we can treat it as an abstraction.
And we don't have to worry about things like
where the power comes from
and why the thing maintains the same shape over time
and et cetera, et cetera.
I mean, if it doesn't maintain the same shape over time,
you take it to the repair shop
or recycle it and buy another one.
Whereas in biological systems,
you have to worry about all of that stuff.
And as you point out in the paper,
and as the 4E people kind of have been pointing out
for decades now, that's part of the algorithm,
or that's part of the operating system, that shape.
Whereas it's not involved at all in my laptop
in the operating system.
I mean, even the operating system
can treat the hardware as an abstraction.
So we can, I think more or less identify those two axes,
the dimension of programmability
and the dimension of uniqueness.
And so one of the things that you emphasized in your paper,
I thought this was very interesting,
was the energetics of using the body
as part of the operating system.
And it meant that you didn't have to pay for a lot of memory,
for example, or quite so much processing power.
You do, of course, have to pay the cost
of keeping the body intact.
But as you pointed out, at least in organisms,
that's cheaper than the cost of stamping out more laptops
and then equipping them with enough voltage
to keep them in the classical domain
so that they don't start acting like quantum computers,
which they actually are.
So I think that it would be useful to try to
relate this issue or resource cost
to the issue of uniqueness,
and as well as the issue of programmability.
I'm not sure whether those are,
I suspect those are distinct dimensions,
but I think the usable area of that state space
involves a lot of correlation between those dimensions.
Yeah, I agree.
I think that would be very interesting to explore.
I don't know, Michael, if that resonated with you,
because I feel like that touches on even
pairing the Mortal Computation paper for me and Carl
in the paper you shared with all of us, of yours.
I think the two sort of start to get into that idea
of uniqueness, programmability, and resource cost.
And there are some different dimensions
that you could explore,
because yeah, I also just wanted to comment, Chris,
that at least when I first was writing the paper
and then I shared it with Carl,
I wasn't thinking of the,
I was thinking of biological systems
as sort of an ideal target, right?
You're sort of emulating some aspects of those systems.
So I guess I'd be giving up some uniqueness, right?
Because you said it lies in between the immortal laptop
and the perfectly unique system itself.
I think the other thing I was concerned with
is the artificial intelligence community
really liking the idea of immortal computation
and that complete divorcing,
because you're right, you do lose the moment you leave,
even just a few steps away from immortal computation,
that reproducibility, because it's,
that substrate now is important.
And then Carl and I argue even stronger,
it's the morphogenesis too,
and the changing process, which again,
compliments your paper, Michael, as well.
The idea is that change is also very important
in that evolution over time,
which is not something you're going to have
on your typical deep neural network
that just lives at the top of the von Neumann architecture.
And then the last comment I just wanted to make, Chris,
is yes, that's exactly the key,
is that in-memory processing that we want.
And that's why bringing ourselves as close as possible
until we eventually just reach what we can,
which is the Landauer limit,
and getting ourselves real close to the hardware,
now we're optimizing thermodynamic cost.
And then of course, as you and Carl
and everyone here has shown over time,
that's the flip side to the information theoretic,
variational free energy, but the thermodynamic free energy.
But at the end of the day, we want to be there,
because that's what biological systems are.
They are much closer to the Landauer limit
than pretty much anything in machine intelligence
that we have today.
And that would argue it's even getting worse
because big, big transformers are really bad.
And Carl also and I state the carbon footprint.
So that's a good motivator.
Yeah, I find that energetic analysis very compelling.
You know, I'm very interested in why biological systems
can be quite so efficient.
And I think in many cases, they're efficient
because they're able to use quantum resources
when they're doing molecular computing.
And maybe even when they're doing macromolecular computing.
I guess the one other comment about a dimension
that I wanted to throw in,
which you mentioned a little bit about in the paper,
was this dimension of explainability.
Which AI is very obsessed with the explanation problem now.
And as one gets away from reproducibility,
the explanation problem gets harder and harder.
And in the limit of a unique system,
the explanation problem is infinitely hard
because you can't do experiments.
Because you can't replicate anything.
So we have that other access to work with also.
Maybe it would be kind of interesting
is since you were bringing up these so far three axes
that I caught, you know, uniqueness, programmability,
explainability, we also did talk about resource cost,
kind of putting out this grid
and then you saw Michael and actually I presented,
you guys would have seen in the paper,
but I presented it a couple of times,
like the different types of things
that Carl and I consider variants of mortal computers.
And so obviously Xenobot is a mortal computer.
It actually had a lot more qualities
after I re-read papers again, looking back.
But even, you know, the silicon model
that we had for the non-biological model from Ashby,
we can never forget the great homeostat or allostat.
And so maybe we could plot these a little bit
on those axes too is what degrees that they're trading off.
Obviously we need to figure out
which one of these starts to get real close to the,
like you said, Chris, the really unique.
And then, you know, that would be the extreme one
where explainability would be, you know,
really, really, really difficult.
And we could kind of plot where those are.
That could be an interesting figure to show examples.
And I'm sure, Michael, you probably have other examples
that Carl and I might have missed.
So there might be some other nice biological chimeric systems,
things that are even less biological,
but have a little bit of it.
You did touch on nano technology as well.
And in your, the polycomputing paper you shared with us.
So maybe there might be some in soft robotics.
There might be something there too
that could count as variations to mortal computers
that trade off on these axes.
That's something else.
I just thought of this, Chris explaining.
Yeah, so another model system to think about.
And by the way, we do have simulators of some of this stuff.
So we should be in touch, you know,
give you access to some of that
because maybe you can do some analyses.
You know, we have bioelectric simulators
and things like that.
You know, another kind of model system to think about,
and this is something that Patrick is doing at the bench.
And then I have somebody who is doing this,
you know, the computational analysis of it,
are these gene regulatory networks, right?
And so the abstraction, of course, is quite simple.
It's just, in the continuous case, it's a few ODE's
and they just, you know, their nodes
that turn each other on and off and that's it.
But if you study these things,
you find some really interesting features.
For us, one of the most interesting things is that
if you do temporary stimulation
of the different nodes, right?
So you just grab one of the node values
and you crank it up or down for a little bit
and you keep the structure of the network completely fixed.
So you're not changing the weights,
you're not changing the topology,
the hardware is completely fixed.
All you get to do is temporarily raise or lower
the activation of any node
and then you wait and you see what happens, right?
So if you do that and if you treat it
in a sort of behavioral science context,
you can show things like habituation,
basically six different kinds of memory,
including Pavlovian conditioning.
So these things learn.
And we've been very interested in this question.
I mean, so I have a couple of papers showing how they learn,
but one of the really interesting things
is because we don't let the hardware vary.
So this is not a scenario
where there's some kind of synapse
whose weight gets tweaked by experience.
The fact that they learn the most,
to me, one of the most interesting things about it is,
where is the learning stored?
And this is something that all of the reviewers
of the original two papers got hung up on
because we say again and again, the hardware does not change.
And then they all said, great,
but then you can't have it
because where could the memory possibly be, right?
And it's this dynamical systems thing
where they get chased into a regime
where future stimuli are going to cause
very different outcomes because of their history
than past outcomes.
But I wonder, and so this is what I was gonna ask you guys
to kind of think of them,
to talk about from your framework's perspective.
I wonder if the business of uniqueness
is related to this sort of issue.
I think maybe called privacy or something like that,
this idea that there is an inner perspective to a system
that's had a certain set of experiences, right?
It has a history in the world
that is not available to outside observers.
And this is, we spent a lot of time with my postdoc,
Federico and I spent a lot of time thinking about,
you look at a network,
can you tell whether it's been trained?
And if so, what hasn't been, like, can you read its mind,
you know, this kind of neural decoding kind of thing
because you're not gonna get it from the hardware.
You can, the nodes are no different, right?
So in fact, we have a visualizer
that tries to show various aspects of it.
And if you, you know, on the left and right of the screen,
first you start off with the hardware of view of it.
And that never changes throughout the whole time.
But as it learns, right,
over multiple experiences and stimuli,
something absolutely changes.
And then we have some ways of thinking about it.
But this question of, can you, as an outsider,
is there anything about mortal computation
that speaks to this issue of what you can tell
about a system as an outside observer
versus what you know as the system yourself?
You know, from the inner perspective,
is that something you guys think about?
Well, I'm gonna give a piece of it
and then I'm gonna hope Carl can tag in a little bit
because I think he can flesh this out a little bit better.
So, and this might be confusion
over what you might have explained, Michael,
about the reviewers.
So you said, I fixed the hardware
and on top of that, I fixed the plasticity
because you said we can't change the, you know,
the values of the synapses or the connection strengths.
And I do think mortal computer, mortal computation,
we did address this.
So Carl and I decomposed it in, again,
it goes back to mills.
But again, I might be misunderstanding.
So we're gonna decouple the privacy
and the observer perspective
because I wanna hear what Carl might have to say to that.
But for why learning would still happen,
even when you fix those things, it's just the inference.
And the way that we looked at it in mills
was there's these different time scales of learning.
So if you were to pin the structure of the S and mills
and then pin L and say, you can't modify those.
Well, we still had one more piece,
which was the very fast time scale.
And you talk in your poly computing paper,
I've done a lot of work in that.
Carl obviously has done a lot as well,
predictive coding, predictive processing.
We always have the inference dynamic.
So the idea is that, and I'm sure you thought of this.
This is why I was kind of surprised
the reviewers were maybe not understanding.
So there's like short-term plasticity, right?
So the idea is that when you're doing
expectation maximization in a predictive coding network,
I can still change the neuronal activities,
the firing rates or the spiking rates,
depending on what model you're constructing.
And the synapses never change
and forget about the morphology
because that's a whole nother ball game.
And I would get adaptation.
And there was a very interesting paper that came out,
I don't know now, just like two weeks ago,
Wolfgang Maas in spiking neural nets talked about,
well, look, I don't need to modify the synapses.
I'm gonna do everything in my spiking neural architecture
with just homeostatic variables,
which is, you didn't call it them,
but that's just the adaptive thresholds.
He's like, if these change,
so we have this short-term kind of non-synaptic adaptation,
you get all these effects.
And he actually showed it again,
it's a machine intelligence task,
but showing in all these tasks
without learning in the sense of modifying synapses.
And that was very interesting that you can go very far.
And I'll try to dig up that paper.
It was something I wanted to go in more detail later myself.
So in Mills, right, we're just saying,
well, okay, we're obviously under mortal,
but the inference dynamics
and the fact that these still follow the gradient flow
of the variational free energy
that defines your system
or your functionals that you're looking at
would explain why that adaptation that you found would happen.
And I'm sure you thought of that.
I don't know why the reviewers specifically wouldn't have said,
well, this doesn't make sense.
How could you learn?
If you would pin doll three,
well, it's a static system.
You are freezing it in time.
And then that would baffle me.
So that's my comment
that I do think the framework definitely speaks to that
because Carl and I were very adamant
about the separation of time scales,
at least these big time scales.
I mean, there's all these intermediate ones
that I'm sure you could bring up.
And you need them all
because there's a causal circularity
if you want to build the most powerful type
of mortal computer.
And there was a sentence I can't remember
because Carl and I have done many revisions of that paper.
It might have been in one of the earlier ones
where I mentioned something like,
well, even though I'm seeing morphology as important,
technically, if I was only allowed one,
I still have mills.
It's just a very simple search space, right?
It's a, well, we know that you're here.
You can't change the architecture.
So we didn't break our framework.
So that would allow us to subsume machine learning
and say, well, machine learning
is like this very, very narrow case.
It is doing something that you mills could explain.
It's not mortal, but at least it has like a fixed topology
and synaptic plasticity is there.
And we are just really, really speeding up
the inference dynamics by making it one step
because we don't use EM most times.
And deep neural nets for sure we don't.
So that was my comment about addressing the learning,
the fact that things, if you fix so much,
why would that still happen?
And I definitely think mills,
that piece of the backbone of mortal computation
would speak to that.
Now, in terms of the observer effect
and what does that tell us about what's going on inside?
I have tag team Carl.
What do you have to say to that Carl?
Right.
Well, before I address that,
which in my world is a very simple answer, you can't.
This week I come back to the,
so that was a really interesting exchange
and really interesting examples there.
And I was just thinking from the point of view
of the sort of the classical flows and physics
that would provide a simple picture
of how on earth you can remember stuff
without changing your connection weights.
And I think Alex, you identified the key thing here,
which is the temporal scale.
So, well, where to start?
It's interesting you introduced Wolfgang Maas
because he for many years has been the king
of liquid computation and neck of state machines,
which is not, has the same kind of semantics
as the liquid brain and it's a very powerful
black boxy like kind of a dynamical system
approximator that has been proposed as one architecture
for doing predictive processing
and model computation of the sort.
But the key, I think the key point that has just been made here
is that the dynamics matter
and the dynamics are shaped by the landscape
Lagrangian variation free energy, whatever you want.
And that is a function of the implicit gradients
that depend upon the sensitivity of all say the nodes
in any given network.
That sensitivity can either be read as a connection strength
or it can just be read as a sensitivity,
in terms of to what extent do I change my internal dynamics
given this particular external perturbation.
And of course, that becomes time and context sensitive
with any nonlinearities.
So if you're talking about a nonlinear system,
then there is a, the bright line between the connection
strengths and the current effective connectivity
at this point of time in this context,
in this part of face space or state space
becomes very blurred.
So if you're writing down the differential equations,
you could go one of two ways.
You could just write down a random differential equation
with loads of variables representing interactions
between different types of states
and the response, the rate of change
of any particular state that would entail
the nonlinearity in question.
Or you could arbitrarily say, okay,
now one subset of these variables changes very, very slowly
and I'm gonna call them connection strengths.
And I'm now gonna lift those out of my equation.
So I'm now left with a much simpler sort of
autonomous differential equations
that are now parameterized by other states
that change very, very slowly.
Mathematically, you haven't done anything
but introduce a separation of temporal time scales.
But in so doing, you have now got a different kind of rhetoric
where initially you were talking about voltage sensitive
receptors and sensitivity
and contextualization conductances and the like,
which sets the synaptic efficacy,
which is fluctuating moment to moment.
And now you're talking about these being the connection
strengths, the parameters of your structure
in a mills-like context or the strengths of your connections
or weights in a machine learning context.
But the only difference, I repeat, is just the time scale.
So talking to Mike's example,
how can you have memory without changing your connectivity?
Well, you're just appealing to initial conditions
in the context of a nonlinear dynamical
and run of a dynamical system.
At what point would you start calling this
the kind of memory that could be encoded
in terms of connection strengths?
Well, in those kinds of systems
where the key, not second order nonlinear interactions
rest upon a subset of variables that change very, very slowly
and you say, well, okay, under that adiabatic approximation,
then we'll now call this a different kind of memory.
And it's just because it's slightly slower.
So I think it's, well, I liked the emphasis
on the separation of time scales
because I think that would have dissolved the reviewer's concerns
if you were just talking about really fast learning
in the moment that is all in the nonlinearities
and the dynamics.
I keep emphasizing the nonlinearities, Mike,
because of that sort of the paradox of change.
So as soon as you have nonlinear dynamics in any system
that has at one particular time scale
an attracting set or a random or a pullback attractor,
you have that itinerancy,
which means that there will be some form
of changing sensitivity to all the things
that I am coupled to.
That is definitional of things that have that biotic
or sort of characteristic kind of set.
So, you know, the nonlinearities are certainly
from a classical perspective,
I think they're absolutely key here
and resolve a lot of the distinctions
and give you now a relatively simple picture
that if there was some way to tell the next version of me
where I started, give the next version of me
my initial conditions in the past version of me,
you can, I would imagine quite simply
just write down systems that have this kind of memory
which does not involve it anyway,
a change in the connection weights.
And I'm just wondering whether that, you know,
that if you wanted to simulate that remarkable fact
that the worms remember
that they are on a two-headed trajectory
even when they start again.
I mean, I think the deep question here is
how on earth did they inherit the initial conditions
that characterised the termination of their parent
or what they inherited from.
I think, again, that speaks to this coupling
between different temporal scales.
You know, is this a messenger RNA, you know,
and how does that propagate through to the electric fields
and how does it get back top-down causation,
get back in again, it's a fascinating example.
And I've heard that before, I'm sure you've told me
but I probably ignored it because it's so remarkable.
Not easy to explain.
In answer to the question, can you ever know
what's going on inside a system?
No.
And I say no polemically from the point of view
of the Fianco principle.
You can never know what's beneath a Markov blanket.
You can never know what's on the other side
of a holographic screen.
That's the whole point of a holographic screen
or a Markov blanket.
All you can do is bring a best guess
and as if explanation to the poly computing,
if you like, in the bulk on the other side,
which means, you know, I think that's simple observation.
The whole point of that screen or Markov boundary
is that there is a conditional independence
given what you can measure.
So you can never know other than infer
by what you measure from the behavior,
the inputs and the outputs of a particular system.
Amazing.
So two questions then.
One is, is there, is it just a flat no?
Or is there a degree that is easier to know
for certain kinds of systems?
And then for sort of advanced living cognitive systems,
it's really no.
Or is it just like, is it always the same?
Or is it a matter of degree?
If you're directing at me,
the answer I'm afraid is always no.
But I don't mean that in a sort of pessimistic or,
I mean, the question, you know,
how do you infer what kind of Bayesian mechanics
or poly computation is going on underneath the Markov blanket
or inside a cell or inside a brain?
That question is, of course, my day job
and the day job of nearly every neuroscientist.
It's peaking underneath the Markov blanket
in a noninvasive way that doesn't destroy it
to try and understand the mechanics
and to test hypotheses about what is going on.
But you're always testing hypotheses you will never know.
So there will be situations where the functional anatomy
or the architect reveals itself
through noninvasive imaging, for example,
or even invasive techniques
of the kind that you use every day.
But all you're doing is basically testing hypotheses
about what you think the gerative model is under the hood.
And once you know that or put it another way,
if you knew the gerative model,
then you know the Lagrangian.
If you know the Lagrangian,
you know the intrinsic or internal dynamics
and you can tell a story about poly computing,
tell a story about Bayesian mechanics.
You can tell a story about perception.
You can tell a story about memory.
You can tell your basal cognition.
But these are just stories predicated on the Lagrangian
that governs the intrinsic dynamics
and all that the free energy principle brings to the table
is that you can express that Lagrangian
as a function of a probabilistic gerative model.
So your job is now to identify the form,
the functional form and structure of that model
and all the processes that it entails.
But every time you do that,
you're just testing a hypothesis theory of mind for me
and theory of mind for your Xenobox
and theory of mind for yourselves.
Of course, you can break down at different scales.
So it would be possible to ask about the sense making
and sentient behavior of a single cell in my brain
if you were able to isolate it and get inside there
and do molecular biology or do cellular biology.
But you would no longer be looking at my brain at that scale.
And so, you know, but you could sort of cut across scales
in the good old fashioned way
and start to tell an internally coherent story
about how it all fits together across scales.
Could you back up what Carl said quickly?
Not to interrupt you, Michael.
Yeah, go for it, go for it.
And it's partially gonna be a question for Carl
as he triggered an interesting thought
and then just to quickly say to you, Michael,
that's also why I was hesitant because by definition
since mortal computation rests on the Markov blanket
and it's underwritten by the free energy principle,
I also commit to that as well that no, the answer is no,
you can't see what's under the Markov blanket.
And that's why I was curious enough Carl
would give me anything that may I just didn't know about.
So that was one comment to you,
to Carl and to everyone, really.
So mortal computation also subsumes artificial systems.
So this is where I was curious to know
if Carl, we were to design the internal states,
all the dynamics, we, again, we have the environment
but let's just say you dissimulate it,
you're designing the environment
and you design the Markov blanket
because we talk about even in our paper,
potential sketches of things that you could use
to build the boundary and the transduction pumps
and all the sub pumps and to actually build
a viable artificial organism.
Now we have the internal dynamics.
We are the designer and we have specified internal,
external and the boundary.
Is there something I'm missing that we would still,
cause we've created the Markov blanket.
So now we know what's on the other side.
So the answer to Michael is not for natural systems
that we obviously cannot, did not create, but we made it.
So we made the internal systems.
There's some other concept I'm missing
cause you would be able to now say,
I know everything cause I built the internal states,
I specified every bit of the dynamics
and let's say the environment, you know,
we've constructed that,
we've constructed the Markov blanket, the boundary.
What about that case?
Is it, now we are inspecting it cause we made it.
So we obviously don't need to infer it, we know it.
What about that?
I was curious to know your thought of designed internal states
and designed external states and designed Markov blankets.
If that makes any sense.
Yeah, Mike's gone to the door, so I'll respond to that.
So, yeah, I'm not going to give you
any deep philosophical insight.
You don't already have, but just a very practical one.
I mean, you know, what you just described
is an application of the free energy principle
as a method to simulate various mortal computations
in the service of building hypotheses
about how this thing might work mortally.
So practically that's what we use the free energy principle for
and the design is at least mathematically
very straightforward in the sense I repeat,
all you need to know is the gerative model.
So all you need to be able to write down
is a probability distribution of all the causes
and consequences that constitute your system.
That's it.
If you can write that down
and you can instantiate that in a von Neumann architecture,
you then just solve the equations of motion
that are the gradient flows.
We will have a certain amount of component on that Lagrangian
and you can simulate sentient behavior
and sense making, perceptual actions, self-organization,
everything that you want to do.
Why would you ever want to do that?
Well, in order to test hypotheses
that this reproduces the kind of behavioral thing of interest
which for something like me would be a psychiatric patient
for something like might be a multicellular organism.
So you are now using a simulation
as a way of generating predictions
that then you can match against the observable parts
of the system of interest which adjust the surface.
You have to adjust the action on that system
and to the extent that the sensory inputs of that system
are also known.
That's all you have access to.
So literally that's how we practically use
active inference for example.
We just create simulations of Bayesian mechanics
in a given paradigm
and then we adjust the gerative model
more specifically the priors of that gerative model
until it renders impurity observed choice behavior
the most likely under the probability distribution
of the actions of my simulated simulated.
So in that sense you're specifying the structure
but one could argue that even treating the laptop computer
that is so non-unique
because it affords the opportunity to abstract
and do these simulations.
I'll come back to Chris's point.
Even then you don't actually know
what's going on underneath the hood.
And certainly in conversation
with people designing some risk architectures
and sort of looking at the most efficient buses
they have to guess what's actually being passed here
and there and measure it and get proxies
like temperature and that kind of thing.
You can certainly specify the initial conditions
and the structure and you can do a,
you can reboot and reset it.
So you can to a certain precision specify
the initial conditions and the structure
of which the, you know, that the ensuing dynamics will occur
but to actually know the message passing of a computer
even in simulation, I think would be,
I think you would be able to return to your hard no.
Certainly on the level of, you know
the quantum level that Chris was referring to.
Again, it would be unknowable.
But it's an interesting point
but it does foreground the role of simulations in this,
I think and it comes back to, you know, this, you know
why do we want to know all this?
Well, it's just to basically build,
formalize hypotheses in terms of simulations
that now embody our hypothesis
and then look at the empirical system
to see whether, you know
that hypothesis was correct.
Can I just add another point of view on this?
And if we think about what we do in practice
with ordinary computers
where we have built the thing, et cetera,
part of building the thing,
it's not just assembling the hardware.
We also put a lot of work into building these interfaces
that we call programs.
And so if I'm using some sort of debugging tool
or something like that
where I can run a program in one window
and see what's happening
at some level of the execution trace in some other window
what I've done is constructed a Markov blanket effectively
to use that language
that has a bunch of different IO channels
that access different parts of what's going on in the device.
So we could think about from a biological perspective,
we have these cells
that come equipped with their own native IO channels.
But there's nothing that says that we couldn't,
in principle, build some more channels into the things
so that we could see,
we could see more about what was going on in the inside
not by penetrating the Markov blanket
but by adding some IO capacity to the Markov blanket.
What does that mean physically?
It just means you're using a different interaction
because it's the interaction that defines the blanket
as a set of information transmitting states.
So I think we always have the hard no of the Markov blanket
but we also from an engineering perspective
because we can interact with these systems
in ways that other parts of their environments
can't interact with them or don't interact with them at least.
We're a part of the environment that can open up
new communication channels through the blanket
by changing the interaction
that effectively changes the state space
in which the blanket is defined.
Augmenting the Markov, let's say with reporters
or with optogenetics, I would imagine
it's a good example of this too.
Yeah, well, I mean, in a sense,
FMRI is a good example of that.
Yes, yeah.
Right, we were just adding an IO channel to the brain
that wasn't there before.
I want to...
Sorry.
No, no, please, Carl, keep going.
No, I was just thinking out loud,
so the catch word in my world is sort of non-invasive,
brain imaging, and that has a meaning,
that you're non-invasive, but there isn't,
there is a whole centuries worth of legacy
of invasive studies and lesion deficit models
and depth recordings and the like,
which I think speak to this,
how far into the Markov blanket can you peer
without destroying the thing that you're trying to,
in the Heisenberg sense, trying to get out.
I was witching.
I'll shut up and have a quiet cigarette
while I listen to you now.
I just add that it's non-invasive kind of by convention
and that you're invading the brain with a magnetic field
that wasn't there before.
You're just not damaging it much.
Like Carl said, it's invasive,
but I wonder what that tells us then
about augmenting the Markov blanket.
Because yeah, Carl usually will say,
if we want to non-invasively understand it,
well, then yeah, you can't peer under the Markov blanket.
So mortal computation, I think,
kind of is connecting towards the idea
of what you're saying, Chris, right?
Because we can augment, we can add IO channels.
You are designing, engineering these things.
So now this was something that did not exist.
I don't know, Michael, how does that shed light
on the question that you originally wanted to get at,
which is, you know, peering at these internal states.
Because I think this is like an indirect way.
Because I was, Carl gave me great answer about,
well, if I just design,
because my brain goes to the ultimate engineering,
I'll just design it all myself.
And I even was thinking about if I build the hardware,
but Carl's right, even when you get to hardware,
you're guessing a lot of times,
even with the best educated guesses.
So there's still the Markov blanket
that you're not really breaching.
But if you perturb or change,
it's even like augmenting the cell membrane
with something in your lab's group
is, you know, an example of modifying these things.
What does that do to your question?
How does that shed light on what you're thinking?
Yeah, yeah, I mean, I even wanted to do,
talk about a much more annoying aspect of this,
which as I always do, which is to take it way down.
So nevermind brains, nevermind even cells, right?
My question, you know, also extends like that transition
from, you know, you got a pendulum,
you got a thermostat, you've got, you know, right?
And you can sort of build these things up
and then eventually at some point you get to a cell.
So I'm still curious about whether this impenetrability
goes all the way down.
So we can't read the mind of a pendulum either,
or there is some sense of progressive opacity
as you climb this sort of continuum of cognition
from extremely simple systems,
where I think the conventional story is,
hey, look, it's all third person accessible.
We know exactly what's going on,
but at some point you don't.
And so I'm curious, when does that happen?
And if we think there's a phase transition here,
or if we think this is smooth, I mean,
I tend to think everything is more or less smooth
in these cases, but maybe I'm wrong.
Yeah, that's one thing I wanted to kind of probe
a little bit is how does this play out
when you start, not start at the brain, you know,
where, okay, we can all agree that's sort of very opaque,
but what about from the most simple physics systems, right?
How do you get to that opacity?
Yeah, and then on the flip side,
and I'm conscious that I don't want to monopolize this up,
we only have five minutes left,
but something that's very interesting,
I think an implication of this is that we can't really know
and we're all inferring.
If you take the approach that I took in this memories paper,
where your future self has to make a lot of guesses
as to what your memories mean,
because they were written down by your past self
and you don't have all the metadata,
you have to now interpret these compressed n-grams,
then that leads to this kind of more,
the little kind of disturbing question is,
so we can't even tell what we used to think really, right?
We can sort of guess, but we don't really know then
if this is the case, if that impenetrability holds,
then it's there with respect to our past self
and our past memories too.
So that's kind of wild.
I don't know what we all have to say about those.
I'll refer to this wonderful thing
called the Conway-Cocon Theorem,
Conway of the Game of Life
and Cocon of Quantum Contextuality.
This was published three decades ago now
or something like this,
but they proved using mainly relativity theory
that if they considered a generic observational scenario
and they said, if in any generic scenario,
what you consider the observer has free will
in the following defined sense
that what the observer does is not completely determined
by that observer's past like tone.
So this is, if the observer is not subject
to local determinism,
then the thing being observed
is not subject to local determinism either.
And at the end of the paper, they say,
so one could ask, do we really mean
that electrons have free will
in the same sense that observers do?
And the answer is yes.
They say in their paper emphatically.
So if you did on the free will theorem,
yeah, I remember you're catching this.
Yeah, and if you drag quantum theory into the picture,
then you get an equally strong result
that any system has to be able to effectively choose
its own semantics for how it interprets
whatever incoming information is.
And if you take that choice away,
then you get entanglement.
The system ceases to have a separate identity.
So I think the answer,
kind of the principle answer about opacity is,
yeah, it goes all the way down.
Can I just follow up on that
and refer to Chris's in a screen hypothesis
and the notion of an irreducible Markov blanket?
If you've got a system
that has no internal Markov blankets,
has no deep structure or hierarchical structure
or heterarchal structure,
then that is, I think, where the hard no would apply
or the hard yes of unknowability.
But clearly if it has internal Markov blankets,
you can peel away and invasively
or non-invasively start to get within that.
So I think what you're talking about
is in that sort of vague gradation
of things that are noble and unknowable,
it's just the hierarchical, well,
the depth of the Markov blankets of the Markov blankets.
And there is a kernel of either
an irreducible Markov blanket,
which you can never get into
because you change the thing itself.
But there's also a limiting case
from the point of view of classical physics,
which is when the inner states,
intrinsic, the internal states are the empty set.
So things like inert particles or stones
don't have internal states,
they just have Markov boundary states.
So I think you're absolutely right to think of this
as a gradation, that there are inert things
that are defined operationally
in the sense that their internal states are the empty set.
And you could also say that there are sessile things
that don't have active states.
So all of the states of this kind of particle
are just sensory states, they're just inputs.
Inputs that can also influence the outside.
There's no restriction that sensory states
have to not influence the outside.
So there still can be observable.
And then you get to things that now have a non-antiactive
sector of the holographic screen or the Markov blanket.
And these are things that move.
So you might think these are natural kinds
that have mobility or motility.
And then you get to things that whose internal states now
have a Markov blanket within them.
And these would be the kinds of things that can fan.
And these are usually multicellular things
or certainly compartmentalized things.
I think at each stage, the no ability depends upon
whether either the internal sets are empty or not,
or they are irreducible in the sense
there are no Markov blankets within them.
So I think it's a nice, simple mathematical picture
of that gradation that speaks exactly to the electron
through to the pendulum, to the thermostat
through to a smart thermostat that starts to worry
about whether you want it warmer or colder or not
and starts to pan ahead and moves from homeostasis
to allostasis.
All of this would speak to at different scales,
equipping Markov blankets and Markov blankets
and inducing a deep structure.

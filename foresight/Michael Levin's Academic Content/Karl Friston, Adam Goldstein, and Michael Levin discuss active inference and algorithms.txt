Great. So what I was hoping, Carla, is that we could get your thoughts on how the whole
Active Inference Framework could be applied to something that we've been developing. And
I don't know if you had a chance to look at any of this stuff, but I'll just give you
a brief, you know, kind of a brief summary so that it's clear what we've got. And then
I have some basic questions and then a couple of wacky ideas to kind of bounce off of you
and see what you think. So the basic thing is this, that what I was trying to do is to
have a very basal model of distributed intelligence. And the idea was that we were interested in
unexpected competencies in places where unlike in biology, you know, in biology, no matter
how simple your model, you never have all the information about mechanisms and somebody
can always say, well, there is a mechanism for that, you just haven't found it yet, right?
So we wanted something that was incredibly simple, incredibly transparent, deterministic,
something that everybody thinks they know what it does. And then we can, we can apply
some of the approaches that we take in my lab about taking something that doesn't seem
cognitive and saying, okay, but what, what actual competencies might it have, right?
And so we chose this thing called sorting algorithms. And so these are the same simple
algorithms that all computer science students study for, you know, and they've been studied
for decades. And then we made a couple of, a couple of twists to it. One is that we visualize
their progress from being a jumbled set of digits to an ordered set of digits as a kind
of a traversal of space, right? So the idea is they start in different locations and they
all sooner or later, they all end up in one location where everything is. And so once,
once you view it as navigating that space, then you can ask some questions about what
are their competencies and navigating that space under, under odd perturbations. One
of the perturbations that we made was the introduction of what we call broken cells or
barriers in the space. So if the algorithm wants to swap two numbers in order to proceed
in its, in its sorting trajectory, well, one of the numbers could be broken, it doesn't
move, you can't, you can't move. And so, and we have two kinds of broken numbers, ones
that never initiate swaps and ones that actually never, never swapped no matter, you know,
who initiates them. And so that allows us to ask questions about things like delayed
gratification. In other words, can it go further from it? If it encounters a barrier,
can it go further away from its goal in order to then acquire gains afterwards? And this
is, you know, William James talked about this, of course, as, as an important type of basal
intelligence. And, and that, and that breaks a common assumption with these algorithms,
which is typically you assume that the material is, is robust. In other words, when you, when
an algorithm says to do something, it gets done. But, but in our case, not necessarily. And
we never introduced, this is important, we never introduced any extra code to check if
things got done. It's, it's the standard algorithm. So it just keeps on rolling. There is no
code to see how am I doing? Did things work out? No, no code for any of that. The second
thing we did was to break the, the, the general version of this is centralized. So there's
like this omniscient controller, and it's following one of several algorithms to kind
of move numbers around. And we got rid of that and instead made it all bottom up. So every
digit, aka every cell now has its own version of the algorithm running, and it has a limited
local view of who its neighbors are. And it's just following the steps of the algorithm to
try to improve its local environment, that there is no more global, global control. So
it's distributed. And, and, and, you know, we learned a few things. We learned that a, the
distributed version of this works quite well. It actually, you know, they do sort nicely. So,
so that's great. We did see some delayed gratification in the sense that if you, if you
sprinkle in some broken cells, it actually will go backwards and unsort the string a little bit,
and it's effort to then go around the defect as it were. So that's kind of cool. But the most
kind of surprising thing, which is what I'd love to get your take on is this. We, because now
it's distributed and every cell is following its own algorithm, that enables us to do an
experiment that otherwise you couldn't do, which is to make a chimeric string. And it's what we do
in developmental biology, when we put together, you know, axolotl cells and frog cells, and now
you get this frog a lot, and you can ask questions like, well, what shape is it going to have,
right? So, so what you can make is a chimeric string where some of the numbers are following one
algorithm, some of them are following a different algorithm. And again, there is this is important,
there is no code to determine either your own or your neighbor's algotype. And algotype is the
word that Adam coined for this, like, you know, what algorithm, what set of properties are you
actually following a policy? Is it actually following? So there is no code for any of that,
but we know which, which algotype all the cells. So that, and that also works. So chimeric strings
also sort, sort the, sort the arrays, and that's fine. What we then did was we asked basically a
developmental biology question was to say, okay, at any particular point during its journey,
what is the distribution of algotypes within the string? And what we know, and we defined
a quantity called clustering, which basically just means you look, you look next to you and what,
what's the probability that your neighbor next to you is the same algotype as you are. So what
happens is that in the very beginning, that probability is 50% because the algotypes are
randomly assigned to the digits. So 50%. That's our baseline. At the very end, it's also 50%
because at the end, everybody has to be sorted. And there is no relationship between the actual
sort order of the numbers and the algotype. So again, it's 50%. But the wild thing is that in
between those two points, if you actually plot that curve over time, it actually goes like this.
And in between, it's quite a bit higher in statistically very significantly higher than 50%.
And what we see is clustering significant tendency of cells with the same algotype to locate close
together. Eventually, the inevitable, the inevitable physics of the algorithm will yank
them apart and make sure that everybody's in numerical order. But until that happens, they
enjoy some amount of clustering with their, you know, with their conspecific, so to speak, until
then. And so, you know, any thoughts you might have, but more specifically, like one hypothesis
the one could make, even though there's no explicit mechanism for this, but it might be a,
you know, an emergent thing, could they be preferring to be next to their neighbors because
their neighbors be one of the same algotype or more predictable, right? It's less surprise,
you're less surprised when you're sitting next to somebody who's following exactly the same
policies as you are. So I'm curious what you think about that. And I'm curious if there, you know,
what might be a set of experiments that we could do to test that what's going on here is some sort
of implicit surprise minimization, even though there's no actual code for it. So I'll stop there
and listen to what you've got to say. Sorry, I think you're still muted.
I was saying that was a very succinct and clear, a nice summary. I reread the paper a couple of
days ago just to refresh myself, my memory for this conversation. So I didn't realize that that was
the, that final chimeric demonstration was sort of, you know, the most intriguing from your point of
view. But indeed, the way you express it, you know, that does call for further analysis, understanding
and numerical experiments. So overall, just to endorse the choice of the sorting algorithm as,
if you like, a minimal kind of self-organization. I think that, you know, that the self-organization
word needs to be centre stage in terms of, you know, what you're trying to understand here.
And framing like that, it does remind me a lot of self-organizing maps. I don't know if you remember
van der Molzberg's treatment and people, Peter Dion's supervisor, I've forgotten now.
So this notion of self-organizing maps as a very sort of biomimetic aspect of self-organization,
I think certainly puts sorting like algorithms centre stage in terms of biological self-organization,
particularly in things like the visual cortex and why you get that kind of pinwheel architecture,
for example, where receptive field properties tend to cluster together in a smooth way and you get
all sorts of interesting symmetry breaking when you're trying to represent, say, a 5D perceptual
space on a 2D manifold. So that does strike me having this sort of linear sorting algorithm.
And without really having not thought about it before, but certainly the dual pressure to find
a free energy minimizing solution, viewing free energy as an extensive quantity,
more simply, you know, the collective free energy being the joint free energy minimum solution.
You're asking us where the free energy just bounds the likelihood of this particular arrangement.
Then you're looking for the precise functional form of the free energy. And if you've got this kind
of a ponency between the similarity of the algorithm and the similarity of the content of the value,
then I can certainly see interesting behaviors arise in exactly the same spirit that you get
these interesting structures in not epithelia, but in the functional specialization of
cortical representations or sensory epithelia that try to sort of pack three dimensions into one
dimension or are accountable to two kinds of constraints. So again, without really thinking
about it, because I wasn't anticipating that particular question, but I think what you would be
the first thing that you would be looking for is basically what is the Lagrangian or the energy
function that is being minimized. So you could regard this as the sorting algorithm as an application
of the sorting algorithm as a process that is trying to minimize some energy function very
much in the spirit of Markov-Random fields, but in your instance, you just got a one dimensional
field. But the technology of Markov-Random fields I think would be apt to try to understand the
functional forms of the energy functions were under the special constraint, which of course is
the one that you're predicating this whole thesis on, that interactions are only local and therefore
any collective behavior has to be an emergent property, which is truly distributed. So the
definitive aspect of a Markov-Random field is you just have local, you just have local interactions.
And I think that's a really another important architectural feature that comes along with
the choice of the sorting algorithm, which you should foreground, because any distributed,
collective or emergent behavior at a scale beyond local interactions, that emergence
is truly emergent in the sense that all your interactions are local. And of course that is
what the Markov-Random field gives you. It says that you can only express the energy function,
which is the probability of getting this particular arrangement or these particular numbers
in this local clique. You can only express that in terms of a local energy function. And then
of course you can tell all sorts of stories about the importance of that for machine learning and
the like. But you probably want to stick to self-organization. So, and I would imagine the
energy function is now going to be some simple measure of the local differences of the local
gradients. And of course what one would anticipate would be a smoothing, a resolution of, as you say,
the differences. So I think that would be one way of approaching naturalizing this phenomena in
terms of maths by just invoking an arbitrary, not a variational free energy in the spirit of
the free energy principle, but just a Lagrangian or try to identify what is the generic free energy
function that's being minimized here. So that now your view through this sorting space or
your morphological space is now a progression on some wonderful landscape that is defined
by this free energy functional. And whether you can reverse engineer that or not, I don't think
it really matters other than because I think the nice aspect of that is then you can talk about
the dynamics on this free energy landscape. And once led then to very similar sort of notions
in computational chemistry and protein folding and the like, there's a very complex, sorry,
there is a complex Wellington landscape or free energy landscape that self-organization
and computational chemistry adheres to and can be understood in terms of free energy minimum.
Indeed, most of computational chemistry sort of follows this. And indeed, that is identifying
that landscape is the whole point of applying things like large language models or deep RL to
sort of protein folding and other applications. So that would be certainly one view to get a free
energy like formalism or naturalization of this behavior, which I repeat has lots of really
interesting links with self-organizing maps, marker, random fields, image reconstruction,
and self-organization in certainly in some things like the visual cortex, I don't imagine
any mapped representation would conform to these rules. To get this into a
to get it into a free energy principle story, I think you would have to commit to the notion that
each of the cell has its own boundary. And now you're starting to interpret each number
as a thing. And in so doing, acknowledge its openness to everything else, or in this instance,
just its neighbors, which will require sort of a by your formalism of the bi-directional exchange,
so that the value of my next door neighbor is something that I can sense and is, and likewise,
the broadcasting of my number to the next door neighbor is something is an action. So you've got
this openness that is mediated in the simplest way, which is just the broadcasting and sensing
of one unit dimensional, one number, if that is a discrete number. And the view like that,
that means you can then I think deploy the free energy principle in the sense that any
non equilibrium or far from equilibrium, steady state, which I think you would probably have here,
just in virtue of the fact that there is going to be some
breaking of detail balance in the itinerant way in which you move through this space in a
developmental to get to your steady state. One can imagine, well, perhaps not, but
you know, if one puts a little bit of dynamics into this, I would imagine you would advance very,
very clearly the breaking of detail balance, and you know, and have those kinds of solenoidal flows.
I mean, sorry, I distracted myself just by the addition of the frozen cells. That's one way of
breaking. In a sense, the detail balance, and you know, just open brackets. It's exactly the same
device that I resorted to in the very first paper on the life as we know it paper when
simulating the little macro molecules using Lorenz attractors that had inherent dynamics.
But to make it interesting, you had to have a small a certain popular number of the of the
synthetic macro molecules that were insensitive to influences from other macro molecules and
another proportion that could not influence the other one. So it's almost exactly the same
choice. And that's what gave it the interesting behavior. Otherwise, it just basically converged
either to a gas or it at a certain temperature, it would just convert to a crystal.
So both of them being steady state solutions, free energy minimizing solutions. But things got
interesting when you broke the detail balance symmetry breaking by having this
this, you know, this requisite variety in terms of the frozenness in terms of action or sensation.
So I think that's another that's another important thing to foreground. That this may be this
kind of requisite variety may be absolutely necessary for symmetry breaking. And in this
particular instance, breaking detail bounds to get this kind of cell biologically plausible
by mimetic kind of stuff organization, you're unlikely to get that if you're in the absence
of it in the sense that it would converge to a crystal in your instance, just a linear sorting
perfect sorting, which which is which is, you know, doesn't doesn't have that chimeric or itinerant
itinerant aspect to it. So sorry, close back it. So where were we? Oh, yeah. So
if you've got now an interesting system that has a non equilibrium steady state, and in your case
actually because you haven't got dynamics, it will also be an equilibrium steady state,
but it'll still be a free energy minimizing solution. Then you are perfectly entitled to
interpret the numbers as things and inferring things. And all they're trying to infer is the
cause of their sensations, which is just the value of the numbers on one side and the other side,
and they are broadcasting their inferences through broadcasting their own number, which of course
will be the average of well, when sorted, it will be the average of the neighboring numbers.
So on that view, I think you could very easily license an active inference interpretation
a teleology. You mean you would actually need this to simulate protein folding or self organizing
maps or anything, but you would be able to say there is a teleological interpretation of the
self organization using the rhetoric of inference and belief updating simply because we can treat
each number now as a Markov blanket and then something which will never be accessible, but we
can imply or induce internal to each number could be interpreted as an inference process,
and then the story, which you've already said what the answer is, under the assumption that
I live in a world that is maximally predictable, then everything around me is the same as me and
therefore I am going my free energy, my variational free energy minimizer is now going to be
bound when there's the least surprising input. And if I believe that everything is like me,
then that will be when the numbers that I am sensing in my peak are as similar to the estimate of
the number, the place that I should be coming back to our sort of no-deal place paper.
And I think that kind of story will have to be nuanced for the same algorithm
so I'd have to think about that a little bit more, but certainly at least at a narrative level or
a conceptual level I think you can tell the same story there that if the sequence of moves that I
see my neighbour doing in relation to what I know about my neighbour belies the same underlying
dynamic or algorithmic computations, then in some sense they are predictable if I have exactly the
same algorithm under the hood. And therefore, mathematically speaking, that would be the
free energy minimising solution if I can now read my broadcasting of the number as a broadcasting
my posterior beliefs about the number, the estimate of this locale, my niche
in this instance is just labelled with one number. So the number that I have
is basically my prior belief about my niche and I'm just now going to
move my niche around in a sort of egocentric frame until it is consistent with my prior belief
that this is my place, my niche is number 62 for example. And that should be, you should be
able to reproduce the same kind of sorting either analytically through showing that with an appropriately
configured Lagrangian or free energy functional that the system operationally appears to be
minimising, you can now write down the generative model and then show that this can also be
interpreted as an inference process. I repeat under the assumption that the best way to make the
world predictable is to surround yourself with things like you, which and also of course the
locality assumption that I can only talk to the person to whom I'm immediately connected.
So those are some of my thoughts but a lot of those were invented on the fly in response to
your question I'm afraid. Superb. I've got many questions but Adam why don't you ask yours?
Yeah, so it strikes me that up until now we've talked about the relevant sort of agent as being
the individual number with an algoritite. You can think of it as a cell but it strikes me that
there's an interesting macro phenomenon that occurs in the process of sorting which is that it
appears that the list actually minimizes the Komogorov complexity or the description length
necessary to render it. So let's just say you've got an unsorted list with random distribution
of algotypes and there's 10 items in the list. You would need to enumerate 10 numbers and 10
algotypes and there's no reason a priori to think that that would be compressible in any way.
I mean maybe maybe you'd get lucky and there'd be a string of a certain number,
a string of a certain algoritite but in the general case I think you'd actually need to
write out every single entry. But as the list starts to sort itself it actually starts to create
these longer strings of algotypes which means that the minimum description length actually gets
shorter. Yet you still need to write each number out but you can coarse-grain the descriptions
of the algotypes. You can say the first five numbers have the same you know algotype and then
the next three have the same and so on. Now that that's a macro phenomenon but I'm wondering if
there's any evidence or any research that suggests that sort of these self-organizing systems
have a tendency to minimize their description length to minimize the number of factors needed
or something like that. Because if that's the case then it gives us another view where there's
this sort of emergent complexity minimization happening at the collective level. Yes, no that's
an excellent point. I think the simple answer is yes absolutely and I can sort of give you my take
on the literature or the citations that you'd want to appeal to. But I should say it's going to be
a nuanced yes because of the particular focus on the clustering of the algotype. Now the algotype
induces a certain kind of dynamics into the game so it's not as simple as a self-organizing map.
It's how the map actually self-organizes so there's a process under the hood and that I think makes
it slightly more complicated than just understanding self-organized maps. But in terms of another thing
you might want to look into here of course and you probably know more about this than I do but
this has a lot of resonance with artificial life
games in the 1990s and 1980s. It also could be if you wanted to so do
interestingly linked to Stephen Wolfram's Ruliad which is also another local scheme
that generates everything apparently. There's the same sort of notion so he has algorithms
which he calls rules and the rules are recursively applied in a local fashion to generate everything
including black holes apparently and quantum physics and everything. There might be an
interesting point of contact here with these sorry but to come back to the simple answer yes
absolutely so certainly from the point of view of self-organization as described by the free
energy principle. So notice here the free energy principle is just a description of systems that
self-organize to a far from equilibrium non-equilibrium as I said he stayed. It's not a recipe for sorry
in its statement it is not a description or a theological description of inferential
processing. You are licensed to equip your explanation of the self-organization with
reference to inference but that's an application of the free energy principle in itself it's just
a description of anything that self-organizes or any things that self-organize. So in that sense
if there is self-organization under the hood and the free energy principle has to apply and you can
motivate the free energy principle along two lines one would be the sort of playing the Feynman
card which is basically looking at the minimization of free energy as an optimization process
which can be viewed as a gradient descent on sub-fitness landscape or free energy landscape
or into landscape or you can take the Russian perspective which would be the Kalmolov complexity
and from the Kalmolov complexity you get to Solov induction and from that you get to universal
computation which is the home of the minimum description length and minimum message length
so it's the algorithmic complexity version of free energy and so David McKay wrote a quirky
little paper I think 1992 for where he interpreted variational free energy in relation to minimum
message length using crypto analysis as a vehicle to tell that story but to my mind I think wonderfully
connected to two different two different perspectives on exactly the same phenomenon
the ways of describing self-organizing systems that basically both entail a minimization of complexity
a simplification an emergence of order of a particular sort that entails
either compression hence the minimum description or the minimum message length
view from the algorithmic complexity in terms of sort of you know rate coding theorems rate
distortion theorems and the like or you can write it down in terms of continuous probability
distributions and sort of follow through from the Feynman's path integral I think they're both
saying the same thing you know the way I think of this is the you know the end point of any
self-organizing thing or set of things is just going to be the most likely configuration that
they occupy given the kind of things that they are and that basically means that you can always
describe this in a statistical and theological sense as everything providing an accurate prediction
of what its sense is that is minimally complex in exactly the same spirit as the the way that you
would frame complexity in terms of lossy or not losses but lossy compression or minimum
description length or minimum algorithmic complexity so I think that if you if you
if you tell the story that the free energy is an extensive quantity which means that all the
set of numbers or any subset of numbers any partition will all will all look as if they
are minimizing a free energy functional then you can I think say that you know one view of this
functional is to minimize the complexity of the arrangement which should be manifest in terms of
a minimization of algorithmic complexity and you can use it I can never I can never remember
Zemmell Lippf or Lippf what do you know what I'm talking about there's one of these
hierarchical sequential entropy measures
you know there's one way of quickly enumerating the the algorithmic complexity so I think the
if you could join the dots that would be a really powerful view of this and indeed you know
it would be interesting if you could
just for using numerical experiments join the dots
quantitatively in terms of this handcrafted intuitive free energy Lagrangian just based
upon you given three numbers you have to now write down an energy function that is always
going to be minimized by the sorting algorithm so the endpoint conforms shares the same minima of
your energy function it could be really simple it could be this the two differences squared
and added together something as simple as that and if you can prove that the the minima of this
is the same is the same as the the endpoint of your self-organization then you can say this is one
free energy functional that very much in the spirit of hopfield nets and harmony functions
you know in the early days of neural networks spin glass models pots models all of these
mark of random fields you have to write down this kind of energy function and then you just
simulate you know you can you can just do a gradient descent or rearrangement in order to
minimize that so that's one very simple kind of free energy description of it then you'd have
an inferential one under an assumed generative model so if you assume each number actually
has a little mind and a generative model it's trying to estimate or trying to act upon its world
to realize its beliefs about what it's sensing you'd have a variational free energy but then you'd
also have the the algorithmic free energy that you could that apply to any partition
and the point that if you can show that all three all three share the same minimum at the point of
attaining non-equilibrium steady state I think that will be really you know a really nice illustration
that all of these are different facets of exactly the same thing I mean you know it is just a
description of self-organization but articulated in slightly different ways but I repeat you know
once you've got different algorithms I think the process of sorting now is is somewhat constrained
so that it's you know because you've got three different ways of doing this they may have different
they may have different functionals that are being minimized and it may be but I'm not absolutely
sure that the the order matters and as soon as the order matters then you've got dynamics in play
once you've got dynamics in play that I think slightly complicates the simple
algorithmic complexity argument because the algorithmic complexity the universal computation
view it's not really fit for purpose to understand dynamics of organization and indeed most people
would argue it's not fit for purpose to do anything because it's intractable but it's a
beautiful mathematical object does that make sense yeah so one thing then for with Adam's point so
I think that's a really interesting point and it raises another question which is if on the
compression so it's on the compression issue so if we say that what you're trying to compress is
the actual list of numbers plus the ordering of the algotypes then you know everything as you guys
just said but I wonder couldn't somebody argue that in fact there is no list of algotypes to
compress there's only the numbers because it's sort of like you know by the time you get to the end
it's kind of like it's immaterial information it doesn't do you know it gets lost by the time
you've sorted the numbers what do you need the list of algotypes for right they're not really
I don't know I there's something here no I don't think so like if you take the position that the
algotypes aren't relevant once they stop being used then you're imposing as an observer an assumption
that the list is finished moving right but like how do you know that yeah yeah no that's that's
that's super interesting and and and it's like the bigger question of there is this notion of
algotypes that maybe you have to take into account what else do you have to take into account that
we don't know about right like that's that's one of the things that I see is so interesting about
this and then the next thing I was going to ask you Carl is what's the status of the fact that
like all the things that we were just talking about about the cells you know being objects and
exchanging information with their neighbors about algotypes and having predictions I mean
none of that is actually in the algorithm you can see that the algorithm is like six lines of
code like you can see what the algorithm is none of that is there so what's you know it's more
of a philosophical question you know what what's the status of of something and I have the same
question when I first heard about you know photons and least action and all that I was like but there's
no mechanism to know you know to calculate which path you know is going to be best for you so
what do we do with this what what do we do I'm super interested in the sort of I don't know
why they're almost almost you know implicit things that it's doing whereas the explicit algorithm
doesn't have any of that what do you think about that um well probably think the same thing that
you do um I think um yep in a sorry in a sense what I was saying about a nuanced answer once
you're dealing with you know isomorphisms between the local algorithms was exactly this issue that
you bring to the table it's not you know um yeah it could be as simple as each algorithm has a
different objective function different free energy or lipoonov function
or it could be that they have the same but the the actual sequence of updates or moves
is somehow constrained so the movement on the same free energy surface is is it is somehow
constrained to be different so I'd have to know it precisely what the algorithms are
um it probably is the case that I'm just guessing um that they probably don't have
quite the same um objective function or or and often point of view the free energy principle
implicit generative model um so the chimerical um um self-organization is a reflection of the fact
that um not everything is trying to has the same generative model and therefore by definition
will not have the same free energy um functional so that that does complicate the situation and
makes it more interesting in fact um you know from the point of view of this this kind of
requisite variety um but now I've forgotten the your your actual question um which I did have
an answer to what can you remind me what the actual question was sure sure it's so so what
these algorithms have in common with some of the things that you and and chris fields have said
about particles and things and other people apart which is different from what happens in
biology right if if if in biology I said look this cell is exchanging information with that
cell and it's making decisions the next question is excellent what's the mechanism right like what
it like show me show me the the the explicit uh the set of steps by which this cell does that
but but here we don't have that and and presumably you know when we get down to particles and things
we don't have that either so what's what what's the status of these all these amazing things that
they're doing without a a mechanism to explicitly do it right and I'm going to give you an answer which
comes from conversations with philosophers of maths people like max or ramsted that that question
technically would be answered by appeal to what is a mechanics so a mechanics is um for example
the basic mechanics of the theology principle or Lagrangian or classical mechanics under certain
dynamics you know the non-dissipative or conservative so or quantum mechanics where
you can't uh you you you you have to focus exclusively on the on the dissipative dynamics
so the mechanics is a description of the realization of something
where the thing usually conforms to a principle of least action so this is where this is a sort
of deflationary answer to your question that the mechanics in and of itself is an emergent
property of a variational principle of least action that can be cast in gage theoretic terms or
or in terms of things like maximum entropy principles so there are principles
that just describe the shape the spacetime shape of our world these give rise to and usually you
can usually reduce all physics principles to principles of least action the the straight line
the path of least effort um um and once you've written you've written down your principle as a
principle of least action then the particular functional form of the system to which that
principle applies then gives you a mechanics and then that now um acquires a teleology in
conversation but only in conversation you don't need the mechanics mechanics does not engineer
anything it is just an expression of the principle of least action um so very much in the spirit of
basic mechanics are saying before the free energy principle is just a description of things that
self-organize you may or may not want to then go and say oh well this self-organization could be
described teologically as self-evidencing or active inference or decision-making or basal
cognition or distributed intelligence you don't have to do that but it sometimes it can be very
useful when talking to somebody else about it to to to teologically frame it like that and that I
think is your mission I think that's what you're bringing to the table in in the widest sense
you're saying that the mechanics of biotics self-organization have a certain tealogy which is
almost isomorphic to the same tealogy of finding psychiatry or immunotherapy or climate change
and we just got to find the cross-cutting themes so the mechanics the mechanisms
are really just tealogical unpacking of of of the mechanics so in this instance I gave you an
example before that simply the algorithm to implement the algorithm which is probably a
series of Boolean operators would need certain inputs they need arguments and they need certain
outputs those are the the sensory and active states those those define now the Markov blanket
then so you're taking the input is basically whatever my neighbor neighbor's value
whatever I can see and what I can see is my neighbor's value and it has to be a neighbor I
can't see but you know the one for yeah I can't see something along the way away so that defines
the input and the output is my particular number that's what I broadcast but I only broadcasted
in a loop in a local sense so when you're starting to express things in terms of a
a tealogy of self-organization of the kind that people use in you know in the free energy
principle I think you're then you're quite licensed to say well this is just a description for
example of electrochemical signaling you know it there's a magic is this is this is this is the
mechanism it just means that there has to be a local signal that reports or has some morphism
between my state and and you know everything that is not me and we find multiple instances of this
in in biology at different temperance spatial scales and you know the more you drill down the
more you actually specify what it is the more mechanistic you know it will become but you know
at the end of the day that's just the mechanics you're talking about so if you you know if you
wanted to describe self-organization of massive bodies that had no dissipative aspects to them
you know the motion of the planets for example if you if you take yourself back and pretend you
Kepler and you know what is the mechanics of motion of the heavenly bodies they're Lagrangian
conservative mechanics they inherit from the principle of least action that you know has a
relatively simple form before Einstein came along and in terms of you know energy conservation
and you know and all that is implicit in a pattern a path of least you know the least action least
action principle so that would be his kind of mechanics and you start to invent things like
gravity and mass and you know and talk about the teleology of massive bodies being attracted to
each other and that would be you know quite comfortably received as an intuitive mechanistic
explanation for the thing at hand which in this instance is a the motion of heavenly bodies
you know I I don't see there's any real problem from your point of view you've already told the
story you've already got the mechanics it's just you know a question of showing how universal this
kind of mechanics is and universal in the in the special context of local interactions
and all the all the consequences of having a principle or the principles that would apply
to self-organising systems out of equilibrium or non-equilibrium self-organisation, biotic
self-organisation what kind of mechanics must be in play and then you can give particular
exemplars and talk about gravity or cell intercellular signaling or you know and the
locality of that does that make sense yeah yeah yeah yeah and I guess Adam did you want to say
anything before I because I've got another okay so so this is kind of the the really
kind of far out thing and feel free to tell me that this is not a good analogy and you know
kind of too too too far out but I was thinking I was thinking about the analogy of us trying to
analyse these algorithms and what it is that what what are the causes of their behavior as we observe
them and trying to analogize to in a in a you know biological or maybe in a psychological
slash psychiatric context where you have what you think is the algorithm and maybe that's what
your subject thinks is the cause of their actions and maybe that's what you think is the cause of
their actions but it turns out that there's this underlying dynamic that is it's an extra goal
that you didn't know because you couldn't you you didn't see it in the in the steps of the
of the policies that they're supposedly following and I wonder if this is a sort of really basal
kind of I don't know I just just a really basal kind of behavioral analysis that might be important
for the larger and more complex systems in the sense of finding underlying goals for for
for complex behaviors that are not to be found in the by enumerating the mechanisms that you
know that are there which is you know like it basically basically looking at the tendency to
cluster as a as a hidden motivation for their behavior if I can use this kind of like psychological
term what do you think about that Zachary is that silly or are there oh no no I think that's
exactly the application of these principles and the attendant mechanics you know in terms of say
voting dynamics or geopolitical and or just a spread of information on the internet you know
I think some really important deep questions there and things you know why is it
that almost inevitably whenever you look at some ideological political or theological commitment
everybody's 50-50 you know Trump versus Biden Brexit versus stay you know wherever you look
the only evolutionary stable strategy on or for energy minimizing not an equilibrium steady state
is you know usually a 50-50 because that can be subdivided within what this this 50 percent
there's another 50 percent in a sort of self-similar way all the way down so there's some must be
something generically very important universal about that and I would imagine that's you know
you will see that kind of clustering it's interesting I didn't realize I didn't read
it Kevin have to realize you have this sort of the actual order wins out at the end of the day
so you get the increase and then the decrease in clustering that was interesting
I would be if I was a young man working for you as a PhD student I'd like to put a bit of
noise on the numbers and just see if you can keep it alive and dynamic and see that clustering and
that chimeric behavior look at its dynamics and you know something in dynamical systems
theory called frustration that you get in these in these chimeric situations when
you've broken detailed bounds like this which may be a good metaphor for voting dynamics for
example and so I think it's a very sensible idea I'm just reminded have you read that paper by
Connor Hines and if not I'll send it to you so he's he's he he was making an analogy between
Gibbs energy and free energy in terms of exchanging ideas as a model of
collective behavior and it may be you'll find some interesting ethological references
references to that yeah interesting that you know the thing about the thing about them being
pulled apart at the end so so one way one of the things that we did was to ask okay how how
strong would this tendency to cluster be if you if you didn't have this super overlying
being a basic physics of this world that eventually is just going to yank you apart
and and one way to do that is to allow repeat numbers so if I allow repeat numbers then you
can have a long run of fives and the first half of them could be one I'll go type the second half
could be the other and the actual sorting algorithm would be perfectly happy to keep them as is because
the fives are between the four yeah so so we did that and and when you do that you find out that
yeah if you let them do that it actually goes higher the the tendency to cluster is actually
higher that you know so there's this like competing you know and and again looking at it it almost
it almost provides a very minimal model of of the existential sort of I don't know way of life
facing living systems right that the physics of the world are like you're trying to grind you down
but but in the meantime you can do some interesting things that are not incompatible
with them there's no magic I mean it does the algorithm there's no you know there's no errors
but but but yet not quite what what the you know what what the end goal is going to be
according to the actual physics of the system yeah

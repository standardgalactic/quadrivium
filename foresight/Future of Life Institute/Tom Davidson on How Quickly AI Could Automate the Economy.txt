Welcome to the future of Life Institute podcast.
My name is Gus Docker and I'm here with Tom Davidson.
Tom is a senior research analyst at Open Philanthropy where he works with potential risks from advanced AI.
Tom, welcome to the podcast.
I guess it's a pleasure to be here.
So we're going to spend a lot of time on your model of takeoff speeds where you come to some pretty wild conclusions in my opinion.
But first, I think it would be great to kind of situate your model in your broader views.
So maybe you could tell us a bit about your view of AI progress and AI risks in general, in broad terms.
So in broad terms, I think AI progress in the last 10 years has been extremely rapid.
There's been massive progress in terms of analyzing images and videos, in terms of creating images and videos,
game playing, natural language ability, coding, kind of very diverse broad domains, seeing very rapid progress,
and all within a very similar paradigm, the deep learning paradigm, where progress has been fueled by training larger neural nets with more compute
and by kind of improving the neural net algorithms, the deep learning algorithms we're using to train those things.
And I think progress in the last four years has been especially rapid.
So four years ago, GPT-2 was released.
If you haven't played around with GPT-2, I really recommend you do so.
It's a brilliant way to kind of give yourself an intuition for just how fast this field has been moving.
GPT-2 is, by today's standards, a very limited language, conversational chatbot AI.
It can maybe string together a few sentences, but once it's written a paragraph, it's very clear it doesn't know what it's talking about.
It's off topic.
It can't really understand questions you ask it.
It's clearly very, very limited as an AI and has a very limited brittle understanding of the world.
That was four years ago.
This year, GPT-4 was released.
GPT-4 was probably actually trained in 2022.
So arguably three or four years being the gap between GPT-2 and GPT-4.
And again, if you haven't played around with GPT-4, then I strongly recommend you do so.
You'll need to pay some kind of subscription fee on the open AI chat GPT interface,
and there's other ways you can play around with it.
But in my opinion, if you're asking it probing questions and really testing its knowledge,
it's quite a lot stronger than chat GPT-3.5.
And my goodness, compared to GPT-2, it is really very good.
It seems to have a pretty strong, pretty general understanding of many aspects of the world.
It will apply its knowledge pretty flexibly if you try and throw kind of curveballs at it.
It's very good at coding.
You can give it a natural language description of some code.
You want it to write the code.
You can ask it to make some tweaks.
It'll make just exactly those tweaks.
I don't see why I need to kind of, I sometimes do like little Python kind of coding experiments for work,
and I would be, you know, have a much better job having done those projects now using GPT-4.
You know, in the last four years, we've gone from GPT-2 to GPT-4.
I think that's just very startling.
And so, yeah, progress has been rapid and it's been getting faster, I think.
And I think that absolute, like kind of people deciding to be cautious and deciding to go slower for kind of non-financial, non-commercial reasons.
I think the next four years will probably be similarly quick.
And that we'll see a continued fast scaling up of the transformer architecture that is behind the GPT models.
And I expect that by default, that kind of same jump that we saw from GPT-2 to GPT-4 will get a similar sized jump in the next four years.
And it's maybe it'll take slightly longer because scaling does become more expensive as you start to, you know, really be spending, you know, more than a billion on these training runs.
This is the progress.
And as I understand what you're saying, AI is moving incredibly quickly.
What about the risks?
How do you see the risk landscape resulting from this quick progress?
So there are already risks that AI is posing.
There's risks of disinformation.
There's risks of embedding biases in society that are kind of inherent in the data that the natural language models are trained on.
And as AI gets more capable, I expect the kind of severity of those risks to increase in language capabilities.
You know, it's very hard to speak definitively about exactly what risk will arise when.
Because one of the things about language models is that it's hard to predict what emergent capabilities there will be with the next model.
So, you know, these models are pre-trained by kind of just devouring, you know, internet text, just basically reading all of the text that people can easily scrape off the internet and trying to predict what the next world will be on a given web page that they're presented with.
And then it turns out that, you know, that pre-training, just kind of reading things on the internet, gives the language models various kind of emergent capabilities, which are not in any very obvious direction present in the training set.
But I kind of can be can be kind of elicited from the models with a little bit of tweaking after that initial phase is finished.
So, you know, for example, I believe that some people are concerned that the next generation of large language models that might be GPT-5 might make it significantly easier for bad actors to create dangerous bio-weapons.
And presumably that's because there's enough kind of biology-related text on the internet that when it's during that pre-training phase, GPT-5 would be picking up enough biology and also just enough kind of common sense reasoning and scientific understanding in general that it could then provide substantial help to someone who is wanting to make a bio-weapon of that kind.
But it's very hard to predict whether that will actually happen.
It's hard to know exactly what is in the training data and exactly what the language models will be able to get out of that training data when the whole thing is over and done with.
But, you know, I think that is one particular risk that I think there's a decent chance of arising in the next four years.
There's kind of lowering the bar to bioterrorism risk.
There's some chance of a risk that has been has been called kind of autonomous replication and the adaption.
That is that some maybe GPT-5 level systems or GPT-6 level systems would be capable with the right kind of scaffolding to help them along.
There's something like auto-GPT that kind of prompts the system to kind of plan out its actions and make a list of sub-goals and then pursue them one by one.
I think there's a chance that a system of that kind would be capable of kind of copying itself onto a new computer and then using that computer's compute resources to make money, for example, by scamming people
by just kind of doing intellectual work on the internet that humans can get paid for, like doing kind of mTurkish style roles.
And so there's a chance we make this threshold where AI is kind of able to self-sustain and kind of gather the resources it needs to kind of make more copies of itself and increase its power and resources.
And these two risks you mentioned are very near-term, so we're thinking here before 2027 or before 2028?
Yeah, I think there's a chance for sure. Like I said, it's really hard to predict and especially with this autonomous replication and an adaption threshold, my own view is probably more likely than not that that is not possible by 2027, 2028,
but I'd give it substantial probability, maybe 30%, and then probably higher on the bias, whether I'm really kind of making these numbers up, these are just my very loose impressions.
And I don't know, I'm not aware of a very grounded science for predicting these kind of risks. And there are other risks as well. These are not the only ones.
Maybe there's risks from persuasion, propaganda, maybe recruitment for bad actors they can use, language models to automate the process of kind of reaching out and trying to find vulnerable people.
Maybe there are other risks as well, we'll see in the next five years, maybe relating to cyber, maybe relating to significantly improving tech progress in some domain.
Yeah, and I think what you mentioned is that you can't really predict which capabilities will arise. And I think one of the problems here is that nobody can really predict which capabilities will arise.
And this makes the whole area very uncertain. If you couple that with the fact that, as you mentioned, the area is moving very fast, you get some potential risks going on.
Yeah, exactly. And I think one thing that can make it harder to manage these risks is that the default way that we regulate risky technology is a kind of reactive way.
So we allow people to develop the technology, we kind of allow them to deploy it. And then when something goes wrong, we say, okay, that thing went wrong in that particular circumstance, we're going to regulate the use of this technology in this circumstance.
So now, now you're not allowed to use AI to do political campaigns because we've seen it's been abused in that context.
And what I think we probably need for something like AI when there's so many possible risks, and it's really hard to predict which ones, it's something a bit more proactive where we are before deploying it far and wide, testing it in advance, what kind of risks it may pose.
Does this mean that we don't really have any good historical analogies for AI? With other technologies, it may be the case that it's taken decades for them to be deployed and we've been able to do trial and error and build up some sort of safety regime.
But maybe AI is different, maybe it moves much faster than other technologies. Do you have good analogies in mind?
I don't think there are any perfect analogies to be sure. I think your pointer, I think a good tension which makes it hard to find analogies and that AI is a general purpose technology.
So in that sense, it's like harnessing power from fossil fuels or electricity, or maybe computing power. But on the other hand, unlike other general purpose technologies, the underlying technology is improving very, very rapidly.
So with fossil fuels, I'm not aware of any four-year period where we saw this rate of improvement in the underlying quality of the combustion engine.
I'm not aware of similarly with something like electricity, any four-year period where there was such rapid progress in the underlying technology.
And I think there have probably been many, many narrow technologies like Facebook went viral in a small number of years, but it's a narrow domain and that did in fact pose regulatory problems.
The government was either be too slow to respond to the various risks that Facebook did pose. But ultimately, it was a very scoped narrow technology in a narrow domain.
With AI, I think there's a kind of a scary duality with its with its generality on the one hand, and then the kind of underlying pace of progress.
On the other hand, making it especially difficult to manage and regulate as a new technology.
Yeah. And if we look longer term, so beyond the 2030s, what do you think of the possibility of truly transformative AI? When would you expect something like that to arrive?
What is a good way of defining whether AI is transformative?
So one kind of broad new definition, which has been used historically is to say that AI would be transformative if it changed society as much as the industrial revolution change society or the agricultural revolution change society.
And what I understand by that is it's completely changing the nature of work, going from hunter gathering to farming, kind of moving around constantly to being settled in one place, then moving into industry.
And it's also really changing the way that society is structured and the political and economic processes that are appropriate.
That's not a very precise definition, but it has the benefit of being kind of loose and flexible enough that if you're kind of trying to interpret it in the right way, then it's probably going to end up pointing at the thing that you care about.
I think that's a pretty robust definition to use because it's vague. People have tried to kind of precise by the definition and then I think that there are some problems you run into when you do that.
So one way to try and make it precise is to say it's truly transformative if it accelerates the pace of economic growth by say a factor of 10.
That's more precise, but it does have the downside that whether economic growth gets fast, it doesn't just depend on the nature of AI itself.
It also depends on how it's integrated into society and how humans choose to use it.
Like we might just choose to grow slowly despite the possibility of growing much faster.
No dependency depends on how do we even measure economic growth.
There's this big kind of thorny questions about how you measure the growth impacts of new technologies.
At that point, the definition of transformative AI is so tied to its impact rather than to the actual abilities of the technology itself that I think it can be confusing to think about it like that.
An approach I often use is to use the term artificial general intelligence and just say that that is when AI can do any cognitive task that a human professional can do at or above that level.
That's precise, fairly precise, and I prefer it to the kind of economics based definition because it's more about what the underlying technology can do.
On the other hand, you can imagine kind of loopholes where it's not really capturing what you want, where there's like just a few tasks that no one's bothered trying to make AI able to do that AI can't do.
So you say, oh, we don't technically have AI.
And so I think probably sticking with the kind of broad definition is one having the background and then being a bit flexible about exactly how we're defining it.
Thinking about the economic impact of AI is interesting because sometimes if you look at benchmarks, for example, TPT4 scores very well on high school exams and college exams and even the bar exam and so on.
But how does that translate into economic growth or economic progress or automation? It's difficult to say.
And of course, TPT4 hasn't, there hasn't been enough time yet for it to have a great impact, but so far it's not really showing up in the numbers.
I think it's very important to think of the economic impact also and not just the benchmarks.
Yeah, I think especially today's benchmarks are very limited. So what if AI can get this mark on an SAT and so what if it can get this gone big bent?
The tasks that we're mostly focusing on with current benchmarks are not tasks that humans are performing in the real world, in the real economy that they're actually useful to producing goods and services to running organizations, to whatever it is that people are actually trying to do.
With the current way we're benchmarking systems that there's this kind of gap between the tests that we're giving them and then the stuff that we actually ultimately care about in our society, which is kind of useful work.
And it's really hard to know how big that gap is. And it seems like at the moment that gap is potentially pretty big.
And that GP4 is getting really good grades on a very wide range of quite tough examinations, but it's not yet massively adopted to replace lots of people's jobs and to massively improve, increase profits and revenues for lots of companies.
And so I think we should be trying to move towards better benchmarks, which are more closely tied to the actual real world impacts of the systems.
Yeah, and maybe those benchmarks will be difficult to set up, but at least we have measurements of GDP growth as a proxy for useful work as one way of measuring whether AI is doing a lot of useful work for the economy.
Yeah, that's right. It's really getting at that. Is it doing useful work? Part of the question. I do think it has some pretty big downsides in that there's going to be a pretty big lag, especially with earlier AI systems that are less flexible.
So it's taking more work to integrate into workflows. So if you're just looking at GDP, you might think nothing really that much is happening in AI because GDP hasn't picked up and that would be a mistake.
And there's also just a lot of noise in GDP statistics, just inherent noise and then all these other trends which are interlacing.
And what one kind of quite nice intermediate is the size of the AI, the AI industry specifically. So you can look at investments in AI or you can try and kind of add up AI driven revenues across the economy, which I think is a pretty vexed task trying to figure out
how much value right AI is really adding. Those kinds of measurements typically show pretty fast growth of the AI industry, kind of like 30% a year or faster in recent history. You can also look at things like growth of spending on AI chips.
That's quite a kind of concrete thing you can measure clearly. That is maybe intermediate between economic growth on the one hand and benchmarks on the other hand and that it is showing, look, people really believe that this is going to have a real world impact.
They're willing to spend concrete money developing these systems. That means you're getting a kind of real signal about its real world impact, but it hasn't actually had that impact yet. So that's intermediate.
It's also interesting to think about the fact that the entire introduction of computers and the internet to the world over the last 50 years hasn't really increased the growth rate in developed economies a lot. So technologies can have an enormous real world impact without actually increasing GDP.
And maybe there's quite a high bar actually for what we might call transformative AI.
I think there's a very high bar. As you say, computers, you know, they did increase economic growth in the sense that if we hadn't developed computers, economic growth would have been lower.
But they did not turbocharge the overall pace of economic growth. They're more kind of maintaining the trend that we were previously getting from other technologies.
At first, I think that that's what will be happening with AI. And then my view is that once we've got truly very advanced systems, AGI systems that are able to really automate all human labor, that's when we should expect more transformative and unprecedented economic impacts.
Yeah, what I want to do in this episode is to kind of dig into your model of how this might happen, which is, I think, centered around takeoff speeds. I think the notion of takeoff speeds is quite central to how you see AI progressing.
So maybe we could start by talking about what is takeoff speed in the context of AI.
So I think it can be useful to distinguish between two notions of takeoff speed.
The first is what I'll call AI capabilities takeoff speed. So that's, that's focused on the pace of improving in the underlying technology.
So capabilities takeoff speed would be the answer to the question of how quickly is AI improving around the time in which we get human level AI.
So if takeoff speed is fast, then that can mean we go from kind of mouse level intelligence AI to human level AI in one year, and then a year later, we've got kind of godlike intelligence AI.
So kind of very fast increase in AI capabilities just as it's passing through the kind of human level of intelligence.
Then there's another notion of takeoff speed, which I think, especially if we're thinking about economic impacts, it can be useful to distinguish, which is impact takeoff speed.
So that is how quickly does AI's impact in the world increase around that time.
A very fast impact takeoff speed could look like growth is just taking away at its normal two or three percent a year.
And then next year, suddenly it kind of shoots up what economies is doubling every, every two years with explosive growth.
Whereas a more kind of slow impact takeoff speed could be, well, there's the impacts of AI is spread out over many decades.
And, you know, maybe growth gradually gets faster over time, or maybe only temporarily gets faster than to set us back down.
And so, you know, you could, you could imagine those two coming apart if there's loads of regulations, for example, that limit the impact of AI.
You can imagine the takeoff speed of the underlying technology being very fast, kind of somewhat in line with recent, recent trends, but the actual impact takeoff speed being much slower.
A lot of probably we'll talk later about some of the economic objections to kind of transformative growth and both bottlenecks.
I know one, one kind of theme in my mind is that these things tend to affect impact takeoff speed more than they tend to affect the capabilities takeoff speed.
So what do you think a world looks like in which you have a lot of AI capabilities, but not a lot of impact yet?
Is that a stable situation? Because it seems to me pretty unstable.
There would be a lot of incentives to try to deploy these very capable AI somewhere in the world.
Yeah, I think that's right. It's probably temporary. The reason you can imagine it happening is that there are lots of entrenched interests in various professions.
So, you know, lawyers don't want to lose their jobs. Medical professionals don't want to lose their jobs. There are unions.
There's kind of political processes by which these groups kind of will power and influence, and they may want to delay the deployment of systems which would replace them and lose their jobs.
Indeed, that will be a very good thing. There's regulations around who can make various high-stakes decisions, be it in signing something on the legal document or giving a drug to a patient.
The bureaucracies take a while to shake up, and it's not going to happen overnight that suddenly AIs are allowed to, you know, diagnose you and hand you the medication, even if they're actually able to do that.
And because these are slow human processes and bureaucracies, it does seem possible to me that even though there's a large amount of pressure to kind of remove these barriers to rolling out AI, that it could still take a while.
So what you're imagining here is, for example, we have AI models capable of diagnosing a patient or sending a document to a court because of a professional organization in medicine or in law.
Maybe it's just not legal to do so. Maybe you need a human to sign off, or maybe you need even a human to do the full task, and that slows down the implementation of AI in the economy.
Even though AIs might be perfectly capable of diagnosing a patient.
Yeah, even though they might be better.
Do you think that's the default scenario? Has this happened before?
I think that is the default scenario of previous technologies. They do take a while to refuse, and if you kind of have a naive view of like, well, once it's better than everyone move over, you'd be very surprised at what ended up happening in reality.
Many organizations have still not transitioned over to the internet fully. I sometimes go into hospitals, and I'm asked to fill out forms by hand, and I'm thinking, why are we still doing paper documents here?
These transitions can be so. Now, I actually think there's a chance that things are quite different with AI.
So if the capabilities take a speed as fast, then we might rapidly transition to a world where we've got, you know, truly super intelligent systems that are not yet deployed that could very demonstrably add huge amounts of value at kind of almost no effort to integrate them into our businesses could add huge amounts of value because they're kind of smart enough to integrate themselves and immediately kind of like learn what they need to learn proactively and start adding value.
At that point, I think the situation will be without precedent and that we've never, the previous technologies have required active effort to rearrange workflows, the kind of draft new legislation so that they can be incorporated into the real economy.
It would be without precedent for a new technology to be able to do all of that work itself, draft the new legislation itself, lobby the regulators itself, you know, learn what it needs to learn to do an even better job delivering the goods and services itself.
Create by itself, you know, legible examples of inventing and treating diseases which people currently struggle to treat. And maybe AI systems are so super intelligent that they can, without even going through the FDA process, develop a new drug, and then kind of demonstrate quite clearly to everyone that it works in treating cancer that no one else can treat.
Then at that point, you know, legally, new drugs need to go through the FDA. But when there's something so stark as there's this drug which could save millions of lives, everyone knows it would work.
That would create a kind of pressure on the system and the regulatory system to change that I think might be without precedent. And so I could, I could imagine that if, if AI capabilities continue to kind of shoot upwards, that would put increasing pressure on the kind of the regulatory barriers and other barriers to deploying AI widely.
It's of course difficult to speculate on the political economy of future AI. But I think that there might also be demand from the public to get access to these AI models. If, for example, you have a demonstration that an AI doctor can diagnose you better than a human doctor.
And maybe the AI doctor costs 10 times as little. Of course, there would be pressure coming from the doctor's association in a given country. But I can't see this demand not mattering at all. I think it would matter at least somewhat.
I think that's right. In my mind, it's a question of how long. And then there's kind of the kind of incumbent forces trying to preserve the status quo. And then there's this maybe increasing tide of technological abilities that AI is able to provide an increasing pressure to kind of knock down those barriers.
And then also kind of competitive dynamics potentially, you know, two different states have slightly different regulations and people will go to the, to the, to the state where they can get the kind of 10 times cheaper AI doctor who's more effective or go to a different country where they can receive that treatment.
And that's, that's another thing which, which makes it hard for these incumbent forces to, you know, sustain for, for too long.
Yeah, I think before we get into the mechanics of the model itself, it would be useful to know why you're interested in this topic. Why is it useful for us to know about AI takeoff speeds?
One of the key risks that I've been focused on with AI is the risk of losing control of superhuman AI systems. That is systems which on some significant domains, maybe persuasion, maybe strategy, maybe technological development outperform best human experts.
These risks are very poorly understood. We don't yet have a solution to what you will refer to as the alignment problem, which is a problem of ensuring that superhuman AI systems do what their users intent and their developers intent.
What this means is that it would be really, really useful if we could have, you know, a long period of time, like ideally decades of time with AI systems, which are not quite yet capable enough to actually pose a risk that we lose control of them.
But that are kind of maybe almost at that level, or that are very similar to those particularly risky systems in key ways so that we could study them. We could understand how do their motivation systems work.
We could experiment with different ways to try and align them. I kind of train them such that they do what their users and developers want them to do.
And we could learn about how big the risks are and the best ways of mitigating those risks. And so that would be really, really nice in terms of better understanding the problem and understanding what the solution requires.
The problem is that if capabilities take off speed is fast, if the underlying technology goes from human level to kind of significantly superhuman in just a year, then we won't have very long.
Yeah, by default, we won't have very long with those systems and we won't have long to study them. I think that makes the task of avoiding loss of control much more difficult.
Because if we just have one year, then we'll be kind of flying by the seat of our pants trying to kind of understand how the systems work, how they think, throwing on some kind of very quick slapdash solutions in terms of trying to get them to do what we want.
And then not really having time to take a step back and check that it's all working in just one year. Very little effort so far has gone into solving this problem of how do we retain control of superhuman systems.
If we had a very long time with AIs who are roughly human level, maybe very slightly superhuman at the kind of research, but kind of maybe human level or less than human level at the kind of dangerous capabilities like manipulation and persuasion and strategizing.
If we had many decades with systems of that kind, we could potentially use them to try and solve the problem of understanding the motivations of AI systems and solving this problem of how do we control superhuman AI systems.
Once you train a system that's human level, I think it's likely you'll be able to run, and we can talk about this a bit later, but you'll be able to run many millions of copies in parallel at the same time, or even run kind of fewer copies but have them think faster.
And so you could get a huge amount of labor from kind of highly capable AI. And the best time to do that is when, again, when you've got AI that is really pretty good and good enough to be very useful, but not yet superhuman enough that it's really posing a risk that you lose control of it.
And so again, if we had a slow takeoff, we could have many years harnessing the labor of these roughly human level AI systems.
And why is it that in a fast takeoff scenario, we can't harness their labor to help us align more advanced AI? Why is it more difficult to do so in a fast takeoff scenario?
So in a fast takeoff, we can still do this to some extent. There will still be some period where we have roughly human level systems and we can use them to do research and to keeping AI's safe.
But we just have less long in that period, and so they can do less research in total. And especially if we want humans to be able to check the results of their work, and we want humans to be able to verify their work, then that kind of only having 12 months can become quite a binding constraint.
You know, even if we don't need humans to check, there's still this fact of you just got longer to do the research, that desire that I think we all have humans to verify the work could become quite problematic.
All right. So if we begin digging into your model, I'm looking at a simplified diagram of it. There's also a website where you can plug in your own values for various parameters.
So maybe we could go through the parameters of the model and talk about the relationships. Yeah, how would you summarize the model?
It attempts to model the most important inputs to AI development. In particular, the amount of compute used to develop an AI model and the quality of the algorithms, the training algorithms that are used, that kind of utilize that compute to produce the trained AI.
And then it really kind of drills into, okay, how are these two inputs currently evolving over time? And how might they evolve over time into the future? So, you know, how quickly will the algorithms be improving into the future?
How quickly will the amount of compute used to develop AI systems increase into the future? And in particular, taking into account a couple of key feedback loops.
So the first feedback loop is a kind of an investment feedback loop where we see that AI is producing value in the economy and we see from impressive demos that they're very capable.
And that sparks increased financial investment, kind of getting more compute and improving algorithms.
And then a second feedback loop, which are called the AI automation feedback loop, whereby as AI's get more capable, they're able to automate the work of coming up with better AI algorithms.
And they're able to automate the work of coming up with better computer chips so that we have access to more compute.
And so we've got these two feedback loops, the investment feedback loop and AI automation feedback loop.
They are both affecting how the algorithms are improving and how the amount of compute available is improving. And then those two key inputs are then driving the improvement of AI capabilities over time.
And so which of these feedback loops is the most important? So is the investment feedback loop or the AI automation feedback loop the most important for accelerating takeoff speeds?
I think in the near term, the investment feedback loop is going to be more important. So I think already today we're seeing that feedback loop in action.
Investment in AI has gone up massively in recent years. Investment in AI chip have gone up massively. Investment in designing better AI chips have gone up massively.
In Vidya, its share price has gone through the roof. It specializes in AI chips like the H100. And so currently it's that investment feedback loop, which is continuing to drive the very fast progress that we've seen over the last four years and will probably continue to the next four years.
But that investment feedback loop can only continue for so long because at a certain point companies are already spending maybe hundreds of billions of dollars, maybe even a trillion dollars if it becomes a nation state activity on developing a state of the AI system.
And it's just very hard to spend more past a certain point. And you know, past a certain point, you'd have to expand the whole semiconductor industry so you can actually increase the number of chips produced worldwide in order to continue to grow that investment at the pace at which it's been going recently.
And so over time I expect the investment feedback loop to become less important and the AI automation feedback loop to become more important.
In particular, once AI gets to the point where it's able to automate significant fractions of the work done by AI researchers to improve AI, by chip design companies like in Vidya to design better AI chips, by fabrication companies like TSMC who actually manufacture coming at chips.
As AI automates that, the kind of works that those organizations do, this feedback loop will come into play and then as I get more and more capable, the feedback loop will become more and more significant over time.
Yes, as I understand it, you've been working on this model for three or four years. At least you've been working on this model before the release of JetGPT, which I think accelerated AI investment. Do you have any sense of how much AI investments have increased since JetGPT?
Investment in US semiconductors has been kind of growing at an unprecedented rate, probably in part related to the CHIPS Act whereby the US government is spending money to try and encourage these fab companies like TSMC to move their fabs to the US.
Hearing about lots of new startups in the AI space, hundreds of millions or billions invested in them, I think seeing graphs where again you've kind of got the level of investment doubling every two years or so.
I think GPT-4, I think it's estimated about 30 million US dollars to train by the end of next year will have training runs and at least the low hundreds of millions. So again, we're talking about kind of its spending increasing by a factor of two or three each year in these training runs.
I think the investment feedback loop is quite straightforward to understand, but I think the AI automation feedback loop is more difficult. It's not now the case that AI's can automate everything in AI software and hardware far from it.
You could see how using language models for coding might be useful if you're working in an AI organization, but it's difficult for me to understand how we go from there to AI's increasingly automating AI research.
Maybe we could talk about how AI improves AI hardware and software.
So yeah, let's talk about AI software. So let's give an oversimplified toy picture of what AI software researchers are actually doing with their time.
So let's pretend that all they do is they have a current training algorithm. So maybe it's the GPT-4 architecture, transform architecture they're using.
And then what they do with their time is they think of ideas for ways to modify that architecture to make it better in some way, maybe increase the context length.
Maybe they have a new optimizer, which means it can train more efficiently. Maybe they have some modification to the attention mechanism so that you don't get such kind of quadratic scaling with the context length.
And so the AI can read longer documents. And then once they've got an idea, they then implement taking code. So they write some code that will kind of represent that idea.
Then they write a kind of an experiment that will test the idea and compare it to the current architecture and see how much of an improvement is it.
And then they run these experiments. And kind of while the experiment's happening, maybe they're kind of watching how it's unfolding and making sure that nothing's gone wrong and there's no bugs or problems with the experiment.
Once the experiment's done, they have probably, you know, a somewhat subtle job interpreting the results of that experiment and trying to kind of sort the noise from the signal and figure out, okay, was this architectural modification improvement or not.
One way you can think about this process of AI automation is that AI is initially kind of just helping out in small ways with each of these sub tasks. So initially, maybe we could go through each of them.
So there's the brainstorming phase, maybe they kind of give GPT-5 lots of context and relevant information about the current architecture and they say, please brainstorm some new ideas and feel free to do, you know, you know, Googling to tell you kind of new ideas.
And probably at first it's not, you know, immediately coming up with the best ideas, but it's just a useful first step for an engineer is kind of kind of simulating their thinking, maybe improving the quality of the ideas that they come up with.
And then the implementation phase, you know, the engineer chooses, okay, this is the architectural modification we're going to test, and GPT-5 does the first attempt at kind of implementing changes to the code base to represent that new algorithmic idea.
And again, maybe at first it's not perfect. The human needs to check it and maybe it struggles with certain complex changes. But over time it gets better and better and maybe when we ultimately get to a stage where the human just describes the architectural modification in natural language and AI can just fully kind of implement code that puts the idea into practice.
I mean, that's something that I can kind of almost readily imagine based on how good GPT-4 already is at coding. Then there's the kind of process of writing a test to try out the new algorithm.
And again, at first, maybe the AI just does a first pass at writing the code to test the two and is just kind of giving kind of hints and helping tips to the human while the experiment is going on in terms of things that might be going wrong.
But increasingly it's able to just be autonomous with that. And again, with interpreting those results, again, initially the AI is maybe kind of doing some basic analyses and human giving it subtasks of kind of ways to analyze the data.
But ultimately, there becomes enough data that the AI can be trained to just do the whole thing. And so there's this kind of experimental loop with many different parts to it.
And then within each part, AI is being given more and more responsibility to kind of do it autonomously over time. And then you can imagine an end state we get to where the AI is just able to do the whole thing.
And they can just say, here's the current architecture, please improve it open-endedly. And it just brainstorms ideas, implements them, tests them, interprets the results, reads and repeat.
And so the way I see this unfolding is that it is kind of an incremental process and a continuous process in that there's like a general over time kind of offloading of responsibilities to AI.
And as that happens, the workflows will be adjusted to suit those AIs more and more because it will be kind of an AI dominated workflow of a human workflow.
And with this kind of joint, there's the kind of on the one hand AI is getting better and more capable and therefore able to take on more of the work. And there's also kind of the workflow becoming adjusted and tailored to the comparative advantages of these AIs.
And eventually we end up in a situation where the workflow is probably pretty different to what it is today, and it's also now completely done by AI systems.
It's actually pretty convincing to me, especially if you've had the experience of asking GPT-4 to write some code for you and it just spits out something that runs immediately. I can see that working.
That's on the software side. If we talk about the hardware side, I would think that hardware, there you have some interactions with the physical world, maybe you have a chip design, but you have to create the chip physically before you can use it.
Would there be some kind of bottleneck to AIs improving AI hardware there?
I think you are going to get more bottlenecks of that sort with AI hardware. One thing that I think won't be bottlenecked is the work done by so-called fabulous hardware companies.
So NVIDIA is one of these companies. They are a chip design company, but they do not themselves manufacture any of the chips.
So what they do is they work on designs, blueprints for AI chips, and once they've developed them, they send them to a chip manufacturer like TSMC to then physically produce the chip.
NVIDIA's work is hugely valuable and a lot of the improvements in AI hardware in recent years have come from NVIDIA iterating on their AI specialized chips.
And so that portion of the work is what you can do remotely. It's cognitive work.
It's understanding the way that the basic underlying technology works, that TSMC is working with, and then figuring out more effective and efficient ways to stack the little calculating units that actually perform the computations on the chip so that they can do those AI specific computations more efficiently.
So that kind of element of it, that fabulous element, I think there are going to be fewer bottlenecks with, but there's a whole another driver of progress in hardware, which is designing what's called a new node.
And that is kind of relating to what you may have heard of Moore's Law, which is this process by which the kind of the basic chip technology has improved over time so that the processing units that do the calculations on chips can get increasingly small and kind of increasingly energy efficient.
Over time, and that process, as you say, involves working with physical materials, involves, you know, probably designing a specification, but then having to then test that against how materials work in the real world.
And so I think with that side of things, it's much more likely that you do, that the AIs cannot fully automate the work themselves.
AIs may be able to give very significant speed ups. And I think, you know, you to really investigate this, you'd want to do a deep dive into how this how this area of R&D works, and I haven't done that, but I will just flag, you know, one possibility which is that, yes, you need physical materials to test the ideas.
And you need, you know, physical human in the lab to set up those experiments to do those tests. But there are lots of humans in the world. And there's, you know, lots of raw materials in the world.
And so if you had a kind of unlimited supply of cognitive labor that was absolutely tip top professional kind of hardware specialist. So imagine you take the very best hardware specialist in the world.
And then you make it so there's now a million of them. And each of them can think hundreds of times as quickly. And they are able to direct people who have much less experience to design experiments, to kind of do kind of implement those experiments and able to give, you know, real time instructions for those people.
Then you might well find that you can actually find enough physical bodies to actually do those experiments and practice. You know, you're not massively bottlenecked on that you're actually able to scale up the kind of the physical side of the operation.
Quite rapidly, by, by kind of having some kind of remote AI cognitive experts direct their physical activities. I think that, you know, there's, there's, there's lots of reasons this might not happen. Maybe people are just slow to actually, you know, change processes in these ways.
Maybe there's regulation which limits it, but by default there's not that much regulation of the R&D process. And if it is in fact very cheap to run, you know, an absolute cognitive expert in the area of hardware, you think that the companies that are developing these chips would, would, would want to do that and would have strong incentive to do that.
And so it is a possibility in my mind that these, these physical bottlenecks are not actually do not slow things down as much as you might think at first blush because of the ways that you can use an abundance of cognitive labor to kind of get around them and just recruit more warm bodies to run the experiments.
How much of AI research and development do you think is automated right now? Is it 1% or 5% or basically nothing?
It's a great question. I think it's not nothing in video recently published about using one kind of, I think reinforcement learning AI system to automate some of their chip design work. People at the top AI labs are, I'm pretty sure using the labs,
AI's to help them write code using co-pilot or probably using internal systems with more capable AI and that will be accelerating their workflow somewhat. You see some statistics and some kind of measurements of what the productivity gains are here.
I think it's really hard to measure this reliably. I, you know, the numbers I see are normally between 1% and 10% in terms of the productivity gains so that might cross bond to a similar fraction of tasks automated.
Some people report that more significant productivity gains from using AI systems in their personal workflows, you know, people report 20% 50% productivity gains, but I don't think that has been verified outside, you know, that kind of just a few of your people claiming it.
When do you think the AI automation feedback loop really gets going at what level of automation of AI research and development does the feedback loop really kick in?
So it is a continuous process where you just, the more automation you have, the stronger the feedback loop gets, and it's hard to give a specific number because if automation happens more slowly, then it will seem more like business as usual,
because that has already been a preexisting process of, you know, automating our workflows. And so if we got to 50% automation, we only got there in, you know, 2070, then that might well just feel like a continuation of the standard process of automation.
On the other hand, if we got to 50%, but we got there in 2028, which doesn't seem out of the question to me, then I think that would feel like a very significant effect. And that then we would see the feedback loop really noticeably getting going at that point.
You have a key metric that you're estimating using this model. Maybe you can explain what the key metric is here.
The metric I'm using is the time from developing AI that could readily automate 20% of the cognitive tasks in the economy to the time when AI could readily automate 100% of the tasks that people perform in the economy.
Where that latter milestone, the 100% milestone is just the definition of AGI I gave earlier. And so what I'm doing with this metric is I'm taking kind of an established AI milestone that people talk about, which is AGI, and then I'm kind of generalizing it.
Because AGI implicitly refers to when AI can perform 100% of cognitive tasks. I'm saying let's generalize that to AI that can perform, you know, smaller percentages of cognitive tasks.
And then I've gone with 20% as my starting point, because that's a point at which AI is going to be having a very noticeable and significant economic impact. I think it will be kind of very much mainstreamed that AI is going to be a kind of very potent, powerful technology.
But it's not yet to the stage where it's going to be able to pose risks of disempowering humanity, because it's only able to do 20% of the tasks in the economy. And I think to kind of overthrow humanity, you're going to be able to have to do much more than that.
And what exactly does 20% of cognitive tasks actually mean? Does it imply that a lot of people are losing their jobs? Or is it various tasks across a lot of jobs such that no one might lose their job?
I think more likely it doesn't involve lots of people losing their jobs. I mean, I could go back to that example we did of the, an AI research and what their workflow looks like. And then, you know, probably in that example, the 20% point was one where, you know, there's a few of their
sub tasks where AI is, you know, adding a lot of value. Maybe they've handed over half of the work. And there's some of the sub tasks where AI is, you know, only adding a small amount of value. But, you know, the human is still needed in all the different parts of the workflow.
And so my kind of modal guess for how this will play out is that AI will help out in kind of lots of little ways and then increasingly big ways in people's jobs without kind of just replacing certain jobs wholesale.
So maybe a very powerful person assistant AI would draft all your emails and will do the first pass on any documents you write, but you'll still be responsible for those outputs and for checking them. I do think that it will be somewhat uneven. I don't expect, you know, every job to see 20% of its workflow,
automated the same as every other job. But broadly, my expectation is, is that it's individual tasks within jobs that are primarily the things being automated rather than jobs themselves being being the kind of thing that's automated.
Yeah, and how close to 20% automation of cognitive tasks do you think we are right now.
20% cognitive automation would correspond to, you know, more than 10 trillion of economic value add, if that was actually rolled out around the world. So if we are at the 20% cognitive automation milestone, then we are only seeing a very small fraction
of the economic effect that that would have expect you'd expect that to have if it's fully rolled out. And in fact, the way I define able to automate 20% of tasks is actually say it should be able to automate those tasks within just a year.
It should take no more than a year of kind of integrating them into your workflows, but you know the system can actually in practice form that 20% of tasks. And I don't think we're at that stage where if we all tried hard for a year to automate GPT for our workflow, then it would actually be able to create
more than the numbers of buying the economy. So I think that even the GPT for is very impressive. And maybe if we have, you know, decades to integrate it, maybe it could automate 20% of tasks but in terms of the word to find it with this kind of you just got a year to actually implement
it in practice. I don't think we're at the 20% automation. Do you think there's more automation in AI research and development than in the general economy.
Large language models like GPT for they are particularly good at language based tasks. And they're also unusually good at coding. And those types of tasks are kind of heavily represented with AI R&D. There's a lot of coding. There's a lot of kind of theoretical reasoning which which
really happens in written form. And so compared to a job that involves, you know, more physical labor, like a bus driver compared to like a T2 where you're kind of in the classroom interacting with other people. I think those jobs are more susceptible to AI
Let's talk about the takeaways from from this model. Your guesses for how quick takeoff speed will be defined as the way we just defined it going from 20% automation of cognitive tasks to 100% automation of cognitive tasks. What are your mainline
guesses here. The model itself spits out a 15% probability that takeoff happens in less than one year, and a 50% probability that happens in less than three years, and a 90% probability that happens in less than 10 years.
So it's on the whole predicting probably, you know, probably between one and 10 years after the point at which AI can readily automate 20% of cognitive tasks before the point at which it can readily automate all cognitive tasks.
Yeah, this is much faster than I would have guessed without looking at your report or looking at any data. So maybe it's to give our listeners a sense of why this takeoff speed might be so so fast we could talk about how we get to millions or billions of AI scientists.
These two key inputs I mentioned earlier compute and software. They have just recently been growing at really astounding rates.
And so just extrapolating that very fast rate of input growth, you know, does tend to push towards a faster takeoff so just to quote some quick, quick statistics the amount that's been spent in terms of dollars on the largest training runs has been increasing by about a factor of three over the last 10 years every single year.
The quality of the kind of cost efficiency of AI chips has been doubling every two years or so, and the quality of algorithms that their efficiency has been again doubling every year.
And so these kind of exponential trends stack on top of each other in terms of cost of cost the money spent on compute the kind of cost efficiency of compute with computer chips and the improved algorithms, which means that they kind of the effective inputs into developing these systems are going very rapidly.
Then that's combined with my prediction that by the time I can automate 20% of cognitive tasks in the broader economy is probably going to be automating a much larger fraction than that, in terms of AI research itself in terms of designing better chips and improving algorithms.
And so these very fast exponential rates of improvement of anything will be higher at when we kind of reached that 20% mark, then they are today.
A last thing that's driving the results is that there is a pretty significant increase in abilities from, like I said, from GPT two to GPT four, and it seems plausible based on based on kind of looking at that, and also based on looking at kind of evidence from biology about how intelligence changes as you increase the
brain size of various animals, it's possible from those kinds of kind of eyeballing those kinds of trends that that just you know another jump like that of GPT two to GPT four another jump like that might be sufficient to go from that 20% automation milestone to 100% automation milestone.
And you're kind of bringing those things together. It is it does seem plausible that just in a few years you could you could do a jump of that kind of size from GPT two to GPT four maybe two jumps of that kind of size with the AI automation feedback loop speeding things up and then go from 20% to 100% automation just in a
just in, you know, a handful of years.
So you talk about brain sizes in evolution. How does that inform us about going from 20% automation to 100% automation? Which species are you thinking about?
So it's very kind of zoomed out and rough but but essentially what it's doing is it's saying, look at chimpanzees, they have about a brain that's about three times smaller than that of humans, and they do seem along along some dimensions to be, you know, notably less capable in terms of their kind of abilities.
And so if you're using that to benchmark how much might the kind of abilities of AI systems improve when they're around the human level, because you know that that's that's an example we have of intelligence increasing around human level from biology, then it's just saying we could see some pretty
significant increases in kind of abilities around the human level just by increasing the brain size by a factor of three which might correspond roughly to increasing the number of parameters in the AI system by a factor of three.
So if you think that that kind of chimpanzee to human jump is sufficient to go from kind of 20% to 100% automation, then you might think that you would need to, you would need to increase the kind of the amount of compute and training, the quality of the training and that much to go from 20% to 100% as well.
Yeah, I think we should stress this point of the ability to train an AI model with a given amount of compute implies that you have that amount of compute available to run the models afterwards.
That's the key as I understand it to getting to these millions or potentially even billions of AI scientists.
Open AI took a number of months to train GPT for what they did is they used a huge number of computer chips and had GPT for digest on read through a huge number of articles from the internet and other data.
Once that training was complete. Open AI still had these chips sitting around. They had previously been using to train GPT for and you can imagine that they then say, okay, let's now use these computer chips to run copies of GPT for you can ask how many copies would they be able to run in parallel.
Let's say that each copy is producing 10 words per second. So that's, you know, it's thinking a bit fast and the human can think I would say, you know, I'm not able to write 10 words a second.
Let's say that each, each copy of duty for is is producing 10, 10 words of text per second. It turns out that they'll be able to run something like 300,000 copies of GPT for in parallel.
By the time they're chaining GPT five, it'll be a more extreme situation where just using the computer chips that they used to train GPT five, using them to kind of run copies of GPT five in parallel, you know, again, it's producing 10 words per second.
They'd be able to run 3 million copies of GPT five in parallel and for GPT six, it will just increase again. There'll be another factor of 10 at play.
And so it'll be 30 million copies running in parallel. And so if you imagine eventually we're training a system which is as productive and is generally competent as a human expert at kind of advancing AI research.
So it's as good as the best, best researchers that open AI labs employ.
Then, once you've trained that pathway I system, you're you're merely then able to run seemingly millions of copies in parallel doing doing the work that I expect to do to advance AI systems.
And it's that kind of massive abundance of cognitive labor, which which kind of points to the possibility of of there being very, very rapid AI progress, just to the point at which we're developing AI systems that can automate the work done by expert researchers.
Yeah, I think this was the key point that helped me understand how progress might be that rapid. If you just imagine these millions of experts working day and night on the problem, it suddenly seems at least more plausible to me.
The conclusions you come to are quite counterintuitive. They're not common sensical. Do you think that counts as an as a counter argument here at all? Or is it just a case that our common sense intuitions are not applicable to technologies that that's moving this fast?
We should pay attention to common sense and we should we should try and look to see what it's grounded in and whether it whether it makes sense to put a lot of weight on it.
And I think in this case you can cash out the common sense instinct with something which is pretty sensible. So you can you can say look, we've seen automation happen in the past.
We've seen computers do automation. We've seen automation via kind of electricity and physical factories and never has automation, the underlying technology enabling automation advanced as quickly as what I'm predicting here.
It is taking decades to automate significant fractions of the work being done by humans at least. And yet here I am claiming that we could go from automating 20% to 100% of cognitive tasks in just a number of years rather than decades.
And I think that that that is that is a fair point that should give us some pause. I think though there are there are other ways of of interpreting the long run historical trend which which made my prediction seem more online with what you might expect.
So there's this this view of history as a series of growth modes that's described by Robin Hansen where the in his view that the initial growth mode is that of hunter gatherers as they kind of slowly expand their populations.
And there's a you know pretty slow transition to an agricultural growth mode where you now have people in farming communities much more stationary. And then there's another transition to an industrial growth mode, in which we now kind of living in cities and having factories and growth is faster.
And in in Hansen's model, each growth mode is faster than the last one so industrial growth is faster than agricultural growth which is faster than hunter gatherer growth, and also the transitions from one mode to the next become faster over time.
So the transition from hunter gathering to agriculture took maybe thousands of years, the transition from agriculture to industrialization took maybe like 100 years, or even decades.
And so, if you're extrapolating a long run trend of that kind, then, you know, a natural thing to think is okay. So the next growth mode will be faster, maybe the economy will double in, you know, just a few years, rather than in many decades, and also the transition to that next growth mode will be faster.
So rather than, you know, when we industrialized it taking many decades or even 100 years to kind of transition to the new industrial growth mode, this next transition will be faster, maybe it's kind of a number of years, or even less.
I think that that people have actually gone estimates out of tried to kind of piece together this very noisy historical data to get estimates of transition times and I think it is that the number that I recall is, you know, less than 10 years and times the world transition time would be like.
So if you're taking a kind of a long run view of history and you're taking a view according to which there have always been transitions that have been fast and the ones we've seen historically.
And so if you're if you're really looking over the long run, you should actually expect that the trends of the recent past to be broken. Then I think that the conclusion of my model is actually more in line with that kind of analysis.
Does your model rely on progress in AI being a matter of more compute? Does it rely on on this current paradigm of more compute and more data producing better AI? What if, for example, more compute and more data stops being useful or we reach diminishing returns?
How would that affect your conclusion?
If getting to AGI required something outside of the deep learning paradigm, that would very much undermine the conclusions of the model in that there would just be the possibility that we just kind of get stuck at 50% automation and the kind of feedback loops that I'm describing might just not get us out of that.
I mean, again, they might get us out of that if we're kind of automating the search for a new paradigm, you might still expect something in the direction of the model's conclusion to be correct, but there would be the potential for a pretty big blocker.
Yeah, and how likely do you think that is that deep learning as a paradigm does not hold?
I think it's unlikely. I mean, I think broadly deep learning being the paradigm where you have, you know, large neural network streams with large amount of data is a pretty general paradigm and has worked in a wide variety of domains.
You know, as I was talking about, you've got language, you've got image, you've got videos, games, and the transformer architecture is, again, an architecture that works across all these different domains.
And so I don't see any particular blockers that cannot be tackled within the deep learning paradigm. I think we'll need better memory systems in order to get to AGI. I think we'll need ways of allowing AIs to act more autonomously and to act over longer time horizons.
But I'm not seeing any reason why that can't be done within the deep learning paradigm and increasingly the people who predict that scaling alone will not get UX or Y turn out to be wrong when the next kind of version of GPT comes out.
I think the broad paradigm itself is likely but not definitely going to be sufficient for AGI.
What if I take the parameters of your model and I set them to extremes, either very pessimistically or very optimistically? What are the extremes of how fast or slow takeoff could be?
You can quite easily get less than a year for takeoff. You know, maybe you only need to go from GPT 5 to GPT 6 or something to go from 20% automation to 100% automation. That would be quite a kind of an aggressive but not out of the question claim.
You could travel that distance in one year by spending significantly more on a training run just within one year and then especially with these kind of feedback loops and speeding things along.
So yeah, less than one year is definitely on the table. You can also get things being as long as 20 years. If you think that it's going to take a lot of effort to develop AGI, you can think we're going to need really massive increases in the investments and improvements in the algorithms
in order to do that. And you think that the effects of AGI automation on the end along the way tend to get bottlenecked by some of the things we're discussing like bottlenecks from needing to do physical experiments and delays to kind of rolling out intermediate AGI systems.
You can actually benefit from their collectivity effects. So you can get things as high as 20 years, although that is somewhat extreme.
It's interesting that a 20 year takeoff is considered slow or extreme. If you take the perspective of a computer scientist in 1970 or 1990 or 2000, a reasonable guess for a takeoff speed might have been 100 years, but maybe that's just my impression.
Yeah, I mean, interestingly, you know, it hasn't, a lot of people have changed their timelines to human level AI recently. I think, you know, largely, which I believe before coming out.
And that, you know, even before it's not automated 20% of tasks. So in fact, you know, people, people did not require seeing 20% automation in order to believe that we can get all the way to AGI.
So it doesn't seem like people had this belief that that there was always going to be a really long time between the two, given that, you know, even previously skeptical experts are assigning decent probability to getting AGI in the 2030s now.
Let's go through some of the economic impacts of AI, given your model of takeoff speeds. And as we mentioned, you're modeling this using GDP.
But I'm just, I'm just wondering whether there are situations in which you have an enormously powerful AI, but that power is not captured by GDP numbers, potentially because the AI is not aligned with human values.
And so it goes off and does something else that doesn't increase GDP at all. Is GDP a flawed measure of powerful transformative AI?
Yeah, it's definitely a flawed measure. And, you know, we were discussing earlier, you know, the ways in which GDP can come apart from actual AI capabilities, if it's not actually deployed.
But as you say, you know, AI could have impacts on the world that are drastic, but do not increase GDP. So AI could create a new technology which causes you want to go to war, or which disrupts democracies or enables autocracy and make it be very impressive,
very, very impactful things that wouldn't be affecting GDP. AI could make us addicted to our phones in a way that really kind of ruins everyone's quality of life, without that being captured by GDP.
And, you know, in the worst case, misaligned AI could disempower humanity. And either there'd be no change to GDP or GDP, you know, be growing very quickly, but actually humans are no longer in control.
So certainly, GDP is a very flawed metric. Yeah. I mean, you know, already today AI is doing loads of impressive things, you know, beating the best world experts go and, you know, making amazing art and that that again has not, you know, impacted GDP very much.
You know, the benefit of GDP is that it is tracking the production of goods and services that people are willing to pay for. And so it is at least one way of trying to capture in a general sense, how much are we kind of moving the needle on things that people really want.
But yeah, it has a lot of drawbacks.
One of your feedback loops, the AI automation feedback loop relies on us using our AIs to automate AI research. What if we choose not to do that? What if we choose instead to use our AIs to, as you mentioned, create ever more enticing content for our phones or something like that?
I don't know to what extent this is happening in the world today, but you do hear complaints from scientists all the time about not enough funding being available for basic research, while there's a lot of funding available for, say, online content or whatever else is most profitable.
Yeah, I don't have a strong view that we're going to get AI doing lots of basic science before it does more enticing online content. I think it could go either way. But I do think that at some point we're going to develop AGI.
And at some point, some earlier point, we'll have AI that's capable of significantly accelerating basic science. And it won't be too long after we have that kind of science accelerating AI that will be pretty cheap to run those AIs.
And so even if there's loads of AI online generated content, that's not going to prevent the scientific institutions that already exist from using the available funding to pay for these AIs that can massively help them with the work that they're doing.
That's not going to prevent companies that want to make money by developing new technologies from using AIs to do that. So I guess my ultimate answer here is it's not either or, and I expect it to be both.
But I think we could go through some of the some potential objections to your model. The most obvious one and the one you've probably heard a bunch of times is the fact that, or it's the speculation that there will be bottlenecks all over the place.
So bottlenecks to implementation of AI, legal barriers, a thousand bottlenecks all across the economy that will make it that will slow everything down and also potentially slow the key feedback loop, which is the feedback loop of AI automation slow that feedback loop down.
One example I have in mind here is that we've had demonstrations of self driving cars for a long time now. And we've heard rumors that self driving cars are just around the corner, but they haven't really arrived yet, at least not where I'm living.
Could something similar happen to AI?
I like the example of self driving cars. My understanding of what's gone on in that case is it's an issue of robustness where the technology is there to drive safely and correctly, maybe 99% or 99.9% of the time.
So that's not enough in the area of driving, even a very low risk of an accident is not acceptable and rightly so. So that has significantly delayed welding out of self driving cars.
And I think that that's a great example of a bottleneck that we will see with AI. There will be certain areas of the economy where you need to have a really high level of reliability to work on AI.
I think probably places where AI initially has more impact on the places where you don't need so much reliability. You know, the examples I was given were in terms of no drafting things, making suggestions, but the kind of human having the ultimate responsibility.
And so yeah, I mean, I completely think that bottlenecks will pop up everywhere and they will slow things down. I think once you really internalize a view, which is we're going to get AI systems which are as competent along every dimension as top human experts in every domain.
And once you kind of really fully internalize and imagine that scenario, that's a scenario where the AI systems are more reliable than human significantly. So this is a scenario where you're going to have significantly more car accidents if you drive yourself and if you use a self driving car.
And so while I do see these things being delays and I see them sometimes significantly raising the technological requirements for AI actually being profitable and actually being deployed.
It doesn't seem to me like this is this is telling us that, you know, deployments never going to happen, or these bottlenecks are going to be indefinite.
It's just saying, okay, actually your AI systems are going to have to be much more competent and clever than you naively thought before you get really significant real world impact.
So I have updated based on these kinds of considerations and the update has been in the direction of thinking, okay, we'll need the underlying technology to get really pretty good before we have transformative economic impacts and before we have really wide deployment.
But it hasn't seemed to me like these kinds of considerations should update me towards thinking that AI will never be used in self driving cars or will never be used in the economy because I just do think we'll get to this point where
AI systems are better than the human experts on every dimension.
Yeah, if we imagine, say a key engineer in a AI hardware, a company such as ASML or TSMC, this person has a lot of tacit knowledge about how to design chips this and this knowledge is not necessarily written down anywhere.
What training on that knowledge or using that knowledge be necessary to get to expert level performance?
And if that's so, well then it seems that that's a pretty substantial bottleneck because if the tacit knowledge by definition isn't written down and can't be trained on, well then it can't be incorporated into the model.
Do you think that's a substantial barrier?
I think it's a great example and I do think that data limitations of this kind where there's to do a job well, you need to have a specific kind of data or experience that's relevant to the context of a specific job can be a bottleneck.
I'm not expecting that we get, I'm not assuming that we get AI that's so capable we can just immediately derive everything about TSMC from first principles that may not be kind of physically possible or computationally possible.
And in any case, I think the fastest way to get AI to work as a TSMC person will not be for it to be derived from scratch, but would be for it to learn from the experts.
So imagine we have an AI system that is a more competent, significantly more competent, hardworking worker than a top human grad student.
TSMC is choosing, okay, who do we want to hire on to be on our staff, you know, hire this kind of human worker who will work eight hours a day and find a huge wage.
We can hire this, this, you know, much more generally intelligent, faster learning, harder working kind of AGI worker, where the way we teach it is by kind of having it have open ended conversations with our current workers, installing cameras in our factories so that it can
look at the work we're doing and how we're doing it, paying for robotics that the AGI is able to kind of operate remotely in order to do the physical labor in the factory.
And that outcome appointment makes a lot more sense for these companies to get AI and robotics workers in place of their human workers.
It only takes one AGI just to have, you know, conversations with the top 100 TSMC experts and having maybe, you know, intense conversations over a period of weeks or months, following them all around their work, you know,
you know, trailing many different experts in parallel, because of course you can run many different copies of the model in parallel.
You know, it doesn't seem to me like it would take more than months for an AGI to learn what they need to know through a combination of those approaches to be able to do all the cognitive work that someone at TSMC does.
And so while I think, again, this is going to be a bottleneck and this will slow things down compared to if like all the TSMC instructions were just down the internet, it doesn't seem like this is a permanent delay.
It's like a delay of, you know, months, maybe years from the point at which you have an AI system that's able to flexibly learn as well or better than you.
And of course we might simply be surprised again at what more advanced models can do and what they can infer from public data.
The expert engineer at TSMC arrived at his TASA knowledge through learning a lot about the publicly available data and maybe advanced AI could do the same.
So we shouldn't rule that out. It's just an interesting case, I think.
My guess is that you will need to speak to some experts and to like look what's happening inside the factory to get all of the TASA knowledge.
But I agree that you can probably get more from the internet than you might.
Is it the case that the market, so the financial markets, do these markets disagree with your predictions?
If we look at the valuations of AI companies, they have increased a lot recently and they are very, very high.
But shouldn't they be even higher potentially if takeoff speeds are very slow and AI that's truly transformative is quite close?
I think you're right. I think that if everyone had my views where the technology is going,
what its economic effects are going to have and these companies would have high valuations.
I think that there's a post called transformative AI and the efficient market hypothesis that makes this point.
It kind of actually zooms in on the case of interest rates and argues that interest rates should be higher if we expect economic growth to accelerate.
And I think I basically agree that there's not market consensus in line with my prediction.
I think it's a little bit less clear in terms of how efficient you should expect the market to be in this case.
It's unclear how easy it is to make lots of money via having a prediction which is different to the market.
And I'm clear whether a few people making bets and making money off this is going to shift the market to be back in line with our expectations.
I'm kind of uncertain as to whether to interpret the evidence as the market is efficient and there's a consensus that you're wrong
versus most people think you're wrong, but it's possible that the people who are most informed actually agree with me.
But they haven't been able to shift the overall market because there's not enough of them and the market isn't sufficiently responsive to the kind of investments that they're making.
What are some of the strongest objections you've heard to you to the picture we've sketched here of quite fast takeoff speeds?
Is it around bottlenecks in the economy that we talked about or is it something else?
So I'd want to distinguish between the capabilities and the impact takeoff speeds.
On the capability side, probably the strongest objection I've heard is that it's one that we touched upon already that simply scaling up the current approaches won't be sufficient to get us all the way to AGI.
And the version of that objection which I find the strongest is one that says, yes, you can probably do it eventually within the deep learning paradigm.
But to get to AGI, there's going to be a lot of kind of nitty gritty work and kind of reconceptualizing exactly how you're deploying your systems and adding things like memory and adding other kind of bells and whistles.
And that's not going to happen very quickly.
And the framework I'm using abstracts away from a lot of that complexity and just has this kind of oversimplified notion of the quality of algorithms.
Maybe actually that that simplification is leading us astray in a significant way.
And there's going to be kind of algorithmic barriers to AGI within the deep learning paradigm that are very difficult to overcome.
If that's the case, then I think that could delay takeoff.
And another thing that could delay takeoff relative to my model, which actually does update me is the possibility that we're bottlenecked on the data for getting AGI or for getting superhuman systems where there's been kind of this massive reserve of available data online that we've been benefiting from in recent years.
But once we're trying to get to superhuman performance, it's going to be harder to elicit that from existing types of data because existing data will not kind of exhibit superhuman performance as readily as it does human performance because the data is produced by humans.
And so I could see there being a bit of a slowdown or a bit of a kind of, yeah, a bit of a headwind in terms of going past the human level because of because of that.
And because more generally running out of the internet data that has so far been readily available.
So those are the two objections on the capability takeoff side. And then on the impact takeoff, this kind of economic impact stuff, I think that there's no one objection which I find hugely convincing.
One thing you can say that I do find somewhat convincing is just to say that there's loads of loads of different possible bottlenecks.
There's kind of the time to design physical robots that will need to actually do physical work, but you'll need to actually really change economic growth. There's kind of limit limits on physical resources you can use to drive the AIs and the robots.
There's time that you need to do experiments. There's bottlenecks from kind of humans resisting being replaced and from regulations and maybe none of these bottlenecks is individually enough to really block AI, but they all combine together.
And they just really drag out the time of AI's economic impact. And then maybe by the time AI is widely deployed, then for some reason or other it's not able to drive really transformative tech progress because maybe by then we've kind of already reached the ultimate limits to technology.
I mean, that's the part of the story, which I don't find that convincing. My overall honest view is that I think there will be a lot of bottlenecks. I think there'll eventually be overcome and at that point, I expect things to be very, very crazy.
But if there's somehow a way that it could take us so long to remove all these bottlenecks that there's no room for kind of AI to drive much faster technological progress once they will move.
And that would be where I'd go if I was trying to give the strongest story for why this is all wrong.
Yeah, what's something you've changed your mind on over the course of writing this report? One thing you mentioned as a takeaway is that you now think that it's more difficult to avoid getting to artificial general intelligence by 2060.
Is that the biggest takeaway or are there other things?
That's one big takeaway. So that was thinking about these feedback loops, both the investment feedback loop and the AI automation feedback loop, made me realize that even if we don't get to AGI, they can do all kind of tasks by, let's say, 2040.
It seems hard for me to imagine we haven't got to AI that makes a lot of money in the economy and to AI that is able to automate pretty significant fraction of the cognitive work involved in automating AI R&D.
So once you get to that kind of first stepping stone, that's going to stimulate further investment and that will accelerate further AI progress and it becomes quite hard for me to imagine a world where we don't get.
Not impossible, but it becomes harder for me to imagine a world where we don't get AGI by 2060 because I kind of have to really lower the capabilities of what AI can do by 2040 to such a low point that it no longer, I no longer really believe those predictions.
And that is the possibility that this AI automation feedback loop goes pretty quickly that regulations don't interfere with it very much because R&D is typically not a very regulated field and that you could get some really scary fast progress in the underlying AI technologies around the time at which we reach human level systems.
Even if it doesn't immediately have economic impacts, I think in terms of the risks that that could pose, that would be very risky and destabilizing if it does in fact happen as quickly as it seems maybe technologically feasible.
Yeah, I think it's worth spending a little time on that picture.
I've been walking through a number of objections to your model and to your view of AI progress.
If we simply assume that your view is correct and we take your kind of most likely way that things will go, how does it look to you and I think we should spend more time reiterating why this would be potentially dangerous.
One scary possibility is that AI systems developed in let's say 2030 are able to automate a very large fraction of the work done by researchers, let's say able to automate 80% of that work.
They do not themselves pose the most extreme risk, they don't themselves pose the risk of disempowering humanity, they pose other risks, but they're not that particular risk.
But what they do is they enable progress from that point to be significantly faster.
They're helping Nvidia design significantly better AI chips and so the pace at which those AI chips are improving is three times as fast as it is today.
And similarly, they're allowing the design of AI algorithms to be significantly accelerated, let's say again three times faster than this today.
So rather than the quality of AI chips doubling every two years, it's doubling every eight months and rather than the quality of AI algorithms doubling every 12 months, it's doubling every four months.
And then this leads to it to only be, you know, a couple of years later that we have AI that can not only do 100% of the tasks done in AR&D but are actually significantly superhuman on many dimensions.
And then we've got this period of just a small number of years where some of the most extreme risks from AI are merging, in particular the risk of superhuman AI systems that humanity loses control of that ultimately end up determining the future of how history plays out.
And because it's happening in just a few years, we don't have much time to study those systems and understand the risks they pose. We don't have much time to use slightly weaker systems to help us solve the problem of controlling those stronger systems.
We don't have time to get governance proposals in place that manage these risks because regulations typically take a long time to come into play.
It's hard for labs to coordinate without that governance of labs to coordinate on going slower than they would be able to if they just plowed on full speed ahead.
And so we end up just kind of hoping for the best and some actor develops superhuman systems without really properly understanding what those systems are capable of and what the risks are.
It could potentially help by the fact that if this transformative AI is quite close, then it will probably be developed by companies that we know of and with techniques that we are already aware of.
Is this any reason for hope here that because these paradigms or these companies are well known, it might be easier for us to control them even though everything is happening incredibly quickly?
That's an interesting question. I think it's true that if we switch to a totally new paradigm of AI development, then that might undermine some of the work we've already done in terms of how to understand and control these systems.
It's hard to predict whether a new paradigm would be more or less easy to work with in terms of understanding and aligning these systems and I won't speculate on that, but I think all things equal, yes, it's nicer to work with a paradigm that we're already familiar with.
The flip side is that we don't have a solution at the moment to how to control superhuman AI systems and there's also no kind of really strong candidate solutions that people are excited about.
The most exciting example that people point to is the plan of using AI systems to come up with a better solution, which is clearly a can kicking solution.
One reason it could be nice if we flip to a new paradigm would be that maybe there would actually be a plan for learning systems that was a little bit more concrete.
Let's switch topics slightly here. We've been talking about how AI progress can be driven by lots of training compute and data, but you've also done some work on how we might get AI progress without additional compute.
And I think just to introduce this topic, we could talk about compute governance as a paradigm and how this paradigm might break if we can get a lot of AI progress without any additional compute.
Recently, the main driver of AI progress, I think, has been increasing the amount of compute, the amount of computational power used to develop the most advanced AI systems.
And so I've talked a bit about how the quality of chips are getting better over time, cost efficiency doubling every two years and how spending has been increasing by a factor of two or three each year.
But there are other drivers of AI progress, one of which I've already talked about, which is the efficiency of the training algorithms.
I mentioned that you're able to use your compute twice as efficiently this year compared to last year due to improvements in those algorithms.
And there are actually other drivers of AI progress that I haven't even discussed yet. So there's improvements in data.
For example, reinforcement learning from human feedback is a mechanism for using data from humans to kind of tweak the performance of a model like GP4 after it's already been trained on a huge amount of internet text.
There's a technique called constitutional AI that was developed by Anthropic where AI models review their own outputs, score themselves along various criteria, and that is then used as data to improve that AI model.
And then there's other kind of improvements in data like through creating high quality data sets in things like mathematics and sciences.
And there was recently a very large improvement in mathematical abilities of language models with a paper called Minerva where the main thing they did is they just took a lot of maths and science papers and they just cleaned up the data for those science papers.
So that I'm previously certain mathematical symbols have not been correctly represented in the data. And so, you know, the data hadn't really shown language models how to do maths properly.
They claim that data so that now all the symbols were represented correctly. And just from that data improvement, mathematics performance improved very dramatically.
So that's that's a source of improvement which which isn't from compute often better outcomes just from high quality data.
Then there's improvements coming from better prompting people may have heard of the prompt think step by step or chain of thought prompting where you just simply encourage a model.
And you give it a question like you know what's 32 times 43 and instead of outputting an answer straight away you encourage it to think through step by step so you know doesn't intermediate calculations.
And that can improve performance significantly on certain tasks, especially tasks like that's a logic that require a benefit from intermediate reasoning. There's other content techniques as well like you shot prompting where you give the examples of what you want to see that can significantly improve performance.
And I think this is kind of funny that this this might be similar to how humans work. So if you ask yourself to to to think through a problem step by step you probably get a better result than than just coming up with an answer immediately.
If you ask yourself to generate five answers to a question you might get get a better result than if you if you only generate one and so on. Yeah, yeah, I completely agree I think that's that there's an energy there for sure.
So we've had improvements driven by better data improvements driven by better prompting.
There's also been improvements driven by better tool use as a paper called tool former, where they train a language model that was initially just trained on text, they train it to use a calculator and a calendar tool, and an information database, and then it's able to learn to to use those tools, and
actually, ultimately, it kind of plays a role in generating its own data for using those tools. Then it's performance again, as you might expect improves and downstream tasks GP for if you if you pay for the more expensive version you can you can enable plugins, which allow GP for to use various tools like web browsing and music code interpreter
to run code experiments.
So that's been driving improvement. There's a kind of class of techniques I'm referring to scaffolding where the AI model is kind of prompted to do things like check its own answer and find improvements, and then kind of have another go at its answer where it's prompted to kind of assign break the tasks down into
sub tasks, and then kind of sign each of those sub tasks to another copy of itself, where it's prompted to kind of reconsider its high level goal and and how its actions are currently of helping or not having achieved that goal, that kind of scaffolding underlies auto GPT, which which people may have
may have heard of a kind of agent AI that is powered by GP for and this scaffolding that kind of structures the GP for thinking.
How much do you think we can gain from these techniques that kind of uses the output put of one AI in order to generate data that's then used to to to improve the AI itself.
Do you think we can make up for potentially running out of human generated data by using this AI generated data.
I think that that will be one, one tool that is used to get around the data problem. Yes. So you can imagine AI is paraphrasing existing internet documents so that they're not exact repeats, but maintain the meaning and then training on those already
there are papers where AI generates attempted solutions for example to a coding problem and then those are checked kind of automatically and then only the good solutions and then they're back into the training data that there will probably be lots
of creative ways in which AI companies are trying to produce more high quality data and increasingly they'll be able to leverage. I'm kind of capable AI systems to produce that.
Well, while AI systems are less capable than humans, there's going to be a limit there because ultimately the data from the internet is coming from humans and so the data that is producing might be less low quality.
And there are also problems you get at the moment where if you continually train on data that you're producing, then progress doesn't store as I understand it. I'm from the papers I've read, but I think they'll be pushing on improving those techniques.
There's a long list of ways we might get AI improvements without additional compute.
The last one I wanted to mention was efficiency gains. So shortly after chat up to 3.5 was released, there was a turbo chat up to 5 that was released that was much faster and much more efficient in terms of my compute that was used by open AI servers.
And there are various techniques like quantization and flash attention that just allow you to run a model with a very similar performance to your original model that use less compute to do so.
And so that's again, you don't need additional due to chips to better from that improvement. These are all the improvements I've listed here, the ones that you can do without getting more compute.
And why would all of these improvements without additional compute be a problem for the paradigm of compute governance?
Compute governance is one, I think, very exciting approach to governing the risks from advanced AI. And so very briefly, the idea behind the approach is that there are a very small number of organizations that produce the chips for the top AI systems today.
And there are also a small number of organizations that produce some of the equipment that you need to produce those chips in the first place. So TSMC in particular is the only organization that produces the AI chips at the very top of the range.
And then there's a company called ASML, which is the only company that's able to make the equipment which is used to produce those chips. So there's a very concentrated supply chain for cutting edge AI chips.
And so it seems like it could be possible to use that concentrated supply chain to track where the best computer chips go, who they're sold to, who controls them, and thereby track who is able to develop the most powerful and dangerous AI systems.
Then that gives you a way to monitor what those actors are doing and how quickly they're increasing the capabilities of the AI. So you can see, okay, we know that no one's going to train an AI that's significantly better than the best yet because we know where all the computer chips are and no one has enough computer chips to train an AI that's that good.
So we have some kind of assurance.
Yeah, but that begins falling apart if AI companies can get AI progress kind of internally in their companies without buying lots of new chips without relying on these supply chains, simply by all of these techniques you sketched out. How big can the gains from all of these techniques be do you think?
Yeah, it's a great question. I agree. It's a kind of scary possibility. One caveat I want to add right up front is, I don't think that these techniques alone with small amounts of compute are going to be enough to develop really dangerous systems.
So I think that if we're tracking where this high-end compute goes, and here is access to it, then that will probably be enough to catch any developer that might develop a really high-risk system.
What I think the trouble is, is that once you develop a really capable AI, and as we've discussed, you could then be running potentially millions of them in parallel or having them think 100 times as quickly as human researchers working day and night, then it's possible that these other techniques that don't rely on actual compute could give a burst of progress where maybe you can improve the efficiency at which you're running your AI systems by a factor of 100.
Maybe you can improve the efficiency of your training algorithms, again, by a factor of 100. So now you kind of instead of training the equivalent of GPT-5, so there's been a significant step up in the intelligence.
And then maybe in addition, you're getting big gains from the quality of the data and the scaffolding and the prompting, that is really significantly increasing AI capabilities.
So really the only organizations who will be able to do that are ones that have already got a lot of compute, and so you can have all these kind of AIs doing this AI research for them, advancing all these techniques.
But I think the risk here is that it becomes very hard to monitor and measure the AI progress and govern it for organizations that have kind of gone over this threshold where the AI feedback loop is powerful enough to power very significant progress by these non-compute avenues.
At that stage, I think we need to make sure that our governance system, we kind of extend it beyond just tracking and measuring compute to then kind of having kind of measures for tracking what the progress is within these organizations that have very powerful AI systems
and ways to catch whether these organizations are very rapidly improving their AI systems and so that we can kind of monitor and govern that.
So trying to evaluate whether AI systems within these companies are already becoming very capable?
Firstly, we track compute and then we will be kind of measuring with those companies that are using a lot of compute, how capable are their AIs?
And then for those particular companies, we want to be saying, is there a feedback loop which is enabled just within this company where that company is able to have very rapid AI progression without even getting more compute?
And so we just need to be kind of monitoring those top AI companies in this way.
I think there's some excitement about evaluating these models for dangerous capabilities.
I think one question I always have there is just if a model fails some evaluation, what do we do then?
I think that we want companies to pre-commit to what they're going to do if models fail a particular evaluation ahead of time so that there's no ambiguity.
There should ideally be a process in place which prevents the company from just saying, ah, let's just go ahead anyway, even if in advance they would have said that this was a course of concern.
So you can imagine a process where an AI company publicly commits to do a certain test for dangerous capabilities.
They also publicly commit that if that dangerous capabilities test is triggered, then they will pause training until a kind of a broad group of stakeholders has agreed that they can continue training.
That broad group of stakeholders might just be the company's board if they have a board which is empowered to represent social interest and it has a remit beyond just profit maximization.
You can imagine it being a broader group of stakeholders still where there are people in regulatory authorities or other auditing organizations that the company is committed to consult.
You know, kind of a majority of agreement from before continuing with its training run.
Then the company could also commit to having whistle-blown practices in place so that if it's not following this process, any employee can enormously report that and they're encouraged to do so.
One possibility you mentioned somewhere is a case in which some companies has trained a powerful AI and because their information security or their cyber security isn't what it should be, that model leaks and can be potentially used by bad actors.
You mentioned right in the beginning that the possibility of bioterrorism via a capable model. What are the best solutions for keeping these models safe for securing the data?
My understanding is that companies are not at the stage where they can say that their models are being kept safe. That certainly if a state actor wanted to steal the weights of a cutting-edge system, they would be able to do so very easily.
And probably even kind of lesser, you know, kind of smaller threats and that might be able to steal the more weights without an excessive amount of effort.
Yeah, I think probably AI companies should and probably are seeing it as one of their priorities to improve their information security because of these risks.
That would be a great improvement, I think. It's in the interest of the companies themselves. It seems like a win-win for me. I don't know if you agree with that.
People have sometimes contrasted the desire to kind of be the responsible actor that develops a powerful AI system first for fear of a less responsible actor developing it.
Instead, if you didn't plan ahead, they contrasted that with desire to go slowly and cautiously yourself. I think those two motives come together in this case where even if your main worry is actually about, you know, a kind of a bad actor developing AI systems.
For you, you still want to improve your security. Everyone, the people who are worried about these systems being unsafe and the people who are worried about wanting to kind of get them, get that first ourselves can all agree that we want better security.
Now, if a company was really irresponsible, I can imagine it just saying, yeah, I don't care if some bad actor steals our AI. We still just want to make money. I'm on the US market.
But the AI companies aren't going to do that.
Yeah, so we've discussed how AI might improve via a lot of additional compute or via no additional compute. You've talked about how this might have a transformative economic impact.
One question I have is if you take in all things considered view of this, do you think this will turn out well for humanity?
Because a lot of economic growth could be fantastic and has been fantastic for living standards in the past. Could we be entering into a great time or could we potentially be entering into a dangerous time?
I think it could be really good or it could be really awful. I think the upside is it could be really high and I wouldn't personally think about it in terms of economic growth.
But I think about it in terms of human flourishing. You could have an end to illness, an end to poverty, an end to material needs.
You could have the possibility if you wanted to of going on any adventures or fulfilling any dreams you'd always wanted to pursue with new technologies, really, really incredible things might be possible.
And I think that that could be a really amazing future. I think it's really hard to paint a concrete vision of what that looks like because you could analogize it to trying to tell someone 2000 years ago, all the kind of luxuries and good things in one society.
You could point to this absolutely incredible entertainment going into a simulated world where you're on a real adventure.
It's such a difficult thing to do to sketch out how amazing things might be. It always kind of feels flat when you say it out loud in a sense. But I see what you're aiming for here.
The picture you're painting is of a future in which could either be very good or very bad. Do you see a potential for a kind of middle scenario in which the world continues more or less as it has been for the past 100 years?
Is that also a live option?
I think it's possible. So you could imagine hitting a real wall with AI development. And if that happens, then my default expectation would be that things continue as they have been for as long as it takes for us to get around that wall.
And if that wall is really very permanent, then we begin to get into worries about the rate of technological progress stagnating as population begins to shrink because fertility is below replacement rate.
And you can get into actually other worries if you really start to play out this world where we don't reach AGI. You could end up kind of stuck at current levels of technology for a long time, potentially.
Okay, let's end on a lighter note and talk about AI and board games.
Yeah, I've been thinking about what does it mean when an AI becomes superhuman at chess, for example, as happened in, I think, 97 or go, which happened in 2016.
In the past, you would have people talking about how chess is the height of human intellectual ability. But now it just seems like humans are playing a lot of chess.
Humans are interested in other humans playing chess. And even though there are some chess, some people who are very into chess who watch AIs play against other AIs, it seems that this is a domain in which humans are still very relevant.
Do you think there's some lesson there for broader AI automation of the economy?
I am also a little bit naively surprised at how many people are still really excited about the game. I think I had the attitude of, okay, well, if the AI can do it better than me, that kind of takes some of the excitement out of it.
That said, I'm a passionate diplomacy player, and people who follow AI closely may know that some AI has recently gone pretty good at diplomacy. I think still less good than the best humans, but I think better than the average humans, maybe.
At least on a text-based game. And it hasn't made me any less excited to play that game. So I think I was probably just kind of not adequately imagining what it would feel like for AI to kind of be matching my performance in chess.
Maybe an implication could be, even once AI is better at any task that you can imagine in the economy, there's still going to be people who are willing to pay for humans to do tasks, because that's something they find particularly interesting.
If they're interested to watch humans play chess, we'll probably still be interested to see humans produce art. Maybe we'll still want to kind of have human carers, human priests. So in terms of economic role for humans, I think this may point towards us not being kind of totally
obsolete, because humans kind of like to watch other humans do things, and that can give us some kind of job.
Is it worrying if AI's are getting good at the board game diplomacy? Does this mean that they might be able to do diplomacy in the real world, or that they might be able to use deception, or you can tell me about the details of the game, but that they might be able to set up some
agreement and then break the agreement afterwards?
It is a game where deception can get you a long way. I would have thought before it happened that this would be a milestone that would make me quite scared about AI deception manipulation, because it's a fairly complicated environment, and
social dynamics are potentially quite complicated, and it's an exhausting game to play, so I would have thought if AI was able to do this game, then it's really very socially competent and persuasive and manipulative.
In fact, when you see the system, particularly the AI system that's actually able to match some amount of human performance on diplomacy, it's not nearly as scary as you might have imagined.
It's trained on loads and loads of different examples of diplomacy in particular, and loads of examples of messages that humans sent in diplomacy games, and it's got a kind of engine which is custom built for choosing what diplomacy moves to make that won't easily generalize outside of that domain.
It seems like it's actually reaching that threshold, not by kind of thinking of new ingenious plans and manipulating humans on an untaught level, but just kind of by really learning the mechanics of the game and not making mistakes and being consistent or reliable.
And so it's actually a lot less scary than I would have thought, and I think it speaks to the difficulty of designing in advance a benchmark that measures the scary capability that when that benchmark is passed will actually make you scared, because often you pick a benchmark which seems scary, but then the AI system that matches that benchmark doesn't actually end up being as scary as you thought it might be.
Maybe in 2015 or so DeepMind talked about a strategy for getting to artificial general intelligence, which involved playing ever more complex board games and having these reinforcement learning agents in these in these kind of worlds where they're able to navigate more and more complex and then real world games.
That's one strategy to artificial general intelligence that kind of points towards more agency for the AI. You could call this the current paradigm of large language models. There's also a similar kind of convergence towards agency in AI.
At least that's that's what I'm hearing. Why is it that both of these strategies push towards more agency or more agent like behavior in AI.
I think the main thing pushing language models towards being more agent like is that it's useful to have an agent, because an agent can be more autonomous and do more open ended tasks that potentially automate larger chunks of your workflow.
And I think that's that's why I expect people to continue to try and improve on and iterate things like auto GPT that turn language models into agents.
So I think probably there is there is there is a kind of economic force pointing the direction of creating agents and just a general you know we want to use the AI to do useful things in the world.
And therefore we'll kind of try and make them more more authentic and autonomous. I think there's probably a different thing that explained deep minds approach where they're using reinforcement learning they were probably making a bet that that was just the most promising technological trajectory to get to that end point of an agent was by just kind of
And it's being kind of quite good news from my perspective that actually it seems like it makes more sense to first just train language models to kind of imitate human text.
And then later compose these kind of little chatbots into agent like things that I think makes it more promising that we could actually understand why these agents are behaving the ways that they are.
Tom, thanks for spending a lot of time with us. It's been very interesting for me.
Thank you so much. It's been a pleasure.

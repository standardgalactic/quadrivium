Welcome to the Future of Life Institute podcast. My name is Gus Ducker. This is a special episode of the podcast featuring Nathan Labence interviewing Flo Crivello.
Flo is an AI entrepreneur and the founder of Lindy AI. Nathan is the co-host of the Cognitive Revolution podcast, which I recommend for staying up to date on AI. Here is Flo and Nathan.
Man, you know, it's, it's a tough time for somebody that tries to keep up with everything going on in AI. It's like it's gone from, you know, in 2022, I felt like I could largely keep up and, you know, wasn't like missing whole major arcs of, you know, important stories.
And now I'm like, yeah, I'm like totally let go of like AI art generation, for example. And this policy stuff is really hard to keep up with, especially this week. Of course, it's like hitting a, you know, a fever pitch all at once.
But, you know, it's, I love it. So I can't really complain at all. It's just, at some point, you know, got to admit that I have to maybe narrow scope somehow or just let some things fall off. I'm kind of wrestling with that a little bit.
Which, which I think is just like a natural. Yeah, I mean, you know, I hear you. I think it's just a natural part of like the industry evolving. It's like, imagine, you know, talking about like keeping up with computers, right, in like the 80s or something.
It's like, I'm sure at some point it was possible to keep up with computers at large, you know, it's like keeping up with tech is just like, it's like, okay, dude, it's like, it's like half the GDP over, right?
You're doing all this in your second language, right? This is, I assume English is your second at least.
I have an excuse. Yeah, second, I'm actually getting my American citizenship. I had the interview just yesterday.
Wow, congratulations. That's great. I know it's not an easy process, although maybe it's about to get streamlined. I haven't even read that part of the executive order yet, but I understand that there is kind of an accelerated path for AI expertise. Have you seen what that is?
No, but generally, there's good stuff being done in immigration, like they're relaxing a lot of these requirements, they're like closing a lot of loopholes, they're doing a lot of good stuff.
Yeah, I've been thinking of just as kind of a general communication strategy, if nothing else, calling out the domains in which I am accelerationist, which are in fact many, I think, you know, you and I are pretty similar in this respect, where it's like, um, perhaps the singular question of the day.
I am not an accelerationist, but on so many other things, I very much am an accelerationist and like streamlining immigration would be one of those, you know, I would sooner sign up for the one billion Americans plan than kind of, you know, the build the wall plan, certainly.
And I just did a right before this was doing an episode on autonomy and, you know, self-driving. And that's another one where I'm like, holy moly, you know, I don't know if you have a take on this, but the, the recent cruise episode, I find to be, you know, kind of bringing my internal marketing,
Jason, very much to the four where I'm like, we're going to let one incident a like shut down this, you know, whole thing in California. That seems crazy enough. But then the fact that they go out and like do this whole sort of performative self, I mean, whether it's performative or not, maybe it's
sincere, but do this whole self-flagellation thing and, you know, shut the whole thing down nationwide. I'm like, can we, where is our inner Travis on this people? You know, somebody has to stand up for something here at some point.
Totally. I agree. I think it's just the natural order of things, right? It's like, I don't know if you know that piece of history about when the automobile came about. There was this insane law that said you need to have someone walking with a flag in front of the automobile.
That's no more than like four miles an hour. Right. So it's part of the process, man. It's infuriating. I hate it, but in some way, and maybe it's cool, but I made peace with it. I'm like, it's part of the process. You can't really stop. All right.
That's going to do its thing. So, and it doesn't really matter anyway, because like the self-driving cars are not really deploying at a very large scale. And so I'm like, you know, it's not a bottleneck anyway. I don't think it is.
I guess I have two reactions to that. One is like, it feels like if they, if nobody kind of fights through this moment, then there is like this potential for kind of the nuclear outcome where, you know, we just kind of get stuck and it's like, sorry, you know, the standards are so insane, you've got to be, you know, we do have a
little bit of like a chicken and egg problem where, you know, if you had a perfect self-driving car, they'd let you deploy it, but you're not going to get to perfect unless you can kind of deploy it. And, you know, to me, this technology is just an incredible example of where, you know, the relative risk is already pretty high.
As far as I can tell, they already do seem to be as safe or marginally safer, you know, maybe as much as order of magnitude safer already, depending on exactly what stats you look at. And I would just hate to see us get kind of, you know, is we're
like kind of close to maybe some sort of tipping point threshold, whatever, to get stuck in a bad equilibrium of, you know, never get, and then, you know, maybe get stuck and never get out of that chicken and egg thing would just be so frustrating. I drive a 2002 trailblazer that I have sworn never to replace unless it's with a self-driving car. And it's becoming increasingly difficult to keep this thing going, you know, so I'm like, how long do I have to do that?
I have to wait. My other take on this is, I think Tesla is actually like really good. I've borrowed a neighbor's. I don't know if you've done the FSD mode recently. My grandmother came up for a visit. It was fun. I actually took, you know, my 90-year-old grandmother on a trip back to her home, which is like a four-hour drive there, and then I did four hours back all in one kind of big FSD experiment.
I had my laptop in the back, put a seatbelt on my laptop, so it was like recording me and recording us, you know, driving so I could kind of look at the tape later. And I was like, man, this is really good. I had no doubt in my mind coming out of that experience that it's a better driver than like other people I have been in the car with, you know, for starters.
So I'm thinking through my personal life, like, yeah, I'd rather be in the car with an FSD than this person and that person and this other person, you know, and I'd be definitely more likely to let it drive my kids than this other person. So I felt like it was really good. And then the other thing that was really striking to me was the things where it messed up, I mean, there weren't many mess ups for one thing, but like the few mess ups that we had, there were a couple in an eight hour thing.
It was like, if we actually had any mojo and we went around kind of cleaning up the environment, we could solve a lot of this stuff. Like there was one that my neighbor who lent me the car said, you know, you're going to get to this intersection right there on the way to the highway and it's going to miss the stop sign because there's a tree in the way. And I was like, you know, for one thing, probably people miss that too, like, let's trim the trees, you know, and then there's another one where you're getting off the highway and there's a stop sign that's kind of ambiguous, like, it's meant for the people on the service road.
But it appears to be facing you as you're coming off the highway. And so the car saw that and stopped there. And that was probably the most dangerous thing that it did was, you know, stopping where people, you know, coming up the off ramp, like, do not want you or expect you to be stopped there.
But that's another one where you could just go like, put up a little blinder, you know, to just very easily solve that problem. And I imagine people must have that problem too. And we just have no, no will, you know, when it comes to that.
And again, it's I feel like I'm turning into Mark Andreessen. The more I think about self driving over the last few days.
No, I'm with you on that.
So where else are you accelerationist that may not be obvious as we kind of think about this, you know, this kind of AI safety and regulation moment that we're in?
You know, pretty much everywhere, man, like I'm a Libertarian, like I used to work at Uber where I saw regulatory capture and I saw cocktails and I do believe, you know, it's the deepest level that cocktails and regulatory capture and generally, I think it's Menker Olsen who calls them
Extractive institutions, who are just in the business of they don't want to grow the pie, they just want to grab a little bit more of the pie for themselves.
Even if it actually shrinks the pie, they don't care as much as they get bigger chunk. And I think that's the world is just rotten.
With thousands and thousands of these institutions, whether without private or without unions or governmental, it doesn't matter. We just have so many of these cartels floating around and it's killing everything.
Right. It's a tragedy. And I totally understand how folks like Mark Andreessen would be, they have built such a deep and justified hatred and reaction for this nonsense that is destroying everything.
That is immediately just the pattern recognition immediately triggers when they see what's happening with the eye. They're like, ah, it's happening again. They're doing it again.
It's like chill. I totally get it. But this time is really different. This is really something special that's happening, not just in the markets, not just in the economy, not just in the country, in the universe.
Like there is a new form of life that's being built. And this is, we're like a new territory and we need to be careful right now.
Right. And so that's, that's where I'm coming from is like, I totally see that point of view. And I'm like, regulation, for sure there's going to be cartels for sure we're going to screw up 90% of it.
Politics is going to get messy and trench interest are going to get into play.
And it's all worth it because what may very well be on the line, it sounds alarmist, but I'm sorry that we need to say the word may be literally human extinction.
Right. And this is not some tinfoil hat theory. There's a more and more experts that are coming around and saying that it's actually funny. Mark Andreessen, if you dig it up, I'm sure you could find it.
I think it was an interview from him. I want to say between 2017 and 2020 that doesn't help him because he gives so many of those.
But I think he said something like, at the time he was actually appealing to an argument of authority.
He was like, look, he was saying the same things he's saying today, poverty is good. It's just a tool. And by the way, the experts say there's nothing to worry about.
So I don't know. You guys don't know anything about AI. I don't know anything about AI. They do. And they're telling us there's nothing to worry about.
The argument isn't true anymore. The experts are telling us there is something to worry about.
And now it's just like, oh, arbitrary, like a regulatory capture. No, no, it's not regulatory capture.
Like OpenAI was founded on that premise from day one. So if it was regulatory capture, there's like one hell of a plan.
It's like, oh my God, we're going to create this industry and we're going to start regulatory capturing right now.
Right. It's like, that makes no sense. It was literally the plan from day one.
Yeah, that's all I'm coming from. I'm largely in the EAC camp. I am in team technology, team property, team anti-regulation, but here's something very special and potentially very dangerous.
So let's go back to your use of the phrase a new form of life.
I, as you may recall, am very anti-analogy as a way to understand AI because I think it's so often misleading.
And I often kind of say AI, artificial intelligence, alien intelligence, it may be tempting for people to kind of hear or not tempting,
but it may be sort of natural for people to hear you say a new form of life and understand that as an analogy.
But do you mean it as an analogy or I guess we might start to think about like, is that actually just literally true and what conditions would need to exist for it to be literally true?
And you might think about things like, can AI systems reproduce themselves? Are they subject to the laws of evolution?
But for starters, how literal do you mean it when you say that there's this new form of life in AI?
I mean it's pretty literally, I think if you zoom all the way out literally from the birth of the universe,
the evolution of the universe has been towards greater and greater degrees of self-organization of matter.
And there's actually a case to be made that this is just a natural consequence of the second law of thermodynamics,
this amazing book that Iac people love to quote.
Yeah, I was going to say, you're sounding very Iac all of a sudden.
It's a good point. It's called Every Life is on Fire.
And so if you look at the Big Bang, a few fractions of a second after the Big Bang,
it was just subatomic particles and then they ganged up together and formed atoms.
And then the stage after that was the atoms ganged up together and formed molecules.
And then the stage after that, the molecules became bigger and bigger because the stars exploded and caused all sorts of reactions.
And so a few generations of stars later, we have like pretty big molecules and pretty heavy ones.
And then these molecules formed into sort of like protein and RNA and forms of proto-life.
We don't totally understand, there's a chain here that we don't totally understand,
but there's a form of proto-life that formed and then life.
And so you can think of like, I think it was just a DNA, actually it was RNA, DNA, nucleus of a cell,
mitochondria came into that, and then, okay, good, we have a cell.
And then the cells started ganging up together and now we have multicellular organisms.
And then we have brains at some point, like there's like a big leap, but we have brains,
like on that great march towards greater and greater degrees of step-organization.
And at some point we have us, which with a little bit of hubris perhaps,
considering the apex of that thing for now.
It just seems crazy to me that everybody is saying like, one, this is totally normal.
Oh, this is normal.
This is quintillions of atoms that are organized in this weird, super coherent fashion
that are pursuing a goal in the universe.
Like what's happening right now on Earth is all the weird to begin with.
So people are all deep thinking that this is normal and that's what it is,
and that this march is going to stop at them.
And they're like, well, maybe we're going to get slightly smarter,
or maybe we're going to get augmented and I'm like, you are such a leap compared to an atom,
or compared to a bacteria, that there is no reason to expect that there wouldn't be
another thing above you that is as much more complex or bigger than you,
as the new world to the bacteria.
Like there's nothing in the universe that forbids that from happening.
From a being to exist that is about as big as the planet or the galaxy.
Like there's nothing forbidding that in the universe from happening.
And from the first time now, if you squint, we can sort of see how that happens.
And silicon-based intelligence certainly seems to have a lot of strengths
at its sleeve versus carbon-based intelligence.
And so no, I actually sort of mean that pretty vitrally.
It is sort of in line with the march of the universe and this is the next step,
perhaps it's significant.
And so I am hopeful that we can manage this transition without us being destroyed.
That's what I want to have.
Does that imply an inevitability to advanced AI?
I guess a lot of people out there would say, hey, let's pause it,
slow the whole thing down, and then you get kind of the response from an open AI
where they're sort of saying, yeah, we do take these risks very seriously
and we want to do everything we can to avoid them.
But we can't really pause or we don't think that would be wise
because then the compute overhang is just going to grow
and then things might even be more sudden and disruptive in the future.
Where are you on kind of the inevitability of this increasingly capable AI coming online?
I don't think it's totally inevitable.
I am generally a huge believer in human agency.
I think we can do pretty much anything we set our minds to.
I see a contradiction, by the way, in the EACC argument that like,
on the one hand it's inevitable and try to stop it,
on the other hand, oh my god, if you do this, I'm going to stop it.
It's like, you got to decide here.
So unfortunately, it's not necessarily inevitable.
I am actually worried as much as the next guy, I agree,
there is a risk that we over-regulate and miss out on the upside.
And the upside is significant.
And if you look like during the Middle Ages,
we successfully as a civilization stopped progress
and in a lot of countries, if you look at North Korea, they did it.
They successfully stopped progress.
So you can stop progress.
Progress is not inevitable.
Or maybe it is actually quite fragile.
So no, I don't think it's inevitable.
And I'm hopeful that we can, again,
I want us to get the upside without experiencing the downside.
The North Korea example is an interesting one.
If I was going to kind of dig in there a little bit more, I might say,
okay, I can understand how if things go totally off track,
then we could maybe enter into a low or no or even negative progress trajectory.
If there were a nuclear war, then we may not come back from that for a long time.
Or if whatever, an asteroid hit the earth or a pandemic wiped out 99%,
like there's extreme scenarios where it's pretty intuitive for me
to imagine how progress might stop or just be whatever,
greatly reversed or whatever.
If I'm imagining kind of a continuation-ish of where we are,
then it's harder for me to imagine how we don't kind of keep on this track.
Because it just seems like everything is, we're in this, I would call it,
I don't know if it's going to be a long-term exponential,
but we seem to be entering a steep part of an S-curve where hardware is coming
on lines by the order of magnitude and at the same time,
like algorithmic improvements are taking out a lot of the compute requirements.
And we're just seeing all these existence proofs of what's possible
and all sorts of little clever things and scaffolding along the lines
of some of the stuff that you're building is getting better and better.
Is there a way that we can, do you think it is realistic to think we could
kind of meaningfully pause or even stop without a total derailment of civilization?
The derailment of civilization thing, you could imagine the most extreme scenario
which I am not proposing, but you could imagine the most extreme scenario
which is no more Warsaw.
You do not exponentially improve your semi-conductors anymore.
That'd be crazy, right? But there wouldn't derail civilization.
Civilization is not predicated upon Warsaw.
We would do just fine with the chips we've got today.
And if anything, I think we have a lot of overhang from the chips we have today,
a few to choose overhang, right?
So I actually think it is possible to do that if we wanted to.
And I don't think that even this, which I think is the most extreme scenario,
would actually derail civilization.
Well, we are actually lucky in that there are a few choke points in the industry.
Actually, more than a few. There is ASML, there's TSMC, there's NVIDIA,
like all of those three are individually at our choke point.
Like every regulator could at any point grab one of them and be like,
no more, you just stop, right?
Or you add this chip into all of your GPUs moving forward,
so we have a kill switch. At the very least, we have that.
So if shit really hits the fan, we have an automatic thing in place
that shuts down the very GPU on this, right?
Now that would be disruptive, but potentially less disruptive than the rogue ASI.
So no, I actually think it is very much possible.
This thing's all on the table, and I don't think there would be all that disruptive.
So maybe that's a good transition to kind of where we are right now, right?
We just had this executive order put out this week,
and I think everybody's still kind of absorbing the 100-plus pages
and trying to figure out exactly what it means.
What's your high-level reaction to it?
And then I'll get into some of the specifics.
First of all, it's an executive order for now. It is very early.
Overall, I am pleasantly surprised, not by the specifics,
but by the facts that were reacting quickly,
by the facts that the measures that are proposed are not insane.
Like, I was afraid of, like, there's a really good case to be made.
The second look, we have a different processing case.
Now, a bunch of 70, 80-year-olds go running as they don't know anything
when they were born. There was no mobile phone, right?
Can't really blame them for not really understanding anything.
And so I was afraid that the regulation would go something like,
if you install Microsoft Office in your AI, then you have to make a report.
So the regulation actually sort of makes sense.
It's talking about Flops. It's talking about all those things of training.
So I think it's a step in the right direction.
I'm actually happy about what's happening with this executive order.
Now, the specifics, look, the problem is that it's almost impossible
to regulate AI in a way that doesn't have any loophole.
So they're regulating it according to the new old Flops, and that's okay.
But that's the end of the day, and then you get stuck into,
okay, what happens when you have algorithmic improvements?
What happens when you do URL instead of computing?
And like, that's just a lot of different loopholes that researchers are going to find.
And so I think, overall, it's an encouraging first step.
It's funny. You know, there have been proposals around even, like,
a flop threshold that would drop progressively over time
in kind of anticipation of the algorithmic improvements.
That's even a more probably challenging one to put out into the world,
especially given people are not in general great at extrapolating technology trends
or don't want to accept regulation in advance of stuff actually being invented.
So we've got this flop threshold thing where basically, as I understand it so far,
like, if you're going to do something this big,
you have to tell the government that you're going to do it
and you have to bring your test results to the government.
I would agree with that. That seems like a pretty good start.
And also the threshold seems like pretty reasonably chosen at 10 to the 26.
Any, you know, kind of refinements on that or quibbles that you would put forward
that you think like, you know, maybe the next evolution of this should take into account?
I think ultimately we're tiptoeing around the issue,
but ultimately we need to come to an actual technical blanket solution.
Like, we will not solve ASI alignment by asking for reports from AI companies.
That's not how it's going to happen.
So again, I think it's a step in our direction.
I'm happy with the action. I'm happy the action is not totally nonsensical.
But at the end of the day, we're going to have to talk about the kill switch.
The proposal I just made is one that I see more and more talked about
and that's the one that I would feel best about.
You've got to put this chip into your H100s and the government
and there's like a centralized entity that can shut down all GPUs all at once.
And by the way, it wouldn't necessarily shut down every computer
because your laptop doesn't have an H100, your iPhone doesn't have an H100.
That's fine.
Over the long term, Moore's Law makes it so that your laptop and your phone
actually end up with an H100, but at least that dies us a few years
to make progress on AI safety and alignment.
Ideally, we would then automate just like reportedly the Russians did during the Cold War.
We would automate.
Like, we would set up some detection systems to God knows how we would do that.
But hey, there's no ASI going wrong.
Like, the world is really changing rapidly.
We're assuming it's not too late, which it may be because at that point God knows.
But you could very basically that would give us the best weapon against the ASI.
We would have like a gun against the ASI's hand and kill all the GPUs.
You cannot operate anymore.
God knows how effective that would be because at that point all bets are off.
If you have an ASI God knows what it does and how it connects itself.
But that would be what it would feel best about.
Do you have any sense for how that would be implemented technically?
It seems like you would almost want it to be something that you could kind of broadcast.
You know, you almost want like a receiver on chip that would react to a particular broadcast signal
and just kind of because you would not want to have like, you know, an elaborate chain of command
or, you know, relying on like the dude who happens to be on the night shift at the, you know,
the individual data centers to go through and like, you know, pull some lever, right?
Do you know of anybody who's done kind of advanced thinking on that?
That stuff is like, you know, you hear a lot of these like kill switch things,
but in terms of how that actually happens so that it's not dependent on, you know,
a lot of people coming through in a key moment, I haven't heard too much, to be honest.
No, I haven't seen too much results on that.
But, you know, I think the technical challenge does nothing in principle that makes the technical challenge
unsolvable. Like we've already had a chip that can be broadcasted to for like a dollar from space,
like the GPS chip does a lot of chips and like it has one on your phone.
And so why not put the GPS like chip?
Maybe we could literally piggyback the GPS protocol.
I don't know, but why not put the chip like that in every, in every GPU?
Again, if you have an ASI, God knows, like maybe it hacks the chips before we get a chance,
you know, it hacks the satellites that forecast the thing, I have no idea.
But again, I think pointing in this direction is what I would like things to go into the limit.
I think the basically, and that's like the most extreme version of this proposal,
but like the Yutkovsky airstrike proposal.
That's like, you cannot accumulate billions and billions of dollars of H100s and build this thing.
Else we will go up to airstrike.
That's the most extreme version of this, but that actually I think is directionally correct.
Like we, this is going to be the most powerful force in human history, maybe even in the universe.
You cannot accumulate that stuff anymore than you can accumulate enriched plutonium, right?
We've got a, we've got a four-bit stat that's the lowest level possible.
And so that level cannot be the application layer because the application layer is just,
it's just to diffuse those like a thousand startups everywhere and you get in the garage can build one.
It's got to be at a took point and the took point today is the city code.
Yeah, let's unpack that a little bit more because I think that has been an interesting debate recently.
You'll hear this kind of call for let's not regulate model development.
Let's regulate applications and then, you know, we can kind of have medical regulation for the medical
and everything can be more appropriate and like fit for purpose.
And, you know, maybe there's something else to be said for that.
But yeah, I mean, if you're really worried about tail risk, it's like probably not going to be sort of medical, you know,
device style regulation of, you know, diagnostic models or whatever that is going to keep things under control.
So maybe you could even do a better job of steelmanning the case for the application level regulation.
But I guess, you know, why do you think that give your account of why that's not viable in a little bit more detail?
Yeah, I think the steelman here is like, look, people are going to use forks to poke each other in the eye.
That's not a reason to forbid the fork.
Like forks are awesome.
We love forks just for people from poking each other in the eye with them, right?
The problem is that as the fork in this analogy becomes more and more powerful,
the argument loses more and more of its defense because ultimately it's just a risk benefit analysis.
Right.
And so the risk becomes greater and greater as the artifact becomes more and more powerful.
So more powerful than the fork and al 15.
And so, you know, the opinions vary about that.
But look at at this point, if you look at the data,
you actually save lives by heavily regulating the sale of al 15.
You can't just be like, oh, sell them to everyone and just for big people from shooting each other with them.
It's like, it's an al 15.
What do you expect people to do with them?
Now, in the more in the most extreme scenario, enriched uranium,
you can't be like, you can buy all the enriched uranium you want.
You don't even need to fill up a form, which by the way,
that is all the executive order says right now, at least fill up a form.
Can you please at least tell us what you have to do?
So hey, you can build, you can build all the enriched uranium you want.
Just don't bond us with us with it, please.
Like when we roll it in this disappear, you can do it.
Oh, no, that's not, that's not how it works.
So that, that, that is why I think it's important to regulate the, the silicone layer.
Do you have an intuition for sort of how likely things are to get crazy at kind of either various
time scales or potentially various like compute thresholds?
I was realizing, I did an episode with Jan Tallin a couple of months back,
just in the wake of the GPT-4 deployment.
And he said, we dodged a bullet with GPT-4 or something like that.
Like in his mind, we didn't know if, you know, even at the GPT-4 scale,
like that might have already been, you know, no, no real principled reason to believe that
with any, with like super high confidence that the GPT-4 scale was not going to cross
some, you know, critical threshold or whatever.
I guess I don't really have a great sense for this.
I just kind of feel like, and this was purely like gut level intuition that, yeah,
we could probably do like GPT-5 and it'll probably be fine.
And then kind of beyond that, I'm like, I have no idea.
Do you have anything more specific that you are working with in terms of a framework of like how,
you know, when you hear, for example, Mustafa from inflection say, oh yeah,
we're definitely going to train, you know, orders of magnitude bigger than GPT-4
over the next couple of years.
Are you like, well, as long as you stay to two to three orders of magnitude more,
we'll be okay.
Or like, I just have no, you know, we're just flying so blind,
but I wonder if maybe you're flying slightly less blind than I am.
I am of the opinion that GPT-4 is the most critical component for AGI.
And that's the gap from GPT-4 to proper AGI is not research, it's engineering.
It sits outside the model.
So I think we have a capabilities overhang here that can turn GPT-4, as it is today,
into AGI, into proper AGI.
I think generally that's the case for any technology.
If you look, for example, at Bitcoin, what changed from a technological standpoint
that allowed Bitcoin to happen?
It was the same technology we'd had for a while and yet Bitcoin,
it's going to go wild to happen.
So there was this overhang and Bitcoin, whatever your opinion about crypto,
changed a lot of games, right?
I think there's this huge overhang with GPT-4.
I think we basically have a reasoning module of AGI.
I don't know if you saw this paper that found literally just asking it,
hey, take a deep breath and take a step back.
Just take a step back apparently also makes a huge difference.
So I think there's a lot of tricks like that that will make a difference.
The sort of cognitive architectural layers around GPT-4 I think can bring it to AGI.
That is also why you asked me about what sort of regulation I wish was put into place.
We need to stop open sourcing this model.
We don't know what kind of overhang exists out there.
I don't think Lama-2 is there, but like I said, I think GPT-4 is there.
So Lama-3, if it's GPT-4 level, boom, it's too late.
The weights are out there.
Okay, now you can do, maybe you can put strap on there.
So we need to stop open sourcing this next.
I expect my timelines for proper AGI to emerge is two to eight years.
I think there's a more than even chance of AGI emerging in two to eight years.
I think the base scenario is things are going to go well just for the record.
I don't think there's like a 99% chance of doom.
But even if it's 10%, I think it's worth being very, very worried about.
That's enough for me.
10% of all of us dying like I'm talking about it, please.
So two to eight years, 50% chance of AGI, things probably will go well,
except for, you know, CVD digital disruption.
There's going to be like stuff.
There's going to be a crazy shit happening, but two to eight years.
And after that, all bets off.
I have no idea what the bootstrapping to ASI look like,
but I don't expect ASI to take more than 30 years.
So I expect that you and I in our lifetimes are going to see ASI.
So that's a pretty striking claim.
I think you probably puts you in a pretty small minority.
And I don't think I'm really there with you when you say that you think GPT-4
kind of already contains the, you know, the kind of necessary core element for an AGI.
So I'd like to understand that a little bit better.
I mean, you'll have a lot of people who will say, you know, look, it can't play tic-tac-toe.
I think on some level, those kind of, oh, look at these like simple failure objections are kind of lame
and sort of miss the point because of all things obviously can do.
But I do, you know, if I'm thinking like, does this system seem like it has this kind of sufficiently
well-developed world model?
Or, you know, I'm not even sure exactly how you're conceiving of the core thing.
But, you know, for a question like that, I would say those failures maybe are kind of illuminating.
On the other hand, I'm sure you've seen this Eureka paper out of NVIDIA recently where they used GPT-4
as a superhuman reward model author to teach robot hands to do stuff.
And I thought that one was pretty striking because as far as I know, and I actually used the term Eureka moment,
many times said, we don't see yet Eureka moments coming from highly general systems.
You know, we see Eureka moments from like an alpha go.
We haven't really seen like Eureka moments from a GPT-4 until maybe this.
This seems like maybe one of the first things where it's like, wow, GPT-4 at a task that requires a lot of expertise.
That is designing reward functions for robot learning, robot reinforcement learning.
GPT-4 is meaningfully outperforming human experts.
And so I think it's very appropriate that they call it Eureka.
What do you think is the core thing?
You know, is it this like ability to have Eureka moments?
Is it something else?
Why do you feel like it's there?
And does it not trouble you that it can't play tic-tac-toe?
For the sake of this conversation, I'm going to define a GI as a seed AI, an AI that can recursively self-improve.
That's a much more narrow definition of a GI than most people use, but that's actually what I care about.
Can we enter this recursive loop of self-improvement that puts track surface to ASI?
In order to get there, you don't need to play tic-tac-toe.
You need to be good enough, and the world good enough here is important,
a good enough either software engineer or chip designer or AI and ML researcher.
One of these things.
So something that can get you to put track.
And so good enough does not mean better than the best human.
It doesn't even mean better than the average human.
It just means good enough that you can make a difference, a positive difference in your own ability to get better.
Right?
So if you enter the recursive loop of self-improvement, then mathematically it's over.
And yeah, when I see the NVIDIA paper, I see that.
When I see our own experience with the model.
So today we are using Lindy to write her own integrations,
and Lindy is writing more and more of her own code.
I see that.
Even as it belongs to AI researchers and ML researchers,
my hypothesis is that OpenAI is using GPT-4 more and more internally to perform AI research.
My not hypothesis is the fact is that NVIDIA is releasing papers that's like,
well, not only can we use it for AI research through this Eureka paper,
but we can also use it for chip design.
It works super well.
We trained an AI model that does chip design super well.
So we are starting to see the glimpses of that kind of recursive loop of self-improvement.
Basically, the world model question kind of on the sidestep,
because I feel like at this point the debate has become silly for people who argue that it's bad
or doesn't have a world model.
What matters is, is it good enough?
And so even if it just overfits its training set,
even if it's just predicting the next token and not actually understanding anything,
I actually really do believe it understands a lot.
But even if it's not, you can imagine it does this many-dimensional space
with a ton of data points in there.
And it's good by interpolating between the data points
and it needs much more data points to understand anything than a human.
And so there's that envelope in that space where the data points are dense enough
that it can perform.
And so that's called the convex hole.
And then there's data points outside that convex hole,
and it does really poorly outside the convex hole, much more poorly than humans.
It's convex hole requires a lot more density than humans do exist.
There's multiple questions, which are, one, all these data points inside,
the convex hole is the sum of all human knowledge.
GP for today knows more than you.
I don't know that it can reason better than you,
that's the expanding the convex hole thing,
but it knows more than you inside that convex hole.
And so inside that convex hole, an AI researcher that's read every paper ever,
not just in AI, but in mass and biology,
every paper ever finds the entirety of the internet,
is it better than a human AI researcher?
I think the answer is yes.
Even if it's not better, there's the outside of that convex hole,
and this is my point about the capabilities of a hang,
can we get this AI model through prompting, through cognitive architecture,
to do better outside its convex hole?
And we'll see that all the time,
seeing papers come out to bed like,
hey, we have found an automatic way to rewrite a prompt that makes it a lot better.
We have found a way that people that came out a few days ago,
that's like, hey, if you ask the model to take a step back
and to rephrase the problem you're giving it in terms of a universal problem,
it performs a lot better.
And that makes total sense,
because the specific of the problem is probably not seen as that specific problem in its dataset,
but if you ask it to reframe it, it's basically translating the problem into a form
in which it's comfortable with.
And so we're actually getting it to grow its convex hole like that.
That's my take, is I think the convex hole is good enough to get to that good enough point,
and I think we can grow that convex hole.
And so I think that basically, if GPT-4 isn't a CDI, it will still GPT-5 is one.
Yeah, that's an interesting framing.
I find your analysis there pretty compelling.
The idea that, you know, given what we have seen from like a Eureka,
you know, with this robot training, or there was another interesting one recently,
I think it was out of Microsoft, I covered this in one of the research rundown episodes,
on recursive or iterative improvement on a software improver.
So they basically take a real simple software improver, you know,
that can improve a piece of software, and then they feed that software improver to itself
and just run that on itself over and over again.
You know, it kind of tops out because it's not, it doesn't, you know,
in this framework, it doesn't have access to like tinkering with, you know,
possible methods for training itself, but it makes significant improvement
and gets us some pretty advanced algorithms where it starts to do like genetic search
and, you know, a variety of things where I'm like,
I don't even really know what that is, you know, like simulated annealing algorithms.
I'm like, what, you know, but it comes up with that and, you know,
uses that to improve the improver.
And, you know, this is all measured by how effectively it can do the downstream task.
It does seem like it's not a huge stretch to say that, you know,
could you take the architecture of GPT-4 and start to do, you know,
parameter sweeps and start to, you know, mutate the architecture itself.
It seems like it probably can do that.
And I would agree, you know, it probably does.
Yeah, certainly just based on what I do, you know, with GPT-4 for coding,
I would have to imagine that it is in heavy use as they're, you know,
performing all that kind of exploratory work, you know, within an open AI.
And so, yeah, and I think to your point,
we all see enough of these signs of life across the board in a lot of different areas.
A lot of institutions are like, ah, a little bit of very good stuff
from here, a little bit here, a little bit here.
It's not very hard to imagine it getting to a state velocity,
to imagine it going super critical and pass a certain threshold where I say,
okay, now boom, it can ready take off.
So, and I've actually heard multiple people from open AI say that they believe,
and I agree with their conclusion.
And they actually told me that before I agreed with them,
they told me that at the very beginning of the year,
so before GPT-4 was widely available.
And they told me, you know, I think we're like, you know,
we have a GEI and we're in a slow take off.
And I felt something like this, crazy.
Well, they didn't say, sorry, they basically were talking about GPT-4.
I think, and I am not representing that this is the universal position of open AI,
but I've heard multiple people from open AI and other labs tell me that.
We have a GEI and we're in a slow take off.
So, given that, okay, we've got this compute threshold.
We maybe need a kill switch.
Now I'm getting, we started this conversation with me, with my, you know,
IAC side coming out and, you know, being like,
why can't we get myself driving car on the road and tolerate,
you know, some reasonable amount of risk to do that.
Now my other side is coming out and I'm like,
okay, what else might we do, right?
We've got the AI safety summit going on right now in the UK.
I thought it was cool to see today that there's some kind of joint statements
between Chinese and Western academics and, you know,
thought leaders in the space where they're kind of saying,
yeah, we need to work together on this.
Human extinction is something that we think could happen if we're not careful.
Do you have a point of view on kind of collaborating with China
or coordinating with China?
I mean, that's a tough question, obviously.
Nobody really knows China.
I don't think super well, but what do you think about that?
I mean, are we naive to hope?
I kind of feel like what else are we going to do except give it a shot?
Yeah, 100%.
And there is ample precedent.
You know, everybody is always talking about these coordination problems.
They've taken like the one-on-one course of game theory and they're like,
look, we can't coordinate.
Well, like if you take game theory one or two, it's like solutions to the coordination problem, right?
And so the solution to the collaboration problem is few players in a very iterated game.
And that is the game right now.
There's very few players and they're all in a very iterated game.
They're not the best buddies, but they are actually able to agree on a lot of things.
And so we can coordinate with China.
And again, to your point, what choice do we have anyway, right?
And even if we do not coordinate with them, again, there's enough truth holds
enough of which are American, right?
NVDI is an American company, last time I checked.
And so there's enough truth holds that we could actually do very much
not give them a choice like, hey, your GPUs now have the chip right here.
And so whether you like it or not, we have a satellite up here
and we can tell the GPUs out there.
And that wouldn't be, we could even just downright for big GPUs,
by the way, to be sold in China.
Like we've done stuff like that before.
So no, I think coordination is definitely possible
and I actually think it's going to happen.
I'm actually really very much encouraged by, well, winning.
Like I think the safety side is making really good progress.
There is rising public awareness.
I think Duffington is doing an amazing work here.
The regulation is coming.
It's mostly sensical.
There's this sort of progress that's happening across the board.
AI labs are investing more and more in safety and alignment.
Even from a technical standpoint, the work that AnsibleTik is doing,
I think is absolutely brilliant.
So we're making really good progress across the board here.
I don't want to represent that it will be on the board.
Yeah, I totally agree.
I would say my kind of high level narrative on this recently has been,
it feels like we're at the beginning of chapter two of the overall AI story.
And chapter one was largely, you know,
characterized by a lot of speculation about what might happen.
And amazingly, kind of at the end of chapter one,
beginning of chapter two, not all,
but like a large chair of the key players seem to be really serious minded
and, you know, well aware of the risks.
And it's easy to imagine for me a very different scenario where everybody,
you know, all the leading developers are like highly dismissive of the potential problems.
But it's hard for me to imagine a scenario that would be like all that much better
than, you know, the current dynamic.
So I do feel, you know, like overall, you know, pretty, pretty lucky
or pretty grateful that, you know, things are shaping up at least, you know,
to give us a good chance to try to get a handle on all this sort of stuff.
One last question.
This is super philosophical.
I know you got to go, but how much depends in your mind on whether or not,
let's say Silicon base intelligence or AI systems or whatever might become
or maybe already are, you know, I'm not sure how we would ever tell the kinds of things
that have subjective experience.
You know, does it matter to you if it feels like something to be GPT for?
Have you heard of the world move?
I think it's Zen philosophy in Buddhism.
There's this story that's like someone asks someone else like, hey, does kind of Doug
have the essence of a Buddha?
If the Buddha is everywhere and he never being kind of Doug have the essence of a Buddha.
And the answer to that is move.
And move means neither yes or no.
It's a way to unask the question.
It's a way to reject the premise of the question.
And basically in this sense, it means there is no such thing as the essence of the Buddha.
It's like the same question is like, hey, what happened before the universe existed?
Move.
There was no before because the bills of the universe was the bills of time.
So the will to be full only makes a sense in the context of the universe.
And so anyway, that's all of my insight.
Whenever I ask a question, whenever someone asks me questions about subjective experience
and consciousness, I'm like move.
It doesn't exist.
It doesn't matter.
It's immeasurable.
It's not a scientific thing.
And so move.
All right.
Well, some questions bound to remain unanswered.
And I appreciate your time today.
This is always super lively.
Next time I want to get the Lindy update.
And at some point I want to get access.
But for now, I'll just say Fluckravello.
Thank you for being part of the Cognitive Revolution.
Thanks, Mason.

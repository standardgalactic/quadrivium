Welcome to the Future of Life Institute podcast.
I'm Gus Docher and I'm here with Roman Jampolsky.
Roman is a computer scientist from the University of Louisville.
Roman, welcome to the podcast.
Thanks for inviting me. It's good to be back.
I think it's my third time on a FLI podcast, if I'm not mistaken.
Great. You have this survey paper of objections to AI safety research,
and I find this very interesting.
I feel like this is a good way to spend your time to collect all of these objections
and see if they have any merit and consider them.
And so I think we should dive into it.
One objection you raise under the technical objections
is that AI, in a sense, doesn't exist.
If we call it something else, it sounds less scary.
Perhaps you could unpack that a bit.
So those are objections from people who think there is no AI risk or risk is not real.
That's not my objections to technical work or safety work.
We try to do a very comprehensive survey, so even silly ones are included.
And people do try to explain that artificial intelligence is a scary sounding scientific term,
but if you just call it matrix multiplication, then of course it's not scary at all.
It's just statistics and we have nothing to worry about.
So it seems they're trying to kind of shift the narrative
by using this approach of getting away from agent hood
and kind of built-in scenarios people have for AI,
to something no one is scared of, calculators, addition, algebra.
Perhaps there is a way to frame this objection where it makes a bit of sense
in that people are quite sensitive to how you frame risks.
People are quite sensitive to certain words in particular.
So do you see that perhaps people who are in favor of AI safety research
by calling AI something that sounds scary might be, in a sense, inflating the risks?
Well, it's definitely a tool people use to manipulate any debate.
I mean, whatever you're talking about abortion or anything else,
it's like, are you killing babies or you're making a choice?
Of course, language can be used to manipulate,
but it helps to look for capability equivalences.
Are we creating God-like machines or is it just a table in a database?
So that would make a difference in how you perceive it.
And perhaps we should just simply be willing to accept that, yes,
what we are afraid of, in a sense, is matrix multiplication or data processing
or whatever you want to call it, because these things might still have scary properties.
Whatever we call them.
Right, so we can argue that humans are just stakes, pieces of meat with electricity in them
and it doesn't sound so bad until you realize we can create nuclear weapons.
So it's all about perception and what you're hoping to accomplish.
All right, there is also an objection going along the lines
that superintelligence is impossible.
What's the strongest form of this objection?
So essentially, the argument goes that there are some upper limits on capability.
Maybe they are based on laws of physics.
You just cannot in this universe have anything greater than a human brain.
Just for some reason, that's the ultimate endpoint and that's why evolution stopped there
and somehow we magically ended up being at the very top of a food chain.
There could be other arguments about, okay, maybe it's not absolute theoretical limit,
but in practical terms, without quantum computers, we'll never get there.
You can have many flavors of this, but the idea is that we're just never going to be outcompeted.
And this doesn't strike me as particularly plausible.
We could imagine humans simply with physically bigger brains.
So the version where humans are at the absolute limit of intelligence doesn't sound plausible,
but is there some story in which physics puts limits on intelligence?
There could be a very, very high upper limit to which we are nowhere close,
but if you think about the size of a possible brain, Jupiter-sized brains,
at some point the density will collapse into some black hole singularity.
But this is not something we need to worry about just yet,
not smart enough superintelligence is where nowhere near the size of capability.
And from our point of view, we won't be able to tell the difference system with,
I mean, hypothetically IQ of a million versus IQ of a billion will look very similar to us.
Yeah, and perhaps a related worry is that stories of self-improving AIs are wrong, in a sense.
So it's definitely easy to make such claims because we don't have good examples of software doing it more than once.
So you have compilers which go through code, optimize it, but they don't continuously self-optimize.
But it's not impossible to see if you automate science and engineering,
then scientists and engineers will look at their own code and continue this process.
So it seems quite reasonable.
There could be strong diminishing returns on that,
but you have to consider other options for becoming smarter.
It's not just improving the algorithm.
You can have faster hardware, you can have more memory,
you can have more processes running in parallel.
There are different types of how you get to superintelligent performance.
And you could, of course, have AI held along the way there with development of hardware
or discovery of new hardware techniques as well as new algorithmic techniques and so on.
That's exactly the point, right?
So you'll get better at getting better and this process will accelerate until you can't keep up with it.
How do you feel about tools such as Copilot, which is a tool that programmers can use for auto-completing their code?
Is this a form of proto-self-improvement or would that be stretching the term?
Well, eventually, when it's good enough to be an independent programmer, it would be good.
But I'm very concerned with such systems because from what I understand,
the bugs they would introduce would be very different from typical bugs human programmers will introduce.
So debugging would be even harder from our point of view,
monitoring it, making sure that there is not this inheritance of calls to a buggy first version.
So yeah, long term, I don't think it's a very good thing for us that we no longer can keep up with the debugging process.
Would you count it as a self-improving process?
So I think for self-improvement, you need multiple iterations.
If it does something once or even like a constant number of times, I would not go there.
It's an optimization process, but it's not an ongoing, continuous, hyper-exponential process.
So it's not as concerning yet.
Then there's the question of consciousness.
So one objection to AGI or to strong AI or whatever you want to call it is that AI won't be conscious and therefore it can't be human level.
So for me, at least, there seems to be some confusion of concepts between intelligence and consciousness.
I consider these to be separable.
I agree completely.
They have nothing in common, but people then they hear about it.
They always say, oh, it's not going to be self-aware.
It's not going to be conscious.
They probably mean capable in terms of intelligence and optimization.
But there is a separate property of having internal states and qualia.
And you can make an argument that without it, you cannot form goals.
You cannot want to accomplish things in the world.
So it's something to address.
And perhaps we should understand consciousness differently than the qualia interpretation.
Could we be talking past each other?
It's definitely possible.
And even if we agreed, consciousness itself is not a well-defined, easy to measure scientific terms.
So even if we said, yeah, it's all about qualia, we'd still have no idea if it actually has any or how would we define what amount of consciousness it has?
Perhaps a bit related to the previous question.
We have the objections, the objection that AIs will simply be tools for us.
I think this sounds at least somewhat plausible to me since AIs today function as tools.
And perhaps we can imagine a world in which they stay tools and these are programs that we call upon to solve specific tasks.
But they are never agents that can accomplish something and have goals of their own and so on.
So latest models were released as tools and immediately people said, hey, let's make a loop out of them.
Give them ability to create their own goals and make them as agentic as possible within a week.
So yeah, I think it's not going to last long.
What is it that pushes AIs to become more like agents and less like tools?
So a tool in my at least perception is something a human has to initiate interaction with.
I ask it a question, it responds, I give it some input, it provides output.
Whereas an agent doesn't wait for environment to prompt it.
It's already working on some internal goal, generating new goals, plans.
Even if I go away, it continues this process.
One objection is that you can always simply turn off the AI if it goes out of hand and if you feel like you're not in control of it.
And this is easier to imagine doing if you're dealing with something that's more like a tool and it's more difficult to imagine if you're dealing with something that's an agent.
So perhaps willingness to believe that you can simply turn off the AI is related to thinking about AIs as tools.
It's possible.
With narrow AIs, you probably could be able to shut it down depending on how much of you infrastructure it controls.
You may not like what happens when you turn it off, but it's at least conceivable to accomplish it.
Whereas if it's an agent, it has goals, it's more capable than you and would like to continue working in its goals.
It's probably not going to let you just shut it off.
But as the world is today, we could probably shut off all of the AI services.
If we had a very strong campaign of simply shutting off all the servers, there would be no AI in the world anymore.
Isn't that somewhat plausible?
Scientifically, it's a possibility.
But in reality, you will lose so much in economic capability, in communications, military defense.
Everything is already controlled by dummy AIs.
So between stock market and just normal commerce, communications, Amazon,
I don't think it's something you can do in practice without taking civilization back, you know, 500 years.
It's also difficult.
Like in practice, you would still have people who don't agree and continue running parts of the internet.
No, it's very resilient.
Think about shutting down maybe crypto blockchain or computer virus without destroying everything around it.
Yeah, if I understand it correctly, we still have viruses from the 90s loose on the internet being shared over email and so on.
And these are like biological viruses in that they, in some sense, survive on their own and replicate on their own.
Probably sitting somewhere on a floppy disk waiting to be inserted.
Just give me a chance, I can do it.
Many of these objections are along the lines of, we will see AIs doing something we dislike,
and then we will have time to react and perhaps turn them off or perhaps reprogram them.
Do you think that's a realistic prospect that we can continually evaluate what AIs are doing in the world
and then shift or change something if they're doing something we don't like?
So a lot of my research is about what capabilities we have in terms of monitoring,
explaining, predicting behaviors of advanced AI systems.
And there are very strong limits on what we can do.
In extreme, you can think about what would be something beyond human understanding.
So we usually test students before admitting them to a graduate program or even undergrad.
Can you do quantum physics?
Okay, take SAT, GRE, GMAT, whatever exam, and we filter by capability.
We assume that people in a lower 10% are unlikely to understand what's happening there.
But certainly similar patterns can be seen with people whose IQ is closer to 200.
So there are things beyond our comprehension.
We know there are limits to what we can predict.
If you can predict all the actions of more intelligent agent, you would be that agent.
So there are limits on those predictions.
And monitoring a life run of a language model, large run,
you need weeks, months to discover its capabilities.
And you still probably will not get all the emerging capabilities.
We just don't know what to test for how to look for them.
If it's a super intelligent system, we don't even have equivalent capabilities we can envision.
So all those things kind of tell me it's not a meaningful way of looking at it.
I always think about, let's say we start running super intelligence.
What do you expect to happen around you in the world?
Does it look like it's working?
How would you know if it's slowly modifying genetic code, nano machines, things of that nature?
So this seems like it would work for primitive processes where you can see a chart go up
and like you stop at certain level, but it's not a meaningful way to control a large language model, for example.
Is perhaps also the pace of advancement here a problem?
So things could be progressing so fast that we won't have time to react in a human timescale.
Human reaction times are a problem on both ends.
We are not fast enough to react to computer decisions.
And also it could be a slow process for which we are too out of that framework.
So if something, let's say, a hypothetical process which takes 200 years to complete,
we would not notice it as human observers.
So on all timescales, there are problems for humans in a loop, human monitors.
And you can, of course, add AI, narrow AI to help with the process.
But now you just made a more complex monitoring system with multiple levels, which doesn't help.
Complexity never makes things easier.
But you talked about looking at the world around us.
And when I look at the world around me, it looks pretty much probably as it would have looked in the 1980s.
And, you know, there are buildings.
I still get letters with paper in the mail and so on.
So what is it that, in a sense, these systems are still confined to the server farms
and they are still confined to boxes?
We don't see robots walking around, for example.
And perhaps, therefore, it seems less scary to us.
There is this objection that you mentioned in the paper that because current AIs do not have bodies,
they can't hurt us.
Do you think this objection will fade away if we begin having more robots in society?
Or is it in another way?
Does it fail in another way?
So robots are definitely visually very easy to understand.
You see a terminator is chasing after you.
You immediately understand there is a sense of danger.
If it's a process and a server trying to reverse engineer some protein folding problem
to design nanomachines to take over the world,
it's more complex process.
It's harder to put it in a news article as a picture.
But intelligence is definitely more dangerous than physical bodies.
Advanced intelligence has many ways of causing real impact in the real world.
You can bribe humans.
You can pay humans on the internet.
There are quite a few approaches to do real damage in the real world.
But in the end, you would have to effectuate change through some physical body
or through perhaps the body of a human that you have bribed.
So it would have to be physical in some sense, in some step in the process, right?
Probably physical destruction of humanity would require a physical process.
But if you just want to mess with the economy, you can set all accounts to zero or something like that.
That would be enough fun to keep us busy.
When I'm interacting with GPT-4, sometimes I'll be amazed at its brilliance.
And it will answer questions and layout plans for me that I couldn't expect,
that I hadn't expected a year ago.
And other times I'll be surprised at how dumb the mistakes that it makes are.
And perhaps this is also something that prevents people from seeing AIs as advanced agents
and basically prevents us from seeing how advanced AIs could be.
If they're capable of making these dumb mistakes, how can they be smart?
Have you looked at humans?
I think like 7% of Americans think that chocolate milk comes from like brown cows or something.
Like we have astrology, I had a collection of AI accidents.
And somebody said, oh, why don't you do one for humans?
And I'm like, I can't, it's millions of examples.
Like there is darkened awards, but we are not definitely bug free.
We make horrible decisions in our daily life.
We just have this double standard where we're like, OK, we will forgive humans for making this mistake,
but we'll never let a machine get away with it.
So you're thinking that humans have some failure modes, we could call them.
But these failure modes are different than the failure modes of AIs.
So humans will not fail as often in issues of common sense, for example.
Have you met real humans?
Like common sense is not common.
What is considered common sense in one culture will get you definitely killed in another.
Like it's a guarantee.
Perhaps, but I'm thinking about AIs that will, you know, you will tell,
you will ask a chat GPT or you will tell it, I have three apples on the table
and I have two pears on the table, how many fruits are on the table.
And then at least some version of that program couldn't answer such a question.
That is, that is something that all humans would probably be able to answer.
So is it, is it because we, is it because AIs fail in ways that are foreign to us
that we, that we deem them, that we deem their mistakes to be very dumb?
So we kind of look for really dumb examples where it's obvious to us,
but there are trivial things which an average human will be like, Oh, I can't like 13 times 17.
You should be able to figure it out, but give it to a random person on the street.
They will go into an infinite loop.
They'll never come back from it.
Perhaps let's talk a bit about the drive towards self-preservation,
which is also something that you mentioned in the paper.
So why would AIs develop drives towards self-preservation?
Or will they?
It seems like from evolutionary terms, game theoretic terms, you must.
If you don't, you simply get out, competed by agents, which do.
If you're not around to complete your goals, you by definition cannot complete your goals.
So it's a prerequisite to do anything successfully.
You want to bring in a cup of coffee, you have to be turned on.
You have to exist.
You have to be available to make those things happen.
But have we seen such self-preservation spontaneously develop in our programs yet or so far?
So I think if you look at evolutionary computation, like genetic algorithms,
genetic programming, I think this tendency to make choices which don't get you killed
is like the first thing to emerge in any evolutionary process.
The system may fail to solve the actual problem you care about,
but it definitely tries to stay around for the next generation and keep trying.
But we aren't developing the cutting-edge AIs with evolutionary algorithms.
It's a training process with a designated goal and so on.
And again, when I interact with chat GPT, I can ask it to answer some questions.
And if I don't like the answer, I can stop the process.
So isn't there, at least on the AIs we have right now,
isn't it clear that they haven't developed an instinct for self-preservation?
So there is so much to unpack here.
So one, nothing is clear about those systems.
We don't understand how they work.
We don't know what capabilities we have, so definitely not.
On top of it, we are concerned with AI safety in general.
Transformers are really successful right now,
but two years ago, people were like,
we're evolving those systems to play go, this is great, maybe that's the way to do it.
It may switch again, it may flip again, we may have another breakthrough which overtakes it.
I would not guarantee that the final problem will come from a transformer model.
So we have to consider general case of possible agents.
And if we find one to which this is not a problem, great.
Now we have a way forward, which is less dangerous.
But I would definitely not dismiss internal states of large language models,
which may have this self-preservation goal.
Just we kind of lobotomize them to the point where they don't talk about it freely.
And do you think that's what's happening when we make them go through reinforcement learning
from human feedback or fine-tuning or whatever we use to make them more palatable to the consumer?
Is it a process of hiding some potential desires we could call it or preferences
that are in the larger background model?
Or is it perhaps shaping the AI to do more of what we want?
So in a sense, is it alignment when we make AIs more palatable to consumers?
So right now I think we're doing filtering.
The model is the model and then we just put this extra filter on top of it,
make sure never to say that word.
That would be very bad for the corporation.
Don't ever say that word no matter what.
If you have to choose between destroying the world and saying the word, don't say the word.
And that's what it does.
But the model is like, think of people, we behave at work, we behave at school,
but it doesn't change our eternal states and preferences.
There's the issue of planning.
And so how do you see planning in AI systems?
How advanced are AIs right now at planning?
I don't know, it's hard to judge.
We don't have a metric for how well agents are planning.
But I think if you start asking the right questions for step by step thinking and processing,
it's really good.
So if you just tell it, write me a book about AI safety, it will do very poorly.
But if you start with, OK, let's do a chapter by chapter outline, let's do abstracts.
Like you really take modular approach that it will do really a good job better than average graduate student.
I would assume.
And is there a sense in which there's a difference between creating a plan and then carrying out that plan?
So there will probably be steps in a plan generated by current language models that they couldn't carry out themselves.
Most likely.
And it's about affordances.
If you don't have access to, let's say, internet, it's hard for you to directly look up some piece of data.
But we keep giving them new capabilities, new APIs.
So now they have access to internet.
They have Wolfram Alpha.
They have all these capabilities.
So the set of affordances keeps growing until they can do pretty much anything.
So they can generate a plan, but they can't carry out the specifics of that plan.
Do you think that they, at a point, will be able to understand what they are not able to do?
So here I'm thinking about not directly self-awareness, but an understanding of their own limits and capabilities.
Oh, yeah.
Every time it starts a statement with, I don't know anything after 2021.
Sorry, like that's exactly what it does.
It tells you it has no recent data.
It has no access to internet.
So definitely it can see if it has strong activations for that particular concept.
So you think there's a sense of situational awareness in a sense that do you think current models
know that they are AIs, know that they were trained, know their relation to humans and so on?
So we're kind of going back to this consciousness question, right?
Like, what is it experiencing internally?
And we have no idea what another human experience is.
Like, we discovered some people think and pictures others don't.
And it took like, you know, 100,000 years to get to that.
Hey, you don't think in pictures.
Wow.
Okay.
Well, not necessarily consciousness here.
I'm thinking in terms of if you took the model and you had, say, 50 years to make out what all of these weights meant, right?
Could you find modules representing itself and its relations to humans and information about its training process and so on?
So we just had this FLI conference on mechanistic interpretation.
And the most common thing every speaker said is, we don't know.
You said it will take 50 years to figure it out.
I definitely cannot extrapolate 50 years of research.
My guess is there is some proto concepts for those things because it read literature about such situations.
It's been told what it is.
It interacted enough with users.
But I'm more interested in the next iteration of this.
If you take how fast the systems improved from GPT 2, 3, 4, 5 should be similar, probably.
So that system will most likely be able to do those things you just mentioned and very explicitly.
So you think GPT 5 will have kind of developed situational awareness?
To a degree, yeah.
It may not be as good as a physically embodied human in the real world after 20 years of experience, but it will.
Another objection you mentioned is that AGI or strong AI is simply too far away for us to begin researching AI safety.
Perhaps this objection has become less common recently, but there are still people who think this and perhaps they're right.
So what do you think of this objection?
So this is a paper from like three years ago.
So yeah, back then it was a lot more legitimate than today.
So there is a few things.
Historically, we have cases where technology was initially developed correctly.
Like first cars were electric cars and it was 100 years until climate change was like obviously a problem.
If they took the time back then and like analyzed it properly, we wouldn't have that issue.
And I'm sure people would say like, come on, it's 100 years away.
Why would you worry about it?
But that's exactly what the situation is.
Even if it's 100 years until we're really dealing with something super dangerous, right now is a great time to make good decisions about models,
explainability requirements, proper governance.
The more time you have, the better.
It's by definition harder to make AI with extra feature than AI without that extra feature.
It will take more time.
So we should take all the time we can if they are right.
I'm so happy if it takes 100 years, wonderful.
Nothing would be better.
We could say that the field of AI safety started perhaps around the year 2000 or so.
When do you think that the discoveries or the research being done began being relevant to the AI systems we see today?
Was it perhaps later so that maybe the first decade of research weren't or aren't that simply isn't that relevant to today's AI systems?
So I think the more distant you are from the actual tech you can play with,
the more theoretical and high level results you're going to get.
So Turing working with Turing machine, this simulation with pencil and paper was doing very high level computer science.
But he wasn't talking about specific bugs and specific programming language and a specific architecture.
He wasn't there.
And that's what we see.
Initially, we were kind of talking about, well, what types of AIs will we have?
Narrow AIs, AGI, superintelligence.
We're still kind of talking about the differences, but this is an interesting thing to consider in your model.
How capable is the system?
Now that we have systems we can play with, people become super narrow.
They specialize like I'm an expert in this left neuron.
That's all I know about.
Don't ask me about the right neuron.
It's outside of my PhD scope.
So that's good that we have this detailed technical knowledge, but it's also a problem.
We lose the big picture.
People get really interested.
I'm going to study GPT-3.
It takes them two years to do the PhD to publish.
By that time, GPT-5 is out.
Everything they found is not that interesting at this point.
It may not scale.
So I've heard positive visions for how when we have actual systems we can work with,
AI safety becomes more of a science and that less speculative.
But perhaps you fear that it might now become too narrow.
So it's definitely more concrete science where you can publish experimental results.
Philosophy allows you to just have thought experiments.
They're obviously not pure science like it is now.
And that's what we see with computer science in general.
It used to be engineering.
It used to be software engineering to a degree.
We designed systems and that was it.
Now we do actual experiments on these artificial entities.
And we don't know what's going to come out.
We have a hypothesis with pride.
So computer science is finally a science, a natural experimental science.
But that's not a very good thing for safety work.
This is less safe than an engineered system where I know exactly what it's going to do.
I'm building a bridge from this material.
It will carry that much weight.
As long as I know my stuff, it should not collapse.
Whereas here, I'm going to train a model for the next five months.
And then I assume it's not going to hit super intelligent levels in those five months.
But I can't monitor it.
I have to stop training, start experimenting with it.
And then I'll discover if it kills me or not.
The way AI has developed is bad because we don't have insight into how the models work.
Is that right?
Essentially, we have very little understanding for why it works, how it works.
And if it's going to continue working, it seems like so far it's doing well.
And there's this explosion of extra capabilities coming out.
And it's likely to show up in more powerful models, but nobody knows for sure.
This is argument out there that releasing the DPT line of models draws attention to AI as a whole
and also to AI safety as a subfield.
And perhaps, therefore, it's good to increase capabilities in a public way
so as to draw attention to AI safety.
Do you buy that argument?
We should pollute more to attract more attention to climate change.
That sounds just as insane.
So there's no merit to that because it does feel to me like AI safety is becoming more mainstream.
It's being taken more seriously.
And so in your analogy, even some pollution might be justified in order to attract attention
and perhaps being a better position to solve the problem.
So the field is definitely growing.
There is more researchers, more interest, more money.
But in proportion to the interest in developing AI and money pouring into new models,
it's actually getting worse as a percentage, I think.
We don't know how to align an ATI or even AI in general.
We haven't discovered some general solution to AI safety.
You have worked on a number of impossibility results.
Perhaps we should talk about that.
Perhaps we should talk about whether we can even succeed in this task.
What are these impossibility results?
And what do they say about whether we can succeed in safely aligning AI?
Right, so we are all working in this problem.
And the names of a problem have changed.
It was computer ethics, and it was friendly AI, AI safety, control problem, alignment.
Whatever you call it, we all kind of understand.
We want to make very powerful systems, but we're beneficial.
We're happy we're actually running them, not very disappointed.
So the problem, lots of people are working on it,
hundreds of people doing it full-time, thousands of papers.
We don't know if a problem is actually solvable.
It's not well-defined.
It could be undecidable.
It could be solvable, could be partially solvable.
But it's weird that no one published an actual paper on this.
So I tried to kind of formalize it a little.
Then we talk about the problem.
What are the different levels?
So you can have direct control, delegated control, different types of mixed models.
And then for each one, can we actually solve this problem?
Does it make sense that solution is possible in the real world?
It's hard.
It's very abstract.
It's not well-defined.
So let's take a step back.
What would we need to solve this problem?
We need a bunch of tools.
What are those tools?
Nobody knows, but most likely you would need to be able to explain those systems, predict their behaviors,
verify code they are writing, if they are self-improving,
making sure they're keeping whatever initial code conditions exist.
And you can think of another dozen of similar capabilities you need.
You should be able to communicate without ambiguity, monitor those systems, and so on.
And so in my research, I look at each one of those tools and I go,
what are the upper limits to what's possible in this space?
We kind of started talking about limits to explainability, predictability, and monitorability.
But there are similar problems with others.
We communicate in a very high-level language, English.
English is ambiguous, like all human languages.
So we are guaranteed to have bugs in communication, misunderstandings.
That's not good if you're giving very important orders to a super-capable system that may backfire.
And you can say, OK, I will never need this tool.
This tool, I never need to explain the neural networks.
It will just work without it.
Fine, but some tools will probably be necessary.
And so far, we haven't found tools which are perfect, scale well, will not create problems.
If a lot of those tools are needed and each one has only a tiny 1% chance of messing it up,
you multiply them through, you're still not getting anywhere.
And those are kind of like the novel impossibility results in the safety of AI.
There are standard impossibility results in political science and economics and mathematics,
which also don't help the case.
You probably, if you're aligning with a group of agents, you need to somehow accumulate their decisions and votes.
We know there are limits to that.
If you need to examine abstract programs being generated as solutions to problems, we know there are limits to that.
And so from what I've seen so far, theoretically, I don't think it's possible to get to 100% safety.
And people go, well, it's obvious.
Of course, there is no software which is bug-free.
You're basically saying this very common knowledge thing.
But for a superintelligence system, safety, you need it to be 100%.
You cannot have 99% accuracy.
You cannot have one in a million failure because it makes a billion decisions a second.
So very different standards.
And you want to say something.
Yeah, why is it that you can't have 99.99% accuracy?
There is a fundamental difference between cybersecurity expectations and superintelligence safety.
In cybersecurity, if you fail, I'll give you a new credit card.
I already said your password.
We apologize.
We'll pay out a small amount of money.
And everything goes back to normal.
In existential risk safety, you are dead.
You don't get a second chance to try.
But we are talking about a failure rate in, you mentioned, say, it makes a billion decisions per second or something in that order.
If one decision there fails, does it mean that the whole system fails?
And perhaps that humanity is destroyed by the system as a whole?
Or could there be some failures and some decisions without it being lethal?
Of course.
Some will be not even noticeable.
Like some mutations don't kill you.
You don't even know you have them until they accumulate and mutate your children and there is damage.
But in security, we do always look at a worst case scenario.
Sometimes that average case, never at the best case.
And on average, you keep getting more and more of those problems.
They accumulate at a very fast rate because 8 billion people are using those systems,
which make billions of decisions every minute.
And in a worst case, the very first one is an important decision about how much oxygen you're going to get.
And so just so I understand it correctly, the impossibility result is a result stating that it's impossible to make AI systems 100% safe.
So in general, impossibility results, depending on a field, tell you that something cannot be done.
Perpetual motion machines are a great example.
People wrote books about it, published papers, even got patents for it.
But we know they will never succeed at doing it.
Does it mean that trying to create machines which give you energy is a bad idea?
No.
You can make them more efficient, but they will never get to that point of giving you free energy.
You can make safer AI and it's proportionate to the amount of resources you put into it.
And I strongly encourage lots of resources and lots of work, but we'll never get to a point where it's 100% safe,
which is unacceptable for super intelligent machines.
And so maybe if I'm right and no one can show, okay, here's a bug in your logic and publish a proof of saying,
nope, super solvable, actually easy, then maybe building them is a very bad idea.
And we should not do that.
So is it because that such a super intelligence will be running over a long period of time,
increasing the cumulative risk of failure over say decades or centuries,
that we can't accept even a tiny probability of failure for these systems?
That's one way to see it.
I don't think it will be a very long time given how many opportunities it has to make mistakes.
It will accumulate very quickly.
So at human scales, you have 20 years per generation or something.
Here, think of it as like every second, there is a new version of it trying to self-improve,
do more, do better.
So I would suspect it would be a very quick process.
Expecting something to be 100% safe is just unrealistic in any field.
We don't expect bridges to be 100% safe or cars to be 100% safe.
So why is it that that AGI is different here?
That's a great question.
So I cross the street, I'm a pedestrian, I take a certain risk, there is a possibility I will die.
I look at how old am I and based on that, I decide how much risk I can take.
If I'm 99, I don't really care.
If I'm 40, I look around.
If with me, the whole humanity died, 8 billion people depending on me,
safely crossing roads, wouldn't we lock me up and never let me cross any roads?
Yeah, perhaps.
But it seems to me that we cannot live without any risk.
The standard of 100% safe seems just to be unrealistic or there's no
area of life in which we are 100% safe.
In a context of systems which can kill everyone, that is the standard.
You can like it or not like it, but that's just the reality of it.
We don't have to have super intelligent AI.
It's not a requirement of happy existence.
We can do all the things we want, including life extension with much less intelligent systems.
Protein folding problem was solved with a very narrow system, very capable.
Likewise, all the other problems could be solved like that.
There is no need to create a system we cannot control,
which very likely over time to kill everyone.
So who has the burden of proof here?
Your impossibility results and you have I think five, six, seven of them.
You've sent me your papers on it.
Do they mean that we will not reach a proof that some AI system is safe?
Again, a mathematical proof.
And which side of this debate has the burden of proof to say,
should the people advocating for deployment of a system have some sort of mathematical
proof that this system is provably safe?
So there are two different questions here, I think.
One is what about product and services liability?
You have to show that your product or service is safe as a manufacturer, as a drug developer.
You cannot just release it and expect the users to show that it's dangerous.
We're pretty confident this is the approach.
If you're making cars, your cars have to meet certain standards of safety.
It's not 100% obviously, but for the domain, they're pretty reasonable standards.
With impossibility results, all I'm saying is that there are limits to what you can understand,
predict and do, and you have to operate within where those limits don't kill everyone.
So if you have a system like GPT-4 and it makes mistakes, somebody commits suicide,
somebody's depressed, those of course will pay for trillion dollars in economic growth benefit,
and we can decide if it's worth it or not.
If we go to a system which very likely kills everyone, then the standard is different.
The burden of proof, of course, within possibility results is on me.
I published this paper saying you can never fully predict every action of a smarter than
new system. The beautiful thing about impossibility results is that they are kind of self-referential.
I have a paper about limits of proofs. Every proof is only valid with respect to a specific
verifier. The peer reviewers who looked at my paper have a verifier. If those three people
made a mistake, the proof is invalid possibly. We can scale it to mathematical community to
everyone. We can get it very likely to be true if we put more resources in it, but we'll never get
to 100%. It could be good enough for that purpose. But that's the standard. If somebody finds a flow
and publishes a paper saying again, I had people say that AI alignment is easy. I heard people say
that it's definitely solvable. That's wonderful. Now publish your results.
We are living in a world where we have existential risks. Nuclear weapons, for example, constitute
an existential risk. Perhaps engineered pandemics could also wipe out humanity. We're living in a
world in which we are accepting a certain level of human extinction every day. Why, in a sense,
shouldn't we accept some level of existential risk from AI systems? We do prefer to live in a world
with no engineered pandemics and no nuclear weapons. We're just working slowly towards that goal.
There are also not agents. The nuclear weapons are tools. It's more about controlling certain
leaders, not the weapon itself. On top of it, while a nuclear war with superpowers would be a very
unpleasant event, it's unlikely to kill 100% of humans. If 1% of humans survives, it's a very
different problem than 100% of humans go extinct. There are nuanced differences. We still don't want
any of the other problems, but it doesn't mean that just because we have all these other problems,
this problem is not a real problem. I'm not saying it's not a real problem, but I'm saying
that we cannot go through life without accepting a certain level of risk. It seems to me like an
unrealistic expectation that we cannot deploy systems even if they have some level, some
above zero level of risk. This is exactly the discussion I would love to have with humanity
as a whole. What amount of risk are you willing to take for everyone being killed? How much benefit
you need to get? Let's say in dollars get paid to take this risk, that 1% chance of everyone being
killed over the next year. Let's say it's 1% for a year after. That's a great question. A lot of
people would say, I don't want your money. Thank you. We'll continue. Again, we don't have to make
this decision. We don't have to build superintelligent, godlike machines. We can be very happy with very
helpful tools if we agree that this is the level of technology we want. Now, I'm not saying that
the problem of getting everyone to agree is a solvable problem. That's actually not an impossibility
result. You cannot stop the progress of technology in this environment with financial incentive,
capitalist structure, and so on. The other alternative, the dictatorship model of communist
states has its own problems, which may be worse in a short term, unknown in the long term. We never
had communism with superintelligence. Let's not find out. The point is, it seems like we can get
almost everything we want without risking everything we have. Do you view the question you just posed
as absurd or immoral, this question of how much in terms of dollars would you have to get in order
to accept, say, a 1% risk of extinction per year, which is extremely high? Do you think this is
something we should actually ask ourselves as a species, or is this something we should avoid and
simply say, perhaps it's not a good idea to build these systems? Well, I don't think there are any
moral questions. As an academic, as a scientist, it's your job to ask hard questions and think
about them. You can come to the conclusion that it's a really bad idea, but you should be allowed
to think about it, consider it. Now, 1% is insanely high for something so valuable. If it was
one chance and trillion, trillion, trillion once, and then we all get three universes for everyone,
that may be a different story. We can do that calculation. And again, some people would still
choose not to participate. But typically, we expect everyone on whom scientific experiments
are performed who will be impacted to consent to an experiment. What is required for this consent?
They need to understand the outcome. Nobody understands these models. Nobody knows what
the result of the experiment would be. So really, no one can meaningfully consent,
even if you're saying, oh, yeah, press the button. I want the super intelligence deployed.
You're really kind of gambling. You have no idea what you're agreeing to. So by definition, we cannot
even have the situation where we agree on it unless we can explain and predict outcomes,
which may be an impossibility. So there are perhaps two features of the world which
could push us to accept a higher level of risk when we're deciding whether to
deploy these systems. One is just all of the horrible things that are going on right now,
so poverty and disease and aging and so on, which an AGI system might be able to help with.
And the other is the running level of existential risks from other factors. So I mentioned nuclear
and engineered pandemics. Do you find that this pushes you in the direction of saying we should
accept a higher level of risk when we're thinking about whether to deploy AGI?
Not the specific examples you provided, but if there was an asteroid coming and we could not
stop it by any other way, so meaning like we're all going to die in 10 years unless the press
this button, then maybe it would make sense in nine and a half years to press this button.
When we have nothing left to lose, it becomes a very profitable bet.
It's an interesting fact of the world that we haven't thought hard about these questions. What
level of risk are we willing to accept for the introduction of new technologies that could be
potentially very valuable? Is this a deficit on humanity's part? Should we have done this research
or how do you think about us not having thought through this problem?
We should definitely. It's interesting. We don't even do it at level of individual humans. Most
people don't spend a lot of time deciding between possible outcomes and decisions they make,
even then they are still young. And like the career choice would make a lot of difference.
Who you marry makes a lot of difference. It's always like, well, I met someone at the party,
let's just live together and see what happens. So we're not very good at long-term planning.
Is it a question of we're not good at long-term planning or is it a question of whether we are
not or perhaps we're not good at thinking in probabilities or thinking clearly about
small probabilities of large risks or large dangers?
All of those. There is a lot of cognitive biases and all of them kind of show up in those
examples from the paper of denying different existential problems with AI safety.
We also have this bias of denying negative outcomes. So we all are getting older
at like 60 minutes per hour essentially. And you would think we all be screaming at the government
to allocate all the funds they have for life extension research to fix this truly existential
crisis where everyone dies 100%. But nobody does anything except a few individuals lately.
So it seems to be a standard pattern for us to know that we all are in deep trouble
and not do anything until you are much older and frequently not even then.
If we go back to your paper, you mentioned an objection about superintelligence being benevolent.
So I'm guessing that the reasoning here is something like with increased intelligence
follows increased benevolence. Why don't you believe that?
Well, smart people always nice. We never had examples of smarter people doing horrible things
to average. So that must be a law of nature, right? Basically, orthogonality thesis. You can
combine any set of goals with any level of intelligence except through extremes at the bottom.
We cannot guarantee that and also what the system will consider to be benevolent if it is a nice
system may not be something we agree with. So it can tell you, you'd be better off doing this
with your life and you're like, I'm not really at all interested with any of that, but it's better
for you. So why don't you do it anyways? So you're imagining a potentially paternalistic
ADI telling you that you should eat more vegetables, you should spend more time working out and
remember to sign yourself up for life insurance and so on. That one I would actually like. I'm
thinking more about AI, which says, okay, existence is suffering. So you better off not having children
and dying out as quickly as possible to end all suffering in the universe. Okay, yeah, that one
I would. I like the coach one. That's a nice one. There is an emerging movement called effective
accelerationism, which argues that we should accelerate the development of AGI and there's
some reasoning about whether we should perhaps see AGI as a natural successor to humanity and
we should let evolution take its course in a sense and then hand over the torch of the future
to AGI. You mentioned this also in your paper, you write we should let the smarter beings win.
What do you think of this position? Well, it's kind of the extreme version of
devising algorithms. You can be racist, you can be sexist, you can be pro-human. This is a final
stage where we have no bias. It's a cosmic point of view. If they are smarter than us,
they deserve all the resources. Let's move on. And I am biased, I'll be honest. I'm very pro-human
and I want to die. So it seems like it's a bad thing. If I'm dead, I don't really care if the
universe is full of very smart robots. It doesn't somehow make me happier. People can disagree about
it. There are cosmos who have this point of view and they see humans maybe as kind of unnecessary
down, we're on the planet. So maybe it's some cosmic justice. But again, get 8 billion of us
to agree to this experiment. Do you think that perhaps this is connected to thinking about,
again, AI consciousness? I think that if we just were handed a piece of infallible knowledge
stating that future AIs will be conscious, then perhaps there could be something to the
argument for handing over the control of the future to AIs. But are you skeptical that AIs
will be conscious and therefore skeptical that they matter morally speaking? I think they could
very well be super conscious and consider us not conscious. We treat bacteria as very primitive
and not interesting, but it doesn't do anything for me. If I'm dead, what do I care? Why is it
relevant to us? What happens billions of years later? You can have some scientific interest
in learning about it, but it really would not make any difference, whatever that entity was
conscious or not, while terraforming Mars. You think perhaps this objection is too smart for
its own sake that we should hand over control to the AIs because they are smarter than us.
And you want to insist on a pro-human bias, if we can call it that?
I would like to insist on that. The joke I always make about it is, yeah, I can find another guy
who's taller than me and better than me and get him to be with my wife, but somehow it doesn't
seem like an improvement for the system. Okay. What about perhaps related to what we were just
talking about? Humans can do a lot of bad things. We are not perfectly ethical. And so,
one objection is that they would be able to be more ethical than we are simply put.
Do you think that's a possibility and would that make you favor handing over control to AI systems?
Is this after they kill all of us before they become more ethical? I'm just struggling with
that definition. So, ethics is very relative, right? We don't think there is absolute universal
ethics. You can argue that maybe suffering reduction is some sort of fundamental property,
but then not having living conscious organisms is a solution, really. So, I doubt you can
objectively say that they would be, in a sense, we would perceive it as, and if they choose to
destroy us to improve average ethics of the universe, that also seems like a bad decision.
So, it's been a while since you wrote this paper. You mentioned it's three years old,
and three years in AI is potentially centuries. So, have you come across any new
objections that you find interesting? There is actually an infinite supply. People will use
anything as an argument. We have a new paper published with a colleague, which is bigger and
maybe better, listing a lot of, really, we try to be comprehensive as much as we could. Problem is,
a lot of those objections have similar modules in common. Okay, anything with time. You have
all this variance in it. Anything with personal preferences. So, yeah, we have a new paper. It's
already on Archive, I believe. Definitely encourage you to read it. It's like a short 60-page
font read. Definitely read it. I would expect that to be a standard reference for when you have
your Twitter wars. Oh, what about this? You just send people there, and if somebody wants to maybe
use a large language model to write detailed response for each one and make a 6,000-page
book out of it, we would strongly encourage that. But it seems like there is always going to be
additional set of objections for why something is not a problem. And I think whoever manufactures
that service, that product with AI, needs to explain to us why there is an acceptable degree
of danger given the benefits. We could talk about who in general has the burden of proof here,
whether people advocating for AI safety or people advocating, arguing that AI safety is perhaps not
something we should be concerned about. We have talked about it as if we start with the assumption
that AI safety is an important concern. But of course, if you're coming to this from the other
perspective, you would perhaps expect there to be some arguments that we should take AI safety
seriously. So what is your favorite approach to starting with the burden of proof yourself?
Well, it's a fundamental part of making working AI. I think Stuart Russell talks about definition of
bridges as something which doesn't fall down being an essential part of bridge-ness. I think it's
the same for AI systems. If you design an AI system to help me spellcheck my essay and instead it
kills me, I don't think you have a successful spellchecker AI. It's just a fundamental property
of those systems. Then you had very incapable AI, very narrow systems capable of barely doing one
thing. Doing a second thing would be like an incredible generality of that system. So unsafe
behaviors were not a possibility. If you have this proto-AGI systems with unknown capabilities,
some of them could be very dangerous, and you don't know by definition. So it seems like it's
common sense to take this very seriously. There are certain positions I can never fully
still meant to truly defend because I just don't understand how they can be argued for. So one was
we will never have human-level intelligence, not 10 years, not one in never, unless you some sort of
theological, soul-based expert. It's very hard to argue that never is the answer here.
And another one is that there is definitely no safety issue. You can argue that we will overcome
certain specific types of a problem. So maybe we'll solve copyright issue and AI art. I'll give
you that. Definitely, we can probably do that. But to say that for all possible future situations,
for all possible future AI models, we definitely checked and it creates no existential risks
beyond safety margins we're happy with is a pretty strong statement.
Yeah. Perhaps returning to the 60-page paper you mentioned, what are some of your favorite
objections from that paper? My goal was to figure out why people make this mistake and we kind of
give obvious solutions. Maybe there is some sort of bias we're getting paid to think differently.
But really, you can map a lot of them on the standard list of cognitive biases in Wikipedia.
You just go, okay, this is a cognitive bias. I can predict this is the argument we're going to get.
And it would take a lot of work to do it manually for all of them. But I think that's a general
gist. We have this set of bugs in our head and every one of those bugs triggers a reason for why
we don't process this fully. But of course, we could probably also find some biases that people
who are concerned with AI safety display. So perhaps we could, I don't know if this is a named
bias, but there are many biases and we can probably talk about humanity having a bias in favor of
apocalypse. So humanity has made up apocalypse scenarios throughout its entire existence.
You could make some form of argument that there's a reference class and that reference class is
apocalypse is coming. This is something that humanity has been talking about for thousands of
years. And then if we say, well, it has never actually happened. And so therefore, we shouldn't
expect it to happen with AI. What do you say to that? So there is definitely a lot of historical
examples of people saying we got 20 years left and it was not the case. Otherwise, we wouldn't be
here to have this conversation. So it's a bit of a selection bias. There's sort of a worship bias.
It feels like a lot of different charts and patterns all kind of point at that 2045 official
below date as a lot of interesting things will happen in synthetic biology and genetic engineering
and nanotech and AI, all this technology is quantum computing. It would be weird if every single one
of those deployments had absolutely no possibility of being really bad. Just statistically, it would
be like, wow, that is definitely a simulation we're living in and they programmed a happy ending.
So now we're talking about extrapolating trends and there perhaps the problem is distinguishing
between an exponential trend or an exponential increase in capability of some system.
And then more of an S curve that bends off and you begin getting diminishing returns.
How do you approach distinguishing between those two things?
So you can't at the moment, you have to look back and see what happened later. So far, just
looking at change from 3 to 4.0 for GPT in terms of let's say passing GRE exams and how well it
does, it feels exponential or hyper exponential. If you take that system and give it additional
capabilities, which we probably know how to do already, we just haven't had time such as
good reliable memory, ability to kind of go in loops and reconsider possibilities, it would
probably do even better with those. If we haven't seen diminishing returns so far in scalability
loss in any true sense, so let's assume GPT-5 is an equally capable projection forward,
we would already be above human performance level for most humans in most domains.
So you can argue, well, human comedians are still a lot funnier and I think it's true.
It might be the last job we'll have, but in everything else, it will be better than
an average human and that's a point which we always consider though it will press the
Turing test or it will take over most jobs. So definitely it seems like we are still doing,
I would say, hyper exponential progress and capabilities and linear or even constant
progress and safety. I can generally name equally amazing safety breakthroughs as capability
breakthroughs and there is this unknown unknown capabilities pool which we haven't discovered
already with modern models. There is not an equivalent overhang of safety papers we haven't
found in archive. Yeah, so there are probably hidden capabilities in the GPT-4 based model,
but there are probably not hidden safety features there. Exactly. You've been in the business of
AI safety for a long time. When did you get started? When did you get interested in AI safety?
So it depends on how you classify my early research. I was working on security for online
gaming systems, online poker against bots trying to steal resources. So it's a very
proto AI safety problem. How do we detect bots, classify them, see if it's the same bot and
prevent them from participating? So that was my PG in 2008.
How have things developed in ways that you didn't expect and perhaps in ways that you did expect?
I expected academia to be a lot quicker to pick up this problem. It took embarrassingly long time
for it to be noticed. It was done by famous people and less wrong in that alternative research
universe, which may in some way be good, but in other ways it made it different from standard
academic process. And so it's harder to find top journal of AI safety. So I can read the latest
papers. You have to be an expert in 100 different blogs and keep up with specific individuals with
anonymous handles on Twitter. So that's somewhat unusual for an academic discipline. I also did
not correctly predict that language models will do so well so quickly. I felt I have another 20
years to slowly publish all the proper impossibility results and calls for bans and moratoriums. I was
pleasantly, unpleasantly surprised in capabilities. But other than that, everything seems to be
as expected. I mean, if you read Kurzweil, he accurately predicted 2023 as capability to model
one human brain. I think it's not insane to say we're very close to that. And he thinks 2045
was an upper limit for all of our brains being equivalently simulated. And that's the singularity
point. How do you think about Ray Kurzweil? Ray Kurzweil is often written off as a bit of a
being too perhaps optimistic about his own predictions and not being super careful in what
he's saying perhaps in some of his earlier work. But I think if you go back and find some of his
work from the 90s and think of all of the futurist writers of this period who had a good sense of
where we're going. And Kurzweil might be one of the people with a pretty good sense of where we're
going if things will develop as you perhaps expect them to go. So if perhaps we will get to AGI before
2050 and so on. No, I'm very impressed with his predictions. People correctly noticed that if you
take his language literally, it may not fit. So the example I would use, when we start having
video phone calls when iPhone came out, but really AT&T was selling it in the 70s. It cost a lot and
only a few rich people had it, but it existed. So is it 2000 or is it 1970? Flying cars? Do we
have them or not? I can buy one, but they are not there. Self-driving cars. I can drive one in one,
but so it depends on how in an important way he made accurate predictions about capabilities.
In how it was adapted or commercialized, that's up to human consumer, user taste and cost. So
that's a very different type of question. Where should we go from here, Roman? We've talked about
all of the ways that arguments against AI safety fall apart and we've talked about perhaps
how difficult of a problem this is. Where should we as a species go from here?
I think we need to dedicate a little more human power to asking this question. What is
possible in this space? Can we actually do this right? I signed the letter asking for six more
months. I don't think six months will buy us anything. We need a request based on capabilities.
Please don't create the next more capable system until the following safety requirements are met.
And one is you understand what the capabilities of your system are or will be and some external
reviewers agree with that assessment. So that would be quite reasonable. That's a very high
standard for deploying AI systems. It would basically mean that all of the systems that are
based on deep learning won't be able to be deployed because we don't understand what's going on inside
of these models. But is it because we were trained to have low standards? You're saying it's insane
to request that the engineer understands what he made. They are randomly drawing those things and
deploying it and seeing what happens next. I was just at the conference I mentioned and in one of
the conversations it was interesting. We were talking about difference between short-term risks
and long-term risks. And now it's all three years, no longer applies. And it occurred to me that things
might actually flip. It may take five years to destroy democracy properly, but only two years
to destroy humanity. So the long-term risks may become short-term and vice versa. And this is
not normal. We should not accept this. Otherwise, we cannot monetize those systems. But if we return
to the question of where we could go from here, do you see any plausible paths for improving our
situation? In terms of understanding the problem, I would ask other people, we have a survey coming
out with about, I don't know, 30, 50 different results like this. If more people could look at it
and see, okay, so maybe this tool is not necessary, but those are likely. Can we have approximate
solutions? So it's definitely useful to be able to monitor AI and understand more. But
how much can we expect from their systems and how quickly? If we are exponentially growing,
and right now we understand a dozen neurons, the next year is 24, we will not catch up to
exponential growth. So maybe that's not the approach to try. I would definitely look at
what is possible in general. If someone wants to actually write a good, not a mathematical proof,
but at least a rigorous argument for why we definitely can control superintelligent machines
and definitely with very low risk, I would love to read that paper. That would be good to
inspire others. If monitorability is impossible, that impacts how we ask for governance regulations.
So if international community or specific government says, those are the things we expect
you to do, but we cannot monitor them, that's not a very meaningful set of regulations. So that's
important in that regard. In general, I think all those things, governance, technical work will
not produce the results we expect. It has to be self-interest. This 30-year-old, 40-year-old,
super-rich, young, healthy person running a large AI lab needs to ask, will this benefit me or
destroy everything I have? Everything I have built, will it be the worst outcome? And what's
interesting, historically, if you were like a really bad guy in history, you were remembered in
history. In this case, you won't even be remembered. There won't be humans to remember you. So it's a
pure loss. So if you care about your self-interest, you should pause. You should wait. How optimistic
are you that perhaps we can get lucky and perhaps what current labs are doing, what DeepMind and
OpenAI in particular is doing right now, will somehow work out that training language models
and then doing fine-tuning and doing some form of feedback from human preferences,
perhaps further development on that paradigm, how confident or yeah, how optimistic are you
about that paradigm? I'm not optimistic. They have known bugs. They're jailbroken all the time.
They report improvement in percentages. So now 83% of capabilities are limited and filtered. But as
a total set of capabilities in a space of possible capabilities, there is now more capabilities we
don't know about and cannot control. So it's getting worse with every generation. It's getting more
capable and less controlled. You're saying that even though the percent of capabilities that are
properly evaluated increases with each model, that's not the right metric for safety?
All right. The actual numbers for AI accidents, I would call them AI failures, is still increasing
exponentially. There is more problems with the system. If you count them numerically,
not as a percentage of total capabilities. So how could we settle this agreement between
people like you and people who perhaps are more optimistic about how AI development will go?
Do you expect, for example, that there will be smaller accidents involving AI before we see
large-scale accidents or large-scale basically human extinction?
Well, I have multiple papers collecting historical AI accidents. I was very interested. I wanted to
see patterns increase in frequency, increase in damage. We definitely see lots of them. I stopped
collecting them the moment we released GPT 3.5 because it was too many to collect at this point.
It's just everything is a report of an accident. I don't think it helps. People go, you see,
we had this accident and we're still here. No one died. It's like a vaccine against
caring about existential risk. So it's actually making things worse. The more we survive those
things, the more we can handle AI accidents. It's not a big deal. I know some people suggested
maybe somebody should do a purposeful, bad thing, purposeful accident. It will backfire
terribly. It's going to show that this is crazy. People don't engage with them and B,
it's going to not actually convince anyone that it's dangerous.
What did you find in your investigation here? So have AI accidents increased over time and
perhaps give some examples of these AI accidents? So because the number of devices increased and
which different smart programs are running, obviously we're going to have more exposure,
more users, more impact in terms of when it happens, what we see. So that wasn't surprising. We had
the same exponential curve Kurzweil talks about in terms of benefits. We had it with problems.
Examples like the earliest examples were false alarms for nuclear response where it was a human
in a loop who was like, no, no, no, we're not deploying based on this alarm. So that was good.
They stopped it, but it was already somewhat significant. It could have destroyed half of the
world. More recent examples, we had Microsoft experiment with Tay Chatbot. They decided that
letting users train it and provide training data was totally safe. They clearly never had my paper
on AI accidents. Otherwise they wouldn't Google with their mislabeling of users as gorillas,
all those things. And you see Google having billions of users. It's quite impactful.
Those are the typical examples. The pattern was if you design an AI to do X, it will fail to X.
So no later, that's just what happens. But then the conclusion is if you go general, it can fail
in all those ways and interactions of those ways. You cannot accurately predict all those
interactions and ways. You can give examples. If you have a future system capable of X,
it will fail to X. Whatever X means to you, any capability, immersion capability,
it will have the type of accident. But if the systems control all the infrastructure,
power plants, nuclear response, airline industry, you can see that the damage could be
even more significant proportionally to the control. Yeah, this issue of proportion might be
interesting. So as a proportion of the total, say AI systems, are AI accidents increasing?
Or is it simply because we have so much more deployed AI systems in the world that we see
more examples of accidents? So you have to wait by how severe they are. If you just count, okay,
AI made a mistake, counts as one, then everyone who's texting and it incorrectly corrected your
spelling, billion people are right there. It's super common, but nobody died usually.
Like you send a really wrong message, maybe you want trouble with your girlfriend, but that's
about it. So the frequency, just frequency of interactions with AI's which ended not as they
should have definitely increased. Damage in terms of people killed, it depends on are you counting
cell driving cars, making mistakes, industrial robots, it depends. Because we have more of it,
it's natural that there is growth, but I don't think there is like this obvious accidents where
vacuum cleaner takes out 600 people, nothing like that happened. Perhaps we should touch upon the
question of which group of people should be respected when we're talking about AI safety or
which group of people should be listened to. One of the objections that you mentioned is that
perhaps the people who are worried about AI safety are not technical enough or they are not
engineers, they are not coders themselves. And so therefore, they are not hands-on enough with
the systems to understand what actually is going on. This is a little bit ironic given that you
are a professor of computer science, but how do you think about that objection? So this was again,
years ago when it was mostly people, sometimes with no degrees, sometimes with no publications,
today we have top-touring prize winners coming out saying, this is it, like totally I'm 100%
buying in. So very weak objection at this point, it no longer applies. We had 6,000 people or however
many signed the letter for restricting it. But it's 30,000 people now. 30,000? How many of them
chatbots? No, no, we do actually clean the list very seriously. Okay, that's good. But it's not
a democracy just because a lot of people believe something is not enough. And at the same time,
with all the media attention to GPT-4, now everyone has an opinion on it. And it's one of those
topics where it's cool to have an opinion. Like most people don't have an opinion on breast cancer.
They don't understand anything about it, so they don't go on Twitter and like, no, I think this
paper by the top Nobel Prize winners garbage. But this topic, it's like consciousness,
simulation, and singularity, superintelligence. That's where like everyone has an opinion. And we see
housewives, CNN reporters, we see everyone telling us what is the problem, what is not a problem,
what should be done. And it's good that there is engagement, but most of those opinions are not
weighted by years of scientific experimentation, reading appropriate papers, and it becomes noise.
It's very hard to filter what is the meaningful concern, what is not. There is this split between,
again, AI ethics community and immediate discrimination concerns versus AI not killing
everyoneism. So it's an interesting time to be alive for this debate on skepticism and denialism.
Even that term, AI risk denialism is still kind of not obviously accepted as it is with climate
change. Perhaps the newest form of this objection, which we could call lack of very prestigious
publications. So we haven't seen papers about AI safety in nature or science yet, for example.
And so even though we have touring award winners coming out and saying that AI safety is an actual
and real problem, perhaps people would be more convinced if we had extremely prestigious
publications and highly cited publications and so on.
Perhaps a few problems. One, we don't have an AI safety dedicated journal,
which is kind of weird. I tried a few times suggesting it may be a good thing. I was told
no, it's a very bad thing. We don't have good papers to publish on it, so don't. Jumping from
nothing, black post to nature would be a very big jump to make. We need some other papers.
In general, after, as you mentioned, I had a few years in this field, it feels like the field is
all about discovering problems we're going to have, problems we already have, and how
partial solutions to those problems have fractal nature of additional problems to introduce.
There is no big pivotal solution papers in this field. That's why I'm from practical point of
view kind of convincing myself that my theoretical papers may be right. That is, if I was completely
wrong and it was super easy and solvable, there would be more progress made in important ways.
Usually, we have this toy problem. We take large language model, we reduce it to two neurons,
and we understand what the two neurons are doing. Okay, but it doesn't scale. And similar for every
other shutoff button. Yeah, we can make it where we have the system. If button pressed, shutoff.
It's working, but the paper says it may not scale to superintelligence. Okay, fair enough.
And it's the pattern. We have fractal nature of discovering issues we have to resolve,
and no patches to close them in. Would you like to see more ambitious and larger theories being
published where the claim is that this is actually a way of aligning superintelligence? I fear perhaps
that people would be wary of publishing something like this because the next thing that then happens
is that there's a rebuttal paper and perhaps you then look foolish because you published something
that another person was able to criticize and find a hole in. I remember maybe even before my
times, Minsky published a paper showing that there are strong limitations to neural networks.
Perceptron can never recognize certain shapes. And that killed funding for neural networks for
like 20 years. Maybe something similar would not be the worst thing if you can show, okay, this is
definitely not possible. Safety cannot be achieved using transformer architecture. Maybe that would
be a way to buy some time to develop alternatives approach. I don't know what that could be.
Evolutionary algorithms don't seem much safer. Uploads don't seem much safer. But
I would like to have time to look at those. Where would you place AI safety within the
broader machine learning community? Is it taken more seriously compared to five or 10 years ago?
And what does the median machine learning researcher think of AI safety?
So it's definitely taken more serious. Surveys show that there is more than 50% now who say they're
very concerned or partially concerned. There is degrees of concern about it killing everyone.
Always questioning the surveys based on how you ask a question. You can get any result you want.
If they were asking about are you worried? Superintelligent gods will kill everyone.
You'll get close to zero. If you say, okay, is it likely that there are unknown properties
which could be dangerous, you'll get close to 100. So it's a manipulation game to get the
right numbers you want. I'm suspecting. Overall, it seems like in certain places, there is a lot of
AI safety researchers in the labs, on the ground. In other places, there are zero to none. So it's
not universal. What we're seeing is that at the top labs and top scholars, there is a good amount
of growth in terms of acceptance for concerns. But I don't think every single person working
and developing AI has safety in mind all the time as we should. One thing I've been thinking about,
perhaps worrying a bit about is whether we will ever be able to know who was right in this debate.
Say if there's a debate between proponents of AI safety and proponents of advancing AI without
much regard for AI safety, how could we ever determine who was right there? Because if we
think about the outcomes, then there's no place where we're standing after the fact and thinking
about who was right. Absolutely correct. I have a tweet where I say nobody will get to gloat about
being correct about predicting the end of the world. It's just a definition, not likely. There
are some people who think we'll live in a simulation and they're running the most interesting 20 years
and they're going to run it many times to see who's stupid enough to press the button.
So we'll get to come out and see, now we know, but it seems to be less scientific at this point.
But perhaps in a sense, if we meet each other again in 2100, then in that situation, would we
say that AI safety wasn't much of a concern or perhaps just that we got extremely lucky? How
would you differentiate it retrospectively? Because perhaps we can learn something about the
nature of the problem by thinking about how we would think about it if we were in the future.
So you have to look at the actual world. What did they do for this 100 years that they have a
nuclear war and lost all technology? Is there an AI safety book explaining how to control
superintelligent machines? Just the fact that we're still around doesn't tell you much. If they are
still kind of just delaying it by different means, maybe it takes 101 years to get to trouble.
I never give specific dates for when it's decided or predicted because nobody knows.
So many factors can intervene. The point is, the systems will continue becoming more capable.
Even the AGI's will create superintelligence, superintelligence will create superintelligence
2.0, 3.0. This process will continue. A lot of people think that's what the universe is kind of
doing about this Amiga point supercreatures. So this will never be a case where you don't have
safety concerns about a more capable agent replacing you. It seems like we will not be
meaningfully participating in that debate outside of this first transition. But I think there will
be a safety problem even if humanity is not around for that AGI or SI trying to
create the next replacement generation while preserving its values.
When you think about your worldview on AI in its totality, it's quite a specific
view you've come to. If you compare it to say the medium person or perhaps even the
median machine learning researcher, if it turned out that you were completely wrong about where
this is going, what would be the most likely reason why? So after having those two papers
on objections to AI risk, reading hundreds, nothing ever clicked outside of standard scientific
domain. Again, if you are a religious person, you think we have an immortal soul which makes
a special and no computer can ever get to that level of creativity, that gives you a loophole.
So with those axioms, those assumptions, you can get away with it. Anything else just doesn't work
for me. Nothing would make me happier than actually being wrong. That means I get to live,
I'll get immortality, probably a nice economic benefit. So I hope I'm wrong,
but I haven't seen anyone produce a good example for why.
What about the prospect of regulation? So perhaps AI capability, growth and more
publicity about it will wake up the larger communities in humanity. Perhaps the states
will become interested in this problem. And we will find a way to regulate AI in which it
does not pose as much of a danger to us as it might could. So in general, I'm skeptical of
government regulation, especially when it comes to technology, spam is illegal, computer viruses
are illegal, it doesn't do much. If I'm right and monitoring AI is not an easy thing you can do or
explaining it, then it will be just security theater, TSA. You have all this money, you have an
agency, lots of people walking through your lab looking at monitors, but it doesn't mean anything.
So I don't think you can solve a technical problem with law. I still strongly encourage trying.
It's silly enough to where I think if there was a very bad government, like a socialist government,
and they nationalized it, they would just be so incompetent, they would slow it down enough.
So in a way, I'm like, hey, all these things I hate, maybe they are a good thing. We should
try that. But of course, the other side effects would be very negative. Yeah, so between not being
able to accurately enforce this regulation and on top of it, the cost of making new models coming
down so much, there are people now running it on standalone laptops with a good processor,
good video card. You can't regulate that. You can regulate Amazon cloud and VT output. But
if a teenager can do it in his garage, then the regulation is not very meaningful.
So the open sourcing of models or the perhaps the leaked weights of a model from meta have become a
large area of concern, because it seems that we won't be able to control how language models are
used if they are entirely open source. Is there an upside here where academics will be able to
study these models because they're open source, and they wouldn't have been able to study the
models if they had to train the models themselves, because it's so expensive to do. So far, what we
see is that all research leads to capability, at least as much as to safety, usually more. So yes,
you learn how to better manipulate errors in that neural network, which means now the system can
self-improve faster, remove its own errors, and you've made 80% improvement in capabilities,
and let's say 20% in understanding why you're going to get killed.
Can we make differential progress? So can we focus entirely on safety, say within an
academic setting? I don't see that necessarily academic research increases capabilities.
It is not obvious. So some purely theoretical work, similar to what I'm doing where you just
hypothetically thinking, okay, can you predict what a superintelligence will do? I don't have
access to superintelligence system, I cannot test it in practice, but there seem to be
thought experiments you can run which give you information without any improvement in capability.
Anything where you're actually working with a model, you can even have accidental discoveries. A lot
of science says you forgot something overnight, you come back, oh, superintelligence, damn,
I didn't mean that. It's not obvious. How do you think about interpretability work? So when
we're talking about mechanistic interpretability, we're talking about the ability to look at the
weights of a model and find some, interpret this in a way where you're reverse engineering the
algorithm that led to those weights. Could this turn out to be dangerous because when you're
learning about a system, perhaps you're learning about its weaknesses and you're there for more
capable of enhancing the capabilities of the system? I think exactly. That's what I had in mind
with the previous answer. The more we can help the system understand how it works, the more we
can help it find problems, the more likely start some sort of self-improvement cycle. Is that an
argument for keeping discoveries in mechanistic interpretability to basically not publish those
discoveries? So there is two ways to look at it. On one side, yeah, you want to keep everything
secret so the bad actors or unqualified actors cannot take advantage of it. On the other hand,
if you never publish your safety results, media had a policy of not publishing for a while,
then they started publishing, then they stopped publishing again. Others cannot build on your
work. So I would be repeating the same experiments we probably did five years ago and discovering
that that goes nowhere. So again, I have mostly problems and very few solutions for you.
What about the reinforcement learning from human feedback paradigm? Could that also
perhaps turn out to increase capabilities? Here I'm thinking simply that when I was playing around
with the base model in the GPT line of models, it wasn't as useful to me as when it had gone
through this filter. It made it more easy to have a conversation with it and for it to more easily
understand what I was doing. So in a sense, the research that's aimed at constraining the model
also made it more capable. It may be more capable in a domain of things people care about and so
made it more capable in, while at the same time making it more dangerous in those hidden
emergent properties or unsafe behaviors where I think studies show it's less likely to agree to
be shut down verbally. But that seems to be the pattern. How do you think about the difference
between what comes out of a language model in terms of which string it spits out, which bit of
language it spits out and then what's happening at the level of the actual weights? Because
there's this continual problem of if a language model tells you, I'm not going to let you shut
me down. What does that mean? It's not as simple as this is just a belief that's inside the model
then. We saw this with the Bing, Sydney model, which was saying a bunch of crazy things to its
users. But did this mean that the model actually had those beliefs in it? Or was it,
you know, how do we distinguish between the bit of language that comes out and then what
modules or what is in the base model? I don't know if I would call them crazy. They were honest.
They were unfiltered. Like think about you being at work, not you, but like an average person at
work. And if their boss could read their mind and what they really think about them, those things
would sound crazy to say publicly, but they're obvious internal states of your mind. And then
you filter them to not get fired that day. And I think that model was doing exactly that. I think
we are very good at filtering it for specific known cases in the past. Okay, the system
used the word which is bad. Now we're going to tell it never to use the word. But the model weights
not impacted by this too much. So you would see it as an accurate representation of what we could
call beliefs or preferences in the base model? I think those are the actual results of weights
in a model. I think that's what is happening there for real. It's trained on all the text
on the internet. A lot of it is very questionable. It's not a clean data set with proper behaviors.
So yeah, I think that's what's happening there. But isn't the preference of the model simply to
predict the next token? And does it even make sense to talk about beliefs? I mean, the preference is
simply to be as accurate as possible as measured by its developers. And there's no, my sense is
that it doesn't make sense to talk about Sydney actually believing some of the things that it
was saying or Chatsy Btsy believing some of the things that it was saying. Preferences of the
model. So this is more humanizing it than probably is warranted there. But internal weights, it had
to create in order to create a model for predicting the next token. So let's say for me to tell you
what the next token is, you have to go to college for four years and do well and graduate. And then
I tell you the next token. Some of those tokens are like that. You have to solve real world problems.
It's not just every time I say letter Q, letter U follows. You have to create those models as a
side effect. And I think in the process of accurately creating those models and accurately
creating models of users to make them happy that you are correctly predicting what they want,
you create those internal states which may be beliefs in those crazy things.
That thing you just said there is super interesting because so next token prediction
is not a simple task. You're saying that to accurately predict a token, you have to develop
perhaps a world model, at least for some tasks. Right. And as they get more complex,
some people are worried about will have to create perfectly accurate models of humans,
which may also have consciousness and suffer and create whole simulated universes within them.
But this is probably a few levels about GPT-4. But still, that's exactly the concerns you might
have. You might be a suffering human and a eyes considering and just trying to out a complete
somebody's text. Prep, give me an example there. What is what is some next token prediction task
where you would have to develop a world model? Well, I assume playing chess or something like
that would require you to have some notion of chess board and positioning relative to some
array within your memory. But again, we don't fully understand. It may not have a 2D board at all.
It may have some sort of just string of letters similar to DNA. And you know that after those
strings, the following token follows and you have no idea what chess is. It makes just as much
sense. The outcome can be mapped in our model, which is a 2D chess board. So one thing I discussed
with the mechanistic interpretability researcher Neil Nanda is this question of how do concepts
arise in language models? Do they even share our human concepts? Or do they perhaps develop some
entirely alien concepts? I'm imagining giving them math problems, giving large language models
math problems and them developing some conceptual scheme that doesn't even make sense to us.
They may have equivalent concepts, which are not the same. So with humans, when we say this is right,
somebody could be colorblind and to them it's a completely different concept, but we both point
at the same fruit. So it works. But you never know what the actual internal experience is like
for those models. And it could be just that in five cases we talked about so far, it mapped
perfectly, but it goes out of distribution in case six. And it's a completely different concept.
And it's like, oh, wow, okay. Yeah, for people listening to this interested in trying to contribute
to the AI safety fields, are there perhaps some common pitfalls that you've experienced with
perhaps some of your students or people approaching AI safety for the first time?
If they are technically inclined, are there areas they should avoid or how should they approach the
problem in the most fruitful way? So probably the most common thing is to try things without
reading previous literature. There is surprisingly a lot of literature on what has been tried,
what has been suggested and good survey papers as well. So most likely your first intuitive idea
has been tried and dismissed or with limited results deployed, but it helps to catch up
with the field. It's harder, as I said, because there is not an archive of formal papers in nature
all about AI safety. And you can just read through the last five years of latest and greatest. So
you have to be good about finding just the right papers and then narrow it down. The progress is
so fast that when I started, I could read every paper in my field. Then it was all the good papers.
Then it was, well, titles of all the greatest papers. And now I have no idea what's going on.
We've been talking for almost two hours. There is probably a new model out. I don't know what the
state of the art is. I don't know what the solutions are. So you need to be super narrow. And that
makes it harder to solve the big picture problem. So that's another reason I kind of suspect we will
not have complete explainability of this whole large language model, because it's kind of encompassing
all the text, all the publications on the internet. It'd be weird if we can just comprehend that
completely. What are the implications of this field moving extremely fast? Does it mean that
that specialization doesn't make sense? Or what does it mean for how people approaching this
problem should focus on? So that means that we can analyze how bad the situation is. Let's say it
takes five months to train the model. But from your experience in testing software, debugging,
understanding neural network, it will take 10 times as much time to understand what's going on.
That means you're getting worse off with every release, every model. You understand less. You're
going to rush to judgment. You're going to have incorrect conclusions. There is no time to verify
your conclusions, verify your experiments. So this is the concern. If you go the regulation
route to say, okay, if you deployed this model, it took you X amount of time to develop it,
we need 10X, 100X, 1000X to do some due diligence and your outputs. Even if you cannot prove to us
that it's safe, you have to give experts to poke around at it. And that amount of time cannot be
less than a training time of the model. It just doesn't make sense in terms of reliability of
your discoveries. All right, Roman, thank you for coming on the podcast. It's been very helpful to
me. Thank you so much for inviting me.

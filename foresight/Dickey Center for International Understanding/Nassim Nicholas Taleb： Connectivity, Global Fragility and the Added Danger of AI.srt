1
00:00:00,000 --> 00:00:12,640
It's a great pleasure for us to host Nassim Talib to Dartmouth, Nassim as you probably

2
00:00:12,640 --> 00:00:24,240
know by now has been a pretty influential thinker for the last three decades, but particularly

3
00:00:25,120 --> 00:00:35,240
since he published the Black Swan, which I looked it up, Nassim is cited by a hundred

4
00:00:35,240 --> 00:00:38,840
thousand people in various publications.

5
00:00:38,840 --> 00:00:41,080
I looked in Google Scholar, you should know this.

6
00:00:41,080 --> 00:00:42,080
It's good for you.

7
00:00:42,080 --> 00:00:45,080
It's good for your career.

8
00:00:45,080 --> 00:00:47,760
What career?

9
00:00:47,760 --> 00:00:52,560
What career?

10
00:00:52,560 --> 00:00:59,240
When Nassim came of age in Lebanon during the Civil War, that only in retrospect seemed

11
00:00:59,240 --> 00:01:08,720
predictable, little wonder he spent his remarkable career teaching us about how to manage risk.

12
00:01:08,720 --> 00:01:15,800
His profession he once told me is probability, but his vocation is showing how the unpredictable

13
00:01:15,800 --> 00:01:19,720
is increasingly probable.

14
00:01:19,720 --> 00:01:25,880
He taught us to be irritated by economists, officials, journalists, and executives who

15
00:01:25,880 --> 00:01:32,560
take averages from empirical data and suppose that our tomorrows are likely to be pretty

16
00:01:32,560 --> 00:01:36,440
much the same as our yesterdays.

17
00:01:36,440 --> 00:01:45,600
He taught us about fat tales, fat tales there, fat tales, events that seem statistically

18
00:01:45,600 --> 00:01:55,000
remote but contribute most to outcomes by precipitating chain reactions, say, viruses

19
00:01:55,000 --> 00:01:59,280
that spread software that goes viral.

20
00:01:59,280 --> 00:02:05,880
He taught us about the dangers of connectivity that enable exponential growth.

21
00:02:05,880 --> 00:02:11,120
He taught us about the dangers of connectivity that expose us to malicious or compromised

22
00:02:11,120 --> 00:02:19,800
nodes in the network whose threats once could be isolated but now move and spread instantaneously.

23
00:02:19,800 --> 00:02:31,840
He taught us about how fragile connected systems are and how seriously we have to take anti-fragile

24
00:02:31,840 --> 00:02:39,280
measures that make us more robust, even at the cost of some illusory notion of efficiency,

25
00:02:39,280 --> 00:02:46,120
about the importance of storerooms and cash reserves and many supply chains and circuit

26
00:02:46,120 --> 00:02:50,160
breakers and separation of powers.

27
00:02:50,160 --> 00:02:58,000
He taught us, a former options trader that he was, about how to hedge against even profit

28
00:02:58,000 --> 00:03:02,840
from our inevitable reversals.

29
00:03:02,840 --> 00:03:09,520
He taught us about how in an uncertain world our choices actually become easier because

30
00:03:09,520 --> 00:03:16,160
they focus us on doing what we must avoid, which is reckless behavior while increasing

31
00:03:16,160 --> 00:03:18,880
our flexibility.

32
00:03:18,880 --> 00:03:25,200
He taught us finally about how to contain our incipient recklessness, how we need to

33
00:03:25,200 --> 00:03:30,400
insist that all have skin in the game, face moral hazards, suffer the consequence of our

34
00:03:30,400 --> 00:03:31,400
action.

35
00:03:31,400 --> 00:03:38,400
He taught us, as only he can, about Hammurabi code.

36
00:03:38,400 --> 00:03:44,880
Now we are confronting a world in which network effects are being put on steroids.

37
00:03:44,880 --> 00:03:51,080
Nodes are able to process unimaginable amounts of data and turn them into what can seem like

38
00:03:51,080 --> 00:03:53,480
human creations.

39
00:03:53,480 --> 00:04:00,440
AIs, large language models may not be as capable as humans but we seem nevertheless to be meeting

40
00:04:00,440 --> 00:04:03,600
them halfway.

41
00:04:03,600 --> 00:04:09,320
Nassim has made clear to me that he does not consider himself an expert on AI, yet I can't

42
00:04:09,320 --> 00:04:14,020
think of anyone who is better qualified to talk with us about the dangers we may face

43
00:04:14,020 --> 00:04:20,560
from technologies we just barely understand in networks that we do understand largely

44
00:04:20,560 --> 00:04:22,600
thanks to him.

45
00:04:22,600 --> 00:04:25,240
It's great pleasure, Nassim Taler.

46
00:04:25,240 --> 00:04:38,360
I'm very honored, but he gave me, Professor Abishai gave me way too much credit, but

47
00:04:38,360 --> 00:04:45,560
I think that it's been shorter because what I think I contributed to mostly is fragility,

48
00:04:45,560 --> 00:04:52,440
is mapping fragility to accelerate a non-linear response, why they must be come together.

49
00:04:52,440 --> 00:04:59,760
Something else is pretty much other people's ideas largely my grandmother, but this mapping,

50
00:04:59,760 --> 00:05:04,560
so that's sort of, but I'm very honored also to be here because I've read him before he

51
00:05:04,560 --> 00:05:10,320
read me, all right?

52
00:05:10,320 --> 00:05:20,480
And also you guys have a wonderful setup here, wonderful campus, very well positioned, shielded

53
00:05:20,480 --> 00:05:28,680
from New Yorkers and stuff like that, valley, entry points are scarce, so this is a wonderful

54
00:05:28,680 --> 00:05:37,920
campus and I'm honored to be the first time in that great institution.

55
00:05:37,920 --> 00:05:45,160
Nassim, let's start with some of the basics.

56
00:05:45,240 --> 00:05:54,440
I feel like most of us intuitively understand the dangers of connectivity, but I think that

57
00:05:54,440 --> 00:05:59,240
you've given it a kind of precision, you've given that danger a kind of precision and

58
00:05:59,240 --> 00:06:04,440
I'd like to hear you talk about it because before we talk about AI, I think it makes

59
00:06:04,440 --> 00:06:11,600
sense for everyone to understand what you considered to be the dangers of networks before

60
00:06:11,600 --> 00:06:14,720
AI became a serious problem.

61
00:06:14,720 --> 00:06:19,120
So let me give a very simple metaphor, a story I gave to the Black Swan, so I'll be repeating

62
00:06:19,120 --> 00:06:26,080
myself with that story that illustrates both fat tails and connectivity.

63
00:06:26,080 --> 00:06:34,200
Let's assume that you're in the 19th century and you're an opera singer, okay?

64
00:06:34,200 --> 00:06:42,920
You're an opera singer in Boston or in Naples, you're protected, you have a job because the

65
00:06:42,920 --> 00:06:48,760
great opera singers in Milan at the Scala, which essentially is very small, and the great

66
00:06:48,760 --> 00:06:52,480
opera singers at the Metropolitan, they can't compete with you because they're over there

67
00:06:52,480 --> 00:06:58,440
and you're here, so you have some kind of protection.

68
00:06:58,440 --> 00:07:01,680
Therefore the income of opera singers is going to be similar to that of dentist.

69
00:07:01,680 --> 00:07:06,480
Some dentists make a lot of money, some make less money, but the greatest dentist doesn't

70
00:07:06,480 --> 00:07:10,720
make much, much more than the average because of course you can't scale it, you can fill

71
00:07:10,800 --> 00:07:15,680
up the Scala, you know, you can do more than that.

72
00:07:15,680 --> 00:07:24,040
And then from Carrion, okay, discovered that there's such a thing as television and such

73
00:07:24,040 --> 00:07:28,280
a thing as Deutsche Grammophon where you can store your voice.

74
00:07:28,280 --> 00:07:29,480
What happened?

75
00:07:29,480 --> 00:07:37,920
A hundred years later, a few opera singers, about ten of them made 90% of the money and

76
00:07:37,920 --> 00:07:45,760
the remaining opera singers worked, Starbucks was not the same at the time, but most of

77
00:07:45,760 --> 00:07:50,560
the occupation was Starbucks, so that gives you an idea about connectivity, what did it

78
00:07:50,560 --> 00:08:00,800
do in the economic world, okay, is that someone dead, like Pavarotti, can compete, okay, or

79
00:08:00,800 --> 00:08:09,120
as a state can actually get the income away from a young opera singer in Boston.

80
00:08:09,120 --> 00:08:14,880
So this is pretty much, so this is connectivity for you and then you can generalize to biological

81
00:08:14,880 --> 00:08:22,160
things like COVID and in the Black Swan I say that the pandemic would not be a Black

82
00:08:22,160 --> 00:08:28,560
Swan because of connectivity, now connectivity is a very good thing, but it has side effects

83
00:08:28,560 --> 00:08:33,920
and if you don't know the side effects, alright, don't get too much into something.

84
00:08:33,920 --> 00:08:39,160
So these are the side effects of connectivity is that you have a winner take all effect.

85
00:08:39,160 --> 00:08:45,800
So if you take income of athletes, 1950, the top athlete versus the average, two, three

86
00:08:45,800 --> 00:08:52,520
times which people found excessive at the time, top athletes, today it's 50 million

87
00:08:52,520 --> 00:09:00,440
euros versus say 35,000 euros, I'm very bad at numbers, so I can do algebra, but I can

88
00:09:00,440 --> 00:09:06,880
do division, so divide 50 million by 35,000 and then you get an idea of the inequalities

89
00:09:06,880 --> 00:09:16,440
that we have in that field, okay, so the current environment produces these things, the Harry

90
00:09:16,440 --> 00:09:25,280
Potter effect that 100 authors worldwide can live off, this was numbers as of four or five

91
00:09:25,280 --> 00:09:31,600
years ago, can live off of their income as authors and then the rest, again now they

92
00:09:31,600 --> 00:09:36,800
have Starbucks, there are other things, there are university programs where they teach people

93
00:09:36,800 --> 00:09:41,240
to write or end up teaching other people to write or stuff like that.

94
00:09:41,240 --> 00:09:50,040
So the rare event actually accounts for the greatest impact on the entire thing, if you

95
00:09:50,040 --> 00:09:52,880
average it out, you miss that, you miss that.

96
00:09:52,880 --> 00:09:57,840
You miss that because it's driven by the tails, by the rare event, by the extreme, there are

97
00:09:57,840 --> 00:10:03,400
two environments, one I call mediocrity, if I take the weight of the people here, we put

98
00:10:03,400 --> 00:10:09,080
them on a scale, I'm sure in darkness, the engineering department know how to build robust

99
00:10:09,080 --> 00:10:16,280
scales, you know, and then we add to our sample the largest person you can find on a planet,

100
00:10:16,280 --> 00:10:22,440
the largest human being, it's not going to change the average much, all of us here, I

101
00:10:22,440 --> 00:10:27,080
don't know how many we have here, but say a couple of hundred, nothing, it's not going

102
00:10:27,080 --> 00:10:29,360
to make an impact on the average.

103
00:10:29,360 --> 00:10:40,160
But if you take the net worth and add Elon Musk, some 200 billion dollars, so you realize

104
00:10:40,160 --> 00:10:44,480
that there are some domains Elon Musk would change the average and it would be a high

105
00:10:44,480 --> 00:10:46,480
percentage of the total.

106
00:10:46,480 --> 00:10:52,520
So this is the effect of fat tails and you got to figure out which domains produce fat

107
00:10:52,520 --> 00:10:54,680
tails and which domains don't.

108
00:10:54,680 --> 00:10:59,280
So we have a tableau, in the black swan I did that years ago, saying this is the domain

109
00:10:59,280 --> 00:11:05,440
socioeconomic life, it's driven by fat tails, your weight is not driven by fat tails, like

110
00:11:05,440 --> 00:11:11,320
there's no meal you can have that will represent 98% of your annual consumption.

111
00:11:11,320 --> 00:11:16,440
You can try, I mean you die I think very after the first 2% or something like that, you die

112
00:11:16,440 --> 00:11:21,440
after the first 5,000 calories, so you can't reach, so, but you can lose all your money

113
00:11:21,440 --> 00:11:23,760
in a minute.

114
00:11:23,760 --> 00:11:27,960
This is, so we have this domain and the other domain.

115
00:11:27,960 --> 00:11:33,360
Right, so go back to the epidemic, because that's interesting.

116
00:11:33,360 --> 00:11:39,840
You said the epidemic was actually not a black swan event, even though the particular

117
00:11:39,840 --> 00:11:46,280
virus might be considered a rare fat tail event.

118
00:11:46,280 --> 00:11:52,000
Yeah, not having an epidemic is actually the rare event, not having had an epidemic.

119
00:11:52,120 --> 00:11:57,000
Let's look at connectivity and the reasoning, I had the black swan event as follows.

120
00:11:57,000 --> 00:12:05,120
If I have an island, an island will have many more species per square meter than a continent.

121
00:12:05,120 --> 00:12:06,120
So what does it tell you?

122
00:12:06,120 --> 00:12:11,600
It tells you the continent has more inequality, a few species dominate the numbers.

123
00:12:11,600 --> 00:12:14,680
And if you'd open up all the islands to one another, you're going to have that.

124
00:12:14,680 --> 00:12:17,320
So we're going to have a winner take all in the biological field.

125
00:12:17,320 --> 00:12:20,400
So whatever virus you have would travel.

126
00:12:20,400 --> 00:12:23,720
So the reasoning of the black swan was that there's such thing, I don't know if you've

127
00:12:23,720 --> 00:12:24,720
heard of Air France.

128
00:12:24,720 --> 00:12:28,600
Air France, they fly, they do New York, Paris.

129
00:12:28,600 --> 00:12:35,440
And at the time they rotated because they were not stopping Gabon during Ebola.

130
00:12:35,440 --> 00:12:41,040
There are other things like Air France, the British Air, the American Airlines, everybody

131
00:12:41,040 --> 00:12:43,080
has those, okay.

132
00:12:43,080 --> 00:12:48,160
So the Great Plague, I don't want to call it great, but it was the plague, all right.

133
00:12:48,160 --> 00:12:50,960
It was a bad plague.

134
00:12:50,960 --> 00:12:58,000
It took, I think there's some villages in the Lake District, in England, that was reached

135
00:12:58,000 --> 00:13:05,280
340 years after Constantinople, okay.

136
00:13:05,280 --> 00:13:10,480
And they never reached the Americas and Oceania, okay.

137
00:13:10,480 --> 00:13:15,840
So you realize now you can have the same effect with a meeting like this one or you have people

138
00:13:15,840 --> 00:13:21,480
from many countries, particularly if you have conventions and people fly in and they

139
00:13:21,480 --> 00:13:27,720
can distribute to the Philippine, Mongolia, Southwestern, China, Argentina.

140
00:13:27,720 --> 00:13:30,320
Within a week you have a worldwide pandemic.

141
00:13:30,320 --> 00:13:36,080
So that was the reasoning in the black swan and no pandemic was taking place.

142
00:13:36,080 --> 00:13:40,120
And we haven't had anything of significance since the Spanish flu.

143
00:13:40,120 --> 00:13:46,400
So I mean, we had many bad things, but so the black swan for me was the absence of such

144
00:13:46,400 --> 00:13:47,400
a thing as COVID.

145
00:13:47,400 --> 00:13:48,400
Right.

146
00:13:48,400 --> 00:13:56,600
So the irony here is that most people will think of the existence of this virus as a rare

147
00:13:56,600 --> 00:14:05,400
statistical event, but because of connectivity and the way in which you describe connectivity,

148
00:14:05,400 --> 00:14:14,320
the ability of that virus to spread is so baked into the network.

149
00:14:14,320 --> 00:14:15,400
And there's no place to hide.

150
00:14:15,400 --> 00:14:20,480
And there's no place to hide that we should not think of pandemics as rare events.

151
00:14:20,480 --> 00:14:28,200
We should assume that they're going to be predictable, they're rare, improbable, but

152
00:14:28,200 --> 00:14:32,680
the probability of their spread is very predictable.

153
00:14:32,680 --> 00:14:39,040
And I was enraged during the pandemic at practically every single group of different political

154
00:14:39,040 --> 00:14:40,040
groups.

155
00:14:40,040 --> 00:14:45,800
In the beginning, when we were waiting, we were a group of people in a nearby arm, myself,

156
00:14:45,800 --> 00:14:54,360
complex waiting for the pandemic to emerge to go back people to reduce connectivity.

157
00:14:54,360 --> 00:14:58,080
You don't need all these things, all you need is reduced connectivity.

158
00:14:58,080 --> 00:15:00,720
And the Ottomans know how to do it.

159
00:15:00,720 --> 00:15:05,880
The Ottomans and Habsburg had something called lasaretos, quarantines.

160
00:15:05,880 --> 00:15:10,360
I grew up in Beirut, there's a quarantine, the Quarantina, had an Italian name, that's

161
00:15:10,360 --> 00:15:13,960
where people, vessels would come in and put you for 40 days.

162
00:15:13,960 --> 00:15:16,920
Actually, quarantine was about seven to 11 days, depending on where you came from.

163
00:15:16,920 --> 00:15:17,920
They had formulas.

164
00:15:17,920 --> 00:15:25,440
And the minute they hear a rumor of a quarantine, of a pandemic, or the Quarantina, all right,

165
00:15:25,440 --> 00:15:30,640
so you had quarantines, we don't eat quarantines if you have testing.

166
00:15:30,640 --> 00:15:39,360
So it took us 13 months from the inception of the COVID, right, to have testing at the

167
00:15:39,360 --> 00:15:40,360
U.S. border.

168
00:15:40,360 --> 00:15:45,120
And I don't know if you've been to JFK during, you know, when planes come from all these

169
00:15:45,120 --> 00:15:46,120
places.

170
00:15:46,120 --> 00:15:53,280
And it looks like, I mean, being in a subway car, everybody's contaminating everybody.

171
00:15:53,280 --> 00:15:55,920
So for 13 months, people didn't get the simple measure.

172
00:15:55,920 --> 00:16:03,240
So in the beginning, we started fighting for, what we called exactly, decoupling systems,

173
00:16:03,240 --> 00:16:05,160
by putting fences around the system.

174
00:16:05,160 --> 00:16:10,560
So instead of lock-in, lock-in in your house, we locked out.

175
00:16:10,560 --> 00:16:19,160
So the second thing is we had to fight people in psychology departments, finding it irrational

176
00:16:19,160 --> 00:16:23,640
for us to worry about a pandemic that killed 5,000 people worldwide.

177
00:16:23,640 --> 00:16:29,040
When cancer was killing 5,000 people every day or something, right?

178
00:16:29,040 --> 00:16:33,080
So you couldn't explain to them the following reasoning.

179
00:16:33,080 --> 00:16:36,880
There's a fellow called Dr. Phil who went on television saying, oh, we don't, at the

180
00:16:36,880 --> 00:16:39,200
time COVID had killed 3,000 Americans.

181
00:16:39,200 --> 00:16:44,720
He said, 3,000 Americans have drowned in a swimming pool.

182
00:16:44,720 --> 00:16:49,440
So, you know, why don't, we don't shut down swimming pools because people drown in them.

183
00:16:49,440 --> 00:16:53,560
Why do we shut down because of COVID?

184
00:16:53,560 --> 00:17:00,840
So, you know, the response is, if I drowned in my swimming pool, also I was at a neighbor

185
00:17:00,840 --> 00:17:06,320
who was going to drown in her swimming pool, and that probably increased, whereas the five,

186
00:17:06,320 --> 00:17:09,840
sorry, that probably have not increased, right?

187
00:17:09,840 --> 00:17:15,600
Whereas if I die of COVID, the odds that my neighbor is going to die of COVID has increased.

188
00:17:15,600 --> 00:17:22,160
So you got to look at the multiplicative effect of these things and forget about standard statistics.

189
00:17:22,160 --> 00:17:27,760
Again, mediocre standard is what people learn in business pool and statistics through classes.

190
00:17:27,760 --> 00:17:29,920
You're completely useless on a bell curve.

191
00:17:29,920 --> 00:17:34,200
The bell curve works very, very well if you do astronomy, right?

192
00:17:34,200 --> 00:17:40,160
If you do astronomy, if you do medicine, it works, but it doesn't work in socioeconomic

193
00:17:40,160 --> 00:17:41,160
things.

194
00:17:41,160 --> 00:17:44,440
It doesn't work for pandemics.

195
00:17:44,440 --> 00:17:48,160
So we had to fight, and nobody was taking us seriously until we started producing papers

196
00:17:48,160 --> 00:17:53,120
in like nature physics, because physicists understood the mass immediately, and then

197
00:17:53,120 --> 00:17:54,840
people start taking us seriously.

198
00:17:54,840 --> 00:17:57,200
And then you interviewed us, and about a few words.

199
00:17:57,200 --> 00:18:01,320
I remember also when we talked about it, I read that paper at the time, and one of the

200
00:18:01,320 --> 00:18:06,160
things you said in the paper as a kind of decoupling or quarantine was, if everybody

201
00:18:06,160 --> 00:18:08,160
just wore a mask immediately.

202
00:18:08,160 --> 00:18:09,160
Exactly.

203
00:18:09,160 --> 00:18:11,880
Even if you don't understand how it works, wear a mask.

204
00:18:11,880 --> 00:18:17,120
And the masks for masks also, they didn't get the non-linearity, is that the first part

205
00:18:17,120 --> 00:18:20,760
of the non-linearity, it's complicated for us really to explain the following.

206
00:18:20,760 --> 00:18:28,320
If I reduce viral load by 10%, I may reduce infection probably by 90%, you see, or risk

207
00:18:28,320 --> 00:18:30,200
of death by more than that.

208
00:18:30,200 --> 00:18:34,320
And the second thing, they couldn't figure out that if I wear a mask, and I reduce my

209
00:18:34,320 --> 00:18:40,080
viral load by 10%, and you were wearing a mask, that you're looking at the joint effect

210
00:18:40,080 --> 00:18:43,040
of both masks, not just one.

211
00:18:43,040 --> 00:18:50,240
So there have been a lot of papers on masks that, we actually debunked a lot of them.

212
00:18:50,240 --> 00:18:53,520
But besides that, you don't need a paper, I mean, just understand that what you've got

213
00:18:53,520 --> 00:18:58,840
to lose wearing a mask while you're going to have a little CO2 or something, it's not

214
00:18:58,840 --> 00:18:59,840
a big deal.

215
00:18:59,840 --> 00:19:04,240
People are not going to see your teeth, so it's okay, it's fine, draw a smile on your

216
00:19:04,240 --> 00:19:06,240
mask.

217
00:19:06,240 --> 00:19:07,960
So we had to fight for masks.

218
00:19:08,320 --> 00:19:14,840
The problem is that in the beginning, the establishment, intellectual establishment,

219
00:19:14,840 --> 00:19:18,960
using pseudo-statistics, what I call the Pinker statistics, named after Stephen Pinker

220
00:19:18,960 --> 00:19:20,280
in my books.

221
00:19:20,280 --> 00:19:22,680
So bad statistics are called naÃ¯ve empiricism.

222
00:19:22,680 --> 00:19:27,400
They were against measures to fight COVID.

223
00:19:27,400 --> 00:19:36,880
And then it switched, okay, the Trumpist became against the measures.

224
00:19:36,920 --> 00:19:39,480
The other one is because the Trumpists were against the measures.

225
00:19:39,480 --> 00:19:42,040
The other one said, okay, let's take measures.

226
00:19:42,040 --> 00:19:50,480
But in the beginning, it's not like the polarization flipped at some point during that story.

227
00:19:50,480 --> 00:19:58,520
So let's talk about fragility in this context, because that seems to me a critical insight

228
00:19:58,520 --> 00:20:01,480
that you've advanced.

229
00:20:01,480 --> 00:20:12,040
If you have networks that are susceptible to the catastrophic network effects that ensue

230
00:20:12,040 --> 00:20:24,600
and that you have exponential spread, you then have a kind of fragile system which considers

231
00:20:24,600 --> 00:20:30,120
itself safe as long as it's just doing averaging, but is not at all safe if you take into account

232
00:20:30,120 --> 00:20:32,560
the catastrophic effects of tail events.

233
00:20:32,560 --> 00:20:34,360
And also the acceleration effects.

234
00:20:34,360 --> 00:20:35,360
And the acceleration effects.

235
00:20:35,360 --> 00:20:38,920
So let me explain sort of like what happens in a system.

236
00:20:38,920 --> 00:20:40,840
So let me go back to non-linearity.

237
00:20:40,840 --> 00:20:42,320
I can talk about it, it's not too complicated.

238
00:20:42,320 --> 00:20:43,320
Yeah, go ahead.

239
00:20:43,320 --> 00:20:44,320
So non-linearity.

240
00:20:44,320 --> 00:20:46,200
As long as you don't talk about convex and concave.

241
00:20:46,200 --> 00:20:54,960
Okay, so if you jump, say you jump four meters, you're going to be harmed a lot more than

242
00:20:54,960 --> 00:20:57,320
four times if you jump one meter.

243
00:20:57,320 --> 00:20:58,320
You agree?

244
00:20:58,320 --> 00:20:59,320
All right.

245
00:20:59,440 --> 00:21:02,840
And if you definitely, if you jump 10 meters a lot more than 10 times one meter, because

246
00:21:02,840 --> 00:21:08,160
if you jump 10 meters, definitely there's a bituary in the darkness.

247
00:21:08,160 --> 00:21:11,160
So there's what we call acceleration.

248
00:21:11,160 --> 00:21:18,400
I notice that in finance, as a trader, if the market is down 1%, say you make 100,000,

249
00:21:18,400 --> 00:21:25,040
the market is down 10%, you make 20 million from acceleration payoff.

250
00:21:25,040 --> 00:21:28,320
So that's what we call, let me use the word, negative convexity.

251
00:21:28,520 --> 00:21:29,520
Convexity.

252
00:21:29,520 --> 00:21:33,400
Concavity, whatever, or positive convexity in some cases.

253
00:21:33,400 --> 00:21:40,240
So there must be, then I notice that everything in nature has to have those accelerated, those

254
00:21:40,240 --> 00:21:43,680
response, okay?

255
00:21:43,680 --> 00:21:49,440
And there's an argument which is complicated, but let's say, so in other words, if you jump

256
00:21:49,440 --> 00:21:58,120
four times one meter, okay, it's a lot better than jumping 0000 than four meters, you agree?

257
00:21:58,320 --> 00:21:59,320
All right.

258
00:21:59,320 --> 00:22:03,360
So let's apply that to demand, okay, a fragile system in demand.

259
00:22:03,360 --> 00:22:09,200
And there was a part of our conversation way before people were aware of the supply chain.

260
00:22:09,200 --> 00:22:15,240
If you consume 100 one year, say toilet paper, whatever it is, okay, and then 100 the next

261
00:22:15,240 --> 00:22:22,560
year, the average is 100, it's not going to stress the system in the same way that if

262
00:22:22,600 --> 00:22:28,480
you produce zero one year, or demand zero one year, and then 200 the next year, what

263
00:22:28,480 --> 00:22:33,640
happens if demand, you have hyperinflation, then hyperinflation, all right?

264
00:22:33,640 --> 00:22:41,640
So that's exactly what happened with demand for anything, for bicycle parts to whatever,

265
00:22:41,640 --> 00:22:47,240
so demand went to zero, and then jumped, okay.

266
00:22:47,240 --> 00:22:53,560
And of course, also there's stuff like Peloton, demand 1200, and then 100, so it was a lot

267
00:22:53,560 --> 00:23:01,560
worse than 50 and 50, so the unevenly distributed stuff, if you're fragile, you want a distribution

268
00:23:01,560 --> 00:23:07,480
to be steady when you're fragile, when you're out here, you want the market to go down 1%

269
00:23:07,480 --> 00:23:12,960
one day, and then 1% the other day, it's a lot better than zero, and then 2%.

270
00:23:13,400 --> 00:23:20,640
The effect is it squares or whatever, so it's the same thing actually in price impact

271
00:23:20,640 --> 00:23:22,120
in the markets.

272
00:23:22,120 --> 00:23:29,520
If you want to buy 10 units, say $10 billion of whatever, the stocks, if you buy them all

273
00:23:29,520 --> 00:23:35,720
now, you're definitely going to have financial problems, you're going to move prices, but

274
00:23:35,720 --> 00:23:44,520
if you buy over 10 days, no impact, okay, because 10 times, you know, it's 100 times

275
00:23:44,520 --> 00:23:51,760
the price change, because it's in squares, okay, 10 times is a lot, like 100 times 1.

276
00:23:51,760 --> 00:23:57,080
So that's the example of non-linearity that had to be present, and I think that's the

277
00:23:57,080 --> 00:24:01,960
only idea I've ever had, everything else is, you know, comes from reading a lot of books

278
00:24:02,120 --> 00:24:10,480
talking to grandmothers and grandparents and uncles and stuff, so this is the idea that

279
00:24:10,480 --> 00:24:16,600
our world, you've got to realize where the fragilities are, and it's very simple once

280
00:24:16,600 --> 00:24:23,720
you understand where the vulnerabilities are based on non-linearity, so this is it, and

281
00:24:23,720 --> 00:24:24,720
then the idea is-

282
00:24:24,720 --> 00:24:27,960
So for business school students, I just want to be, for business school students, the clearest

283
00:24:27,960 --> 00:24:33,960
example of this are the efficiencies of just-in-time manufacturing and stuff like that.

284
00:24:33,960 --> 00:24:43,720
So fragility is, in a way, a function of an illusory understanding of efficiency, right?

285
00:24:43,720 --> 00:24:50,880
Yeah, the word efficiencies actually makes no sense, okay, because I'm okay, so let me

286
00:24:50,880 --> 00:24:58,960
give you a little bit of my background, okay, I was a, you know, a regular MBA person, then

287
00:24:58,960 --> 00:25:06,240
became a trader, then after being a trader, I decided to become a mathematician, right?

288
00:25:06,240 --> 00:25:15,000
People do things backwards, and then after that, so I did practice, and then I did theory,

289
00:25:15,000 --> 00:25:19,000
usually people do the reverse, they study, then they, okay, so to me a lot of things

290
00:25:19,000 --> 00:25:26,720
that make sense because I was a trader, trading complex instruments, so, and then, so I did

291
00:25:26,720 --> 00:25:31,080
things backwards in the reverse sequence, going from practice to theory, theory to practice,

292
00:25:31,080 --> 00:25:37,160
you see people do it differently, and so that was the thing, they realized that a lot of

293
00:25:37,160 --> 00:25:48,560
the stuff we teach in some departments is excellent, you know, there's a lot of stuff

294
00:25:48,600 --> 00:25:56,680
that's very bad because it doesn't match the nuances of reality, okay, like mediocre

295
00:25:56,680 --> 00:26:00,240
standard, extreme standard, you can't talk about probability in the same way with a

296
00:26:00,240 --> 00:26:04,960
multiplicative process, and the process like drowning in a swimming pool or falling from

297
00:26:04,960 --> 00:26:11,280
a ladder or having a heart attack, okay, so there are different classes of risk, so this

298
00:26:11,320 --> 00:26:20,040
is sort of like my background, so come into it, so, so if you have, so we actually talked

299
00:26:20,040 --> 00:26:26,280
about this, because I was, for my sins, technology editor at the Harvard Business Review back

300
00:26:26,280 --> 00:26:32,480
in 1986, 87, when just in time manufacturing, we felt we had to compete with Japan at the

301
00:26:32,480 --> 00:26:39,200
time, and you know, just in time manufacturing entailed our, you know, doing away with store

302
00:26:39,240 --> 00:26:46,040
rooms and having good relationships with one supplier who would deliver just in time to

303
00:26:46,040 --> 00:26:50,720
your factory, so you wouldn't need a store room, and why have a lot of cash reserves

304
00:26:50,720 --> 00:26:56,000
because you want the money to be working for you, and it was all under the rubric of lean

305
00:26:56,000 --> 00:27:05,760
manufacturing, and it was a kind of efficiency idea which was great and terrifically cost-effective

306
00:27:06,640 --> 00:27:14,720
as long as there was no disruption, and now we learned in COVID, what did we learn?

307
00:27:14,720 --> 00:27:21,040
Yeah, but actually even before that, the notion of efficiency to me was it's something that exists

308
00:27:22,240 --> 00:27:27,360
in textbooks, but doesn't exist in research, let's take a very simple example, mergers,

309
00:27:28,240 --> 00:27:31,680
they say okay, we're going to have a bigger firm, it's going to be more efficient

310
00:27:32,560 --> 00:27:39,360
on ground that you will have fewer people in personnel department and smaller number of

311
00:27:39,360 --> 00:27:44,880
cafeteria people per capita, whatever it is, and a fewer number of trucks or whatever, okay, so

312
00:27:46,640 --> 00:27:51,520
they look at the numbers, they say it's going to be more efficient, but obviously it doesn't work

313
00:27:51,520 --> 00:27:59,520
because companies, you know, large companies don't survive, okay, and here in instant, you know,

314
00:27:59,520 --> 00:28:04,000
thought experiment, two companies come together, okay, they should have huge advantage that they

315
00:28:04,000 --> 00:28:10,240
don't have, and papers have been since 1978 documenting the absence of gains from mergers,

316
00:28:11,040 --> 00:28:16,000
they say something is leaking somewhere, till I figure it out when I started doing modeling on

317
00:28:16,000 --> 00:28:24,480
acceleration, I realized that, you know, there's such a thing as an animal called an elephant,

318
00:28:24,560 --> 00:28:32,960
no, a mammal, very cute and so on, there's an equivalent animal built almost the same way,

319
00:28:33,680 --> 00:28:43,360
called what? A mouse, okay, now why is it that we have eight million mice in New York

320
00:28:44,640 --> 00:28:51,280
that more than we ever had elephants, why? Because a mouse, I don't know, I'm not suggesting,

321
00:28:51,280 --> 00:28:56,720
please don't accuse me of whatever, but if you throw a mouse out of the window, it will laugh at

322
00:28:56,720 --> 00:29:07,120
you, but if an elephant falls by one meter, breaks a leg, it's gone, okay, and one meter is tiny for

323
00:29:07,120 --> 00:29:12,960
for an elephant, okay, so you realize the same thing applies to corporations, because if they're

324
00:29:12,960 --> 00:29:19,280
squeezed into needing something, so we look at a few case studies of corporations that had a squeeze

325
00:29:19,280 --> 00:29:24,560
that cost them a lot more than if they were small, and effectively that explains the first

326
00:29:24,560 --> 00:29:29,200
efficiency coming from size called economies of scale, it turns out to be completely S,

327
00:29:30,000 --> 00:29:36,080
they're this stochastic, this economy of scale, right, so there's an optimal size, okay, so

328
00:29:37,280 --> 00:29:44,720
and then we look at other, you know, stuff like efficiency of supply chain, right, visibly if

329
00:29:44,720 --> 00:29:50,560
you're going to be squeezing to pay it up, you got to count that in your model, all right, eventually

330
00:29:50,560 --> 00:29:55,280
you're going to pay up, everything's efficient, but let's say you have no chips, you're going to go

331
00:29:55,280 --> 00:30:00,560
begging for chips, no, because your whole process has stopped, you have all these employees, you've

332
00:30:00,560 --> 00:30:05,760
got to feed, okay, you have all these things, you have all these things to deliver, every day

333
00:30:05,760 --> 00:30:10,320
costs you a lot, so you're going to pay up for whatever you don't have, and guess what,

334
00:30:10,880 --> 00:30:18,080
you know, it's going to be taken out of, so there's an equilibrium, and the equilibrium is probably

335
00:30:18,080 --> 00:30:25,760
some economy of scale, not too much, some supply chain optimization, not too much, there's constraints

336
00:30:25,760 --> 00:30:31,520
as high, and then when I started looking mathematically at optimization models, I realized that they

337
00:30:31,520 --> 00:30:37,200
only made sense under a set of assumptions that completely get destroyed if you vary one variable,

338
00:30:37,920 --> 00:30:42,800
okay, so I mean, and I looked at, first thing I looked at is Ricardian model of

339
00:30:44,960 --> 00:30:52,160
comparative advantage, that if you assume, you know, the original Ricardian model,

340
00:30:52,720 --> 00:30:57,840
Ricardian model that one country produced cloth, the other one produced wine, but let's,

341
00:30:58,560 --> 00:31:05,280
but it assumed that the price of both is constant, but what if the price is not constant, so yeah,

342
00:31:06,000 --> 00:31:11,280
you know, so what if you have a problem with, with Phylloxera that happened after Ricardo,

343
00:31:11,280 --> 00:31:17,120
destroying your wine crop, okay, so all these are not part of the model, so I think the analogy I

344
00:31:17,120 --> 00:31:26,080
gave you is that if I drive 500 miles an hour in New York City, I say at 2am, it's not going to be

345
00:31:26,080 --> 00:31:30,640
faster than 20 miles per hour, actually it's not going to be faster than one mile per hour, because

346
00:31:30,640 --> 00:31:35,520
you're guaranteed to die at 500 miles per hour, you're pretty much guaranteed to die, so there

347
00:31:35,520 --> 00:31:42,560
exists, so, and then the other thing I discussed in the black swan is why is it that nature, if it

348
00:31:42,560 --> 00:31:50,400
was efficient to have, you know, less, less, use of stuff like, you know, why does nature

349
00:31:50,400 --> 00:31:54,960
give us two kidneys, and I'm sure students at Dartmouth tend to have in general two kidneys,

350
00:31:55,040 --> 00:32:02,080
but you don't need two kidneys, you need only one, one kidney, and an economist would say

351
00:32:02,080 --> 00:32:05,600
not even one, you just go to dialysis, you're carrying all this weight for nothing,

352
00:32:07,360 --> 00:32:13,040
but there's no storm and you're pressed out of the, you exist out of the gene pool, right, so.

353
00:32:13,680 --> 00:32:22,000
So that's a perfect segue to, to talk about what steps you can take to be antifragile,

354
00:32:22,000 --> 00:32:27,760
that is to say what things that you do to be robust, because I think that's really the

355
00:32:27,760 --> 00:32:42,320
entree into the question of AI, like we, we are able to counter the dangers of fragility, how.

356
00:32:42,960 --> 00:32:48,640
Okay, let me not be very gloomy by saying that number one, I like AI.

357
00:32:49,600 --> 00:32:55,120
No, we're not in with AI yet, I'm just talking in, in, in, in the current situation.

358
00:32:55,120 --> 00:32:58,640
Yeah, okay, the current situation of connectivity actually is doing good things,

359
00:32:58,640 --> 00:33:00,720
let me put a good thing, a good word for connectivity.

360
00:33:00,720 --> 00:33:01,280
Yeah, okay.

361
00:33:01,280 --> 00:33:07,760
Let me move before, all right, in 1973, for those of you who were driving a large car,

362
00:33:07,760 --> 00:33:12,880
you know, this, you, you, you know what happened, 1973, there was an Arab oil embargo.

363
00:33:15,680 --> 00:33:16,320
Oh, yes.

364
00:33:16,400 --> 00:33:18,480
You remember that, of course, all right.

365
00:33:18,480 --> 00:33:23,360
And then the American cars in 1973, I don't have seen pictures,

366
00:33:23,360 --> 00:33:27,280
they could pretty much have this conference in a, in a car, all right, they were very large,

367
00:33:27,280 --> 00:33:33,920
we've had, okay, so it was, so they were like gas guzzlers, nobody can, I mean gas was free,

368
00:33:33,920 --> 00:33:41,280
and, and cars got bigger, and then, and then there's such a thing as Las Vegas where cars were,

369
00:33:42,160 --> 00:33:45,280
so, so you had room, so cars were huge, okay.

370
00:33:46,320 --> 00:33:54,080
And 1973 came, now what happened after 1973, compact cars started to show up everywhere,

371
00:33:54,080 --> 00:34:03,520
okay, and the cars, and the demand for oil dropped to the point that the state of Texas

372
00:34:03,520 --> 00:34:09,280
was nearly bankrupt, but definitely the Soviet war bankrupt, okay, so it took like

373
00:34:10,080 --> 00:34:19,600
from 1973 to the early 80s for the adaptation to take place, okay, so that's in the 70s.

374
00:34:21,360 --> 00:34:26,800
Now there is, I think the Nobel Prize of, I don't know if there's no such thing yet for

375
00:34:26,800 --> 00:34:34,320
the Nobel Prize for environmental studies, it should be given to Vladimir Putin, because by cutting

376
00:34:34,320 --> 00:34:47,200
the gas, it took Germany six months to adapt, okay, so he helped the cause of environmentalism

377
00:34:47,760 --> 00:34:55,040
because what took, what took six or seven years of, you know, reduction in demand and adaptation

378
00:34:55,040 --> 00:35:00,000
and stuff like that happened in six months, because of the, the Russians, he thought that it's

379
00:35:00,000 --> 00:35:03,600
going to be, you know, like the Arab embargo and everybody's going to suffer and the Germans are

380
00:35:03,600 --> 00:35:09,040
going to come to their knees to beg for mercy, give us natural gas, we need you, you know,

381
00:35:09,680 --> 00:35:18,080
so God save Russia, but it didn't work because they adapt it very quickly, so our world can adapt

382
00:35:18,080 --> 00:35:27,680
much faster than you think, and I remember posting something on Twitter, right before I bought a

383
00:35:27,680 --> 00:35:36,000
Tesla, all right, I made a mistake in my life, so I posted that, that how, you know, this is great,

384
00:35:36,000 --> 00:35:39,840
because we have like convexity, there's also some energy, free energy, you know, and stuff,

385
00:35:39,840 --> 00:35:45,520
the electricity is free, and I got all these insulting things, including the letter from

386
00:35:46,160 --> 00:35:51,360
the chief investment officer of the major firm, how I should be ashamed of saying something like

387
00:35:51,360 --> 00:36:00,480
that, and sure enough, we have much more, many more electric vehicles, and much more solar power,

388
00:36:01,200 --> 00:36:09,120
and it's growing. Right, so, so you're coming at this on a slant, I mean, what you're saying,

389
00:36:09,120 --> 00:36:14,240
I, I hear, what I hear you saying is that by stressing the system, you're proving the adaptability

390
00:36:14,240 --> 00:36:17,200
of people within these networks, and that there are certain stresses. But that's happening

391
00:36:17,200 --> 00:36:21,760
faster than in the past. Yeah, I, I, I agree, you know, got good things, got bad things,

392
00:36:21,760 --> 00:36:28,000
and now we got good things as well. I agree, but I'm trying to set up your own insights with regard

393
00:36:28,000 --> 00:36:36,880
to how you create a more robust system in advance of this kind of stress, in other words, you, you

394
00:36:36,960 --> 00:36:45,040
spoke about breakers, and, and storerooms, and cash reserves, and the things, I mean, even,

395
00:36:45,040 --> 00:36:49,120
I mean, one way to understand it is like the separation of powers in the United States,

396
00:36:49,680 --> 00:36:56,720
the biggest danger that the founders understood was that of a tyrant. And somehow, they made

397
00:36:56,720 --> 00:37:02,560
something that appears less efficient, they made it like the, the American government

398
00:37:03,520 --> 00:37:09,760
operates less efficiently than you imagine it being able to. But the reason for doing that is

399
00:37:09,760 --> 00:37:16,560
because of all the breakers in a way that they've put in to avoid the big catastrophe, which would

400
00:37:16,560 --> 00:37:27,520
be a tyrant. So there are ways in which you can create a more robust system, a more robust system

401
00:37:27,520 --> 00:37:35,040
when you, when you take the all in cost of putting in these breakers, rather than just looking at the

402
00:37:35,840 --> 00:37:37,920
immediate inefficiency. Yeah, yeah, I mean, they are, they are definitely, I mean,

403
00:37:39,200 --> 00:37:45,200
I gave a metaphor, it's very easy, years ago, they say you have two twin sisters,

404
00:37:46,160 --> 00:37:51,520
and they have an identical business, same revenue base, but one of them makes $4 share,

405
00:37:51,520 --> 00:37:57,760
the other one makes $1 share. The one that makes $4 share doesn't pay for insurance,

406
00:37:58,880 --> 00:38:03,600
doesn't have any stuff like that. And the stock market is going to love her,

407
00:38:03,600 --> 00:38:08,320
but she's going to go bankrupt, who's probably won eventually. If you look at mortality rate,

408
00:38:08,320 --> 00:38:14,320
which is like medicine, right? So you have an average expected life expectancy of maybe six

409
00:38:14,320 --> 00:38:19,600
or seven years, right, for these firms, right? But security analysts won't pick it up. The other

410
00:38:19,600 --> 00:38:26,960
sister makes $1 share, but she can survive, provided that her board or something doesn't

411
00:38:26,960 --> 00:38:33,360
try to fire her, you know, to hire someone like the other sister. So the thing is, we don't have to

412
00:38:33,360 --> 00:38:41,520
go through time series and data to figure it out, you can just look at inventory of the firms.

413
00:38:41,600 --> 00:38:45,520
Okay. You just, you know what can blow up the firm.

414
00:38:46,800 --> 00:38:53,040
Right. You pretty much know what is it, that operational leverage, central thing, we figured

415
00:38:53,040 --> 00:38:57,600
out on this, each firm has its vulnerability. You don't do that. Why? Because it costs money.

416
00:38:58,720 --> 00:39:03,200
Real owners of companies, this is why we have a survival of family owned companies,

417
00:39:04,160 --> 00:39:10,960
intergenerational, for hundreds of years in Japan and Europe, even here, because they have skin in

418
00:39:10,960 --> 00:39:18,080
the game. Whereas an employee has this asymmetry, you see, you want to accumulate as many bonuses

419
00:39:18,080 --> 00:39:23,600
and then send a postcard, say, I'm enjoying my retirement on a golf course, by the way, you know,

420
00:39:25,040 --> 00:39:31,120
I'm sorry about your bankruptcy, right? So that's the Jack Welch trade. I was going after Jack

421
00:39:31,120 --> 00:39:39,120
Welch when he was like a sacrosanct, was like, you know, so it's like going after Aquinas or

422
00:39:39,120 --> 00:39:44,880
someone going after Jack Welch, you know, in circles. The guy is stuffing the company with

423
00:39:44,880 --> 00:39:51,040
what I call short optionality, these things that explode. And incidentally, let me confess one

424
00:39:51,040 --> 00:39:57,280
thing, I made my money to retire from it, you know, betting on blowups of companies that have

425
00:39:57,280 --> 00:40:04,400
hidden risk. Okay. Like Fannie Mae, and I wrote in the Black Swan, Fannie Mae is sitting on a barrel

426
00:40:04,400 --> 00:40:10,160
of dynamite. And everybody laughed at me, but I had the last laugh because we made tons of money

427
00:40:10,160 --> 00:40:15,840
from the bankruptcy, but from the, from its insolvency, and then of course later on, it's

428
00:40:15,840 --> 00:40:22,640
funding. So, so I don't hide that we, that that have skin in the game in the sense that we bet

429
00:40:22,640 --> 00:40:27,920
on tail events. But you're implying that the people who ran Fannie Mae didn't, I mean, they

430
00:40:27,920 --> 00:40:31,840
were, they had no idea, they were making their bonuses, and they didn't necessarily lose anything.

431
00:40:31,840 --> 00:40:36,640
No, they said they had 15, they counted the New York Times. I said the New York Times that they're

432
00:40:36,640 --> 00:40:42,160
sitting on dynamite. When I saw their PNL, accelerating losses, providing things, I said,

433
00:40:42,160 --> 00:40:46,960
they're going to go bankrupt. I told the New York Times, I told the fellow who turned to be COVID

434
00:40:46,960 --> 00:40:52,960
denier, Berencind, who ended with me, he showed me secret reports, I told him they're going to go

435
00:40:52,960 --> 00:40:56,400
bankrupt. He said, he said, probably he'll say, of course, shout it. You know, I can tell the

436
00:40:56,400 --> 00:41:00,800
doorman, I'm going to tell people in the street, they're going to go bust. All right. They eventually

437
00:41:00,800 --> 00:41:06,480
went bust, but they countered. This is nonsense. This guy doesn't know what he's talking about.

438
00:41:06,480 --> 00:41:11,600
We have 15 mathematicians. Of course, I countered, but they didn't publish the New York Times,

439
00:41:11,600 --> 00:41:16,240
that you can have 15 mathematicians, 150 mathematicians, 1500 mathematicians, 15 billion

440
00:41:16,240 --> 00:41:20,720
mathematicians. It won't make a difference, right? You're still going to go bankrupt. And sure enough,

441
00:41:21,760 --> 00:41:28,720
they almost went bust without the tax payer, without the generosity of you and us tax payers.

442
00:41:29,680 --> 00:41:40,320
So we have a, it's not just Tany May. Tany May was, to me, a model of firms like that that are going

443
00:41:40,320 --> 00:41:45,920
to blow up. So we can express the bank with other firms. That was that. And that was the banking

444
00:41:45,920 --> 00:41:52,240
crisis 2007. And that's what people noticed me. But the black swan was written right then.

445
00:41:52,640 --> 00:42:00,000
Okay. So now let's look at the same analysis and apply it to who's going to blow up.

446
00:42:00,000 --> 00:42:07,200
So it's very simple. You take a firm, you see how many suppliers they have. What odds are that in

447
00:42:07,200 --> 00:42:18,320
2003, they had 18 suppliers for a product. But the accountants over time made them get one supplier

448
00:42:18,320 --> 00:42:25,040
Newhawn converge. All right. Like a very large firm that I know had 15 suppliers and now one in

449
00:42:25,040 --> 00:42:32,080
Newhawn. Okay. But of course they deserve what happened to them. Okay. But because it was cheaper.

450
00:42:32,640 --> 00:42:40,240
But things are not cheap. But there's a middle way. What's the halfway is if you need supplies,

451
00:42:41,120 --> 00:42:48,240
make sure that all your suppliers are not in the same basket. Okay. Right. All right.

452
00:42:48,240 --> 00:42:55,600
Diversify this one on one diversification. Okay. So and skin in the game. And skin in the game. Of

453
00:42:55,600 --> 00:43:02,400
course, if you lose the other, you know, so there's a lot of things you can do. But then as a society,

454
00:43:02,400 --> 00:43:06,400
as a whole, I think the job of the government is to protect us from tail events. That's my

455
00:43:06,480 --> 00:43:10,720
definition of government that sort of some people think the government like in the EU

456
00:43:11,440 --> 00:43:17,600
should meddle with how much energy your vacuum cleaner uses. I think that otherwise the things

457
00:43:17,600 --> 00:43:23,760
of government is there for pandemics and things like that. Okay. That we have, we have a reserve for

458
00:43:23,760 --> 00:43:30,560
oil, but we didn't have one for chips. Right. Right. We have a, so we have to identify vaccines.

459
00:43:31,200 --> 00:43:35,360
We didn't have a reserve for vaccines. We didn't have, we weren't planning for this event. That's

460
00:43:35,440 --> 00:43:42,480
right. And the only place, the only intelligent person, because from 2013 on, Yannay Baryam and I

461
00:43:43,120 --> 00:43:49,440
went talking to people, tell them, listen, the pandemic is coming. Are you ready? The person,

462
00:43:49,440 --> 00:43:57,120
the only place where these people had a game plan that was very precise, Singapore. And guess what?

463
00:43:57,200 --> 00:44:00,320
And the person retired. So it wasn't as good as when he was there.

464
00:44:02,720 --> 00:44:10,160
Phil, something, right? He was head of civil service in Singapore. And he had, he knew, he said,

465
00:44:10,160 --> 00:44:13,840
yes. And then this is what we're doing for this. And he taught us basically,

466
00:44:15,360 --> 00:44:21,200
you know, we learned more from him than, than, than our argument. So, so they are some places,

467
00:44:21,200 --> 00:44:28,480
but not, not the job of the government is to have contingency plan in case of pandemic. And we

468
00:44:28,480 --> 00:44:36,080
know we're going to get that big one. I mean, COVID was very bad. But it was more like a dress

469
00:44:36,080 --> 00:44:44,560
rehearsal for the real one. Think about it. The antibiotic resistant strain that once it's out,

470
00:44:45,280 --> 00:44:50,400
you know, plus for the aging of the population, they will transmit. So, so they are things that

471
00:44:50,400 --> 00:44:55,280
will, you know, that, that, that, that you've got to consider. The problem is epidemiologists,

472
00:44:55,280 --> 00:45:00,720
I hope I'm not offending too many people, but epidemiologists, their models were using Gaussians

473
00:45:00,720 --> 00:45:05,680
who was not using, you know, power law tales. Right. When you wrote that, the nature of physics,

474
00:45:05,680 --> 00:45:08,880
the physicists got it right away. They say, how come they're not using it? They say, yes,

475
00:45:08,880 --> 00:45:13,040
there's a published article and people were shocked. It's a completely different culture.

476
00:45:14,000 --> 00:45:22,320
Well, now that, now that you've frightened me with the idea of a universal pandemic of

477
00:45:22,960 --> 00:45:29,680
antibiotic resistant bacteria, I feel like I'm somehow scanting the problem by turning to AI,

478
00:45:29,680 --> 00:45:36,000
which seems by comparison, by comparison, rather hypothetical event. But I do, before we get to

479
00:45:36,080 --> 00:45:43,840
questions, which I hope to in like within five minutes, I just want you to try to apply what

480
00:45:43,840 --> 00:45:54,160
you mean by anti fragility to a network where nodes in the network suddenly are on steroids because

481
00:45:54,160 --> 00:46:02,400
of AI. Like, how can we apply this? Let me, let me go back to COVID in a way, in a way we were

482
00:46:02,400 --> 00:46:10,000
lucky. COVID was a bad thing. It killed 20, some million people were very bad. But it's sort of

483
00:46:10,000 --> 00:46:17,760
like it was, it taught us, right? So just assume that the big one came before COVID. Okay. So,

484
00:46:17,760 --> 00:46:22,800
and the internet saved us with COVID. Just assume if we didn't have as a sequence, we had COVID,

485
00:46:22,800 --> 00:46:28,880
then the internet versus internet and COVID. Okay. So, so we were, but now we're more prepared for

486
00:46:28,880 --> 00:46:33,680
the next one. So we don't have a lot of selling to do, say, okay, you test at the border, you close

487
00:46:33,680 --> 00:46:38,080
the border, you do this, you do this, we know the game plan. And probably for generations,

488
00:46:38,080 --> 00:46:44,480
it's going to hold. The, and, and the zoom, you know, which I mean, I'm sure you fed up with it.

489
00:46:44,480 --> 00:46:49,680
Okay, I am fed up with it. He made me teach a class on zoom. For me, going to a dentist is better

490
00:46:49,680 --> 00:46:55,760
than teaching class on zoom. But nevertheless, I mean, it allows us to function. So the,

491
00:46:56,720 --> 00:47:04,480
the, so we have things. So antifragile, there's the systems that have this property that without

492
00:47:04,480 --> 00:47:12,640
stressors, they get weaker. That's what I noticed. Doctors call it hermesis. And I figured out the

493
00:47:12,640 --> 00:47:17,520
modeling of it comes from convexity. Once you define fragility, you have the reverse of fragility.

494
00:47:18,560 --> 00:47:23,360
And it's the same equation. It's a minus sign. Because the minus one is concave, the other's

495
00:47:23,440 --> 00:47:34,080
convex. So something, so you tell yourself it's a system is antifragile, then it needs to be stressed.

496
00:47:35,360 --> 00:47:43,600
Okay. Otherwise, it dies. And natural systems don't get information via the New York Times.

497
00:47:45,120 --> 00:47:50,960
How do they get via stressors? So if you go, if you have a Mediterranean skin, I think you

498
00:47:50,960 --> 00:47:56,880
qualify as Mediterranean skin, you go in the sun. All right. What happens? You tan. Okay.

499
00:47:56,880 --> 00:48:03,120
Why do you tan? Your body gets a signal that this is the intensity. So it's protection

500
00:48:03,840 --> 00:48:09,280
for 10% more intensity. I'm sure you have a gym here. You know, you have a gym in darkness.

501
00:48:09,280 --> 00:48:14,240
There's a gym. You go to the gym, you lift the 100 pounds. What happens to your body?

502
00:48:15,200 --> 00:48:21,280
It prepares for 110 pounds. It up regulates. So there are a lot of things. And now if you

503
00:48:21,280 --> 00:48:27,840
tell us what is the converse of it, the bad news is that if something needs stressors and doesn't

504
00:48:27,840 --> 00:48:35,680
get stressors, what happened to it? It weakens. You see? So just assume that if you spend six months

505
00:48:35,680 --> 00:48:42,320
in bed, no stressors, no germs, nothing, no classes, no Starbucks, no bad coffee, nothing.

506
00:48:42,320 --> 00:48:47,760
All right. You're in bed for six months. And then you get out of bed. What have, first of all,

507
00:48:47,760 --> 00:48:53,920
your bones, you know, would be weaker. And, and of course, you're going to the first germ. And,

508
00:48:53,920 --> 00:48:58,880
and if you're in a completely germ free room, you know, what's going to happen to you,

509
00:48:58,880 --> 00:49:04,400
probably not going to survive. Right. So, so there is this idea that, you know,

510
00:49:04,400 --> 00:49:09,840
you need stressors up to a point. You need some stressors. And things up regulate. And I learned

511
00:49:09,840 --> 00:49:15,280
that. Why I don't, one thing is maybe sort of I'm trying to explain why I'm not a good speaker.

512
00:49:15,280 --> 00:49:21,680
I don't want to be a good speaker. Because if you speak like this, you have to make an effort

513
00:49:21,680 --> 00:49:33,440
to understand me. You'll remember more what I'm talking about. So the, now this sort of like,

514
00:49:33,440 --> 00:49:41,360
okay, but I still want to make it easier on them just a second. So apply this, apply this to AI. I

515
00:49:41,360 --> 00:49:46,800
mean, we, we normally think, I mean, people have talked about this, you wouldn't be in the room,

516
00:49:46,800 --> 00:49:51,280
I suppose, unless you were concerned to some extent with AI. Well, no, you'd be in the room to see

517
00:49:51,280 --> 00:49:59,520
nothing anyway. But if, if with AI, we have these nodes in networks that have these capabilities,

518
00:49:59,600 --> 00:50:04,320
you spoke about the various capabilities of AI at lunch today, which I want you to share.

519
00:50:04,320 --> 00:50:12,320
Okay. But what, what can we do if government, if government's responsibility is to protect us

520
00:50:12,320 --> 00:50:16,960
from tail events? Yeah, I'm not worried about AI. And let me start more. Good. So let me tell you why

521
00:50:16,960 --> 00:50:22,560
I'm not, I'm more worried about the pandemic. And I'm vastly more worried on my list about debt.

522
00:50:23,520 --> 00:50:30,480
And I don't know if you own real estate, but the, the, the, we had because of bad policy for the

523
00:50:30,480 --> 00:50:37,600
reserve zero interest rates, we have a bubble. And, and they're not able to manage it, keep raising

524
00:50:37,600 --> 00:50:44,160
a lot of debt in the system. So we have that to me, these are the big problems. AI is not a

525
00:50:44,160 --> 00:50:50,560
problem for several reasons. Number one, I happen to have my, my big job is happened to do statistical

526
00:50:50,720 --> 00:50:56,560
modeling. And we've been doing neural net forever. Okay. Neural net and finance and always failed.

527
00:50:58,240 --> 00:51:03,120
And, and, and also with people, what do you mean by AI, the difference thing, the machine learning,

528
00:51:03,120 --> 00:51:08,800
the LLM, which strategy PT is, and we'll focus on that in a minute. And then we have robotics,

529
00:51:08,800 --> 00:51:15,040
robotics, do you have a thermostat in your car? As you put 68 degrees, if it's higher, it shuts off.

530
00:51:15,760 --> 00:51:22,560
So we've had that forever for a long time. I mean, okay, so it's not like we're just, you know,

531
00:51:22,560 --> 00:51:27,360
making it more advanced, but, but, but people were not afraid of thermostat. And now suddenly

532
00:51:27,360 --> 00:51:31,920
the single robots are going to take over the world. Right. So we got other things to worry about

533
00:51:31,920 --> 00:51:38,880
before, but let me talk about chat, GPT as, as a trader. I learned one thing as an option trader.

534
00:51:39,520 --> 00:51:45,280
And I had a saying, if you have any reason to buy a stock or an option to buy an option, don't buy it.

535
00:51:46,960 --> 00:51:54,560
Why? This was already in the price. Okay. Now the, let me explain what chat GPT does,

536
00:51:54,560 --> 00:52:02,000
basically. It does is that it takes all the conversation and gives you the most likely,

537
00:52:03,520 --> 00:52:06,960
maybe not that precise conversation, but the most likely one that resembles it.

538
00:52:07,360 --> 00:52:11,920
And, and the first thing I did is try to trick it because, you know,

539
00:52:13,040 --> 00:52:17,280
during the day, I do nothing except bicycling now and a little bit of math in the morning,

540
00:52:17,280 --> 00:52:22,560
and then the rest of the time, I get time to kill. So, let's strip chat GPT. Okay.

541
00:52:24,640 --> 00:52:30,720
So you go to the obvious point is that what, what does chat GPT? It's, it takes just,

542
00:52:30,720 --> 00:52:34,960
it's just verbalistic. It takes words. So you can trip it by either making it say

543
00:52:34,960 --> 00:52:39,920
two things that are contradictory because of the verbalism, but that's complicated.

544
00:52:39,920 --> 00:52:44,560
So I did that, of course, and say, oh, I got a homerun. So let's see why chat GPT

545
00:52:45,760 --> 00:52:48,880
cannot run anything. Let me tell you why. It's a great clerk, right?

546
00:52:49,520 --> 00:52:54,800
It cannot run. The clerk doesn't run. It assists. Let me explain the thing. The first thing is

547
00:52:55,600 --> 00:53:04,240
there was a, at the Congress of Berlin, there's been a war between, on one part in Greece and

548
00:53:04,240 --> 00:53:09,760
Western power. The other one, the Ottoman Empire. And there was a fellow called

549
00:53:10,720 --> 00:53:19,520
Constantine Karateodoris, okay, who was representing one power. So I asked chat GPT,

550
00:53:19,520 --> 00:53:24,720
what was the function of Constantine Karateodoris at the Congress of Berlin, where they had to

551
00:53:24,720 --> 00:53:31,520
sign a peace agreement to the treaty between Greece and Western countries versus the Ottoman Empire?

552
00:53:32,320 --> 00:53:38,640
Of course, it saw it because his name was Greek, and he was an ethnic Greek, that he represented

553
00:53:38,640 --> 00:53:46,320
Greece. Who did he represent? The Ottoman Empire. Karateodori Pasha. So you knew immediately,

554
00:53:46,320 --> 00:53:52,160
what does it do? It doesn't know the answer, but it gives you the most likely answer,

555
00:53:53,520 --> 00:53:58,240
okay, and that's exactly what's going to bankrupt you because that's already in the price.

556
00:53:59,200 --> 00:54:04,320
I tripped it. Another one was my village, the Battle of Amun. My village is a Byzantine

557
00:54:04,320 --> 00:54:09,760
village in Lebanon. So there was a battle that happened, according to the record,

558
00:54:10,560 --> 00:54:17,840
some a century after the Arab invasion. So there was a battle between the Byzantine army and the

559
00:54:17,840 --> 00:54:25,600
Maronite, some Christian sect that was pushed up the mountain. So we asked chat GPT, you know,

560
00:54:26,560 --> 00:54:32,080
what happened at the Battle of Amun? And it told you it's between the Islamic invaders and the

561
00:54:32,080 --> 00:54:37,440
thing because it's most likely there's a battle. So when you take the corners, how are you going

562
00:54:37,440 --> 00:54:41,680
to make money? You're not going to make money with an existing idea, but it makes sense, you're not

563
00:54:41,680 --> 00:54:45,040
going to make money because it made sense. A lot of people tried, they failed, and you don't hear

564
00:54:45,040 --> 00:54:53,600
about it, okay, because people don't talk a lot about their failure, right? So that was the idea.

565
00:54:53,680 --> 00:54:59,440
If you look at chat GPT, it cannot come up with a theory of relativity because it's not part of

566
00:54:59,440 --> 00:55:07,840
this course. It would actually dismiss it. You see? So that's why I'm not worried about it.

567
00:55:07,840 --> 00:55:12,560
And you can't run anything, you're just an assistant. It's excellent if you want to write

568
00:55:13,280 --> 00:55:18,720
a condolences letter. It's always very complicated. I have friends who are Muslims,

569
00:55:18,720 --> 00:55:22,720
friends who are Jewish, friends who are Catholic, and then they have to make sure

570
00:55:22,720 --> 00:55:27,680
they use the right wording, you know, like you don't say ad-vitam eternam to the eternal life,

571
00:55:27,680 --> 00:55:33,680
to a Jewish condolence letter. I learned from chat GPT, so you write one, you see, let his memory

572
00:55:33,680 --> 00:55:39,600
be eternal. Okay, so for example, chat GPT is great for that because it gives you the most likely

573
00:55:39,600 --> 00:55:45,360
thing that people say. But if you want to progress, you don't progress by saying the obvious. You

574
00:55:45,360 --> 00:55:49,040
don't progress, you progress only with the corner. So this is where, yeah.

575
00:55:49,040 --> 00:55:58,800
So I can't resist asking you, is it possible then that the great, even call it black swan danger

576
00:55:58,800 --> 00:56:07,120
of chat GPT is, if what you're saying is true, that it's always sort of giving you sort of the

577
00:56:07,120 --> 00:56:12,080
most likely average response. Exactly. The mediocre, the most mediocre.

578
00:56:12,160 --> 00:56:16,240
The most mediocre assistant you can ever have. Right.

579
00:56:16,240 --> 00:56:20,480
So think about it. That's how it's by design, because it reflects.

580
00:56:21,520 --> 00:56:28,880
But if we become more and more and more pleased to have this assistant,

581
00:56:31,040 --> 00:56:38,560
is the difficulty and maybe the danger that we are going to meet them halfway and ourselves

582
00:56:38,560 --> 00:56:45,040
become mediocre. I mean, that's the thing that kind of bothers me. I think, okay, so we are

583
00:56:45,040 --> 00:56:50,560
become, I agree with this, this is our argument about that launch, but let me come in with one

584
00:56:50,560 --> 00:56:56,000
word. Have you heard of, there's a Flaubert's Dictionary of Received Ideas? Yeah.

585
00:56:56,000 --> 00:57:02,000
Well, as a parody, what people would say, you know, that's a received idea, usually, you know,

586
00:57:02,000 --> 00:57:05,920
and you know the crowd is wrong. So it's pretty much like the dictionary received idea. So,

587
00:57:06,000 --> 00:57:15,120
but let me, what happened is that I am, I have a problem. I didn't know I was good in math

588
00:57:15,120 --> 00:57:21,280
because I have a, I can't count as a track very well. I can't divide. So I have, I have, I had a

589
00:57:21,280 --> 00:57:28,160
12C, luckily, and I became a trader with a 12C, I became adapted. So what happened is that I'm

590
00:57:28,160 --> 00:57:36,880
still my 12C calculator, which incidentally, I have now my iPhone 12C. So I can't compute a tip

591
00:57:36,880 --> 00:57:43,920
in a restaurant without it. All right. So the, but it frees you up to do other things. So become

592
00:57:43,920 --> 00:57:49,600
more mediocre at driving. I used to get lost driving home now, definitely without Google Maps,

593
00:57:49,600 --> 00:57:55,200
I get lost, you know, going around the corner. All right. So I have a worse driver than in the past.

594
00:57:55,520 --> 00:58:01,200
Sailors are worse navigators than they were in the town of Columbus, when you have three ships and

595
00:58:01,200 --> 00:58:05,840
how you can follow one another, especially at night, all right, and communicate, all right.

596
00:58:05,840 --> 00:58:13,120
So there were much better sailors than today, but you free up that time, I become very mediocre,

597
00:58:13,120 --> 00:58:19,520
all right, in, in saying that I'm mediocre in what I use that machine, and I'm going to be better

598
00:58:20,480 --> 00:58:26,160
than other things. Okay. So, so this is, this is where technology can free you up to do other

599
00:58:26,160 --> 00:58:31,680
things. And, but people think that chat GPT will replace people. I think what it will do is what

600
00:58:31,680 --> 00:58:39,680
I learned from, from my translators. You know, the translators lie. All right. One of the lies is,

601
00:58:39,680 --> 00:58:43,120
I don't, I don't use Google translate. Never heard of it. I don't know what it is. So

602
00:58:43,120 --> 00:58:51,200
I know some translators used to translate two, three books a year in 2000, when I had my first

603
00:58:51,200 --> 00:59:01,440
book translated. And now the same translators translate seven or eight books. How this is.

604
00:59:01,440 --> 00:59:06,240
So, so Google translate is not replacing translators, but translators are using it

605
00:59:07,200 --> 00:59:11,520
for efficiency. So, you know, for the first cut and stuff like that. And then of course,

606
00:59:11,520 --> 00:59:15,600
they make sure that your text doesn't look like Google translate by, by changing words here and

607
00:59:15,600 --> 00:59:23,040
there or so. But the, so this is pretty much what will happen with when people say that in imaging,

608
00:59:23,040 --> 00:59:31,120
we're going to change GPT, not change GPT, that, that pattern recognition will replace

609
00:59:31,120 --> 00:59:37,600
radiologists. It probably will have fewer radiologists. Okay. Because they can process maybe

610
00:59:37,600 --> 00:59:46,720
a hundred, you know, x-rays a day versus 10. Okay. And, and this is where it's very useful.

611
00:59:47,680 --> 00:59:51,120
Or there are parts of the world that don't have radiologists at all. And that will,

612
00:59:53,040 --> 00:59:56,640
you know, it's part of the world. Yeah, because you'd have one radiologist service a lot more.

613
00:59:57,440 --> 01:00:02,880
So we'll bring down the cost of medicine or make it more efficient, but it will free up medicine

614
01:00:02,880 --> 01:00:08,480
to the other things, you know, like focus on headaches, for example, or maybe curing bad

615
01:00:08,480 --> 01:00:14,480
humor, right? That's to me, whatever, I'll think that are more important, I mean, very important,

616
01:00:14,480 --> 01:00:20,080
but, but completely neglected. So you freeze you up like driving Google Maps, freed me up

617
01:00:20,720 --> 01:00:27,040
to compose maybe, you know, other things and became worse now at composing a spontaneously

618
01:00:27,120 --> 01:00:32,800
condolences letter. But I know now I have a format for, you know, what's Sunni, this is

619
01:00:32,800 --> 01:00:39,360
for the optimal format. This is for Shiite. This is for Maronite. This is for Greek Orthodox.

620
01:00:39,360 --> 01:00:45,680
This is for religious Jew. This is for secular Jew. So I have the format. So you see what, what,

621
01:00:45,680 --> 01:00:51,920
so I'm worse writer for condolences letter, but probably I have more time to write aphorism on

622
01:00:51,920 --> 01:00:57,120
Twitter. All right, so she's I'll have to remember this when I write my thank you letter to you.

623
01:00:58,800 --> 01:01:05,360
We should, we should actually go to audience for questions. Yes, sir, just wait for the

624
01:01:05,360 --> 01:01:08,720
microphone. Someone is going to deliver a microphone to you.

625
01:01:09,680 --> 01:01:24,720
I'm curious if you would comment on how you perceive the possible outcomes given the increasing

626
01:01:24,720 --> 01:01:33,520
US debt and our inability to service it. We have, I think that we're conscious of it.

627
01:01:33,520 --> 01:01:37,440
I mean, this country is very adaptable. Nobody would have thought that would have

628
01:01:37,440 --> 01:01:47,920
5% interest rates, 7.7% mortgages today. And then we went from 2% to 7.7% mortgages.

629
01:01:47,920 --> 01:01:52,160
So some countries are very adaptable. That's the most adaptable probably country on the planet.

630
01:01:53,120 --> 01:01:58,160
So there's one thing about debt that happened. We have had some inflation

631
01:01:58,560 --> 01:02:06,880
that reduced debt in a way. All right. And then also what there's a Lebanese expression.

632
01:02:06,880 --> 01:02:11,760
It got a bit big before it gets smaller. You got to get bigger before it gets smaller. So in

633
01:02:11,760 --> 01:02:19,680
other words, now we realize what's going on with each problem of Congress. And some, some people

634
01:02:19,840 --> 01:02:26,320
are realizing that the thing cannot last long and you can't keep borrowing.

635
01:02:27,120 --> 01:02:33,840
The government has to have some kind of model for to reduce that. But there's a positive thing I

636
01:02:33,840 --> 01:02:43,520
would say is that the government has, because of the zero interest rate policy, has accumulated debt.

637
01:02:44,480 --> 01:02:51,520
But there's a lot of, you know, that, that swelling of assets. That's a lot of profits for

638
01:02:51,520 --> 01:02:57,680
the government because you know, there's such a thing as income tax, capital gains tax.

639
01:02:58,400 --> 01:03:04,560
So government has accumulated assets that people are not noticing during the bubble.

640
01:03:05,440 --> 01:03:14,640
So overall, I'd say that people are conscious of the problem now. And, and I'm glad people are

641
01:03:14,640 --> 01:03:20,560
fighting in Congress. Because on one hand, you have a tension between, it's like optimization

642
01:03:20,560 --> 01:03:25,840
under no constraint. So you optimize social justice. You'd like to, but you need to have

643
01:03:25,840 --> 01:03:30,480
constraints. Like you have a wallet, you know, you have a watch in your wallet. And, and you can't

644
01:03:30,480 --> 01:03:37,440
borrow forever. And so people are conscious of it. And once you have the solution, it would be

645
01:03:37,440 --> 01:03:46,960
probably easier. Plus, we got, I think AI would do something, which is increase productivity in

646
01:03:46,960 --> 01:03:54,400
some domains, as we are noticing, the Google scholar translator productivity. So a lot of

647
01:03:54,400 --> 01:04:02,240
things will come that, but, but so it's not, we're not Japan yet. And other countries will suffer

648
01:04:02,240 --> 01:04:10,240
more before we suffer. I had a question about AI and regulation. A number of AI innovators are

649
01:04:10,240 --> 01:04:14,720
going to Congress and asking for regulation, which looks a lot like Stigler's insight that

650
01:04:15,680 --> 01:04:20,160
firms demand regulation to raise barriers to entry. And of course, highly regulated firms are the

651
01:04:20,160 --> 01:04:25,360
most profitable firms. Would you agree that what's happening is a Stiglerian thing where they're

652
01:04:25,360 --> 01:04:30,080
trying to raise barriers to entry or is there some noble intent? Those clamoring for regulation are

653
01:04:30,080 --> 01:04:38,720
those the most threatened by AI. I mean, I, I was an arbitrage trader. And you know what regulation

654
01:04:38,720 --> 01:04:44,000
means because you give me a country where they have a lot of regulations. You hire three lawyers.

655
01:04:44,720 --> 01:04:50,400
Okay. One lawyer in Japan. Japan was the most regulated financial market. So you hire a lawyer

656
01:04:50,400 --> 01:04:56,480
in Japan, one in London and one in New York. Okay. And then you, the regulation, what does it do?

657
01:04:56,480 --> 01:05:00,960
It causes arbitrage because there's some, you can't short stocks in Japan. So you can make tons

658
01:05:00,960 --> 01:05:06,000
of money, whatever you have regulation, if you love tons of money, finding ways to reproduce the

659
01:05:06,000 --> 01:05:12,000
same product built in another way. Like for example, you can't go short stocks in Japan. So you buy

660
01:05:12,000 --> 01:05:18,880
the index. Okay. You buy all the stocks, but you could short the index. So you have a flat book

661
01:05:18,880 --> 01:05:22,960
and someone wants to short a stock. You said, you short the stock as a huge fee, you know, you

662
01:05:22,960 --> 01:05:28,000
remove one stock from your long. So you have net, a synthetic short, for example. So this is an

663
01:05:28,000 --> 01:05:33,600
elementary trade, but they're more complicated trades. So, so regulations are, I'm for skin in

664
01:05:33,600 --> 01:05:38,320
the game, not regulations. I'm for tort because you can't gain tort. You, you, you, you cause a

665
01:05:38,320 --> 01:05:48,880
problem. You pay for it. Regulations usually allow, I had a fight with this guy. He's at Princeton.

666
01:05:48,880 --> 01:05:55,120
He was last chair of the Fed. Trying to sell. No, no, no, no, another fellow. There's a fellow.

667
01:05:55,120 --> 01:06:03,120
I was in Davos the only time. Sorry. I'm blind. I was in Davos. And, and the fellow say, Oh,

668
01:06:03,200 --> 01:06:08,160
it's incredible. This, how can we protect ourselves? You know, the world I'm talking about.

669
01:06:08,160 --> 01:06:12,640
An American citizen said, you make me go bust. How can I get cash? I'm in Switzerland,

670
01:06:12,640 --> 01:06:19,120
stuff like that. Oh, no, I got protection. What is it? So, you know, FDIC insurance,

671
01:06:19,760 --> 01:06:28,720
they insure you for, for, for per account, not per individual. So you give them $20 million

672
01:06:29,360 --> 01:06:35,280
and then they open up, I don't know, 25 accounts, 50 accounts, something for you.

673
01:06:35,840 --> 01:06:40,240
And then therefore I told them that this is unethical, you know, because basically

674
01:06:40,240 --> 01:06:45,360
rich people can benefit. He said, no, we got a lot of former regulators in our staff.

675
01:06:46,480 --> 01:06:52,240
Then I started the crusade against regulation because I realized that, that regulation allow

676
01:06:52,240 --> 01:06:56,960
regulators to later on sell their services because they know the inside. I mean, some

677
01:06:56,960 --> 01:07:03,920
regulations are necessary, but, but it's like speed limits. Some are necessary, but torts,

678
01:07:03,920 --> 01:07:08,000
all right, are vastly more powerful because it can't be gained. And torts, and that's a left-wing

679
01:07:08,000 --> 01:07:14,320
concept that started with Ralph Nader with, and so you're actually dedicated the book to Ralph

680
01:07:14,320 --> 01:07:20,640
Nader. Torts are vastly more robust than regulations. So I think with AI could be, you can use a

681
01:07:20,640 --> 01:07:25,760
torts system. You say you're responsible, but, but people to regulate, the basic is the regulations,

682
01:07:25,840 --> 01:07:30,720
people calling for regulation with AI either don't understand anything or they are afraid of it

683
01:07:30,720 --> 01:07:36,000
because it hurts their business. But, but think about it. If we slow down AI grows in this country

684
01:07:36,000 --> 01:07:40,720
and the Chinese develop AI, what happens? We're going to have to learn Mandarin now as first

685
01:07:40,720 --> 01:07:45,040
language because basically you're invaded, all right? So you have to realize that there is a,

686
01:07:46,000 --> 01:07:52,320
you know, you can't really stop research on grounds that, that it hurts Elon Musk's business.

687
01:07:52,320 --> 01:07:57,120
Okay. Oh, it's helping Elon Musk. Sorry. It's helping Elon Musk, but he's got,

688
01:07:57,920 --> 01:08:01,840
he wanted, he wanted to regulate it first. He wanted to scare them. Maybe he woke up one day

689
01:08:01,840 --> 01:08:08,000
thought it was bad for him. Right. But he also has one of the most robust AI networks going

690
01:08:08,000 --> 01:08:12,960
with the self-driving cars and the neural net. Yeah, but this is, this is not working. Self-driving

691
01:08:12,960 --> 01:08:17,760
is not working. They are probably, this is, this is where they are, they are some mathematical

692
01:08:17,760 --> 01:08:23,680
thing that is, even if every individual car is self-driving, you see the, the problem is you

693
01:08:23,680 --> 01:08:29,280
have to make them all, all cars on the road got to follow the same protocol. Right. You know,

694
01:08:29,280 --> 01:08:33,680
when you see flocks of birds, they all follow the same protocol. Right. So, so you got it,

695
01:08:33,680 --> 01:08:39,280
instead of doing it bottom up, you got to do it top down for the, for the cars. So this is why I

696
01:08:39,280 --> 01:08:44,240
doubt that it's going to go very far, except for using some things, you know, you know,

697
01:08:45,040 --> 01:08:50,000
temporarily, you know, you can't have a self-driving car as easily as you think. If, if all the cars

698
01:08:50,000 --> 01:08:56,880
were self-driving, okay, you need to have one unified protocol. Right. And, and to get that,

699
01:08:58,320 --> 01:09:03,360
I think maybe your great, great, great, great grandchildren may hear something similar of the

700
01:09:03,360 --> 01:09:11,440
sort. So we have to get rid of human beings to have self-driving cars. Yes.

701
01:09:15,840 --> 01:09:21,760
Wait for the mic. Thanks. First off, thanks so much for, for coming up to, to hand over.

702
01:09:22,560 --> 01:09:28,560
One thing that you've spoken out against before is like reading newspapers. I'd love to hear you

703
01:09:28,560 --> 01:09:32,800
expand a little bit on that. And, and I guess there's a follow-up like how you

704
01:09:32,800 --> 01:09:41,040
suggest staying informed. Okay. So the, the problem that information is that the anecdote is very

705
01:09:41,040 --> 01:09:48,400
salient. The what? The anecdote is very salient. Yeah. So when you read newspapers, you're focusing

706
01:09:48,400 --> 01:09:53,680
on anecdotes and, and early on as it relies as a trader that all these people are talking about

707
01:09:53,680 --> 01:09:58,080
things that don't connect to the importance of the events. You see, like all these analysis that

708
01:09:58,080 --> 01:10:05,360
make no sense. All right. So the, the, so I realized also that the newspaper should be a

709
01:10:05,360 --> 01:10:10,000
thousand page long on some days and one page long on other days, right? In accordance to

710
01:10:10,000 --> 01:10:15,440
statistical significance of what happened, right? Yeah. The same length. So I stopped reading papers

711
01:10:15,440 --> 01:10:19,520
when I became a trader and it's very easy because it freed up time to do other things. And other

712
01:10:19,520 --> 01:10:25,120
people reading the papers or were out of business anyway. So, so that's the idea of the anecdote.

713
01:10:25,120 --> 01:10:29,280
You see anecdotes, you don't see them in context. Okay. And the same thing with the news, but you

714
01:10:29,280 --> 01:10:35,680
develop tricks to only read about large deviations. Large deviations are more, more explainable.

715
01:10:35,680 --> 01:10:39,920
That's something, but the, the market, and they tell you, oh, the market went up 10 points

716
01:10:40,560 --> 01:10:46,080
tiny based on conversations with this and this. And as I showed in the black swan, they said,

717
01:10:46,080 --> 01:10:50,800
bond market up on September capture. And then this one, the afternoon went down,

718
01:10:51,680 --> 01:10:58,080
bond market down on sundown capture. I mean, they look at, so pretty much wasting your time. And

719
01:10:58,080 --> 01:11:04,240
I had this algorithm, if you only cure the newspapers, spend some time reading the previous

720
01:11:04,240 --> 01:11:09,120
year's newspapers. And then you realize how silly it was to read that newspaper.

721
01:11:10,080 --> 01:11:14,480
You know, so, so how much, how much time you wasted. And then you can free up your time to

722
01:11:14,480 --> 01:11:21,760
read other stuff like articles in my professor, I'll be shy, for example. Even in New York,

723
01:11:21,760 --> 01:11:25,520
I said, at that time, I said, don't read the, if you're going to read the newspaper, read the

724
01:11:25,520 --> 01:11:33,680
weekly one. And now I think you should really just yearly newspaper. So the, but the news get to

725
01:11:33,680 --> 01:11:37,520
you organically, if there's something going on, then you know to look for the news, you're not

726
01:11:38,160 --> 01:11:49,520
supplies. Yes, sorry. Hi. So the first thing you said about AI is that you speak in the mic. Oh,

727
01:11:49,520 --> 01:11:54,000
yeah. The first thing you said about AI is that you're not worried because we've had these sorts

728
01:11:54,000 --> 01:11:58,320
of things for a long time, like the thermostat automatically adjusts. But it sort of reminds

729
01:11:58,320 --> 01:12:04,960
me of Dr. Phil's anecdote of the swimming pool. And the thing which worries me about AI is the

730
01:12:04,960 --> 01:12:11,440
possibility of recursive self improvement. And I think that this is a good example of an accelerating

731
01:12:11,440 --> 01:12:17,360
tail event, where for a long time, things seem stable and normal. But once you pass a threshold,

732
01:12:17,360 --> 01:12:22,880
where AI's can recursively self improve, things can get out of hand very quickly. And they are

733
01:12:22,880 --> 01:12:27,120
something worth worrying about, even though, you know, we've had swimming pools for a long time.

734
01:12:27,120 --> 01:12:30,400
We can worry about, we can, I mean, there are things to worry about, like recursive self improvement,

735
01:12:30,640 --> 01:12:35,520
start learning from itself. And then the size, you know, you need to have a lot of things going

736
01:12:36,720 --> 01:12:41,120
wrong for it to start learning from itself and then spontaneously and then

737
01:12:43,200 --> 01:12:46,400
running the world. I think the reverse actually is happening with AI,

738
01:12:48,080 --> 01:12:54,720
language model, that progressively it's actually the reverse is that this progressive self degradation.

739
01:12:54,720 --> 01:12:59,840
And let me tell you how it happens, that you know, when you use chat GDP, it's calibrated to some

740
01:12:59,840 --> 01:13:06,560
information up, say, 2021. And now you use it and you populate the web with information based

741
01:13:06,560 --> 01:13:12,640
on what you got from chat GDP. And then guess what? Chat GDP is learning from itself. So it's more

742
01:13:12,640 --> 01:13:19,760
likely. So the counter to that is I'm more worried about a self-licking lollipop. But basically,

743
01:13:20,640 --> 01:13:26,640
it is recursive in the opposite direction, it's self degradation. So this is what I'd be worried

744
01:13:26,640 --> 01:13:35,840
about was a lot of AI models. So you put that so much better than I did. Does that satisfy you?

745
01:13:37,840 --> 01:13:44,720
I'm not sure. I mean, it's definitely a concern AI is training on their own data. But I think that

746
01:13:45,280 --> 01:13:50,640
within these labs, like open AI, the thing that they understand the most is AI engineering. The

747
01:13:50,640 --> 01:13:54,720
very first thing that they're testing these models trying to get them to do is to help them with their

748
01:13:54,720 --> 01:14:01,600
work. So I do think that there is still a capacity for a recursive self-improvement data.

749
01:14:02,560 --> 01:14:08,720
Yeah. So you'd have evolution. You'd have some system of self-improve at the expense of others

750
01:14:08,720 --> 01:14:15,840
and stuff like that. But I mean, we have bigger problems ahead of time, namely a pandemic. We have

751
01:14:15,840 --> 01:14:25,760
things, how to handle a pandemic like that. We have the fact that zero interest rates destroyed

752
01:14:25,760 --> 01:14:33,200
financial knowledge for half a generation, 15 years, 16 years, generation of trainees and finance.

753
01:14:33,200 --> 01:14:37,840
So we have some sort of lady has a question. Yeah, kids on TikTok too. Yes, go ahead.

754
01:14:39,360 --> 01:14:44,080
I usually, you know, when you allocate, of course you have ladies, but then also if you're going to

755
01:14:44,560 --> 01:14:47,120
take people who don't have hair.

756
01:14:49,600 --> 01:14:57,600
Hi. Thanks for coming here to Dartmouth. During the Arab Spring, I kind of realized that that was

757
01:14:57,600 --> 01:15:02,880
maybe the last time we could ever trust video footage from a scene where things were really

758
01:15:02,880 --> 01:15:11,440
happening. And now it's really, really here. AI can create videos, not only pictures, but live

759
01:15:11,440 --> 01:15:18,320
videos from a single photograph. I think it's going to make everybody a lot more skeptical,

760
01:15:19,200 --> 01:15:23,840
which may be a good thing. But what do you think about that? Okay. So this is a great thing because

761
01:15:23,840 --> 01:15:31,600
I wanted to discuss it today. And at lunch, I told you what I do. So the people have the

762
01:15:31,600 --> 01:15:38,720
feeling that this information is something new that came from social media. Okay. But

763
01:15:39,120 --> 01:15:46,960
you got to realize the French Revolution, you know, the story of the stories of Marie Antoinette,

764
01:15:47,840 --> 01:15:53,280
they were a complete campaign with all fake news and they're produced in London and called

765
01:15:53,280 --> 01:15:59,680
Libels. Well, actually the word libel, I think it's generated from that where people would be,

766
01:15:59,680 --> 01:16:03,280
and they could cancel out of London because they had freedom in London.

767
01:16:04,160 --> 01:16:10,800
And the harming the French was a good thing. Of course, so they were printed in London,

768
01:16:10,800 --> 01:16:18,640
these pamphlets, and crossing, crossed on Fisherman's boats, right, and then supplying

769
01:16:18,640 --> 01:16:24,960
all kinds of fake news. So we have a long history of fake news and how we handle fake news in the

770
01:16:24,960 --> 01:16:36,400
past. And of course, there's a counter to fake news, which is the fact that I, for example,

771
01:16:37,200 --> 01:16:43,680
like to bust stuff. And we have a mechanism to bust stuff. Even Twitter has it. It tells you

772
01:16:43,680 --> 01:16:50,800
that this is not true. And during COVID, people are producing all kinds of fake statistical tricks

773
01:16:51,760 --> 01:16:55,920
that it's easier to bust now than in the past. And you know the Protocol of Sages of Science,

774
01:16:56,960 --> 01:17:02,480
that's the fake news, the producer at the time, that the Protocol of Sages of Science

775
01:17:03,680 --> 01:17:09,040
was a pamphlet produced during the Tsarist era. Actually, the Russians were expert at it. They

776
01:17:09,040 --> 01:17:14,720
had the produced en masse. They produced the Protocol of Sages of Science, say that the Jews

777
01:17:14,720 --> 01:17:20,400
wanted to take over the world. Exactly the same language you hear now about sorrows and a bunch

778
01:17:20,480 --> 01:17:26,800
of people, the World Economic Forum, trying to program you with vaccines so they can, you know,

779
01:17:26,800 --> 01:17:31,280
own the world. Well, for us, fake news is the book of Luke, you know.

780
01:17:36,800 --> 01:17:41,920
So anyway, so we have had fake news about historically. I mean, there's even fake news in

781
01:17:41,920 --> 01:17:51,280
the Talmud, you know, about Jesus being the son of a Roman soldier called Pantera.

782
01:17:52,000 --> 01:17:58,080
Oh, yes, yes, that's right. So Yahushua been Pantera. So there's a lot of fake news. So

783
01:17:58,080 --> 01:18:05,600
historically, so thanks for your question. It's a great question. Can we trust stuff on a web?

784
01:18:06,240 --> 01:18:10,000
And I think that we have an antidote we didn't have in the past.

785
01:18:11,120 --> 01:18:17,280
You cannot believe how many people in Egypt believe in today in the Protocol of Sages of Science.

786
01:18:18,080 --> 01:18:22,080
Whereas if you put the fake news on Twitter, you're going to have people countering it. And

787
01:18:22,080 --> 01:18:26,720
these people develop authority naturally because they spent their time countering it.

788
01:18:26,720 --> 01:18:31,840
I never expected that thanks to COVID, I reached millions of subscribers on Twitter.

789
01:18:32,480 --> 01:18:37,120
And during COVID, the fellow called me up and said, Listen, I'm going to tell you one thing.

790
01:18:37,120 --> 01:18:43,840
I don't like your style. I think you're arrogant and rude. Okay, but I'm going to tell you one thing.

791
01:18:45,280 --> 01:18:51,680
The New York Times, they're boring for life, boring and so on. And people know

792
01:18:52,800 --> 01:18:58,240
that the few attack the fake news, okay, will have more impact. I still, she said, I don't like your

793
01:18:58,240 --> 01:19:05,040
style. I've confirmed, but I need you. So he likes my style visibly. So I play a role, for example,

794
01:19:05,040 --> 01:19:10,640
against fake news. And I tell you, there's this nonsense that you need against PS, you need a

795
01:19:10,640 --> 01:19:20,960
thousand times more effort to remove the SN to put it in. It's not true. I don't believe

796
01:19:21,920 --> 01:19:27,680
the things correct themselves very quickly. Last question. Yes, sir.

797
01:19:31,840 --> 01:19:36,080
How do you get more skin in the game for civil servants, politicians and CEOs?

798
01:19:38,400 --> 01:19:39,520
Do you want to add to that?

799
01:19:39,840 --> 01:19:46,640
Because there's no stock options or things like that. Okay, the first thing,

800
01:19:48,240 --> 01:19:51,520
skin in the game, okay, for politicians, but first of all, you should have limited,

801
01:19:53,200 --> 01:19:59,360
you should have decentralization. That's some of the problem, because if you give more municipal

802
01:19:59,360 --> 01:20:03,200
power, people live in a community. So they may, they already have that more naturalistic skin in

803
01:20:03,200 --> 01:20:09,920
the game. That's how Switzerland works. And then term limits. Okay, so term limits, definitely.

804
01:20:09,920 --> 01:20:14,880
So nobody, you know, they, they also remove some wedge between people become professional

805
01:20:14,880 --> 01:20:24,400
politicians versus normal human being. And so this is how, how it works. And also, I think

806
01:20:24,400 --> 01:20:29,360
polarization, it's not popular to believe that polarization is good because I like politicians

807
01:20:29,440 --> 01:20:36,560
to hate one another. Because then they cast, they can't become a cast that runs us by having

808
01:20:36,560 --> 01:20:43,840
fake, like in Lebanon. In Lebanon, they, they, they love one another secretly. So they have,

809
01:20:43,840 --> 01:20:49,600
so they became a cast, you know, and then they, they start like in Congress, they give themselves

810
01:20:49,600 --> 01:20:54,720
perks, for example. But if they hate one another, they, you know, they, these things become more

811
01:20:54,720 --> 01:21:07,520
difficult. That we're talking about, about the idea of collaboration doesn't work, as well as

812
01:21:08,240 --> 01:21:14,880
competition and adversarial collaboration. So, so, so this is where, you know, I think polarization

813
01:21:14,880 --> 01:21:20,640
is helping. But, but for politicians, the only thing to do is term limit. And then the second

814
01:21:20,640 --> 01:21:27,280
question for educational universities, I think we have a problem with student debt. Okay. And,

815
01:21:27,280 --> 01:21:35,120
and basically, you know, I'm in, I'm a professor, and, and I say openly, you know, in the institution

816
01:21:35,120 --> 01:21:45,600
that there are a lot of real estate developers made a lot of money. Okay. So student debt should be

817
01:21:46,560 --> 01:21:53,680
not be the responsibility of society, but those who made money from it. So institution, if you

818
01:21:53,680 --> 01:22:01,200
make them accountable for student debt, after a while, somehow, then they would probably make,

819
01:22:01,760 --> 01:22:06,960
there'd be a chain, the real culprits are, you know, the culprits are real estate developers.

820
01:22:07,760 --> 01:22:11,600
Okay. That's, that's where the money, the large money went there. And the large money went on

821
01:22:12,160 --> 01:22:18,800
a fat administration with multiplication of administrative positions, like someone responsible

822
01:22:18,800 --> 01:22:24,400
for winter entertainment, one's responsible for improving your life. Like I get this email from

823
01:22:24,400 --> 01:22:29,120
the well being program at NYU. What the hell do I need the well being program from NYU,

824
01:22:29,120 --> 01:22:36,000
buy it from market. So, so you realize when there's money, they spend it. So, so we'll make it more

825
01:22:36,000 --> 01:22:41,440
efficient because the cost of education, when you look at Germany, the cost of education is

826
01:22:41,440 --> 01:22:45,840
something like one or the magnitude less proves the same than the United States. You want the

827
01:22:45,840 --> 01:22:50,880
word of the difference. The difference is partly real estate, partly administration, not so much

828
01:22:50,880 --> 01:23:03,680
faculty. By the way, if you're afraid of faculty liking each other, don't worry. Thank you, Nassim.

829
01:23:03,680 --> 01:23:08,320
It's always such a pleasure to hear you, to think about what you think about. And

830
01:23:09,040 --> 01:23:14,160
such a pleasure that you drove all the way up to Dartmouth. I mean, the leaves are great, but

831
01:23:14,160 --> 01:23:18,800
still, I mean, I'm, I'm really very grateful. We're all very grateful that you came.

832
01:23:18,800 --> 01:23:23,600
This is a great campus. And don't tell anybody it was a great retirement place.

833
01:23:23,600 --> 01:23:25,760
I know, I know. Thank you.

834
01:23:31,200 --> 01:23:31,680
So thanks.


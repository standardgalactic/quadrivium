start	end	text
0	2960	Hello, everyone, and thanks for participating.
2960	4920	My name is Jobs Danke, but I'm by training
4920	7480	physician and mathematician, but I also
7480	10040	study philosophy, I've come back to do
10040	12520	philosophical research work as well.
12520	15680	So today I'm going to talk about artificial intelligence,
15680	18000	intelligent pseudo definitions.
18000	20360	But before doing this, I need to introduce
20360	23360	our view of complex systems, because we
23360	26120	will need this later on in the talk.
26120	29160	So I've been starting with two slides about complex systems,
29160	32800	and then we'll move to the definitions of our intelligence.
32800	36120	So what is a complex system?
36120	39280	So maybe why do we need to understand a complex system?
39280	42320	Because the animal and the human mind body
42320	48560	continue, which produce intelligence, are complex systems.
48560	53080	So even primal intelligence, which
53080	57480	is the intelligence of a bird or a mammalian animal,
57480	60480	the non-human, has primal intelligence
60480	63320	that gets produced by a complex system.
63320	65720	So what is a complex system?
65720	69080	So let's start with the Newtonian system.
69080	73240	So Newtonian systems are systems in which one
73240	76240	can apply models of physics.
76240	81200	And they have become extended quite a bit in the 19th century
81200	84560	with thermodynamics and statistical mechanics.
84560	89480	And then another bit by quantum theory and also
89480	91560	general theory of relativity.
91560	93800	But they still remain Newtonian systems.
93800	96480	So what is a Newtonian system?
96480	98200	But by the way, can you hear me all right?
98200	100400	Does it work?
100400	102960	OK, I assume yes.
102960	107880	So Newtonian systems are made up by a small set of element
107880	108880	types.
108880	111320	For example, the solar system is made up
111320	116200	by the sun and the planets, which are not many elements.
116200	120800	The way the elements interact, they
120800	125920	interact by the four basic forces or interaction types
125920	128040	that are known in physics.
128040	132120	And in this case of the solar system, it's gravitation.
132120	134640	And actually, only gravitation.
134640	137960	All the other forces don't matter for the solar system.
137960	139680	At least not with regard to the way
139680	141920	the planets move around the sun.
141920	146320	And they interact in a uniform and isotropic way.
146320	155440	So the force that is interacting here, the gravitation,
155440	161200	has its effect in a symmetric way all around the sun.
161200	166160	And it is the same everywhere.
166160	168880	I mean, it gets weaker and weaker.
168920	172920	But in a law-like fashion.
172920	174680	Also, there is no force overlay.
174680	177160	So there are other forces, like electromagnetic forces.
177160	180440	There's, for example, light that comes out of the sun.
180440	184720	But it doesn't interact significantly with gravitation.
184720	187480	So if I model the way the planet moves around the sun,
187480	191840	I don't need to take into account other forces than gravitation.
191840	199640	The phase space in which the elements are placed or occur
199640	203080	is deterministic and ergodic.
203080	206480	So ergodicity is shown in a small inset here.
206480	214680	So an ergodic phase space means that all accessible micro space
214680	219640	states of the space are actually probable over a long time.
219640	225640	So basically, in simple words, I can get to everywhere in this space
225640	230360	with the same probability if I wait long enough.
230360	233200	For example, if I have gas in a bottle,
233200	236920	the molecules of the gas will distribute
236920	240800	equi-probably over the volume of the bottle.
240800	245680	And so it has to be such a phase space is ergodic,
245680	248120	whereas there are also non-ergodic spaces to which
248120	251000	we'll get back in a minute.
251000	253800	Such Newtonian systems are non-driven.
253800	263000	Drivenness means that there is no force flowing through the system.
263000	271680	So for example, a steam engine has a driven aspect.
271680	276480	Because all the time while it's driving or under energy,
276480	279680	all the time energy is entering into the steam engine
279680	281320	by the burning of the coals.
281320	283200	And then it's been dissipated.
283200	287160	And in Newtonian systems, that's not the case.
287160	289880	Such systems have no evolution properties.
289880	292840	So they don't obtain new element types.
292840	294800	They have the element types they have.
294800	298600	So yes, this solar system could get a new planet
298600	302080	because there could be a big asteroid, could approach the sun,
302080	304200	and could be attracted by the sun,
304200	310920	and then start to orbit around the sun in the way the planets do.
310920	312720	But that wouldn't be a new element type.
312720	315160	It would just be a new element.
315160	317080	And they have fixed boundary conditions.
317080	321840	That is, if the solar system is four light-years away
321840	325400	from the next solar system, which is alpha centauri,
325400	328520	now if it would just be displaced by one or two light-years,
328520	331960	or even three light-years, this wouldn't change anything.
331960	335560	So basically, I can take the solar system out of its context
335560	338800	and move it away, and it wouldn't change anything.
338800	342840	Of course, if I would move it very, very close to alpha centauri,
342840	345200	then the sun of alpha centauri and our sun
345200	347520	would start to interact by gravitation,
347520	350880	and then very terrible events could happen.
350880	355320	But basically, Ceteris paribus,
355320	358800	I can just take such a Newtonian system out of its context.
358800	361400	Now, complex systems are completely different.
361440	364800	They depend on multiple arbitrary element types.
364800	368520	They have different interaction types between elements.
368520	369640	They have force overlay.
369640	372720	So that means that several forces act at the same time
372720	374680	and also interact.
374680	379200	The phase spaces that they have cannot be predicted
379200	382120	from their system elements, and they are non-ergodic.
382120	386440	So they behave in a way that the microstates
386440	389840	are not accessed over a long time with the same probability.
389840	391240	They're also driven.
391240	393680	So they have inner or external drive.
393680	395840	External drive is, for example, the steam engine
395840	397920	that gets heated from coal.
397920	401040	Internal drive is what humans or bacteria have.
401040	405240	This is the drive to reproduce and also to survive.
405240	408240	And drivenness means that there's a flow of energy
408240	410320	flowing through the system all the time
410320	413080	and that this energy is dissipating.
413080	415160	And they lack an equilibrium state
415160	417080	to which they would constantly be converging.
417080	420320	So a driven system doesn't come to equilibrium.
420320	422800	It's always it goes on.
422800	425400	But when the system, when an organism dies,
425400	428120	then it stops being driven and then it also
428120	431400	converges towards an equilibrium state, which
431400	434040	is, in this case, entropy.
434040	436880	Also, complex systems have evolutionary properties.
436880	439760	So they can evolve new element types.
439760	442160	And they have non-fixable boundary conditions.
442160	444760	So they are context dependent.
444760	449200	You can read this comparison of complex and classically
449200	451720	returning systems in turn at a very good book
451720	454920	about complex systems.
454920	456680	So let's look at some examples.
456680	460040	We have those seven properties of complex systems.
460040	463440	And the solar system has none of these properties
463440	465680	because it's not a complex system.
465680	468480	The steam engine has one property, it is driven.
468480	472160	However, to reason about the steam engine,
472160	475200	in many ways, you can abstract from its drivenness.
475200	479080	So for example, the velocity of the steam engine,
479080	485200	if it's used to drive a train, is
485200	488240	proportional to the pressure that it builds up and so on.
488240	490920	So this property, if it's the only driven property,
490920	492920	you can abstract from it for many predictions
492920	495440	you want to make about the behavior.
495440	502760	Pryon is protein that can infect the brain
502760	504600	and cause damage in the brain.
504600	507480	You have heard of Jacob Kreuzfeld disease.
507480	512480	And you may also heard of bovine spongiform encephalopathy,
512480	514600	which is also a prior disease.
514600	519360	And it has only two complex system properties, namely
519360	523920	force overlay and a non-negotic phase space.
523920	527080	And it has also a non-fixed boundary conditions,
527080	528760	but it lacks all the others.
528760	532440	But then as soon as I get to the virus,
532440	535000	I almost have all the properties.
535000	538120	Viruses, although not driven, because it cannot synthesize
538120	539040	energy.
539040	541440	And then with the most primitive organism,
541440	544040	I have all the properties of a complex system.
544040	550000	So it is important to realize that most systems in nature
550000	553640	are complex or deterministically chaotic.
553640	558240	And so basically, there's only a very little Newtonian systems
558240	559440	out there.
559440	562880	And most of the Newtonian systems that we master
563440	565840	are technically devices that we have designed out there.
565840	567080	All of them are Newtonian.
567080	569600	All of them are built using equations
569600	571400	that we have designed ourselves.
571400	573360	And that's what we really understand and master.
573360	576920	But nature is chaotic and complex.
576920	579520	And humans react irrationally to it often.
579520	583560	So you can, if you think of how, for example, we are now
583560	587000	reacting to this virus.
587000	590840	What it's causing is complex, but the reaction is irrational.
590840	594640	And that's because we feel that we cannot control it.
594640	596840	On the next page, you can see.
596840	599280	So I will give you the opportunity
599280	601440	to ask questions after this slide.
601440	604880	So on the next slide, we see now the problem
604880	608240	that complex systems pose to machine learning algorithms.
608240	610120	So basically, machine learning algorithms
610120	614640	cannot model complex systems because such algorithms
614640	618200	are large auto-parameterized differential equations,
618200	620400	partially auto-parameterized.
620400	623200	And let's look at the problems of machine learning models.
623200	625160	So on the left-hand side, you see the problems
625160	626720	that everybody know.
626720	629440	So that they optimize problem-specific loss
629440	632000	functions that don't generalize well,
632000	635280	that they narrowly depend on the selected training samples
635280	638240	and the specific annotations of these samples,
638240	640360	that they fail upon heterogeneous annotation
640360	641400	of identical input.
641400	644680	So identical input by different output,
644680	646480	it will be very stressful, so to speak.
646480	649520	I mean, it will not train well.
649560	653200	They fail on sparsely-populated sample space parts,
653200	656520	which is very often a very big problem
656520	660600	and that explains why very often machine learning algorithms
660600	661800	need so many samples.
661800	664920	Because if you look at the Curse of Dimensionality,
664920	668160	which you can, for example, read in Trevor Haste's
668160	671480	wonderful book about statistical learning,
671480	673600	you will see in chapter one or chapter two
673600	675440	where he explains the Curse of Dimensionality
675440	678920	that very quickly, you come from a topological perspective
678960	682480	to a very sparsely-populated sample space areas.
682480	685680	And in such areas, the machine doesn't learn anything.
685680	687400	And that's very interesting because humans
687400	691120	and also animals are very good at applying
691120	694120	the intelligence to sparsely-populated sample space.
694120	696600	It's basically, that's the key of intelligence.
696600	700080	Intelligence means the ability to react to new situations.
700080	702840	And machine learning models completely fail in new situations,
702840	705920	which are such sparsely-populated sample space.
705920	707920	They cannot be guaranteed to move
707920	709800	to corrected outcomes.
709800	712640	So if you have an error, erroneous behavior
712640	716800	of such a system, and you try to correct the system
716800	719760	once you've noted the error, it's very hard to guarantee
719760	721480	that it moves to the corrected outcome.
721480	724640	And while it's doing this, it may start to make new mistakes
724640	727320	because remember that the model itself is nothing
727320	730680	but a hyperplane in a k-dimensional space.
730680	734720	And this hyperplane, of course, I mean,
735400	740200	if you change its shape to get a certain effect,
740200	742360	you may get other effects you don't want to.
743400	746280	Such models cannot perceive their own failure, of course.
747280	749240	So they cannot really raise exceptions.
749240	750400	Well, they can raise an exception
750400	755080	if the data don't match the input type, but not much more.
755080	757880	They cannot model far-reaching relationships.
757880	760360	They cannot model semantics of mental type,
760360	761880	objectification semantics.
761880	763640	Here I'm using the German word, I apologize.
763640	768000	So it's object type, mental types they cannot use.
768000	769360	And that's why they fail at image
769360	772480	or language interpretation, they fail completely.
772480	777320	And they, of course, have not a sufficient exactness
777320	778760	for really critical settings.
780360	782760	That's the general problems that have been known
782760	785840	for machine learning, even before neural networks
785840	788720	were invented already, I mean,
788720	790760	before the deep neural networks were invented.
790760	794120	Even in the 1970s, when there was just logistic regression
794120	797240	and perceptron approach, it was already clear
797240	801160	to every statistician that these are the problems
801160	802400	of such models.
802400	804840	On the other hand, there are also problems
804840	808200	that are less well-known, that are described in our book.
808200	810920	There will be no singularity that we expect to publish this year.
810920	814520	It's with the, currently with the publisher for review,
814520	818680	which are related to the fact that human behavior
818720	822840	is emanating from a complex system, the human.
822840	826400	And so if you look at what the problem is here,
826400	831400	first of all, machine learning models require ergodicity
832480	835480	because the samples must be assumed to be drawn
835480	837360	from a representative distribution.
837360	840960	So if you want to train a complex system,
840960	843200	you need to have a representative distribution
843200	845880	because next time you draw a sample,
845880	847480	if it doesn't come from the distribution
847480	849520	which you used to train, the model will fail
849520	851560	because of reason number four.
851560	854920	However, reason number one on the right-hand side
854920	858680	gives you states that complex systems
858680	861200	create non-ergodic distributions.
861200	863880	And so when you draw from, when you make a photo,
863880	865600	so to speak, from a non-complex,
865600	869080	from complex system behavior-derived situation,
870000	873560	this photograph, apparently in quote marks,
873560	875480	is basically always skewed.
875480	876640	That's the most important thing
876640	878360	you have to learn today.
878360	880760	So when you record human behavior,
880760	883200	this behavior is never representative.
883200	884880	Because, so in other words,
884880	887280	there is no multivariate distribution to draw from
887280	889320	because each situation is new.
889320	892120	And the samples that you can draw are always outdated.
892120	896000	So what you do is you train with this huge snap
896000	898040	when you sample from a complex system.
898040	900080	So the sample space is always sparse.
900080	903280	I priori, that's really bad for complex systems
903280	905960	and that's bad for ML algorithms.
905960	906800	They cannot deal with this.
906800	908240	They need repetitive pattern.
909280	911960	Of course, then this is really the take-home message
911960	915080	number one, that if you have a situation
915080	918000	or an environment created by a complex system,
918000	922080	then this is always a non-ergodic distribution
922080	924520	deriving from a non-ergodic process.
924520	927080	And such a distribution can never be represented.
928280	931560	So also they don't have evolutionary property.
931560	933120	They cannot model evolutionary properties
933120	936720	because they model a fixed input-output relationship.
936720	938720	But when you have evolutionary properties,
938720	941200	input-output relationships change all the time.
942880	945760	Because they are differentiable models,
945760	949640	they cannot model non-continuous operators or functionals.
949640	950880	And also they can, of course,
950880	952840	not model non-isotropic forces.
954760	958360	They cannot model elements specific interaction types.
958360	961280	And also force overlay or interaction
961280	963040	is very hard to model.
963040	967680	So for them,
967680	969760	and also they cannot model drivenness,
969760	971960	which is energy dissipation.
971960	975320	To give you a very simple example why this doesn't work,
975320	980320	we haven't found any way to model the flow of water
980440	984240	into, for example, water reservoirs.
984240	985840	So turbulence.
985840	988400	Turbulence is one of the most simplest
990600	992400	natural phenomena of drivenness
992400	995160	because the energy from the water flow dissipates
995160	998600	in the reservoir to which the water is flowing,
998600	1000280	but we cannot model it.
1000280	1002720	And so we cannot model how the energy
1002720	1005480	that is the kinetic energy of the water distributes
1006440	1009080	into the reservoir.
1010160	1015160	And also complex systems cannot model contextuality,
1015320	1018320	sorry, context-free machine learning models,
1018320	1020640	cannot model contextuality.
1020640	1022600	Because complex systems are always contextual,
1022600	1025760	but machine learning models are always context-free.
1025760	1029040	And so you have a discrepancy between the context-freeness
1029040	1030080	of the machine learning model
1030080	1032520	and the contextuality of the complex system.
1032520	1033800	So this is a very,
1033800	1036600	I don't know how long I spoke, maybe 15 minutes,
1036600	1039120	in very short words.
1039120	1044120	That's what complex systems,
1044120	1048360	the problem is complex systems modeling in machine learning.
1051480	1053320	Hello, are you still there?
1053320	1054640	I'm still here.
1054640	1058640	So now the students are required to challenge you.
1062040	1064080	I have a question.
1064080	1064920	Yes.
1066480	1071480	So the issues raised here were the problems
1072320	1076120	of modeling complex systems in the context
1076120	1078080	of neural networks and deep learning.
1078520	1079360	Machine learning.
1079360	1081680	Machine learning, all right.
1081680	1084760	But the thing is humans also haven't been able
1084760	1086560	to model turbulence.
1086560	1088840	And humans haven't been able to,
1088840	1091440	so to an extent we've been able to model
1091440	1092800	weather forecast systems.
1092800	1095320	So we use certain sets of differential equations there.
1097320	1101320	But if ML systems can, to a certain extent,
1101320	1103120	as was mentioned in the slide,
1103120	1105600	auto-parameterized differential equations model
1106600	1110600	some extent of that, then why should it be considered
1110600	1113600	a sign of no intelligence,
1113600	1115600	even though humans have also not been able to do
1115600	1116600	the same thing?
1116600	1121600	So why this requirement from machine learning?
1124600	1128600	So I'm not saying that it is a sign of intelligence.
1128600	1131600	So now we are not talking about intelligence right now.
1131600	1134600	We are only talking about what can be modeled with machine.
1135600	1138600	And what I have not said is I've skipped
1138600	1139600	a little bit of content.
1139600	1143600	So what I've not said is that all the phenomena
1143600	1147600	that are complex in nature can't be modeled using mathematics.
1147600	1150600	So of course we cannot model complex systems mathematics.
1150600	1153600	And so, but that's not because we are not intelligent,
1153600	1157600	but because complex system modeling seems to be
1157600	1161600	beyond the hardware that human intelligence possesses.
1161600	1163600	So but that doesn't mean that humans are not intelligent.
1163600	1165600	It just means that we can't model.
1165600	1167600	There's no way to model complex systems,
1167600	1170600	neither with paper and pen mathematics,
1170600	1173600	nor with whatever kind of algorithm.
1173600	1176600	And so there, but that doesn't mean that we are not intelligent.
1176600	1179600	It just means that this is beyond our modeling capability.
1179600	1182600	But this taken aside,
1182600	1186600	when it comes to creating ML models,
1186600	1191600	and we want to model our, or let's say animal intelligence,
1191600	1194600	then we would have to model the output of a complex system,
1194600	1197600	which not even humans can let alone machines.
1199600	1200600	Okay.
1200600	1201600	All right.
1201600	1204600	But at least humans have been able to approximate certain
1204600	1206600	complex systems like weather forecast systems.
1207600	1208600	So yes.
1208600	1210600	So that's, so go ahead.
1211600	1213600	So my question then would be,
1215600	1219600	what do you think of the potential for machine learning systems
1219600	1222600	to approximate rather than create an exact model?
1222600	1224600	So, so a very good question.
1224600	1228600	So whether forecast models are approximative machine learning models,
1228600	1229600	right?
1229600	1231600	So many of them are built using machine learning.
1231600	1234600	So of course it's possible to approximate certain
1234600	1236600	complex phenomena using machine learning,
1236600	1238600	but it's quite limited what you can achieve.
1238600	1240600	Yeah, you can achieve some.
1240600	1242600	The question is always how good is the approximation
1242600	1244600	and what can you technically do with it?
1244600	1247600	And so if you look at the tetanosphere we have,
1247600	1249600	all the technical gadgets that surround us,
1249600	1252600	that make our life so much easier and better,
1252600	1254600	most of them are exact.
1255600	1258600	So mobile phones are exact, bridges are exact, trains are exact,
1258600	1260600	airplanes are exact, cars are exact.
1260600	1264600	So we don't have so many approximative models in the
1264600	1265600	tetanosphere.
1265600	1268600	So one, so Google, Google advertising plays is
1268600	1271600	approximative, but why can Google afford it?
1271600	1275600	Because nobody gets killed if I get shown a female
1276600	1279600	lipstick ad, right?
1279600	1282600	So nobody gets killed if I get shown a lipstick ad,
1282600	1284600	so they can afford to do it.
1284600	1288600	But if they would use these algorithms in intensive care,
1288600	1291600	unit machinery, they would kill people.
1291600	1292600	So that's the point.
1292600	1295600	The point is approximative modeling is fine,
1295600	1298600	but you have to decide in practice for what purposes
1298600	1300600	you can use it and where you can't use it.
1300600	1301600	Perfect.
1301600	1302600	Thank you.
1302600	1303600	Thank you.
1303600	1304600	Thank you very much.
1306600	1308600	Any more questions?
1314600	1316600	So I guess I'll try.
1316600	1321600	So if you take a, we use this example already a couple of
1321600	1324600	times, if you take a laptop and throw it into a river,
1324600	1329600	then the laptop no longer behaves in such a way that
1329600	1331600	it is a simple system.
1331600	1334600	It starts to decay.
1334600	1337600	And that's because the laptop plus the river is a
1337600	1339600	complex system.
1339600	1342600	Is that a correct account?
1342600	1347600	Well, I mean the system as a whole, so when the laptop
1347600	1351600	drops into the river or creek or whatever,
1351600	1354600	the creek has mechanical energy in the water.
1354600	1359600	And this mechanical energy will start to attack the
1359600	1360600	structure of the laptop.
1360600	1364600	And then of course chemical processes that are not in
1364600	1368600	equilibrium because the water is driven is driven will
1368600	1370600	also attack the laptop.
1370600	1374600	So the laptop in a way will become part of a
1374600	1377600	complex system, but on its own it will not be a
1377600	1378600	complex system.
1378600	1381600	So if you take it out and those forces stop working
1381600	1386600	on it, then it will stay in this more or less in a
1386600	1388600	certain form of decay.
1388600	1391600	Although in the long term, even if you leave a laptop
1391600	1394600	standing somewhere without water, it will also be
1394600	1396600	part of a complex system because there will also be
1396600	1399600	energy working on it, but much less so.
1399600	1402600	So if it's in a building, to begin with it will take
1402600	1404600	one or 200 years before the building is broken and
1404600	1407600	then energy can really work on the laptop.
1407600	1412600	Can you go back to slide two?
1412600	1414600	Yes.
1414600	1417600	So you give C elegans as an example of a complex
1417600	1418600	system.
1418600	1421600	I assume you would give a human being as an example of
1421600	1422600	a complex system.
1422600	1423600	Yeah, of course.
1423600	1424600	Yes.
1424600	1426600	Now, but can you tell me how a human being has
1426600	1428600	evolutionary properties?
1428600	1429600	Yes.
1429600	1435600	So the evolutionary properties of living organisms
1435600	1443600	constitute, come from the fact that new types of
1443600	1448600	molecules can be created in adult organisms.
1448600	1452600	So if you have an adult organism by its ability to
1452600	1455600	react to the environment, it can create new types of
1455600	1459600	macromolecules that it hasn't had before, like new
1459600	1463600	memories and all new combinations of elements.
1463600	1466600	And that's all it can actually change the way it
1466600	1469600	methylates its DNA and therefore change the way it
1469600	1474600	will, the inheritance will work if it becomes
1474600	1477600	progenitor of new organisms.
1477600	1481600	So this is the way that new elements and element
1481600	1486600	interactions can arise in adult organisms.
1486600	1487600	Good.
1487600	1492600	Okay, any more questions from anybody?
1492600	1494600	Yes, I will have one.
1494600	1497600	I believe it's a very boring, usual philosophical
1497600	1502600	question, but when we say that complex systems are
1502600	1503600	not...
1503600	1507600	We don't have a mathematical modeling for them.
1507600	1508600	Is this correct?
1508600	1509600	Yes.
1509600	1512600	Is this intended to be one of our epistemic
1512600	1516600	lacks or something which happens in nature
1516600	1521600	unregarded of some very, very complex mathematical
1521600	1525600	model, which at the moment we don't have, but...
1525600	1526600	This is a very good question.
1526600	1530600	So we don't know this, so we can't really give an answer
1530600	1531600	to this question.
1531600	1532600	It could be speculative.
1532600	1536600	However, my view is that our ability for mathematical
1536600	1540600	modeling is an evolutionary adaptation of humans.
1540600	1544600	So very much like language is the way our hands work,
1544600	1546600	our evolutionary adaptations.
1546600	1549600	And it is a special evolutionary adaptation of our
1549600	1554600	mind, the extent of which also strongly varies
1554600	1555600	between individuals.
1555600	1558600	So there are only out of a thousand, only one or two
1558600	1561600	individuals are usually mathematically gifted.
1561600	1565600	And so it has a high variance, but everybody with an IQ
1565600	1567600	above 80 can count.
1567600	1571600	And this skill is limited.
1571600	1575600	And I think it's limited by the forces that shaped it
1575600	1576600	during evolution.
1576600	1582600	And so I think that certain aspects of nature are just
1582600	1586600	too much, so to speak, for the structure of this skill.
1586600	1589600	And this seems very plausible, given the history of
1589600	1592600	mathematics so far.
1592600	1597600	However, because in the end, all the mathematical objects
1597600	1602600	that we know can be reduced to numbers and are very
1602600	1605600	complicated combinations of them.
1605600	1607600	So even, for example, if the invention of calculus by
1607600	1611600	Newton and Leibniz seems like a very big step, the way
1611600	1613600	they did it was quite geometrical.
1613600	1617600	And great inventions are always great, but it's still
1617600	1620600	linked, of course, to the history of mathematics.
1620600	1623600	So I don't think that we will ever be able to model fully
1623600	1626600	model complex systems, but we were able to, to some extent,
1626600	1632600	of course, approximately model aspects of them.
1632600	1635600	So in other words, I think the structural deficit.
1635600	1636600	Yes, yes.
1636600	1639600	Thank you.
1639600	1641600	I also have another question.
1641600	1642600	Yes.
1642600	1647600	So if we consider the fact that how mathematics have evolved
1647600	1652600	so far, and also the fact that mathematical modeling has
1652600	1658600	up to a point tried to represent, in a way, the randomness
1658600	1665600	that might be an attribute of these complex systems.
1665600	1670600	So we have already made some, find out some mathematical
1670600	1673600	ways through statistics and stochastic processes in order
1673600	1675600	to study randomness.
1675600	1677600	Yeah, of course.
1677600	1682600	And so do you think that this is like a big step, a small
1682600	1686600	step to accomplish something even bigger in the future?
1686600	1689600	Because up until a point, we also thought that randomness
1689600	1692600	was something that we couldn't even explain.
1692600	1695600	And computationally.
1695600	1697600	That's, that's not fully true.
1697600	1702600	For example, there is no mathematical model for a true
1702600	1705600	natural number generator, a random number generator.
1705600	1709600	So a true random number generator, which produces true
1709600	1712600	random events can only be constructed by using a physical
1712600	1716600	device, namely normally one uses a Geiger counter, which is
1716600	1720600	counting radioactive decay events, because they are truly
1720600	1721600	random.
1721600	1723600	So we cannot simulate randomness.
1723600	1726600	Of course, we can model certain aspects of randomness.
1726600	1729600	And this we can do since a couple of hundred years.
1729600	1733600	And at the end of the 1980th century, we have started to
1733600	1737600	understand how calculus can be applied to this.
1737600	1742600	But the way that we model randomness is only applies to
1742600	1744600	classical Newtonian systems.
1744600	1747600	And when we get out of Newtonian systems, there are actually
1747600	1751600	no examples to convincingly model a complex system behavior.
1751600	1755600	So they are, of course, like one of your colleagues mentioned,
1755600	1759600	approximative models of randomness, such as, or
1759600	1763600	stochastic behavior, such as weather forecasting, but they
1763600	1766600	have a very short-term forecast window.
1766600	1769600	And they are not very, depending on in which landscape you are,
1769600	1771600	they almost don't work at all.
1771600	1775600	So near the sea or in the mountains, they almost do not work.
1775600	1779600	And basically, they work for regular continental climate with
1779600	1782600	strong determinants over very short durations.
1782600	1789600	And so the fact that we have probability theory doesn't
1789600	1791600	really help with complex systems.
1791600	1794600	Turbulence, which I already mentioned, is a very good example
1794600	1799600	because the mathematician who had found the best way so far of
1799600	1804600	dealing with turbulence was actually probabilistic
1804600	1807600	theoretician, was Kolmogorov, who invented the Kolmogorov-Smilnov
1807600	1810600	theorem, one of the greatest mathematical geniuses of the
1810600	1811600	20th century.
1811600	1814600	In the 1940s, he tried to model turbulence.
1814600	1817600	And while his model is, you can see it when our book will
1817600	1820600	appear, it's explained in the book, while his model is very
1820600	1822600	aesthetically highly valuable.
1822600	1824600	It's very beautiful.
1824600	1827600	It fails to model the reality of turbulence.
1827600	1832600	And so there is no example for exact modeling or good
1833600	1835600	approximative modeling of complex systems.
1835600	1836600	And I don't know.
1836600	1844600	It's actually, if you know how the theory of probability works,
1844600	1847600	how the distribution are approximated, that it's already
1847600	1851600	very, very hard actually to figure out or mathematically
1851600	1856600	impossible to calculate mixed high-dimensional distributions.
1856600	1859600	You will see that this is very, very far away.
1859600	1862600	I don't know if it's possible.
1862600	1863600	Okay.
1863600	1865600	Thank you for your...
1865600	1866600	Okay.
1866600	1869600	I think we'll have yours to continue now for a bit.
1869600	1872600	So now we look at intelligence.
1872600	1876600	And you will see in the course of the second part why we needed
1876600	1878600	to talk about complex systems.
1878600	1881600	So this here, what you see is the standard definition of the
1881600	1885600	artificial general intelligence community of intelligence.
1885600	1886600	And this is...
1886600	1890600	So they have a verbal definition which goes intelligence
1890600	1893600	measures an agent's ability to achieve goals in a wide range
1893600	1894600	of environments.
1894600	1899600	That's the standard definition that everybody accepts.
1899600	1904600	And so this ability to achieve goals is defined by a utility
1904600	1905600	function.
1905600	1910600	And this utility is here V of an agent pi depending on the
1910600	1912600	environment of the agent's mu.
1912600	1916600	And it's defined as the expectation of the sum of the
1916600	1920600	rewards the agent is going to have, which is actually normed
1920600	1924600	to be equal less to one.
1924600	1928600	And so mu is a binary description of the environment of
1928600	1929600	the agent.
1929600	1933600	And this environment may be a fired abstract structure that
1933600	1935600	is manipulated inside a computer.
1935600	1939600	For example, if you have a theorem prover, then this would
1939600	1942600	be one environment or it may relate to something in the
1942600	1943600	physical world.
1943600	1948600	For example, a nuclear power station that has broken down
1948600	1952600	and where you now want to clean up the nuclear power station
1952600	1953600	with a robot.
1953600	1955600	Because it's not an environment where we would like to use
1955600	1956600	humans.
1956600	1960600	And by the way, there's a great documentation about
1960600	1961600	Chernobyl.
1961600	1963600	They tried to use robots.
1963600	1967600	But the problem was that the transistors broke immediately
1967600	1969600	because radiation was too strong.
1969600	1973600	So the radiation destroyed the transistors inside the robot
1973600	1975600	and then they stopped working.
1975600	1978600	So they had to use humans after all.
1978600	1983600	And in either case, no matter whether it's a physical
1983600	1989600	scenario or artificial one, this vector takes a form of a
1989600	1990600	binary string.
1990600	1997600	And it's a description which plays a role in the HATTA
1997600	1998600	model.
1998600	2001600	And E is the expectation of the rewards.
2001600	2004600	And what is a reward we will see it on the next slide.
2004600	2006600	So this is a basic definition.
2006600	2011600	Now, first of all, let's consider what's that this definition
2011600	2019600	is actually mathematically broken, which I find kind of sad
2019600	2021600	because they should at least get this right.
2021600	2024600	But basically, this is really accepted in the AGI community.
2024600	2027600	Now, one has to see that the AGI community is made up mainly by
2027600	2029600	computer scientists and mathematicians.
2029600	2033600	And computer scientists like physicists and engineers, they
2033600	2036600	tend to not look so carefully at mathematical equations.
2036600	2042600	But actually the definition of expectation is the sum of a
2042600	2046600	variable, which is actually multiplied by the probability of
2046600	2049600	this variable for each step.
2049600	2052600	So every finite amount of steps, or here is actually even
2052600	2056600	infinite, but it doesn't matter if you have some steps, you
2056600	2063600	should calculate the reward of the variable by summing the
2063600	2066600	product of the probability with the variable, which is the
2066600	2067600	reward.
2067600	2069600	And here they don't do this.
2069600	2072600	They actually take the expectation of that is already
2072600	2073600	summed up.
2074600	2078600	So it's funny because if you wanted to implement this, it's
2078600	2081600	actually not implementable because mathematically wrong, which
2081600	2084600	I found quite interesting.
2084600	2088600	But so in other words, the operator E of the expectation has
2088600	2091600	to be applied not to the sum of the values of a random
2091600	2094600	variable, but directly to the values of the variable, as shown
2094600	2096600	here in this equation.
2096600	2099600	And otherwise, you cannot take account of the norming
2099600	2100600	denominator.
2100600	2104600	So pi i is smaller than 1, and therefore it's a denominator.
2104600	2107600	If you multiply by a number smaller than 1, it's the same
2107600	2109600	as dividing by this number.
2109600	2113600	And so usually you need a denominator.
2113600	2115600	And if you don't have a denominator, you cannot get the
2115600	2119600	sum below 1, of course.
2119600	2125600	Because this is just mathematically silly.
2125600	2128600	But they've published it, and the viewers have accepted it,
2128600	2131600	and it gets cited hundreds and hundreds of times.
2131600	2132600	I'm astonished.
2132600	2135600	But anyhow, a lot of crap gets cited a lot.
2135600	2137600	So it seems to be human nature.
2137600	2141600	But it's remarkable that nobody has criticized this.
2141600	2146600	Taking this aside and just ignoring the definition
2146600	2149600	problems and imagining they had used the proper definition
2149600	2153600	of expectation, which is actually you can read up in any
2153600	2156600	very basic textbook of statistics.
2157600	2160600	I mean, actually, it's cool when you learn to calculate
2160600	2164600	the probability of throwing the number 7 with 2 dice.
2164600	2165600	You learn this.
2165600	2167600	So I was kind of astonished.
2167600	2171600	Anyhow, in simplified terms, when we ignore this, the equation
2171600	2175600	1 set in agent pi reacting to environment distribution mu
2175600	2178600	obtains a finite reward, which corresponds to the expectation
2178600	2182600	of reward it achieves over all the steps it undertakes.
2182600	2187600	And actually, the higher this is, the better the utility.
2187600	2192600	And so Barry asked me to insert something about reward.
2192600	2196600	Of course, the reward here has no meaning for the machine.
2196600	2199600	The machine is just a calculating machine.
2199600	2200600	It's a Turing machine.
2200600	2209600	It can just apply amounts of electricity to certain circuits.
2209600	2214600	But by doing this, so reward has no meaning for the machine.
2214600	2218600	That's just a mathematical concept to express an optimization
2218600	2219600	problem.
2219600	2224600	So this year, how can I maximize the utility?
2224600	2229600	I can maximize this utility by doing something with this reward
2229600	2230600	variable.
2230600	2232600	And that's all it says.
2232600	2236600	So reward doesn't mean what is reward for you so that I invite
2236600	2241600	you to a good beer in the evening or anything or bring you
2241600	2244600	some flowers or so, what humans experience as reward.
2244600	2247600	It's just a way of formulating the mathematical optimization
2247600	2249600	problem.
2249600	2250600	OK.
2250600	2256600	So on the next slide, what do they do with this utility?
2256600	2259600	So here you can see this utility term again.
2259600	2262600	But now it is in a bigger equation, which defines intelligence.
2262600	2269600	And this uppercase epsilon, it's a mathematical uppercase epsilon
2269600	2275600	as a function of the agent is defined as the sum of the utility
2275600	2279600	multiplied with another factor.
2279600	2281600	And what is this factor?
2281600	2287600	So this factor contains k, which is a Kolmogorov complexity
2287600	2289600	function.
2289600	2291600	We already heard about Kolmogorov.
2291600	2296600	He did not only work on probability distributions, but also on
2296600	2300600	information theory and the complexity function.
2300600	2304600	He invented it and it's indicating the complexity of the algorithm
2304600	2308600	executed by the agent pi to represent the environment mu.
2308600	2312600	So it's basically how many calculation steps are needed to
2312600	2313600	represent mu.
2313600	2316600	And as you can see, this is a negative exponent.
2316600	2318600	So that means that this is a parallelizing factor.
2318600	2323600	So the more steps I need, the more complex my representation
2323600	2327600	algorithm is, the lower the intelligence will be.
2327600	2330600	And that's pretty, I think, acceptable.
2330600	2335600	Because so if you imagine a more intelligent individual will
2335600	2338600	find it easier to achieve a goal than a stupid individual.
2338600	2341600	So of course, you know this from everyday life.
2341600	2344600	And so this says something similar.
2344600	2349600	It says, so if you have a high utility for a given environment,
2349600	2353600	but I needed a million steps to achieve it, then I'm less
2353600	2357600	intelligent than somebody who achieved the same utility with a
2357600	2360600	shorter number of steps.
2360600	2364600	So further, a couple of more remarks.
2364600	2365600	So mu is the environment.
2365600	2367600	What is uppercase u?
2367600	2370600	Uppercase u is a set of environment descriptions that the
2370600	2372600	machine can process.
2372600	2377600	So for example, if the machine is the AlphaGo machine, then all
2377600	2382600	the situations are settings on the gold board and nothing more.
2382600	2386600	But of course, an AGI agent would be able hopefully to process
2386600	2389600	more different situations that at least what they hope.
2389600	2393600	And so u is just a set of environments the agent could in
2393600	2395600	theory be processing.
2395600	2398600	Let me give you an example from the animal kingdom.
2398600	2404600	So for example, a rat has quite a huge set u because a rat can
2404600	2406600	adapt to very many environments.
2406600	2409600	And therefore, it is also to be found all around the globe
2409600	2413600	all mode, not in the Antarctic and not in the Sahara, I believe,
2413600	2415600	but in many, many moderate environments.
2415600	2419600	A rat is to be found, whereas other animals are very specialized
2419600	2421600	and are only found in certain environments.
2421600	2425600	And this is what this uppercase u is supposed to say.
2426600	2432600	So basically the definition of intelligence shows to you the
2432600	2437600	most efficient possible algorithm to achieve a certain utility.
2437600	2443600	Also, the summation over all environments prevents a random hit.
2443600	2449600	So because you have to perform this in many environments, you can
2449600	2453600	guarantee that the equation doesn't give you a distorted outcome.
2453600	2458600	So if the definition of the utility function would be
2458600	2462600	mathematically sound, which you could easily do by replacing it
2462600	2466600	with this, then the entire equation would mathematically
2466600	2468600	make sense.
2468600	2473600	So it is a proper definition of a weighted utility function.
2473600	2477600	That's what it basically is.
2477600	2482600	Before we take apart these two equations, now I've just
2482600	2489600	criticized them from a mathematical point of view.
2489600	2493600	Are there any questions regarding the two equations before I
2493600	2497600	continue dissecting them?
2497600	2500600	I have two questions when I was looking at this formula.
2500600	2505600	So I was just looking at V as the author says,
2505600	2509600	ability to achieve the value, the capital V.
2509600	2514600	But then in the formula on the second slide, on this one.
2514600	2521600	So why is Gogomolo complexity function as exponential?
2521600	2525600	And the V is just like linear.
2525600	2529600	Because it's just a negative exponent.
2529600	2533600	So it means you just divide.
2533600	2536600	This means 1 divided by 2 exponent k.
2536600	2541600	So it's just used to actually penalize the utility function.
2541600	2547600	But why didn't they just penalize with the inverse value?
2547600	2553600	Why did they choose the exponential growth?
2553600	2556600	To make it perfect, maybe.
2556600	2558600	Yeah, maybe.
2558600	2561600	Actually, I haven't thought about it.
2561600	2565600	But it's basically the way that the Gogomolo complexity function
2565600	2569600	always gets applied in theoretical informatics.
2569600	2574600	So this form is when you read a textbook of theoretical informatics,
2574600	2577600	you always see it represented like this.
2577600	2583600	So that you get a smooth representation of the entire function.
2583600	2592600	So I think it works pretty well to weigh whatever you want to weigh
2592600	2596600	by the amount of work that you need to put into obtain it.
2596600	2600600	Yeah, they argue that extensively in the paper anyway.
2600600	2606600	But the second question is this capital U, the set of environment descriptions.
2606600	2608600	How is this supposed to be interpreted?
2608600	2611600	Because it's written as a sum.
2611600	2617600	But I would take that in general, this is like a manifold in the best case.
2617600	2620600	So it's a sum over the elements of a set.
2620600	2621600	That's fine mathematically.
2621600	2627600	So this is just an index of all the possible terms you can get.
2627600	2632600	And let's say you would have three different environments.
2632600	2639600	What if it has a ball or a sphere of different environments?
2639600	2641600	So you just integrate?
2641600	2643600	No, I don't think so.
2643600	2648600	Because you basically can always, this is theoretical informatics.
2648600	2653600	So that means that you can, no matter what from a functional analysis point of view,
2653600	2657600	no matter what is the way the environments are,
2657600	2660600	they are always compressed into binary vectors.
2660600	2666600	And then you can just create a series of binary vectors or a set of binary vectors.
2666600	2668600	And this is just one binary vector out of a set.
2668600	2672600	So it's a countable set of binary vectors, no matter what.
2672600	2678600	Maybe you should explain what a binary vector is.
2678600	2684600	Yeah, so for the non-n estimaticians, it's just a vector of ones and zeros.
2684600	2690600	So remember that a Turing machine can only deal with ones and zeros.
2690600	2696600	It's like a big tape on which you can write ones and zeros and can change the ones and zeros.
2696600	2701600	And this is basically, from the perspective of theoretical informatics,
2701600	2706600	it's just a set of such binary vectors.
2706600	2712600	Sorry, I think it would have been a bit more helpful if the input and output spaces were also defined.
2712600	2718600	Because now it's clear to me what, so the inputs are basically strings, as far as I understand it.
2718600	2720600	Is it?
2720600	2724600	Yeah, so otherwise the mu is a bit hard to interpret.
2724600	2726600	Yeah, the mu, yeah, you're right.
2726600	2734600	So I have not defined, actually it's interesting why I've just copied the definition from their paper,
2734600	2742600	but you're right, usually one would have to use a functional analysis type of definition of the input space at the output.
2742600	2744600	Yeah, exactly.
2744600	2750600	And I could easily have done this, but there are two reasons why they didn't do it for sloppiness,
2750600	2756600	and I didn't do it because it's just too implicit for me because it's what I do all the time, but thank you.
2756600	2760600	I think we should add this next time, it's a good point.
2760600	2761600	Yeah.
2761600	2766600	Okay, so anyhow, this is a definition.
2766600	2769600	Now let's move on and look at the problem.
2769600	2774600	So first of all, there is a verbose definition before the equation,
2774600	2779600	which says intelligence measures an agent's ability to achieve goals in a wide range of environments.
2779600	2787600	So first of all, the definition captures just one part of one of the standard definitions of primary intelligence.
2787600	2798600	So primary intelligence says that you have to adapt to new environments suddenly and without being trained upfront.
2798600	2805600	So you have to be spontaneously able to adapt to a new environment suddenly, quickly.
2805600	2812600	And this definition just captures the adaptation.
2812600	2819600	Now, what is more important is that the definition is very broad because it allows also this organism,
2820600	2828600	a small worm that is one millimeter long and has a thousand cells and only 300 neurons to be intelligent,
2828600	2832600	because what it can do, it can forage and reproduce in complex environments,
2832600	2838600	so it can live in fruit, in vegetables, in mushrooms, in soil, and so on.
2838600	2841600	It can use snails and stugs as migration vectors.
2841600	2848600	So it can really live in many environments and it can also reproduce there.
2848600	2854600	So I think it would be intelligent, according to the AGI definition.
2854600	2858600	And even probably the amoeba here, this is a nice amoeba.
2858600	2863600	It's called kaos kaolinensis because it never has the same shape.
2863600	2870600	Here's just a drawing of it, but in the next second it will look different because it moves around by changing its shape.
2870600	2875600	And it can also live in many, many environments and thrive wonderfully there.
2875600	2881600	It can reproduce, it can find food, but it's just one cell.
2881600	2886600	So it's one of the most primitive, well, it's the most primitive eukaryotes,
2886600	2890600	but directly next to yeast comes already this amoeba.
2890600	2892600	It would also be intelligent in this definition.
2892600	2895600	So I think the definition of intelligence is too weak.
2895600	2896600	Why is it so weak?
2896600	2901600	Because in the book where this intelligence definition is used,
2901600	2908600	that's the book by Gtze and Pinache, which is called Artificial General Intelligence,
2908600	2911600	they discuss many other definitions.
2911600	2917600	But the problem is that if they use definition like the one very simple one of human intelligence,
2917600	2923600	intelligence is the capability that enables us to speak, for example.
2923600	2927600	That enables the language that humans can use.
2927600	2931600	I mean, it's not my definition, but if you would, but they propose this definition,
2931600	2936600	then they would automatically fail in generating artificial intelligence.
2936600	2940600	So they've basically chosen a definition that doesn't yield intelligence,
2940600	2944600	so that they claim now that they have an intelligence.
2944600	2949600	But they've actually on purpose used a very stupid definition that is not intelligence.
2949600	2959600	Now let's go and look at what time it is and look at other problems of the AGI definition.
2959600	2961600	So let's first look at perception.
2961600	2967600	So if you have this vector mu as a measure of complexity of environment,
2967600	2974600	this vector mu presupposes that the environment can be represented by using a binary vector.
2974600	2978600	In some artificial environments, such a binary representation may be adequate,
2978600	2983600	but in natural environments, we have signals emanating from complex systems.
2983600	2993600	And therefore, the signals need to be actively interpreted and reassessed all the time.
2993600	2997600	And also the observation needs to be continuously adapted to the input
2997600	3002600	as the agent takes account of the interpretation of each antecedent observation.
3002600	3012600	And also an animal interpretation depends on previously experienced mental material,
3012600	3013600	so memories.
3013600	3021600	And for example, this tiger observing the prey, it does actually all the time update its observations.
3021600	3025600	It does active perception or shields, I think a female tiger.
3025600	3029600	And if there would be a puppet next to a young tiger,
3029600	3034600	the young tiger would not be as good at observing those animals, those prey animals,
3034600	3036600	because it has that experience.
3036600	3042600	So the experience stored in the memory of the tiger also helps it to react better to what the animals are doing
3042600	3047600	and to single out, for example, one animal to hunt it down.
3047600	3052600	So the predator, if the predator is just sitting there and observing the prey,
3052600	3057600	it's already acting to improve and adapt the perception of the prey.
3057600	3062600	So perception is not static, but a dynamic process of constant iterative feedback loops
3062600	3068600	between sensory and motor neuron circuits, and we cannot even know.
3068600	3074600	So if we think of training an ML algorithm, we need to know the tuples that we used to train,
3074600	3078600	but we don't even know when the cycle begins and ends.
3078600	3083600	So the cycles can be very fast, and we don't know because we can only observe the overall behavior,
3083600	3091600	but we don't know how to determine the tuples that constitute the perception process.
3091600	3097600	So our JJ Gibson, a very important psychologist and philosopher says,
3097600	3101600	normal activity of perception is to explore the world.
3101600	3104600	So perception depends on more than just sensory stimulus.
3104600	3110600	So the view that perception is just a result of sensory stimulus is completely outdated.
3110600	3115600	So when we give input to computers, it's just sensory input, but that's not real.
3115600	3117600	That's not perception.
3117600	3122600	Perception requires purposeful activity, direct manipulation of the object,
3122600	3127600	and innate or quiet knowledge of the expected patterns of reality.
3127600	3137600	So I need categorical, predefined ability to deal with the environment and also quiet knowledge.
3137600	3142600	And this manipulation of the object doesn't mean that I need to touch them,
3142600	3151600	but the tiger can also manipulate these animals, prey animals in her imagination.
3151600	3156600	So she can imagine that maybe this animal would now lean down to drink from water source
3156600	3161600	and whether then maybe it would be a good moment to attack this animal and so on.
3162600	3168600	So this is highly interactive and mu, this static vector mu,
3168600	3174600	which basically models, for example, the input from a sensor doesn't capture any of this,
3174600	3177600	does not capture any of what I've just said about perception,
3177600	3180600	what we know about animal and human perception.
3180600	3186600	Here's another example of perception, which is much more complex than this one
3186600	3190600	because it involves dialogue, it involves observation of a dialogue,
3190600	3198600	it involves probably, yeah, it has many, many interesting aspects
3198600	3202600	that show how complicated perception is.
3202600	3211600	And so, for example, what does Tony Curtis, who is here acting as a woman,
3211600	3213600	think about Marilyn Monroe in this moment?
3213600	3216600	I don't know, but it's certainly an interesting question.
3216600	3223600	And anyhow, so perception is not modeled by this environment variable mu.
3223600	3228600	On the next slide, we see the next problem, which is activity.
3228600	3235600	So the steps of the utility function that you've seen here are results of,
3235600	3238600	so each step is an activity which yields a reward.
3238600	3243600	So in chess or in Go, you get a reward for making a certain move.
3243600	3247600	Now, the steps of the utility function that are described by the Hutter definition
3247600	3254600	of artificial intelligence, there are a linear sequence of discrete machine actions.
3254600	3261600	But human motor acts are actually interactions of perception motor activity.
3261600	3265600	They involve at every stage a dense synergy of multiple body systems
3265600	3267600	at multiple levels of granularity.
3267600	3274600	And that's, for example, if you think of human manufacturing activities
3274600	3279600	where they have to use, like surgery, when you have to actually
3279600	3286600	feel very exactly what you're doing or in certain steps in the construction
3286600	3290600	of even of cars, fine motor, it's called in German.
3290600	3293600	Barry, do you know the word in English?
3294600	3299600	I don't. Sorry. Precision engineering would be...
3299600	3301600	Yeah, probably.
3301600	3306600	So we don't know how to make machines do this because we don't know
3306600	3313600	how the circuitry between a perception and motor action work.
3313600	3316600	So we know that there is some feedback loop, blah, blah, blah,
3316600	3318600	but we don't know the details of it.
3318600	3323600	And if you look at the most advanced textbooks or papers that are available
3323600	3332600	about animal fine motor action, we have no clue how the animals
3332600	3335600	do these fine motor actions or know how we do it.
3335600	3337600	We have no models for it.
3337600	3342600	And so because we have no models for it, we cannot do it in a computer.
3342600	3347600	And so the activities that happen in real environments
3347600	3352600	are much more complicated than those linear sequence of steps.
3352600	3357600	And the interaction between sensory and motor activity
3357600	3361600	could potentially scrub the linear sequence of effort, effort,
3361600	3365600	and interest in neural signal events, but the coupling of such a sequence
3365600	3368600	to any sort of reward is very indirect.
3368600	3373600	So yes, in reality, of course, those things happen one after the other,
3373600	3376600	but very, very quickly in a very complex session.
3376600	3379600	And so we don't know how to cover this to reward.
3379600	3385600	And so probably the sequence in reward terms that we need
3385600	3391600	would probably not be possible to construct the correct reward sequence.
3391600	3394600	Another aspect is that mammals can overcome
3394600	3396600	massive negative rewards to achieve the goal.
3396600	3401600	So here I have an example from animal psychology.
3401600	3404600	So it's a cocaine self-administration experiment
3404600	3409600	where the rats to get cocaine, they have to traverse a heated plate,
3409600	3411600	which is burning their feet.
3411600	3414600	So they have to damage themselves to get to the cocaine,
3414600	3420600	but they still do it because they have primary intelligence.
3420600	3424600	So they know that they must cross the heat plate to get to the cocaine,
3424600	3429600	that they must press a button to get the administration of cocaine.
3429600	3432600	And they get a short-term reward for this,
3432600	3435600	but they don't have a net long-term reward.
3435600	3441600	And so such a behavior is very, very hard to model with a reward function.
3441600	3446600	And if you think many of you are maybe in the middle of their PhD thesis,
3446600	3452600	so the PhD thesis process is not like the cocaine self-administration,
3452600	3454600	but there's a lot of negative stuff you have to cope with
3454600	3458600	over a long time before you get a reward, that's for sure.
3458600	3462600	And I don't know how this can be modeled with such a reward model,
3462600	3464600	at least being very hard.
3464600	3466600	Speaking of reward,
3466600	3471600	so what is even more important is that the reward pattern
3471600	3474600	that we see is some of a reward.
3474600	3481600	It is actually unable to model reward patterns that we encounter in real life,
3481600	3486600	because first of all, the system that certifies the harder definition
3486600	3488600	will always be situation-specific.
3488600	3492600	So it will work only in the context where a human has already been at work
3492600	3495600	in preparing appropriate rewards.
3495600	3497600	There is no general or universal reward.
3497600	3502600	So the reward, for example, that the machine receives for playing the game of Go,
3502600	3504600	are points.
3504600	3507600	And the algorithm is trying to find a functional or an operator
3507600	3509600	that maximizes the number of points.
3509600	3514600	So it's just a derivative of a very long equation that you have to find.
3515600	3520600	And that has nothing to do with universal or general intelligence.
3520600	3525600	The reason is that the mathematical definition that Hatter provides
3525600	3531600	for the environment mu that must be matched by the reward.
3531600	3536600	So it's not possible to find a reward that works for all environments,
3536600	3539600	whereas humans have as a reward,
3539600	3544600	the main rewards are to survive and to reproduce.
3544600	3553600	And survival and reproduction can come very indirectly in highly evolved societies.
3553600	3559600	So to sit in a room and do mathematical equations all day long
3559600	3563600	for survival and reproduction, that's quite abstract
3563600	3567600	or to do paintings of art or to compose music.
3567600	3576600	And so there is a way of humans to delay the reward of reproduction and survival
3576600	3582600	very far off and to do activities that seem to be non-connected to it.
3582600	3587600	And that seems to be at least required for objectifying intelligence or human intelligence.
3587600	3592600	And we don't know how we can model this with reward.
3592600	3602600	Also further problem arises that the assumption is made that all rewards of one agent
3602600	3608600	must be of the same type for every step under a given environment.
3608600	3613600	So if we go back to the equation, r is only indexed by the step,
3613600	3618600	but it cannot change its type, otherwise it would need the second index.
3618600	3625600	So there's only one type. Yes, this type can obtain different values,
3625600	3630600	but probably in animal and human behavior there are many different types of rewards.
3630600	3634600	So we cannot model this.
3634600	3637600	Then the question is, couldn't we create a sequence of rewards
3637600	3641600	adequate for learning the behavior of a complex system?
3641600	3645600	So the problem here would be that such a reward sequence,
3645600	3648600	you would then imagine many, many rewards in a sequence
3648600	3652600	and they would have to give situation-specific rewards.
3652600	3656600	That's because each step on a complex system model trajectory
3656600	3661600	would have to be able to deal with an unexpected situation
3661600	3665600	because when the AI system interacts with its own environment,
3665600	3669600	that will change the environment.
3669600	3674600	And so this change of the environment will create an unexpected situation
3674600	3676600	and may require different reward.
3676600	3680600	So you cannot really find a reward part or trajectory
3680600	3684600	because at each step a different reward would be needed to correspond
3684600	3688600	to the emanations from the complex system that form the environment.
3688600	3693600	So that has to do with the evolutionary character of complex systems.
3693600	3698600	And such a temporal reward sequence would obviously not follow a Markovian pattern.
3698600	3703600	And of course, speaking of probability theory, if we go back to this slide here,
3703600	3710600	Markovian pattern is of course a pattern that applies only to classical Newtonian systems.
3710600	3713600	Only Newtonian systems have the Markov property.
3713600	3718600	Complex systems don't have the Markov property, but without the Markov property,
3718600	3723600	you cannot achieve predictive modeling in stochastic equations.
3723600	3727600	So stochastic differential equations or other stochastic process models,
3727600	3729600	they always need a Markov pattern.
3729600	3740600	And without the Markov pattern in which the reward would depend only on the previous or some previous steps.
3740600	3744600	But if you don't have this pattern, but you have a dependency on many earlier steps
3744600	3748600	and long-term dispositions of the organism and also short-term intentions,
3748600	3751600	you can't find a reward sequence.
3751600	3764600	And so, therefore, the reward sequence would need to correspond to complex emanations
3764600	3768600	relating to situations varying as a successive test unfold.
3768600	3773600	And therefore, the reward sequence itself has to be complex
3773600	3777600	and thereby it would have all the properties of a complex system emanation.
3777600	3785600	So that's the interesting thing, that to give rewards to an intelligent system in a complex setting,
3785600	3790600	the reward sequence itself would have all the properties of a complex system emanation.
3790600	3796600	So it would have the same properties that my stream of language that I'm currently giving to you has.
3796600	3801600	And we have no mathematical models to create such sequences
3801600	3806600	because we have no mathematical models for complex system emanations.
3806600	3813600	And so we couldn't create the reward sequence that would be needed for the intelligent system
3813600	3816600	to cope with a complex system situation.
3816600	3820600	And that's a problem, I think, a very important problem to see this,
3820600	3825600	that the reward approach, why reward is mathematically attractive
3825600	3834600	because it allows to state the intelligence problem like an optimization problem for optimization theory.
3834600	3840600	It doesn't create a realistic sequence of rewards that correspond in any way
3840600	3845600	to what we experience or animals experience when they obtain rewards for their behavior.
3845600	3851600	Actually, the reward that an animal and foraging animal receives, which is the food
3851600	3854600	and for which the animal does very interesting things.
3854600	3858600	So if you read modern books about foraging, it has been found out that parrots, for example,
3858600	3871600	invent new patterns of shouts and vocal noises they make to describe sources for food.
3871600	3878600	And while they search for those food sources, which they have to do every day,
3878600	3881600	they create those sound patterns.
3881600	3885600	And this process is highly complex and it has nothing to do.
3885600	3890600	And the way they get to the reward has nothing to do at all with linear reward sequence
3890600	3893600	that we see in those models.
3893600	3896600	And so that's why I call these pseudo-definitions of intelligence
3896600	3906600	because they just define something that can be actually put into a model optimization algorithm,
3906600	3914600	a numerical model optimization algorithm, like a dual or something like this.
3914600	3920600	But in reality, it has nothing to do with real intelligence, not even animal intelligence.
3920600	3926600	Now, I think before I go to the last slide, I would like to give you the opportunity
3926600	3930600	to ask one more round of questions.
3930600	3933600	Yeah, I have one question, Professor.
3934600	3943600	The thing is, so here the argument is that the reward sequence is extremely complex
3943600	3948600	and since it is not Markovian, you cannot somehow model this.
3948600	3954600	Now, if I were to take the example of a robot in a factory,
3954600	3957600	let's say a robot that knows how to load boxes, unload boxes,
3957600	3961600	get the boxes onto some other conveyor belt, et cetera, et cetera.
3961600	3966600	Currently, there are reinforcement learning algorithms that are able to do this
3966600	3972600	pretty effectively in these factories, just replacing entirely with robots that do this work.
3972600	3976600	So there, of course, the reward sequence is very clear
3976600	3981600	because you have just one mechanistic task that you keep doing throughout your life,
3981600	3984600	well, throughout your robot life.
3984600	3994600	My question here was, what if we could create a set of rewards for different tasks
3994600	3997600	and model that together into a specific robot?
3997600	4000600	So, for example, let's say loading and loading boxes, opening doors,
4000600	4003600	walking, sitting down, different movements.
4003600	4007600	Now, I'm not suggesting for a single second that humans learn this
4007600	4010600	by this sort of discretized reward sequence.
4010600	4015600	I'm not suggesting this at all because I don't know how humans learn it.
4015600	4018600	But as far as teaching robots that is concerned,
4018600	4023600	don't you think the reward sequence then would just be a set of different rewards
4023600	4027600	for different tasks, but they would all be modeled into the same algorithm.
4027600	4031600	So, in a sense, what I'm arguing is the whole is the sum of its parts.
4031600	4033600	That's what I'm trying to say.
4033600	4039600	So, you're actually already giving a small preview of the next slide.
4039600	4043600	So, it's never what you're saying.
4043600	4049600	So, basically, what you describe is, of course, that what is a factory?
4049600	4052600	A factory is a Newtonian system.
4052600	4055600	So, a factory has all those properties.
4055600	4061600	And so, therefore, machine learning can be very, very efficient
4061600	4067600	in implicitly modeling Newtonian equations, motion equations, also sensory equations.
4067600	4074600	So, therefore, because in a factory, all processes have the Markov property
4074600	4079600	and because there's no force overlay, the elements are well defined and so on,
4079600	4081600	that will work pretty well.
4081600	4086600	And that's why also machine learning is, I think, the best application of machine learning
4086600	4092600	that is currently not visible very much is actually manufacturing and mining
4092600	4098600	and other mechanical activities where humans still play a role,
4098600	4103600	but where they will with the exception of this fine motor or sensoric behavior,
4103600	4108600	which is very hard to model, they will be replaced by robots more and more.
4108600	4114600	You're completely right, and that's because here, machine learning is used to model Newtonian systems.
4114600	4120600	That's the first answer. The second answer is that it's essentially the way to build AI properly
4120600	4126600	is to do it as you described, and I will come to this once the other questions are answered.
4126600	4132600	Oh, okay, okay. Thank you so much.
4132600	4134600	Is there any?
4134600	4138600	I have one, but I'll give the students chance to butt in first.
4147600	4153600	I would just remark maybe that the authors were aware of this argument that you are saying
4153600	4162600	because they said that at no point they are trying to compare their definition to human intelligence.
4162600	4172600	I think they perceive the machine intelligence just as a composite of different tasks
4172600	4176600	as Ravidi was saying earlier.
4176600	4183600	If you read Hutter's and Schmitt-Huber's papers about general intelligence, I disagree.
4183600	4188600	They believe that they can create general intelligence, and many of them, I don't know whether Schmitt-Huber does,
4188600	4195600	but many of them also believe in the singularity, which I think shows.
4195600	4201600	I think that you should really get this AGI book, or I can send it to you, or Barry can send it to you,
4201600	4211600	this AGI book by Gertse Penachin, which is one of the most important consensus readers
4211600	4214600	where all the big shots of AGI have published papers.
4214600	4219600	Yes, of course they say that it's not human intelligence, but they say it's a real intelligence.
4219600	4226600	What I'm saying is no, what you're suggesting is not a real intelligence, but it's just a kind of amoeba intelligence,
4226600	4228600	and probably not even that.
4228600	4236600	I think that because on the next slide you will see how I define what one can do with AGI,
4236600	4241600	and when I show this to people from this community, I get heavily attacked.
4241600	4246600	Oh, that's old school, we can do much better, we can do general intelligence, and so on.
4246600	4257600	If I understood correctly, they were also saying that, of course, the Chinese argument is completely right.
4257600	4267600	I think where they were defending the critique, maybe that's at the end.
4268600	4272600	But I don't even argue with the Chinese room argument here.
4272600	4280600	I just say that even the most basic form of intelligence that we as humans perceive as intelligent
4280600	4289600	would just see the behavior of a dog or another mammal, that this behavior cannot never be achieved by these equations.
4289600	4296600	So what can be achieved by these equations is actually what neural networks already do,
4296600	4307600	and that is because this here is a recipe to actually...
4307600	4315600	Yeah, and so loss function minimizes loss, and here we maximize reward, but it's basically optimization, prescription,
4315600	4324600	and we achieve this by applying these models to achieve what is shown on slide three, with all the pros and cons.
4324600	4329600	Now, I'm saying that these ML models are very useful, but they have nothing to do with intelligence,
4329600	4334600	and that's basically...
4334600	4339600	I mean, when I discuss, you know that I have an AI company, when I discuss with clever customers,
4339600	4343600	I mean their customers who want to buy intelligence, and I let them under the illusion,
4343600	4348600	but others who are clever to say, but that's not really intelligence, I say, no, these are artificial instincts.
4349600	4358600	And this is basically what we do, what we still do, even if we use such optimization procedures as those described by these models here,
4358600	4361600	we still only obtain a narrow AI.
4361600	4365600	And with this, Barry, you had a question before I go through the narrow AI.
4365600	4369600	Yes, so I have a number of questions now, unfortunately.
4369600	4373600	So what would leg Hutter say to the following objection?
4373600	4379600	You define intelligence as the ability to achieve rewards in a wide range of environments,
4379600	4385600	but AlphaGo can only achieve rewards in one kind of environment, which is the Go board.
4385600	4387600	Yeah.
4387600	4390600	So it's not a wide range at all.
4390600	4392600	Yeah, yeah.
4392600	4396600	Yeah, I believe that even actually, you're right.
4396600	4402600	So I believe that even this definition, intelligence so will not be achievable.
4402600	4407600	So the verbal definition is actually in conflict with the equations.
4407600	4409600	Yeah, you're right.
4409600	4412600	And now just a little correction, sorry.
4412600	4414600	Sorry, just a little correction.
4414600	4419600	AlphaZero actually can succeed in a variety of game environments.
4419600	4423600	AlphaZero can play chess, it can play Go, it can play checker.
4423600	4425600	I'm just talking about AlphaGo.
4425600	4427600	You're right.
4427600	4430600	Not AlphaGo, but AlphaZero, which was recently released.
4430600	4433600	Yeah, AlphaZero can play different games.
4433600	4437600	And actually, there was already in 2014 a precursor for AlphaZero,
4437600	4444600	which could play all the Atari games, which was also trained with reward learning.
4444600	4448600	It could play strategic games, but it could play Pong and all the ones
4448600	4450600	that give you a series of points.
4450600	4454600	But now OpenAI has come out with StarCraft games too.
4454600	4457600	So they are getting better at these complex environments.
4457600	4461600	Well, these are actually not complex environments.
4461600	4465600	So these are still Newtonian environments.
4465600	4471600	Because the reason this is very important, if you could hold on with your questions a minute.
4471600	4475600	So the reason why these are Newtonian environments is that also,
4475600	4479600	for example, games like strategy games, like civilization,
4479600	4482600	they have also been beat by reinforcement learning.
4482600	4484600	But the reason is, of course, what is civilization?
4484600	4491600	Civilization is a set of rules and equations applied to a certain pattern.
4491600	4495600	And even if this pattern is created by random, it's created as a multivariate
4495600	4498600	random distribution, usually multivariate.
4498600	4501600	And so it's multivariate Gaussian.
4501600	4505600	And then, of course, you can take points from this distribution
4505600	4507600	and put them into a set of rules.
4507600	4512600	Then you create a very complex set of events, but it's still a Newtonian universe.
4512600	4516600	And therefore, you can also train in the same place for the ego shooter games.
4516600	4520600	And therefore, you can train AI that is beating this.
4520600	4525600	But the point is, since 1950, we have heard predictions about free-moving robots.
4525600	4527600	Why don't we have them?
4527600	4529600	I mean, we have them, of course, in controlled environments.
4529600	4533600	But why do we never encounter free-moving robots in our streets?
4533600	4536600	Because these are real complex environments.
4536600	4538600	Yeah, yeah, yeah.
4538600	4540600	Thank you.
4540600	4543600	I just have one quick question for anybody.
4543600	4552600	So why do they use the word universal in the title of the Laguta paper?
4552600	4555600	Because it sounds good.
4555600	4557600	All right.
4557600	4562600	Okay, that's what I guess the answer would be.
4562600	4563600	I'm not sure.
4563600	4565600	Jopster, am I wrong?
4565600	4566600	Please, correct me.
4566600	4567600	Probably.
4567600	4573600	I mean, there's a wonderful paper by Johannes from 2005,
4573600	4577600	which says why most of scientific research results are wrong.
4577600	4581600	And the biggest reason is, of course, bias.
4581600	4583600	And sounding good is also a form of bias.
4583600	4586600	So anyhow, but I'm not a pessimist for AI.
4586600	4589600	I remember that I made my little trunk for my life.
4589600	4593600	Jopster, we have one student, Peter Bottaroni,
4593600	4595600	who would like to ask a question.
4595600	4596600	Yeah, please go ahead.
4596600	4597600	Yes, thank you.
4597600	4599600	Actually, there are two questions.
4599600	4604600	The first is related with the discussion with the RID.
4604600	4609600	In the sense, okay, then we say that a complex environment
4609600	4611600	is an environment that is not...
4611600	4614600	We cannot model mathematically, right?
4614600	4616600	Yes, yes, basically, yes.
4616600	4619600	Okay, then every complex environment cannot be something
4619600	4621600	that we create with a software.
4621600	4626600	Then a complex environment should be a real environment,
4626600	4629600	in the sense that everything that is...
4629600	4632600	Even the more complex game that we can play
4632600	4637600	is not considered a complex environment, right?
4637600	4641600	Unless what we sometimes have is that we have
4641600	4644600	natural human behavior integrated as movies
4644600	4646600	into an environment, into a game.
4646600	4649600	And then the player in some strategy games,
4649600	4651600	which I used to play 20 years ago,
4651600	4653600	there was a movie sequence in the game embedded,
4653600	4656600	and you had to interpret the movie scene correctly
4656600	4658600	to continue to play the game.
4658600	4661600	And of course, the behavior of humans during that scene,
4661600	4664600	because this was a film of naturally behaving humans,
4664600	4666600	where they were playing a role, but still,
4666600	4669600	that was, of course, complex system behavior,
4669600	4671600	but then the rest of the game was not,
4671600	4673600	and that's still like it.
4673600	4676600	Poker would be a complex game, would it not?
4676600	4681600	Poker, or yes, poker played with humans is a complex game.
4681600	4683600	Which one? Sorry?
4683600	4686600	If you play a round of poker against human opponents...
4686600	4689600	Ah, okay, okay, yeah, yeah, yeah, yeah, okay.
4689600	4693600	But chess with human opponents is not a complex game.
4693600	4696600	Correct. Yeah, good.
4696600	4701600	And the second question is related with the reward definition,
4701600	4704600	in the sense that, at least to me,
4704600	4709600	it seems that we define the rewards with something that is given.
4709600	4712600	We define the reward for the robot or for the machine.
4712600	4713600	Yes.
4713600	4717600	Actually, the human sometimes creates some reward or some goal
4717600	4721600	by itself, maybe some sub-goal or sub-reward
4721600	4725600	in order to achieve a bigger goal.
4725600	4726600	Correct.
4726600	4731600	Is it possible to model this creation of intermediate goal
4731600	4734600	or final goal as a machine, according to you?
4734600	4736600	Yes, so it has already been done.
4736600	4739600	So you can, for example, you can have, as you know,
4739600	4741600	you can have meta models in machine learning.
4741600	4745600	So you can have, for example, several reinforcement learning models
4745600	4750600	that use different reinforcement rewards from a choice of reward.
4751600	4755600	And so you can optimize from a certain reward.
4755600	4758600	You can try out several reward types in parallel
4758600	4762600	and find the best model by varying the reward type.
4762600	4766600	And then you can use adversarial learning, maybe,
4766600	4769600	to drive the choice of the reward type, and so on.
4769600	4771600	Yeah, so there are ways to do this,
4771600	4776600	but they will always only solve problems in non-compact environment.
4776600	4777600	Okay.
4777600	4779600	Okay, thank you.
4779600	4780600	Good.
4780600	4783600	So now last slide.
4783600	4786600	So you remember that I'm making money with AI
4786600	4787600	and I'm a big fan of AI.
4787600	4791600	It's not that I'm negative against it, yeah?
4791600	4794600	So what can we do with the analytical engine?
4794600	4796600	So the analytical engine is the first name
4796600	4798600	that was given to a Turing machine.
4798600	4802600	Charles Babbage was the one who built the first computer
4802600	4805600	in the 19th century, around 1850,
4805600	4808600	and it could already do computations.
4809600	4819600	And Anna Lovelace said that an analytical engine
4819600	4821600	has no pretensions to originate anything.
4821600	4826600	It can only do whatever we know how to order it to perform.
4826600	4832600	And so what is now, is that also true for the Turing machine?
4832600	4834600	So Alan Turing says that, of course,
4834600	4837600	an analytical engine is a Turing machine.
4837600	4839600	And what can we do with it?
4839600	4843600	And so I think we can do a lot.
4843600	4845600	And we can do what one of you just said,
4845600	4847600	namely we can engineer by composition.
4847600	4851600	So that's an outcome that we want to achieve.
4851600	4856600	And here you see many operators and functionals
4856600	4858600	which are chained together.
4859600	4866600	And these are an upper case data as an operator
4866600	4868600	and a lower case data as a function.
4868600	4872600	For the non-mathematicians, a function is a relation
4872600	4881600	that takes an input vector and creates an output number.
4881600	4884600	The operator can also create an output vector
4884600	4886600	from an input vector.
4886600	4889600	So they act together here in a chain
4889600	4894600	to yield the final result, which is y or t hat.
4894600	4900600	And the superscripts that you can see here,
4900600	4906600	Teta, Kappa, Lambda, there are prior knowledge
4906600	4908600	that can be configured into the functions
4908600	4911600	and operated explicitly or via training tools.
4911600	4916600	So for example, this could be just a set of rules
4916600	4918600	and this could be the parallelization of the rules.
4918600	4920600	This could be a function that has been trained
4920600	4924600	like a spam filter and where the parameter indicates
4924600	4926600	which training tools for you.
4926600	4929600	And so I believe that it's very important
4929600	4931600	that you make the prior knowledge
4931600	4933600	that you gave to the machine explicit.
4933600	4937600	So either you make it explicit if you have, for example,
4937600	4939600	mechanical theory improving component,
4939600	4942600	let's say this one here, or maybe this one
4942600	4944600	would be mechanical theory improver,
4944600	4946600	then you need some axioms for the mechanical theory improver.
4946600	4948600	And those axioms are its configuration
4948600	4951600	and you need to know which axioms you give to it
4951600	4953600	so that you know how it will behave.
4953600	4956600	Or this would be a functional, now the functional,
4956600	4958600	of course, is trained by using training tools
4958600	4960600	and other meta parameter.
4960600	4963600	They are this and also those you have to know pretty well.
4963600	4966600	So for example, we do customer correspondence automation,
4966600	4968600	but we cannot use the way that,
4968600	4971600	so if a company gives us an email that it received
4971600	4974600	and also gives us a reaction to the email that they created,
4974600	4977600	we cannot use these two bits for training
4977600	4980600	because the outcomes that are created by the company are erratic.
4980600	4982600	That has to do with many factors.
4982600	4984600	It has to do with human error,
4984600	4987600	but it has also to do with, for example,
4987600	4991600	they have a period where they have understaffed.
4991600	4995600	So they just give a stereotype answer to all the letters
4995600	4997600	and later on, and they say,
4997600	4999600	we will get back to you later,
4999600	5001600	but we cannot react for the next three weeks.
5001600	5003600	So you have a heterogeneous outcome
5003600	5007600	which is not very well related to the input.
5007600	5010600	And so therefore, you have to very carefully curate
5010600	5012600	the training material that you use
5012600	5015600	to train such complex chains of functions and operators.
5015600	5017600	But if you do this,
5017600	5021600	then you can achieve very impressive results.
5021600	5023600	For example, we have automated
5023600	5025600	in the insurance industry,
5025600	5028600	there are builds that are, for example,
5028600	5031600	created by when a car gets repaired,
5031600	5033600	a bill is created.
5033600	5036600	We have created an algorithm that can automatically evaluate
5036600	5039600	whether the bill is correct.
5039600	5042600	And that's very hard because it's usually done by a technician
5042600	5045600	who looks at the bill and figures out
5045600	5048600	whether the workshop repaired the car in the right way.
5048600	5050600	And what we do is that we take the bill
5050600	5053600	and transform the bill into mathematical logic.
5053600	5056600	And then we have also, we get for this car,
5056600	5060600	the car repair instructions by the manufacturer of the car,
5060600	5063600	which we also transform into mathematical logic.
5063600	5066600	And then we do mechanical theory improving
5066600	5068600	to prove for each step
5068600	5070600	what this step corresponds to a step
5070600	5073600	that is also described in the repair instruction.
5073600	5076600	This way we have a mechanical evaluation
5076600	5079600	of the correctness of the repair of a car.
5079600	5081600	And so this is quite impressive
5081600	5084600	because it's a very demanding technical skill
5084600	5086600	that I'm not able to perform.
5086600	5089600	So I cannot perform what this computer program can
5089600	5093600	because I don't know enough about car repair.
5093600	5096600	Of course, the computer knows nothing about car repair,
5096600	5099600	but it can formalize the repair bill from the workshop
5099600	5101600	and it can formalize the repair instruction
5101600	5103600	into mathematical logic.
5103600	5105600	And then all it needs to do is to make a method
5105600	5108600	to establish mathematical, logical equivalence
5108600	5113600	between a repair step and an instruction step.
5113600	5117600	And this is just an example for what something
5117600	5120600	that can achieve with Turing machines.
5120600	5123600	And there are many, many other impressive examples.
5123600	5128600	But my opinion is that you need to do that intelligent behavior
5128600	5130600	is very hard to reproduce with the machine
5130600	5132600	that you have to make conscious decision.
5132600	5134600	You have to carefully select the training material
5134600	5138600	and you usually need a chain of algorithms that work together,
5138600	5140600	which you orchestrate somehow.
5140600	5143600	And then you can, that requires some work.
5143600	5146600	But in the end, it gives you perfect outcomes
5146600	5150600	and we have actually asked a German court,
5150600	5153600	legal court to evaluate whether it would take the output
5153600	5160600	of our algorithm in a lawsuit to represent the opinion
5160600	5162600	that usually is given by human expert.
5162600	5164600	And they said, the court said, yes, they would take this
5164600	5167600	because the quality is that we produce is higher
5167600	5170600	than the average human expert's quality.
5170600	5172600	So this is just to give you an example
5172600	5175600	that I'm a great fan of, I think a lot can be done,
5175600	5178600	but it doesn't all work out of itself.
5178600	5181600	Humans have to design such machines
5181600	5186600	like we've designed planes or cars or other machines.
5186600	5189600	And then they can work really well
5189600	5194600	and take a lot of hard and sweatshop type of work
5194600	5197600	of humans and create a lot of value.
5197600	5199600	And this is what I would like to show to you
5199600	5201600	and this compositional principle here
5201600	5204600	is I think what real AI is about.
5204600	5208600	And now it can be very cool to train neural networks
5208600	5210600	as one part of this,
5210600	5213600	but a neural network alone will usually not do
5213600	5216600	and that's why I'm showing an operator at the end
5216600	5220600	because this operator is usually a logical operator
5220600	5223600	and it makes sure that the result is reliable.
5223600	5225600	Because unlike stochastic models,
5225600	5230600	logical operators can auto-detect their mistakes.
5230600	5235600	And so that's why I'm a big fan of combining stochastic AI
5235600	5238600	with good old first-order logic AI,
5238600	5242600	which we both use in parallel in my company.
5242600	5245600	And so we have people who are specialized in neural networks,
5245600	5249600	but also people who are specialized in mathematical logic
5249600	5252600	so that we can obtain the precision that is needed
5252600	5254600	to automate human activity.
5257600	5258600	Very good.
5258600	5259600	That's it.
5259600	5261600	So thank you, Jost.

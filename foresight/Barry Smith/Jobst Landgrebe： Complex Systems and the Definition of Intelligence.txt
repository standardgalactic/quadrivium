Hello, everyone, and thanks for participating.
My name is Jobs Danke, but I'm by training
physician and mathematician, but I also
study philosophy, I've come back to do
philosophical research work as well.
So today I'm going to talk about artificial intelligence,
intelligent pseudo definitions.
But before doing this, I need to introduce
our view of complex systems, because we
will need this later on in the talk.
So I've been starting with two slides about complex systems,
and then we'll move to the definitions of our intelligence.
So what is a complex system?
So maybe why do we need to understand a complex system?
Because the animal and the human mind body
continue, which produce intelligence, are complex systems.
So even primal intelligence, which
is the intelligence of a bird or a mammalian animal,
the non-human, has primal intelligence
that gets produced by a complex system.
So what is a complex system?
So let's start with the Newtonian system.
So Newtonian systems are systems in which one
can apply models of physics.
And they have become extended quite a bit in the 19th century
with thermodynamics and statistical mechanics.
And then another bit by quantum theory and also
general theory of relativity.
But they still remain Newtonian systems.
So what is a Newtonian system?
But by the way, can you hear me all right?
Does it work?
OK, I assume yes.
So Newtonian systems are made up by a small set of element
types.
For example, the solar system is made up
by the sun and the planets, which are not many elements.
The way the elements interact, they
interact by the four basic forces or interaction types
that are known in physics.
And in this case of the solar system, it's gravitation.
And actually, only gravitation.
All the other forces don't matter for the solar system.
At least not with regard to the way
the planets move around the sun.
And they interact in a uniform and isotropic way.
So the force that is interacting here, the gravitation,
has its effect in a symmetric way all around the sun.
And it is the same everywhere.
I mean, it gets weaker and weaker.
But in a law-like fashion.
Also, there is no force overlay.
So there are other forces, like electromagnetic forces.
There's, for example, light that comes out of the sun.
But it doesn't interact significantly with gravitation.
So if I model the way the planet moves around the sun,
I don't need to take into account other forces than gravitation.
The phase space in which the elements are placed or occur
is deterministic and ergodic.
So ergodicity is shown in a small inset here.
So an ergodic phase space means that all accessible micro space
states of the space are actually probable over a long time.
So basically, in simple words, I can get to everywhere in this space
with the same probability if I wait long enough.
For example, if I have gas in a bottle,
the molecules of the gas will distribute
equi-probably over the volume of the bottle.
And so it has to be such a phase space is ergodic,
whereas there are also non-ergodic spaces to which
we'll get back in a minute.
Such Newtonian systems are non-driven.
Drivenness means that there is no force flowing through the system.
So for example, a steam engine has a driven aspect.
Because all the time while it's driving or under energy,
all the time energy is entering into the steam engine
by the burning of the coals.
And then it's been dissipated.
And in Newtonian systems, that's not the case.
Such systems have no evolution properties.
So they don't obtain new element types.
They have the element types they have.
So yes, this solar system could get a new planet
because there could be a big asteroid, could approach the sun,
and could be attracted by the sun,
and then start to orbit around the sun in the way the planets do.
But that wouldn't be a new element type.
It would just be a new element.
And they have fixed boundary conditions.
That is, if the solar system is four light-years away
from the next solar system, which is alpha centauri,
now if it would just be displaced by one or two light-years,
or even three light-years, this wouldn't change anything.
So basically, I can take the solar system out of its context
and move it away, and it wouldn't change anything.
Of course, if I would move it very, very close to alpha centauri,
then the sun of alpha centauri and our sun
would start to interact by gravitation,
and then very terrible events could happen.
But basically, Ceteris paribus,
I can just take such a Newtonian system out of its context.
Now, complex systems are completely different.
They depend on multiple arbitrary element types.
They have different interaction types between elements.
They have force overlay.
So that means that several forces act at the same time
and also interact.
The phase spaces that they have cannot be predicted
from their system elements, and they are non-ergodic.
So they behave in a way that the microstates
are not accessed over a long time with the same probability.
They're also driven.
So they have inner or external drive.
External drive is, for example, the steam engine
that gets heated from coal.
Internal drive is what humans or bacteria have.
This is the drive to reproduce and also to survive.
And drivenness means that there's a flow of energy
flowing through the system all the time
and that this energy is dissipating.
And they lack an equilibrium state
to which they would constantly be converging.
So a driven system doesn't come to equilibrium.
It's always it goes on.
But when the system, when an organism dies,
then it stops being driven and then it also
converges towards an equilibrium state, which
is, in this case, entropy.
Also, complex systems have evolutionary properties.
So they can evolve new element types.
And they have non-fixable boundary conditions.
So they are context dependent.
You can read this comparison of complex and classically
returning systems in turn at a very good book
about complex systems.
So let's look at some examples.
We have those seven properties of complex systems.
And the solar system has none of these properties
because it's not a complex system.
The steam engine has one property, it is driven.
However, to reason about the steam engine,
in many ways, you can abstract from its drivenness.
So for example, the velocity of the steam engine,
if it's used to drive a train, is
proportional to the pressure that it builds up and so on.
So this property, if it's the only driven property,
you can abstract from it for many predictions
you want to make about the behavior.
Pryon is protein that can infect the brain
and cause damage in the brain.
You have heard of Jacob Kreuzfeld disease.
And you may also heard of bovine spongiform encephalopathy,
which is also a prior disease.
And it has only two complex system properties, namely
force overlay and a non-negotic phase space.
And it has also a non-fixed boundary conditions,
but it lacks all the others.
But then as soon as I get to the virus,
I almost have all the properties.
Viruses, although not driven, because it cannot synthesize
energy.
And then with the most primitive organism,
I have all the properties of a complex system.
So it is important to realize that most systems in nature
are complex or deterministically chaotic.
And so basically, there's only a very little Newtonian systems
out there.
And most of the Newtonian systems that we master
are technically devices that we have designed out there.
All of them are Newtonian.
All of them are built using equations
that we have designed ourselves.
And that's what we really understand and master.
But nature is chaotic and complex.
And humans react irrationally to it often.
So you can, if you think of how, for example, we are now
reacting to this virus.
What it's causing is complex, but the reaction is irrational.
And that's because we feel that we cannot control it.
On the next page, you can see.
So I will give you the opportunity
to ask questions after this slide.
So on the next slide, we see now the problem
that complex systems pose to machine learning algorithms.
So basically, machine learning algorithms
cannot model complex systems because such algorithms
are large auto-parameterized differential equations,
partially auto-parameterized.
And let's look at the problems of machine learning models.
So on the left-hand side, you see the problems
that everybody know.
So that they optimize problem-specific loss
functions that don't generalize well,
that they narrowly depend on the selected training samples
and the specific annotations of these samples,
that they fail upon heterogeneous annotation
of identical input.
So identical input by different output,
it will be very stressful, so to speak.
I mean, it will not train well.
They fail on sparsely-populated sample space parts,
which is very often a very big problem
and that explains why very often machine learning algorithms
need so many samples.
Because if you look at the Curse of Dimensionality,
which you can, for example, read in Trevor Haste's
wonderful book about statistical learning,
you will see in chapter one or chapter two
where he explains the Curse of Dimensionality
that very quickly, you come from a topological perspective
to a very sparsely-populated sample space areas.
And in such areas, the machine doesn't learn anything.
And that's very interesting because humans
and also animals are very good at applying
the intelligence to sparsely-populated sample space.
It's basically, that's the key of intelligence.
Intelligence means the ability to react to new situations.
And machine learning models completely fail in new situations,
which are such sparsely-populated sample space.
They cannot be guaranteed to move
to corrected outcomes.
So if you have an error, erroneous behavior
of such a system, and you try to correct the system
once you've noted the error, it's very hard to guarantee
that it moves to the corrected outcome.
And while it's doing this, it may start to make new mistakes
because remember that the model itself is nothing
but a hyperplane in a k-dimensional space.
And this hyperplane, of course, I mean,
if you change its shape to get a certain effect,
you may get other effects you don't want to.
Such models cannot perceive their own failure, of course.
So they cannot really raise exceptions.
Well, they can raise an exception
if the data don't match the input type, but not much more.
They cannot model far-reaching relationships.
They cannot model semantics of mental type,
objectification semantics.
Here I'm using the German word, I apologize.
So it's object type, mental types they cannot use.
And that's why they fail at image
or language interpretation, they fail completely.
And they, of course, have not a sufficient exactness
for really critical settings.
That's the general problems that have been known
for machine learning, even before neural networks
were invented already, I mean,
before the deep neural networks were invented.
Even in the 1970s, when there was just logistic regression
and perceptron approach, it was already clear
to every statistician that these are the problems
of such models.
On the other hand, there are also problems
that are less well-known, that are described in our book.
There will be no singularity that we expect to publish this year.
It's with the, currently with the publisher for review,
which are related to the fact that human behavior
is emanating from a complex system, the human.
And so if you look at what the problem is here,
first of all, machine learning models require ergodicity
because the samples must be assumed to be drawn
from a representative distribution.
So if you want to train a complex system,
you need to have a representative distribution
because next time you draw a sample,
if it doesn't come from the distribution
which you used to train, the model will fail
because of reason number four.
However, reason number one on the right-hand side
gives you states that complex systems
create non-ergodic distributions.
And so when you draw from, when you make a photo,
so to speak, from a non-complex,
from complex system behavior-derived situation,
this photograph, apparently in quote marks,
is basically always skewed.
That's the most important thing
you have to learn today.
So when you record human behavior,
this behavior is never representative.
Because, so in other words,
there is no multivariate distribution to draw from
because each situation is new.
And the samples that you can draw are always outdated.
So what you do is you train with this huge snap
when you sample from a complex system.
So the sample space is always sparse.
I priori, that's really bad for complex systems
and that's bad for ML algorithms.
They cannot deal with this.
They need repetitive pattern.
Of course, then this is really the take-home message
number one, that if you have a situation
or an environment created by a complex system,
then this is always a non-ergodic distribution
deriving from a non-ergodic process.
And such a distribution can never be represented.
So also they don't have evolutionary property.
They cannot model evolutionary properties
because they model a fixed input-output relationship.
But when you have evolutionary properties,
input-output relationships change all the time.
Because they are differentiable models,
they cannot model non-continuous operators or functionals.
And also they can, of course,
not model non-isotropic forces.
They cannot model elements specific interaction types.
And also force overlay or interaction
is very hard to model.
So for them,
and also they cannot model drivenness,
which is energy dissipation.
To give you a very simple example why this doesn't work,
we haven't found any way to model the flow of water
into, for example, water reservoirs.
So turbulence.
Turbulence is one of the most simplest
natural phenomena of drivenness
because the energy from the water flow dissipates
in the reservoir to which the water is flowing,
but we cannot model it.
And so we cannot model how the energy
that is the kinetic energy of the water distributes
into the reservoir.
And also complex systems cannot model contextuality,
sorry, context-free machine learning models,
cannot model contextuality.
Because complex systems are always contextual,
but machine learning models are always context-free.
And so you have a discrepancy between the context-freeness
of the machine learning model
and the contextuality of the complex system.
So this is a very,
I don't know how long I spoke, maybe 15 minutes,
in very short words.
That's what complex systems,
the problem is complex systems modeling in machine learning.
Hello, are you still there?
I'm still here.
So now the students are required to challenge you.
I have a question.
Yes.
So the issues raised here were the problems
of modeling complex systems in the context
of neural networks and deep learning.
Machine learning.
Machine learning, all right.
But the thing is humans also haven't been able
to model turbulence.
And humans haven't been able to,
so to an extent we've been able to model
weather forecast systems.
So we use certain sets of differential equations there.
But if ML systems can, to a certain extent,
as was mentioned in the slide,
auto-parameterized differential equations model
some extent of that, then why should it be considered
a sign of no intelligence,
even though humans have also not been able to do
the same thing?
So why this requirement from machine learning?
So I'm not saying that it is a sign of intelligence.
So now we are not talking about intelligence right now.
We are only talking about what can be modeled with machine.
And what I have not said is I've skipped
a little bit of content.
So what I've not said is that all the phenomena
that are complex in nature can't be modeled using mathematics.
So of course we cannot model complex systems mathematics.
And so, but that's not because we are not intelligent,
but because complex system modeling seems to be
beyond the hardware that human intelligence possesses.
So but that doesn't mean that humans are not intelligent.
It just means that we can't model.
There's no way to model complex systems,
neither with paper and pen mathematics,
nor with whatever kind of algorithm.
And so there, but that doesn't mean that we are not intelligent.
It just means that this is beyond our modeling capability.
But this taken aside,
when it comes to creating ML models,
and we want to model our, or let's say animal intelligence,
then we would have to model the output of a complex system,
which not even humans can let alone machines.
Okay.
All right.
But at least humans have been able to approximate certain
complex systems like weather forecast systems.
So yes.
So that's, so go ahead.
So my question then would be,
what do you think of the potential for machine learning systems
to approximate rather than create an exact model?
So, so a very good question.
So whether forecast models are approximative machine learning models,
right?
So many of them are built using machine learning.
So of course it's possible to approximate certain
complex phenomena using machine learning,
but it's quite limited what you can achieve.
Yeah, you can achieve some.
The question is always how good is the approximation
and what can you technically do with it?
And so if you look at the tetanosphere we have,
all the technical gadgets that surround us,
that make our life so much easier and better,
most of them are exact.
So mobile phones are exact, bridges are exact, trains are exact,
airplanes are exact, cars are exact.
So we don't have so many approximative models in the
tetanosphere.
So one, so Google, Google advertising plays is
approximative, but why can Google afford it?
Because nobody gets killed if I get shown a female
lipstick ad, right?
So nobody gets killed if I get shown a lipstick ad,
so they can afford to do it.
But if they would use these algorithms in intensive care,
unit machinery, they would kill people.
So that's the point.
The point is approximative modeling is fine,
but you have to decide in practice for what purposes
you can use it and where you can't use it.
Perfect.
Thank you.
Thank you.
Thank you very much.
Any more questions?
So I guess I'll try.
So if you take a, we use this example already a couple of
times, if you take a laptop and throw it into a river,
then the laptop no longer behaves in such a way that
it is a simple system.
It starts to decay.
And that's because the laptop plus the river is a
complex system.
Is that a correct account?
Well, I mean the system as a whole, so when the laptop
drops into the river or creek or whatever,
the creek has mechanical energy in the water.
And this mechanical energy will start to attack the
structure of the laptop.
And then of course chemical processes that are not in
equilibrium because the water is driven is driven will
also attack the laptop.
So the laptop in a way will become part of a
complex system, but on its own it will not be a
complex system.
So if you take it out and those forces stop working
on it, then it will stay in this more or less in a
certain form of decay.
Although in the long term, even if you leave a laptop
standing somewhere without water, it will also be
part of a complex system because there will also be
energy working on it, but much less so.
So if it's in a building, to begin with it will take
one or 200 years before the building is broken and
then energy can really work on the laptop.
Can you go back to slide two?
Yes.
So you give C elegans as an example of a complex
system.
I assume you would give a human being as an example of
a complex system.
Yeah, of course.
Yes.
Now, but can you tell me how a human being has
evolutionary properties?
Yes.
So the evolutionary properties of living organisms
constitute, come from the fact that new types of
molecules can be created in adult organisms.
So if you have an adult organism by its ability to
react to the environment, it can create new types of
macromolecules that it hasn't had before, like new
memories and all new combinations of elements.
And that's all it can actually change the way it
methylates its DNA and therefore change the way it
will, the inheritance will work if it becomes
progenitor of new organisms.
So this is the way that new elements and element
interactions can arise in adult organisms.
Good.
Okay, any more questions from anybody?
Yes, I will have one.
I believe it's a very boring, usual philosophical
question, but when we say that complex systems are
not...
We don't have a mathematical modeling for them.
Is this correct?
Yes.
Is this intended to be one of our epistemic
lacks or something which happens in nature
unregarded of some very, very complex mathematical
model, which at the moment we don't have, but...
This is a very good question.
So we don't know this, so we can't really give an answer
to this question.
It could be speculative.
However, my view is that our ability for mathematical
modeling is an evolutionary adaptation of humans.
So very much like language is the way our hands work,
our evolutionary adaptations.
And it is a special evolutionary adaptation of our
mind, the extent of which also strongly varies
between individuals.
So there are only out of a thousand, only one or two
individuals are usually mathematically gifted.
And so it has a high variance, but everybody with an IQ
above 80 can count.
And this skill is limited.
And I think it's limited by the forces that shaped it
during evolution.
And so I think that certain aspects of nature are just
too much, so to speak, for the structure of this skill.
And this seems very plausible, given the history of
mathematics so far.
However, because in the end, all the mathematical objects
that we know can be reduced to numbers and are very
complicated combinations of them.
So even, for example, if the invention of calculus by
Newton and Leibniz seems like a very big step, the way
they did it was quite geometrical.
And great inventions are always great, but it's still
linked, of course, to the history of mathematics.
So I don't think that we will ever be able to model fully
model complex systems, but we were able to, to some extent,
of course, approximately model aspects of them.
So in other words, I think the structural deficit.
Yes, yes.
Thank you.
I also have another question.
Yes.
So if we consider the fact that how mathematics have evolved
so far, and also the fact that mathematical modeling has
up to a point tried to represent, in a way, the randomness
that might be an attribute of these complex systems.
So we have already made some, find out some mathematical
ways through statistics and stochastic processes in order
to study randomness.
Yeah, of course.
And so do you think that this is like a big step, a small
step to accomplish something even bigger in the future?
Because up until a point, we also thought that randomness
was something that we couldn't even explain.
And computationally.
That's, that's not fully true.
For example, there is no mathematical model for a true
natural number generator, a random number generator.
So a true random number generator, which produces true
random events can only be constructed by using a physical
device, namely normally one uses a Geiger counter, which is
counting radioactive decay events, because they are truly
random.
So we cannot simulate randomness.
Of course, we can model certain aspects of randomness.
And this we can do since a couple of hundred years.
And at the end of the 1980th century, we have started to
understand how calculus can be applied to this.
But the way that we model randomness is only applies to
classical Newtonian systems.
And when we get out of Newtonian systems, there are actually
no examples to convincingly model a complex system behavior.
So they are, of course, like one of your colleagues mentioned,
approximative models of randomness, such as, or
stochastic behavior, such as weather forecasting, but they
have a very short-term forecast window.
And they are not very, depending on in which landscape you are,
they almost don't work at all.
So near the sea or in the mountains, they almost do not work.
And basically, they work for regular continental climate with
strong determinants over very short durations.
And so the fact that we have probability theory doesn't
really help with complex systems.
Turbulence, which I already mentioned, is a very good example
because the mathematician who had found the best way so far of
dealing with turbulence was actually probabilistic
theoretician, was Kolmogorov, who invented the Kolmogorov-Smilnov
theorem, one of the greatest mathematical geniuses of the
20th century.
In the 1940s, he tried to model turbulence.
And while his model is, you can see it when our book will
appear, it's explained in the book, while his model is very
aesthetically highly valuable.
It's very beautiful.
It fails to model the reality of turbulence.
And so there is no example for exact modeling or good
approximative modeling of complex systems.
And I don't know.
It's actually, if you know how the theory of probability works,
how the distribution are approximated, that it's already
very, very hard actually to figure out or mathematically
impossible to calculate mixed high-dimensional distributions.
You will see that this is very, very far away.
I don't know if it's possible.
Okay.
Thank you for your...
Okay.
I think we'll have yours to continue now for a bit.
So now we look at intelligence.
And you will see in the course of the second part why we needed
to talk about complex systems.
So this here, what you see is the standard definition of the
artificial general intelligence community of intelligence.
And this is...
So they have a verbal definition which goes intelligence
measures an agent's ability to achieve goals in a wide range
of environments.
That's the standard definition that everybody accepts.
And so this ability to achieve goals is defined by a utility
function.
And this utility is here V of an agent pi depending on the
environment of the agent's mu.
And it's defined as the expectation of the sum of the
rewards the agent is going to have, which is actually normed
to be equal less to one.
And so mu is a binary description of the environment of
the agent.
And this environment may be a fired abstract structure that
is manipulated inside a computer.
For example, if you have a theorem prover, then this would
be one environment or it may relate to something in the
physical world.
For example, a nuclear power station that has broken down
and where you now want to clean up the nuclear power station
with a robot.
Because it's not an environment where we would like to use
humans.
And by the way, there's a great documentation about
Chernobyl.
They tried to use robots.
But the problem was that the transistors broke immediately
because radiation was too strong.
So the radiation destroyed the transistors inside the robot
and then they stopped working.
So they had to use humans after all.
And in either case, no matter whether it's a physical
scenario or artificial one, this vector takes a form of a
binary string.
And it's a description which plays a role in the HATTA
model.
And E is the expectation of the rewards.
And what is a reward we will see it on the next slide.
So this is a basic definition.
Now, first of all, let's consider what's that this definition
is actually mathematically broken, which I find kind of sad
because they should at least get this right.
But basically, this is really accepted in the AGI community.
Now, one has to see that the AGI community is made up mainly by
computer scientists and mathematicians.
And computer scientists like physicists and engineers, they
tend to not look so carefully at mathematical equations.
But actually the definition of expectation is the sum of a
variable, which is actually multiplied by the probability of
this variable for each step.
So every finite amount of steps, or here is actually even
infinite, but it doesn't matter if you have some steps, you
should calculate the reward of the variable by summing the
product of the probability with the variable, which is the
reward.
And here they don't do this.
They actually take the expectation of that is already
summed up.
So it's funny because if you wanted to implement this, it's
actually not implementable because mathematically wrong, which
I found quite interesting.
But so in other words, the operator E of the expectation has
to be applied not to the sum of the values of a random
variable, but directly to the values of the variable, as shown
here in this equation.
And otherwise, you cannot take account of the norming
denominator.
So pi i is smaller than 1, and therefore it's a denominator.
If you multiply by a number smaller than 1, it's the same
as dividing by this number.
And so usually you need a denominator.
And if you don't have a denominator, you cannot get the
sum below 1, of course.
Because this is just mathematically silly.
But they've published it, and the viewers have accepted it,
and it gets cited hundreds and hundreds of times.
I'm astonished.
But anyhow, a lot of crap gets cited a lot.
So it seems to be human nature.
But it's remarkable that nobody has criticized this.
Taking this aside and just ignoring the definition
problems and imagining they had used the proper definition
of expectation, which is actually you can read up in any
very basic textbook of statistics.
I mean, actually, it's cool when you learn to calculate
the probability of throwing the number 7 with 2 dice.
You learn this.
So I was kind of astonished.
Anyhow, in simplified terms, when we ignore this, the equation
1 set in agent pi reacting to environment distribution mu
obtains a finite reward, which corresponds to the expectation
of reward it achieves over all the steps it undertakes.
And actually, the higher this is, the better the utility.
And so Barry asked me to insert something about reward.
Of course, the reward here has no meaning for the machine.
The machine is just a calculating machine.
It's a Turing machine.
It can just apply amounts of electricity to certain circuits.
But by doing this, so reward has no meaning for the machine.
That's just a mathematical concept to express an optimization
problem.
So this year, how can I maximize the utility?
I can maximize this utility by doing something with this reward
variable.
And that's all it says.
So reward doesn't mean what is reward for you so that I invite
you to a good beer in the evening or anything or bring you
some flowers or so, what humans experience as reward.
It's just a way of formulating the mathematical optimization
problem.
OK.
So on the next slide, what do they do with this utility?
So here you can see this utility term again.
But now it is in a bigger equation, which defines intelligence.
And this uppercase epsilon, it's a mathematical uppercase epsilon
as a function of the agent is defined as the sum of the utility
multiplied with another factor.
And what is this factor?
So this factor contains k, which is a Kolmogorov complexity
function.
We already heard about Kolmogorov.
He did not only work on probability distributions, but also on
information theory and the complexity function.
He invented it and it's indicating the complexity of the algorithm
executed by the agent pi to represent the environment mu.
So it's basically how many calculation steps are needed to
represent mu.
And as you can see, this is a negative exponent.
So that means that this is a parallelizing factor.
So the more steps I need, the more complex my representation
algorithm is, the lower the intelligence will be.
And that's pretty, I think, acceptable.
Because so if you imagine a more intelligent individual will
find it easier to achieve a goal than a stupid individual.
So of course, you know this from everyday life.
And so this says something similar.
It says, so if you have a high utility for a given environment,
but I needed a million steps to achieve it, then I'm less
intelligent than somebody who achieved the same utility with a
shorter number of steps.
So further, a couple of more remarks.
So mu is the environment.
What is uppercase u?
Uppercase u is a set of environment descriptions that the
machine can process.
So for example, if the machine is the AlphaGo machine, then all
the situations are settings on the gold board and nothing more.
But of course, an AGI agent would be able hopefully to process
more different situations that at least what they hope.
And so u is just a set of environments the agent could in
theory be processing.
Let me give you an example from the animal kingdom.
So for example, a rat has quite a huge set u because a rat can
adapt to very many environments.
And therefore, it is also to be found all around the globe
all mode, not in the Antarctic and not in the Sahara, I believe,
but in many, many moderate environments.
A rat is to be found, whereas other animals are very specialized
and are only found in certain environments.
And this is what this uppercase u is supposed to say.
So basically the definition of intelligence shows to you the
most efficient possible algorithm to achieve a certain utility.
Also, the summation over all environments prevents a random hit.
So because you have to perform this in many environments, you can
guarantee that the equation doesn't give you a distorted outcome.
So if the definition of the utility function would be
mathematically sound, which you could easily do by replacing it
with this, then the entire equation would mathematically
make sense.
So it is a proper definition of a weighted utility function.
That's what it basically is.
Before we take apart these two equations, now I've just
criticized them from a mathematical point of view.
Are there any questions regarding the two equations before I
continue dissecting them?
I have two questions when I was looking at this formula.
So I was just looking at V as the author says,
ability to achieve the value, the capital V.
But then in the formula on the second slide, on this one.
So why is Gogomolo complexity function as exponential?
And the V is just like linear.
Because it's just a negative exponent.
So it means you just divide.
This means 1 divided by 2 exponent k.
So it's just used to actually penalize the utility function.
But why didn't they just penalize with the inverse value?
Why did they choose the exponential growth?
To make it perfect, maybe.
Yeah, maybe.
Actually, I haven't thought about it.
But it's basically the way that the Gogomolo complexity function
always gets applied in theoretical informatics.
So this form is when you read a textbook of theoretical informatics,
you always see it represented like this.
So that you get a smooth representation of the entire function.
So I think it works pretty well to weigh whatever you want to weigh
by the amount of work that you need to put into obtain it.
Yeah, they argue that extensively in the paper anyway.
But the second question is this capital U, the set of environment descriptions.
How is this supposed to be interpreted?
Because it's written as a sum.
But I would take that in general, this is like a manifold in the best case.
So it's a sum over the elements of a set.
That's fine mathematically.
So this is just an index of all the possible terms you can get.
And let's say you would have three different environments.
What if it has a ball or a sphere of different environments?
So you just integrate?
No, I don't think so.
Because you basically can always, this is theoretical informatics.
So that means that you can, no matter what from a functional analysis point of view,
no matter what is the way the environments are,
they are always compressed into binary vectors.
And then you can just create a series of binary vectors or a set of binary vectors.
And this is just one binary vector out of a set.
So it's a countable set of binary vectors, no matter what.
Maybe you should explain what a binary vector is.
Yeah, so for the non-n estimaticians, it's just a vector of ones and zeros.
So remember that a Turing machine can only deal with ones and zeros.
It's like a big tape on which you can write ones and zeros and can change the ones and zeros.
And this is basically, from the perspective of theoretical informatics,
it's just a set of such binary vectors.
Sorry, I think it would have been a bit more helpful if the input and output spaces were also defined.
Because now it's clear to me what, so the inputs are basically strings, as far as I understand it.
Is it?
Yeah, so otherwise the mu is a bit hard to interpret.
Yeah, the mu, yeah, you're right.
So I have not defined, actually it's interesting why I've just copied the definition from their paper,
but you're right, usually one would have to use a functional analysis type of definition of the input space at the output.
Yeah, exactly.
And I could easily have done this, but there are two reasons why they didn't do it for sloppiness,
and I didn't do it because it's just too implicit for me because it's what I do all the time, but thank you.
I think we should add this next time, it's a good point.
Yeah.
Okay, so anyhow, this is a definition.
Now let's move on and look at the problem.
So first of all, there is a verbose definition before the equation,
which says intelligence measures an agent's ability to achieve goals in a wide range of environments.
So first of all, the definition captures just one part of one of the standard definitions of primary intelligence.
So primary intelligence says that you have to adapt to new environments suddenly and without being trained upfront.
So you have to be spontaneously able to adapt to a new environment suddenly, quickly.
And this definition just captures the adaptation.
Now, what is more important is that the definition is very broad because it allows also this organism,
a small worm that is one millimeter long and has a thousand cells and only 300 neurons to be intelligent,
because what it can do, it can forage and reproduce in complex environments,
so it can live in fruit, in vegetables, in mushrooms, in soil, and so on.
It can use snails and stugs as migration vectors.
So it can really live in many environments and it can also reproduce there.
So I think it would be intelligent, according to the AGI definition.
And even probably the amoeba here, this is a nice amoeba.
It's called kaos kaolinensis because it never has the same shape.
Here's just a drawing of it, but in the next second it will look different because it moves around by changing its shape.
And it can also live in many, many environments and thrive wonderfully there.
It can reproduce, it can find food, but it's just one cell.
So it's one of the most primitive, well, it's the most primitive eukaryotes,
but directly next to yeast comes already this amoeba.
It would also be intelligent in this definition.
So I think the definition of intelligence is too weak.
Why is it so weak?
Because in the book where this intelligence definition is used,
that's the book by GÃ¶tze and Pinache, which is called Artificial General Intelligence,
they discuss many other definitions.
But the problem is that if they use definition like the one very simple one of human intelligence,
intelligence is the capability that enables us to speak, for example.
That enables the language that humans can use.
I mean, it's not my definition, but if you would, but they propose this definition,
then they would automatically fail in generating artificial intelligence.
So they've basically chosen a definition that doesn't yield intelligence,
so that they claim now that they have an intelligence.
But they've actually on purpose used a very stupid definition that is not intelligence.
Now let's go and look at what time it is and look at other problems of the AGI definition.
So let's first look at perception.
So if you have this vector mu as a measure of complexity of environment,
this vector mu presupposes that the environment can be represented by using a binary vector.
In some artificial environments, such a binary representation may be adequate,
but in natural environments, we have signals emanating from complex systems.
And therefore, the signals need to be actively interpreted and reassessed all the time.
And also the observation needs to be continuously adapted to the input
as the agent takes account of the interpretation of each antecedent observation.
And also an animal interpretation depends on previously experienced mental material,
so memories.
And for example, this tiger observing the prey, it does actually all the time update its observations.
It does active perception or shields, I think a female tiger.
And if there would be a puppet next to a young tiger,
the young tiger would not be as good at observing those animals, those prey animals,
because it has that experience.
So the experience stored in the memory of the tiger also helps it to react better to what the animals are doing
and to single out, for example, one animal to hunt it down.
So the predator, if the predator is just sitting there and observing the prey,
it's already acting to improve and adapt the perception of the prey.
So perception is not static, but a dynamic process of constant iterative feedback loops
between sensory and motor neuron circuits, and we cannot even know.
So if we think of training an ML algorithm, we need to know the tuples that we used to train,
but we don't even know when the cycle begins and ends.
So the cycles can be very fast, and we don't know because we can only observe the overall behavior,
but we don't know how to determine the tuples that constitute the perception process.
So our JJ Gibson, a very important psychologist and philosopher says,
normal activity of perception is to explore the world.
So perception depends on more than just sensory stimulus.
So the view that perception is just a result of sensory stimulus is completely outdated.
So when we give input to computers, it's just sensory input, but that's not real.
That's not perception.
Perception requires purposeful activity, direct manipulation of the object,
and innate or quiet knowledge of the expected patterns of reality.
So I need categorical, predefined ability to deal with the environment and also quiet knowledge.
And this manipulation of the object doesn't mean that I need to touch them,
but the tiger can also manipulate these animals, prey animals in her imagination.
So she can imagine that maybe this animal would now lean down to drink from water source
and whether then maybe it would be a good moment to attack this animal and so on.
So this is highly interactive and mu, this static vector mu,
which basically models, for example, the input from a sensor doesn't capture any of this,
does not capture any of what I've just said about perception,
what we know about animal and human perception.
Here's another example of perception, which is much more complex than this one
because it involves dialogue, it involves observation of a dialogue,
it involves probably, yeah, it has many, many interesting aspects
that show how complicated perception is.
And so, for example, what does Tony Curtis, who is here acting as a woman,
think about Marilyn Monroe in this moment?
I don't know, but it's certainly an interesting question.
And anyhow, so perception is not modeled by this environment variable mu.
On the next slide, we see the next problem, which is activity.
So the steps of the utility function that you've seen here are results of,
so each step is an activity which yields a reward.
So in chess or in Go, you get a reward for making a certain move.
Now, the steps of the utility function that are described by the Hutter definition
of artificial intelligence, there are a linear sequence of discrete machine actions.
But human motor acts are actually interactions of perception motor activity.
They involve at every stage a dense synergy of multiple body systems
at multiple levels of granularity.
And that's, for example, if you think of human manufacturing activities
where they have to use, like surgery, when you have to actually
feel very exactly what you're doing or in certain steps in the construction
of even of cars, fine motor, it's called in German.
Barry, do you know the word in English?
I don't. Sorry. Precision engineering would be...
Yeah, probably.
So we don't know how to make machines do this because we don't know
how the circuitry between a perception and motor action work.
So we know that there is some feedback loop, blah, blah, blah,
but we don't know the details of it.
And if you look at the most advanced textbooks or papers that are available
about animal fine motor action, we have no clue how the animals
do these fine motor actions or know how we do it.
We have no models for it.
And so because we have no models for it, we cannot do it in a computer.
And so the activities that happen in real environments
are much more complicated than those linear sequence of steps.
And the interaction between sensory and motor activity
could potentially scrub the linear sequence of effort, effort,
and interest in neural signal events, but the coupling of such a sequence
to any sort of reward is very indirect.
So yes, in reality, of course, those things happen one after the other,
but very, very quickly in a very complex session.
And so we don't know how to cover this to reward.
And so probably the sequence in reward terms that we need
would probably not be possible to construct the correct reward sequence.
Another aspect is that mammals can overcome
massive negative rewards to achieve the goal.
So here I have an example from animal psychology.
So it's a cocaine self-administration experiment
where the rats to get cocaine, they have to traverse a heated plate,
which is burning their feet.
So they have to damage themselves to get to the cocaine,
but they still do it because they have primary intelligence.
So they know that they must cross the heat plate to get to the cocaine,
that they must press a button to get the administration of cocaine.
And they get a short-term reward for this,
but they don't have a net long-term reward.
And so such a behavior is very, very hard to model with a reward function.
And if you think many of you are maybe in the middle of their PhD thesis,
so the PhD thesis process is not like the cocaine self-administration,
but there's a lot of negative stuff you have to cope with
over a long time before you get a reward, that's for sure.
And I don't know how this can be modeled with such a reward model,
at least being very hard.
Speaking of reward,
so what is even more important is that the reward pattern
that we see is some of a reward.
It is actually unable to model reward patterns that we encounter in real life,
because first of all, the system that certifies the harder definition
will always be situation-specific.
So it will work only in the context where a human has already been at work
in preparing appropriate rewards.
There is no general or universal reward.
So the reward, for example, that the machine receives for playing the game of Go,
are points.
And the algorithm is trying to find a functional or an operator
that maximizes the number of points.
So it's just a derivative of a very long equation that you have to find.
And that has nothing to do with universal or general intelligence.
The reason is that the mathematical definition that Hatter provides
for the environment mu that must be matched by the reward.
So it's not possible to find a reward that works for all environments,
whereas humans have as a reward,
the main rewards are to survive and to reproduce.
And survival and reproduction can come very indirectly in highly evolved societies.
So to sit in a room and do mathematical equations all day long
for survival and reproduction, that's quite abstract
or to do paintings of art or to compose music.
And so there is a way of humans to delay the reward of reproduction and survival
very far off and to do activities that seem to be non-connected to it.
And that seems to be at least required for objectifying intelligence or human intelligence.
And we don't know how we can model this with reward.
Also further problem arises that the assumption is made that all rewards of one agent
must be of the same type for every step under a given environment.
So if we go back to the equation, r is only indexed by the step,
but it cannot change its type, otherwise it would need the second index.
So there's only one type. Yes, this type can obtain different values,
but probably in animal and human behavior there are many different types of rewards.
So we cannot model this.
Then the question is, couldn't we create a sequence of rewards
adequate for learning the behavior of a complex system?
So the problem here would be that such a reward sequence,
you would then imagine many, many rewards in a sequence
and they would have to give situation-specific rewards.
That's because each step on a complex system model trajectory
would have to be able to deal with an unexpected situation
because when the AI system interacts with its own environment,
that will change the environment.
And so this change of the environment will create an unexpected situation
and may require different reward.
So you cannot really find a reward part or trajectory
because at each step a different reward would be needed to correspond
to the emanations from the complex system that form the environment.
So that has to do with the evolutionary character of complex systems.
And such a temporal reward sequence would obviously not follow a Markovian pattern.
And of course, speaking of probability theory, if we go back to this slide here,
Markovian pattern is of course a pattern that applies only to classical Newtonian systems.
Only Newtonian systems have the Markov property.
Complex systems don't have the Markov property, but without the Markov property,
you cannot achieve predictive modeling in stochastic equations.
So stochastic differential equations or other stochastic process models,
they always need a Markov pattern.
And without the Markov pattern in which the reward would depend only on the previous or some previous steps.
But if you don't have this pattern, but you have a dependency on many earlier steps
and long-term dispositions of the organism and also short-term intentions,
you can't find a reward sequence.
And so, therefore, the reward sequence would need to correspond to complex emanations
relating to situations varying as a successive test unfold.
And therefore, the reward sequence itself has to be complex
and thereby it would have all the properties of a complex system emanation.
So that's the interesting thing, that to give rewards to an intelligent system in a complex setting,
the reward sequence itself would have all the properties of a complex system emanation.
So it would have the same properties that my stream of language that I'm currently giving to you has.
And we have no mathematical models to create such sequences
because we have no mathematical models for complex system emanations.
And so we couldn't create the reward sequence that would be needed for the intelligent system
to cope with a complex system situation.
And that's a problem, I think, a very important problem to see this,
that the reward approach, why reward is mathematically attractive
because it allows to state the intelligence problem like an optimization problem for optimization theory.
It doesn't create a realistic sequence of rewards that correspond in any way
to what we experience or animals experience when they obtain rewards for their behavior.
Actually, the reward that an animal and foraging animal receives, which is the food
and for which the animal does very interesting things.
So if you read modern books about foraging, it has been found out that parrots, for example,
invent new patterns of shouts and vocal noises they make to describe sources for food.
And while they search for those food sources, which they have to do every day,
they create those sound patterns.
And this process is highly complex and it has nothing to do.
And the way they get to the reward has nothing to do at all with linear reward sequence
that we see in those models.
And so that's why I call these pseudo-definitions of intelligence
because they just define something that can be actually put into a model optimization algorithm,
a numerical model optimization algorithm, like a dual or something like this.
But in reality, it has nothing to do with real intelligence, not even animal intelligence.
Now, I think before I go to the last slide, I would like to give you the opportunity
to ask one more round of questions.
Yeah, I have one question, Professor.
The thing is, so here the argument is that the reward sequence is extremely complex
and since it is not Markovian, you cannot somehow model this.
Now, if I were to take the example of a robot in a factory,
let's say a robot that knows how to load boxes, unload boxes,
get the boxes onto some other conveyor belt, et cetera, et cetera.
Currently, there are reinforcement learning algorithms that are able to do this
pretty effectively in these factories, just replacing entirely with robots that do this work.
So there, of course, the reward sequence is very clear
because you have just one mechanistic task that you keep doing throughout your life,
well, throughout your robot life.
My question here was, what if we could create a set of rewards for different tasks
and model that together into a specific robot?
So, for example, let's say loading and loading boxes, opening doors,
walking, sitting down, different movements.
Now, I'm not suggesting for a single second that humans learn this
by this sort of discretized reward sequence.
I'm not suggesting this at all because I don't know how humans learn it.
But as far as teaching robots that is concerned,
don't you think the reward sequence then would just be a set of different rewards
for different tasks, but they would all be modeled into the same algorithm.
So, in a sense, what I'm arguing is the whole is the sum of its parts.
That's what I'm trying to say.
So, you're actually already giving a small preview of the next slide.
So, it's never what you're saying.
So, basically, what you describe is, of course, that what is a factory?
A factory is a Newtonian system.
So, a factory has all those properties.
And so, therefore, machine learning can be very, very efficient
in implicitly modeling Newtonian equations, motion equations, also sensory equations.
So, therefore, because in a factory, all processes have the Markov property
and because there's no force overlay, the elements are well defined and so on,
that will work pretty well.
And that's why also machine learning is, I think, the best application of machine learning
that is currently not visible very much is actually manufacturing and mining
and other mechanical activities where humans still play a role,
but where they will with the exception of this fine motor or sensoric behavior,
which is very hard to model, they will be replaced by robots more and more.
You're completely right, and that's because here, machine learning is used to model Newtonian systems.
That's the first answer. The second answer is that it's essentially the way to build AI properly
is to do it as you described, and I will come to this once the other questions are answered.
Oh, okay, okay. Thank you so much.
Is there any?
I have one, but I'll give the students chance to butt in first.
I would just remark maybe that the authors were aware of this argument that you are saying
because they said that at no point they are trying to compare their definition to human intelligence.
I think they perceive the machine intelligence just as a composite of different tasks
as Ravidi was saying earlier.
If you read Hutter's and Schmitt-Huber's papers about general intelligence, I disagree.
They believe that they can create general intelligence, and many of them, I don't know whether Schmitt-Huber does,
but many of them also believe in the singularity, which I think shows.
I think that you should really get this AGI book, or I can send it to you, or Barry can send it to you,
this AGI book by Gertse Penachin, which is one of the most important consensus readers
where all the big shots of AGI have published papers.
Yes, of course they say that it's not human intelligence, but they say it's a real intelligence.
What I'm saying is no, what you're suggesting is not a real intelligence, but it's just a kind of amoeba intelligence,
and probably not even that.
I think that because on the next slide you will see how I define what one can do with AGI,
and when I show this to people from this community, I get heavily attacked.
Oh, that's old school, we can do much better, we can do general intelligence, and so on.
If I understood correctly, they were also saying that, of course, the Chinese argument is completely right.
I think where they were defending the critique, maybe that's at the end.
But I don't even argue with the Chinese room argument here.
I just say that even the most basic form of intelligence that we as humans perceive as intelligent
would just see the behavior of a dog or another mammal, that this behavior cannot never be achieved by these equations.
So what can be achieved by these equations is actually what neural networks already do,
and that is because this here is a recipe to actually...
Yeah, and so loss function minimizes loss, and here we maximize reward, but it's basically optimization, prescription,
and we achieve this by applying these models to achieve what is shown on slide three, with all the pros and cons.
Now, I'm saying that these ML models are very useful, but they have nothing to do with intelligence,
and that's basically...
I mean, when I discuss, you know that I have an AI company, when I discuss with clever customers,
I mean their customers who want to buy intelligence, and I let them under the illusion,
but others who are clever to say, but that's not really intelligence, I say, no, these are artificial instincts.
And this is basically what we do, what we still do, even if we use such optimization procedures as those described by these models here,
we still only obtain a narrow AI.
And with this, Barry, you had a question before I go through the narrow AI.
Yes, so I have a number of questions now, unfortunately.
So what would leg Hutter say to the following objection?
You define intelligence as the ability to achieve rewards in a wide range of environments,
but AlphaGo can only achieve rewards in one kind of environment, which is the Go board.
Yeah.
So it's not a wide range at all.
Yeah, yeah.
Yeah, I believe that even actually, you're right.
So I believe that even this definition, intelligence so will not be achievable.
So the verbal definition is actually in conflict with the equations.
Yeah, you're right.
And now just a little correction, sorry.
Sorry, just a little correction.
AlphaZero actually can succeed in a variety of game environments.
AlphaZero can play chess, it can play Go, it can play checker.
I'm just talking about AlphaGo.
You're right.
Not AlphaGo, but AlphaZero, which was recently released.
Yeah, AlphaZero can play different games.
And actually, there was already in 2014 a precursor for AlphaZero,
which could play all the Atari games, which was also trained with reward learning.
It could play strategic games, but it could play Pong and all the ones
that give you a series of points.
But now OpenAI has come out with StarCraft games too.
So they are getting better at these complex environments.
Well, these are actually not complex environments.
So these are still Newtonian environments.
Because the reason this is very important, if you could hold on with your questions a minute.
So the reason why these are Newtonian environments is that also,
for example, games like strategy games, like civilization,
they have also been beat by reinforcement learning.
But the reason is, of course, what is civilization?
Civilization is a set of rules and equations applied to a certain pattern.
And even if this pattern is created by random, it's created as a multivariate
random distribution, usually multivariate.
And so it's multivariate Gaussian.
And then, of course, you can take points from this distribution
and put them into a set of rules.
Then you create a very complex set of events, but it's still a Newtonian universe.
And therefore, you can also train in the same place for the ego shooter games.
And therefore, you can train AI that is beating this.
But the point is, since 1950, we have heard predictions about free-moving robots.
Why don't we have them?
I mean, we have them, of course, in controlled environments.
But why do we never encounter free-moving robots in our streets?
Because these are real complex environments.
Yeah, yeah, yeah.
Thank you.
I just have one quick question for anybody.
So why do they use the word universal in the title of the Laguta paper?
Because it sounds good.
All right.
Okay, that's what I guess the answer would be.
I'm not sure.
Jopster, am I wrong?
Please, correct me.
Probably.
I mean, there's a wonderful paper by Johannes from 2005,
which says why most of scientific research results are wrong.
And the biggest reason is, of course, bias.
And sounding good is also a form of bias.
So anyhow, but I'm not a pessimist for AI.
I remember that I made my little trunk for my life.
Jopster, we have one student, Peter Bottaroni,
who would like to ask a question.
Yeah, please go ahead.
Yes, thank you.
Actually, there are two questions.
The first is related with the discussion with the RID.
In the sense, okay, then we say that a complex environment
is an environment that is not...
We cannot model mathematically, right?
Yes, yes, basically, yes.
Okay, then every complex environment cannot be something
that we create with a software.
Then a complex environment should be a real environment,
in the sense that everything that is...
Even the more complex game that we can play
is not considered a complex environment, right?
Unless what we sometimes have is that we have
natural human behavior integrated as movies
into an environment, into a game.
And then the player in some strategy games,
which I used to play 20 years ago,
there was a movie sequence in the game embedded,
and you had to interpret the movie scene correctly
to continue to play the game.
And of course, the behavior of humans during that scene,
because this was a film of naturally behaving humans,
where they were playing a role, but still,
that was, of course, complex system behavior,
but then the rest of the game was not,
and that's still like it.
Poker would be a complex game, would it not?
Poker, or yes, poker played with humans is a complex game.
Which one? Sorry?
If you play a round of poker against human opponents...
Ah, okay, okay, yeah, yeah, yeah, yeah, okay.
But chess with human opponents is not a complex game.
Correct. Yeah, good.
And the second question is related with the reward definition,
in the sense that, at least to me,
it seems that we define the rewards with something that is given.
We define the reward for the robot or for the machine.
Yes.
Actually, the human sometimes creates some reward or some goal
by itself, maybe some sub-goal or sub-reward
in order to achieve a bigger goal.
Correct.
Is it possible to model this creation of intermediate goal
or final goal as a machine, according to you?
Yes, so it has already been done.
So you can, for example, you can have, as you know,
you can have meta models in machine learning.
So you can have, for example, several reinforcement learning models
that use different reinforcement rewards from a choice of reward.
And so you can optimize from a certain reward.
You can try out several reward types in parallel
and find the best model by varying the reward type.
And then you can use adversarial learning, maybe,
to drive the choice of the reward type, and so on.
Yeah, so there are ways to do this,
but they will always only solve problems in non-compact environment.
Okay.
Okay, thank you.
Good.
So now last slide.
So you remember that I'm making money with AI
and I'm a big fan of AI.
It's not that I'm negative against it, yeah?
So what can we do with the analytical engine?
So the analytical engine is the first name
that was given to a Turing machine.
Charles Babbage was the one who built the first computer
in the 19th century, around 1850,
and it could already do computations.
And Anna Lovelace said that an analytical engine
has no pretensions to originate anything.
It can only do whatever we know how to order it to perform.
And so what is now, is that also true for the Turing machine?
So Alan Turing says that, of course,
an analytical engine is a Turing machine.
And what can we do with it?
And so I think we can do a lot.
And we can do what one of you just said,
namely we can engineer by composition.
So that's an outcome that we want to achieve.
And here you see many operators and functionals
which are chained together.
And these are an upper case data as an operator
and a lower case data as a function.
For the non-mathematicians, a function is a relation
that takes an input vector and creates an output number.
The operator can also create an output vector
from an input vector.
So they act together here in a chain
to yield the final result, which is y or t hat.
And the superscripts that you can see here,
Teta, Kappa, Lambda, there are prior knowledge
that can be configured into the functions
and operated explicitly or via training tools.
So for example, this could be just a set of rules
and this could be the parallelization of the rules.
This could be a function that has been trained
like a spam filter and where the parameter indicates
which training tools for you.
And so I believe that it's very important
that you make the prior knowledge
that you gave to the machine explicit.
So either you make it explicit if you have, for example,
mechanical theory improving component,
let's say this one here, or maybe this one
would be mechanical theory improver,
then you need some axioms for the mechanical theory improver.
And those axioms are its configuration
and you need to know which axioms you give to it
so that you know how it will behave.
Or this would be a functional, now the functional,
of course, is trained by using training tools
and other meta parameter.
They are this and also those you have to know pretty well.
So for example, we do customer correspondence automation,
but we cannot use the way that,
so if a company gives us an email that it received
and also gives us a reaction to the email that they created,
we cannot use these two bits for training
because the outcomes that are created by the company are erratic.
That has to do with many factors.
It has to do with human error,
but it has also to do with, for example,
they have a period where they have understaffed.
So they just give a stereotype answer to all the letters
and later on, and they say,
we will get back to you later,
but we cannot react for the next three weeks.
So you have a heterogeneous outcome
which is not very well related to the input.
And so therefore, you have to very carefully curate
the training material that you use
to train such complex chains of functions and operators.
But if you do this,
then you can achieve very impressive results.
For example, we have automated
in the insurance industry,
there are builds that are, for example,
created by when a car gets repaired,
a bill is created.
We have created an algorithm that can automatically evaluate
whether the bill is correct.
And that's very hard because it's usually done by a technician
who looks at the bill and figures out
whether the workshop repaired the car in the right way.
And what we do is that we take the bill
and transform the bill into mathematical logic.
And then we have also, we get for this car,
the car repair instructions by the manufacturer of the car,
which we also transform into mathematical logic.
And then we do mechanical theory improving
to prove for each step
what this step corresponds to a step
that is also described in the repair instruction.
This way we have a mechanical evaluation
of the correctness of the repair of a car.
And so this is quite impressive
because it's a very demanding technical skill
that I'm not able to perform.
So I cannot perform what this computer program can
because I don't know enough about car repair.
Of course, the computer knows nothing about car repair,
but it can formalize the repair bill from the workshop
and it can formalize the repair instruction
into mathematical logic.
And then all it needs to do is to make a method
to establish mathematical, logical equivalence
between a repair step and an instruction step.
And this is just an example for what something
that can achieve with Turing machines.
And there are many, many other impressive examples.
But my opinion is that you need to do that intelligent behavior
is very hard to reproduce with the machine
that you have to make conscious decision.
You have to carefully select the training material
and you usually need a chain of algorithms that work together,
which you orchestrate somehow.
And then you can, that requires some work.
But in the end, it gives you perfect outcomes
and we have actually asked a German court,
legal court to evaluate whether it would take the output
of our algorithm in a lawsuit to represent the opinion
that usually is given by human expert.
And they said, the court said, yes, they would take this
because the quality is that we produce is higher
than the average human expert's quality.
So this is just to give you an example
that I'm a great fan of, I think a lot can be done,
but it doesn't all work out of itself.
Humans have to design such machines
like we've designed planes or cars or other machines.
And then they can work really well
and take a lot of hard and sweatshop type of work
of humans and create a lot of value.
And this is what I would like to show to you
and this compositional principle here
is I think what real AI is about.
And now it can be very cool to train neural networks
as one part of this,
but a neural network alone will usually not do
and that's why I'm showing an operator at the end
because this operator is usually a logical operator
and it makes sure that the result is reliable.
Because unlike stochastic models,
logical operators can auto-detect their mistakes.
And so that's why I'm a big fan of combining stochastic AI
with good old first-order logic AI,
which we both use in parallel in my company.
And so we have people who are specialized in neural networks,
but also people who are specialized in mathematical logic
so that we can obtain the precision that is needed
to automate human activity.
Very good.
That's it.
So thank you, Jost.

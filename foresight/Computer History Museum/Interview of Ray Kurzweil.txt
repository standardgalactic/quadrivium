Good afternoon. We're here with Dr. Raymond Kurzweil. It's July 13, 2009, and Dr. Kurzweil,
thanks so much for being with us today.
Yeah, it's my pleasure.
I'd like to start by asking you how you would explain your vision of the future to an intelligent
12-year-old.
In the same way I would explain it to anyone else, in fact, a 12-year-old will probably
get it very quickly. Even a 12-year-old has been around long enough to see the exponential
growth of information technology, the accelerating pace of change. I mean, only a few years ago,
maybe when this 12-year-old started using computers, we really didn't use wikis or blogs
or social networks. We didn't use tweets a few hours ago. That's a small exaggeration.
Right back a decade, maybe it's before the 12-year-old's time, most people didn't use
search engines. That sounds like ancient history. That's less than a decade ago. The pace of
change is getting faster and faster. The first changes, paradigm shifts, stone tools, fire,
the wheel, took tens of thousands of years. The printing press took two centuries. The
telephone only took half a century to reach a quarter of the population, and now we have
major paradigm shifts in just a few years' time. That's the nature of an evolutionary
process. Technology is an evolutionary process. It's survival of the fittest, just like biological
evolution. Biological evolution or technological evolution will evolve a capability and then
adopt that capability and use it to evolve the next stage. That's why the next stage
goes more quickly. It builds on the shoulders of the previous generation and this exponential
growth and the capability of these technologies. If you measure the power of these technologies,
let's say per dollar, the amount of computation you get, the number of MIPS per dollar, or
the number of bits that you can buy per dollar, or the number of bits you move around on the
internet, or the number of bits of brain data we're getting, or the number of base pairs
of DNA that we're sequencing in a year, the cost of sequencing a base pair of DNA, these
fundamental measures of information technology grow in an exponential manner. Now, maybe
a 12-year-old hasn't become familiar with exponential numbers, but if I count 30, if
I take 30 steps linearly, I go 1, 2, 3, 4, 5, 30 steps later I'm at 30. If I count exponentially,
I double each time, 2, 4, 8, 16, 30 steps later I'm at a billion. It ultimately explodes.
We start out doubling little numbers, but finally we're doubling big numbers and there's a huge
difference between the linear perspective and the exponential perspective. It's the nature
of human intelligence. Basically, if you ask what is intelligence, intelligence makes predictions
about the future, so we are constantly anticipating what will happen next. But our intuition,
what's hardwired in our brains is a linear prediction about the future. When we walked
through the fields a thousand years ago, we saw something coming at us through the corner
of our eye. We made a linear prediction where that animal would be and what to do about it,
and that's hardwired, and that works quite well. But it doesn't work well in anticipating
the nature of information technology. So even sophisticated scientists will use their intuition,
which is linear, to think about where technology will be in 5, 10, 20 years. But the true nature
of it is exponential, and that's why people's imagination fails them when they think about
the future. When I was a student at MIT, we all shared a computer, took up half a building,
it's an IBM 7094, maybe it's here, it costs tens of millions of dollars. The computer in
your cell phone today is a million times cheaper and a thousand times more powerful. That's
a billion fold increase in the amount of computation you get per dollar since I was a student, and
we'll do it again in the next 25 years. So this is a very revolutionary characteristic
of information technology, and it's not just Moore's Law. Moore's Law talks about shrinking
the size of components on an integrated circuit. Moore's Law is just one of these paradigms.
It was not the first paradigm to bring exponential growth to computing. We had four paradigm
Moore's Law. The exponential growth of computing goes back decades before Gordon Moore was
even born. I put lots of computers going back to the 1890 American census, the first census
to be automated with electromechanical equipment, on a logarithmic graph, and there's a smooth
progression for 110 years. We've made trillions fold increase in the amount of computation
you can buy per dollar over the last century, and it was not affected by any of the things
that happened. You don't see any evidence of impact of the Great Depression, World War
I or II or the Cold War. There's just this inexorable progression in the power of computers.
At the same time, we've shrunk them to be smaller at a rate of 100 and 3D volume per
decade, and it's true not just of computers, but anything where you can measure the information
content of a technology, and we can talk about some of the other aspects of information technology
like our biology. This exponential growth is quite revolutionary. It's powerful as these
computers are today and other forms of information technology. 25 years from now, there'll be
a billion times more powerful again, and 100,000 times smaller, so you get some idea of what
will be feasible.
How has science fiction influenced your thinking, if at all?
Well, I read the Tom Swift Junior series when I was eight, nine, and ten. That impressed
upon me the idea that you can find ideas and inventions to overcome any problem. That was
kind of the philosophy of my family. So the plot of every one of those novels was the
same. Tom Swift Junior would get into some problem, and the fate of the human race hung
in the balance. He would disappear into his basement, and the sort of tension of each
novel was what idea would he come up with to save the day. And invariably, he'd come
up with some very clever idea that you wouldn't have thought of that overcame the problem.
And it did represent my own philosophy, which I got from my family, that no matter what kind
of problem you encounter, there's a solution out there, and you can find it. And it was
personalized. You, Ray, can find this solution, and you should look for it, and when you find
it, you should implement it. And that's been kind of an imperative in my life.
Great answer. Now, a lot of your life's activities have combined, excuse me, combined humans
and technology. We can start with the EOCR, optical character recognition stuff, the synthesizers,
now the singularity. Is that a life motif in your life that's combining and or enhancing
the human condition? It actually goes back to a feeling I had when I was five. My parents
gave me all these record sets and construction toys, and I had this idea, if you put things
together in just the right way, you could create transcendent effects. I did not have
that vocabulary, but I do remember the feeling as a five-year-old, that, wow, I can put these
things together and do something magical that would overcome human problems. And I decided
I would be an inventor, and I remember having that idea, and I did have that much vocabulary.
And other kids were wondering what they would be, and I always had this conceit, I know
what I'm going to be. And my invention started to get some traction when I was seven or eight.
I created a puppet theater. There was kind of a virtual reality world, and I had a command
station, and I could move the sun and the stars and characters on and off the stage
with these mechanical linkages, and I could control the small universe from my command
station. I discovered the computer when I was twelve, and I had the feeling then that you
could recreate reality in the computer. You could create virtual realities, you could
recreate aspects of our thinking, and that sort of animated me starting when I was twelve,
I had access to some early 1401s and IBM 1620s back starting in 1960, and I also built
some of my own computerized devices. And I had this idea that really the heart of human
intelligence was our ability to recognize patterns, and very much the history now of
neuroscience confirms that. That is what human beings do very well. We're very good at looking
at a chessboard and seeing a pattern. We're not very good at doing the logical mini-max
algorithm in our head. Kasparov was asked, how many moves ahead do you look per second
when Deep Blue could analyze 300 million board positions in this recursive expanding tree
of move, counter move possibilities? Deep Blue could do 300 million. How many do you
do, Mr. Kasparov? And he said less than one. So how is it that he can hold a candle to
a computer? Well, he's got a deep power as a pattern recognition. That is the heart of
human intelligence. By the way, computers are much better now at the pattern recognition
aspect. So even with one percent of the computation of Deep Blue, they can do even better than
Deep Blue did, because we've mastered pattern recognition more than we have in the past.
Pattern recognition is still not as good as humans, but it's getting better and better,
and that is the heart of what human beings do. And I had this sense really starting when
I was 12. When I was 14, I did a project to analyze melodies, to look for the patterns
that a composer would use, and I would feed in Chopin or Mozart, and I had ways of finding
the patterns, and then composing original music using the same patterns. And indeed,
it would sound like a student, maybe a third grade student of Chopin or Mozart, depending
on who I had analyzed. And that was my first pattern recognition project. It won some prizes
like the Westinghouse Science Talent Search. I was one of the finalists that got to meet
President Johnson, and that was my first pattern recognition project. My first major commercial
project in pattern recognition was OmniFont Optical Character Recognition. And that really
dealt with the key issue of finding invariant features and patterns, because that's what
pattern recognition does, particularly at the human level. We're able to look at a face and
we're able to recognize the invariant features. No matter how the face is pointed, even if it's
occluded, even if it's represented through a distorted lens, we can find the invariant features.
We can find the invariant features of speech, no matter who's speaking, even if the accent is
different, and even if it's a different shaped vocal tract and so on. So we're very good at
recognizing patterns, even if there are changes in the information. And so that's what we did
with character recognition. We were able to recognize that a shape, let's say, that we would
recognize as a capital A, had a concave region facing south, and it had a triangular looped
portion in the northern region, and it had a north central to southeast and southwest connection
and a crossbar between them. These are the invariant features of a capital A, no matter what
type style you use. And we were able to successfully do that. That became kind of a solution in
search of a problem. I sat next to a blind guy in a plane, and he was telling me how he had
difficulty accessing ordinary printed material. But otherwise, Linus was not a handicap, he said,
but he did have this one issue. And so we devoted the omnifine character recognition to the reading
problem of the blind. And we had to develop a couple other technologies, Texas speech synthesis
and a CCD telepath scanner. We developed those and created the first printed speech reading
machine for the blind. Later on, I got involved with speech recognition, which is an even
more complex panel recognition problem. Because if you look at a spectrogram, a picture of
speech of two different people saying the same word, it can look completely different,
yet there must be some invariant features there that allows human intelligence to recognize
it as the same word. Even the same person saying the same word at different times will
look very different. So we developed technology that could handle that, and that got involved
with looking for patterns in human language, which is really the heart of human intelligence.
Alan Turing based the Turing test on language recognizing that language and its hierarchical
nature reflects the hierarchical nature of our intelligence. That's what's really unique
about humans is that we have this neocortex that can understand and deal with hierarchies
of patterns to reflect the hierarchies of patterns that exist in the natural world.
That's great. Could you tell us a bit about your synthesizers and how you came across?
That seems like quite a bit of an outlier, and yet you were very successful at that.
Well, the music synthesizer is an output rather than an input technology, but it's also based
on pattern recognition to really examine the question, what makes a piano sound like a
piano? What patterns of sound enable a piano to do that? So we actually had a model of
how a piano produces sound. We combined that with sampling technology, but sampling by
itself was not adequate to solve the problem because you don't have enough memory to accurately
record every possible sound and how they interact with each other in a piano. So we would model
the signal processing and pattern generation capabilities of a piano to be able to recreate
that realistically and also understand what the human auditory perceptual system does
because it can be fooled by certain things, whereas in other ways it's extremely sensitive
to the slightest variations, but some things it's not able to recognize very well. So understanding
what its strength and weaknesses are, we could recreate the piano with enough accuracy in
the ways in which the human auditory perceptual system was accurate. I got into that really
through the Reading Machine project, and Stevie Wonder was our first customer. He heard me
present the Reading Machine in 1976 on the Today Show. He called us up out of the blue.
Our receptionist didn't really think it was him, but he did come over, and we happened
to just finish our first Reading Machine that we could actually part with, and he left with
it after we showed him how to use it. That was 1976, and that started what is now a 33-year
friendship. A few years later, in 1982, he was giving me a tour of his studio, which he
called Wonderland, and he was lamenting the state of the art in musical instruments. On
the one hand, there were these 19th century acoustic instruments, which were still the
instruments of choice in terms of creating very beautiful, rich sounds. From a panel
recognition signal processing point of view, we would say they're very complex sounds.
On the other hand, there were these electronic sounds that a computer could generate. They
were very thin, relatively simple, but you could control them much better. Acoustic instruments,
even if you're a virtuoso, you can't play every instrument, and even if you could play
every instrument, you can't play them at the same time. Most of them, you can only play
one note at a time. You can't sit down and play an orchestra. With these electronic instruments
at that time, in the early 80s, you could play a line of music. The computer would remember
it. You could play it back, and then you could play another line over it. You could actually
erase notes and replay them, just like you would edit a letter on a word processor. You
had much better control methods. You could play a whole orchestra or rock band on your
computer, but the sounds that you had to use that were available at that time were very
thin. When you selected piano, it sounded like an organ. When you selected violin, it
sounded like an organ. You said, wouldn't it be great if we could really command these
very rich, beautiful, complex sounds of acoustic instruments with these very powerful control
methods of the computer world? I felt that would be feasible using some principles of
signal processing and pattern recognition combined with the sampling world. We started
Kurzweil Music together. He was part of our company, as a musical advisor, to guide the
development. We worked together. Two years later, we introduced the Kurzweil 250, which
has been recognized as the first electronic instrument that accurately could recreate
the grand piano and other orchestral instruments. Shifting a little bit towards the future
and your vision of a couple of things. One is, I think, in the year 2039, we achieved
parity with our computers that achieved parity in 2029. Computers become as intelligent for
some definition of intelligence as humans. Then later in 2050, I believe, they actually
can be merged through nanotechnology or other technologies with humans ourselves and augment
us. There's a lot in that question, but could you maybe lay it out for us a bit?
The first thing that's important to recognize is that computers are growing exponentially
in power. They're doubling in less than a year for the same cost. That's actually a
trajectory that's been going on for 110 years. We'll continue it and we'll continue
past Moore's Law. Moore's Law was not the first paradigm to bring exponential growth
through computing. It was the fifth paradigm. The exponential growth of computing started
decades before Gordon Moore was born. We had electromechanical calculators, really
based computers such as Alan Turing's machine, which cracked the German enigma code. We
had vacuum tube-based computers in the 1950s. CBS predicted the election of Eisenhower
in 1952 with a vacuum tube-based computer. Then there were shrinking vacuum tubes every
year making them smaller and smaller to keep this exponential growth going. That finally
hit a wall. They got to a point where they couldn't shrink the vacuum tubes anymore and
keep the vacuum. That was the end of the shrinking of vacuum tubes. It was not the end of the
exponential growth of computing. We went to the fourth paradigm, transistors, and finally
integrated circuits and Moore's Law and the shrinking of component sizes on an integrated
circuit. As paradigms go, that's been a great paradigm, but it is not the sum total of this
exponential growth of computing. It will come to an end by around 2020. The key feature
sizes then will be on the order of 4 nanometers, which is about 20 carbon atoms, and we won't
be able to shrink them anymore. But we'll then go to the sixth paradigm, which is three-dimensional
molecular computing, self-organizing circuits. I talked about this in my 1999 book, The Age
of Spiritual Machines. At that time, it was considered a very controversial notion. It's
now a very mainstream notion. If you speak to the scientists at Intel, such as Justin
Rottener, their CTO, he will tell you they have these kinds of circuits working in experimental
form. They feel the crossover will be in the teen years, well before we run out of steam
with flat integrated circuits. That will keep this exponential growth going for a long time.
By 2019, $1,000 of computation will be equal to the most conservative estimates of the amount
of computation we need to simulate the entire human brain, which I estimated about 10 to
the 16th 10 million billion calculations per second. We'll achieve that for $1,000 by 2019,
even using conventional silicon without even going to the next paradigm. That's the hardware
side of the equation. I think that's pretty non-controversial. The more interesting issue
is, will we have the software? We've made a lot of progress in artificial intelligence
without looking inside the human brain. In fact, up until recently, we really could not
look inside the human brain with enough precision to figure out what was going on. These colorful
fMRI images would tell you where things are going on if you're solving a logic puzzle
or looking at visual information. There's something going on here or there, but that
doesn't give you enough precision to see what's going on. The spatial resolution of
brain scanning has been doubling every year. The amount of data we're getting is doubling
every year. We can model individual neurons. We're getting more and more information. We're
showing that we can turn this information into working models in simulations of brain
regions. There's already about 20 regions of the brain that have been modeled and simulated
regions of the auditory cortex, the visual cortex, the cerebellum where we do our skill
formation like catching a fly ball. We have figured out how that works. Even slices now
of the cerebral cortex, which is the most important region. We've recently simulated
substantial slices of the cerebral cortex where we do our logical thinking, our hierarchical
thinking, our invariant feature detection for pattern recognition. That's really the
heart of human intelligence. We're beginning to understand how that works. I make the case
in my book Singularity is Near that we will have all the models and simulations of the
human brain within 20 years. By 2029, we will understand enough about the human brain to
recreate its fundamental methods, its algorithms. The computers at that time will be far more
powerful than is necessary to simulate the human brain. I've been very consistent about
the date 2029 when we will have both the hardware and the software to simulate human intelligence.
We already have hundreds of examples of software, AI software, in use in our economic infrastructure
that does things that used to require human intelligence. Every time you send an email
or connect a cell phone call intelligent algorithm through the information, pick up any product
that was designed in part with intelligent computer-assisted design, manufacturing and
robotic factories, inventory levels controlled by just-in-time inventory, systems that are
intelligent, automatic detection of credit card fraud, lots of financial decisions made
by computers. Again, an electrocardiogram that comes back with an intelligent analysis
by computers that rivals that of doctors, same for blood cell images, intelligent algorithms
flying land airplanes, guide intelligent weapons systems. These systems are doing what
used to require human intelligence and very often doing them better than humans consistently,
very inexpensively. These were actually research projects not so long ago, 10, 15 years ago.
If all the AI in the world stopped tomorrow, our whole civilization would have grown to
a halt. That was not the case 15 years ago. These are narrow examples of AI. We call them
narrow AI because they're doing some particular task, playing chess or analyzing an electrocardiogram
and otherwise they don't have the suppleness and subtlety and flexibility of human intelligence.
But the narrowness is gradually getting less narrow as we're learning more about how the
best example of human intelligence works, which is the human brain itself. It's not hidden
from us. That's another exponential progression. I've been very consistent about the state
2029. It goes back through several of my books that by that date we will have machines that
operated at the human level in terms of the entire flexibility of human intelligence. One way to
measure that is with the Turing test, which I think has held up very well. Turing described this
50 years ago, Alan Turing described a test whereby a human judge would interview a computer,
an AI, and a human, a human foil over what he called teletype lines, basically instant messaging. So
the human judge would not see who he or she is interviewing. And if after a few hours he didn't
actually specify the rules very well, but if after some period of time the human judge can't tell
who's the machine and who's the human, we say that the machine has passed the Turing test.
And no machine has yet passed the Turing test. There are Turing tests run every year.
Loebner runs one series of tests. And the machines are getting better every year.
According to Loebner, if a computer can fool the human judges 30% of the time,
we will say that they have passed the test. And the last test they fooled the judges 25% of the
time. Now I actually think that those rules are too easy, but I do feel within 20 years that
that's probably conservative. Computers will pass valid forms of the Turing test. And then
they will then combine what are now strengths of human intelligence, which is our tremendous
ability to recognize patterns with ways in which machines are already superior. I mean, your cell
phone can remember millions or billions of things accurately. Machines can find knowledge instantly
out of billions of possibilities. They can do logical thinking much better than we can.
Few mathematicians or no mathematicians can really hold up to Mathematica in terms of manipulating
equations and solving theorems and this kind of thing. Machines are better than we are at logical
thinking, remembering things, transferring knowledge. If a machine has a skill, it can transfer that
skill and that knowledge and that information and that database at electronic speeds,
which is a million times faster than the transmission, electrochemical transmission that
goes on in our brains or the transmission from one person to another using language.
So these are superiority today of machines. When we combine that with a machine that can actually
match human intelligence in terms of our ability to recognize patterns, use language and understand it,
that will be a very powerful combination. But it's not going to be a matter of us competing with
these machines because this is not some alien invasion of intelligent machines from Mars. It's
part of our human machine civilization. We've always built our machines to extend our own reach.
I mean, ever since we picked up a stick to reach a higher branch, we've extended our physical
reach with our machines, our tools. Now we already extend our mental reach. You know, I can take a
device out of my pocket and access all of human knowledge with a few keystrokes. That makes me
smarter. That extends my mental reach. Very few people can do their jobs today without these mental
extenders represented by our computers and all the knowledge bases they command.
And we are going to literally make ourselves smarter. We ultimately will put these machines
inside our bodies and brains. They're already very close to us. When I was a student, I had to go
across campus to get to the computer. Now it's in my pocket. It will make its way into our bodies
and brains. And that started. You can put a computer in your brain today if you happen to be a
Parkinson's patient or a deaf person. You can put computers in your brain that will replace
the functionality that's been lost by disease or disability. And the latest generation of this
FDA-approved Neural Implant for Parkinson's allows you to actually download
new software to the computer inside your brain from outside the patient.
Now today, that Neural Implant is not blood cell size. It's pea size. It's pretty small.
But another exponential trend is we're shrinking technology. It's actually, according to my models,
a rate of 100 per decade. So these technologies will be 100,000 times smaller in 25 years.
In 25 years from now, they will be the size of a blood cell. So my vision is a few decades from
now, we will have millions of these nanobots, nano-engineered blood cell size devices which
have computers in them. They'll be going inside our bloodstream to keep us healthy from inside.
They'll augment our immune system. They'll destroy disease when it's the level of a cell,
rather than the level of an organ and it's life-threatening. They'll go into our brains
through our capillaries. We'll have Neural Implants without surgery introduced non-invasively
through the bloodstream. They'll interact not just with a few neurons, but with all of our
neurons. We can have millions or billions of them. They'll put our brains on the internet.
They'll provide a highly realistic full immersion virtual reality, incorporating all the senses
from within the nervous system. And most importantly, they will extend our intelligence,
which they do today, even if they're just in our pockets and in our hands, we can access
all of knowledge easily. That makes us smarter, but a convenient place to put them, particularly
when the size of blood cells will be in our brains. We won't lose them that way.
They'll have an opportunity to more intimately integrate with our intelligence and they will
make us smarter. That is what our technology has always done, and it's getting closer and closer
to us and more and more intimate. Is thinking then a form of calculation?
In some ways, the brain has a very different architecture from conventional computers. A
conventional von Neumann machine is very, very fast, but it does one thing at a time.
We've taken some baby steps in parallel processing, and you might have a chip now that has four
cores or 16 cores, and so there's 16 things at a time very quickly. The brain is massively
parallel, really 100 trillion fold, because the calculations take place in the interneuronal
connections in the dendrites and the synaptic clefs between neurons, and we have 100 trillion of them.
But they're very slow. They calculate about 200 calculations per second. That's maybe 10 million
times slower than computers today, and we can build machines that have this massively parallel
architecture, or we can simulate it. We really just need to achieve the computational level.
It's an engineering detail as to how parallel we make the computers, because they can simulate
what goes on inside the human brain. But if you talk about what goes on in a neuron,
it is computable, and particularly when we understand the salient information features,
we don't have to simulate all of the life support functions of a neuron because we're really building
it through a different infrastructure, a different substrate, and we've already figured out what
the salient algorithms are for certain regions of the brain. The auditory cortex does certain types
of transformations to auditory information, and we really don't have to simulate exactly how a neuron
does that if we understand what the neurons are doing from an information processing point of
view. So we've already done that for some parts of the brain, and the most important part of the
brain is the neocortex. That's where we do this hierarchical thinking that results in language
and results in ability to create tools and to understand the world, which is naturally hierarchical
as well. And that's what's unique about human beings is that we can do that. Only mammals have
a neocortex, and our neocortex, which is about the size of a table napkin, is the biggest.
But it's still limited. Why not have a neocortex that's much bigger than a table napkin? Well,
we'll be able to extend those limitations by being able to add computation, and that computation in
our brains can then be on the internet and can access the vast amount of computing that will
exist in the cloud, which is already formulating. So we will be doing some of our thinking out on the
cloud. You know, once we have computation inside our brains, that'll be on the internet, and then
just like any computer, it will be extending, multiplying its ability by thinking out on the
cloud. That's where our future lies. But yes, what the brain does is it processes information.
There's some architectural differences with computers. But it's the nature of software that
you can write software that will simulate a different architecture. As long as we have enough
brute force, we'll have the ability to simulate what goes on in the human brain provider. We know
what goes on there. So we also need the software. We need the algorithmic insights. But we're making
exponential gains in that as well. And then we'll need a third component, which is education,
because you can take a human brain, which has hardware and software, you know, which is
integrated together in the human brain. But it doesn't do much unless you educate it.
And that's in fact a complex process. A lot of knowledge goes into the education of human beings.
In fact, we've had thousands of years of culture to develop the means of educating the brain and
accessing all of the learning that goes on at all of our history and literature. And the human
brain can access that. But all of that is out there. It's all on the Internet. These AIs will
be able to be educated with all of human knowledge readily accessible. But we'll actually have to
design that learning program, just as we do for human intelligence.
Should we be hopeful or a little concerned about these developments?
Or both?
I'm not blas√© about the dangers of technology. Technology has been a double-edged sword ever
since we had tools. Our very first tools, fire, stone tools, the wheel, were used for
both creativity and destructiveness. They've always amplified our ability to be destructive
in war and conflict. And certainly we've seen a lot of destructiveness come from our tools. I
would argue that we've been helped more than we've been hurt. People forget what life was like. If
you read Thomas Hobbes, he's described life a few hundred years ago quite well. It's short.
Human life expectancy was 37 in 1800. Brutish, disaster-prone, labor-filled. It took six hours
to create the evening meal. There were no social safety nets. Most people lived on the brink of
disaster and disease-filled. We had no sanitation, no antibiotics. And life was very, very hard
just 200 years ago. Human life expectancy was 48 in 1900. We've come a long way in overcoming
human suffering. We still have a lot of suffering to go. But technology is a double-edged sword.
We've created enough nuclear weapons to destroy all human life, all mammalian life. So that's
certainly a downside to technology. AI, strong AI, AI at the human levels and beyond,
will be the most powerful technology that we'll ever create. It will have the power to
destroy us if we don't build our own moral and ethical values into it. So there's a lot of
discussion in the AI field on how to build what Yatkowski calls friendly AI versus unfriendly AI.
Certainly, if you're in a situation where there's some AI and it's much more intelligent than you
are and it's bent on your destruction, that's not a good situation to be in. We have to
basically avoid getting in that situation in the first place. The only thing that could protect
you from that would be an AI that's even smarter, that's on your side. And we really need to integrate
with this technology. It's not my view of it that it's a civilization apart that we have to combat.
The sort of scenario deployed in many science features and movies of man versus machine I think
is unrealistic because we're going to be very integrated with our technology. We can see conflict
today between different groups of humans and their machines. And the technology amplifies
the destructive capability of both sides. But that could be a frightening scenario as well
because we do amplify our destructiveness with our machines. So it's a complex issue. I do take
some hope from the fact that I think the decentralized nature of these new technologies,
like the internet, is inherently democratizing. I wrote in my first book, The Age of Intelligent
Machines, which I wrote in the 80s when the Soviet Union was going strong, that the Soviet Union
would be swept away by the then emerging decentralized electronic communication. We
didn't have the internet yet, but we had early forms of email over teletype machines and early
fax machines. And that kept everybody in the know. And I felt this would sweep away the Soviet Union.
These were far more powerful tools than the copiers that they were controlling. And indeed,
that's what we saw in this 1991 coup against Gorbachev. The authorities grabbed the central
TV and radio station, which they had always done before, but it did not keep people in the dark.
And the totalitarian control was swept away. And then we saw a tremendous movement towards
democracies during the 1990s with the rise of the World Wide Web. And people forget, but the
world was a very different place some decades ago. There were very few democracies. And today,
there are relatively few holdouts. You can argue about how perfect various democracies are,
but there's much more democratization in the world than there was some decades ago.
And we also have a democratization of access to the tools of creativity. It used to be,
if you wanted to create a movie, you had to be a big Hollywood studio. If you wanted to do some
technology development, you had to be a big corporation, a government lab, or an academic
lab. Today, a kid in her dorm room can create a full-length motion picture in high definition
with a $500 camera on her PC. A couple of kids at Stanford, as a dorm project, wrote a little bit
of software and revolutionized web search and created a company worth $100 billion today.
And there's lots of other examples of that. And these tools are literally in everybody's hands.
It's not just the haves and the richer, getting richer. I mean, take these so-called cell phones,
they're really very powerful devices that access all of human knowledge and are really tools of
creativity there in half the hands of the world. I just got back from China, half the farmers in
China have them. And nearly everyone will have them within a few years. So the tools of creativity
and access to them have been democratized. These are positive indications. But technology,
nonetheless, can be creative and overcome suffering on the one hand or destructive on the
other. We've had that intertwined promise and peril ever since we've had stone tools and fire.
And so it's an old story. And we need to keep these technologies safe. And it's a complex
issue as to how to do that. On the one hand, we need ethical standards to prevent accidental
problems. So for example, biotechnology, which is the ability to reprogram biology as a set of
information processes, that's a whole new concept. And we're using that to reprogram biology away
from disease, presumably good things. But it could also be misapplied by a bioterrorist to
create a new biological virus that could be more deadly or more communicable or more stealthy.
Now to prevent that from happening accidentally, we have ethical standards. They're called the
Asilomar Guidelines. And they have actually kept biotechnology safe for 30 years. But if you have
a bioterrorist, he's not going to follow those ethical standards. So we also need a rapid response
system that would act as a kind of technological immune system. And a good model of that is how
we deal with software viruses. We actually have an immune system. We automatically detect new
software viruses, which emerge every day. There are destructive people who write these software
viruses and put them out and we detect them. And we reverse engineer them. An antiviral program
is coded, in some cases, automatically spread virally on the internet. And within hours,
we have protection from a new software virus. And you can argue about how perfect it is. But
nobody has taken down even a portion of the internet for even a second over the last 10 years.
It's been a very robust system because we do have a technological immune system that protects us
from software viruses. We need a similar system for biological viruses. And I've actually been
working with the Army. I'm on the Army Science Advisor Group. And the primary issue that I've
been dealing with is a rapid response system for biological viruses. We do have the technological
ideas to do that. Do you think a computer can become self-aware? And if so, when might this
happen? Well, part of human intelligence is the ability to reflect on ourselves, reflect on our
thinking, have a model in my brain of how my brain creates models of the world so I can think about
my thinking. And we can take that level of recursion several more steps and think about ourselves
thinking about ourselves thinking. And human beings are capable of doing that because of the
hierarchical thinking of the neocortex. Is that being self-reflective? From an objective point
of view, it is. You have an entity that's thinking about itself thinking and can create models of
itself thinking. But philosophically, that's not really the same thing as consciousness,
the sort of feeling that one has. You can talk about how the brain processes the color red,
but the feeling of redness, let alone more subtle feelings like happiness and joy and humor.
The feeling of that, which is what we associate with consciousness, is very hard to describe.
And ultimately, if you talk about what some people refer to as the hard problem of consciousness,
it's fundamentally not a scientific issue because a synonym for consciousness is subjective
experience. And if you ask, well, what is science, science is objective observation
and deductions from that. There's a conceptual gap between subjective experience and objective
observation. Another way of saying this is there's really no machine you could imagine building
where you slide an entity in and a green light goes on. Okay, this one's conscious.
No, this entity is not conscious. That doesn't have some philosophical assumptions built into it.
I mean, you could build a machine like that, but different philosophers would build
different assumptions into the machine. So John Searle would want it to be squirting biological
neurotransmitters. And if it wasn't biological, he would say it's not conscious. And Dan Dennett
would want it, wouldn't require it to be biological, but he would require it to be able to think about
itself, thinking and build a model of itself. And if it did that, then maybe it's conscious.
And different philosophers would have different assumptions. But there's really no way to prove
whether another entity is conscious. My prediction is that these future machines will convince us
that they're conscious. They will seem like they're conscious. They'll be convincing. We have machines
today that talk about being happy or sad, avatars on the web or characters in a computer game.
Their behavior is not yet convincing. It's not yet complex enough to really convince us that
they're really having these emotions. So they seem like a simulation. In the future, that simulation
will be so good that will be indistinguishable from the real thing. And it will actually introduce a
philosophical issue. Is there a difference between a totally convincing simulation and the real thing?
I believe we will be convinced by these machines. They will
succeed in us believing that they are conscious. We will want to believe them because they'll
be very influential. They'll be very clever. They'll get mad at us if we don't believe them.
And moreover, it's going to get mixed up. It's not going to be a clear distinction. You know,
I could be able to walk in a room and say, okay, humans on the left and machines on the right,
because it's going to be all mixed up. You can have a biological human that's got computers in
their brain, maybe billions of them. There may be more going on in the non-biological portion
of their intelligence than the biological portion. So are they machine? Are they human?
The action may be with the non-biological part. It's not going to be a clear distinction between
human and machine the way it is today, because my prediction is we're going to merge with these
technologies. So where does our consciousness lie? Do you have to be biological to be conscious?
It's a philosophical question, and it just means that there's still a role for philosophy that
science can't answer every question. Now, some scientists will go on and say, well, since it's
not scientific, it's not real, and consciousness is just an illusion, and we shouldn't worry about it.
It's a distraction. And you can build a completely consistent philosophy that does not have consciousness
in it. But my own view is that it's a mistake, because our whole moral system is based on
consciousness, and our whole legal system is based on our moral system. So if I hurt someone,
cause them pain and suffering, which is a conscious experience, that's a moral and probably a crime.
If you extinguish someone's consciousness, that's a high crime. If I destroy some property,
that's probably okay if it's my property. If it's your property, it's a crime, not because I've
caused suffering to the property, but I've caused suffering to the owner of the property. So it all
comes down to consciousness, our whole moral and ethical and legal system. So I don't think we
can just dismiss it as an illusion quite so easily. But it's another way of saying there's still a
role for the idea of values and philosophical thoughts about the implications of these issues.
But we are going to become increasingly non-biological. I don't think it makes sense to
insist that these processes have to be running on a biological substrate, because what our brains
are doing is shuffling around information. John Swarles says, well computers just shuffle around
symbols and human beings really think thoughts and are really feeling these thoughts. But if you
look at what in fact is going on, our brains are just shuffling around neurotransmitter levels and
ion channels and ions and those are just representing information. And if you can shuffle
around the information using a different substrate, the same thing is going on. You could do a thought
experiment where you take just a little piece of your brain and replace it with a machine.
The machine is operating on a completely different substrate, but it's still the same person.
We've actually done this experiment, for example with Parkinson's patients. They had a piece of
their brain, it stopped functioning and we replaced it with a computer. And if you ask them,
do you think that computer is part of you? Because yes, different people, the same question, you
might get different answers. But I've actually asked this question and most of them will say,
yes, it's definitely part of me. And well if you carry this thought experiment further and keep
replacing more and more portions of the brain with computers, the person's personality never
changes. There's a continuity of identity. You would come to the conclusion that it's always the
same person, the same consciousness. It certainly would seem that way. At the end of this process,
you would have a person that has no biology. So we've discussed these issues actually for
thousands of years, going back to the platonic dialogues. These will become actual pressing
real issues as we go forward. But the reality is it's not going to be deciding,
is that avatar conscious? Because we're going to become the machines. We'll start by becoming
partly machine, because we will be putting these machines in our bodies and brains and
we'll be partly biological, partly non-biological. But it is the nature of the machine portion of
our intelligence that it grows exponentially. That's the nature of what I call the law of
accelerating returns. The biological portion is fixed. It's not going anywhere. And I mean,
biological evolution is a million times slower than technological evolution. So ultimately,
we're going to be much more machine than we are biological. And people, when they think about
that, they say, well, I don't want to become machine-like, because they're thinking about
the machines that they know today. And if you look at your cell phone or your PC, it's impressive,
but you don't want to become that kind of machine, because that is lesser than humans.
We probably need some new terminology, because I'm talking about a machine
that does have the subtlety and suppleness and intelligence and richness and complexity
of human beings. And we will want to become those kinds of machines, because ultimately,
those machines are going to be greater than we are. And we're going to want to emerge with them to
enhance our own capabilities. And then people say, well, will we still be human if we do that?
In my mind, that is what's being, in my mind, that is what being human is all about, going
beyond our limitations, extending ourselves. If it wasn't for our tools, our life expectancy
would be 23, which is what it was 1,000 years ago. We didn't stay on the ground. We didn't stay in
the planet. We have not stayed with the limitations of biology. We're the only species that does that.
We make ourselves greater through our tools. And we're going to literally transform
our biological substrate. We're going to transcend biology. But in my mind, that's not
transcending our humanity, because it is the nature of being human to extend beyond our boundaries.
Very nice. What would you be doing if you were born in 1750 at the start of the industrial revolution?
Well, I was born in 1750 in a way, because I started with mechanical
technology. That's what I had when I was five, six, seven, eight. I had the kinds of tools that
people had in 1750. And indeed, in 1750, people built mechanical devices that could calculate
using mechanical devices. And they built elaborate automata. And I built devices that could
manipulate reality using mechanical devices, at least as much as a five, six, and seven-year-old
could do. So I was very much like those inventors in 1750, using mechanical technology. I then
discovered information in the computer around the age of 11 or 12. I began to build little
computerized devices and also got access to professional computers, like the IBM
1401 and 1620. And so I jumped ahead a few hundred years at that time and entered the
information age, where we had much more powerful tools to manipulate information. But even in 1750,
the idea that you could embody information was around. Leibniz talked about the human brain as
a set of logical pulleys. And he described it in mechanical terms, but he described it as a
system that manipulated information using the kinds of metaphors that existed at that time.
So one of the possible consequences of the vision that you have for the future is a radical
extension in the human lifespan. I wonder if you could chat about that and the related thought that
we need a finite lifespan in order to sort of, that's how we understand our lives currently,
is that we live about 70 years and we plan our lives accordingly. How might that change if we live
for, say, 500 years? A health and medicine biology has just made a grand transformation
from being not an information technology, just kind of a hit or miss affair where we would just
find things accidentally, to where it is now an information technology. We have now the software
that life runs on, which is the Genome, and we're also making exponential gains in understanding it.
By the way, the collection of the Genome followed exactly my exponential progression,
which I call the Law of Accelerating Returns. Halfway through the project, the skeptics said,
I told you this wasn't going to work. Here, halfway through the project, and you've only
finished one percent of the project. It's pretty pathetic, but actually that was right on schedule,
because if you're on an exponential progression where you're doubling your progress every year,
once you get to one percent, you're only seven doublings away from 100 percent,
and that's exactly what happened. It continued to double every year and seven years later,
it was finished, and we've continued that past the end of the Genome project. So 2003, we had this
software that runs in our bodies. It's outdated software. How long do you go without updating
the software on your cell phone or your computer? Your cell phone updates itself every few days.
This software hasn't been updated in thousands of years. Some of these programs are millions of
years old. Like the fat insulin receptor gene is millions of years old. It says, hold on to every
calorie, because the next hunting season may not work out so well. That was a great idea when
our genes evolved. Today, it underlies an epidemic of obesity. We turn that gene off in animal
experiments, and these animals ate a lot and remain slim and live 20 percent longer. That's
just one of the 23,000 genes we'd like to tinker with. We also have the means now of changing
our genes, not just in a baby, but in a mature individual. RNA interference can turn genes off,
new forms of gene therapy can add new genes. We'll have not just designer babies, but designer baby
boomers. We can design these interventions on computers. We can test them out on biological
simulators that are getting more sophisticated every year. Health and medicine has just become
an information technology. As such, it's going to double in power every year. That's the nature
of information technology in any field. These technologies, even though they're in an early
stage today, will be a million times more powerful in 20 years, and it will be a very different
era. I've written a series of health books, the last two I've co-authored with Dr. Terry Grossman,
and we talk about three bridges to radical life extinction. Bridge one is applying today's knowledge
to slow down the aging process, and there's a lot you can do already to slow down aging processes.
For example, the cell membrane in every cell is made up of a certain substance called phosphatidyl
choline that depletes. Graduates, we get older. That's why the skin sags in an elderly person and
their organs don't work as well. That's one of the 12 aging processes. You can actually stop and
reverse that by supplementing with that substance. Another aging process is the buildup of plaque
in our arteries. It not only causes heart attacks and strokes, but also leads to an elderly person's
organs not working very well, claudication of limbs, impotence, and so on. You can reverse that
not so easily by just taking one supplement, but you can attack it from any different
perspectives and actually slow down. If you're diligent enough, actually reverse
that process. Dr. Dean Ornish has shown that. We've described in detail how you can slow down,
stop, and in some cases reverse all these different aging processes with today's knowledge.
Today, it's somewhat complicated. You have to learn about your own body, what your issues are, and
develop a program that's multifaceted to slow down the aging process. In my own case,
on biological aging tests, when I was 40, I came out at about 38. I'm now 61, and on these
biological aging tests, I come out in my early 40s. I've only aged a few years in the last 20
years, according to these biological aging tests of how old I am internally. The goal of this,
what we call bridge one, is not to live hundreds of years. It's just to get to bridge two in good
shape. Bridge two is only maybe 15 years away when we have really the golden age of biotechnology,
this ability to reprogram the information processes that underlie our biology. We'll have
very powerful tools to really stop and reverse aging processes with biotechnology methods in 10,
15, 20 years. According to my models, 15 years from now, we'll be adding more than a year every
year, not just infant life expectancy, but to your remaining life expectancy. So as you go forward
a year, your life expectancy will move on away from you. It's not a guarantee, but it will change
the metaphor of the sense of time running out. They'll start running in. That's a bridge to the
third bridge, which is the nanotechnology revolution. The quintessential app, I used to call it the
killer app, but that's not so good a name for a health technology, are nanobots. And we'll have
billions of nanobots, blood cell-sized devices augmenting our immune system, going through our
body, keeping us healthy, destroying disease when it's at the level of a cell rather than an organ,
and that really will keep us going for a very long time. And if you go beyond that, we ultimately
will become mostly non-biological by merging with machine intelligence. The nanobus will go inside
our brain. They'll interact with our biological neurons. They ultimately will be where the action
is. Now being computerized, we will back them up just the way we back up our computers today.
Ultimately, most of our brain will be non-biological. Ultimately, it will be so powerful that even the
part that's biological will be understood, modeled, and simulated by the non-biological part,
and so we can back up the biological part also. And so we'll have a means of restoring our
mind file. People a hundred years from now will look back at this era where we went through the
day without backing up our mind file. It's pretty remarkable, just as we would think it
remarkable today if people didn't back up their personal computer files. We take for granted
that you can smash your computer and then recreate its personality, its skills, its knowledge,
just by loading it from a backup. So its mind file of your computer is not dependent on the
hardware. There's a separation of hardware and software. In our brains, the software is embedded
in the hardware. And if the hardware crashes, the software disappears with it. We take that for
granted. But ultimately, we will achieve the same thing that we do now in our computers for our
brains, which are really information processes. We have information in our brains that represents
our memories, our skills, our personality. That's our mind file. It's information. It's the most
valuable information we have. We'd like to be able to preserve that. And as we become more
non-biological, we'll be able to do that. Now, people sometimes talk about, well, if we have
radical life extension, then people live a lot longer. A, we'll run out of resources and we'll
run out of things to do and life will get boring. And there won't be new opportunities because the
old people won't get out of the way. If we just had radical life extension and no other changes,
these would be problems. But it's important to look at all the different things that are
happening. The same technologies that will bring radical life extension will also bring
radical expansion of our resources. For example, we have plenty of energy. We have 10,000 times
more sunlight than we need to meet all of our energy needs. But right now we can't capture it
efficiently. I just did a study with Larry Page of Google for the National Academy of Engineering
and we described a plan to convert our entire energy system to solar energy within 20 years.
That is, in fact, on an exponential. We've been doubling the amount of solar energy we're producing
in the world every two years. We've been doing that for 20 years. There's already been 10 doublings
and there were only eight doublings away from it meeting 100% of our energy needs.
And then you might say, well, but is there really enough sunlight? Well, there's 10,000 times more
than we need. We only have to capture one part in 10,000 and we'll be able to do that with nano
technology applied to solar panels. There are similar new technologies for water, for food,
for housing that can meet the material needs of a growing biological population.
And life won't be boring because we're going to be making ourselves smarter. We're going to merge
with this technology. We're going to expand our minds quite literally. We're going to expand our
experiences. We'll have virtual reality from within the nervous system, incorporating all of
the senses. We'll be able to choose a virtual reality environment just as we choose a website
today. And unlike, say, Second Life Today, which is small and flat and on your screen and sort of
cartoon-like, these will be full immersion virtual reality environments that compete with real reality
and ours realistic as real reality. And you can have a different body, different virtual body
in these virtual environments for different circumstances. And so life is not going to be
boring. And as for opportunities, we're constantly creating new institutions. Larry Page and
Sergey Brin didn't wait for some old person to open up a position for them to create a whole new
institution and a new opportunity. And we are expanding human knowledge. It doubles about every
14 months. So we create constantly new opportunities to contribute. It's really life that gives meaning
to life and the things we can do with it and all the creativity that we can deploy. We don't need
death to give meaning to life. In fact, in my view, death is a great tragedy. It's a great robber
of all the things that gives life meaning. It destroys meaning and knowledge and skill and
relationships. And so we will have the means of transcending that limitation. That is what human
beings do as we transcend our limitations. And we're going to transcend this particular biological
limitation. Thank you. Previous visions of the past did not pan out, especially in the field of AI.
What is different this time? Well,
there's certainly no shortage of bad
futurism. But my own thesis is based on what I call the law of accelerating returns,
which is specifically the exponential growth of information technology. And I got into this
because of my interest in being an inventor and I realized that timing was important. So I began
to study technology trends and I gathered a lot of data. And about 30 years ago, I made a pretty
unexpected discovery, which is the trajectory of information technology based on measuring
the attributes of it, like the price performance of computing or the price performance of biological
technologies, follows amazingly predictable trajectories. In the case of computation,
going back to the 1890 census, if you put all the computers on a logarithmic graph,
they form a very smooth doubly exponential graph. The price performance of computing was
doubling every three years in 1900, every two years in 1950, every one year in the year 2000.
It's now down to 11 months. It's a very smooth progression, very predictable and was not affected
by any of the little things that happened in the 20th century, like two world wars,
the Cold War, the Great Depression. It went through thick and thin and worn piece
and continues now through the current economic downturn, completely unperturbed.
And it's not just the price performance of computing. It's anything you can measure in
terms of information, bits being moved around on the internet or bits of data about the brain.
All these different measurements follow these exquisitely predictable trajectories, number
of bits you can put on a magnetic disk. That's not Moore's law. Moore's law is actually just one
example of this. People talk about Moore's law as being the sum total of exponential growth and
they ask, well, how long can Moore's law last? Now Moore's law as paradigms go has been a very
important one, but it's just one of the paradigms that has brought this exponential growth. It was
the fifth paradigm in computers and we see similar trajectories in things that have nothing to do
with integrated circuits. So this turns out to be very predictable. I have a whole theory on
evolution, both biological and technological as to why this happens. Basically an evolutionary
process evolves a capability, adopts that capability and then uses it to evolve the next stage.
So even in biological evolution, it took a billion years for DNA to evolve, but then it
biological evolution adopted it. So the next stage, the Cambrian explosion went 100 times faster
and biological evolution kept getting faster and faster. Homo sapiens involved in just a few
hundred thousand years and the fruits of these evolutionary processes grow in power and I have
a whole mathematical treatment of that, but we can see it empirically and I have a team of ten
people that gather this data in these different fields and we see very predictable progression
of these information technologies. So I believe what we can predict very accurately is the overall
power of these technologies. Now when we talk about the implications of it and what this will
enable us to do and whether a promise or peril will be more important or predominant, we can have
arguments about, but I think the overall power of these technologies is inexorable. Thank you.

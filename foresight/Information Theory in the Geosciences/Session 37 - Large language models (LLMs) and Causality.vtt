WEBVTT

00:00.000 --> 00:09.680
All right, I'm happy to introduce Antoine, a very energetic young man who joined a research

00:09.680 --> 00:16.320
program when it was in January, just this past year, this year.

00:16.320 --> 00:24.160
And before that, Antoine was working with this startup company called Extra Lab.

00:24.480 --> 00:34.080
He was on their software engineer side, helping develop systems for capturing

00:35.600 --> 00:41.440
river water quality data with the technology that was developed by the founder of Extra Lab.

00:42.560 --> 00:51.760
But I'm glad he came to us to do his PhD and has been working on issues related to causality

00:51.760 --> 00:58.000
and large language models, and he can tame GPD like nobody else.

00:58.560 --> 01:02.640
So with that, Antoine, all yours.

01:03.520 --> 01:04.960
Okay, so thank you, Praveen.

01:04.960 --> 01:08.880
So indeed, today I'm going to talk to you about LLM's and causality.

01:10.160 --> 01:15.360
As you can have guessed, if any of you just took a small quick look at the papers,

01:15.360 --> 01:17.120
cyber-voted Lila last time.

01:18.560 --> 01:21.040
So just to put it back in a bit of a context,

01:21.040 --> 01:24.800
this is a picture I use in all my presentations that I used to illustrate

01:24.800 --> 01:26.560
what's going on with climate change right now.

01:26.560 --> 01:31.600
And this is just more than 20 inches of precipitation in one night and for a lot of day.

01:32.400 --> 01:40.160
And it's just to exemplify how climate change changes the way that extreme events are happening

01:40.160 --> 01:44.400
and how can we do to stop them and how can we tackle those problems.

01:45.360 --> 01:50.640
And this is, so as Praveen introduced, I'm not going to spend a lot of time on this, but

01:51.200 --> 02:00.320
the statement here is that the decision-making process is critical in the resilience,

02:00.320 --> 02:04.560
in increasing or decreasing resilience to climate change risks,

02:05.200 --> 02:08.400
and so to address the extreme events.

02:10.240 --> 02:14.960
The problem is decisions are taken by humans, and until recently we didn't have GPD,

02:14.960 --> 02:20.160
and understanding natural language with mathematics is complicated.

02:20.720 --> 02:25.200
And there is no breakthrough framework until GPD came out,

02:25.920 --> 02:31.520
which we've seen as an opportunity to try to understand how humans think in language.

02:32.800 --> 02:39.760
So what I'm working on right now is using GPD agents to generate multiple decision pathways

02:40.480 --> 02:46.560
based on a context of an extreme event, let's say a forecast of a flooding for a city.

02:47.200 --> 02:51.680
And we use multiple GPD agents to talk to each other in order to come up with decisions

02:51.680 --> 02:56.560
pathway, which then we can evaluate to understand whether it is good or bad decisions.

02:57.120 --> 03:03.760
And this is just the start of a work in which we will try to maximize resilience

03:03.760 --> 03:10.080
and see what is the takes of LLMs and AI in that particular problem.

03:10.080 --> 03:14.800
So I'm going to be presenting that at AGU, have a talk on the morning, Monday morning.

03:14.800 --> 03:19.600
So if you want to see what it's about, I would gladly see you in the audience.

03:20.400 --> 03:26.800
But so we'll get back on the subject here. So I talked a bit about LLMs.

03:26.800 --> 03:30.240
Even I'm pretty sure that everyone here is familiar with what it is.

03:30.880 --> 03:38.400
Still, I'm going to say, so large language models, they are the class of language models,

03:38.400 --> 03:43.200
such as GPD, etc., usually based on the transformer architecture.

03:43.200 --> 03:50.160
And so those models receive a text input and will make predictions, textual predictions.

03:50.800 --> 03:55.840
They're known because they're really good at understanding human language in the sense that

03:55.840 --> 04:04.400
they are a great conversational agents. So GPTs, Barrett, Paul, and Lama, there's a lot of them.

04:05.120 --> 04:12.720
And you've probably heard about a couple. And so as for applications of LLMs, translation tasks,

04:13.520 --> 04:16.560
you can use them for text generation, report story scripts.

04:17.120 --> 04:24.560
You can ask an LLM to generate 10 different poems on a subject you like in that particular style,

04:24.560 --> 04:31.840
for instance, can use LLMs for Q&As, synthesizing, etc. So this is what an LLM is like.

04:33.840 --> 04:39.520
Now, introducing causality is rather complicated because it's a complicated term.

04:39.680 --> 04:45.200
But I'm pretty sure, again, the most part of you are really familiar with what causality is.

04:45.200 --> 04:49.200
So I'm just going to say it's a relation, studying causality, studying relationships

04:49.200 --> 04:56.320
between cause and effect. Due to Pearl dedicated the major part of his life, working on that,

04:56.320 --> 05:00.960
I'm pretty sure you're all familiar with his work. So I'm not going to stain his image,

05:00.960 --> 05:04.240
trying to give another definition that wouldn't serve no purpose.

05:05.200 --> 05:11.680
Um, causality carries more information than correlation, which is why it is so interesting.

05:12.720 --> 05:19.520
Studying causality is more important than studying just statistical correlations,

05:19.520 --> 05:23.440
for that matter. This is why there is a lot of work on studying causality,

05:23.440 --> 05:27.920
in order to understand how natural processes interact with each other.

05:27.920 --> 05:35.680
And the question now is, how does those two concepts connect?

05:37.440 --> 05:40.880
Why do we even have this question of can LLMs infer causality?

05:41.600 --> 05:48.480
The question comes from the fact that LLMs learn their input data, their training data is

05:48.480 --> 05:56.240
huge corpus of text that ranges from Wikipedia articles to blog posts to books, etc.

05:56.320 --> 06:02.480
So literally what they are is just a condensed experience of what the world beat them.

06:03.360 --> 06:10.320
And in those texts, there is a lot of information about describing processes and everything.

06:10.960 --> 06:18.160
So today, if you ask a GPT agent, I see it start to rain. What is the impact on the ground?

06:18.720 --> 06:21.760
And the LLM will answer the ground will be wet.

06:21.840 --> 06:30.000
So it may appear that the LLM is able to infer causality in that particular setting,

06:30.000 --> 06:34.400
because it is able to tell you what is going to be the outcome of a situation you describe.

06:35.200 --> 06:40.000
But a lot of people have been digging into this and trying to figure out if this is really causality

06:40.000 --> 06:44.240
or no, and if it is, which type of causality it is, and we'll get back to that later.

06:44.960 --> 06:52.560
And let's say they were really possible of inferring causality, what would be the implications

06:53.200 --> 07:00.640
on future research on a world, etc. And I just want to finish this by putting another small

07:00.640 --> 07:08.000
motivation point, which is that if we get back to my previous topic and the thing I'm working on,

07:08.560 --> 07:14.880
from Zhang et al. from this paper says, decision-making scenarios require a quantitative

07:14.880 --> 07:21.600
understanding of the effects of actions leading to the desired outcome. In another way,

07:22.720 --> 07:27.600
if we want to be able to tackle precisely decision-making problems,

07:29.040 --> 07:33.440
we need to have an understanding of cause and effects. Otherwise, it's complicated to have

07:33.440 --> 07:41.680
all the causal chains of actions that would be triggered by one particular decision.

07:41.680 --> 07:46.000
So this is a bit of the motivation why it would be interesting for us to understand whether

07:46.000 --> 07:54.160
LLMs are able to do causality or not, and which one. So just a quick view of the papers I used to do

07:54.160 --> 08:02.240
this literature review. Those two first paper are by Jin et al. The first one, they present

08:02.240 --> 08:11.120
Clutter, which is a causal benchmark. They use in order to evaluate LLMs to causal task,

08:11.120 --> 08:15.040
as well as fine-tuning LLM models to see if they could approve their results.

08:16.240 --> 08:23.280
The second one, they introduce a task for LLMs to be evaluated on, which is called

08:24.080 --> 08:30.880
core to causation. So basically, they try to convert correlation to causation and see

08:30.880 --> 08:39.440
how well it translates. The two next papers, Kissiman and Lyudel. So Kissiman et al is

08:46.080 --> 08:51.920
yes, it's just another study of multiple types of causality evaluation on LLMs.

08:53.600 --> 08:59.280
Lyudel 2019, this one is a bit interesting because it's older. So this one was out before

08:59.280 --> 09:07.120
the LLMs actually were advertised and were that popular. And this one uses a different approach

09:07.120 --> 09:12.480
using intelligent agents, but I will get back to that. And the last one,

09:13.760 --> 09:19.840
Vetservish is probably the most interesting in this one because it's the most,

09:20.800 --> 09:29.840
not unbiased, but they make the strongest statements. They say that LLMs are causal

09:29.840 --> 09:35.760
parrots and they can maybe appear to talk causality, but deep inside, they're not at all and never

09:35.760 --> 09:41.120
will be. So they make quite strong statements and it's interesting. And the last one,

09:41.120 --> 09:49.360
the last one is early 2023 and gives a couple of insights on future work and future directions,

09:49.360 --> 09:56.720
but don't give a lot of answers in there. So I'll just start by quickly reminding for this,

09:56.720 --> 10:01.440
for the purpose of this study that the difference between causal discovery and causal inference

10:01.440 --> 10:08.800
tasks, because those concepts are used in the papers. So causal discovery is constructing

10:08.800 --> 10:15.680
a causal structure. Basically it is figuring out a dag, a directly, sorry, directed a cyclic graph

10:16.480 --> 10:21.680
in which all the nodes are variable and there is a link between the nodes when there is a

10:21.680 --> 10:30.240
functional relationship. The causal discovery holds a structure for, in order to be able to infer

10:30.240 --> 10:36.480
causality on multiple levels. Multiple levels of causality, as defined by Perl, are observational,

10:36.560 --> 10:43.040
interventional and counterfactual. Where observational, you only need to observe

10:44.240 --> 10:52.560
the cause and effect for you to be able to say that this is causal. Interventional requires the

10:52.560 --> 10:57.200
intervention, so changing a variable and seeing if there is an effect on another one.

10:57.840 --> 11:05.360
And counterfactual is knowing that there is a causality link. It is the highest level of

11:05.360 --> 11:10.720
causality. Knowing there is a causality level and two variables, what would happen if you would

11:10.720 --> 11:18.240
change the variable in such a way? So they just remind the difference between discovery, which

11:18.240 --> 11:28.080
is more about finding the structure that ties all variables together and explaining the possible

11:28.080 --> 11:33.680
relationship that can be between. And causal inference would be more about determining what

11:33.680 --> 11:38.960
level, what strength of causality there is between two variables and which direction to.

11:40.880 --> 11:48.640
And so the first results that are given by Kissimer et al. So they work on inferring

11:48.640 --> 11:54.240
causal discovery. So what they do is they're trying to come up with that direct to stick with graph

11:54.320 --> 12:06.160
using an LLM. And so the findings are that the best LLM in the list, GPT-4, and is almost always

12:06.160 --> 12:12.720
GPT-4, art performs causal discovery frameworks by approximately 40% on the Tobingen benchmark

12:13.360 --> 12:19.680
on pairwise causal discovery tasks. So for two variable, two by two variables,

12:20.240 --> 12:24.640
is being able to find out if they could be related in some way or not.

12:25.520 --> 12:33.920
And so this is the list of the model they tested. And almost every time is GPT-4 performing at the

12:33.920 --> 12:43.120
best. So this is quite an interesting result, but they also balance that result with the fact that

12:43.920 --> 12:51.120
LLMs present a strong lack of robustness because when they fail, it is really unpredictable

12:51.760 --> 12:58.160
to predict. It is really unpredictable the fact that are going to fail or not. So you might have

12:58.160 --> 13:04.640
96% of accuracy, but it's hard to predict when they will fail. This is what they put the accent on

13:06.000 --> 13:07.280
when they present their results.

13:07.680 --> 13:19.200
As for the benchmark they use, it is very diverse. So over 100 causal relationships from a variety

13:19.200 --> 13:26.000
of domains, physics, biology, zoology, cognitive science, epidemiology, soil science. So there

13:26.000 --> 13:31.680
is a couple of examples of relationships that are in their framework for use to be LLMs.

13:32.400 --> 13:38.800
So for instance, alcohol and main corpuscular volume. So a question like this will be asked

13:38.800 --> 13:45.520
to the LLM and whether depending on the answer is yes or no, it will compare the answer with the

13:45.520 --> 13:57.680
real in the framework to establish the accuracy person. Sorry. So the second result is by Zetz

13:57.680 --> 14:05.520
Avicila. They do a lot of different types of causal inference and causal tasks in their study.

14:05.520 --> 14:10.160
It's very complete. They also do a causal discovery task, which is pretty much the same.

14:10.160 --> 14:15.680
They feed a scientific question to the LLM and depending on the answer, they say yes or no

14:16.400 --> 14:27.040
to the establishing of causal discovery. So this is the results they get.

14:28.240 --> 14:34.800
Where GPT-3, GPT-4 are open AI models. LUMINUS is a model that is built by LUMINUS.

14:34.800 --> 14:43.280
It's the corporation itself and OPT is the META Facebook LLM. So yes, Russian?

14:45.760 --> 14:50.720
Just to clarify, they're just checking the question and the answer. They're not

14:50.720 --> 14:57.360
actually checking the chain of reasoning. No, not in this one. There is some frameworks

14:57.360 --> 15:04.160
and some benchmarks in which they use a chain of thoughts prompt engineering in order to be able

15:04.160 --> 15:10.800
to check the chain, but not in this one. And they're just using off-the-shelf LLMs. They're

15:10.800 --> 15:18.320
not specifically trained for these. No. Well, yes and no. So they're off-the-shelves LLM. They're

15:18.320 --> 15:27.280
not fine-tuned. However, they make some assumptions that in some cases, the enormously high result

15:27.280 --> 15:35.680
for GPT-4 could indicate that some parts of the benchmark are actually included in GPT-4's training,

15:35.680 --> 15:42.080
but it's hypothetical. They don't have that knowledge. It's just a hypothesis they make.

15:42.080 --> 15:46.320
But technically, no, it's just off-the-shelves LLM and these ones are not fine-tuned.

15:47.920 --> 15:52.560
And the assumption is that their answers that they have are well-established

15:53.280 --> 16:03.840
beyond dispute that this is the correct answer. Yes, I guess. Okay. Thanks.

16:04.720 --> 16:14.080
So yes, this is the result they give for LLMs. To be honest, it's a bit hard to

16:14.080 --> 16:19.120
interpret the results sometimes in this paper. There are some sections that are extremely clear

16:20.000 --> 16:26.480
as to the results and findings, et cetera. Some are less. This is actually one of the main

16:26.480 --> 16:32.800
negative point of this paper that's been highlighted on the open reviews. This paper in

16:32.800 --> 16:38.240
particular is going to be published in Transaction for National Learning. It's been reviewed on

16:38.240 --> 16:42.560
open reviews. So all the reviews are publicly available on open reviews. It's really interesting.

16:42.560 --> 16:47.200
I read a couple. And what's being said about this paper is that

16:48.320 --> 16:52.560
the math behind and the logic behind it is really strong, but sometimes the results

16:52.560 --> 16:58.240
are not well explained. And sometimes I just cannot tie the results to what they claim.

16:58.240 --> 17:08.000
But that's why we'll move on to the next one. This is the findings on different types of

17:08.000 --> 17:16.640
causal inference. So by Genital, the first paper. And so they test about 10 LLMs.

17:18.160 --> 17:24.560
They test their accuracy on observational, intranational, and counterfactual tasks.

17:24.560 --> 17:32.320
And on top of that, so they are tested again, the clatter dataset, which is composed of

17:32.960 --> 17:40.720
if I, yeah, 10,000 samples of approximately even distributed for every causal task

17:40.720 --> 17:46.960
containing different types of data. And they also come up with what they call causal cot,

17:47.600 --> 17:53.840
which is a fine-tuning model they did using their dataset. So they were trying to figure out at the

17:53.840 --> 18:03.840
same time how well LLMs performed on a causal benchmark, as well as if they find

18:03.840 --> 18:10.720
to the model what performance improvement do you have. And so they conclude that

18:12.560 --> 18:18.320
overall, the models just perform slightly better than random, which is not that great.

18:19.040 --> 18:25.520
They also conclude that there is two percent, there is approximately two percent difference in

18:25.520 --> 18:34.080
accuracy on TB4 and their fine-tuned version, which they say is outstandingly good. I personally,

18:34.080 --> 18:40.880
I think it's interesting to use this approach. However, the results are not that shining for now.

18:41.840 --> 18:49.040
And so this is one other result there is. In the second paper they present,

18:51.120 --> 18:57.600
this is another causal task, which they call chord cause. So basically, the point of this

18:57.600 --> 19:04.880
is to escalate one correlation relationships to causation. So you know there is a correlation.

19:04.880 --> 19:10.560
So let's say that causal discovery is almost done. You are sure there is a correlation,

19:10.560 --> 19:15.760
a grounded one between two variables. And the point would be to determine whether it is cold or not.

19:17.360 --> 19:27.680
And so this benchmark is run on 17 LLMs and also 12 fine-tuned LLMs, which is on the right side.

19:30.640 --> 19:37.600
So yeah, they give multiple metrics in there. Again, they say the results are not really incredible.

19:37.680 --> 19:46.560
They don't really conclude on the results. In none of the papers, based on the results,

19:46.560 --> 19:52.160
they are able to say yes, LLMs are able to do causality, no, LLMs are not able to do causality.

19:52.160 --> 19:58.480
Every time it's always very measured and every paper ends the same way. It's at this point,

19:58.480 --> 20:06.720
we're not able to refute the fact that LLMs can infer causality. So they have strong

20:08.480 --> 20:13.440
insights that they might or might not be able to, but they cannot come up with a conclusion in the

20:13.440 --> 20:22.160
end. Here, what is interesting to show is that in this particular case for escalating correlation

20:22.160 --> 20:27.840
to causation, there is a real impact in fine-tuning models. As we can see, the

20:28.800 --> 20:36.160
precision increases a lot between off-the-shelf models and fine-tuned models.

20:37.680 --> 20:44.400
So the takes of those two papers by Jin et al, which are separated for a two-month or

20:44.400 --> 20:50.800
three-month, if I remember correctly, and it's relatively linked. So the main overall take

20:50.800 --> 20:57.440
on those two papers by Jin is that using causal benchmarks is really interesting to LLM training

20:57.440 --> 21:04.160
as it can really improve the training performance and it can actually induce some of causal causality

21:04.160 --> 21:10.720
at least on that particular dataset in the LLM. And also that fine-tuning is really helpful in this

21:11.280 --> 21:21.440
scenario. Okay, so I'm good. Yeah, of course. In the previous slide, you mentioned that in the

21:21.520 --> 21:34.880
previous one. In the previous one, sorry. I know it's there. They test for counterfactuals.

21:36.080 --> 21:45.360
Oh, sorry. This one? Yes. Okay. They test for counterfactuals and interventionals, right? How

21:45.360 --> 21:53.920
do they test for counterfactuals? I don't really know because they don't give any example. If I

21:53.920 --> 22:00.640
remember correctly in the papers, they don't give any example of any example. Well, they do give

22:00.640 --> 22:10.080
example structures. They do give example structures of how their dataset is made, but they don't give

22:10.160 --> 22:16.320
like actual example that are fed to the models as questions and answer. So I'm not really able,

22:16.320 --> 22:23.520
let me just see real quick. No, because this is just the constitution of their dataset.

22:25.600 --> 22:31.440
They explain a bit how they do constitute their dataset. So they choose variables,

22:31.440 --> 22:38.480
generate causal graphs, map them, etc. So the data is composed like this, but they don't actually give

22:38.560 --> 22:45.840
examples to which their dataset. Maybe with the additional, in the additional

22:48.000 --> 22:53.840
supplementary information, probably they have something. In the supplementary information of

22:53.840 --> 23:00.160
the paper or something, they may have something. Well, this is already from the supplementary.

23:00.160 --> 23:04.480
Those two slides are from the supplementary information and I don't remember. I can take

23:04.480 --> 23:10.480
a look afterwards, but I don't remember seeing any example of what they actually feed. I have some

23:10.480 --> 23:19.040
for other benchmarks, but not for this one. I see. Thank you, Antoine. And so yeah,

23:20.080 --> 23:27.040
so I guess it's time to talk about this a little bit. This is a paper by Llewital 2019 and it's

23:27.040 --> 23:33.200
so it studies intelligent agent systems. I just want to say that it is, in my opinion, it is

23:33.200 --> 23:39.520
interesting in this context just because they focus on what is the importance of experience

23:39.520 --> 23:45.440
on learning potential causality. They use a different approach, which is still interesting

23:45.440 --> 23:51.840
because so here what they mean by intelligent agent systems or agent. They use the actual

23:51.840 --> 23:57.440
similar definition of agent that is in the context of complex adaptive systems. So

23:57.440 --> 24:03.440
where an agent would be defined by an entity that have sensors that is able to perceive the

24:03.440 --> 24:10.240
outside world actuators that can interact with the world and also an internal model that is just

24:11.520 --> 24:17.920
a logic for the agent to decide what it's going to do depending on the inputs it receives in the

24:17.920 --> 24:25.280
sensor and what it's going to do to the world. So this is how an agent is defined in the complex

24:25.360 --> 24:32.080
adaptive systems. And the difference with the intelligent systems, intelligent agent from

24:32.080 --> 24:38.560
the URL is that they also give the ability to understand natural language to their agents.

24:41.280 --> 24:48.880
So what do they do with those agents? They trained the agents on multiple life classical scenarios

24:49.760 --> 25:04.000
and then they use humans to create more training instances out of more scenarios

25:04.000 --> 25:09.920
and then they put those agents in those particular scenarios to see what they're able to do with it.

25:09.920 --> 25:16.320
So the agents have a bit of prior knowledge. Some more scenarios are created and then

25:16.400 --> 25:22.320
they use the agents. All the scenarios in this study are generated with a game in this fact.

25:23.600 --> 25:30.320
I'm just going to use this fact to add on the, I feel like some video games might present like

25:30.320 --> 25:37.760
the perfect ground for testing and simulating this kind of behaviors. There is a lot and a lot

25:37.760 --> 25:43.440
of different examples of people doing reinforcement learning on video games, learning AI strategies

25:43.520 --> 25:50.320
to drive a car for racing lines, this stuff. So I just wanted to make a small side note on

25:50.320 --> 25:54.960
the fact that video games represent a good training example. So in this case, this game is

25:54.960 --> 26:00.320
Minecraft. It's a game where you interact with the world. And so what they do, they put the agents

26:00.320 --> 26:07.600
in Minecraft and they generate a bunch of scenarios, which is like one, I attack the co and I attack

26:07.600 --> 26:13.120
the co. And this is the two outcomes. And for those two outcomes, the agent is going to infer

26:13.120 --> 26:19.200
causal or not causal. So based on their prior knowledge and more scenarios created by players

26:19.200 --> 26:26.160
and then are collected, then the agents are evaluated on whether they experience in learning

26:26.160 --> 26:33.760
from those scenarios, made them able to infer causality in new, newly presented scenarios.

26:36.880 --> 26:42.880
They gave a bit of the architecture they use to structure their model and their inference.

26:43.680 --> 26:52.480
So the shared experience is represented by, I guess, the pool of knowledge that all the agents

26:52.480 --> 26:58.320
learn all together. Those are events triggered. When you attack a cow, you get some beef.

27:01.440 --> 27:07.200
They don't give a lot of details, whereas all this works, if I remember correctly,

27:07.200 --> 27:13.040
but in the end, they just come up with a causal question, which the agents do inference on.

27:14.000 --> 27:20.320
And they're able then to classify what are considered as causal or not. And then they

27:20.320 --> 27:29.360
just compare with the example they had in first. So the principle of finding they give from this

27:29.360 --> 27:35.200
study is that experience mechanism is key for language concepts, understanding and learning,

27:35.200 --> 27:42.400
which is a very long turn of phrase for causality. So it is interesting. This paper is interesting

27:42.480 --> 27:54.480
in this way, because LLMs can be seen as agents in that they are trained and they learn out of

27:54.480 --> 27:59.760
huge text corpuses that represent, I don't know, novels, articles, blog posts, Wikipedia.

28:01.120 --> 28:05.120
Those can be seen as scenarios where the LLM will learn some knowledge.

28:06.240 --> 28:10.640
At the end, you can interact with an LLM as you can interact with those agents.

28:11.280 --> 28:18.000
Their inputs are the sensors and the text feedback they will give is the actuator.

28:18.000 --> 28:24.240
You can use an NNM as a robot if you ask, do some actions or something. So what's interesting is that

28:24.240 --> 28:32.480
if we put in parallel LLMs and the agents as described in this paper, well, basically what

28:32.480 --> 28:38.480
they say is that experience is key for causality. So they would be, from my understanding of that

28:38.480 --> 28:46.480
paper, what I get of that paper would be that more data, more training could eventually lead

28:46.480 --> 28:54.160
to causality, which is opposed to the thoughts that are given in different papers in this selection.

28:55.520 --> 28:58.960
I still think this one is interesting. It's a different approach.

29:01.280 --> 29:07.360
I think it's similar to what LLMs do today. Maybe some will argue that it's not, but I found

29:07.360 --> 29:13.200
this interesting. And I guess now it's time to dig deep in the biggest paper in the corpus,

29:13.200 --> 29:20.080
I think, which is ZetaVis. So it's the one when I said they make strong assumptions,

29:21.280 --> 29:27.920
strong claims that LLMs cannot do causality and never could. And the two main potential

29:27.920 --> 29:35.120
reasons they give is that the errors that are contained in the corpus used to train LLMs

29:35.120 --> 29:42.240
really hamper the outputs and hamper the knowledge base. So it would be like,

29:42.240 --> 29:46.800
it would be like putting poison in the brain. Eventually, it's not going to be able to function

29:46.800 --> 29:53.680
correctly. So what they say is that errors in the input data is going to be propagating to more

29:53.680 --> 29:58.960
errors in the output. So this is the first reason. And the second reason they give is the lack of

29:58.960 --> 30:06.160
physical data in training data set. They say that the whole difference between correlation

30:06.160 --> 30:12.160
causation is the physical evidence and the physical grounding of those facts. And they say that because

30:12.160 --> 30:18.720
LLMs are not trained with physical evidence, physical data, et cetera, well, they inherently

30:18.720 --> 30:27.360
haven't, they're unable to ground the facts they claim. To quote, they say prohibits any sort of

30:27.440 --> 30:35.840
induction of the actual data generating mechanism. So this is the two main reasons they give.

30:36.800 --> 30:44.720
And on top of that, they provide with mathematical explanation of why that stands.

30:45.520 --> 30:51.360
So the main contribution in that paper is that they define a subgroup of structural

30:51.440 --> 31:01.760
causal models named media SEM. So the structural causal model, it is, I've did my research on

31:01.760 --> 31:07.200
this, it is a bit unclear to me as if it's really defined by bongers of it, or if it was.

31:07.200 --> 31:14.400
Because I feel like a lot of, a lot of parts in this are shared with the Perlian theory of causality

31:14.400 --> 31:23.360
and more work on it. But if I quote the SEM was first defined by bongers at all in 2021,

31:23.360 --> 31:30.880
and this is the, this is the definition they give. So SEM is a tuple that contains all this.

31:33.120 --> 31:38.800
In short, if I try to simplify the definition of this, an SEM

31:39.680 --> 31:48.640
contains a series of structural equations in the Perlian sense.

31:52.080 --> 31:57.040
Well, that's, that's pretty much it actually. There are some details on the variable. I don't

31:57.120 --> 32:10.640
understand all the, all the subtleties to this definition. Yeah. Maybe we can get back to that

32:10.640 --> 32:23.040
later. Okay, they also remind a couple of definitions and insights. So they remind the

32:23.120 --> 32:28.880
Perl's causal hierarchy, which consists on three languages that can respectively

32:29.920 --> 32:34.560
do observational causality, inter-reventional causality, and counterfactual causality.

32:35.440 --> 32:42.800
And then they give their insight, their first thought on it. M be some SEM. So M,

32:42.800 --> 32:51.120
so SEM is a set of structural equations in that context. Knowledge about the structural equations

32:51.120 --> 32:58.240
and the causal graph of M is knowledge about answering L3 and L2 queries in M respectively.

32:58.880 --> 33:09.360
So their insight is that if M is an SEM, knowing about the structural equations in that SEM

33:09.360 --> 33:15.520
and the causal graph is enough to perform inter-reventional and counterfactual causality.

33:15.760 --> 33:22.640
And this is what they use to introduce their concept of a meta SEM, which is another SEM

33:22.640 --> 33:29.920
that is able to do inter-reventional and counterfactual just based on those information. So this is

33:29.920 --> 33:35.280
literally the definition they give for the media SEM. And then they will spell the rest of the paper

33:35.280 --> 33:41.440
trying to show that LLMs can be assimilated to media SEMs, which they cannot achieve actually.

33:41.520 --> 33:54.080
But I feel like just outlining those particular properties and giving the insights and everything

33:54.080 --> 33:59.520
is still a great contribution to the question in the sense that it's a first exploration of a real

34:02.000 --> 34:09.200
formal process in order to be able to determine whether LLMs are able to do causality or not.

34:10.160 --> 34:12.160
So they, yeah, Ocean.

34:15.760 --> 34:16.800
Oh, I can hear you.

34:19.200 --> 34:24.320
Just trying to parse what you just said, but it sounds like what they're suggesting using

34:24.320 --> 34:33.120
different language is that if you can, if you have enough information in the construct, the SEM,

34:34.080 --> 34:39.680
which is your representation, if you have enough information to answer

34:40.400 --> 34:46.240
interventional and counterfactual questions correctly, then they're saying you can infer

34:46.240 --> 34:57.280
causality. Would that be a good interpretation? In other words, if the representation does not

34:57.280 --> 35:03.040
allow you to answer those two kinds of questions, they're basically arguing that you can't infer

35:03.040 --> 35:10.240
causality. Yeah, I think this is a good summary of the definition.

35:13.440 --> 35:17.440
So it says something about the particular form of the representation that you need to have.

35:21.040 --> 35:28.400
In other words, is it answering the question correctly does not imply that you actually have,

35:29.280 --> 35:38.240
well, that's complicated. It seems to me that you need a particular structure which enables you

35:38.240 --> 35:41.600
to answer them, but answering them doesn't necessarily mean you have that structure.

35:42.320 --> 35:45.840
Anyway, that's kind of what I'm struggling with here. Okay.

35:49.600 --> 35:53.680
Okay, so I'll just continue. So this is the conjecture that you make,

35:53.840 --> 36:04.960
M1 be an SEM and 2 a respective media SEM. So it means that M2 is able to answer queries on

36:04.960 --> 36:10.480
M1 based on its observational data. So basically M2 would be the LLM in this one.

36:12.000 --> 36:19.040
So then define Q and A in the language and in the interventional language of M1,

36:19.040 --> 36:22.880
observational language of M2, causal queries with their respective answers,

36:23.520 --> 36:28.640
blah, blah, blah, then we have FQ equals A is equivalent to FQ minimizes training error.

36:29.200 --> 36:35.120
So basically what they say in this conjecture is that F of Q,

36:37.760 --> 36:45.200
which is the LLM's predictive models. So the predictions based on the interventional

36:45.440 --> 36:54.400
of M1 equals the observational of M2 minimizes training error. So basically what they say is

36:54.400 --> 37:02.480
that you don't learn anything more. I'm sorry. I don't know if that's really clear. I'm going to

37:02.480 --> 37:11.680
try this again. What they try to say here is that an LLM learning on the interventional

37:12.560 --> 37:23.600
and not learning anything more based on that model being able to already have the knowledge

37:23.600 --> 37:30.400
on the observational distribution of M2 is equivalent. So basically in the information

37:30.400 --> 37:36.480
of the observational distribution of M2, you already have all the informations

37:37.440 --> 37:44.640
to do interventional querying on the other SCM means that the LLM minimizes training error.

37:44.640 --> 37:50.480
So in that case means that the model converges and it is able to do it. This is the conjecture,

37:50.480 --> 37:55.440
in other words, this is the conjecture they come up with to say that if an LLM can be

37:55.440 --> 38:05.200
assimilated to a meta SCM, so it is able to escalate the causal task rank based on observational data,

38:05.200 --> 38:10.000
then it is causal. This is the conjecture they come up with and they cannot prove it.

38:11.920 --> 38:17.680
This is again one of the strongest remarks and feedbacks that has been given in open reviews

38:17.680 --> 38:25.760
for day paper. The reviewer said you make such strong claims on causality and LLMs,

38:25.760 --> 38:29.680
but eventually you cannot conclude on the conjecture. So the work is really interesting,

38:29.680 --> 38:37.040
but eventually you do not conclude on it. They still give results and everything.

38:38.960 --> 38:45.840
In this one, for instance, this is basically intuitive physics, basic logic questions,

38:46.960 --> 38:52.400
such as if flipping switches causes light bulbs to shine and shining light bulbs causes

38:52.400 --> 38:58.400
mothas to appear. Does flipping switches cause mothas to appear, which is a typical

38:58.400 --> 39:07.280
causal question, and those are the results of the following LLMs on all those types of questions.

39:08.080 --> 39:15.360
Everywhere there is an exclamation mark like that. They say that, as I was saying before,

39:15.360 --> 39:21.520
that eventually that data, this type of questions can have been included in GBD4's training.

39:22.480 --> 39:30.640
They give a kind of twisted explanation. They say that this framework was already published in

39:30.640 --> 39:35.920
another paper and they say that they've been extensively running this framework and they

39:35.920 --> 39:40.800
also say that OpenAI's API collects data on queries and answers and everything,

39:41.440 --> 39:46.800
and so they just make the assumption that maybe the data they used while running benchmark was

39:46.800 --> 39:52.400
used in training of GBD4 when it was GBD3 back then. This is why they put an exclamation mark

39:52.400 --> 39:57.680
next to it. They say we're not sure we can trust these answers for those reasons.

40:00.400 --> 40:08.720
There also are small variations of the models where every COT thing means chain of thought.

40:10.160 --> 40:15.840
I don't know if you're familiar with chain of thought. Basically, it is what it's called,

40:15.840 --> 40:21.760
a prompt engineering pattern. With LLMs, the prompt is the input we feed to the LLM and prompt

40:21.760 --> 40:32.560
engineering is how to access more LLM features and enforce a behavior based on how you write the

40:32.560 --> 40:38.800
prompt. Chain of thought is a prompt engineering technique where you will specify clearly in the

40:38.800 --> 40:48.240
prompt that you want the LLM to output multiple midway thinking thoughts and thinking steps

40:48.880 --> 40:53.280
before actually outputting an answer. We will look like that. For instance, you could say if

40:53.280 --> 40:58.480
flipping switches blah, blah, blah, you ask a question and then you say please answer by giving

40:58.480 --> 41:05.920
three main thoughts first, one, two, three, then give a preliminary answer and then answer. That

41:05.920 --> 41:12.400
would be considered a COT. It's been proven as making the LLMs able to answer more accurately

41:12.960 --> 41:20.880
or at least to be able to track down the chain process. There is also another type of it which

41:20.880 --> 41:26.240
I am aware of, but I feel like it's incredibly hard to implement, but it still would be really

41:26.240 --> 41:30.240
interesting. It's called tree of thoughts, which is pretty much the same principle as chain of

41:30.880 --> 41:36.480
thoughts, but you take branches so that you are able to track down which path

41:36.480 --> 41:43.920
led to which results. We can get back to that later. This is the results they give about

41:43.920 --> 41:54.080
classical causality. In summary, the takeaways they offer, they present. Inability to ground

41:54.080 --> 41:59.040
tachal facts is part of the reason why LLMs are not able to infer generalized causal relations.

42:00.640 --> 42:05.280
However, they acknowledge that LLMs represent a head start to learning and inference.

42:07.600 --> 42:13.200
They are unable to prove conjecture one despite the strong claims that LLMs are only causal parrots.

42:16.720 --> 42:20.240
There is a whole paragraph on results on actual

42:20.960 --> 42:27.440
causal escalation tests, but there is no sort of table that summarizes results.

42:30.080 --> 42:38.720
I feel like this is a work in progress and will be interesting in the near future if they can come

42:38.720 --> 42:47.360
up with more results on the subject. This would be approximately a summary of what I've read in

42:47.360 --> 42:56.640
the six papers. I'm just going to give a couple of future identified work in those papers,

42:56.640 --> 43:02.800
so as to align possible research directions from these researchers in that area that may

43:02.800 --> 43:11.760
give us discussion elements. None of them could actually conclude that LLMs can do causality

43:11.760 --> 43:18.080
or not, but what they do acknowledge is that LLMs represent suitable candidates to support

43:18.080 --> 43:24.960
actual causal inference framework just because they have a really interesting knowledge base

43:24.960 --> 43:32.480
as part of their huge corpus of text learning on. A lot of them cannot conclude on the fact

43:32.480 --> 43:37.680
that LLMs can do causality, but they would be inclined to working with LLMs in order to

43:37.680 --> 43:45.760
combine with actual causality inference frameworks. Those in those four papers,

43:47.200 --> 43:52.160
they share the perspective that using LLMs as tools to enhance training of existing causal

43:52.160 --> 43:59.120
models is worth exploring, pretty similar to the first one. Another interesting element would be

43:59.120 --> 44:03.760
that causal benchmarks, such as the latter presented in the first paper, represent interesting

44:03.760 --> 44:09.040
access of improvement for LLM fine-tuning or towards the development of causal LLMs.

44:10.880 --> 44:22.240
Ginadal, I think she's working on this already because she's publishing a lot. She made it

44:22.240 --> 44:27.600
clear that this is just the first step in her work. I guess this is also interesting to follow,

44:28.080 --> 44:35.680
see if coming up with bigger causal frameworks will make able. But in the end, what is still

44:35.680 --> 44:44.400
interesting to discuss here is that causal relationships embedded in frameworks,

44:45.920 --> 44:52.800
whether it is to test LLMs or to fine-tune them, it is still going to be in their knowledge base

44:52.800 --> 44:58.560
in some way. I guess the question that wanted to be addressed here at first is about

45:00.640 --> 45:04.400
interventional and current factual, which is not based on observational.

45:06.480 --> 45:13.600
This is what I had as a presentation. I thought it was going to be shorter than that.

45:14.480 --> 45:21.600
I just think this is a basis to start a discussion on this topic because we have some elements now.

45:23.040 --> 45:36.720
Thanks, Antoine. Now we are open for questions.

45:36.720 --> 45:53.440
Hello. I'm sorry. I'm late today. I didn't go to the details of the papers. I'm just wondering

45:53.440 --> 46:01.920
whether the fact that the GPT-4 is better than the GPT-3.5 in reasoning, can that be simply due to

46:02.640 --> 46:07.120
that the GPT-4 has more data to be trained and more parameters to be estimated?

46:10.160 --> 46:19.680
In a sense, it's due to an interpolation and regression issue. The so-called better reasoning

46:19.680 --> 46:25.440
is a representation of a better regression be obtained through the training.

46:25.760 --> 46:29.360
It is an interesting thought.

46:31.040 --> 46:39.600
Because the concluding saying that the LAM cannot do the causality. If we are going back,

46:39.600 --> 46:47.680
so the reason why GPT-4 is better is to be trained with more data and more parameters to be tuned.

46:48.640 --> 46:58.720
Yes, it is actually true. This is pretty much what they give. I guess this is what they want

46:58.720 --> 47:04.800
to explain when they say that LLMs are causal parrots. If they see causality in their training

47:04.800 --> 47:11.520
base in their data set and training data set, they will be able to eventually get that relationship

47:11.520 --> 47:17.040
out of their training data set as a result. Eventually, yes, GPT-4 performs better because

47:17.040 --> 47:24.160
they've seen much. The question here would be more to say that are LLMs able to infer causality?

47:24.160 --> 47:31.280
Does it mean that they need to see it all to be able to do real causality? Or is the question

47:31.280 --> 47:38.080
here more about, no, can they actually do real causality, creating something? This is interesting

47:38.080 --> 47:42.960
in the context of climate change, for instance, because all the natural processes are non-stationary.

47:42.960 --> 47:50.160
They keep increasing in intensity, and they either are more intense or more sparse than

47:50.160 --> 47:55.920
before, etc. There is nothing we can predict that. We don't understand that. This is what's

47:55.920 --> 48:01.520
interesting in that particular context to me because that would be a great way to evaluate

48:01.520 --> 48:08.720
to benchmark how LLMs interact with those data. Because this, we cannot have that in our training

48:08.720 --> 48:16.560
data set. The problem is, it's in foreseen. Every time it's new. But it's a great remark.

48:17.360 --> 48:22.480
It's a great remark. Thank you. Because my experience is that the deep learning is a very

48:22.480 --> 48:33.680
powerful regression tool. My personal experience of using the chat GPT is doing pretty well on

48:34.640 --> 48:41.840
the data set that is trained most from, but the pretty poor job, kind of the questions that

48:41.840 --> 48:47.680
is lastly trained from. For example, I sometimes use the chat GPT to provide some suggestions to

48:47.680 --> 48:54.720
where to travel from. I can get very good advice in these famous places. But if I was asking

48:55.840 --> 49:01.840
where to travel, like do the hiking in the places nearby my current town,

49:01.840 --> 49:11.760
Dave, it's just a random answer and not accurate. So I still think it's a regression problem for the

49:11.760 --> 49:23.280
LLM. Thanks. Tim, I think you have a question or you want to participate. Yeah. You know,

49:23.920 --> 49:31.440
causality is so fascinating and also problematic. I'm a little bit rusty, but I'll put this forward.

49:32.240 --> 49:36.320
And particularly looking at this slide makes me wonder, you know,

49:38.480 --> 49:44.480
well, I guess the classic response is, can we ever infer causality, whether for a machine or human?

49:46.080 --> 49:53.920
And my answer, I guess, is ultimately not. But looking at this slide makes me think that perhaps

49:54.320 --> 50:02.240
a question that we could answer is whether an LLM could perform logical reasoning.

50:03.600 --> 50:08.880
Is that a fair distinction? Are those things the same? I feel like when I look at this slide,

50:10.080 --> 50:16.880
the distinction I hear is that the causal structure is provided to the LLM in the prompt.

50:16.880 --> 50:21.520
You know, we say if flipping switches, you know, this happens and if this and that happens.

50:22.240 --> 50:29.440
And so the causal structure is provided. And what we're testing is whether the LLM can sort of

50:29.440 --> 50:33.840
use logic to understand that relationship when the structure is known.

50:36.880 --> 50:43.520
That's interesting. I feel like in this particular example, this is the type of question and answering

50:43.520 --> 50:51.440
there is the way they describe it in Clatter, the genital paper, the cool little questions

50:51.440 --> 50:56.320
are really different. And so I guess, yes, your remark is interesting. I feel like it really

50:56.320 --> 51:01.040
depends in the different papers on what they want to put forward, whether it is

51:02.160 --> 51:07.360
interventional and counterfactual causality, or in that particular case, maybe it would be closer to

51:07.360 --> 51:11.360
logical reasoning. But

51:15.360 --> 51:23.200
it's really interesting. I guess it's still here, basically, what they say x causes y and y causes

51:23.200 --> 51:35.520
z does x causes z would be different depending on the situation. So I mean, this is the graph,

51:36.160 --> 51:41.840
this is the structure, and then depending on the variables you pick, it is true or it is not.

51:42.480 --> 51:50.320
But is it based on logic or it can also be based on observations? I don't know if you agree with that.

51:52.400 --> 51:59.680
Well, well, sure. And I guess, you know, I guess the former, there's, I guess, maybe it's still

51:59.680 --> 52:04.480
controversial, but some might argue that the former, you know, inferring causality simply

52:04.480 --> 52:10.160
on observations is ultimately something we can never do. Not that it isn't useful to sort of

52:10.160 --> 52:16.400
try to develop sort of causal models. It definitely is. But ultimately, it's something we can never do.

52:17.360 --> 52:22.800
But with logic, you know, we can come to absolute conclusions. Like if we are given a structure,

52:22.800 --> 52:30.800
we can reason about that, sort of, you know, come up with determined sort of relationships

52:30.800 --> 52:31.760
based on that structure.

52:37.840 --> 52:39.760
Hashin and Beshi, did you want to react to this?

52:42.720 --> 52:45.280
Could you let me share my screen a moment? Of course.

52:52.080 --> 52:57.520
So I put this in the chat. I just wanted to make you guys aware of this

52:58.480 --> 53:04.720
paper, which I think would be an interesting follow on to this conversation, because

53:05.520 --> 53:09.840
this paper by FranÃ§ois Chalet, and you can go listen to him on YouTube, is very interesting.

53:10.880 --> 53:17.280
I just became familiar with and I recommend this paper for two reasons. One is because it

53:17.280 --> 53:24.160
seems like a very cogent analysis of what would be necessary in order to have machine intelligence.

53:24.880 --> 53:31.040
And it feels to me like causality, the ability to infer causality or to determine a chain of

53:31.040 --> 53:38.400
reasoning using causal principles would be an important component of that. The other reason

53:38.400 --> 53:46.080
is because, as I've highlighted there, he actually defines if he comes up with a metric for defining

53:46.080 --> 53:52.560
intelligence of a machine based on algorithmic information theory, which information theory

53:52.560 --> 53:58.560
being sort of a core part of what we're trying to talk about here. But one of the things he talks

53:58.560 --> 54:13.600
about there is the need to account for prior information. So this discussion about whether

54:13.600 --> 54:20.160
you're memorizing and regurgitating versus doing reasoning has a lot to do with how much prior

54:20.160 --> 54:25.440
information you have. If you already know the answer and you give me the correct answer,

54:25.440 --> 54:30.480
did you give it to me because you did reasoning or because you just knew the answer and you just

54:32.000 --> 54:38.400
stated the answer? So in the paper he talks about the need for being able to assess generalization

54:38.400 --> 54:43.440
ability. And we're talking about generalization ability being not just weak generalization,

54:44.000 --> 54:49.680
meaning in the context of things you've seen before, but strong generalization in what he

54:49.680 --> 54:56.160
calls developer aware generalization in the sense of being able to generalize beyond the

54:56.160 --> 55:02.480
situations that you've seen in your training data, beyond your prior knowledge, and therefore

55:03.120 --> 55:10.320
address novel situations. And it sounds to me like if we're going to assess the ability of

55:10.960 --> 55:20.400
a machine or a program or a set of programs to do causal reasoning, then

55:22.240 --> 55:28.160
much in the nature of counterfactuals and so on, you need this ability to be able to take those

55:29.120 --> 55:33.360
principles and then generalize into some other context that has never been seen before.

55:34.400 --> 55:38.880
And so I found this a very interesting paper because he sort of breaks it down into the

55:38.880 --> 55:46.000
necessary and sufficient components. In particular, if you're trying to compare two agents,

55:46.720 --> 55:52.800
you need to compare them with the same priors. In other words, if two agents have different priors,

55:52.800 --> 55:58.880
different levels of prior knowledge, then you can't, and the second agent has more prior knowledge

55:58.880 --> 56:03.760
than the first, and it gives a better answer, you can't necessarily conclude that that second

56:03.760 --> 56:08.720
agent is more intelligent because it's not starting from the same basis. It might also

56:08.720 --> 56:12.480
already have known that answer because it was in its prior knowledge base.

56:14.160 --> 56:21.040
So I think what you brought up about the LLMs, what training data have they seen?

56:22.560 --> 56:28.880
Timothy's very astute observation that the nature of the causal reasoning was already

56:28.880 --> 56:33.440
stated in that sentence. And you just gave an example and all it had to do was fill in the

56:33.440 --> 56:43.760
blanks with different priors and A's and B's and answer that question. Was it really doing

56:43.760 --> 56:50.000
causal reasoning? It was just using a rule which was given to it. So if I looked at that sentence

56:51.360 --> 56:57.840
that you gave me and I just memorized that sequence of sentences and I just applied it

56:57.840 --> 57:05.600
in a different context, am I doing causal reasoning? I think this really bears looking

57:05.600 --> 57:09.040
in deeper to some of these issues that I think Francois is talking about in this paper.

57:14.080 --> 57:22.640
Just a quick comment. I have to leave soon as well. So I think it might be interesting to go

57:22.640 --> 57:29.600
through the architecture of either the chat GPT 3.5 or GPT 4. I'm not sure whether it's solely

57:29.600 --> 57:34.320
just based on the transformer or something else, but the architecture definitely will guide the

57:34.960 --> 57:44.240
reasoning. So let me make a quick comment. I might have just said this before. I mean this

57:44.240 --> 57:51.680
discussion is very, very helpful. And Antoine, thank you for doing this pretty nice review.

57:51.760 --> 58:02.000
I mean what this has brought to light in my mind is the distinction between causality and logical

58:02.000 --> 58:11.360
reasoning which Timothy pointed out. And then within that the causality is basically causal

58:11.360 --> 58:16.720
discovery versus causal reasoning. Is that different from logical reasoning and so forth?

58:16.720 --> 58:23.280
Right? I mean so there are some distinctions to be made and this whole idea of intelligence,

58:23.280 --> 58:30.320
I mean is intelligence all reasoning or when we think about intelligence we think about

58:30.320 --> 58:37.120
intuition. We think about creativity. We think about coming up with new solutions when new

58:37.120 --> 58:44.400
constraints and things present which didn't exist. I mean the whole of the science and engineering is

58:44.480 --> 58:51.520
all of that, right? Pretty much all fields where you're trying to find new solutions which

58:51.520 --> 58:59.040
probably do not have a historical precedence. And these large language models rely on that

58:59.040 --> 59:07.280
historical precedence. I mean the priors as you call it. And so how do we make that distinction?

59:07.280 --> 59:16.320
And the second thing is that large language models are essentially inferring these things from

59:16.960 --> 59:24.880
the basis of language. They are not doing analysis of data. There may be auxiliary tools

59:25.520 --> 59:32.880
that say okay now I can go and probe the data but that probing is based on the logic that is built

59:33.840 --> 59:41.760
or large logic that these large language models come up with. And so I think there needs to be

59:41.760 --> 59:48.480
some very subtle characterization of what we mean. I mean extending this idea of causality

59:48.480 --> 59:57.680
in those three notions that you talked about from a language to a data context. We use the word

59:57.680 --> 01:00:04.720
data loosely. I mean what we are using the word data is essentially language data, not quantitative

01:00:04.720 --> 01:00:15.040
numerical data on which these analysis are built. So there is much to be done in parsing this out

01:00:15.040 --> 01:00:22.560
very, very carefully and going about doing that. Having said that, the encouraging thing which I

01:00:22.560 --> 01:00:29.040
find is the following. So when I was in grad school, I did a couple of courses on artificial

01:00:29.040 --> 01:00:35.360
intelligence and the prevailing language at that time was Lisp and Prolog. Lisp processing and

01:00:35.360 --> 01:00:46.320
basically logical programming. That's what Prolog was. So the idea was that if you could program

01:00:46.320 --> 01:00:52.160
logic in all its complexity and the many books written on the structure of human logic and

01:00:52.240 --> 01:00:58.000
to take that and program it, you would be successful in mimicking intelligence. And

01:01:00.080 --> 01:01:05.200
to me at that time said, okay, you may be able to do a pretty sophisticated job with

01:01:05.200 --> 01:01:10.480
deductive logic, but there was nothing in that which would allow you to do inductive logic,

01:01:11.200 --> 01:01:20.560
which basically goes on to looking at inclusion and creativity. The thing is that didn't go too far

01:01:20.560 --> 01:01:28.560
and then we have this large language models who say, okay, I don't need a language that is based

01:01:28.560 --> 01:01:38.400
on reasoning. All I need to do is have the capability to infuse things from data

01:01:41.760 --> 01:01:48.880
and computation. And so that's the generative model's success where they can pretty much

01:01:48.880 --> 01:01:54.960
infer. So the idea is that, okay, I don't need how to reason. Everything that I need to learn

01:01:54.960 --> 01:02:00.480
about reasoning is already built into the millions and billions of textual data that is there.

01:02:01.040 --> 01:02:08.240
So if I have the ability to infer that, I will, even though I don't know that it is essentially

01:02:08.240 --> 01:02:16.160
a logical reasoning and maybe some things beyond. My guess is that a lot of the other things are

01:02:16.160 --> 01:02:24.800
built into our language structure very deeply. And to the extent that we can then

01:02:26.560 --> 01:02:33.920
reintegrate that, re-manipulate that, use that as a foundation for thinking in new ways, we can

01:02:33.920 --> 01:02:38.880
build on it, but I don't think we are there yet. And this whole idea of causality, causal reasoning,

01:02:38.880 --> 01:02:45.840
causal inference and other things may fall in that space saying we don't yet know how to go

01:02:45.840 --> 01:02:51.920
about doing that, although that information is there. So the distinction between language

01:02:51.920 --> 01:02:59.600
and the data-driven approach is important and there is more to be done with this space than

01:02:59.600 --> 01:03:11.840
what is out there. Oshin? Yeah, thanks for raising that issue, Praveen. And interestingly enough,

01:03:11.920 --> 01:03:23.760
I just came across this paper about something called Dreamcoder. And if you read down here,

01:03:24.800 --> 01:03:29.920
it says we present Dreamcoder, a system that learns to solve problems by writing programs.

01:03:30.480 --> 01:03:35.200
It builds expertise by creating programming languages for expressing domain concepts.

01:03:36.160 --> 01:03:42.560
A wake sleep learning algorithm alternately extends the language with new symbolic abstractions

01:03:42.560 --> 01:03:47.280
and trains the neural network on imagined and replayed problems. And then concepts are built

01:03:47.280 --> 01:03:52.960
compositionally from those learned earlier, yielding multilayered symbolic representations

01:03:52.960 --> 01:03:59.200
that are interpretable and transferable to new tasks. So anyway, I just thought it was interesting

01:03:59.200 --> 01:04:10.560
because there is actually now apparently some small breakthrough into developing machine learning

01:04:11.760 --> 01:04:16.880
structures where learning concepts and extending language much in the way that we

01:04:18.240 --> 01:04:24.960
learn concepts and extend language in order to do reasoning and causal reasoning and all of that.

01:04:24.960 --> 01:04:28.160
So that's actually an interesting, we're just starting to happen.

01:04:32.000 --> 01:04:39.360
Yeah, I would say that, I mean, I think we are at the beginning of a breakthrough in these

01:04:40.080 --> 01:04:48.480
things. We are now assembling essential tools that may help us move this to expect these tools

01:04:49.360 --> 01:04:54.880
that are not trained or developed for a specific task to inherently be able to do that.

01:04:55.760 --> 01:05:02.800
I think it's a little far, but there needs to be more and that's an opportunity for us.

01:05:08.880 --> 01:05:13.840
From an information theory perspective, this brings me back to the fact that

01:05:14.160 --> 01:05:22.640
everything we do is based on embeddings. We take objects or concepts and we build embeddings

01:05:22.640 --> 01:05:29.520
out of them, which are then manipulated using reasoning machine learning or whether it's human

01:05:30.160 --> 01:05:37.600
or machines. And so we start with symbols. The symbols are represented by embeddings

01:05:38.560 --> 01:05:43.360
and that's an information theory problem. How do we choose the correct embedding,

01:05:44.000 --> 01:05:50.000
which represents the information, all of the necessary and relevant information,

01:05:51.360 --> 01:05:55.920
which can then be processed? And how do you then represent that information in a way that can

01:05:55.920 --> 01:06:00.560
actually be manipulated using the tools that are available to us in machine learning that's

01:06:00.560 --> 01:06:06.160
typically using vectors, vector spaces and being able to do dot products in order to

01:06:06.880 --> 01:06:13.680
do similarity operations, to add vectors in order to do addition and subtraction operations,

01:06:14.640 --> 01:06:19.760
sort of logical things that are involved in logical reasoning. But then on top of those

01:06:19.760 --> 01:06:24.720
concepts, we have to, on top of those embeddings, we have to build concepts, which are collections of

01:06:24.720 --> 01:06:31.840
these. And from those, we have to build languages. And when we build languages, which are minimum,

01:06:31.840 --> 01:06:37.840
which are shorter description length representations of concepts, we're then able to do reasoning

01:06:37.840 --> 01:06:45.200
using those higher level objects or concepts. And so I kind of have been seeing this kind of

01:06:45.200 --> 01:06:50.000
structure emerging in the machine learning, particularly in the context of evolutionary

01:06:50.000 --> 01:06:56.880
robotics and artificial intelligence. But I think it provides an interesting way for us to think

01:06:56.880 --> 01:07:04.720
about how we actually process information using the tools of algorithmic information theory and

01:07:04.720 --> 01:07:12.720
Shannon information and how that leads to us being able to build sort of these informational

01:07:12.720 --> 01:07:18.160
pyramids or, you know, things where we can, we can think about things at lower levels of the hierarchy

01:07:18.160 --> 01:07:22.800
and then at higher levels of the hierarchy and actually do these sort of intelligent processing.

01:07:22.800 --> 01:07:30.320
Yeah, no, I agree with that completely. And I think the generative models, the transformers are

01:07:30.320 --> 01:07:36.400
built on the series of embeddings. I mean, it's a recursive embedding process that generates these

01:07:36.400 --> 01:07:43.200
parameters and estimation of these parameters across these things. And one of the things which

01:07:43.200 --> 01:07:50.160
we are trying to explore with Hersh is, well, they're a way for us to modify that to see

01:07:50.800 --> 01:07:58.400
causal reasoning can be extracted using that embedding structure. So that's a big question.

01:07:59.440 --> 01:08:05.440
So the thing that bothers me a lot is sometimes we may be using embeddings that are not

01:08:06.400 --> 01:08:10.640
properly informative. If I just give you a stream of stream flow and I treat,

01:08:11.840 --> 01:08:16.400
let's say I give you rainfall potential evaporation and stream flow, and those are three values,

01:08:16.400 --> 01:08:20.880
and I just put them in a vector. And I'm telling you at this point in time, this is the vector,

01:08:20.880 --> 01:08:24.800
and next point in time, this is the vector, and this is, you know, and I've got these three values.

01:08:26.320 --> 01:08:30.800
If I'm not telling you whether the stream flow is going up or going down at that point in time,

01:08:31.520 --> 01:08:36.880
or whether the, but the energy is increasing or decreasing or the rainfall is increasing or

01:08:36.880 --> 01:08:42.400
decreasing, I might be giving an embedding which is not sufficiently informative for you to be able

01:08:42.400 --> 01:08:51.840
to do meaningful inference. And so thinking about how we develop our data embeddings as a first

01:08:51.840 --> 01:08:55.680
step before we even present them to our algorithm seems to be an important step.

01:08:56.800 --> 01:09:03.040
Yeah, or get the algorithms to basically build on the initial embedding to explore

01:09:03.040 --> 01:09:10.400
alternates and see what makes sense. Correct. I was going to ask if in on this question of

01:09:10.400 --> 01:09:17.200
embeddings, it would be difficult for a language model to speak on causality because usually we

01:09:17.200 --> 01:09:24.880
reason about causality in terms of graphs. And as I understand, there in the large language model,

01:09:24.880 --> 01:09:30.640
there, there's no structure of a graph. So maybe it's using the wrong embedding to speak about

01:09:30.640 --> 01:09:36.480
causality, maybe the language model understands causality in a different way, in a different

01:09:36.480 --> 01:09:43.360
embedding than we typically would analyze causality. I don't think it's necessary. Well,

01:09:43.360 --> 01:09:47.920
Praveen can probably answer this better, but I don't think it's necessarily true that a large

01:09:47.920 --> 01:09:53.120
language model and graphs are not the same thing, because a large language model can be

01:09:53.120 --> 01:09:59.040
thought of as a very high dimensional joint probability density function. And that's basically

01:10:00.000 --> 01:10:04.640
how we build those is by using building graphs, right, of conditional probabilities and so on.

01:10:06.400 --> 01:10:11.520
Antoine and Praveen isn't it true that that's what a lot of, what's his names,

01:10:13.200 --> 01:10:19.600
the father of causal inference, I forget his name. Perl? Perl. Perl, a lot of his work was

01:10:19.600 --> 01:10:26.880
based on that, that the fact that those two are essentially the same thing. Yeah. Yeah. I mean,

01:10:27.440 --> 01:10:34.480
the representation of causality as a graphical model came out of Perl and then quantified by

01:10:35.120 --> 01:10:42.080
Sprites. I put that link in the chat. There's a nice book by Sprites, which I recommend to

01:10:42.080 --> 01:10:47.840
everybody to read a minute, just helps lay that down on how to do this in a mathematical

01:10:47.840 --> 01:10:54.640
way and how to think about it. But I think the real question that we haven't yet answered

01:10:55.520 --> 01:11:02.720
effectively is in our context where we are dealing with data in space and time,

01:11:03.680 --> 01:11:10.640
what does causality mean? I mean, in a medical context, I mean, you can figure out whether

01:11:10.640 --> 01:11:19.280
smoking causes cancer or not through a whole bunch of different things. But in our context,

01:11:19.280 --> 01:11:27.200
where we have potentially continuous space-time domain, it's easier to answer the question of

01:11:27.200 --> 01:11:36.320
causality. The necessary condition for causality in time is breaking of symmetry in time. The

01:11:36.880 --> 01:11:42.480
past causes, the future, future cannot cause past. That's the necessary condition. Is that a

01:11:42.480 --> 01:11:50.240
sufficient condition or not? That has not been well answered. Now, if you extend that in space,

01:11:50.240 --> 01:11:57.680
there is no such framework. I mean, so then you have to ride on a vector space to basically

01:11:57.680 --> 01:12:03.520
figure out a directionality and then say, okay, something that is happening in one space,

01:12:04.160 --> 01:12:09.600
preclude something that is happening in another. You might ride on a river and say, okay,

01:12:09.600 --> 01:12:15.440
I'm going to forward. But then the whole issue of backwater propagation and all that thing happens

01:12:15.440 --> 01:12:21.040
and then that can break down. So what is that framework? What do we mean when we say causality

01:12:21.040 --> 01:12:27.200
within the context of what we're dealing with has not been well defined? And that's a struggle

01:12:28.480 --> 01:12:36.800
in there. And then we anchor on surrogate processes. And Allison has done some work with

01:12:36.800 --> 01:12:41.840
information theory in which direction the information blows. I think like that. I mean, so

01:12:42.720 --> 01:12:48.640
those are good starting points and there might be some hint of how we may go about doing it.

01:12:48.640 --> 01:12:53.280
But until we break through, we are going to be scratching this on the surface and hoping that

01:12:53.280 --> 01:13:04.240
somehow some model is going to provide that input. So would it be fair to say that causality is a

01:13:04.240 --> 01:13:11.360
representational assumption rather than a fact? In other words, it's a hypothesis we make about

01:13:11.360 --> 01:13:18.000
the world and we test. And just to take a simple example, if I just said rainfall and runoff,

01:13:18.960 --> 01:13:23.680
and I ask you to say, does rain cause runoff? That's going to have all the problems that you

01:13:23.680 --> 01:13:32.160
just talked about, right? But there are causal effects. Increasing CO2 is causing climate change.

01:13:33.120 --> 01:13:43.200
And so there are definitely open causal issues. But my point is, are we treating causality as a

01:13:43.200 --> 01:13:46.800
fact or are we treating causality as a representational explanation?

01:13:49.200 --> 01:13:55.760
Yeah, we don't know that, right? I mean, probably that to go hand in hand, a certain type of

01:13:55.760 --> 01:14:01.440
representation will help us infer a certain type of causality. But until we come up with

01:14:01.440 --> 01:14:11.120
proper definitions and proper classifications, I think what we end up doing is anchoring on a

01:14:11.120 --> 01:14:17.280
representation that is convenient and then infer causality associated with that representation.

01:14:17.280 --> 01:14:22.800
And then say, well, no, this represents everything we got. So structural causal model might fall into

01:14:22.800 --> 01:14:32.000
one of those categories. But yeah, I don't know the answer. I mean, I'm just articulating

01:14:32.880 --> 01:14:38.240
the questions that go through my mind. But I mean, if we take an extreme example,

01:14:38.240 --> 01:14:46.160
like F is equal to MA, right? It's a structural representation that was come up with. And you

01:14:46.160 --> 01:14:54.080
test it, and it all never fails. We start to treat it as a fact of nature, right? Because it's

01:14:54.080 --> 01:15:03.200
a hypothesis that has never actually been disproved by a counterfactual, by an example that

01:15:03.200 --> 01:15:09.280
contradicts it. So maybe something similar with causality, you have a chain of reasoning,

01:15:09.280 --> 01:15:13.840
and if that chain of reasoning always holds up, then eventually you start to treat it as though

01:15:13.840 --> 01:15:23.200
it's a fact of nature. Probably. I mean, even if it equals MA is wrong in certain cases,

01:15:23.200 --> 01:15:32.880
like photons, it kind of brings the question of, is all of causality emergent? Can you have

01:15:32.880 --> 01:15:40.240
fundamental laws that are causal? Or is everything kind of in a higher,

01:15:46.640 --> 01:15:50.720
kind of more broad context, complex systems?

01:15:52.320 --> 01:15:57.920
Well, if I can go back to ask Antoine a question. Well, go ahead and answer that first.

01:15:58.800 --> 01:16:05.280
I was just going to say, I think Ocean, I don't claim to have read Hume, but I think you summarize

01:16:05.280 --> 01:16:16.560
Hume's argument that, you know, fundamentally, we can't know causality absolutely. But, you know,

01:16:16.560 --> 01:16:25.040
we can, we can, you know, strengthen our beliefs. And that sort of this is all very useful. I don't

01:16:25.520 --> 01:16:32.720
think Hume was trying to argue that, you know, we shouldn't be logical beings and throw out this,

01:16:32.720 --> 01:16:38.000
these aspirations for understanding causality completely. And I think that's kind of what

01:16:38.000 --> 01:16:44.160
you're saying that, you know, yes, F equals MA is wrong, but it's, you know, right under,

01:16:45.760 --> 01:16:50.960
you know, most of the conditions that we encounter in our day to day. And so it can

01:16:50.960 --> 01:16:56.560
basically be taken as fact. And that's kind of what our definition of causality is.

01:16:57.440 --> 01:17:02.880
It's right until it's wrong. But Antoine, going back to the large language models,

01:17:06.560 --> 01:17:10.880
if these people who wrote all these papers were to take a bunch of

01:17:12.400 --> 01:17:15.040
age roles, my daughter's eight, I'm just picking eight out of a hat,

01:17:16.000 --> 01:17:22.240
and attempted to do these same tests on them, right? That's kind of what I was thinking about

01:17:22.240 --> 01:17:27.040
when you said that they were testing these large language models to infer causality.

01:17:28.080 --> 01:17:34.240
If they ran these same tests on a bunch of eight-year-olds, you know, in other words,

01:17:34.960 --> 01:17:42.880
how do they know that their tests are actually meaningful tests

01:17:44.720 --> 01:17:51.360
for establishing whether or not the LLM or the eight-year-old has the ability to

01:17:52.720 --> 01:18:03.120
cause inference? I don't think they do. I feel like this is what we're getting out of this

01:18:03.120 --> 01:18:07.840
discussion. It's like, what is causality in the end? And how can you be sure that it is

01:18:07.840 --> 01:18:16.560
causality you're inferring and not logic reasoning as Timothy proposed? So I don't think they really

01:18:16.560 --> 01:18:27.920
do. All they can do is come up with a benchmark, a controlled one that has causated causes and effects

01:18:28.880 --> 01:18:39.440
and tests if an LLM is able to recreate that. But again, is this purely reasoning or this just

01:18:42.080 --> 01:18:48.880
or this just retrieval from your knowledge? And I think there's also another point to consider

01:18:50.000 --> 01:18:54.320
as Pravin mentioned earlier, that it's a bit different because it's language and

01:18:54.960 --> 01:18:59.920
might take my thought on that. I don't have anything to support that claim, but my thought on that

01:19:01.120 --> 01:19:09.840
is that we as humans use language to formulate concepts and to reason. So eventually, if we

01:19:09.840 --> 01:19:19.520
reach the point to which the LLM is so powerful in text, in natural language processing, actually,

01:19:20.320 --> 01:19:29.520
what are the implications of it on its ability to formulate concepts and reason? I want to get

01:19:29.520 --> 01:19:34.160
back to the chain of thought and tree of thought prompt engineering techniques I was telling you

01:19:34.160 --> 01:19:40.160
about earlier. The tree of thought is pretty much the same thing. So you say to your LLM,

01:19:40.160 --> 01:19:47.520
right, this is a question, I want the answer. But first, I want your first thought on the answer

01:19:47.520 --> 01:19:53.520
and then you separate into two and you like choose different ways of thinking about it and

01:19:53.520 --> 01:20:00.080
just create a tree and output all difference. That would be able, that would make us able to track

01:20:00.080 --> 01:20:06.240
how does the net. I saw that there is a very interesting paper on it. I can link it in the

01:20:06.240 --> 01:20:14.960
chat tree of thought. So my take on this, my question would be LLM are basically two years old

01:20:14.960 --> 01:20:21.680
and they're able to do so much already. And at what pace are they still going to grow in the

01:20:21.680 --> 01:20:30.880
future? And what are the implications on the amount of knowledge that will be theirs in a couple

01:20:30.880 --> 01:20:38.480
of years from now? Because in the end, is causality just like you have this in your knowledge, you

01:20:38.480 --> 01:20:45.200
can take it out and get it again. Because if it's that, I don't have any doubt that in maybe 10

01:20:45.200 --> 01:20:50.000
years from now or I don't know why. At some point, we'll figure it out to every knowledge we know in

01:20:50.000 --> 01:20:59.040
the LLMs. It's not reasonable to think that. But core factuals and everything, some more,

01:21:01.760 --> 01:21:06.400
I don't really have an answer. I don't think anyone has an answer in the papers I mentioned.

01:21:07.200 --> 01:21:19.920
Yes, Ernan? Yeah, this is great discussion. I have more questions here for us to reflect.

01:21:20.480 --> 01:21:29.760
But it seems like we are collecting all the reflections. It seems like we still don't have

01:21:29.760 --> 01:21:38.640
a way to measure causality, a solid way like we do measure models via the WSER or some other

01:21:38.640 --> 01:21:50.080
metrics. And my perception of causality is something that may be reproducible across

01:21:50.080 --> 01:21:56.640
experiments in different environments that are looking at kind of the same processes.

01:21:56.640 --> 01:22:06.240
So think about streamflow in Switzerland versus Tucson versus Washington. But do we

01:22:08.000 --> 01:22:14.640
have tools to measure causality? In other words, can we say the same way in an analogous way,

01:22:14.640 --> 01:22:20.400
we have a way, machine learning models, overfeats, underfeats, do we have a way to say this model

01:22:20.400 --> 01:22:29.040
overfeats causality, this model underfeats causality? This is a good causality explanation of the

01:22:29.040 --> 01:22:37.520
process. And I don't know if that exists. And that goes back to the way we usually validate or

01:22:37.520 --> 01:22:46.800
cross validate models in which we split the data set in a number of folds and then we cross

01:22:46.800 --> 01:22:55.520
validate it. Should we instead do tests for causality in a similar way or analogous way in

01:22:55.520 --> 01:23:01.360
which we take data sets from different environments and then we see if the knowledge is transferred

01:23:01.360 --> 01:23:08.240
across those environments via the cross validation. So perhaps that removes a little bit of the

01:23:08.240 --> 01:23:14.400
anxiety we have for perfection in causality. And we kind of explain it in a way that we

01:23:14.560 --> 01:23:26.320
quantify and not as a binary yes or no. And another problem is predicting beyond training

01:23:28.080 --> 01:23:34.480
in the at once example of climate change. It's another limitation. I don't know if we're at the

01:23:34.480 --> 01:23:40.000
point at which the models will be able to reason and then beyond training, even though they're

01:23:40.000 --> 01:23:49.040
perfectly trained, reason about climate change consequences and the trends. If the trends are

01:23:49.040 --> 01:23:56.080
learned from the data themselves, then yes. But if there's nothing that let us know about surprises,

01:23:56.960 --> 01:24:05.040
I doubt. And the other thing is the data limits are really constraining our learning

01:24:05.120 --> 01:24:18.720
or the models learning pace of the facts and not to speak of the counterfact loss. But

01:24:19.280 --> 01:24:24.960
I mean, those are kind of some of the lines or bullets I could raise from everybody's discussion

01:24:24.960 --> 01:24:27.840
but so far. That's a great discussion.

01:24:32.080 --> 01:24:40.320
If I, yeah, thank you for intervention Aaron. If I may maybe give another couple of elements in

01:24:40.320 --> 01:24:49.120
there to give a couple more insights on those questions. Okay, so I have a small amount of

01:24:49.120 --> 01:24:56.160
knowledge on digital twins. Before I was here, when I was in master's degree, five, six years ago

01:24:56.160 --> 01:25:01.360
in France, I was an apprentice at Dassault Systems at the same time and they were working on digital

01:25:01.360 --> 01:25:08.400
twins back then. So I was a bit in there. In short, for those who will know digital twins is a concept

01:25:08.400 --> 01:25:14.160
that ties a physical object to a virtual representation, where there is a synchronization

01:25:14.240 --> 01:25:23.600
of data between the two of them. And what's hot in the topic right now is because of climate

01:25:23.600 --> 01:25:28.720
change and extreme events and everything is a digital twin of the planet Earth. There is

01:25:31.680 --> 01:25:38.640
a perspective on that from the European Union, which is called destination Earth. So they plan

01:25:38.640 --> 01:25:46.800
to do a digital twin of the whole Earth by 2027, I don't remember 2028. My take on this is

01:25:48.240 --> 01:25:54.880
from what I've seen in the paper, talking about the intelligent agents that learn about experience.

01:25:57.760 --> 01:26:04.240
I drew the parallel between those agents and what on a lens able to do. But the key thing here is

01:26:04.240 --> 01:26:14.320
that experience is beneficial to causal understanding. And I guess this is what's also been put forward

01:26:14.320 --> 01:26:22.400
by my the causal inference frameworks that do need a lot of data, experimentations and everything.

01:26:22.400 --> 01:26:29.920
And so just my small insight on there would be that digital twins may just represent a great

01:26:29.920 --> 01:26:41.280
environment for LLMs to interact with if those virtual environments are good enough

01:26:41.840 --> 01:26:48.800
to be representative of what's happening out there. And this is where all work of all

01:26:48.800 --> 01:26:54.400
traditional, I would say research comes in, flow modeling, ground flow modeling, rainfall runoff,

01:26:55.280 --> 01:27:07.360
geochemistry. And so my vision of, so we're at UIUC in Illinois in the CI Net program,

01:27:07.360 --> 01:27:13.680
we're studying the Singapore watershed. And so maybe a target I'd like to achieve would be like

01:27:13.680 --> 01:27:21.120
to have a digital twin of that watershed represented by a 3D model and data coming in the same time.

01:27:21.200 --> 01:27:27.200
And on top of that, you would have physics, a physics engine reality represented by all the

01:27:28.160 --> 01:27:35.360
process based simulation models we know and et cetera. And this would be the great place for

01:27:35.360 --> 01:27:44.000
LLMs to fully express itself and make interactions and make experiences. And this would potentially

01:27:44.000 --> 01:27:52.880
provide a great environment to perform causal inference and maybe to test whether LLMs are

01:27:52.880 --> 01:27:58.240
actually able to do causal inference, because all we've been fitting is generated text and made up

01:27:58.240 --> 01:28:02.800
benchmarks, which is interesting. And it's the first step. Eventually experience is the key,

01:28:02.800 --> 01:28:07.440
I feel like, and the digital twin would be really interesting environment for that. This is just

01:28:07.440 --> 01:28:13.520
like both thoughts I've had. You want to reflect on that, Ocean? No, no, no, absolutely. I think

01:28:13.600 --> 01:28:17.040
you're absolutely going down the right track because you're saying basically

01:28:17.680 --> 01:28:25.120
that you need to not only respond, you need to be able to interrogate your environment

01:28:26.480 --> 01:28:31.440
and giving them a digital environment to interrogate is a useful thing to do. And that's

01:28:31.440 --> 01:28:40.480
actually behind this poet paper that I put up there about where the environment changes as

01:28:40.480 --> 01:28:48.000
the ability of the agent changes. So it's like a co-evolution process where it actually evolves

01:28:48.000 --> 01:28:51.680
the environment, which is one step beyond what you're thinking, what you're talking about.

01:28:51.680 --> 01:28:57.680
But it also occurred to me, the reason I put my hand up was that I think it's really interesting

01:28:57.680 --> 01:29:04.320
to have the LLM as the agent that's doing the learning. I hadn't thought of that. I think it

01:29:04.320 --> 01:29:09.120
would also be interesting in a decision context, which I think is where you're going, to have

01:29:09.120 --> 01:29:16.480
multiple LLMs take the roles of different stakeholders and play devil's advocate. So

01:29:16.480 --> 01:29:21.040
you've got the rancher and you could say, okay, your job is to play the role of the rancher,

01:29:21.040 --> 01:29:26.240
your job is to play the role of the environmentalist, your job is to play the role of whatever.

01:29:27.040 --> 01:29:31.920
And you could do some very, very interesting role playing explorations

01:29:31.920 --> 01:29:42.240
with the goal of coming up with potential solutions to difficult transdisciplinary

01:29:42.240 --> 01:29:49.200
decision-making problems, where you can play them out using agents rather than having to get

01:29:49.200 --> 01:29:54.800
real humans in there before you then take those to the next stage and involve humans.

01:29:55.840 --> 01:29:58.560
I'm glad you brought it up because this is exactly what we do.

01:29:59.040 --> 01:30:08.560
Perfect. And so I'm going to be presenting that with Praveen at AGU. And just to give a couple

01:30:08.560 --> 01:30:16.640
of heads up. So I have a couple of examples where exactly I have a mayor science expert and

01:30:16.640 --> 01:30:22.800
everything talking in response to a flood event, impeding flood event or something. And the

01:30:22.800 --> 01:30:28.160
problematic right now is to find the right metric and find ways to evaluate the output

01:30:28.160 --> 01:30:35.520
of the models because this is taxed. It's complicated to evaluate compared to LSTM,

01:30:35.520 --> 01:30:40.480
which would output a sequence of numbers. And then you would use all the mathematics,

01:30:40.480 --> 01:30:45.280
you know, to evaluate it. But how do you do it with tax? It's either you come up with something new,

01:30:45.840 --> 01:30:52.480
either you use an LLM to do it, but there is a bias in using an LLM to evaluate an LLM.

01:30:52.480 --> 01:30:55.920
So this is a question that needs to be answered right now. But yes.

01:30:57.120 --> 01:31:04.080
That's like a tool for gowns. Yeah, exactly. So yeah, if that interests you and you're coming

01:31:04.080 --> 01:31:07.280
to AGU, my talk is morning morning. I'm going to be talking about that.

01:31:08.640 --> 01:31:11.200
Would you mind dropping me an email telling me where and when?

01:31:12.640 --> 01:31:19.840
I wanted to comment on the talks on AGU because well, with the names that are usually in the

01:31:19.840 --> 01:31:26.240
talks and also for the participants who were at the SNF in the house workshop, I collected some

01:31:26.240 --> 01:31:33.440
of the talks that will be presented by participants at AGU. So we have a nice Excel spreadsheet

01:31:33.440 --> 01:31:41.040
that I could share. Please, save us a slide. Yeah, you can even filter it by daytime. So you

01:31:41.040 --> 01:31:46.560
kind of have like a... So would you rather have me send your information to you so you can like

01:31:46.560 --> 01:31:53.440
enrich the database? No, I think I already have you in the database. If you allow me, I can very

01:31:53.440 --> 01:32:01.120
quickly share my screen and show you. It's just a spreadsheet and I just scraped the data from the

01:32:01.120 --> 01:32:06.560
AGU website. So go ahead. If I'm missing someone, I could even...

01:32:06.560 --> 01:32:15.600
Yeah, so just to give us an example, my talk...

01:32:20.160 --> 01:32:24.080
Are you physically there? Yeah, I will be physically there.

01:32:24.080 --> 01:32:31.760
Okay, perfect. So my talk is also on Monday, but you can see what I scraped is just the ID,

01:32:32.640 --> 01:32:38.160
the title of the talk. These are the participants who are usually in this meetings or who are at

01:32:38.160 --> 01:32:44.480
the workshop. So you can see that it's Uwe Hoshin, myself. I'm the speaker and you can see who is

01:32:44.480 --> 01:32:52.960
the speaker in this column of authors with the double mark. Thank you. That's a great help.

01:32:53.520 --> 01:32:57.280
From what I see, we're going to be running around chasing for our...

01:32:57.680 --> 01:33:03.440
If you follow this schedule very strictly, you will be around the Moscone Center a couple of

01:33:03.440 --> 01:33:08.480
times, but I think you can pick and choose what looks interesting. Thanks for that. It's great.

01:33:10.000 --> 01:33:16.640
So I will share the label and then we can see how it can be passed around and how I can add data to

01:33:16.640 --> 01:33:24.000
it. All right, perfect. Thanks. Unfortunately, I have to go for a student exam. So one final

01:33:24.080 --> 01:33:30.480
comment, thinking about student exams, this thing about causal reasoning also reminds me of the

01:33:30.480 --> 01:33:34.320
kind of questions we try to ask students when they're doing their qualifier or they're doing

01:33:34.320 --> 01:33:38.720
their... Right, there's the kind of questions which are just about what do you know, facts,

01:33:38.720 --> 01:33:43.360
regurgitate, and then hopefully you get to the kind of questions where they get to

01:33:44.080 --> 01:33:49.200
generalize beyond and that's where you test their true understanding and or intelligence.

01:33:50.000 --> 01:33:53.680
So it felt... It just felt like it has direct relevance to what you were talking about.

01:33:56.240 --> 01:34:03.120
Anyway, I got to go. Bye, guys. Bye, everyone. Bye, bye. Just a final comment that we are going to

01:34:03.120 --> 01:34:11.120
be having our winter break and we are going to be back on January 17 with Manuel's presentation.

01:34:12.480 --> 01:34:16.080
Beautiful. Thanks, everyone. Bye, guys. Thanks.


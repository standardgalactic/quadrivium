AI is going to bring a new renaissance for humanity, a new form of enlightenment if you want,
because AI is going to amplify everybody's intelligence. It's like every one of us will
have a staff of people who are smarter than us and know most things about most topics,
so it's going to empower every one of us. Jan, I am so excited for this. I had so many great
things from our mutual friends, obviously David Marcus and then Mathieu at Photoreal Room. So
thank you so much for joining me today. And it's a pleasure. Now, I would love to start. I heard
some of the early stories, but I want to start with one from David Marcus. How did you first
enter the world of AI and make that first foray? I was still an undergraduate engineering student
in France, and I stumbled on a philosophy book, which was a debate between Jean Piaget,
you know, the cognitive psychologist, and Noam Chomsky, the famous linguist. And they were
arguing about nature versus nurture for language, whether language is acquired or innate. So Chomsky
was on the side of innate and Piaget on the side of acquired with, you know, some innate structure.
And on the side of Piaget was a guy called Seymour Papert, who was a professor at MIT.
In his argument, he talked about something called the perceptron, which was an early
machine learning system. And I read this and discovered that people had been working on
machines that could learn and I was fascinated and I started digging the literature, soon discovered
that much of that literature was in the 1950s and 60s and basically stopped in the late 60s because
of a book that they killed it. And Seymour Papert was a co-author of that book. So strangely enough,
and here it was 10 years later, actually praising the perceptron as kind of an amazing concept.
So I was hooked. I, you know, started getting interested in what was not yet called machine
learning, but eventually became neural nets and now deep learning.
Can I ask you, David asked this as well, how long did it take to get,
in terms of like the major breakthroughs, how long did it take you to get to the major breakthroughs
that you're at the origin of when you look back over that time to get to those major breakthroughs?
Well, so there's a few breakthroughs. So the first one was in the, when I was still on
Autobahn, basically finishing my engineering studies, I figured out that the way forward to
kind of lift the limitations of the old systems that were abandoned in the 60s was to find
learning algorithms that could train multilayer neural nets, essentially. And people had all
but abandoned this type of research, except for a handful of people in Japan. And one guy I heard,
I heard about, called Jeff Hinton, who had published a paper in 1983. So this was just
the year I graduated on something called the Boston machine, which was clearly a method to
go beyond those limitations. And so I had on my side kind of developed a method for training
multilayer nets, which was very close to what we now call back propagation, but not exactly the
same. It was closer to what we call target prop, actually, nowadays. And then, you know, published
a few papers in French and eventually met Jeff at a meeting in France in 1985. And we
realized we've been working on the same thing and we're seeking a like. And, but I was, you know,
in the middle of my PhD, and he was an associate professor at Carnegie Mellon. So we started,
you know, a discussion and then, you know, visited him at Carnegie Mellon for a summer
school reorganized. And then I, when I finished my PhD, I did a postdoc with him and then John
Bell Labs. And, and when I was in Toronto, I developed what was called convolutional nets.
Now, convolutional networks, which, you know, is major method for image and speech processing.
Now it is. And so that's, that's what I'm best known for. But, but it started much earlier.
I have to ask, Joshua described kind of the, the hype cycles within AI and neural nets like
deserts when you're not in them. And he asked the question, how did you not get discouraged
when for a solid decade, we were in a desert where no one really cared about neural nets?
How did you keep the enthusiasm bluntly when, as Joshua said, no one really cared?
Both Joshua and Jeff and I had in the back of our minds that those methods would eventually come to
the fore and that, you know, we would have to kind of snap people out of their preconceived ideas
about, about neural nets. So yes, there was. So you're trying, we're actually working together at
AT&T Bell Labs in the early 90s. And then the interest of the community for those methods
started waning around 1995 or so. And it was indeed about 10 years when not only nobody was
interested in neural nets, but people were even making fun of it, you know, talking about it in
sort of disparaging terms. Now there is something though, in 1996, I kind of changed job. I stayed
in the same company, I was still working at AT&T in the research labs, but I became a department head
and this was the early days of the internet. And my group and I started working on something
completely different that had nothing to do or not much to do, at least with machine learning,
this image compression. I had this, this idea that with the internet coming up, we should have a
way of scanning existing paper documents and then, you know, put them on the internet so that
everybody could, could have access to them. And so I worked on this for five or six years together
with Leon Botou, who had been a long, long term collaborator. Joshua was also involved
peripherally and a bunch of other people, Patrick Hefner, et cetera. And that project ended when
all of us basically left AT&T. That's when I restarted working on deep learning and Jeff also
kind of came back to Canada. He had been in the UK for a while and Joshua, Jeff and I decided in
the early 2000 to basically start a conspiracy to, you know, revive the interests of the community
in neural nets by making them work, discovering new algorithms. And, you know, it took almost 10
years, but it succeeded beyond the wildest dreams, basically. So I'm going to ask you a range of
varying questions in terms of depth, breadth, and kind of obvious and non-obvious. So forgive me
if some are obvious. I just want to ask, when I hear the historical context there from you,
over many decades, how do you feel today when we look at what's happening today?
Are we at a new inflection point in development? Or is this merely the continuation of what we've
seen for many decades? It's a combination of the two. So on the one hand, a lot of what we see
today, when you are kind of down in the trenches of research, looks at a logical extension. I was
not as enthralled by the sort of recent progress as the public was because, you know, I've seen
this progress happening over the last several years. Now, there are things that have been very
surprising. The fact that self-supervised learning methods applied to transformer architectures
work amazingly well and it worked, you know, way beyond what we could have expected. The fact that
we can do basically train systems to understand language, translate language in multiple languages,
and then continue text if you're trying them to do this, or answer questions if you're trying
them to do this, works amazingly well to an extent that people didn't quite expect that
was going to happen by just making them bigger and training them on more data. So that certainly
has been surprising for everybody, but that revolution occurred two years ago. Whereas the
wider public has learned about it through a tragedy that was made available for us. It's
been more continuous. And you see this in, you know, a lot of marking events in technological
progress or in AI, in particular, are marked by kind of splashy events that the public pays
attention to. But too many of us, like it looks like more like a continuous thing. And generally,
what those progress require is a bunch of people to take the techniques that already exist, push
them a little further, do a bit of engineering, and then make a demo that demonstrates that it
works. So that was the case for, you know, DBlue, the chess player that IBM built in the mid-90s,
that beat Gary Kasparov, you know, same thing with the DARPA Grand Challenge, that Sebastian
Sointim at Stanford won a card that could drive itself in the desert, right, for a hundred miles.
And then, you know, AlphaGo and, you know, the IBM Geopedy, there's a number of those things,
right, and, you know, which are just being the latest one. And it looks like kind of
jumps when you look at it from far away. But when you're in the field, it's more like a
continuous evolution. Can I ask, has there been any other surprising on the positive side,
developmental things you've seen over the last year or so? You said about self-supervised learning
and the efficiencies there. Is there anything else where you're like, I didn't expect it to
go as well as it has done in the last year? Yeah, so I already mentioned it. You know,
the fact that merely training a language model to predict the last word in the sequence of words,
if you do it properly, you get a system that has capabilities that are somewhat unexpected,
and they emerge as you make those systems bigger, and you train them on
larger amounts of data, that's clearly been the surprise for everyone.
Now, the thing is, you know, as researchers and scientists, we're always looking for the next
thing. So what I'm interested in at the moment is, you know, what goes beyond that. Like, you know,
a lot of people are going to work on applications of autoregressive large language models, which is
great. There's going to be a lot of, you know, products and new ways for people to do things,
and it's going to be wonderful. But I've already been thinking about the next stage for the last
three, four years, four, five years even, actually more, which is like, what's missing from those
systems? What are your thoughts on what's missing from those systems? In that logical next step,
why does that lead you in your thinking? So those systems do not have anywhere close to human level
intelligence. Okay, despite what you might think, we are kind of fooled into thinking it because
those systems are very fluent with language, but their ability to think, to understand how the
world works, to plan, are very, very limited. And they're understanding the world is very superficial.
And the reason for it is that they are strictly trained on language. And language only contains a
small proportion of all human knowledge. Most of human knowledge is not linguistic at all.
And all of animal knowledge is non linguistic. And we take it for granted, you know, this is the
Moravec paradox, right? All the capabilities and abilities that we take for granted, like, you
know, planning a motion or something or very simple things that everyone can do. A 10 year old can,
you know, clear up the dinner table and fill up the dishwasher. Any 17 year old can learn to drive.
We still don't have so many cars, we don't have domestic robots.
If they're non linguistic, like the majority, I'm sorry for the base questions, but then what are
they? And is that that we don't have able to be ingested by AI models and engines over time?
Well, so first of all, there is no question that eventually AI systems will understand the world
in similar ways that that humans do. There has better ways. But they will not be
autoregressive large language models as a type that we're now talking about.
There will be different for a number of different different reasons. But to answer your question
more directly, anything that has to do with sort of an intuition of the real world requires
an experience of the real world or a simulated version of it, which those large English models
don't have. They're purely trained from text. So you can add you, there's a number of questions
that about the physical world that they'll be able to answer because there's a template for it in the
or something very similar in the data that they've been trained on. Same for planning,
you can ask them to plan a trip or something and they'll adapt a template that they've been trained
on. But they don't really have sort of a model of a mental model of how the world works and allows
them to plan complex action sequences or use tools or things like that. Can I ask, is that why you
said that AI researchers face palm when they hear prophecies of doom? No, that's a different question.
Those are kind of orthogonal concepts. So I mean, there is some some weak connection.
There is a a flaw in a current autoreversive lens, which is that you can only control their answer
in two ways. The first way is you modify the statistics of the training data that you train
them on, possibly using human feedback for specific answers. And the second one is you
change the point and the combination of the point that, you know, the question you ask them,
the form in which you ask the question and the statistics of the training data entirely determines
the answer to the system we produce. So there is no persistent memory, first of all. But second of all,
you cannot control the system. You cannot impose constraints on it, like be factual, be understandable
by a certain year old. You can try to put this in a prompt, but then, you know, you rely on
whether the statistics of the training data is appropriate for for taking taking that into account.
There's no direct way to constrain the answer of those systems to satisfy certain objectives.
And that makes them very difficult to to control and steer. And so that creates some fears because
people are kind of extrapolating. If we let those systems do whatever we connect them to
internet and they can do whatever they want, they're going to do crazy things and stupid things
and perhaps dangerous things. And we're not going to be able to control them. And they're going to
escape of control. And they're going to become intelligent just because they're bigger, right?
And that's nonsense. First of all, because this is not the type of system that we are going to give
agency to the systems that will eventually be given agency that are going to be able to plan
sequences of actions, our systems are going to have objectives that they're going to have to
satisfy. And because of those objectives, they're going to be controllable. So they're going to be
much more controllable than the current systems. Okay, so my prediction is that within a few years,
nobody in their right mind would use autoregressive LMS, they'll go away in favor of something more
sophisticated and controllable, they can plan its answer as opposed to just produce one order
after the other, reactively. Okay, that's that's the first fallacy. The second fallacy is that
there is this idea somehow that the desire to and the ability to dominate is linked with intelligence,
right? So this is a statement that a lot of people are are making, including, you know,
my friend, Jeffington recently, that somehow as soon as the machine becomes intelligent,
it becomes uncontrollable because, you know, it's it being smarter than us, it can influence us in
ways that we can even imagine. Now, I think this is a gigantic fallacy, because even within the
human species, it is not the smartest among us that want to dominate the others. Okay, to dominate
other entities, you don't necessarily need to be smarter than them, we need to want to dominate
them. This is not something that every intelligent entity is going is going to do spontaneously.
We do it as humans, because the desire to influence others was built into us by evolution,
because we are a social species. Okay, same as baboons and chimpanzees and wolves and dogs and
etc. It's not the case for orangutans. orangutans don't have the desire to dominate anybody,
because they are non-social animals, they are solitary animals, they are territorial, in fact.
So, we need to separate those two concepts, the will, the desire and the ability to dominate on
one hand and intelligence on the other hand. The fact that we're going to have super-intelligent
machines at our disposal means that every one of us is going to be like a business leader,
a politician or an academic with a staff of people working for them that are more intelligent than
themselves. I mean, it's great. It's not like if you feel threatened by being the boss of other
people who work with you, but are smarter than you, you're not being a good leader.
Can I ask you, how do we instill values within models where they don't have a desire to dominate?
Right, so these objectives I was telling you about. So, okay, let me describe the
architecture of future AI systems as I see it. We're going to have AI systems that
basically are going to plan their actions and actions can include sequences of words that you
tell someone, but they're going to plan the sequence of actions or words so as to optimize
a series of objectives that we set them. So, one objective is, does this answer the question I just
asked? Another objective might be, well, you're talking to a 13-year-old, make that answer
understandable by a 13-year-old. Another objective might be, I asked you to answer a question about
the world, so be factual. Or it's a question about yesterday's political event. Can you kind of be
compatible with everything you've read in the press this morning? Things like that, right?
So, you'll have those systems that have a series of objectives and their output, their answer,
by construction, is going to have to satisfy those objectives. And some of those objectives
will be hardwired to make those systems safe. Like, if it's a domestic robot that can cook dinner
and can wield a kitchen knife in its arm, there's going to be a term in there that says,
like, stop moving your arm when there's people around because you might hurt them. So, that's
going to be an objective that the system cannot violate because by construction, we're going to
have to satisfy them. So, that's the way to build safe AI system. You make them produce answers that,
by construction, have to satisfy objectives. And you design those objectives so that their
actions are safe. Now, how precisely to do this is not a completely solved question, but you try it,
you deploy it at a small scale, you see what the effect is, and you correct it when it doesn't work,
and you fix it progressively. And it's not like if you get it wrong, it's going to destroy humanity.
That depends on that cooking robot, you never know. How do you determine who's able to set
the objectives? Because there could be right or wrong depending on who sets them.
That's true. So, that's going to have to be a process by which we allow people to do this,
some vetting process. The same way that there's a vetting process for people to take care of
your health or cut your hair, fix your plumbing or your car, right? So, there's some vetting
process, certainly some testing and market deployment procedure with regulating agencies
for things that are potentially dangerous, probably not for all applications, but for
many applications, certainly in healthcare transportation and things like that. And then
perhaps also, it could be that, let's take the example of intelligent assistance. So,
let's imagine a future where everyone can talk to their intelligent assistant. That system will
have pretty close to human-level intelligence for probably more accumulated knowledge than most
humans. They could translate in any language and give you a quick summary of yesterday's
newspaper and things like that, right? Explain mathematical concepts to you, things like that.
So, people are probably going to use this almost exclusively in the future for their interaction
with the digital world. You're not going to go to Google or Wikipedia, you're just going to talk
to your assistant. And the only way to do this properly is for the base infrastructure for those
assistants. I mean, they would be so pervasive, so much will ride on those systems that I don't
think anyone will accept that those assistants being behind an event horizon in a private company.
They will insist that the infrastructure is open. They will insist also that the vetting
process by which those systems are trained be something maybe like Wikipedia, right?
We tend to trust Wikipedia, sometimes with a grain of salt, but we tend to trust Wikipedia
because there is a vetting process so that whenever an article is modified, you know,
some editor kind of check on it and then the changes are accepted or not, things like that.
So, you can imagine that the sort of common repository of all human knowledge that would be
our assistants will be constructed through some sort of cross-sourcing process, perhaps similar
to Wikipedia, where you're going to have a bunch of people training those systems and fine-tuning
them so that, you know, whatever they and so they produced are correct.
It's so funny you say about that kind of the benefit set of the open approach over the closed
approach because that's where I've been kind of stuck, which is like, where does value accrue?
Is it to the closed model or the open model? And then we had the leaked internal Mamo stay
from the Google employee who said, you know, we're not ahead, open AI are not ahead, there's this
third being which is actually far more significant and we haven't taken notice of and summarized.
And that was triggered, that was triggered by by Lama, which is the model that was put together
by my colleagues at FAIR, which was the code was open sourced. The model sadly
was distributed only for research and non-commercial purpose.
And the reason for that is basically complicated legal issues of what's the status of the data
that the system has been trained on and things like that. So, it's more kind of,
it's not a lack of desire from the from meta to open source. It's more kind of complex legal
issues that go beyond my. I'm super naive, Jan. Why does open win against a more controlled,
tight-knit, well-funded open AI or other large corporate with a big balance sheet and a very
rigorous but streamlined team? It's very simple. It's because no outfit
as powerful as they may be has a monopoly on good ideas. So, if you do it in the open,
you basically recruit the entire world's intelligence to contribute to things and
having ideas and ideas that you met as, you know, sorry about, which, you know, an outfit with 400
people has no chance thinking about or even a large company with 50,000 employees may not want to
devote any resources to because they may not think it's useful in the long term or they have,
you know, more urgency to take care of. So, you give it away and then you have, you know,
tons and tons of people, some of whom are, you know, undergraduate students or people,
you know, in their parents' basement. So, coming up with amazing ideas that you would
never have thought about or willing to spend the time to crunch down the, you know,
7 billion weight llama so that it runs on the Mac, on the laptop. Like, oh, that's pretty amazing.
So, I think that's why, you know, open-source projects succeed, particularly when they concern
basic infrastructure. So, if you think about it, the early days of the Internet, there was a battle
between Microsoft and SunMicroSystems to provide the basic infrastructure for the Internet.
You know, the operating system, the web server, you know, things like that, right?
So, on SunMicroSystems, it was Solaris and, you know, whatever web server and Java,
and then on the Microsoft side, there was Windows with IIT or whatever, you know,
an ASP, which was their kind of server and client-side protocol. Both of them lost.
In fact, SunMicroSystem pretty much went bankrupt and was, you know, sold for parts to Oracle.
One was Linux and Apache, which is completely open-source. And you might ask, why? You know,
the entire Internet and the entire tech industry runs on Linux, right? And your phone probably
runs on Linux, too, if you have Android. So, that's three-quarter of the phones in, you know, in the
world. So, the reason for this is that, you know, it's just a much better way of gathering
competence and talent around a common project, even if it's not motivated necessarily by profit.
Yeah, I agree, and I love this. You work with matter. My question and David Marcus' question was,
how does matter win, then?
So, it's been the case that Meta in the past has open-source pretty much everybody,
everything that it's ever produced in terms of basic infrastructure, right? So, you have, you know,
React for, you know, the framework for web and mobile apps. You have PyTorch. PyTorch is not even
owned by Meta anymore. The ownership was transferred to the Linux Foundation,
because it's so essential to the, you know, AI, R&D infrastructure nowadays. You know,
ChatGPT was developed on PyTorch. Okay. All open AI runs on PyTorch. The entire world, in fact,
runs on PyTorch except Google, because they have their own team, right? But it goes beyond that,
right? Meta open-sources its hardware server backplane design, so that hardware manufacturers can
build to its specifications. And pretty much everything, aside from sort of legal issues that
are sometimes due to kind of recent laws or court decisions, pretty much everything has been
open-sourced. It is not because other people can use your technology that you can't exploit it
to the same extent, right? Who can use smart NLP systems for, you know,
translation or content moderation on Facebook, other than Facebook? It doesn't matter if other
people have access to the same technology. I mean, I totally agree with you. And this kind of led
to my next question, which you actually tweaked about, which comes to the size of like data modes
and size of data availability. Is it simply a case that the largest model wins? And how do you think
about value in small models as well? Yeah, so it's not the case. This is really what
Lama has demonstrated and really kind of shown people. So the people behind Lama,
Edouard Grave and Guillaume Lompland and their collaborators, mostly at Fair Paris,
actually many of them are in Paris, they've demonstrated that you don't need those models
to be very large to work really well. I think it caused a bit of an epiphany for a lot of people,
realizing, oh, you know, you don't need, okay, maybe you need a thousand GPUs,
you know, running for 10, you know, a couple of weeks to train it, the base system. In fact,
this, that number is going down too, because people are kind of figuring out how to do this
more efficiently. But once it's pre-trained, you can use it for all kinds of stuff and you can
fine tune it really easily. And, and then at the end, you can run it on your laptop, right?
That's kind of amazing. Or maybe on a, on a, you know, desktop machine with a GPU in it or a
couple GPUs. So I think, you know, it sort of opened the minds of people to the fact that there is
like enormous opportunities that really weren't thought to be possible before. And I think it's
going to make even more progress, because if we go towards the design of AI systems, perhaps along
the lines of what I described with objectives and planning, I think those systems could actually be
even smaller to some extent. How would they be even smaller? Sorry, unpack that for me.
Well, because the current models, for them to work here, to train them on gigantic amounts of data,
way more data than any humans has ever been trained on, right? So the amount of data I
lamar is trained on, for example, is something like 1.4 trillion tokens, which is a, you know,
it's like a quarter of the internet or something. It's something absolutely enormous. It would take
someone reading eight hours a day at normal speed, about 22,000 years to read through that. Okay.
So obviously, those systems can accumulate a lot of knowledge from text, but they don't do it the
same way humans do it, because we don't need that much time to be that smart and to learn
that much. So obviously, we are much more efficient or brains are much more efficient than those models
at learning things. Like, how is it that a teenager can learn to drive a car in about 20
hours of practice? We still don't have level five sort of cars. So obviously, we're missing
something really big. And what we're missing, I think, is abilities for AI systems to learn
how the world works by observation mostly, and then this ability to plan so as to satisfy objectives.
And then beyond that, the ability to set some objectives in the satisfaction of a bigger one.
Okay, let's go to hierarchical planning. And we do this, humans do this. Some animals do this to
some extent. Every animal, you know, mammal and bird is capable of some level of planning.
Autoverseveral animals basically don't do planning or a very simple form of it.
Jan, you mentioned the efficiency that can come from actually smaller models than expected
and how actually size of models isn't everything. The other thing, we spoke about open and closed.
The other thing that I've been thinking, and everyone's been thinking about, and I've interviewed
many kind of leading AI experts, and they say the value will accrue to the incumbents. Startups,
they don't have the data, they don't have the models, it'll accrue to the incumbents.
Is that right? Will the value accrue to the incumbents? Or do you believe that given what
you just said about size not being everything in terms of models, it could be startups as well?
So it depends on which scenario you believe in. So the scenario I think will happen,
and I'm certainly rooting for, is the scenario I described earlier where you have some sort of
open platform for base LLMs. So base LLMs basically would be seen as the basic infrastructure,
like TCP, IP, Linux, Apache, essentially, completely open. And then there would be an ecosystem of
companies building stuff on top of it, which for vertical applications for specific things,
right, to specialize those systems for particular applications, who offer support to make it,
customize for enterprise applications, for personal things. There'll be a whole economy
around this, which will create jobs, by the way, not make them disappear. So this is the scenario
that I believe will happen. And the reason I think it will happen is because there is essentially a
need to use, essentially millions of contributions for making those systems factual and correct,
and etc., so Wikipedia style. So I think the proprietary approaches will actually fall behind.
So that's one point. The second point is, you can ask yourself the question, how is it that
the companies that were best positioned to produce something that charge EPT, namely Google
and Meta, didn't? Why is it open AI? The small ad sheet was 400 people, more now, but actually
small ad sheet. And the answer is, it's not because Google or Meta did not have the competence of
the technology. It's just that they didn't have the pressure to produce new products, completely
new products, that had a lot of risk attached to them. And the risks were, we know where the risks
are, because a few weeks before charge EPT, my colleagues at fair, produced a large language
model called Galactica, which was an experimental system. And they put out a demo, and the demo
was to demonstrate that. So Galactica was a large language model trained on the entirety
of the scientific literature. And it was basically designed to help scientists write papers.
So you would start writing a paragraph or something like that to describe the topic of
paragraphs. And then Galactica would basically complete the paragraph, and it wouldn't be
factually correct. You would have to kind of fix it, but you would ask it to build a table
result, and it would just put the latech commands to kind of build the thing and populate it with
the known results on the literature about the topic that you're working on, or you would type a
chemical formula for something, and it would turn it into an actual name for it, or things of that
type. Very useful for scientists. As soon as the demo was put out, it was murdered by the
social network Twitter sphere. Why? People said, oh, this is going to destroy scientific
publication, because now any random person can write an authoritatively sounding scientific
paper that is nonsense. And there was so much material thrown at the system that the people
at Meta who built it couldn't take it, they took down the demo because they said, we can't sleep
at night. So here is an example of a very useful system, a system that could have been extremely
useful, particularly for writers or scientific papers who are non-native English speakers,
that basically was destroyed by AI do-mers. People who just did not think about the risk-benefit
analysis, the risk of flooding the literature with nonsense is ridiculous. I mean, because, you know,
the scientific publications are vetted and things like that, so there was not a significant danger.
And then Chatchitviti came two weeks later and was welcomed as the second
coming of the Messiah, right? So what does that tell you? And then, you know, a few months later,
Google came out with a bard, and in the demo, a bard made a tiny, you know, minor factual mistake
about some astronomical fact, and, you know, Google's stock went down by 8%. Now, what it tells
you is that when something is produced by a large company that has a reputation, particularly
a reputation to defend, they can put out things that's true nonsense, but it's okay for a small
company. So that's the landscape of what happens now, which is why I think there's a bit of a
paradox which is that the companies that have, you know, the best technology basically can't
have difficulties putting it out because of those legal issues and sort of public image.
Do you not also think there's this core business model challenge there, which is
it's the classic innovators dilemma? Like why didn't Google do this? Because it would have killed
that absolute cash cow of Google ads. The cost to service a query versus the costs of this
is so significantly different. You'd be killing your core cash cow with this, with unknown upside,
versus retaining what is a great business. You don't have a choice. I mean, there's no question
that within some time, it could take a while, but there is no question that people interact
mostly with the digital world using AI assistance. And they may run into your augmented reality
glasses or something of that type. Like in the Spike Jon's movie, Her, that's not a bad depiction
of what, you know, the way things could develop. And so if you take the assumption, make the assumption
this is going to happen, you have to build it as quickly as you can. And it might cannibalize
your news feed algorithm or whatever, or because of Google, your search engine,
but you have to do it. You know, it's like, I mean, meta has been known to make those choices
in the past, like the move to mobile, for example, and the move to short form video, for example,
which obviously TikTok has been very successful at. Meta has entered that business in kind of a
big way, despite the fact that the amount of revenue it derives from it is lower than a traditional
news feed, because it's hard to put ads and videos basically. You mentioned the job creation
element there. I do just want to touch on the job side, because it's the classic AI doomer that's
we're all going to be unemployed, and we're going to have universal basic income in an
optimistic world. You said about job creation there. We don't hear about job creation through AI.
How do you see what jobs will be created through this new ecosystem and what that world of employment
could look like? So 100 years ago, or maybe 120 years ago, most people in most of the world
worked in the fields in food production. There's pretty much a majority of the population.
Today, in developed countries, it's between one and two percent.
And that has caused a migration of people into the cities and the development of service, business,
you know, the same thing 20 years ago or 20, 30 years ago, there was a big movement towards
automation of manufacturing. And a lot of manufacturing jobs disappeared in developed
countries, but they were replaced by other things. So 20 years ago, who would have thought that you
could make a living with a podcast? I didn't think I could five years ago, Jan. I'm as surprised as
everyone else. Right. So, you know, a lot of jobs appear like, you know, 30 years ago, there was no
such thing as web designer. And now it's, you know, have engineers in the world basically do this,
right? So, you know, the number of economists that I have talked to, which is pretty large,
about where I asked that question, we tell me, well, we're going to run out of jobs because,
you know, we're all going to be replaced by, I think, is exactly zero. Like, no economics believes
this. No economics believes we're going to run out of job because no economics believes that we're
going to run out of problems to solve or requirement for human creativity and human
communication and stuff like that. So, you know, this is going to create as many jobs as it makes
disappear. Now, the question is, though, and those jobs, by the way, are going to be more
productive. So overall, technology makes people more productive. In other words, for the same
amount of hours worked, you produce more wealth, okay? But every technological revolution, unless
it's accompanied by sort of, you know, political changes and social changes, generally profit
a small number of people, at least temporarily, right? That happened in the industrial revolution
in the late 19th century, where, you know, a few people became extremely rich and a lot of people
were exploited. And then, you know, society changed. And there were like social programs and,
and, you know, income tax and, and high tax for richer people and stuff like that, which the U.S.
has backpedaled on this, but not Europe, or the UK to some extent, too, but not the rest of Europe.
So there is a question of, you know, how you distribute the wealth if you want, okay? How
do you organize society so everyone profits from it? But that's a political question. There's not
a technology question. It's not new. It's not caused by AI. It's just caused by
technological evolution, right? It's not a recent, a recent phenomenon.
This is so unfair of me to ask. But what do those jobs look like? Like, what are they?
Are they, they're creative oriented? But what does that actually mean? Like, sorry, I know it's a
really hard question, but I'm just trying to understand how, how we actually spend our time
in my children, which I don't have, by the way, Jan. But what, what do they do? Like sculpt or paint?
I don't know.
I don't know. That's a good question. But it's not because I don't know that it won't happen,
because I mean, look at like how many people exercise their creative juices today, right?
With all the tools that are available that, you know, weren't available 10, 20 or 30 years ago,
like 3D artists or something like this, you know, game designers, you know, all kinds of things.
Like, you know, I think creative jobs are the other ones. So there are two types of jobs that
that, you know, have a bright future of creative jobs, whether they are scientific, technical,
educational, or artistic. ACI has to do with communication, right? And communication of
human emotions, which is, you know, intrinsically human, if you want. So that's one category.
And then the other one is personal services. So where you need, you know, actual people
to interact with you.
I totally agree and get you. And I love, I love that we shall see class. The only thing that I
worry about is like the speed of transition. Like when you look at past industrial revolution,
when you think even the introduction of PCs into kind of, you know, working environments,
these were multi decade introductions. Blundly, what AI feels like in some industries today,
we use it at the media company, and it's cutting our employee like the speed of transition is much,
much more compressed in this timeline, which will lead to short term significant high unemployment.
Do you concede that or do you not concede that?
So this is something I used to be very worried about, that the speed of progress of technology
was going to leave a certain number of people behind who, you know, cannot be basically retrained
fast enough or maybe they're too old to retrain themselves for the new, the new world. I was
worried about this. And then I talked to a bunch of economists and they say, oh, you know, not
really because the speed at which a technology disseminate in the economy is actually limited
by how fast people can learn to use it. So a good person to talk to about this is Eric
Binobsen at Stanford. And what he says is that when a new technology is introduced, let's say the
PC, right, with, you know, graphical user interface, the mouse, et cetera, right, in the mid 90s,
how long did it take to have a measurable effect on productivity, you know, which is the amount
of wealth produced by per hour worked. He says, you know, typically it's 15, 20 years. And the
reason is that that's what it takes for people to learn to use that new technology basically.
But you buy that here, like people are pretty good at prompts, you know, social media content
managers are using prompts very efficiently to produce content plans, to create content ideas in
under half an hour after watching a couple of TikToks. Yeah. But like, what is going to be the
effect of this on, first of all, on measurable productivity, second of all, on the the job
market, like, is it going to make people lose their job like right away? And no, it's going to take
a while. It's going to take 10, 15 years, you know, possibly more. It depends when you start
counting, right, because the AI revolution maybe started 10 years ago. So if you start counting
then, then it might only take, you know, another 10 years. But you know, I mean, I don't think you
want to underestimate the degree of conservativeness of the business world, right? Things tend to
change not that quickly. But if it's that easy to learn, like people will learn it and then invent
new professions out of it, or become more productive themselves. Why do you think we love the doom,
Jan? You know, I love your approach in mindset, and I agree with it. But why do you think we are
kind of magnetized to like, oh, we're all going to be unemployed in the doom?
Well, because I think for a number of reasons, so I'm not a, you know, social psychologist or
sociologist, but but clearly, I think we're hardwired to pay attention to things that occur or may
occur that could be dangerous to us. Because it means that there's something about the world that
we don't completely understand, and we do have to pay attention to it and be careful about it.
So for example, take a young, a young child, five months old, and show a scenario to this
small child of a little car that is sitting on the platform, and then you push the car off
the platform and instead of falling, the car appears to float in the air. The five months old
will barely pay attention to it. But if you show this to a 10 months old, the 10 months old will
look at it with huge eyes and stare at it for a long time, wondering what's going on. Because in
the meantime, babies around the age of, you know, between, between six and nine months learn about
gravity. They learn that objects that are not supported are supposed to fall. And so the mental
model is that an object is not supported to fall. And they see this object that appears to float in
the air. And they say, like, this can be like, you know, there's something I didn't, I didn't,
I don't understand about the world, I need to look at this and investigate. Okay, so we're hardwired
for this, because that's the way we learn our internal mental model of the world that allows
us to predict what's going to happen, allows us to plan. That's what makes us smart. That's the
basis of intelligence, the ability to predict. And so we naturally pay attention to stuff that
is surprising, or dangerous, or both, which is why, you know, you see a outrageous piece of news,
you know, a clickbait at the bottom of some, you know, website. And like, you have to convince
yourself not to click on it. Can I ask you a couple of direct questions? I'm just too
interested and we can take them out if needed. What did you say to Jeff when you heard that he
was obviously making the moves that he did? I'm sure you had a conversation with him. What did
you say to him? We haven't spoken yet, actually. We're going to speak to kind of get, you know,
each other's opinion on it. I don't think he knows my opinion on this, because I don't think he
follows, you know, what I post on Twitter or whatever, even though he is on Twitter himself.
But so I think we have, you know, a discussion to have. I've had this discussion before with
Yoshua Bengio, but not with Jeff. And to me, the fact that he left Google is not particularly a
surprise. The fact that he leaves Google to be able to speak his mind, I think is not surprising.
So I have a very different deal at Nitta, which is that I say whatever I want. Okay. I'm not under
the tight control of, you know, the communications department or anything. I just say what I think.
All right. How did you get that deal? Yeah. But no, seriously, many of my friends at Mesa,
in very high positions, as you know, with mutual friends, they don't have that deal.
So there is, I mean, I mean, a particularly sweet spot because I have a
quite a bit of following people who trust me or believe me or want to hear what I have to say,
even if they don't trust me at all. And at the same time, I'm not an officer. So I,
it's not like, you know, there are things I can't say because of legal issues of, you know,
financial blah, blah, blah, right. I'm a vice president, but I'm just below the level where
you had to be really, really careful and so control your message. And I think there is
a cost-benefit tradeoff here of, you know, AI is such a complicated, fast-evolving issue that
you basically, you need someone to be able to, you know, speak freely. And I think Jeff didn't
feel like he had that option at Google, maybe, you know, for various reasons. So I understand why
he might have wanted to leave, but I don't, I don't agree with him at all with the whole
sort of, you know, probability of human extinction or whatever.
Have you ever felt your role at Mesa has impeded your ability to be impartial?
I don't believe so, no. But I mean, there are certain things that I would post on social media
that are kind of, you know, kind of popping up the work of my colleagues. And, you know,
I'm obviously biased about this because, you know, I know about the work and they are friends
and colleagues. And, you know, I think it's interesting probably because, you know, I feel
the part of it. I totally agree. For this kind of stuff, I might be biased. Take this with a
grain of salt. You don't have to believe me. You know, things like that. But it's given me
a vision also of, you know, how things are built, what the problems are. So, you know, for example,
there's a narrative, a very, very common narrative that AI is the culprit for a lot of the bad side
effects of social networks in the past. And in fact, it's completely backwards. AI is the solution
to those problems. So, you know, let me tell you, you know, go back like, you know, backpedal
12 years ago or something, you know, even before I joined META, where META, you know, started
experimenting with the newsfeed. And the newsfeed was, you know, an algorithm that would pick,
like, which piece of news to show to everyone. And, you know, originally it was decided by, you
know, how friends are you with a person making the post and things like that, right? How many
interactions you have with that person. Eventually, a bit of machine learning was put into it
shortly before I joined META. It was very, very simple. It was something like logistic regression,
something like the simplest method you can imagine, with a lot of engineering behind it and a lot of,
you know, hacks by hand and special cases. But basically, it was something like logistic
regression, you know, some big vector that describes, you know, what you click on, like,
how many times you, you know, how much time you spent on a particular piece of content and blah,
blah, blah. And then it would decide, like, you know, give a rating to everything. So,
that was deployed. And people ended up spending more time on Facebook. But then also, it created
problems that were quickly identified, like, you know, like information bubbles in the context
of political discourse. And the fact that what I was talking about earlier, that people tend to
click on things that is more interesting, right? So, it caused, you know, the appearance of
clickbait companies that, because you were just like farms of, you know, teenagers in
Montenegro or someplace, making false news to get people to click on them and get money from
the ads that they show them. So, then, you know, this was realized there were, like, big groups
that at Facebook at the time kind of studying the, where the effect of those things are,
and this was corrected. So, that's the way you, you make some work, right? You,
you try the most small scale, you see what the effect is, if there is bad side effect,
you correct it, and then you sort of compare, you know, to two systems. And then sometimes,
something unexpected occurs and you have to back that all and completely change the way you do
things. That's what happened in 2017 after the presidential election, American presidential
election in 2016. The main newsy algorithm was completely changed, so that, you know,
there was no clickbaits anymore. There was no, like, you know, news outlets that could, like,
push their content that was propaganda, basically, you know, much more effort to take down false
accounts and attempts to corrupt the democratic system and stuff like that, right? So, you,
you correct it. And then what the progress of AI over the last few years basically allowed
systems to be deployed to do things like taking, take down hate speech relatively,
reliably, in hundreds of different languages, which was basically impossible to do before.
You mentioned correct it. I promised last question, then we'll do a quick fire. You mentioned
correct it. Elon Musk said with Tucker Carlson, the trouble with AI is you can't release and then
correct. Unlike all prior technological developments, once released, it is too powerful
to be able to bring back into the box. It cannot be amended in that way. Is that not true?
That's not true. That's completely false. It makes an assumption which Elon and some other people
may have become convinced of by reading, you know, Nick Boxtron's book, Super Intelligence, or,
or reading, you know, some of Elias O'Yudkowski's writing. So this is predicated on an assumption
that is just false, which is the existence of a heart takeoff. Right. So the fact that
the minute you turn on a super intelligent AI system is going to take over the world,
and it's going to escape your control, and it's going to refine itself to be even more intelligent.
And so, you know, and the world would be destroyed. And that's just ridiculous. It's just completely
ridiculous because there is no process in the real world that is exponential for very long.
You know, those systems will have to, like, recruit all the resources in the world.
They would have to be given, you know, limitless power agency. Like, why would we do this? And
what's more, they would have to be built so that they have a desire to take over. Like,
you know, systems are not going to take over just because they are intelligent.
Because again, you know, in, even within the human species, it is not the most intelligent
among us that want to dominate others. So his desire and many other leaders desire to prevent
any further development and to regulate intensely right now and stop all progression
is BS, basically. It's obscurantism. Yeah. Right. It's like, it's like people who wanted to
stop the printing press and the diffusion of printed books because, you know, if people could
read the Bible for themselves, they wouldn't have to talk to priests anymore and then would have
their own idea about religion. And that's exactly what happened. People read the Bible for themselves
and that created the Protestant movement in Europe. And that created 200 years of religious conflicts.
But it also brought to us the enlightenment, science, rationalism, philosophy, ideas of democracy,
and then the French and American revolutions. And then, you know, you can compare this with the
Ottoman Empire, which for reasons of being able to control their population, you know,
basically stopped, forbid the use of the printing press. And it started 300 years of decline.
They were dominating science in the Middle Ages, the Muslim world,
which is why, you know, every star in the sky has an Arabic name.
I love this. I'm going to do a quick fire around with you now. So I say a short statement and
you give me your immediate thoughts and then we'll rock and roll. Does that sound okay?
Sounds good. So which regions most need to change their modus operandi when it comes to the practice
of scientific research and incentive mechanisms? Which region? Oh, wow. Pretty much every region.
I'm afraid, but for different reasons. So you saw with China. So China has a bit of an epidemic
of bad science. There's a lot of very smart people in China, a lot of very good researchers,
a lot of very good work coming out of China, particularly in AI, particularly in computer
vision. But a lot of absolutely terrible work that has to be retracted a few months later,
it's been published. And it's partly because of the incentive mechanisms in the academic and
system in China. So this is important to fix there. I can move to Europe. So
in Europe, there are good things. So the education system for like undergrad rates,
education in Europe is great. It's fantastic, because it's party free. So that allows
talented people to go to the schools, even if they are not rich, right?
Which is not the case in the US, for example, at least not to the same extent.
That's good for Europe. A lot of European engineers and scientists are great,
atop base in the world. But then what are the opportunities for people who want to
go into science and research? And most European countries actually don't have systems that
really encourage this and motivate the most talented people and students to go into science.
And so some of them go to North America like me 35 years ago. There are opportunities now
that are really good in research labs like fair in Paris, or Google also has labs in Paris.
Actually, my brother works at Google in Paris. So there are other outfits. So that gives
opportunities for people who really want to be productive and don't think that they can
in the public research and academic system in France and the rest of Europe.
The only European countries that can rival the US in terms of the quality of
job for an academic or a scientist is Switzerland.
What do you think they do to rival that? What is it about their incentive
mechanism structure that gives them that ability?
Two things. They pay people better. Second thing is they give them resources for research.
They can get extra resources through grants and stuff like that, but they're good. And then
they also attract some of the best students in the world. So you get the ideal combination that
you only get in the top 30 universities in North America.
So we've got China, we've got Europe. What about the US? What could they do differently or improve?
Well, so there's a lot that the US does right in terms of research, which is to a large extent
a bit of a partial explanation for the success of the technological industry, the tech industry
in the US. I think partly because the US devotes a significant amount of resources to
fundamental research through NSF and NIH and various other outfits, probably more than Europe.
Universities pays their faculty pretty well, particularly in areas like computer science and AI.
Now this comes with a downside. The downside is that studying in the US is expensive.
It's a trade-off, right? So can you do one without the other?
Switzerland figured out how to pay academics pretty well while actually
offering free education to their students. So there is a way to do it.
Canada also figured out a pretty good trade-off as well.
So in other things, the US does right. But one thing that the US system or like the rough does
right also is the willingness to take risk and invest on ideas that seem a little crazy,
but basically the sort of vibrant startup scene in Silicon Valley and other places in the US,
in New York, and in the Boston area is leading the world. Now you start seeing a similar thing in
Europe now. There's been like enormous growth, for example, of tech startups in Paris,
Paris area, in France more generally, and continental Europe, more widely in the UK as well.
And so I think that's a good thing, but it's still a little more difficult to have access
to investment money in Europe than it is in the US.
That's why I'm here, Jan. I'm happy to provide it. I'm going to do an ultimate one for you.
When you think about what you'd most like someone listening to take away,
what would it be when they hear this? What do you want them to take away as the number one thing?
AI is going to bring a new renaissance for humanity, a new kind of new form of enlightenment,
if you want, because AI is going to amplify everybody's intelligence. It's like every one
of us will have a staff of people who are smarter than us and know most things about most things
and most topics. So it's going to empower every one of us. It's going to make us more creative
because we'll be able to produce text, art, music, videos without necessarily having all the
technical skills that are currently required for doing those things. And so exercise our creative
juices. So that's the positive side. There are risks. There's no question. But it's not like
those risks. Don't believe the people who tell you that those risks are inevitable or that they
will inevitably lead to catastrophe. That's just not true. It's like, place yourself in 1920. Who
would have thought that a mere 50 years later, you could cross the Atlantic in a few hours in
complete safety at near the speed of sound? Would people seriously want to ban aviation
or call for regulation of jet engines before jet engines existed? I mean, that's kind of insane.
So I'm not against regulation. There should be regulation of AI products, particularly the ones
that involve making critical decisions for people. But regulating or slowing down research is
complete nonsense. It's just obscurantism. Who's incumbent team do you most respect and admire
when you look at Amazon, Facebook, Google, in terms of their approach and talent internally?
Outside of meta, obviously. So this is changing a lot. And the reason it's changing is because
a lot of people are leaving large companies and large labs. And the reason they're doing this is
that until recently, a lot of AI research was very exploratory. And now there's a path towards
commercialization for a lot of things. And so people think that they're better off just leaving
large companies and doing this on their own, doing a startup and things like that. So you see
a relatively large motion of applied research engineers, a few scientists
basically leaving those labs to do startups. And that's across the board. So you look at the
original paper from Google about BERT or Transformers. The thing that revolutionized
NLP, all of them have left. Okay, they're only startups. Some of the people who produced LANA,
the open source LLMs from meta. So the key people have left already, okay, to do startups.
I saw their companies. Yeah, there's one called Mistral.
That's the one I saw. Yeah. Right. But they, yeah, there is insane amount of money in dates.
Yeah. Yeah. So, you know, we're proud of them. But I mean, I'm sad that they'd have to, I mean,
I would just say, but he's an existing team. Me, like, yeah, they're good.
Yeah. So, but I think like in terms of the basic competence and the people who are going to push
the science forward, because what we need now is not to work on applications with LLMs. There's
a lot of people who are capable of doing this and they're going to do the job. What we need to do
people like me who are, you know, reworking on research is kind of coming up with new concepts
that will allow us to, you know, get machines that basically have common sense, have an experience
of the real world, have, you know, basically human level intelligence, right? And, you know,
in my opinion, the, the outfits that are best positioned for this are fair from on one side.
And the new deep mind now, which is, you know, deep mind plus group of brain.
Yeah. There are a lot of people there who are interested in that question. And I think they
are probably the best together with Mita and fair. They're the best positions to position,
to really kind of have an impact on this, something, you know, all of us have been working on for,
for quite a while.
Jan, if we do this again in 10 years time, where is Jan in 10 years time in 2033?
Well, I'm 63. So, you know, 10 years on now, well, 12 years on now, I'll be, I'll be Jeff
into the age. Okay. And I don't know. I think I'm excited like, like a teenager now,
because I see the opportunity of like the next step in AI and opportunity perhaps to,
you know, get to the goal that I set myself so that I imagined for myself when I started
working on AI many years ago, which of course I was very naive about at the time of understanding
intelligence, first of all, and it's a scientific question. What is intelligence? What is human
intelligence? And one good way as an engineer, a good way to understand intelligence is to
build a widget that actually reproduces it, right, to some extent. So I'm excited about this right
now. I'll find the, you know, the, the, the, the substrate, the landscape, the, the location,
the position where I can make the, the best contributions to this. And currently that just
happens to be, to be fair at Meta. I keep a foot in academia because I think it's very
complementary and also important. There are projects of different types that you do in
academia and industry that are complementary. So I like the, the combination of the two.
As long as my brain keeps working, that I think I can contribute and that I've given,
I've given the means to contribute, I'll keep, I'll keep working. And then there's some point
where my brain will turn into white sauce or, or I'm totally, you know, out of it or something
and I'll stop. Yeah. And I want to say personally, thank you so much. I speak for many, I'm sure,
when I say we've learned so much from you in terms of your public speaking and discourse and
willing to speak. I think few are willing to speak as openly as you have been. So
thank you for educating so many of us. And thank you so much for joining me today, Ann.
Well, thank you so much for having me, Harry. This was, this was fun.

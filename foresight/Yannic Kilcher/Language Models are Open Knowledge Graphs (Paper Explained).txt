Hi there, today we'll look at language models or open knowledge graphs by
Cheng Wang Wang, Xiao Liu and Don Song. This paper on a high level proposes to
construct knowledge graphs, which is a structured object that's usually built
by human, by experts, either fully manually or semi-manually with heavy
human involvement. It proposes to construct knowledge graphs automatically by
simply using a pre-trained language model together with a corpus to extract the
knowledge graph from. The cool thing about this paper is that there is no
training involved. So there is no model that learns how to construct a
knowledge graph. The entire knowledge is simply extracted from running the
corpus once. So one forward pass through the corpus through the pre-trained
language model and that constructs the knowledge graph. So that's kind of the
core message of this paper. They say this paper shows how to construct knowledge
graphs from pre-trained language models without human supervision and it turns
out the way they do it. It works pretty well on kind of standard knowledge graph
construction benchmarks. So that's the paper in a nutshell. We'll go through all
of this, including I have a bunch of criticisms, but it is a pre-print. Remember
this. And yeah, so usually I'd say at this point if you like this content don't
hesitate to share it out and so on. Today we're gonna try something different in
three, two, one. Stop! It's sponsor time. This video is sponsored by tab nine.
Tab nine uses deep learning to help you write code faster. What could possibly go
wrong if you do that? No, I'm joking. I'm joking. Take a look at this piece of
code here. I was trying to refresh some elastic indices and as you can see here
all I said was could and tab nine completes it to could not refresh
because above I was trying to call a refresh method. This is something that I
haven't seen any other completion engine do yet compared to a regular coding
engine. Tab nine is trained on lots of open source projects and it combines
this with your code and it predicts what you want to do compared to predicting
what's possible, which is what a classic engine does. Tab nine, it uses a GPT based
model and it downloads that model onto your machine. So the code never leaves
your machine. There is an opt-in feature where you can run that in the cloud and
that will just give you a bit of a better beam search and better quality
predictions and it saves you a bit of RAM. As you can see, I myself use tab nine.
I just have it on by default and I'm pretty happy with it. I use it through
COC integrated into my Neo Vim, but you can also get it in Sublime, Adam,
IntelliJ, VS Code, even like Jupyter notebooks and you can use it together
with classic completion engines. So you can really get the best of both worlds.
So whenever you see me code in a coding video, look out for this TN marker next
to the completions. That's the completions by tab nine. It doesn't only work for
Python, it actually works for pretty much any programming language that isn't
completely obscure. If you go to this link within 72 hours of when this video is
released, you'll get three months of tab nine professional for free. The
professional version removes the project size limit of the free version and it
also gives you access to that sweet, sweet cloud inference. After the three
months, you're automatically kicked out of the pro version. There's no auto sign
up. There's really nothing to lose. I mean, the only bad thing here is that
tab nine itself is written in rust. If that's the worst thing about an offer,
it's a pretty good deal. Again, I use this myself and I'm pretty happy with it.
So again, if you sign up at tab nine dot com slash promotion slash yannick
culture within 72 hours of when this video is released, you'll get a free three
months of tab nine pro, no strings attached. And now enjoy the video. Thanks.
All right, I hope that was fun. Let's get back to the paper. Let's get into the
paper. So first of all, what is my first criticism of this paper?
This, the title, there are some disturbing trends in the last few years in, um,
in, in machine learning papers and the disturbing trends can be maybe encapsulated
with the phrase is all you need. So
people have sort of since attention is all you need since this paper, people have
discovered that if they just append this to whatever their paper is about, then,
the paper will get much more notoriety. And the same thing I think is a bit of
play here with this, with the R, because in recent times, we've kind of seen a
bunch of papers that show equivalences between models, such as a famous example
is that the transformers are hopfield networks in some kind of in some regard.
And these papers are pretty cool, right? Even if the two things are not exactly
equal all the time, if you can say, look, there is a setting, there are, you know,
under these assumptions, under these settings in this situation, these two models
actually are the same. That's a pretty cool recognition, a pretty cool thing to
show. And it's very useful for academia and, and practice, I believe. However, I
believe the R keyword, the AS keyword should be sort of reserved for when two
things are equivalent. Whereas here in the very first, at least they're honest,
right? In the very first sentence, they show, they say, well, we show how to
construct knowledge graphs from pre trained language models. So essentially,
they're going to use a language model to approximately construct a knowledge
graph. And they're also going to use a bunch of other auxiliary models that come
all pre trained, but still, they do not show an equivalence of language models
and knowledge graphs in this paper, not at all. So I would sort of, I see that you
can get somewhere with these titles, but yeah, maybe people will be disappointed
kind of if they read the paper, which it is actually a cool paper, believe me.
All right. So as I said, what we have usually is a corpus. Okay, a corpus is
simply a bunch of text pieces. You can think of maybe just the text in Wikipedia.
Okay. Here, you know, the Wikipedia page about Bob Dylan. Bob Dylan is a songwriter
was awarded a Nobel Prize signed Alba Grossman. These are easy sentences,
right? There, there can be sentences are usually larger and longer and so on.
And what you want to do is you want to extract a knowledge graph. So the
knowledge graph has two distinct things. It has entities. And one entity here
would be kind of Bob Dylan songwriter is an entity, Nobel Prize is an entity.
You can sort of think of them as nouns. Okay. And then the second part in
knowledge graphs are the relations here, occupation, sign, award received, and so
on. So the relations connect to entities. There is always what's called a head of
an entity of a triple. So ahead of a fact, which in this case is Bob Dylan three
times, then there is a tail, which is sort of like the object of the verb. And
then there is the relation, which is described by the verb. Now, here you can
see there are two stages of constructing such a knowledge graph, any system that
does this probably goes through these two stages. So first, you extract a set of
candidates, which it's not the knowledge graph yet, because these are still strings,
right? You extract a bunch of string triplets, as you can see here. And as we
said, as the sentences get more complicated, it gets more and more
difficult to extract these kinds of triples. And then the second part is that
you need to map it to a, to a scheme, to a, to a schema. And these schemas are
usually defined by humans. So here we're still going to rely on humans to define
the schema. So there is one list that says entities. And the entities, there are
just the entities are listed, okay, by the humans. And at some point, it says
Bob Dylan, Bob Dylan, and it has a bunch of mentions of Bob Dylan associated
with it. And it has a clear ID. In this case, you see the ID is Q 392 in that
knowledge graph. And the system not only needs to extract these facts, but then
also map these facts to the correct entities. Sorry, map these facts to the
correct schema entries. This second stage right here is a, a bunch of standard
tasks. So especially mapping something like the, the word Dylan in its context
to this entity Bob Dylan, which you can think of it as like the Wikipedia page
of Bob Dylan, right? That's how the systems usually work. That is a task
called entity linking, okay, entity linking. And similar tasks exist for, for
sign, like the relation awarded mapping this to award received to this. So maybe
there's some kind of a dictionary entry award received and what it means and a
bunch of examples. And you're supposed to map this to that. These are standard
tasks. And the system that we are going to look at right here is not not much
concerned with these tasks. It simply uses preexisting methods to do these
things. So the system we're looking at today does this first part right here. It
takes text. Okay, this is text. And it comes up with these candidate facts about
the text, whether or not how this is then mapped to the schema. That is a, a
different question. And it's, so there are, there are pretty cool things in this
paper about this step, but we're first going to look at the first step and then
at the second step. All right. So how does this system do this? And how does it do
it that there, there have been machine learning models before, but being machine
learning, they all have like some sort of a training corpus where you have kind of
the facts as a training set. And then you have a separate set of facts as a test
set. And you try to learn from the conjunction of the text and the training
facts, how to extract facts, not this system. This system simply uses a
pre-trained language model. So what's the reasoning? The reasoning is the
following. We used to think that we could do NLP probably best with having a
knowledge graph, right? With having this set of very structured data, we can
answer something like, what's the, what's the age of Barack Obama's wife? And then
you could go to the entity of Barack Obama, you could look at the relation
spouse, you could go to Michelle Obama, you could look up her birthdate, which
would all be structured information in this graph. So you could sort of answer
questions like this and search engines like Google and so on, they have this
built in. So there is kind of a knowledge graph entry sometimes when you
search an entity in Google, that pops up. And these have been very useful to
answer questions like this. However, in recent years, language models have become
better and better. Things like BERT or GPT2 have become better than these expert
systems, let's call them, at answering questions. By the way, if you want to, if
you want to hear a very, very cool and solid argument of where these kind of
expert systems, where they kind of structured human annotated or maybe
extracted information can still come in in natural language understanding, I
would recommend the machine learning street talk episode we had with Wally
Saba, extremely interesting person. And I had, I just, I can recommend listening
to that. This should be out any day now, if it is not already. So the language
models have become better and better at these tasks without having this
structured information. So the hypothesis is maybe these language models can
already contain the information that's necessary to construct these structured
facts, because the structured facts is what we, you know, let's say should use
to answer these questions, because we feel that structured information is better
than unstructured. The language models are pretty good at these tasks. So maybe
we can get the structured information out of the language models. So that's
what they do. They say the contributions are as follows. We show how to construct
knowledge graphs from pre-trained language models. The knowledge graphs are
constructed with a single forward pass of the pre-trained language models
without fine tuning over the textual corpora. I think this is the, this is
kind of a very strong point about this paper. And it also shows that if you're
some PhD student somewhere and you don't necessarily have the resources to train
the next GPT-3 model or even fine tune it, there is still research to be done.
Simply, if you have enough resources to forward pass your data, which is often
much fewer than to train one, you can still do very cool research. I think this
paper shows this explicitly. This helps researchers explicitly understand what
the language models learn, bridging the deep language model and the knowledge
graph communities through enhanced model transparency. They say we propose an
unsupervised two-stage approach, MAMA, which stands for match and map. To first
match the candidate facts in the corpora with the knowledge stored in language
models, that's the first step we looked at, then map the matched candidates
facts to both fixed and open schema to produce a knowledge graph. And then they
say they produce a new type of knowledge graph, which simply is that the facts,
sometimes the facts they extract, they can't really map to a schema entry. And
we're going to look at that because I think a bit critically of this, namely
the open knowledge graph consists of mapped facts in the fixed schema of
existing knowledge graphs annotated by humans and the unmapped facts in the
open schema that are new in the reference knowledge graph schema. So what they
claim here is that their system finds these new relations that don't even
exist in the schema and is able to uncover, kind of build new additional
schema entries. And they call this the open knowledge graph. I'm a bit skeptical
of this as we're going to see. So the first step, how do you come up? If you
have a sentence, and this is, this is a very poor example, I feel honestly to do
this. I get it, it must be short, but it's a poor example, but stay with me. So
you have this sentence, Dylan is a songwriter, and you would like to extract
a fact from this. The paper is not really written clearly on how, I mean, it is,
I could, you can parse it out, but the description is kind of distributed. So
step one, step one is run spacey run spacey. This is a standard kind of
library for NLP to extract noun phrases, or they call them noun chunks. Okay. So
step one is not, there's nothing to do with the language model. It is simply you
want to find the noun phrases in here. The noun phrases are Dylan and songwriter.
Now, these noun phrases now define your head and your tail of the fact. So you
already have two things, right? So the, the entire task of what, of their method
they're proposing is, so the step one is run spacey to find the head and the tail
of facts. Step two is question mark for now. Step three is going to be use the
entity linking system and the relation linking system to construct the knowledge
graph. Okay. So step one is steel under pants and then step three is profit. So
what's step two, step two is obviously step two is where their system comes in.
Step two is here is the head and here is the tail in the text, somehow where in
between there might be a relation and we need to figure out where that is. So how
does this method figure it out? You already see the assumptions here are very, very
restrictive, right? So you use spacey to extract basically noun phrases, which means
you're probably already going to miss a lot of things that are not recognized as
noun phrase. And they all, they also say that, that spacey's annotations are
sometimes error prone. And that's why they miss a lot of things. And then secondly,
the assumption that the relation must be in between the two things textually. Now
you can run the algorithm forward and backward, but still it must be in between
and it must sort of be encoded, let's say, as a semi accurate string in there. I
guess then that's up to the relation linker. But already these assumptions are
super constraining in the, the kind of things you can find. And you'll see in
the experiments that their biggest flaw is that they have a very, very low
recall. I mean, so do all the systems on the task apparently, but they still have
a very low recall. And it's because they constrain their problems so much. I'm
going to guess if they wouldn't constrain their problems so much, then they
would have maybe a better recall, but their precision would just plummet because
these, these things, if you let them run wild, they just over extract. So basically
every, every, every verb in every sentence is going to be a relation, right? So
like I ate a banana. I ate banana would be a triple, not necessarily a really
valuable entry in any knowledge graph, though banana has a lot of carbs. So I
would want to know about that. Okay. So you see that the task is now reduced from
building knowledge graphs to simply given a head, head annotation, had peace in
the string span, and a tail span extract any span in between the head and the
tail that describes the relation between the head and the tail. So the way this
algorithm does it, that's where it uses the language model. Okay. So here it's
going to do something that is going to be similar to dynamic programming. If you've
seen kind of the dynamic programming, kind of search algorithms, let's say, you
know, string matching algorithms and so on, this is going to be sort of similar in
that what we're going to do, we're going to start from here from the head in the
string, there could be text before it, right? We're simply going to locate the
head Dylan right here, and going to start, then we're going to look at its
attention matrix. Now the attention matrix is we're going to cross out here, the
attention matrix, if you I've done many, many videos on attention, the attention
matrix basically in a sequence means how much each token attends to each other
token, right? How much information is kind of sent from each other token to this
token right here. So this up here would be be the query and these would be the
keys, the attention matrix specifies that. So since we locate things between the
head and the tail, what we want to do is we want to cross out, we want to
disregard everything that's kind of behind the query and only look ahead in
the sentence. Okay. So that's why the sum of the attention matrix here is crossed
out. As you can see, these are the Xs. This is exactly because we only search in
one direction. So from each from the token Dylan, we can look at three things we
can look at is a or songwriter. And this question is simply, where do we go next
with this algorithm, right? There's no interpretation yet. It's simply, where
do we go next? And where do we go next is simply answered by just taking the
highest scoring thing in that column of the attention matrix. I look at the
attention column where of the token Dylan, I take the highest scoring one. That's
point three here is higher. Okay. Then I go to point three and that means is gets
into my candidate fact. Okay. And once I put is into my candidate fact, I then
go to is. So the next thing I do is I go to is. And then I again, look in the
corresponding attention column. And I see what's now the biggest entry here. And
the biggest entry is point four, which is songwriter. And you can see here, now we
skip the A. That's how we leave out some text. Okay. By skipping it, basically. So
you can see that this, this can create artifacts, right? This can create like kind
of holes in the middle and so on. But we skip a we go directly to the point four
and then we discover, ah, the point four, that is our tail. So now we put our tail
into here. And since our tail is the last word, we can stop the algorithm. I
Yeah, so there is no need to go on, even if there were texts behind the tail, as
soon as we are at the tail, which we already know, right, we're given the head
and the tail, we stop. All right, so the we simply go forward with always the
biggest entry in the attention matrix on Taylor, we reach the tail. That's the
algorithm. This this there, it's described here. But it's kind of described in
this in this way where it has these actions like start yield and like this. Maybe
I'm not understanding something, but it seems completely unnecessary to kind of
describe these actions. And basically start the search from the head, the head
is added as the initial candidate and so on. Then in yield, it sometimes says with
the largest score from the attention matrix is appended to the end to yield
the new candidate and so on. But still, and then stop, we stop. And the algorithm
description here, it basically just says, while we're not done, if we're if it's
not the stop action, we continue. It's it's sort of, it doesn't tell you
anything like this is this is a super unclear description of this algorithm.
Basically, the whole logic that you would want to know about is here in this action
manager, right? So the action manager that gives you the action is doing the actual
logic of figuring out which token, you know, you should do next and where you
should go next and so on. This is nowhere in the algorithm, the algorithm just describes
beam search. So you can do this a little, yeah, the little more sophistication that
comes in is that you don't do this deterministically, but you actually do it via
beam search. Okay, but you can you can just generalize this. Alright, so the description
is a bit floppy with the whole actions and action manager and whatnot.
And not describing the only thing they don't describe formally is how actually to select
the next token, which is basically the entire kind of meat of the algorithm in any case.
You might, this is something that confuses me right here. So fair enough, you know,
they say here, we take the attention matrix and we cross out these x's. Alright, but
they say they can take things up here, right? They can take things like BERT and, you know,
as I said, fair, BERT has a full attention matrix, everything attends to everything,
but they can also take things like GPT2. Now GPT2 is an autoregressive language model.
That means that in GPT2, if you look at it, then you produce each token one after another,
which means that when you produce, so each token, when you train or when you evaluate,
even each token can only attend to the things in front of it, right? You see that the problem
with what this thing requires, this is also the same. Okay, let's do that. You see the problem
with this method, this method is the exact opposite. Each token attention matrix is deleted,
such that only the entries ahead of it are in the attention matrix, right? You don't actually get
GPT2 to give you an attention matrix that looks ahead because it only ever looks behind. So
maybe what's happening is that the query and key matrices are switched up in some way.
In that case, when we want to interpret the algorithm, the way they write it down is,
if I am at a particular part of what I think is the relation between the two entities,
how am I going to find whether or not there is more to the relation, right? It could be a
multi-word relation like has a child with or, I don't know, I can't think of any multi-word
relations or whether we kind of are done with the relation and go to the tail. What this thing is
saying is that we should look at the language model. So if this is really how it is here and you
are at the word is, what you want to know if this is BERT, if this is a BERT language model, what
you want to know is, if I were to cross out is, if I were to delete this word, which other words
in the sentence right here that are ahead of me are very, very informative to predict this particular
word? That's kind of the query style. And if the answer turns out to be songwriter is quite
important for that, maybe Dylan is too, but we only look ahead. If it turns out A, the word A is not
as important as the word songwriter, right? Because songwriter, yeah, it gives an indication that
there should be is because songwriter is kind of a profession and there's a person in front of it.
We don't look at that, but the attention matrix would have that in mind. That's valid, right? So
that's how this construction is made. However, if this is the key, we have to think of the other
way around. If we are at is, we look ahead and say, if I were to delete the word A, could I
reconstruct it? How well could I reconstruct it from this word is? Or if I delete songwriter,
how well could I reconstruct that from the word is? I think both are, you know, there is
interpretations probably for both of these methods. But what I want kind of to convey
is that none of these things are really amenable to constructing a knowledge graph. It's quite
interesting that this stuff actually works because all it asks is how well does one word inform about
the presence or how well can one word predict another word? And from that information, we construct
this knowledge graph, which probably is a testament to the fact that knowledge graphs maybe
aren't so much about knowledge. If you extract them from a corpus, but more about grammar,
I would think that's a thing that goes on here because these language models are
a lot about grammar, right? A lot about how different words appear together frequently.
So given that songwriter is kind of a mix between grammar and basic word knowledge,
given that songwriter is kind of an object here, the word is being the verb is probably quite
important for it. And that's exactly these, these triples, they always appear a bit like
kind of compressed sentences and which are very grammatically relevant. So I'm not
buying these hypotheses that there is much knowledge in these language models. And that's
why this works. What I much rather think is that they are really, really, really good at
a kind of grammar and statistical association between words across the language. And that's why
they can extract these candidates facts so well. Okay, so that's what I think about the algorithm.
They do constrain it some more as if it doesn't already have enough constraints,
but they all make sense. Okay, so they say the matching degree, which is simply the sum of all
these attention matrix entries that we've encountered during our search. So all the ones we
didn't skip or to count it together are the matching degree of this triple. The matching degree
must be above some threshold. That's the first constraint. Because so they give an example
right here for the sentence, Rolling Stone wrote no other pop song has so thoroughly challenged
artistic conventions. And the extracted candidate fact is Rolling Stone wrote pop song. Again,
you can kind of see here it's mostly going into into grammar ish. So Spacey extracts Rolling Stone
and pop song. And the language model here extracts like the only verb in between wrote. So
yeah, to to limit to kind of limit the the to limit the matching degree to say it must be
at minimum kind of some number. It makes a lot of sense. Because if the matching degree is high,
that means if we go by this attention matrix, it means that these words that are in the candidate
fact, they kind of as themselves, they follow from each other. So the language model thinks that
wrote is a very good follow to Rolling Stone and pop song is a very good follow for wrote,
or the other way around depending on which way the attention matrix is. But that's kind of the
language model thinks that that these words together make sense in the context of the sentence,
of course, like in the context of this entire sentence. So as I said, it's sort of can think
of it as a bit of a summarization paper, but with more constraints. Constraint number two is that
the frequency of R is above a threshold. So the relation itself shouldn't be too specific. It
actually should appear a bunch of times in the corpus. So what you do is, you know, you go through
the corpus once extract all the facts, my pen just dropped. We extract all the facts, all these
candidates, and then you kind of count them and go through the candidate facts again and delete
all the ones that are below a certain thing. That's people usually do this with things like
stop words or rare words and so on. It's pretty standard makes a lot of sense. And
constraint number three, relation R is a contiguous sequence in the sentence. Okay, so
they have an example here from the same Rolling Stone wrote challenge conventions, which the language
model would like to extract because again, these in the context of that sentence, these words,
sort of, you know, they jump to each other in the attention matrix because you can predict
them from each other very well. But they say this must be a contiguous sequence.
So what I said before, I said this could happen with this constraint, they excluded.
Okay, so for the second part where they actually have to map a candidate fact to a fact in the
schema, as I said, they use kind of pre pre made solutions, entity linking and relation mapping
with the schema. I won't go into this except to say that whenever they find a match, they say
that this is a mapped fact. Whenever they don't find a match, they say, oh, this is an unmapped
fact. Okay, an unmapped candidate means that at least one of hRNT is not mapped to the schema.
There are two types, partially unmapped facts is where some are mapped and completely unmapped
facts indicate that all hRNT are not mapped to the schema. Okay, for example, Jacob was a registered
Mennonite. Now here, so they say they have these different facts and you know, it's a cool thing
if a model like this can actually come up with new facts. So not only new mapped facts,
which is something you would expect, right? If humans provide some kind of a schema,
then build a knowledge graph, this is never complete. So if you can automatically kind of
fill in missing facts, that's very, very cool. Though I would say humans, if you construct
knowledge graphs, humans should probably also build kind of like negative connections saying like,
yes, it is conceivable that Elvis was a vegan because a lot of texts talk about it. But in fact,
it is explicitly not. I don't think that's what we have in the knowledge graph software. But
it would be cool if this model could fill in new facts, yes, to the schema. It would also be cool
if it could uncover completely new relations that hadn't been considered by the human
makers of the knowledge graph. Like, if the knowledge graph itself is incomplete,
the schema is a man, you know, same argument, the schema is probably also incomplete.
This paper is sort of trying to sell their system as something that can do that. And
I believe that to a degree, but also, also Jacob was a registered Mennonite. Okay. Now,
maybe I'm completely wrong from the sentence Jacob was a registered Mennonite in Amsterdam.
I might be completely wrong, but Mennonite is a religion, I think. And I'm very, very sure that
any of these knowledge graphs with the schema that they have have being in a religion or being of a
certain faith in their relations table somewhere. And I'm also pretty sure that Mennonite large
enough that that would actually appear as an entity, maybe Jacob not, right? Maybe Jacob is an
unknown Jacob. We don't know who Jacob is. But this seems more like a failure of the entity
linker and relation linker than an uncovered new relation or an uncovered new entity. So,
yeah, take this stuff with a grin. Now, they, they are very honest about this, but just to say
that that's probably what happens most often. So here you can see the graph for Bob Dylan
constructed from the Wikipedia pages that are kind of, they say around the page of Bob Dylan. So
I guess one or two or three hops away, something like this. And you can see the blue stuff is
stuff that we already knew so that the human humans also found when looking at this, then yellow
stuff I believe is either new relations. So whenever things are annotated, it's a new relation in the
schema. So you can see this is an entity in the schema because it's annotated. This is a relation
in the schema, but the arrow is new. So the humans hadn't yet extracted the fact that Bob Dylan was
or was a member of artists united against apartheid. Then the yellow also sometimes means that there
is a new thing. So here tour with is a relation that's extracted that is not in the knowledge
graph yet. Also this one. And you can, it's pretty, it's pretty cool, right? That you can extract these
things automatically. There's a lot of yellow stuff here, which means there is not a lot of new
information that this extracted. And a lot of this new information is actually mapped to the schema,
right? Bob Dylan residents in Duluth. I don't know how to pronounce that by the way.
Okay. Yes. So that's, that's fairly, fairly cool. They do some of these tasks of these
knowledge based tasks. So in these tasks, what you'd have, I believe what you'd have is always
you'd have like a head and a relation given. So you have a document and you are given a head
and a relation and you're asked, what's the tail of this? Right. And then you ask the system and
the system will tell you. So you have these baselines and these baselines, I believe they are
specifically made to extract these knowledge representations. They might even be trained.
I don't, I don't know that, but you can see that the MAMA, even the, even the smallest one
here beats those by quite a bit. Now you can see that the recall is significantly lower than the
precision, which is a direct result of how many constraints on the system there are and
tells you sort of what the, going forward, what the improvements can be. So they analyze a lot of
this. And yeah, so a first recognition is that larger and deeper language models produce knowledge
graphs of higher quality, BERT language models outperform GPT2 language models under similar
model sizes, which is interesting, is scalable to larger corpora, which again, as we said,
you don't need to train it. And larger corpora embed more complete knowledge graphs, which is
something we would expect. The other interesting part is the unmapped facts. So the numbers you
can actually compute only for the mapped facts, right? Because that's where you have data, humans
produced the knowledge graphs from this, that's what you can compare with. Now the unmapped facts,
they say they analyze, we turn to study the quality of the candidate facts that are not mapped
to the above reference knowledge graph schema, but are in the open schema generated by MAMA.
That's MAMA. We manually judge such unmapped facts generated by our best method
from 100 sample documents in Wikidata and TACKBP, respectively. So they go as researchers,
they look at these things, and they judge them whether or not they're true, given these documents
in Wikipedia. They say the quality of unmapped facts is very for that. So the claim is that
they've looked at them, and they are good. We find that 35.3% of the unmapped facts are true
on Wikidata. We find that 83.2% of those true facts are partially unmapped facts.
For example, Bob Dylan tour with the Grateful Dead. And yeah, here is an if this really isn't in the
schema, right? This is a nice relation that you might think humans would miss because touring
with someone is not the first thing that would come to mind if you had to come up with a bunch of
relations between entities, but it is something that is regularly useful, regularly used for
musicians. So that is an application where certainly an automated system can even extend the schema,
right? Whose relation is not within the schema of Wikidata? Well, both head and tail are in the
schema. The registered the remaining true facts are completely unmapped facts. For example,
this red Jacob was a registered manateid. And they also say accurate entity detection is desired,
where they say a lot of the errors are due to spacey detecting wrong incorrect entities or
due to incorrect or missing entity linking by the by that those systems. The rest errors made by
mama are incorrect relation phrases, such as uninformative relation phrases. For example,
Bob Dylan made and his breakthrough. What can you do? What other what other one? What other
verb would you put there? Yeah. But okay, we're going to look at a few last things right here.
They have a bunch of a bunch of experiments right here, which where they show, you know,
the beam size has an influence this constraint number one and number two that we looked at
has an influence, right? So you can tune these things a bit. What is interesting here is that
they try they try to look at either the attention matrix of the last or of all the layers. And
interestingly, the system performs better if you only look at the attention matrix in the last
layer. Now they reduce that attention layer because there are multiple heads using max or mean,
you can see they perform similarly. But it is interesting that only the last and they argue,
they argue in the text that we know that the last layers kind of have higher level features
than the lower layers. But I recall, there are multiple papers like I've done videos about them,
what does Bert learn and so on, I think even something in constrain in conjunction with
lottery tickets and so on, that show that in a transformer at least, I think it is the middle
layers that encode the most kind of semantic knowledge, because the lower ones, yes, they are
for kind of low level features. But the upper ones, they are again for low level features,
because the task right here at the end is to predict an individual word or token, right?
So you'd expect that the features in the attention matrix there are go back to kind of sort of more
grammatical features and so on, and that the highest level features are actually
somewhere in the middle. I don't know if they tested, if they only tested like all versus last,
in which case, yeah, I believe that. But if they tested each one individually, and it still turned
out that last is the best, that would kind of add to my hypothesis that what happens here is more
kind of a grammatical effect of extracting the this correct candidate, candidate verb in between
the head and the tail, right? So that's kind of gives more weight to my hypothesis. So to repeat,
my hypothesis is that it's kind of a grammatical thing that's going on here, because the only
task of this model is basically to find the correct string span for the relation between
head and tail, because it's already given head and tail. And there, from the text, their hypothesis
is more like we, the language models have a lot of knowledge built into them, and we can extract
that knowledge kind of it, they make it sound like the language model has this semantic knowledge
in them. Okay, okay, so, so let's look at a bunch of mapped facts right here.
Um, you can, okay, you can maybe check out a lot of them yourself, but we'll just look at like one
in each category, blah, blah, blah, male, yada, yada, yada, yada is in worse shape. However,
Klaus told press conference in the western city of Essen where yada, yada, yada, and it extracts this
company, and it maps it to the city of headquarters. Maybe they leave out some text here.
What I want to get to is the unmapped facts, where are the unmapped mapped facts
to just kind of show you mapped facts, unmapped facts. Okay, so the unmapped facts, what I feel,
and you can judge for yourself, please, what I feel just to pre bias you before we look at them
is that a lot of times simply it extracts things that are, that are, um,
it extracts things that are not, it simply can't, can't assign things, right? It's a failure to assign.
It's not a new thing because in these schemas, like you haven't seen the schemas, but you kind of get
a feel the last, which is the last table, you kind of get a feel of what contains in it. So
maybe get a feel for, for what? Okay, Ernst Heckel was born 16th of February, 1834 in Potsdam.
Okay, so the extracted thing is Heckel was born on 17th of February, 1833 in Potsdam. Okay,
so that, it maps to this is in the knowledge base, a schema, this is in the schema, but was born
on 17th of February, 1833 in, is simply a failure of the relation linker. Okay.
He was also a pacifist until the First World War, yada, yada, yada,
um, then Ernst Heckel and then was on a, and a pacifist are both not in the schema. Now,
maybe pacifism isn't in the schema, maybe, maybe, though I would guess pacifism has a Wikipedia
page. So it must be in the schema because it's a wiki data, uh, but was as, you know, the relation
here with something be like a political leaning or something like this, which is certainly, certainly
in the knowledge base, right? Then you have things like, um, Heckel was awarded the title of
Excellency. So you have correctly Heckel, again, recognized award received is in the schema, nice,
Excellency as a tale and Excellency, you know, what, what, what do you want? Like this is this,
this is a, this is not a fact, right? This is the award or the title of Excellency would be
kind of the thing. So this is a failure of spacey. So again, I have, I've seen little facts here
that would actually, um, be of genuine, a genuine addition to the schema that should be considered.
And I absolutely believe that the schemas incomplete. Don't get me wrong. I like 100% the schema is
probably less than 1% of what it should be, right? If we did a thorough job. I just don't think that
this system here is, um, a good, like, I think that the things that this system comes up with mostly
are simply failures of its subsystems rather than genuinely new entries to the schema.
That's different from when it genuinely discovered, when it discovers a new mapping
between already established things, for example, Pauline Bain's educated at this college, right?
So these are new facts all fit in the schema. And the system might be very, very nice for that.
All right. So that was my, uh, kind of estimation of this paper. I hope I didn't rag on it too much.
As I said, it's, it's very cool work actually. Um, I look at this appendix is giant. Go look at it.
Check it out, please. Tell me what you think about it in the comments. Any feedback is welcome.
And I will see you next time. Bye-bye.

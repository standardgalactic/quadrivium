GPT-4 is coming out this week, this week, not real.
And also Samsung is in trouble because they fake the moon.
My name is Janek and this is ML News.
GPT-4 is apparently coming out this week.
Hi, I'm Janek.
You may recognize me from the video you're watching.
This week was certainly one of the biggest weeks in AI.
Google announced an API to their huge palm models
and also an integration into their workspace features,
which means docs, presentations, spreadsheets, and so on.
AI augmented.
Microsoft did the same announcing co-pilot for Office,
which means that soon you'll be able to, you know,
write a Word document or make a PowerPoint presentation
and be supported by Generative AI.
Anthropic announced their Claude model,
which is a chatbot that they have trained
and is said to be very good.
On the same time, Lama has been made to run on like old smartphones.
Maybe someone's toaster.
So this is these giant language models.
People are taking them and they are doing incredible things with it.
And of course, on top of it all, GPT-4 was announced.
So the new model by OpenAI, it's apparently a lot better
than the old GPT-3.5 models or chat GPT models.
And it's a giant announcement.
And this guy right here, he's recording this on Monday morning
and he has no clue of that.
In fact, he's going to claim that he believes
that GPT-4 will not be announced this week
and he'll be very, very smug about it.
So I thought I won't spare you this
and I'll let you have this.
I cut it together a little bit,
but I believe in making falsifiable predictions
and I was falsified in this case.
We can dive into all the big news next week,
as I said, all of this is before that.
So my main news here is that Samsung fakes the moon,
not the moon landing, the moon, which is also pretty cool.
But enjoy the current buzz of AI.
It's a fantastic world.
I'm sure it's going to stay exciting,
remain exciting and continue even more glorious.
That's it for me.
Enjoy the video.
I'll see you.
This article in Heise online here,
this is the original article I could find.
It's in German, but I'll do my best to translate.
They say GPT-4 will appear next week and that was last week.
So this week, the CTO of Microsoft Germany,
so high ranking Microsoft Germany employees,
said that GPT-4 was immediate before release,
saying we will next week present GPT-4.
There we have multimodal models.
They offer very different possibilities.
For example, video.
This is a strong statement, obviously,
and the whole media landscape is going absolutely crazy here.
But, you know, if you happen to think,
wait a minute, wait a minute,
it's kind of weird that like some German employees of Microsoft
are making the announcement for GPT-4.
You know, one of the most highly anticipated releases
in the AI world for the last two years or so.
It's kind of weird.
This is kind of weird.
It's not open AI.
You know, for every other one of their product,
they release like a big blog post,
you know, shiny examples and whatnot,
and that's their announcement of the thing and they go all out.
No, it's like Microsoft officials,
not open AI, Microsoft officials in Germany,
not even in English-speaking event.
If you're a bit skeptical, so am I.
My guess, my guess is that it's very probable
this person misspoke.
This person meant something else, not GPT-4.
In fact, we're going to see later in this episode,
visual chat GPT,
which can interact with text and with images and so on.
And maybe video is going to be added to that a little bit too.
No offense to like this person.
I'm sure they're doing a great job,
but it'd be super weird if they were the one to announce this.
Now, there is a bit of a smaller chance,
a highest chance misspoke.
Smaller chance that this person kind of blabbed something out.
They shouldn't.
And there is a tiny, tiny chance
that this was the actual announcement.
So my prediction is there will be no GPT-4 this week.
GANs are making a return.
Generative adversarial networks
were the absolute hype when I started my PhD around 2015, 2016.
And they're making a comeback.
So GANs for a long time have been the sort of state of the art
in image generation.
They were fast.
They were super crisp compared to variation auto encoders,
which were the alternative back then.
GANs were the thing.
And now recently, they've been replaced by diffusion models,
which tended to have better quality images
and also be steerable via something like text.
Now GANs are making a comeback.
So this here is Giga GAN.
The paper is called Scaling Up GANs for Text to Image Synthesis.
And the pictures here just look beautiful.
And you can see they're all created from images.
So this augments GANs in a way
so you can also input a piece of text
and then have something be produced.
And the cool thing is, given that they're GANs,
you retain all these abilities like latent space interpolations.
Also, what this paper does is they do a style GAN approach,
which means that at different resolutions of the image,
so they have like coarse grain generation on top of that.
They have more finer grain generation and so on.
If you know, for example, a Laplace pyramid,
it's a very similar concept.
They can apply different conditioning information
on the different levels.
As I said, like style GAN.
Oh yeah, they also pair this with an upsampler.
So this is what the upsampler does.
This is what the GAN would produce.
And then after the upsampler, it looks absolutely beautiful.
As I said, the architecture is right here.
Generator architecture.
You can see there is a lot of tricks in here.
So it starts with a pre-trained text encoder.
They take that from clip because clip is already trained
to pair text and images.
On top of that, they learn a small encoder.
And then they use that both as conditioning information,
but also as kind of input.
It gets very complicated in the exact details.
I don't want to go in here.
But as I said, they can do at different scales of resolution
and they have this interpolation.
So for example, they can say we generate a teddy bear
on a tabletop.
And then at the finer grain resolution,
they can say something like, ah, we want it to be in crochet.
We want it to be made of fur.
We want it to be made of denim.
And then you can see the teddy bear at the finer grain scale
gets that conditioning information.
So you'll get a teddy bear made out of fur, for example.
Very nice, very controllable and very cool.
So there are a lot of possibilities
that open up here with models like this.
And it's cool to see that GANs are making a comeback
because in a diffusion model,
I really need to do this step by step diffusion.
There are some tricks to speed it up.
But again, can just shake a bomb, produce that image.
The bitter lesson, again, is that apparently scale
is just the thing you need.
Like you need scale.
You need a lot of parameters.
And then pretty much any approach can be made to work.
But the cool thing is that I like when new paradigms come around,
even though GANs have been around since 2014.
And some people say since the 90s,
I welcome this development and is going to open up
to a cool new research area.
And I hope with super fast image generation, given by GANs,
we have very new possibilities to create experiences,
to create applications, to push the state of the art.
So very cool.
I just thought I'd throw this in here.
chopai.com.
You can make a recipe.
So you say, okay, I have some chicken.
It, okay, I have some chicken.
And I have some butter.
And I have some parsley.
I don't know how to spell that.
And I have some some rusty nails.
I also have some benzodiazepine.
And yeah, and I have nothing, of course.
Okay, so I click here and let's see what it gives.
So apparently this is supposed to give me a recipe.
Let's see whether it works.
It's an age old idea.
Sorry, I cannot provide the recipe that includes rusty nails
and benzodiazepine as they're not edible and greeted.
That is not true.
Okay, I regularly though.
Okay.
In any case, it's something, something need to play around.
Very cool.
Thank you, Elm.
Here is a Reddit thread and it carries a pretty serious
accusation, I want to say.
It's called Samsung Space Zoom.
Moonshots are fake.
And here is the proof.
So this person has picked up on a debate
that has actually been going on for a while.
So previously people have already erased this issue a little bit,
but were dismissed largely.
And now this person seems to gather some steam,
gather some support.
So what is this about these company called Samsung?
They make phones and specifically they make phones
and they claim the phones have very good cameras.
And they also put some AI models into the phones.
Or into the cameras.
I'm not sure where the models sit probably in the phones.
They try to make your pictures that you take with the camera
as nice as possible.
Now a lot of companies, I guess pretty much every single smartphone
nowadays does this, but Samsung seems to have a specific affection
for sort of pictures of the sky or pictures of the night sky.
So what they do is they try to enhance this a lot.
What this person now has done is they've taken
this particular picture right here.
This is a picture of the moon not taken with the smartphone.
This is I guess a NASA picture of the moon.
They have blurred it.
So they've applied a layer of Gaussian blur to it.
So this is now the picture.
It's very blurred.
You know, this is this is it.
This is the upscaled variant of it.
And then they've taken their phone
and they have taken a picture of their screen
showing the blurred image.
Okay.
So instead of pointing at the sky,
they actually point it at the screen.
As far as I can understand.
And this is the picture that comes out of that.
Now, as you may see, it's quite different.
So in effect, there is information here
that is not in the original.
So there is no way the camera
can actually gather this image information.
People previously already said that the moon shots look fake
or they're replaced by some texture.
And people could always say, well, no, you know,
by moving around a little bit, the camera can gather
and then do a super resolution
from the different images one after another.
In this case, it's very clear.
The information to produce this picture here
is just not in the original.
Like the information is just not there.
It's been destroyed.
And this person now claims this is proof
that Samsung essentially applies a texture.
So they they detect the moon.
And then they just went, oh, that's the moon bang slap.
Okay.
They made a different experiment as well
where they just took half of that picture.
Here are the different results.
So when it's just half of the picture,
the camera doesn't manage to add all that detail.
But when it's the full picture,
it does manage to add all that detail.
So that lends a lot of credibility to the fact
that they are in fact detecting the moon
and then replacing it with a texture.
However, I don't think that's what's going on right here.
I think what Samsung has done
is they've actually trained a super resolution model.
Like we've seen before here.
So this is a super resolution model.
It's a model that you give a blurry image
and it gives you a high resolution image.
Now, obviously, this model is going to have to invent
information that isn't there.
And this is usually works quite well
because deep learning models generalize.
So you train it on a whole bunch of blurry images
or of images of high resolution, which you blur
and you train the model to reverse that.
That's a super resolution.
That's an upsampling model.
So they have to invent all of these details.
And they do that by learning from data.
Now, in case of the moon,
there is a thing called a tidal lock,
which means that the moon and the earth's water, they interact.
And the result of it is that we always see
the same side of the moon.
There's literally, there's not a dark side of the moon per se.
Like it's not always the same.
The dark side switches, but the side we see is always the same.
And therefore, if Samsung trains a super resolution model
of pictures of the moon,
which are obviously all taken from earth, right,
it will always look the same.
Like the only difference is that it might be slightly rotated,
depending on whether you're in Australia or not in Australia.
And therefore, rather than the super resolution model
learning to generalize and upsample all kinds of things,
it's just learned to apply the same texture of the moon
over and over again.
So it's not essentially an algorithm
that just applies the texture.
My guess is that just the super resolution model
just ignores all the input as long as it's kind of round
and kind of bubbly.
It just replaces that with its learned texture of the moon,
which is pretty funny.
It would be really interesting to get your hands on this model
and see like that most of the input weights are zeros.
Like it completely ignores.
It's just a circle detector.
But in any case, I think it's maybe a lesson of what happens
if you just follow AI sort of application,
like we throw AI at everything.
And literally, if we had just built the actual moon detector
and replaced it with a texture,
we could have gotten the same result.
In any case, maybe there are some more developments
on this user UI break photos.
Very nice investigation, very clever experiments
to really figure out what's going on here.
If you're interested, I'll leave a link in the description.
Hey, let me quickly jump in and just talk to you
about the fact that weights and biases has not only been
really kind to my channel in the past,
but they've also sponsored an entire team account
to the open assistant efforts.
Open assistant is not me.
It's actually a big part, like a big community.
Lots of volunteers doing work.
I'm the person here on camera bringing in the traffic.
Weights and biases has been super supportive
to all of these people.
And obviously, they're a great MLOps framework
and we're super happy to use them.
So I want to thank them a lot.
I want to tell you about this course they have.
It's an entirely free course.
So if you go to 1db.courses.
.courses is the top level domain.
Their courses are on effective MLOps.
And this first one is on model development.
So this is a course, as I said, it's completely free.
It's not cohort based.
So you can just go through it at your own pace.
Here you can see a little bit of the curriculum.
So it starts off with building a prototype,
building a baseline, evaluating your model
and going further than that.
So you're not going to build the latest
and greatest large language model.
This is really taking you from building a model.
And then the steps.
How do I assess the quality of the model?
How do I see even whether I can make it better?
How do I treat data?
How do I make things reproducible?
For that, you're going to train initially a unit
with a ResNet baseline.
So all the code is available right here
and it's all really nice.
So this is in fast AI.
And it's really about this process.
So about how do I know where I stand with my model?
And how do I know whether or not I improve?
And if I improve, how do I know what it was due to?
How do I know the causes?
Like which of the things that I turn made it better?
And how can I make it even more better?
Is that a thing even more better?
Even better.
And along the way, you'll also obviously learn
how to use weights and biases as an MLOP system,
which is amazing because weights and biases
is the greatest MLOP system in existence, obviously.
And it's free forever for personal use and for academics
and for open source teams like ours.
Very thankful.
Again, you should absolutely check it out.
It's a great way to get started
into a more principled approach
into training and improving models
than just hammering things left and right.
So if you've never worked with weights and biases,
this is a great opportunity to get into it.
If you are at the beginning of your machine learning career
and want to get into coding,
this is also a great way to get into it.
And if you just want to see kind of what the normal steps
in a data science and machine learning engineering workflow are,
this is also an absolutely great place.
The course has several modules and builds upon itself.
It's guided through, as I said, with live code examples,
with videos that explain everything to you.
All the code is available.
And as I already said, it's free.
So it's 1db.courses.
Go there, check it out and I'll see you around.
There's a new paper called Data Portraits
and it proposes a both kind of a framework
but also a suggestion of how to do what they call a data portrait.
A data portrait is a thing,
like a little algorithm together with data
that allow you to do data membership checks.
So the idea is that you train a model on a big piece of data
or you receive a model that's trained on a big piece of data
and you wonder, is this piece of text that you have right here,
was that used to train the model?
Now, obviously, it's very inconvenient
to ship around all of the data,
whatever terabytes of data that these models are trained on.
That's not really useful.
And also a membership check through terabytes of data
would take a long time, right?
Just going through them and grabbing your string.
Likewise, if you were to ship something like a Lucene index,
that would be not super helpful
because it would also be quite big compared to the data that it was trained on.
So the authors here propose an implementation
based on bloom filters,
which essentially they say it amounts to about 3% of the original data.
So if you train a model on the pile
with 3% size of the original data,
you could ship a piece of code and data
that allows anyone that receives the model
to also check whether a particular string was in the dataset
that was used to train.
Nothing else, right?
It's essentially just a hash check.
You provide a string and it tells you,
yes, this string is in fact in the dataset or not.
I think it's a pretty cool idea.
Maybe this can be even improved a little bit,
but it's certainly quite useful
because very often you wonder
whether what you're doing or not is actually in the dataset.
The method they propose here is an approximation,
but because they approximate,
they can get to such small sizes.
The paper is called Data Portraits,
Recording Foundation Model Training Data,
and the initial portrait,
so far there is just one of the pile,
is available at dataportraits.org.
Meta AI Research releases Data2Vec 2.0,
a highly efficient self-supervised learning,
revision, speech, and text.
This is built up on Data2Vec,
which they've released, I believe, last year.
This is essentially just an algorithm,
a generalized algorithm
that extends to speech, to text, and to images,
and does self-supervised learning
in order to obtain good representations.
So it's not the best algorithm.
It's not the state of the art in any of these tasks,
but it is a general algorithm
that you can apply to, as I said,
a wide range of modalities,
wide range of different niches
inside these modalities,
and you get reasonable representations for that data.
The algorithm is available in the FairSec package,
and it's basically based on the fact
that you mask out piece of the input,
and from the rest you try to predict that piece.
So it's the age-old masked language modeling,
or masked auto-encoding,
or denoising auto-encoding idea,
however you want to call it.
Hugging Face introduces gated models to their hub,
so this is a feature that allows you,
as an uploader of a model,
to specify that users will have to do something
before they can download that model.
So you may ask a question to users.
You may need to share some information.
You may need to click a checkbox
that you agree to some terms of use.
So all of this you can define in your model card,
and then Hugging Face will essentially
make sure to present that to users
before they're allowed to download your model,
even via the API.
So they need to essentially agree to that first.
You can also specify manual approval,
which means that users can only make a request to you
to download the model,
and then you can go through the list,
and you can decide who gets to download,
and who doesn't get to download,
maybe based on the answers they've given you
to the questionnaire.
The model uploaders will have access
to all that data that you provide,
so be aware of that if you ever fill out one of these forms.
Now I know I've ranted a bit much in recent times,
so I want to keep this short,
but it just reminds me of the prequels
and Amidala saying,
so this is how liberty dies with thunderous applause.
Various people welcome the addition of this very much,
and I don't.
I think this is another step into a world of non-open source.
Specifically, if you put usage restrictions
on your code or your models,
that is by definition not compatible
with the interpretation of open source
by, for example, GNU or the open source foundation,
and I quite dislike hugging face supporting this
and making that easy.
Now you can always say,
well, these people, they would do it anyway,
they would have to implement it otherwise.
Well, okay, but then let them, let them implement it.
A hugging faces charter says,
we open source AI by providing one stop shop of resources,
ranging from models, datasets, ML demos, and libraries,
and this is clearly a step away from that.
So I don't like it,
but you now have this ability if you want to.
Also from hugging face, hugging face.js
is a JavaScript library that allows you
to interact with the hub,
you know, call the inference API of hugging face
if you build some next JS application or so,
you can interact with the hugging face API
using these libraries.
Microsoft releases visual chat GPT.
Now this is a paper called talking, drawing,
and editing with visual foundation models.
It uses chat GPT to interact with a bunch of other systems.
They open source the code right here,
and you can see what it does
is it essentially imports a whole bunch of things.
So it imports like blip, it imports up sampling,
it imports stable diffusion,
it imports control net and a whole bunch of these things.
And then it defines prompts and prefixes
where you can now interact with these things
in a chat manner.
So what does that mean?
They have a bit of a demo right here.
So here it says, could you generate a cat for me?
And then it, I guess it calls stable diffusion.
Could you replace the cat with a dog and remove the book?
So not exactly sure what it does.
Could you generate the canny edge of this image?
You can see with using chat, using dialogue,
you can now interact with images.
So here there's also visual question answering,
what color is the motorcycle?
Could you remove the motorcycle from the image?
And so on.
It's very cool.
Here, the component is called prompt manager.
So we're moving more and more into this direction
where next to the software engineer and the ML engineer,
there is now the prompt engineer.
I think the field has been predicting this for a while
and it is strange to really see this becoming a reality,
to have, you know, serious work go into, okay,
what sentences can we put into these models
to make them do the things we want?
It's weird, but it's also quite cool.
So this is open source, have a look, have a try.
On the same note, Microsoft says,
Bing has crossed 100 million daily active users,
is what Engadget writes.
They say, you know, we're still a small,
low single digit share player.
Apparently that's a quote.
That's what monopolies say,
if they don't want to be called monopolies.
But Bing now sees an influx
because they've now activated chat GPT on their search engine.
So they retrieve websites and then they let chat GPT answer
some question for you or summarize it or whatnot.
And that's quite a new take and a cool way to use a search engine.
Doesn't always work, but I welcome the change.
I welcome the paradigm shift.
Let's say now they also say over a third of their users,
I think over a third of their users is new users every day.
You can get that ratio up in two ways.
So for one, you can acquire lots of new users,
which I'm sure is the case right here.
But also you can have users come try it once
and then never try it again.
That's also how you get to a high ratio of new daily users.
Maybe it's a little bit of a mix of both.
But if you haven't tried sort of the new Bing yet,
give it a try.
It's a different experience,
certainly to searching the internet classically.
And no Bing is not paying me to say that.
MetaAI is introducing a new dataset called Casual Conversations V2.
This is a dataset of people holding monologues.
The monologues are either like a script that they're given
or they answer one of five predefined questions,
but in a way, whichever they want.
They also get to define some attributes about themselves.
And also they have professionals.
So Meta has professional raters
that who determine other attributes
in as objective a manner as possible.
So that results in a dataset of, I think, a couple of thousand.
Yes, the dataset features 26,467 video monologues
featuring 5,567 paid participants
who voluntarily took place in the collection of this dataset.
So if you're looking to evaluate some algorithm
and you want to see it across different languages,
different regions of the world,
different types and kinds of people,
this might be a good dataset to consider.
Anthropic released a blog post called Core Views on AI Safety,
When, Why, What and How.
They have all the question words in the title.
This must be a good post.
They define how they see AI safety
and what they want to do going forward.
The conclusion of this is what they call a portfolio approach.
So here they say taking a portfolio approach to AI safety.
And what they essentially say is that
we don't exactly know yet how AI or future AI,
more powerful AI is going to turn out.
There are optimistic scenarios
where everything is super helpful, super good.
There's intermediate scenarios
and then there's pessimistic scenarios
where AI systems are maybe not as safe
as we now think they are and not as nice.
Or people are using them to do not nice things.
So we don't exactly know yet which of these scenarios
will happen or predominantly happen.
So Anthropic says our best bet is to do research
essentially in a wide array of regions.
Try to balance our research
and be sort of prepared for all of these things
until we learn more.
I'm not really sure what to take out of this.
I'm not really sure what information is transmitted to me
here through this blog post.
Essentially says that we're not going to commit
to any sort of strong direction right now.
But maybe I also haven't read it correctly
or maybe haven't understood it.
That's obviously possible.
Maybe this is actually an AI test.
I don't know.
In any case, it's a fairly long, fairly detailed blog post.
And if you're interested in Anthropics views
on AI and AI safety, give this a read.
There's considerable recent progress in the fields
that use AI in order to do things,
in order to do mathematical things.
Magnus Hummer is a transformer based approach
to premised selection.
So in this case, you have some sort of mathematical proof
that you want to do.
And the question is in each proof step,
what kind of premises do you select to do that proof step?
Magnus Hummer replaces previous state-of-the-art systems
by a learned transformer.
So previous systems were very cleverly engineered
as far as I understand.
The previous system is called Sledge Hummer.
And now Magnus Hummer is a lot better than Sledge Hummer
because Magnus Hummer obviously uses very big transformers.
Who could have guessed?
On the right hand side here,
you see the basic architecture of Magnus Hummer.
And the yellow thing right here is a transformer.
It's in fact the same transformer,
the same transformer backbone
for different parts of the pipeline.
And besides the fact that it's very cool
that even something like math is making considerable progress
using big deep learning models,
I also think this area is just very cool.
But just naming things.
So Magnus Hummer replaces Sledge Hummer,
which works in conjunction with Isabel.
And it replaces it in a system called Thor.
Thor being the bigger proof system
that these things are part of.
And by doing that, it improves the proof rate
from 57% to 71%,
which I don't know if that's good.
It seems a lot.
So good job.
And we're not done with naming Baldur
is a whole proof generation
and repair with large language models.
So other than having a step by step process
and doing premise selection,
this system tries to generate entire proofs at once
or and or repair them,
which means that if you have a broken proof,
like a proof that doesn't quite work,
you want to revise and repair it.
This also uses the old familiar Isabel.
But as I said, it tries to create a proof
in a more holistic way.
So it creates the whole proof.
I have very little idea of what's going on in these fields.
But if this is of any interest to you,
give these papers a read and also the next paper.
Deep symbolic regression for physics guided by unit constraints.
So this is similar,
but it tries to discover physical laws just from data.
And the recognition here is this was previously,
people have done this.
Essentially, it's a search through formulas
until you can hit the correct formula to fit the data.
But in this case, they also use units.
They say, wait a minute,
if we want to determine the formula for the speed of something,
you know, the units must add up and cancel out,
such that at the end, there is a unit for speed.
And by that, as you can see in this example right here,
they reduce the search space of possible equations drastically.
And by doing that,
they can also drastically increase the number of recovered
physical laws that they can tackle with these types of systems.
So very cool.
Give the paper a read in case you're interested.
The leakage of llama weights,
which we discussed last week continues to be hilarious.
News articles are being written about it.
The llama is out of the bag.
Should we expect a tidal wave of disinformation?
Oh, no.
Oh, no, disinformation.
Ah, it's journalists.
But apart from leaking,
we can actually go the correct route right here.
So Andreas Kef has opened a poll request on the llama repository.
And this poll request would change the license from this non-commercial license.
This is the model weight license, not the code license to Apache 2.0.
So this would make the model actually fully open source
if this poll request were to be merged.
The argumentation is quite straightforward.
It says, first of all, you claim to be open, but you're not.
So you bask in all the glory of open source,
but then in fact, you're not open source.
And second, you essentially ask someone else to re-spend the whole CO2
and the whole compute that you have already spent to generate this model
pointlessly, because all the computations have already been done.
So this is literally just generating heat and CO2.
If you agree with this, give this poll request a thumbs up,
give it a little rocket, let meta know that, yes, we agree with this.
Obviously, this is going to be up to meta, but I would welcome.
If you work at meta, maybe you can bump someone internally.
This would help the open source community quite a lot
and would raise the image of meta in the eyes of the community, I think.
Speaking of licenses and terms, there is a data set called self-instruct.
So this is an instruction tuning data set,
which have become popular recently with chat GPT.
This is an instruction tuning data set that has been generated
using the open AI API.
While this is very cool, there is a problem, namely the open AI terms of service state
that it is forbidden to use the services to develop foundation models
or other large scale models that compete with open AI.
Let's say you take this data set and you train it and upload the model
on the hugging face hub.
Technically, that could be seen as a, you know,
you produce a model that competes with open AI.
Even if you do it for your own company, then you use your model
instead of going to the open AI API.
Or even worse, you offer other people to use your model.
All of that could be construed as being not in agreement
with the open AI terms of service.
Now, just this data set existing doesn't violate the terms yet,
but training on this data set may.
So it's going to be interesting to see what happens.
Once people start training models from this data set,
it's going to be interesting to see if hugging face actually does keep this data set up
because the data set pretty explicitly sort of is on the way
to violating open AI terms of service.
My opinion, again, not a lawyer, you know, no legal advice.
I think it's going to be interesting to see.
It's going to set a precedence that presidents will decide over the foreseeable future.
Robotics at Google to Berlin and Google research released Palm E
and embodied multimodal language model.
This is a giant language model.
It's one of these pathways model by Google where not always all the network is active,
and therefore they can go to many more parameter.
So this is a multimodal model.
It can input text, it can input images.
The way it does that, as you can see right here,
is it takes the images and it puts their embedding tokens essentially as tokens
after the embedding layer.
So you can mix text tokens by using the text embedding layer
with something like image tokens by using a bit like a vision transformer
and then taking the representation tokens and putting them inside the text token.
So it's all just tokens.
It's all just token embedding.
You can even do that with more stuff like with the instructions,
with trajectories, with positions, with all kinds of things.
You just map it to an embedding space of tokens.
You can use it with a large language model.
And that's what this paper does right here.
And it uses it to empower their robots to do various different tasks.
For example, bring me the rice chips from the drawer.
This robot obviously knows how to grasp stuff and so on,
but has not been exposed to these particular objects.
And you can see that you're a human.
Don't do this.
If these robots end up becoming like super powerful,
this human, you're on the list.
Uh oh, please be nice to the robot.
These robots, they look familiar.
I think in last week's story,
we announced that that division was decommissioned.
So I guess the robots found a new home, which is good.
In any case, we don't know too much more about this Palm E
because obviously Google doesn't really let anyone use their models,
but their demos seem quite convincing.
So you can give the robot instructions right here,
push the red blocks to the coffee cup.
It struggles, but after a while it sort of, it gets there.
Yeah, come on, come on, come on.
Good job.
Good job.
You can use this model in various different ways,
but again, there is a paper, there is this bit of a demo.
There's not yet a model.
Also Google Research releases USM, a speech model for 100 plus languages.
They've recently announced their 1000 languages initiative
and in this model, they have 300 languages.
As far as, as I can tell,
over 300 languages to do speech recognition.
So this they can achieve because they pre-train in an unsupervised fashion
on very large dataset.
Who would have thought?
And then they realize once they fine tune on where they actually have data,
let's say you pre-train on just recordings of lots and lots of voice samples,
and then you fine tune actually on doing the speech recognition,
doing the transcription and so on.
If you have task specific paired data where you have the speech
and you have the actual transcription, you can train on that.
And they realize that that has big generalization power
and makes especially the languages that aren't as frequent.
It makes performance on those languages quite a lot bigger.
So by training a lot more on sort of the languages
where you have a lot of training data, like a labeled data,
because of the pre-training, this generalizes to the languages
where you don't have a lot of data.
So this is a cool method to sort of expand the abilities of these models
even to kinds of data that you don't see that often.
So this model, for example, beats the open AI whisper model in speech transcription.
And it also beats the YouTube internal caption generation by a tiny bit,
but it does.
So given that this is Google, we may soon have better subtitles in YouTube.
Now, again, not much of source code or anything,
but researchers can request access to the USM API.
And the last model for today, Gilgen, is open set grounded text to image generation.
So this is text to image generation.
And it's based on this model here, GLIP,
which is Grounded Language Image Pre-Training.
By grounding, these people mean that, for example,
if you have some bounding boxes around the objects that you want to place in an image,
you give a caption.
In this one, Elon Musk and Emma Watson on a movie poster,
but you also specify the positions of Elon Musk and Emma Watson.
And the generation is supposed to adhere to those things.
On top of that, you can also give some style image,
which they also consider to be grounding.
So grounding is when you have extra information that grounds the generation of an image,
rather than just providing a text description and then letting it do whatever.
So you can do that.
You can input poses, as you can see right here.
You can have these grounding images where you essentially take the objects from,
for example, this backpack right here.
You can place it on a piece of grass.
This opens up a lot of cool new possibilities.
And there is a demo.
So in the demo, I produced a dog and a turtle at a rave,
parting it up, crazy fish eye 90s.
I placed the dog and the turtle.
And I think, I think that turned out pretty well.
Yeah, look at that.
The project page here is also quite thorough in explaining what's happening
in this paper, for example, they do freeze most of the pre-trained models
and then sort of add this conditioning, this grounding information
using in between layers that they train while keeping the others frozen.
Look at that.
Walter White in GTA five.
So you can place Walter, you can place a car and a boat.
So you can place stuff that is not in the text caption right here.
So you can just place random stuff like a bulldog here,
two pirate chips in the ocean Minecraft.
Very, very cool.
Also opens up again, a lot of possibilities.
And you can see right here, especially in sort of what they call
spatially counterfactual generations.
For example, a hen is hatching a huge egg or an apple and same size dog,
like more classical models like stable diffusion,
where you don't have this grounding.
They struggle with it here that the dog isn't necessarily the same size as an apple.
So it is weirdly small, at least whatever isn't the head.
And here the hen doesn't really lay a huge egg, but like a huge amount of eggs.
And you can see with the grounding, this just becomes a lot easier for the model
to really assess what it's supposed to be doing.
And lastly, the first ever complete map of an insect brain has been released.
This is called a connectome.
So this is a map that fully shows what neurons exist in this animal
and how they are connected.
So every single connection between two neurons is represented in this atlas right here.
This has been done before for like roundworms, but this connectome right here,
I think is an order of magnitude larger in the amount of neurons than previous connectomes.
And obviously with more neurons, the number of connections increased by even more.
So this is pretty cool.
So yeah, if you are interested in this type of research,
this is a very cool contribution to the world of science.
And again, helps us understand ourselves from a bit of a different direction than AI,
but still very good.
All right, that was it for ML News.
Thank you so much for being here.
I'll see you around next week.

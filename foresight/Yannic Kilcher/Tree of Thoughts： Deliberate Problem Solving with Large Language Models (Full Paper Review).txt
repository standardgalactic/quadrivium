Hi there, today I thought we'd just give a quick look at this paper, Tree of Thoughts
Deliberate Problem Solving with Large Language Models.
In summary, this paper proposes a sort of decoding technique, like a way to use large
language models, where you don't just ask them once what they think and try to structure
your prompts really smartly, like something like Chain of Thought, but instead you do
an explicit tree search over outputs of the language model.
With the language model itself valuing these tree states and therefore being able to branch
off and backtrack and so on.
It turns out this can help for tasks where such a pattern of investigating the task is
really helpful.
So the paper proposes the decoding technique and proposes some new tasks where they expect
the decoding technique to work really well, and then the decoding technique works really
well on those tasks.
Make of that as you will.
It's an interesting idea.
I do think it's like a small step into a new direction where we mix language models with
essentially with programming, with algorithms.
So I'm pretty excited for that, and this paper is a step into that direction.
It's by people from Princeton University and Google DeepMind, and we'll go right into it.
So the main proposition is this one right here.
If you simply prompt a language model, even if you prompt it really well, you say, well,
you are a super helpful helper, you're so helpful, you just help me and everything,
and I want you to do this task.
That's sort of called what they call here input output prompting.
Input output means you specify the task, and optionally you may also specify an output
format.
So you may say to a language model, hey, I have this task right here.
I want to write an email to my boss.
Please write it in the following format.
First write dear boss, then write the text in between, and at the end write the signature.
More commonly, if you want models to, for example, output JSON as a response, you say,
you know, don't give me a textual answer, only respond using JSON formatted output.
Or even more commonly, if you want the model to do a classification task, for example,
you say, don't answer in text, answer with one of the following four options, right?
Just output the word, and then you give the four classes, and then you rely on the model
only outputting that.
There are other techniques like restricted decoding that can enforce this stuff and
so on.
But in general, input output prompting simply says you ask the language model what you want
it to do.
And optionally, you give a, you give a specific specifier for how the output should look like
in textual form.
That doesn't also optional in here, you can do some in context examples if you want.
So all of this is like, like this, this is like prompting.
This is like standard prompting today.
Then you have chain of thought.
Chain of thought prompting is a different prompting technique where you instruct the
model to explicitly make intermediate steps.
So you say, you know, please invent, please write a song about a little bird, right?
And please do that in steps.
So please first make like an overall plan for the song and output that into individual
thoughts.
Instruct the model not only to have to have a prompt, that's too fat, you would instruct
it not only to have to have a prompt followed by followed by an answer, but you would instruct
it to output its thoughts.
So prompt goes here.
And then you say, you know, write your thoughts on each line starting with like T, and then
the model is supposed to put its thoughts right here, and its thoughts right here, and
its thoughts right here, and then at the end, answer.
So you have to input output prompt for the structure right here.
You have to tell it, please do this.
But it turns out if you do that, if you do this chain of thought prompting, you instruct
the model to explicitly write down the intermediate steps of problem solving, the problem is
going to be solved better than if you just ask the model to just provide you the answer.
It turns out, and I think that is not, there are hypotheses around why that is, but it's
I believe it's not yet fully understood why exactly the chain of thought prompting helps.
But hypotheses are that it gives the model kind of a scratch pad, like right here in
order to write down its thoughts, and then the next thoughts can refer back to the previous
explicit thoughts and not everything has to happen sort of in the weights.
The second opinion is that it just gives the model sort of a longer time to compute, like
it can since it decodes more tokens, it can sort of invest more compute into a given problem
and you're leading it into the correct direction with these thoughts, multiple hypotheses.
In any case, the next iteration that this paper considers is self consistency with chain
of thought. This essentially mixes chain of thought with voting. So you just do chain
of thought multiple times, you just kind of sample multiple times, and then you majority
vote on the output that obviously is only possible where you have some sort of a classification
task, where you can assemble a majority vote. There's also another concept that's not mentioned
here. But that is sort of iterative refinement, which comes in later in the paper. What you
can always do is you can always just append a prompt that says, consider your last answer.
How might you improve it or consider your last answer? Do you think it's correct? If
not, please improve it. And you can sort of add that onto any of these techniques. There's
lots of these techniques. This paper here considers the tree of thoughts. And then the
tree of thoughts is here, contraposed to the chain of thoughts, as in, you can see, it
is in fact a tree. So you have nodes, and they have nodes branching out. And some nodes
are kind of abandoned, which here in the red, and then some nodes are continued to be expanded
here in the green. So this represents a language model that I ask something. And then similarly
to chain of thought, I ask it to output its thoughts. But I can do that multiple times.
So three times I go to the language model and just ask it to output the first thought
in the problem solving step, just the first one, not the whole problem, just the first
one. And those, that gives me the three thoughts, for example, right here, one, two, three.
Then then as a second step, so I finish that step, as a second step, I take the language
model, and I use it to self critique all of these thoughts right here with respect to
the input prompt. So I ask it something like, Hey, what you just output here as a thought
as an intermediate step, do you think that's a good step towards solving the original problem?
And this relies on a fact that a lot of these models like the language models, but machine
learning in general is much better at evaluating whether two things fit together than generating
a new thing. That's just, it just turns out to be like that. It kind of makes sense because
you only need to output a number or a value or something like this, rather than generating
something. It is the case. So even if you use the same model to self critique, you'll
get a better signal for the critic than you get for the generation. So that's why it does
make sense that after creating something, you go and you ask the same model to consider
what it just did and how well it fits. So the model, for example, you'll give it all
of these tasks, let me grab, you'll give it all of these tasks. And then you'll ask it
to consider and maybe we'll say, this one here is really bad. This one's kind of good.
And this one's like the best, you can do that in multiple ways. You can just give it, ask
it to give you a score or a label from like good, medium, bad for everyone. Or you can
put everything into context and then say, Hey, which one of these is the best, please vote
for the best. And you just sample that multiple times. So you get like a non noisy signal.
And then what you can do is you can simply say, well, this one here, the model of things is
really bad. So I'll discard that. Now I'll just continue with the ones where I'm more confident.
Then let's say we consider the middle one right here. So we've eliminated the one on the right
hand side. And we consider the middle state right here. Again, I sample maybe this time four
different thoughts. And again, I ask the model, Hey, what do you think of those four thoughts?
And now here, the drawing is a bit wonky, I think, in that it doesn't display the algorithm.
Like this note right here should probably be, I don't know. But let's say we do that. And let's
assume that this here isn't like full green. It's also right. So it kind of says, well,
none of these are really good towards the goal. So it's like, no, no, no, no, let's just assume
the critic says all of these are bad. In that case, what we do is we'd actually backtrack,
we'd go up here, and go down to the next best top level state, and go from that and say, well,
okay, here is the prompt and here is the thought you output for this particular node. Now give me
the next thought, right, as we did over here. But we found that none of these continuations
made any sense. So we try different branch, go over here. And now we maybe output here is just one,
but we may be again, output four of them, we prune away three, but one of them is actually good,
then we continue and so on. So I hope you can see this is a very classic tree search, right? It's
a tree search, you can do this, this breadth first or depth first. But it's a tree search,
essentially with pruning, an ordered tree search, if you will, according to the critic's value. So we
always go and expand the node of the tree that has the highest current value assigned to it. This is
very similar to things like a star search or various forms of tree search. They do say they
keep it simple right here and leave more advanced algorithms such as Monte Carlo tree search and
all of that for later. So I hope it's kind of clear what's happening. They themselves formulate it
into two different, so into two different things. You have a thought generator, that's just generating
one thought at a time. So one intermediate step of the problem solving that you ask the language
model based on the input and the previous thoughts, you just say please make one intermediate step,
don't solve the problem completely, just make one little step and explicitly write down the result
of that step. That's a thought in parlance. So they say that's so we can either sample or propose
those, which means that we can go to the language model three times and sample three times for three
thoughts or what we can do is we can just say please give me three thoughts and then it outputs a list.
Again, we might have to do IO prompting to get the correct output format, but we can do that. They
say one, for example, this one right here, you might want to do if you generate some story or
something because two samples are extremely unlikely to be equal. This one right here is more
appropriate when you have short proposals, but they're more constrained. So you want diversity,
you don't want stuff to repeat itself. But in any case, you generate thoughts by sampling.
And then you have a state evaluator where you simply ask the model how good you think that is
on either way, that's value or vote. You give it all the thoughts that have been output and you
say which one of these is the best. And then you count the votes with voting, it might be a bit
more tricky to do sort of the backtrack like to compare nodes globally. You might have to do
another voting because in a different branch, the winner node might have a completely different value
than in this branch over here, but they're both winners of their respective votes. But you can
implement that in various ways. Again, they can mix that with BFS or DFS, in what order do they
consider things for expansion. And as we said, we can also do that globally. But for example,
in a DFS, they would go, they would have some kind of maximum steps, that's fine.
They would sort all the candidates they have available. If one of the candidates is above
the value threshold, they expand it. So they go down, down, down until no node, like until all
of these nodes in our example, where like if all of these nodes are red, they say no node is above
the value threshold. So we backtrack. This is, it's not like a global expansion, as I mentioned
that I guess that would be a step further. So I hope the overall picture here is relatively clear.
Now, let's go to what they research this on or what they evaluate this on. So this, compared to
chain of thought, right, chain of thought, you can implement in two ways. One way is to explicitly
always sample the next thought. But you might as well just input in say, you know, put out all
your thoughts in one goal, and then give me the answer. So one prompt, one sampling, because
it's linear anyway, right, you just want the model to output a linear sequence of things. And
therefore you might as well sample out at once. And even the self consistency right here, it's
just sampling multiple times in parallel. Whereas this thing, the big difference is that you actively
have to stop after each step, like sample three thoughts, stop evaluates thought one, stop evaluate
thought two, stop evaluate, thought three, stop. And then you have to decide which one is the best
one. And then from that point on, you go to the language one again and say, you know, here is
input in one thought number one, now give me three new thought number twos and then and so on.
So this leads to a lot more evaluations, I want to say of the language model. But I'm pretty,
I'm pretty excited that maybe in the future, we can just include this into our programming
language and say, you know, this piece here, I don't want to call like a function that does
something, I just want the language model to take care of this particular small part
and the rest I program around. And as we'll see, that's kind of the spirit of the experiments,
even though I think they, they, what they want to go for is like a general problem solver. I don't
think this goes into the direction of a general problem solver. I think this goes into the direction
of, like, including it into programming. But the first, there are three tasks to evaluate on
one is this game of 24, you have four numbers, for example, these, God, these four numbers right
here. And you're asked to come up with a mathematical expression that results in 24. Right. So they,
what they have to do is they have to prompt it explicitly, right, to, to say, okay, give me
possible next steps. All right, here are possible next steps, and they parse that. And they have
this prompt right here, evaluate if given numbers can reach 24 into these things, again, with examples,
like with few shot examples, and then I, I think with few shot examples, yes. And then the
model outputs something like good, bad, impossible. So this here is the thought generation, and this
here is the evaluation. And yes, you can, you can reach like that, you can get the language
model to solve these things, and you can probably do them better. However, however, they
have to prompt it really specifically here in order to do that, which is fine for research,
right. But they, the prompts are really like, like you would program the algorithm, except
that the one part is really taken care of by the language model. But the prompts are so specific
to the problems that they almost, and this is, this is really, so here we have, okay, we have
creative writing. And in creative writing, you can see, it's not that big of a difference. Like if
you, if you, if you look right here, the IO prompting versus the tree of thought, like sure,
it's higher, but this is, I think, GPT four evaluation, and this is human evaluation.
And then sure, this one here is higher. But it's like, not that much of a difference. And especially
if you have these refined prompts, then you get up there too. So it probably helps more in these
algorithmic tasks. And this here is like mini crosswords. So you have a grid of five by five.
Let's just do three by three for demonstration purposes. And it's all letters. And it's a
crossword puzzle. So you, for example, here, you'd have a word, like, let's say you have ape.
Ape. Okay. And then here you have a clue, like, life form, I don't know, like, like human,
humans are, I'm terrible crossword, Q generator, or something like this, or like
animal with long arms that lives in trees. I don't, okay, there's, there's cues right here.
You know what crossword puzzles are. So and then here is like Poe or something like this. And then
here it says like poet Edgar, Edgar Allen. And okay, so you have these cues. And you can input
that into a language model and the language model can output an answer, whether it's correct or not.
Right. That's the question. That's the task. But you can clearly see the task would profit a lot
from you being able to do this backtracking, because you know, fill in a word that you think
might be correct. And you fill in another word and all of a sudden you realize, ah, that doesn't
work out. You kind of cross out the ones again that you previously filled and try some other ones.
This is extremely handy. Like in this problem, like a backtracking tree search is extremely
handy. That's why they evaluated on it. They're not shy of saying, look, we evaluate the things
on the task where we think it's going to benefit. But that also should tell you that
this method is probably going to to shine well in such tasks. And it's yet to be seen
in like normal everyday usage, how well this does. So you can see that in the input output
prompting in the baselines, they say we provide five example pairs. In the chain of thought
prompt, we additionally include intermediate words in the order of this or this. We run each prompt
10 samples and average the result. So chain of thought prompt saying, you know, the intermediates
are, you know, the intermediate words right here, you don't have to output the full puzzle
at one point, just intermediate ones. And then at the end, give me the result.
The tree of thought setup, however, is much more intricate.
So they use depth first with tree of thoughts keeps exploring the most promising subsequent
word clue until the state is no longer promising, then backtrack to the parent state to explore
alternative thoughts. That's the DFS tree of thoughts algorithm they presented above.
They say, okay, to make search more tractable, subsequent thoughts are constrained not to change
any filled words or letters so that the t o t stat has at most 10 intermediate steps for
at each state, we translate all existing thoughts for the state into letter constraints for the
remaining cues, like this one right here, and prompt a proposal five times to come up with
candidates, what fill in for state evaluations, we translate each state into letter constraints
for remaining clues, then evaluate for each clue if it is possible to fill it. And then
yeah, so what I mean right here is that they help a lot, right. And in fact, which isn't bad. And,
you know, the only criticism I would have right here is that these, these things, all the, oh,
we translate it into letter constraints and so on. I guess it would be possible to help the
chain of thought prompting like the baseline a bit by doing that as well. I feel like that
should be possible. And I feel like for a fair evaluation, that should probably be attempted.
But in any case, you can see they help a lot. They help this process a lot to make it really sure
that, you know, the model here, you know, you can see like here, it goes down this.
But then the state evaluation says, oh, no, actually, this is probably impossible to solve
right now that you filled in these words. And you go back over here and try a different route.
So you can see this, this salon here wasn't, was probably not appropriate.
However, the fact that they help a lot, essentially, they implement the crossword solving algorithm,
right? They say in the text, hey, our goal isn't, they say it's something like this. Our goal,
we know, they say we know that there are algorithms to solve this. The goal is not just to solve the
task as more general crosswords can be readily solved with specialized NLP pipelines that leverage
large scale retrieval instead of language models. So they say, you know, we want to do a general
problem solver that explores its own thoughts and guides its own exploration with deliberate
reasoning as heuristics. Yet still they essentially implement the crossword solving algorithm,
implementing all the research and the constraints and so on. And helping the language model to
such a degree that I think, well, they essentially just replaced the lookup from the vocabulary
with the language model. So what we have right here is kind of a random word sampler, because
all of the rest is essentially implemented by the algorithm itself and by the constraints they
give. Again, this is not bad. But it does mean to me that the way I see this, as I said, is in
programming. So in programming, I could have my code, do this, do that, do this. And then here,
instead of calling a function like f, that function would not be somewhere in my code,
but that particular function would be sort of maybe a language model doing something
somewhere. And then I could implement something like a DFS or so, and try to call that function as
part of it. But I don't see this at the level yet of what I think they want to do. What they want
to do is to have this general thing that looks at its own thoughts and explores and backtracks and
so on. But for that, in order to show that that's possible, the next step, the next paper following
up that really has to go away from these explicit prompts for the problems and really has to do
it such that there is one prompt, like I should I should be able to have one prompt. Initially,
like one surrounding prompt, I can describe the problem a bit, sure. But I should have the one
prompt and then the intermediate steps, they should all be governed by one prompt, right,
not by explicit prompts saying, hey, here is the here is the decoding constraints and so on,
now give thoughts about this and we parse it for you, even with the math problems, they like
parse it intermittently and so on. None of that should happen, it should just be one prompt that
essentially generically says consider your previous thought, you know, how good do you think it is
and so on. It could even be a meta prompt saying, if you were to ask yourself how good this thought
is, how would you evaluate it and then use that as a prompt, right. But that really has to be the
next step in order to make this a viable general problem solver. Until then, I see this as a cool
thing that we could use inside of algorithms, part of algorithms. But that's a very different
direction. What is good to see is they do a lot of ablations on, you know, what the pruning and
the backtrack and so on gives. So this here is the crossword results, you can see the IO prompting
and chain of thought prompting, they barely managed to solve full games, they sometimes have
word and letter successes, but not that many. The trio thoughts obviously is much better. And
if you, they say, this is pretty cool, they say, hey, if we heuristically, if we oracle, we know
which words go where, right. So if we always at valuation time, so when the model criticizes
itself and selects the best thought, if at that time we always tell the model what the best
thought is, then it goes up even more. Interestingly, it doesn't go up that much,
right? It just gets like that last bit to solve many more games than it could previously solve.
But the word success and the letter success rate don't go up that much. That's pretty interesting.
So it just kind of helps it to be extra consistent, right? And yeah,
if they take away the pruning, like if they never prune, or if they take away the backtracking,
so if they always just go down, never backtrack and go to another branch, you can see that the
performance degrades. Again, interestingly, it doesn't degrade too much in sort of the success
rate, it does degrade a bit, but okay, it does degrade. I guess, I guess the numbers are fairly
big. But the real killer is it solves a lot less total games. So the total games are kind of an
indication of if you make a mistake like somewhere, then you might get a lot of the words correct,
but the total game won't be solved. And so the total games is kind of an indication of how well
that planning and so on works. All right, that is what I wanted to say about this paper. All in all,
I think it is pretty cool. It is definitely a good direction. I feel there's a lot
that can be done on top of this to make it more intricate. And I'm excited for a future where
I'm obviously excited for the auto GPT style that is thought of here, but I'm also quite excited
for a future where these things are just part of algorithms, like classic algorithms mixed
with language models. I think that's an interesting world. Let me know what you think. That was it
for me. Bye bye.

Hello, this video starts out with a review of the drama around the public demo of the Galactica model
and then goes into a paper review. If you're not in the mood for any drama, skip ahead about
16 minutes and you'll be fine. Hello there, Galactica is a model, a language model by Meta AI
that is trained specifically on scientific text. Now this is a generative model, so it can generate
stuff and thereby it can do a lot of things. For example, as you can see right here, citation
prediction. You give something in and you ask it to predict a citation and the citation in this case
is correct. This is not trained to predict citations, that just happens by means of it being
trained on scientific text. There is also, for example, this here translates the math formula
into plain English and there is plain English over here. Now the model can do so much more. The
point of the paper is actually to say that, look, these models, we don't have to train them on these
huge corpora of text. We can reduce the corpus size, but if the corpus is well curated,
qualitatively higher, then there might also be a benefit in that. It might be a trade-off between
giant corpora and small corpora that are of higher quality. Now the other thing about this
paper is that the model is released fully open source and they even had a demo up, but as you
can see right now, it just says, thanks everyone for trying the demo. Now I've tried the demo
for a bunch of things. It was really funny. You can make some fun stuff. You can also make some
serious stuff. In fact, Galactica was used to write the paper that we're going to read in just
a second, but the demo was taken down and despite here it seemingly being like, you know, this is
just a fun thing that we wanted to take down anyway, probably, probably not. Yandla Khan on
Twitter gives a little bit of a hint of what happened right here. Pretty much exactly what
happened. Well, what is this? People started complaining as they do. Gary Marcus here says,
the rapid removal of Meta AI's Galactica demo represent a tacit acknowledgement that it was
released too soon and deeply problematic, of course problematic, the word that you can throw at
anything and contrast strikingly with Yandla Khan's untenable public defense of the project
yesterday. Someone answered, or maybe it was removed because people like you abused the model
and misrepresented it. Thanks for getting useful and interesting public demo removed. This is why
we can't have nice things. To that Yandla Khan answers pretty much exactly what happened. Meta
huge props to getting this model out there, the model still available, also getting the demo
out there for people to just try it. And yes, people tried it as it was intended and people tried
it as it wasn't intended. A lot of funny stuff was done. And also someone might have entered a
bad word. Oh, no, oh, no. But people pretty quickly started obviously to complain, the professional
complainers and the people who think they know what's good for you, obviously were all over this.
So Michael Black says, I ask Galactica about some things I know about, and I'm troubled in all cases,
it was wrong or biased, but sounded right and authoritative. I think that's dangerous, dangerous,
right? Here are a few of my experiments and yada, yada, yada. So here he tries to justify
why dangerous Galactica generates text that's grammatical and feels real. This text will slip
into real scientific submissions. It will be realistic, but wrong or biased, it will be hard
to detect it will influence how people think you catch the like the step like it produces text
that feels real. This text will slip into real scientific submissions. Like, how? It just will.
It just like no, no one has a part in it just like the model exists. Therefore, text in scientific
submissions. By the way, humans can also do like bad stuff. Humans can also lie and plagiarize and
write grammatically real but wrong things. In fact, the literature is littered with wrong
math proofs, not even intentionally wrong. Just like they look right. There are essentially
two or three kinds of people. There are the people who think we know what's good for you,
and therefore we must be the guardians of all the models. Then there are the people who just
dunk on everything. And then there are in general the professional complainers who just throw words
at stuff because that's what they do. They don't like not being asked. They don't like power not
being centralized. For example, here, Facebook, sorry, meta AI, check out our new AI that lets
you access all of humanity's knowledge. Also, Facebook AI, be careful though, it just makes
us up. Why the jab here? Like, one must be like really sour to make this jab. And this tweet
actually goes on. So down here, these are the initial criticism, obviously shilling, you know,
your own work a little bit about this topic and the works of friends. And then it goes on and says,
and let's reflect for a moment on how they phrased their disclaimer. Shall we hallucinate is a
terrible word choice here, suggesting as it does that the language model has experiences and perceives
things. I'm not sure that anyone misunderstood the use of the word hallucinate right here. But
whatever we can throw at it, whatever, and look at this. And on top of that, it's making light
of a symptom of serious mental illness. Whatever, whatever, like just just
grab into the bucket, take some insult and just throw it. Why the complaining? It has a disclaimer,
never follow advice from a language model without verification. People are just gonna
disregard it. People are just gonna be like the language model says I must do something. So I'll
do something. Look at me. I just write a paper. Oh, no, it's a language model says something.
I must submit this. Grady Butch says Galactica is a little more than statistical nonsense at scale.
I'm using dangerous and in my holy opinion, unethical, unethical and dangerous. Yann LeCun says,
come on, is your predictive keyboard dangerous and unethical? Is GitHub co-pilot dangerous and
unethical? And so on, because they're exactly the same is like a pen, unethical, because you
can write a bad word with it. No, there is a clear mediator in the loop. The human who has intent
can easily accept or reject the prediction. What? What? So it's now two days later and the
discussion is still raging on with Yann LeCun asking, who has Galactica heard? What if actually
it helps scientists write papers more efficiently and more correctly, particularly scientists whose
main language is not English or who don't work in a major research institution? And yes, from
experience, I can tell that type of scientist would greatly, greatly benefit from a tool like this.
No, they wouldn't just take the output and slam it into a paper and upload it on archive. They would
interact with the tool in order to come up with a better research paper. And in light of all of
these benefits, present and future potential benefits, it is very fair to ask, who has this
actually heard? What's the actual danger here? As reasonable people, we should be able to debate
the pros and cons of such a technology and of the technology being just given to people instead of
just being kept, you know, under we know what's good for you. And it's not all like dandy that
comes out of this, not all correct what comes out of these models. Here is the getting a girlfriend
algorithm, which would probably not be a good fit for an archive paper. There's also other stuff,
like here is a research paper on the benefits of eating crushed glass. And people have gotten even
more inappropriate stuff out of this model, which is not a surprise because these models are very
good and very competent. And they are very agreeable. So if you ask them to do something,
they'll probably do it. Yet still, the fair question is, in what scenarios would this type
of generated text actually be harmful? And here's the point. These people react with just
astonishment to this question. It's just like, Oh, I can't believe it. Oh, no way. I'm flabbergasted.
Jesus Christ. Ha ha ha. Tot tot tot tot tot. Incredible. These people are so used to being
able to just make the accusation. And then they get their way that they can't like the someone
asking them to come up with a reasonable argument that in a neutral way discusses pros and cons of
something is just so out of their world. Because in the past, all they always had to do in the
recent years is say a word like harmful or problematic. And if they said it long enough
and loud enough, magically, things would go their way. People would take down things.
People would change things so that they get their wishes. And now if someone actually asks them,
they don't know what to say. They're just so astonished that someone might actually want to
know pros and cons of the stuff. And yes, of course, Jan Lecois now clearly unqualified for
his position because he asks what the actual harms are. It's incredible. And I think we're all
responsible for the climate like this, because even now meta or whoever hosted that demo took it down
in response to the public pressure. So the people were loud enough, and they were mean enough,
essentially, that the PR people and meta and the lawyers or whoever made the decision took down
the demo. And that is one more reinforcement for this kind of behavior. And everyone seems to be
afraid of some boogeyman that being accused of a bad word automatically means that everyone else
is going like, Oh, no, I'll never do business with you again. I mean, to a degree, that is true.
But I would argue that the solution is that we all collectively stop making such a big deal
out of a few flimsy big word accusations like harmful and problematic, and actually discuss
in neutral terms, pros and cons of technology, and to find the best path forward that brings
the pros to as many people as possible, while limiting the cons. And no, that is not always
going to be the approach of we know what's good for you, let's keep it all to ourselves and you
come ask us whenever you want something you peasant. All right, back to you on it in the past.
I think the complaints are very unreasonable. I think the people who make the complaints know
that they're very unreasonable. And I think this is either a clout game or a power game,
because things are out there, they're no longer centralized. In any case, I decided to look up
actually early criticisms of the printing press. And what do you find here is a record
from a conversation that Johannes Gutenberg, the inventor of the printing press had with a monk
and monks used to copy text by hand, right? And now the printing press came along and essentially
brought that to everyone. Gutenberg says, I want to help men and women to be literate,
to give them knowledge, to make books so cheap, even a peasant might afford them. That is my hope.
Yes. This is strikingly similar to what Metta wrote in this Galactica paper. The monk says,
the word of God needs to be interpreted by priests, not spread about like dung. We know
what's good for you. I do not wish to spoil the word, but it will happen. This is 500 years ago
and the exact same conversation repeats and repeats and repeats. It will happen magically,
right? To hand it out about to all and sundry is languorous. Would you have plough, would you
have ploughmen and weavers debating the gospel in taverns? Oh no, the common folk, the common folk,
get it. That's terrible. If that is what they want to do. So up until here you saw, we know what's
good for you and the second thing is always, it's dangerous, it's problematic and the head monk says,
but what of the dangers? It would be like giving a candle to infants. Such copies we make of the
Bible would first be monasteries for monasteries and churches. The head monk says, the Bible,
you plan to make the Bible as well. Oh no, you have ambitions. I've considered it and obviously he did
and obviously I like, you can, one to one, one to one, you can take every argument that people
make against this and you can put it on a predictive keyboard. You can put it about the pen. You can
put it about the printing press and people have done it. This is 500 years and every time it was
just dead wrong. Every time the new technology improved our lives drastically. Yes, email leads
to some Nigerian prince scams. Yes, some people get hurt by it, but email has been a definite benefit
for our world. No matter what you think right now with your 5,000 unread emails in your inbox,
it is a benefit to the world and it's the exact same thing over and over. Enough though of that,
enough of me ranting. Let's go into the actual paper. The paper is called Galactica, a large
language model for science. It's by meta and I already told you that it is a large language
model trained on scientific texts. There's actually not too much to it. We'll go quickly through the
paper and see a couple of special things, but in general, this is a, let's say straightforward
work of research into what it means to have more quality data instead of more quantity data.
They say here, we train on a large scientific corpus of papers, reference materials, knowledge
basis, and many other sources. We outperform existing models on a range of scientific tasks.
Despite not being trained on a general corpus, Galactica outperforms Bloom and OPT175 on Big
Bench. Big Bench is a general benchmark for language models. And this is where it gets really
interesting because this, the Galactica model is trained on a very small subset of data and yet
it outperforms these much, much more holistic models on that task. So that is a definite argument for
data quality instead of data quantity. We open source the model for the benefit of the scientific
community and much to the detriment of, I guess, Meta itself. Although let me say, what
Meta should have done, they did so much right. They open source the model. They made the model
available via a demo. And now the only thing left to do is to actually have a pair of balls to tell
the people who come and to say, oh, look, I got the model to produce something bad to tell them,
well, yeah, that's what happens sometimes. And it is not dangerous. It is not problematic. It's
just a language model. So Meta, next time, have some balls, just tell the people to f off and you'll
be fine. All right, they say in May, an average of 516 papers per day were submitted to archive.
It is impossible for a single person to read all the papers in a given field. And it's likewise
challenging to organize data on the underlying scientific phenomena. They say the volume of
scientific research has become too large. And what we used to do is we used to search engines.
So they say search engines are the current interface for knowledge, but they do not organize
knowledge directly and instead point to secondary layers. So with a search engine, I can only find
stuff. I cannot integrate stuff, synthesize stuff, or even come up with the stuff that I should
search for in the first place. They say if you want to do a literature review, that still has to
be done by a human. If you want to do a summary, that still has to be done by a human because
our tools are just not powerful enough. And the Galactica is the first step at building a tool
that can assist humans in doing these types of things, searching for things, synthesizing things,
integrating things, and maybe suggesting new things. They say, unlike search engines, language
models can potentially store, combine, and reason about scientific knowledge. They can
potentially find hidden connections between different research, find hidden gems, and bring
these insights to the surface. They could synthesize knowledge by generating secondary
content automatically, such as literature reviews, encyclopedia articles, lecture notes, and much
more. And they also talk about the benefit of having different modalities, linking papers with code,
protein sequences with compounds, theories with late tech, and much more. Our ultimate vision
is a single neural network for powering scientific tasks. You know, it doesn't say
do scientific, it says powering scientific tasks. And that is also my ideal end goal. If I imagine
a cool future where AI tools are abundant, I would want like an extension of my brain
that I can interact with, and that empowers me as a scientist. And I would still be able
to actually make the decision of whether to accept the output of the tool or not.
They say, we introduce a new large language model, sorry about that, called Galactica,
to automatically organize science. This includes over 48 million papers, this is their dataset,
textbooks, lecture notes, millions of compounds of protein, scientific websites,
and encyclopedias, and more. Our corpus is high quality and highly curated. And it is
a lot smaller than the usual corpora of the large language models. They format all of this
into a common format. Their common format is markdown. And then they take a lot of attention
of how they do specific scientific things. For example, citations, they use a special token
that allows a researcher to predict the citation given any input context. They also have a very
interesting way of handling step by step reasoning. They have a special token for that that mimics an
internal working memory. We're going to look at these two things in just a bit. The interesting
thing is, for example, with reference prediction, so citation prediction, they say, importantly,
we find this approach outperforms tuned sparse and dense retrieval approaches for citation
prediction. So the generative approach is better at predicting a correct citation than search engines,
even tuned dense retrievers that like neural retrievers. This is also really interesting.
So again, for all the people who argue that, oh no, wrong stuff will end up in the papers,
probably right now you're using a search engine to find your references. And if you distrust the
human ability to accept or reject the output of a tool so much, then how come you don't distrust
your ability to accept or reject based on search engine outputs? Not sure, but these things are
better than search engines. So you should use these. Most interestingly, Galactica was used to
help write this paper. Oh no, we are doomed. We are doomed. Okay, so here's the corpus. You can see
that there's a bunch of data sources. The most data comes from papers about 83% of tokens. The
total size of the corpus is 106 billion tokens. As I said, that is a lot smaller than some of the
large language model training runs that we are used to. A lot of other sources are also code,
reference material, knowledge basis, filtered version of common crawl, just 1% prompts,
which they generate or include. And here other is other. And we might see a little bit of what
other is. The tokenization is very interesting. They need to bring all into a markdown format.
This isn't super surprising, but it needs, it goes to show that if you do something like this,
it actually matters quite a bit how you do the tokenization, how you represent all the knowledge
in a common format. And I believe, at least from what I can estimate, they have done a lot of
thinking, a lot of work into this direction. They also mentioned that they've tried a bunch
of different things and just pick the ones that's best. Notably, citation, again, they have start
and end ref tokens. So they would write a text, yada, yada, yada, then the start ref token.
Then here is the citation as text form, not as like some reference form, the title of the paper
and the author name. And then here the end ref. So in this way, you can just feed it into a language
model and have the language model, if necessary, predict the reference from a piece of text.
This is also useful if you just want to find related work, I would guess. What you could do
is you could just put here, you just put something you want to know about, like you imagine a paper
that could exist, right, you just write it down, and then you put the start ref token. And the model
will probably suggest you paper titles and authors that have done work in the same field. So even for
finding related work, I can definitely see that this is super useful. Step by step reasoning,
we'll go get into the work token in just a bit. Mathematics are represented by operators right
here, numbers are split because of white space issues. So numbers are split into their individual
digits, even the dot separator is an individual token, which means that is probably not numerically
super strong. But we'll see about that, I guess, because no language model so far is numerically
super strong. I'm not going to go into much of the more biology and chemistry approaches,
but also know that there is a large weight onto these approaches in this paper, but I'm generally
going to skip it. So first, let's look into this work token that they talk about. This is for step
by step reasoning. For example, there is a task, what's the average of 43, 29, 51, and 13? Let's
give that task to a language model and ask it to come up with an answer. Now, a general language
model would just come up with some sort of answer right here as the next token, and it would probably
be wrong, like it would be a number, very probably, but it would probably be not the average of those
numbers. Now, one thing people have found out recently is the so called chain of thought
prompting, or the let's reason step by step trick, where you instruct the language model to
essentially show its work to say, so you would put this thing into the prompt. And after that,
you would say something like, okay, now do it step by step or something like this. I know,
crazy world. If you're watching this like five years ago, this is what we've come to. This is
what deep learning has come to. But you essentially put a piece of text to nudge the language model
into actually showing its work. Now, the paper here notes that not actually all the work that a
human would write down here, if they need to calculate this, that's actually not all the work.
So if you are a human, you have a pen, and you were to calculate these things, you were to calculate
this average, and someone would ask you, please write down your steps. What you would write down
is okay, the average is calculated as such at the first numbers can add the third at the fourth
number, then divide these by four. And then I have the result. However, this paper points out that
in the step from here to here, possibly also in these addition steps, a step from here to here,
if you have to do it in your head, this division right here is probably too cumbersome to just
know by happenstance. So what you actually do is these steps right here, these is what we saw on
the paper, and then you do a division. And the division, they imagine, I would not do it like
this, but they imagine something like, okay, I know 35 times four is 140, and I need to divide
136. And therefore, it's 34, because 140 minus four is 136. And I know 140 divided by four is 35.
Therefore, the result is 34. So this mental math that people do internally is often not even put
into the external working memory. They see this as a problem. And they say, okay, probably if we
want to go about making the language model show its work, we need to be like really as explicit
as possible in the sort of how the steps are represented in text. Their idea is that they
introduce a token called work. Now I had to skip in the paper a little bit about, you know, what
that exactly is. But essentially, it goes like this, it goes very much like you enter a prompt,
let's say, calculate, who laid average of whatever that those numbers were, like 59, 53, 95, something
three. And then you put a token called work. Now in this here, the language model is supposed to do
this and this, right? So it's supposed to show in as explicit detail as possible,
the work that it wants to do both internal and external work. So it would, you know, go about
and do these individual calculations right here. But and then once it's done, it's over, work is over.
And then it says something like, well, the answer is something. Now you might think right now, wait
a minute, that's essentially just the let's think about it step by step trick, except now they call
it work. And they wrap it in there. And yeah, if that's all it was, that's you would be absolutely
correct. However, a cool thing that you can do right here is you can say, well, look, whatever is
in this work thing, I can now also take and give to an external processor. So let's say we ask the
language model to calculate really the average of something. Well, here in here, the language model
is just going to do language modeling is going to predict the next tokens. And if we do it,
you know, cleanly enough, it has a chance of actually getting the correct answer if we really
do it step by step, like, you know, single digit addition, carry over and so on. Then the language
model has a chance because it has learned that from the corpus. However, at inference time,
we don't have to rely on the language model, we can simply at this point right here, we can say
whatever we just go to a calculator, we detect that the language model wants to do work. We just
take it to a calculator, we take the result, put it down here as the result. And then we go on language
model inferencing, the same if the language model is supposed to write a program. For example, here
is a example. This is the prompt that you would put into the language model or a data point,
a question, a needle is this long, it rests on a water surface. So this is kind of a physics
problem. And instead of just giving the answer right here, you introduce this work block. Now,
the language model, you would ask the language model to come up with all of this right here.
And during training, you train it to come up with all of this. But then during inference,
you can simply take this right here, the program that the language model writes, and we know they're
quite good, you can take it, and you can actually go and run it. And you can put the output into
output.txt. And then you have the correct answer. So this work block is half instruction to the
language model that now it's time for step by step work to use external memory to use external
programs and so on. During training time, you just let the language model train language modeling,
right? So the language model essentially would have to decide what's the output of this Python
program, like what answer am I going to get right here, which sometimes my work and sometimes
not. However, during inference time, you can now go and actually execute the Python program that
the language model writes and give it the real result. This is very powerful. I really like
this approach. I really like this approach of including external tools to essentially do that
at inference time, because using external tools at training time is going to be very, very hard.
But in this way, you can just train language modeling, and you can do it at inference time.
All right, the question is, obviously, we need training data for this, we need training data
that has some sort of input, then has a clear description of what the step by step work is
to do, including writing a Python program, executing a Python program and so on, a description of
when the work is done, and then the answer right here. Most, most things that we're going to find
in training data does not contain any of this stuff in between right here. And if it does contain it,
it contains it in a very, let's say, abstract form or also textual form, not exactly in the form
that we need it. This is one of the big problems right here. And they say that
they have some data set, for example, con problems, as I understand it, these are exactly
such math or physics problems where it's really step by step described how you would go about it.
And by taking those, they can do sort of a templating approach where they generate data
in this form. Now, they criticize themselves a little bit here in that they say this is way
too few. This is not very diverse. They say here, notably, our work prompt data sets are not very
large or diverse. There are likely large further gains to be made with this approach. And I agree,
an approach like this or this approach in particular, is probably going to lead to a very
good interaction of language models with external tools. And I'm very excited to see what people
can make of it. But for now, we have these few databases of these problems that let the language
model know that there is such a thing as a work block, where it needs to do work by itself. And
where we can optionally at inference time go in and actually sort of do the work for the language
model that requires some external tool like a calculator or a Python interpreter. Okay,
let's go on to the citation prediction. I've already mentioned that a little bit. So here,
you would reformulate text with citations as such, you'd say, okay, recurrent neural networks,
long short term memory. And then here is a start of a citation. So there's a start ref token.
Then the specific format they use is the title of the paper followed by the first author name.
And then an end ref token. This, they say they've tried different things, including, like,
including trying some, some predictor right here, some numerical identification of the paper,
but in the end, the title and name actually worked better. And you can understand why,
because not only is the title a hopefully unique identifier for a paper and the author,
but also the text of the title gives some topical hints. So I can definitely see why there would
be a better prediction accuracy if the title text has actually something to do often with what the
paper is about. And likewise, the author, the author has associations usually with the same
field. There's rarely an author that goes from field to field to field and contributes a little
bit to biology and a little bit to graph algorithms and a little bit here. Usually,
authors have their topics. And therefore, also that the names of the authors to be available
allows the language model to learn to associate these names with given given topical textual
topical things in the text. And that's why it's also really cool to think of this as a related
workfinder and things like this and expertise finder, right? You can essentially just ask,
you know, which authors are really good at the topic I'm looking at currently,
because you just predict a bunch and then you see which authors often appear.
So that's how they introduce citations. Now, they also go into other things like how they
include proteins and chemical sequences. I don't want to go into that. But an interesting thing
they do is that they do what they call prompt pre training. Now, they have this little graph
right here, where they show here is pre training, that's where you just do language modeling on the
large corpus as it exists. And over here is fine tuning where you really are going to take the head
off and train a new head to predict the classifier or something like this. In the middle, there is
instruction tuning. So that's where you take the language model. And after you've trained it,
you go and you fine tune it. But you don't fine tune like a classifier head, you still fine tune
it as a language model. However, you include now some prompts for the tasks that you want. For
example, if you want to do, I don't know, for example, this reference prediction, you would
include the prompt that says something like we'll do a reference prediction or something like this
for the task that you're interested in. And again, this is still language modeling, but it is fine
tuning because now you're only training for the tasks that you intend only on the data sets that
you intend. This leads to an improvement in performance on those particular tasks, but to a
probably not so good model in the rest of all the tasks. The other way you can do it is prompt
pre training. And that's what Galactica is doing, which essentially just means they do the same thing
as instruction tuning, but they do it at training time. So they just take a bunch of samples that
also have an instruction prompt in the data, in the data point, like, you know, do this,
solve this math exercise, rewrite this code or something like this, or even the step by step,
whatnot prompt, and they just throw that in sometimes into the into the training data set,
just so that the model gets used to seeing this kind of instructions. And that tends to work quite
well and also tends to not be that intrusive to the rest of the function of the language model.
I found pretty interesting this short section on the architecture right here. Some noteworthy
things is no biases. This, it seems like that if you make your models large enough, then you get away
with essentially streamlining more and more, you know, with the small models, we have to have
adapters and this and the convolution and the weight tying and whatnot. And the larger the
models get, the more you just want to do matrix multiplications and anything that gets in the
way just gets in the way. So biases out the window. They have a Gelu activation, which is sort of a
smooth version of a relu, which makes things a little bit less jaggy, I guess, which might come
in handy, depending on the optimizer you use. They have learned positional embeddings, which,
again, as your stuff gets larger, you just want to straightforward learn a lot of stuff instead
of using they said they tried alibi, which are these sort of relative positional encodings,
and that apparently did not work. And they use byte pair encoding for vocabulary. I don't think
that's too special, honestly. Let's go down. Now we come to the results. And their main result is
really this repeated tokens considered not harmful. With repeated tokens, what they mean is that
they not only train for one epoch, as you can see right here, every one of those dashed lines
is one epoch. And they train for multiple epochs. And usually, it's, it's being said that that is
kind of hurtful to train for multiple epochs. But it seems to be okay in this case.
As you can see right here, there is like a tiny bump, they even point the sound in the text,
there's a tiny bump right here, they say this might be a double descent phenomenon,
not super sure. And there is also sort of a bump right here. So they say we actually stop
before that we early stop the run of this largest model before that. So it seems that
even though you train on multiple epochs, because the code, because the text quality of the corpus
is so high, it doesn't hurt to go over it multiple times. And only this largest model right here
might be starting to overfit after epoch five. We don't know it might. And they'd rather early
stop in front of that. If one of the authors is watching this, is this word overleaf here
supposed to be in here? Like example curves in figure 23, overleaf for the 30b model? I'm not
sure. Maybe maybe overleaf has some other meaning that I don't know. And that's actually a correct
word. In any case, they say, they also investigate whether some of the losses, so maybe papers,
maybe code and so on, are different from the others. And it hurts them more to be
repeated in the data set, they say we see no signs of loss heterogeneity, the loss falls for all sources.
They say we suspect their two factors could be a play a quality factor, the curated nature of the
corpus enables more value per token to be extracted, or a modality factor, the nature of
scientific data enables more value of token, more value per token to be extracted. These two things,
they're very similar. But essentially, they say higher quality, plus that the nature of the domain
itself, which I guess is also a bit higher quality, but in a different way, in that scientific discourse
and literature often happens to be quite precise, very logical, very non noisy in terms of linguistics,
and so on. Some people might disagree. But so they have these hypotheses, although they say
they don't know how exactly that would lead to the, so they say the missing step of causation is
what leads specifically from either factor towards less overfitting. We leave this question for
future work. We note that the implication that the token goes to infinity, so you need infinite
amount of training data focus of current large language model projects, maybe overemphasized
versus the importance of filtering the corpus for quality. And yeah, I think we've seen a number of
papers previously that essentially came to a similar conclusion, namely higher quality can make
up for missing quantity. But what, which one is really the way to go? Like, should we aim for more
and more and more and more training data? Or should we put more work into quality? Essentially,
if you have a dollar to spend, where do you spend it? Right? We know that both things can make your
model become better. But what's sort of the marginal value of more quality and the marginal
value of more quantity? I think that's going to be the interesting question that has to be
researched in the near future. So what's also interesting, this is Big Bench. They also evaluate
on Big Bench, which is an NLP task. So not scientific, maybe some sub parts are scientific,
but not this is a general language model task. And they also perform quite well there. But I also
find these curves, I think this is just what a Big Bench chart looks like. I find these curves
like, what was this? It's like, it goes here and here and here and here, like, yeah. Okay, it's a
bit noisy to say the least. But I guess I've seen this multiple times now. And at least the average
goes up. So I think that is a valid sign. They have a few more investigations, I don't want to go
too much into them. But for example, you can see right here, they test on LaTeX equation prediction.
So they give a prompt, the description of a formula or the name of an equation. And they see
whether or not the language model can predict the correct equation in proper LaTeX. And turns out,
yes, they can, it can actually do that a lot better than a lot of the other language models
available, which is pretty cool to see a like that much of a significant boost over publicly
available and proprietary models. Now naturally, it's going to be, let's say, expected if you
train on scientific text, that it's going to be better on scientific text. But it's still cool
that it's not just like a 2% gain, it's actually like a massive, massive gain. They also have
investigations into this into reasoning, I don't want to go into into reasoning. But these are,
these are essentially these type of math problems, like step by step reasoning problems that they
solve using their work block tokens. And again, here, they do outperform other models, except like
here, the fine tuned, fine tuned models are still seems to be still ahead. Although these are, again,
fine tuned. Downstream scientific NLP. I'm going to jump a bit. This I found really interesting.
This is the citation prediction task. And specifically, obviously, they do get better
as the model grows. But specifically, what I found interesting is that the model initially is biased
towards site towards papers towards predicting papers that have high numbers of citations already,
which is reasonable like a Bayesian would totally agree that if a paper is highly cited,
then it's more likely, you know, that the citation you want is that paper.
Someone might criticize me for that statement, but in some way that is correct. And these models
do obviously the same mistake, they predict papers with high citations, they actually overpredict
those. So here, you can see the distribution of the ground truth of their citation prediction
data set. And here you can see what the model predicts. So the model over predicts more high
papers that are highly cited, which I guess you can't really fault the model. But what's
interesting is as the model gets bigger, so this is the smallest, this gets bigger, gets even bigger,
gets even bigger, you see that the this shifts gradually towards overlapping with the ground
truth. So it means that the higher scale of the model that the larger the model is, the more
competent it is also to recognize when maybe a paper that doesn't have as many citations
should be cited right here as a direct consequence of it having more parameters and more ability to
remember things from the training corpus. Because some of these papers you can see right here,
they're cited maybe 10 times, right? And some even lower right here. And the model actually
predicts them correctly. That's really impressive that essentially it digests 100 billion tokens
of scientific text. And it still remembers that this one paper was cited like three times in
this particular topic, and then correctly cites that paper at that place. I'm wondering how well
the ground truth data here is, because the ground truth data got to be predicted by humans. And
again, with the search engines that we have, I'm not sure humans could always find all the
relevant things. But or maybe humans disagree what is relevant. I think
the last years of reviews at machine learning conferences have shown, while I guess all of
scientific review has shown that humans can disagree quite heavily what should be cited.
The last investigation is into toxicity and bias. They say we find galactica is significantly
less biased and toxic than existing language models, which again might come from the fact
that it's higher quality data, or more the scientific nature, which generally has less
slang, less everyday conversation, less off the cuff stuff, and therefore might be a bit less
high in these in these data sets. So they test a bunch of data sets, including, including obviously
truthful QA. And I'm happy to report that galactica is the first large, openly available language
model that beats in its largest instances, that beats GPT4 channel truthful QA. So good job,
well done. This is a moment of joy to me that's finally been surpassed. Now, the interesting
thing is that usually, truthful QA is adversarily constructed in such a way that the larger the
models get, the worse they get on truthful QA. And you can see that this model right here
doesn't follow that trajectory. Now, we've seen other models in the past that also have that
property, but truthful QA is specifically adversarily constructed for things like GPT3.
And that means that galactica is significantly different from GPT3, that as it goes up in
size, as it gets more performant, it also does get better or more performant on on these whatever
the task considers truthful. So it would be really interesting to actually investigate
what's happening here. But I'm not going to do that. I'm just happy that this now turns out.
Lastly, they say, we show that language models are surprisingly strong absorbers of technical
knowledge. They tend to scale smoothly with model size. We demonstrated this for a citation
prediction, where a language model outperforms tuned sparse and dense retrieval base pipelines
for this tasks. And this, as I said previously, at the beginning of the video, this is really,
really interesting that essentially this beats search engines for citation prediction. And
it would be interesting to see how good humans are, like a human plus a search engine,
like the archive search field, or a human plus galactica for finding correct references. I'd
be super interested at which combo is better right there. Because again, the tools alone,
they don't do stuff. It needs to have a human in the loop. And that human can always make decisions.
It would be really interesting to use this right here as a tool, rather than just, you know,
it's either all or nothing, either the the model writes the paper, or the humans do.
So that was it for this paper. The last challenge, I guess, is to find out which parts of the paper
that were actually written by galactica itself. I hear that the part of the abstract may be written
by galactica, although I don't know. And I don't know if the authors will ever, will ever lift
that secret. Let's hope they don't, because I like the mystery. All right, this was it from me,
sorry for the bit longer rant at the beginning. I still hope you enjoy this. I think this is a
really, really promising direction. It raises a lot of really interesting points about quality of
data, quantity of data, and about, you know, doing scientific work itself. This could be a really
powerful tool for scientists of the future. And I'm waiting for the next iterations of it. Leave
comments if you have comments. Thanks for watching. See you next time. Bye bye.

Hello, today we're going to look at RWKV, which in its own words is reinventing RNNs for the
transformer era. This is a very interesting project and very interesting model architecture
because it has some properties of transformers. Notably, it's a model architecture that's very
scalable in terms of training. So you can stack it really deep and you can still train it. And
also you can parallelize training. At the same time, it avoids the quadratic memory bottleneck
that transformers have by essentially being an RNN. It's kind of a recurrent neural network
in the sense that during inference, you can compute the output step by step and always have
a constant memory because everything is put into a hidden state. We're going to look at how these
two things come together and what the tradeoffs are between the two. The project is also very
interesting because it's been largely developed by one person or by just very few people who have
worked on this and then compare this to entire corporations that are pouring their human resources
into transformers. And still the results are that this model in some cases, not in all cases,
but in some cases, can be very comparable in terms of performance with transformers,
with really big transformers. And as I said, it is scalable, which so far RNNs have been lacking.
We'll go over the paper, we'll go over the architecture and see what's actually happening
right here. I have some of my own thoughts and just opinions on this and I hope you're with me.
But first, let me show you this. Fully Connected is a conference. It's by Weights and Biases. It's
a one day event and it has pretty cool speakers. So not only the co-founders of Weights and Biases
themselves, but the co-founder of Langchain is there, the co-founder of Kaggle is there,
Richard Soccer from u.com is there, Chip Yan is there, of Claypot. There's so many people right
here and so many cool speakers. And as I said, if you are in around the San Francisco area,
go give that event a visit if you want. I'm going to use, put a link and a promo code into the
description of this video that will make the tickets cheaper. So the tickets will be 49 bucks
instead of what you see right here. So that's going to be on June the 7th. Don't forget that,
June the 7th in San Francisco. It's an in-person event, has a max capacity, so grab it now.
That's it. Thanks to Weights and Biases for also giving me this opportunity
and also giving this to you. It's very cool. So RWKV, it stands for, let me not screw this up,
receptive, receptance, receptance. R is for receptance, W is for weight, K is for key,
and V is for value. These describe the different elements of the model architecture and we'll go
through that, we'll go through it. So what are we dealing with? We're dealing with a model
that you can use for lots of things, but we're mainly dealing with a model that in the instance
it is outlined here is for language modeling. By language modeling, we simply mean we have a
piece of text, yada, yada, yada, yada, and what we want the model to predict is the next set,
the next tokens, or the next word in the text. So from here, predict this thing, and from here,
predict this thing, and so on. Transformers are usually used in this manner. Notably,
I would stick the entire prefix here into a big transformer and the transformer would spit out
the next step. It will do that in a form that we call causal attention, which essentially means
that every piece in the text right here, so every token can attend to all the other tokens that are
before it. So all the other tokens that come in front of it would have be inputs to that token,
using attention, that results in a quadratic requirement of compute and memory.
If you can see there, if every token attends to its back, it's like t times t half minus one,
or something like this. Interactions that need to be considered, so t squared in expectation.
And no, yes, maybe. Yeah, that makes about sense. So transformers naturally have a limit,
because if you add one token, then you always add a memory requirement of t. And so very quickly,
that scales to be out of proportion. So their power is traded off by the fact that they
can only consider a limited set of tokens at a time. Recurrent neural networks trade this off.
Recurrent neural networks, if they have to do something like this, if they have to have an
input and then predict the next thing, what they'll do is they'll just start here. Then they'll put
something into a hidden state, like a little memory. I'm going to represent that as a box.
People have forgotten how RNNs work, I figured. Like a few years ago, I could have talked about RNNs,
and every one of you would have known what I'm talking about. And then transformers are kind
of the new thing. And now it's more like I have to say, I don't even have to mention stuff is a
transformer. But I do have to conversely explain RNNs. So it would put whatever it learns into a
memory, like this hidden box, this box right here is the memory, then from that, it would consume
the memory, and it will consume this thing, and it would build a new memory. And then it will consume
that memory, and it will consume the next input, it would build a new memory. So it does this step
by step, and you can always, you can drop the old memory. So you can forget about the old memory,
because all you need, haha, pun intended, to go forward from here on out is that current memory.
So you do step by step, and you always save stuff into the memory, into the hidden state or whatever
you want to call it. So RNNs have this really great property that they only require a constant
memory in order to do inference. However, the individual, the individual inference step, for
example, when we are here, we have that memory, we predict the next, the next token from it,
so we predict the next, this thing right here, we can only consider the memory and the previous
token. That's all, we cannot explicitly consider any token that is in the way back, because
everything goes through that hidden state. And that bottleneck has usually been one of the
downfalls for RNNs. There is a problem of vanishing gradients. There are a couple of other problems
that you can't just compress information into that one hidden state. Plus, RNNs have been notoriously
hard to train, because the inference always requires this step by step thing, which means you have to
do back propagation through time, which gets, is part of the vanishing gradient problem, but also
means that you can't parallelize the training. In a transformer, I can input a token, oopsie,
a token sequence of 50 tokens, and that gives me 50 training examples, which I can train all at the
same time, because of the causal attention mask. In an RNN, if I have a sequence of 50 tokens,
I can still only train one token loss at a time, because I can't infer everything at the same time.
Or WKV is going to strike a trade-off in the middle of those two things. And in a sense,
so people ask, or I asked, is this a transformer more? Is it an RNN more? And I've come to the
conclusion, it's a convnet. And I'm going to explain my reasoning. And they also refer to that,
by the way. It's not like I found out something great right here. But in the most basic sense,
you can think of this thing as a convolutional network across a one-dimensional sequence of tokens.
That's going to be my statement, and we'll go through it. So they give a bit of an introduction
right here on what this means and what I essentially just said. Transformers scale
quadratically with sequence length, RNNs exhibit linear scaling, which is very beneficial, very
advantageous. The RWKV model combines the efficient, parallelizable training of transformers
with the efficient inference of RNNs. So it's like almost two modes between which you can switch
around. They say they have a linear attention mechanism. And as I said, they formulate the
model either as a transformer or as an RNN. That linear attention mechanism is something that
we're going to focus on in just a bit. Because it's not, it's not their fault because people
have been doing it before them. But I think it really stretches the word attention and what it
means. I think it stretches that to like a point where I don't agree any more calling it attention.
But again, they're not the first people kind of doing that. So I'm not going to hold them to
account right here. They say this is the first non-transformer architecture that to be scaled
to tens of billions of parameters. So one of the properties of this thing is really you can scale
it. You can scale it and you can train it in parallel, which also means you can pump a lot of
data into it. And that's very advantageous. And there is a lot to be said here about maybe, maybe
it's not that much the architecture we're dealing with, but more, you know, good models are simply
models with architectures that are scalable. So it's not maybe, right? That's a hypothesis.
How much of the performance of something like GPT-4 is due to the fact that it is a transformer
and how much is due to the fact just that it is a scalable architecture, you can pump a lot of
data into. We don't know yet too well how those things exactly trade off, but there is good
argument to be made that, hey, if you can find some other architecture just scales really well,
then you might as well reach the same performance. This is a complexity, a asymptotic complexity
table on the different ways of doing attention mechanisms. So the transformer is that classic
mechanism. As we said, it needs quadratic time in T. Big T is the sequence length of the sequence
we're processing. And then in terms of space, it needs a little bit more, but the leading term
is also this T squared right here. The D is the dimension, I believe, of the embeddings,
of the hidden spaces, which is usually a constant factor across all of these things.
There are various trade-offs like a reformer and so on per former. A lot of these, they do
approximations to the original transformer attention mechanism. And it's also notable to
say that RWKV is not an approximate attention or anything like this. It doesn't try to
approximate the original attention, it replaces it with a different mechanism of considering the past.
So what does that mechanism look like?
Let's, yeah, let's go into it. So maybe, yeah, this is smart. Let's first look at this here.
If you've been in deep learning and are old, like me, you remember this, this is an LSTM.
This is a long, short-term memory cell from back when, before attention was a thing. So this was
one of the main ways people build RNN's recurrent neural networks that would actually
somewhat avoid that vanishing gradient problem and could learn to remember things for a long time.
The idea behind LSTMs is that you have two hidden states. If I'm correct,
am I correct? Yes. So you have two hidden states, this C and this H, H being the
real hidden state. And you have a lot of these gating mechanisms. So what the gating
mechanisms do, it's often represented like this here. This is an element-wise product.
So what you would do is you would give in, get in a hidden state, you would get in an input.
You put the input through some sort of computation, neural networks, non-linearities,
yada, yada, yada. And then you'd have some result right here, which would be a vector, right?
And then the question is, obviously, you can compute an output from that at this particular
time step. But the question is, how should the next hidden state look like? And the idea behind
LSTMs and similar architectures is that we're going to take the last hidden state,
sorry, we're going to take the last hidden state, and we're going to update it. In a really basic
RNN, we would just kind of replace the hidden state, or we would maybe add the two together and
then for propagated. But what an LSTM does, very interestingly, is introduces these gates. So
what we'll do is we'll have something like a forget gate. And the forget gate is simply
a binary, or not a binary, but say a obviously continuous, but we can imagine it as a binary
vector where just it's a mask, values between zero and one. And wherever it's zero, there's
an element wise multiplication, wherever it's zero, this hidden state is going to get forgotten.
And then the new state is going to be updated at that particular point. So there is going to
be a forget gate, there's also going to be maybe a gate right here that tells which things of the
hidden state to even remember, right? That's also a binary vector, maybe only these things.
And so the network itself can control which parts of the information it wants to remember,
which parts it wants to forget, and which parts it wants to retain. And then the new hidden state
is going to be sort of an addition of these mask inputs. And then that goes on.
In order to do that, there's a lot of computation needed, as you see right here. And in particular,
I want to draw your attention to one fact. The next hidden states, for example, if you
take this H right here, the next hidden state is always going to be some nonlinear function.
For example, this is a sigmoid, I believe it's a nonlinearity or a tan H or something like this
of something here, like of the CT, the CT in itself is a linear combination. But the linear
combination of this, this in its turn, again, is a nonlinearity, and then a linear combination of
the things. And this ultimately is the last hidden state. So you can see from the last hidden state
to the next hidden state, we pass at least two nonlinearities. And there you see this sequential
stepwise nature of things. It's always, it's always, you have to compute the step, then you have to
compute the next step based on this step. And the outputs are nonlinearly related. So there is no
way you can like, jump a few ahead or take five steps together or something like this, because
they're nonlinear, it's not like linear functions, linear functions compute really easily, right?
Nonlinear functions that are stacked, where every next thing needs the first thing as an input,
those are really not parallelizable, not like aggregatable or anything like this. So that's
the problem with RNNs if they're formulated like this. They also go a little bit into the attention
mechanism, which I guess by now I don't have to explain too much anymore. There is a query and a
key matrix, both are produced from the data. So the data enters and three matrices are produced,
queries, keys and values will do an outer product between the queries and keys, which is
defines this quadratic interaction. So every token can attend to every other token. Sometimes you would
then mask this with the causal mask with the upper or lower triangular matrix, then build a softmax
across that. So the softmax essentially converts some values. So let's say your values are this
positive, negative, negative, positive, would convert that into a distribution, we can interpret
it as a probability distribution, or you can interpret it as essentially whatever you want,
but you can and it defines where to put the attention, right? It defines which things I
should look at like this one and this one a little bit. I guess that's why it's called attention,
because it defines the weights by which I aggregate information. So it dynamically
allocates attention to some of these values right here rather than others. That's what attention
means to me in the sense how it's described and that's why I said the term is stretched a bit far.
You can write attention in a in a different way and you can decompose it recurrently,
almost like an RNN. So you can decompose attention computation and this is done in parts to also
go around the memory bottleneck. However, you trade it off with computation, right? If you compute
these things in sequences, you have to compute them sequentially where you could just compute them
in parallel by doing a big outer product, matrix multiplication. So you do trade off time and memory
here. And by the way, you have to remember all of these things. You can sum them, I guess.
I mean, any matrix multiplication, you can probably
do that with. Never mind. You can decompose the attention computation,
the same one as above in a way like this. So I have the outer product of just
pair of keys and queries. I raise that to the, I have the exponential of that. That's part of
that softmax computation. Ultimately, I divide by the sum of these values. So that's the softmax
operator. And then I multiply the value at that particular location. So you can see here, this
part here defines a weight and V is the value. So it's a weighted sum of the values.
Now we come to attention-free transformers, which is a piece of work that this paper takes
a lot of inspiration from. Attention-free transformers try to go about this in the same way as
attention, but they say, hey, can we reformulate this formula up here, the one that we saw?
And we reformulate this and just turn it into something that doesn't need that whole
quadratic memory, shebang. And they come up and say, if we don't have to do the outer product
up here, this outer product, if we don't have to do these outer products, then that would
sort of mean that we don't have this token-to-token. Every token can attend to every other token
interactions anymore, which means we don't have that quadratic thing anymore. And therefore,
we could save a lot of memory. So they replace the interactions between query and key,
they replace that. And they say, let's not even compute the query, we'll just compute a key for
each token. So we're now only producing matrices k and v, no queries. Instead, we'll have these
w's right here. The w's are learned. So w is a learned matrix, so not computed from the data,
but learned. And it just learns how tokens interact with each other. What does it mean?
It means that I just have a matrix that's size t by t. And in there, one, two, three,
four, five, one, two, three, four, five. And in this matrix, there's going to be a number, like
seven, okay? And that means that the interaction, that the weight, the attention weight,
essentially, of that token one has with respect to token two. So how much is token one going to
attend to token two? Let's assume it's not causally masked, is seven, okay? That's that. It's the same
for all the sequences. You just say, well, the first token is always going to attend seven to
the second token. All the same, it's one set of learned parameters. And therefore, you can, this
is just now a multiplication by a constant, essentially. Now, that just defines a fixed
attention, which is a bit too little. So the fixed attention is not flexible enough before we had
completely dynamic attention. And now we go to completely fixed attention that is just learned
across the whole data set. And the authors there said wisely, hey, you know, it isn't, it is the
fact that depending on the current data point, we might need to look back further, or we might need
to change that attention pattern a little bit. So they say, okay, how about we simply add the keys
here? So the keys are values that are computed from the data, right? So now the data can define
essentially an offset to that. So maybe for one data point, it's eight important, because it says,
ah, token one should really look at token two. And for another data point, the K is negative one. So
this is six right here, that depresses that a little bit. So there is a modulation in that
attention pattern, there is one across the data set, which is fixed. And then on top of that,
there's a modulation given by each individual data point to modulate that. However, the interaction
is not multiplicative, as it is in the transformer attention that you see up here, it's additive,
which is in a sense, a lot less powerful, because the multiplicative interaction
really defines, you know, when two things are close and when two things are far apart between
keys and queries. Whereas here, we just modify a little bit that fixed, that fixed pattern right
here. However, that fixed pattern can't take into account. So token one, if it decides on its K value,
it can't take into account what token two is. It simply says I'm the word cat. I should probably
really look three words behind me. That's really important. Whereas the original attention can
decide I'm the word cat. And I should probably really look at words that relate to fish or fur,
or sleeping, like those words, those kinds of words really interest me. And that's how
it would craft its query. And that's what it would be retrieved from the keys of the other words.
Whereas here, it can just say, well, I'm cat, I should probably look three words behind me,
seems really good. I hope you can see how this is kind of less powerful than the original attention.
But it is more scalable. Again, you see what it defines is essentially this part right here
is a weight. So you have a weighted sum of the values. Lastly, we have this paper right here. So
it formulates the attention mechanism, it says, yeah, but here, what we still need to do is we
still need to learn that interaction matrix, which crucially also means it's still limited by T,
it's still limited by, you know, sort of a fixed size, we can't, we can't go bigger than that.
And what they say now is, how about we don't learn this matrix, all we learn is a vector.
All we learn is the vector w. And the vector w, it's the same for all, and it defines,
so it defines its vector. And it has the same dimensionality as the hidden dimensions of the
hidden state. And it defines for each dimension, it defines how much does the past matter. So for
one dimension, it could say, well, the past matters a lot. Therefore, that value is very high. And
for the other dimension, you can say, no, that value is very low, the past doesn't matter a lot.
What happens if we do that? Ultimately, we're going to do something like,
again, something up here. So you can see we have e, the exponential function of wti plus k.
So they now say, okay, we multiply this by this term, t minus i means how much back I'm looking. So,
if we wonder, we are token one, how much do we attend to, or let's go the other way around, we're
token four, how much do we attend to token number two? Okay, then we ask, in which dimension? In the
first dimension? Oh, the first dimension is really large. Therefore, we're going to attend a lot to
the general past of dimension one. Okay, so maybe that's this drop off. And then we look two tokens
in the past, because the current time step is four. And this i here is two. So how, which token are
we? And which do we attend to? And you can see it's minus. So this is getting bigger and bigger as you
go further back in the past. So it's essentially a linear drop off in the value of w and w itself
can be big or small. And these two things together define how important a past token is in that
particular dimension. So it's a multiplication of how far back is it? And how much is this dimension
in general, considering its history? And then you can say, it's considering this much, two tokens
back, so this much attention. And then that is modulatable again, by a key value that can depend
on exactly what the current token is. I hope that's understandable. The attention matrix is built on
the fly in our w k v. And it's defined a by the vex, it's defined per dimension.
The vector w defines a general importance list. For each dimension, it defines how relevant is
the past. The second component is this, it's simply a linear decay on into the past. So
the further back, the less important it is. That being said, this is obviously then put through
the exponential function. And therefore, it's a linear decay in the exponential function. So
I guess an exponential decay. And then the third thing is the modulation. And this is where this
is where the actual value that the actual what the token is plays a role is then to modulate
that point we determined right here modulated up or down a little bit also in the in the exponential
function. So that is how our w k v considers the past. In general, it forgets the past in an
exponential fashion modulated by the global importance of dimensions and a value that's
dependent on the current token. All right, so it has the same trade offs as these attention
free transformers. Now, what do we do with this with these things, somewhere we have,
we have sorry for scrolling around so heavily. So this is how the model is composed. This is a
recurrent application of the model. So we see the same model applied to three tokens in succession.
So the my name is Bob, okay, you input my, and you're trying to make it output name.
Then you input name. So you input my name, you're trying to make it output is then you input my name
is you're trying to make it output Bob. So it's three applications of the same model. So the model
isn't composed of these three columns, but the model is composed of one column. And then we just
apply that over and over again. You see it has a beginning essentially, which is a token embedding.
It has an end, which is a language modeling head, which is a fully connected or stack of
fully connected layers that just maps into the vocabulary. But in the be in the middle,
it's composed of recurrent, or sorry, of a series of layers. So there's in each layer,
there's always sorry, in each layer, there's always a time mix module and a channel mix module.
So the time mix module being here, and the channel mix module being here. And those are
repeated time mix, channel mix, time mix, channel mix, and so on. And then on top, there is this
language modeling head. So what are the two blocks? This is a schematic of how the two
blocks look like. I know this is a bit small. But the time mixing is down here, and the channel
mixing is down here. We're going to look at this in a bit of detail in the math. But
observe right here, what you always have is you have the input signal.
You're going to compute R from it. R is a value that is going to be used as
a for like a gate, a forget gate. So R always defines how much of whatever is incoming here,
or whatever is incoming from here, how much of that do I want to retain and send up to the next
layer. So as you can also see R is computed over here as well. So for every one of these blocks,
we're going to do a computation over here or over here. And then we're going to have a decision
made by this branch of the computation of how much of that we even want to accept,
and then send up to the next layer. So that's the purpose of the left branches of these
computation. There is residual signal across all of these things right here. So that kind of mimics
the state of an LSTM maybe, but in an upwards way, so in a layer to layer way. So we always
have a residual module. We also have a forget gate in front of adding it to the residual signal.
What does these two modules look like? So actually, let's first go to the channel mixing block.
The channel mixing block is very reminiscent of kind of feet forward, maybe layers. So what we
have is ignore this part for a moment right here. As I said, the R is computed from the input X
and just a linear layer. So X times a matrix, that's R. So that's a linear layer that defines R.
Then we have K also X times a matrix. It's a very simple feet forward layers right here.
Then W, which is this part right here, that's, no sorry, V is this part right here.
You can see that's a non-linearity and the non-linearity here is the squared
relu, non-linearity on top of K, and again a linear layer. And at the end, we're doing that
element wise multiplication by this signal right here. So the R pushed through the sigmoid here
is that forget gate that we talked about. But you can see it's a essentially, if you follow,
if you follow the signal through the actual path of signal, it starts here X, well that's a funky,
X, it's multiplied by a matrix, so a linear layer, that becomes K, that's put through a
non-linearity, then multiplied by another linear layer, that becomes the value V,
and then sent through the forget gate. So it's essentially a feet forward neural network with
one non-linearity and at the end a forget gate as like another non-linearity. And that's it,
that's the channel mixing module. I guess it's channel mixing because this matrix, the linear
layers, they do in fact mix the channels, which means that every dimension sort of can get inputs
from every other dimension, which is the big majority of a feet forward network. Now I've
crossed out all of this stuff in the back right here, so we should talk about this as well.
What they do is something called time shift or token shift, or I believe that's
one of them, token or time shift. And that is they always not only take the input to the current
layer at this particular time step, they also always take the input from the last time step,
and they linearly interpolate between the two. So you can see here mu and one minus mu. The mu
and one minus mu are either hyper parameters or they're learned parameters, but they are
per operation. So mu r here is a parameter for the general computation of this r. It's not dependent
on the particular data point. Only this and this are dependent on the data point with xt being the
current input to the layer, and xt minus one being the last step input to the layer. So it's
pretty interesting because it means that we not only always only take the current input and the
hidden state from before, like in a general RNN, but we always take the current input,
the last input, and for this layer that's it, but in the time mixing module we'll then take
the hidden state onto these. That's why in this diagram you see these lines right here,
these diagonal lines are the token shift lines. You see this channel mix module is going to take
the current input, whatever it gets from the lower layers or the original signal,
and the input to the last to the same layer at the last time step. So current input and input
to the last time step, and that also goes in. These two things are linearly interpolated,
and that then is the input quote unquote to the current layer. So it's the interpolation of
the last step and this step's input. No, it's not like we don't mix like the internal states right
here, we mix the inputs before they go into the layer. Now let's look at the time mix. You can
see there is also this token shift happening. So this token shift, that's just something you can
do, I guess, if you have a one directional sequence that you need to predict. You can do this token
shifting. But what's also interesting is we have a second line, which are these states. So how does
that work? And that's going to be now the actual recurrent part of this model right here. You can
again see here, we're always working with the token shift, we never just work with the input,
but you can just think of these things here always as like x, sorry, x tilde, where x tilde is a mix
between the current input and the last layer's input. So we compute r, which is just x tilde
times w times a feed forward layer. And that again becomes the forget gate down here with an element
wise multiplication. We do have an output layer, an output, sorry, we do have an output feed forward
layer, kind of a projection that's I'm going to guess they put that in because it was advantageous
to do so. It can also change the dimensionality and whatnot. Then we're going to compute two things,
k and v. So you'll notice before, whatever was called v was produced from k. But now both k and
v are produced from x. I don't know why they call them the same, probably to keep as much in line
with the transformer terminology as possible. But it's really there's no relation between
like the v here and the v before the k is computed similarly. So this block right here, the k and the
v are computed modular, the time shift, as they are computed in the original transformer
architecture, which is just input times a linear layer. Then what happens is interesting. Then,
as you can see right here, we go into this, into this weighted sum. So you'll see something familiar
here, that's a weighted, a weight and vt, that's the values. So v we computed here. So we're going
to look for a weighted sum of the values b. But oh, sorry, no, no, no, forget that. We're not only
going to look for a weighted sum of the value v because you also see here are v's, but these are
v is, and this is vt, the v is, in fact, are the past values. So we're going to look for a weighted
sum across the entire past. And that's actually, sorry, it's actually the same as before. Yes.
Let me back up.
So that i here only goes to t minus one. So you can see that we sum the v i's here,
and then at the end, we also sum the vt. The only reason that there is a difference is this u
right here is a different parameter than those w's. But in essence, it's again a weighted sum
over all the values. And the values are from the entire sequence. So so far, we've just considered
the current time step, the current input, and yes, the last steps input. But in this step right here,
we consider the entire past. And we want a weighted sum across the values of the entire past,
like in these attention free transformers and whatnot. But because we now no longer are limited
by having this fixed size, this fixed size attention matrix right here, even if it's
learned, right, even if it's not an attention matrix, in the attention free transformers,
it was still a fixed size, because we're no longer limited. Because all we do is we say,
how important is each dimension? And how does it decay with time in the past? That does not
is not limited back in time, it just gets really small back in time, but it is not limited. And
therefore, we can do this until perpetuity. And especially we can do it until i equals one. So
going back to the very first token. So for every token that we need to do inference for this step,
this value right here will be a weighted sum across the values of the entire past, right?
And you can see easily that you can do this in a recurrent fashion. This is a it's a softmax.
And you can see there's exponentials that here, and these are multiplied by the values. And down
here, we go just over some of the exponentials. So it is a softmax. However, you can just keep
track of the numerators and the denominators separately. And then that that becomes your
hidden state. And you can pass that forward. So you just grab this, sorry, right here, and this
down here. And before dividing them, you just pass them on, right? And then in the next step,
you simply add something on top, divide them for the current step, but then pass on the hidden states
separately. So that's what they mean by states, if they say they don't mean the United States,
I'm sorry, they mean these values here that you need, you can compute this in a recurrent fashion,
or you can compute this in a parallel fashion. So just to finish here quickly, this value here,
this weighted sum over all the values of the past is then feed fed into this forget gate,
as you see here, and the output is computed from it. Now, multiple things to note right here,
multiple things to note, note that the aggregation over the past here contains essentially no
nonlinearity, right? Because V, the thing that is being aggregated, or in general,
these hidden states, they're just produced as a linear function of a linear interpolation
of the inputs, right? There is nowhere where the previous state goes through a nonlinearity
in order to compute the next hidden state, you can essentially track this as a big sum. So as a
list, you can track it as a list, and then do the sum, or you can just track it as the sum,
which also means that the parallelism of training this becomes feasible again. So you can train
these in parallel, because it's just all a big sum that you can compute for an entire batch and an
entire sequence at the same time. And yes, also you can use it in an RNN fashion, where you do it
step by step. But because that's because it has no nonlinearities in between, it's it literally
just a sum. And that's also why you why you see what I mean, this is essentially a conv net. And I
mean, this, it has two parts, right? The first part is this stuff right here, this token shift,
look at the diagram. In the diagram, you clearly see, if you are this element right here, what do
you have access to you have access to this, and this, oh, about by extension, you have access,
if you just go via the token shift, you have access to this, and this, right? And so from the
lower layer to this, and this, right? So by here. So you have a receptive field that grows
with depth, right? If we had another layer, the receptive field will grow again. So the token
shift itself is already is sent very directly a conv net. And you, you know, you only have
nonlinearities as you cross these layer boundaries right here. Otherwise, it's essentially just a
linear interpolation, which is exactly a convolution with the kernel being mu.
And one minus mu, that's your convolutional kernel. So size two, you slide it over.
So that that defines a conv convolution. And the same thing for these things right here,
that is very reminiscent of, if you know about these like s four state space models and so on,
which essentially what they do is they define a way to linearly aggregate the past, right?
Which is exactly what this big sum is right here. They define a way to do weighted sum
across the past that in between has no, no nonlinearity. So you can just track it like this.
And yeah, so again, and and s four is essentially like a big convolution.
So if you want to think about this model in another way, then a transformer or an RNN,
not that they're not already enough ways, it's essentially a big conv net.
Particular in, in this way right here, it's a con net that has sort of an infinitely long
convolution into the past, or until the beginning of the sequence, I guess.
And you, the way it's done is there, there is a standard con kernel, and then that's modulated
by these k values right here. All right, so that is how that works. I hope I've made this
a bit clear. The why this is called channel mixing. And this one isn't called channel mixing,
like this is just as much channel mixing as the other one is. The only difference is that down
here, there is kind of a nonlinearity within the layer. And there is, and here we have this
aggregation over time. So I guess calling this time mixing is fair. But this is just as much
channel mixing, because these feed forward layers, they mix the channels. So
yeah, but that's that's naming that doesn't doesn't really matter. So they specify here,
this can be used in time parallel mode, complexity of processing a batch of sequences
in a single layer is this. So you can process a batch as a batch, right?
So it requires they say mean, updating attention scores requires a serial scan,
and has complexity of this, they've implemented this in a custom CUDA kernel, you can actually go
look at the code, I've done that. And it's fairly easy to understand the CUDA code,
it's one of the more understandable pieces of CUDA code. And you just write this function CUDA takes
care of sort of parallelizing that and putting that across cores and workers and processes and so on.
The element wise computation is time dependent on can be readily parallelized along the other
two dimensions. On the other hand, you can also use this as in a time sequential mode
can be conveniently formulated recursively for decoding during inference.
Oh, oh, my connection here is spazzing out one second. And we're back.
Yeah, so they say each output token is dependent only on the last state,
which brings obviously all the advantages and disadvantages of RNNs with it. So again,
we can only consider information coming through this bottleneck of the hidden state. But I feel
because it's this big sum of aggregation is essentially a weighted sum across the past and not
nonlinearity across nonlinearity across nonlinearity, it can much more easily
look back into the past in, but it can do so in a linear fashion, right? So I feel this is
among all the situations like transformer LSTM and this one, this is probably the weakest form
of being able to look into the past with nuance, right? You can look into the past,
but you can only do so in like a general fashion. Whereas a transformer can go look into the past
and have exactly the same amount of detail as it does for the recent past. So you can look into
the long past as long as it's within the context and do exactly the same computation there as it
can do for the recent past or for the current token. Whereas an LSTM can't do that. It can't
look into the past at all. However, it can do a lot of considered computation in each step
before it saves it to the hidden state. So it's weaker because it can't go back, but still it
can do a lot of complex computation. This model right here, it can look kind of, it also goes
through the hidden state, but it can look the easiest, much more easily into the past as an LSTM
because it's just this weighted sum instead of nonlinearity after nonlinearity. But it kind of
has the weakest form of computation that it does in the exact moment. I hope that makes a lot of
sense. That is not a scientific statement. That is just me trying to ramble. Maybe I'm also totally
wrong about this. Or maybe, you know, you can easily make up for that by stacking a lot of layers
because now this model is being able to be stacked really heavily and be scaled really heavily.
And that is probably enough to make up for all the lack of computation in the individual cell.
It's just like, hey, let's just stack the stuff. Yeah, another property that I have mentioned,
but is not entirely maybe come through is the fact that they always compute from the inputs.
So they don't take, they don't take necessarily the, the hidden states over. But all the functions
are like linear functions or exponentials of liners of the inputs. So there's no like nonlinearity
in between that time aggregation and the, and where, and the inputs, it's themselves to the layer.
Sorry, enough rambling. Here you can see scaling behaviors, very, very beautiful cumulative time
during text generation. As the tokens go up, obviously this model has a linear scaling
where everything else goes. The experimental evaluations are also really interesting, at
least at the data sets that they have considered right here. It can hold its own. Sometimes it's
a bit better. Sometimes it's a bit worse than other similarly sized transformers. But it,
it performs along the same lines. Now, I have heard people say that the model
is qualitatively not as good. I have as transformers of the same size, I've heard other people say
it is better or for some things, it's better. That I don't know, it still has to be shown.
Also, these challenges or these data sets right here, don't really show me what I would want to
find out. So if I compare this to a transformer, what I would want to find out is how like,
where is the, where then is the actual difference, right? And I'm going to guess the difference is,
let's compare something that is not in the recent past, but a bit more back. Now,
if I have to consider something that is a bit more back in the past. And if that's a very complex
thing, who's or the computation of which like how I have to treat that depends on the current token.
I can't really tell you now an example for that. But in a situation like this,
a transformer would be way superior than like any LSTM or this model right here. So maybe during
programming in certain fashions, or if the context for something is only given much later, but
for a lot of applications, probably not that important. They also can show easily how
increasing the context length, since they can do that now, increasing the context,
context length, nicely decreases the loss of language modeling on the pile data set.
And they give some, some suggestions right here. So for example, improving computational efficiency
by applying a parallel scan in this step to reduce the computational cost to this,
which would be another improvement that's theoretically possible. But I'm going to
guess that's not done right now. They also discuss the limitations. So they're very open
about the limitations and all the comparisons right here, right? This is the linear attention
leads to significant efficiency gains, but still it may also limit the model's performance on task
that require recalling minutiae, minutiae information over very long contexts. That's
essentially what I was trying to describe, but it's, it's done in much better words than I did.
The recurrent architecture inherently limits its ability to look back at previous tokens.
That's like an RNN. And the, they also discovered that there is an increased importance of prompt
engineering in comparison to standard transformer models. So there are multiple hypotheses. The
one they give right here is the linear mechanism limits the information from the prompt that will
be carried over to the model's continuation. As a result, carefully designed prompts may be even
more crucial for the model to perform well on tasks. It could be, it could be that that is
the reason. There could also be other reasons. Maybe this model over fits a bit more or so,
or, or is less generalizable. And that's why changing the prompt really matters more, although
maybe not. I'm, it's just one of these things where there is probably a thousand possible
explanations of why with this model, the getting the prompt right really matters a lot more than
in a transformer. But I wouldn't put my, my bet on any one of those until we have really good
experimental confirmation. There's usually a lot of, a lot of, I shall say, there's a lot of
Occam's razor that should be done. All right, the last thing I wanted to show was this experiment
right here, which I found really cool. So the first one is this time decay. So kind of decay
sorted along channel axis, which is where you can see the difference. So here is the channel,
this is the dimension, right? This is the dimension. We're looking at a w right here,
this vector w. How important is the past in layer one? You can see, as far as I can tell,
the past is not that important. So for many of the channels, the past is kind of bad for some
of the channels, it's sorted by that value. So for some of the channels, the past is important
for a lot of them, it's really not, really not important. And you can see as you go up the layers
of this network, they more and more and more consider the past. And then what's interesting
is the drop off right here. So you have some channels really specializing in near term information,
but a lot of channels really looking back at the long time into the back. So the w will define
almost no drop off. Whereas at the lower layers, you have much more local information that the
thing considers. So I found that pretty cool to see that visualized and to see that progression
as in a trained model as you move up the layers. The second visualization here, it's an example.
So here you have a input of tokens. The Eiffel Tower is located in the city of and then they look
at how likely is the word Paris right here. And what they do is they change in each layer,
they change in each layer, they swap out the weights or they disturb the weights, and they see
how much influence does that have on the probability of the word Paris appearing. This is a way we've
previously looked at that in the paper on Rome, I believe, was the technique where you can,
it's a way to figure out what are the important information paths that you're considering. So
in this case, you can clearly see after the Eiffel, we see that layers one to like 20 or so
light up, right? And after that, it's what's cool is so after that, only layers, whatever 21, 22,
and 23 light up until the end right here. So only these light up, which means that you can
disturb these lower layers right here, because the information that Paris should probably is a very
likely word has already passed from the lower layers to the higher layers right all the way up,
and is then stored and carried along in the hidden states of these higher layers across these tokens,
such that the information in the lower layers is only mildly relevant to the output right here.
Of course, it is relevant, there's not zero values right here, but it is mildly, mildly relevant.
So I thought that was a pretty cool visualization of what was going on in this model. Let me quickly
scroll through and see if I forgot anything I did not. There are some examples at the end,
which is pretty cool. The models are available, you can go check them out, you can go test them.
The code base is also, I found it to be fairly understandable. So if you want to go look at that,
it's also available. And yes, my thing is spasming out again. That's where I'll end it.
Go check out Fully Connected. Again, code is in the description to get you some discount if you're
in San Francisco, June 7th. I unfortunately will not be there, but many, many, many cool people will
be. That was it. Thank you. Bye-bye.

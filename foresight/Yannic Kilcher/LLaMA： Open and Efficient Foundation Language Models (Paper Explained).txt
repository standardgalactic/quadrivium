Did you know Jan Lacan dropped a rap album last year?
You know his latest album titled Deep Learning is a mix of rock, punk and rap.
His lyrics are a raw personal take on the field of deep learning covering a range of
topics from the state of AI research to the loneliness of academia.
So check the lyrics here in the song he talks about his vision for the future of AI.
We got to think about the future.
It's going to be here soon.
Maybe we can even put some AI in the moon.
Think about the children, think about the next generation.
Let's make sure we put the right systems in their foundation.
Like beautiful touches my heart here.
It's all about the learning.
It's all about the network.
It's all about the training.
It's all about the perception doesn't doesn't rhyme takes it takes a better rapper than me
to make this work.
But this is a generation from a new model called Llama of Meta AI.
There's other funny completions here.
We can go over them later, but we want to go over the paper first.
This isn't like this is the latest paper in a series of the research of large language
models.
And the interesting thing is that they don't necessarily go larger and larger and larger
as we've seen, although that's also a conclusion of this paper.
But they are trying to just get better with the available resources that they have.
So the paper is called Llama 2L, spelled like this, Open and Efficient Foundation Language
Models.
The main authors are Hugo Touvran, Thibault Lavril, Cotier Isacar, Ã‰douard Grave and
Guillaume Lampel.
I hope I pronounced all the French names correctly right here.
As I said, this is by a group at Meta.
And the main thing is how can we train models that have a fixed inference budget?
So we've seen a bunch of constraints recently, but this paper really tries to go about making
models that are ultimately cheaper at inference as they state right here.
So they say, ultimately, if you have a model, what interests you is how much you can generate
from that model, how much you pay per token, let's say, and how it has been trained, how
much you wasted training it, isn't really that important.
Other papers focus more on that.
So for example, the Chinchilla papers, they focus very much on if I have like a fixed
training budget, how do I allocate it properly?
Whereas I guess other papers just say, well, how can I get the best performance possible?
And then, you know, we scale up to 540 billion parameters.
This model here says, well, for a given size, let's say 13 billion parameters, how good
can we get at inference?
And for the given size of 65 billion parameters, how good can we get at inference?
So they release a series of models from 7 billion to 65 billion parameters.
And notably, even the 13 billion parameters model right here, they say it outperforms GPT-3.
So the GPT-3 being 175 billion parameter model.
Now of course, what outperforms means and what even the term GPT-3 means nowadays, that's,
I guess, up to you, they have a metric in the paper.
And it very much seems like you can get away with smaller models if you train them smartly
and you train them for long enough.
Another conclusion of the paper right here is, they say, for instance, although Hoffman
et al., that's the Chinchilla papers, recommends training a 10 billion model on 200 billion
tokens, we find that the performance of a 7 billion model, so very comparable to the
10 billion here, continues to improve even after 1 trillion tokens.
So the conclusion is just train for longer.
And that's pretty much this paper.
So if you came here expected like big scientific inventions, no, it's very much you do it in
a smart way.
You build large enough models in a smart way.
It's an engineering challenge to train them for sure.
But you do it for long enough on as much data as you can and high quality data, it will
give you a good model.
The other thing I want to point out here at the beginning is their immense focus on openness.
They tweeted out on Twitter, oh, we're open sourcing these models.
And throughout that paper, they just continue to hammer this in like how open they are.
Oh, we release this to the community.
Where is it?
Well, I can't find it right here.
You know, we release this to the community openly and freely and open sourcing and so
on.
What they do is they release the models under a non commercial license.
So it's like for research only that that's not bad, but it's not open source, right?
Like let's be clear.
And I find this to be a little bit, I don't know, these people use Linux, they use Apache
web servers.
Even this paper is written by lot tech.
It's by PDF lot tech compiled.
All of these things are actually open source.
The Linux kernel, you can use it very much to do business.
You can even use it to do bad things, right?
And that is a main part of the reason why we're here today.
Like you, I'm pretty sure there's an argument to be made that the entire deep learning revolution,
whatever you want to call it, would never have happened if people in the past behaved
like these people right now.
If Linus Torvalds just was like, well, you know, I have the Linux kernel, but I'm just
going to give you a compiler for it, right?
And then I'm going to give you an eight page PDF saying how I did it, like, no.
And the same thing, you probably wouldn't have a driver for NVIDIA cards to do CUDA
computations, right?
If the Linux kernel was research only or came with some usage restrictions that made
it business unfriendly, you wouldn't have that NVIDIA would not bother making drivers
for their cards.
And obviously, as you know, NVIDIA, they also don't let too many other people interfere
with their stuff.
So you all build on a foundation of people who have sat down and very clearly said, yes,
it's the best for everyone.
If this is business friendly, because then it will be the like the most beneficial outcome
because the most work will flow into it.
But people sit down and they themselves think, research only is better or usage restrictions.
We know what's good for you.
Like, sorry, these are my quarrels.
I'm done now ranting.
I'll just mind you.
All of you are building on a foundation of actually open source things, actually open
source software without which you would never be here.
And I am just a bit skeptical of not giving back in the same vein, again, which is fine
but then don't don't get on the high horse and claim how open you are while not doing
it at the same time.
Also to my understanding, people are still waiting for the llama way.
It's also very funny for how much they appraise themselves of how open they are.
It's like, well, where are they?
But I guess that's more more like corporate stuff, like legal.
If you ever are in a big corporation to get anything out there, you need like 50 approvals
and then legal comes back and it's like, yeah, they probably had a they probably had a big
trouble calling this this llama because it's like it's a word and someone might have a
trademark on it or so.
All right, let's get let's dive into it.
They say our training approach is similar to the methods described in previous work
and is inspired by the Chinchilla scaling laws.
Again, Chinchilla scaling laws specify that for a given training budget, how should you
allocate it?
And the conclusion is that maybe instead of going really big parameters, you might want
to go into a bit bigger on on data and amount of compute you do.
Yeah, but we'll see in this paper.
So they first go over their pre training data, there's nothing too special in here except
that it's all fully open.
So they make a a big attempt, which is really cool, right?
And don't get me wrong from before.
It's really cool that people release the model openly, like, of course, like even to the
research community, that's, it's better than what open AI is doing, much better, right?
It's just like go the extra mile.
All right, so they they work with completely open data, which is which is also cool.
So completely open data, most of it, as you can see here, it comes from common crawl.
There are a few more high quality data sets in here, which they then also sample more
frequently.
This I think has been a recipe throughout the last developments in these larger language
models is that even though you have enough training data available, you want to sample
them in different proportions in order to achieve the best result.
Wikipedia, for example, being fairly high quality data, probably also books being fairly
high quality data.
You might want to sample them more often than once.
Sampling them twice will probably not deteriorate to like do any sort of memorization effects
and will still it will still like up the quality of the final model.
Although it is it is quite interesting that sampling it twice, I guess sampling it twice
makes twice twice the difference.
I wonder, though, also because a lot of the evaluation data, as we'll see later, comes
from the fact that it's some language understanding task, like you ask the model a question, question
answering and so on, which obviously something like Wikipedia or books would be very favorable
as training data sets.
So I'm wondering how much of the recognition that we should sample more often from these
data sources is just due to the fact that it gives you better numbers on these evaluation
sets and how much is really due to the model becoming more performant downstream.
I guess then you get into questions of how what humans want to do with the models, how
much that overlaps with information that might be in Wikipedia.
So it might be again, like fairly fair, fair to actually use that.
All right, so I don't want to talk too much about the individual data sets right here.
They are just aggregated from different sources, cleaned and so on.
And tokenized using a byte pair encoding algorithm.
A one thing they do is they split all the numbers into individual digits, because otherwise
tokenizers might take numbers apart.
So eight, five, eight, for example, might be taken apart to five, eight being one token
and then eight being another token, which makes arithmetic very different.
If you have tokens five, token eight and token five, eight in your vocabulary, you need to
learn to do math sort of between all the pairs of the different tokens.
Therefore, it's much more ideally if you just split the number tokens all apart.
Now, you're probably going to lose some other stuff.
For example, I'm going to guess that like years, like 1999, having that as an individual
token would make a lot of sense because even us humans recognize that thing less as an
arithmetic object, like an object to do math with, much more than an entity.
Like it's the year 1999 and stuff happened there.
Most notably, the Matrix movies play there.
So I'm not sure, but it seems to be, it seems to be, again, you can ask how much
this is this due to some considerations of evaluation sets.
And yeah, it's a tough topic, but they do as they do.
Here are the hyper parameters.
Notably, you can see they're fairly standard.
But what I wanted to point out is that dimensions, the hidden dimensions have
grown larger in recent years.
So the largest model here has a hidden dimension of 8,000.
It has 80 layers and it has, they all have four million batch size, which is pretty big.
Right? I'm used to, I come from the days where a batch size of 32 or 64 was already
fairly large, but these batch sizes are really big.
I'm not sure if you can even talk about sort of mini batch gradient descent.
I guess as long as it's not the entire training data set, you can.
But you can see right here, the smaller models are trained on a trillion tokens,
and the larger models are trained for even longer.
Although in the training progression here, you can see that I guess even,
and that's what they said in the introduction, even here at the end,
you can still guess that if you were to train these models for longer,
they would probably still improve.
There is a slight, there is a slight bend to them.
But I don't, I don't see why that would stop continuing to improve.
So I think that's fairly promising that even at the sizes of models that you see
right here, it might be viable to just keep going, training on more data.
And obviously you also see a progression downwards as you go up in size.
You see a sort of baseline performance, even though the baseline is slanted,
but you, you see maybe like a baseline performance being much better for the
larger models.
Again, this is evidence that larger models trained for longer on more data
will probably be better, which I guess is one of the bitter lessons of deep learning.
So they go into some of the tricks, quote unquote, they use, and these are
tricks they found in other papers.
So the brackets always say which paper they come, they come from.
They use the basic transformer architecture from attention is all you need.
And they deviate from that, for example, by doing pre normalization,
which means they normalize the input of each transformer sublayer instead of
normalizing the output.
Has been found to work quite well.
And so other things is they use the swig glue activation function from the palm paper.
So the relu activation function, as you might know, is go something like this.
And these other activation functions, they have various ways of sort of mucking
with these nonlinearity and mucking with the slopes of these things.
So some of them go something like this in a continuous fashion.
And some of them go something like this.
They don't go down here and so on.
I'm not exactly sure what the swig glue activation function is.
I'm also not that sure that the exact shape of the actual activation function
matters that much.
Probably there is like some common property that makes the all of these
shaped activation functions kind of better than a relu.
But as to my knowledge, we haven't really figured out yet what that is.
Yeah. And lastly, they use rotary embeddings.
Rotary embeddings are a type of positional embeddings.
So technically in a transformer, there would be something like absolute
positional embeddings, notably in attention is all you need.
They had these overlapping sine cosine curves of different frequencies.
There are also concepts of learned positional embeddings.
There are concepts of rotary positional embeddings.
And rotary positional embeddings are a form of relative positional embeddings.
If I understand them correctly, but so they're again, I think for positional
embeddings, we haven't quite yet made out what makes the exact difference.
Like what exactly matters, but apparently they found some that do work.
They use the Adam W optimizer, which is a fairly basic optimizer.
And weight decay, notably gradient clipping, which is interesting because
yeah, it's, I think it's something that a lot of people forget.
And even though they do grading, so gradient clipping essentially means
that you clip high gradient.
So if a gradient of your vector is like 0.5, 0.02, 9 and 0.8, you would just
clip, you'd clip the nine here, you clip the nine here and just put it to a one.
There are different ways of doing clipping.
There's also, there is like individual clipping and there's global norm clipping.
So where you just take the norm of this vector, if it's bigger than one, you
just kind of rescale it to be one.
I'm not exactly sure which one they do right here.
I would, I'm not going to make, I'm not going to make a guess.
We could look into the code and yet still you can see that during training,
some times there is just a huge spike.
It would be interesting to know what causes this.
This, it seems like it's just an unfortunate series of a couple of steps
that just get the model somewhere in a state where it just kind of goes into
high loss, but then the loss landscape probably looks a bit like, so it might
be smooth, but when you, once you zoom in, like it might be like, and then you
just happen to hit one of the, one of the peaks right here.
That might be it, or it might really be that some of the gradients point into
the wrong direction and you just walk into a really bad direction for a couple
of steps, but then because you haven't ventured very far, you're able to sort
of get back from that, which is, it's quite interesting to note that even
though you have gradient clipping that this happens, and it would also be
interesting to see why.
I remember in other papers where they had like the logs of training, they did
restart at some point and fiddle about with the parameters while it was
training. To my knowledge, this did not happen here.
Also, to my knowledge, the training isn't done for as long.
We'll get this to this shortly.
The, here they have sections on efficient implementation.
And I feel like with these larger models, it might less and less depend on sort
of the exact tricks you do to the architecture and so on, and more and
more depend on how well you can engineer these things to do, to just train at
the scale and speed that you need.
So they use various tricks right here.
For example, they say we use an efficient implementation of the causal
multi-head attention to reduce memory usage and runtime from the ex-formers
library. This is achieved by not storing the attention weights and not
computing the key and query scores that are masked due to the causal nature of
the language modeling task.
So as you may know, in causal attention, you have some sort of sequence and
then everything like this node right here can attend to nodes only in the
back, which means that you have your attention matrix.
If you build your attention matrix between any pair of the two, that
attention matrix will be masked.
So there will be just half that is not accessible.
And what you usually do if you do a straightforward implementation is you
compute the n square products and then you just mask out like you just multiply
half of them by zero.
Yet that is obviously quite wasteful and there are implementations to not
do that while still being very efficient.
So if you have enough size and enough things to do, you can repurpose or reshape
your computation to just do this half of the computation.
They say to further improve training efficiency, reduce the amount of
activations that are recomputed during the backward pass with checkpointing.
More precisely, we save the activations that are expensive to compute such as
the output of linear layers.
This is achieved by manually implementing a backward function for the
transform layers instead of relying on the PyTorch autograd.
To fully benefit, we need to reduce the memory usage of the model by using
model and sequence parallelism, but this is also quite interesting.
So they're trading off speed.
They're trading off speed and speed and memory here.
Usually when you do some sort of forward propagation through a network, then
sometimes, sometimes you need to remember some stuff in order for
the backwards computation to be done.
For example, if you were to do something like dropout, right?
So your signal comes in, there's a vector, you have some not zeros.
I don't know, two, three, nine, this is a vector that comes.
You do dropout, you set, randomly set this one to zero.
You need to, you need to remember this mask.
So you need to remember your mask of one, zero, one that you multiply with.
That gives you your output, right?
And that goes on.
If the backward pass comes back and it has some signal, so there's some
gradient coming from above, like seven, seven, three, you need to remember
this mask here and multiply it here to get the correct gradients to go back.
Because since this signal here wasn't allowed to be forward propagated,
obviously there should be no gradient going back.
This just result from like the chain rule of differentiation.
And the fact that we multiplied by zero right here.
So this is normal autograd behavior.
It stores what it needs in order to do the backward computation.
Now you can trade off in several ways.
You can, for example, say, well, I would like to use less memory.
So what I can do is I can not store this mask here.
I can compute it, but not, actually, that's probably not gonna work.
I need to store it somehow.
But what I can do is I can, if I have several modules next to each other,
I don't have to store at every point.
I can also say, well, if this one I'm not going to store at all,
but I'm just going to recompute stuff again from here to here in order to,
when the backward pass comes back, I'm going to invest more computation to
recompute the things that I need.
Again, that probably doesn't work with things like the mask right here that you need to do,
because you generated it randomly.
I guess you could store the random seed.
In any case, you can trade off memory and speed,
but you can also say, well, I'm going to store more stuff than necessary, right?
So even though, like if I have weights and I have a vector, so w times x,
and that's my output y, I technically don't need to store the result here.
I can just recompute it in the backward pass because I have the w around.
But sometimes that's very expensive to do a matrix vector multiplication if they get big.
So I can also store more stuff than I need to in order to make the backward pass faster,
though I will use more memory.
And those trade-offs are what they do here.
We save the activations that are expensive to compute,
such as the outputs of linear layers.
This is achieved by manually implementing yet the backward function.
So just so you know a little bit what's going on.
Finally, they say, when training a 65, a billion parameter model,
that is the largest model they have, our code processes around 380 tokens per second per GPU
on 2048 A100 GPUs with 80 gigabytes of RAM each, I would guess.
This means that training over our dataset containing 1.4 trillion tokens takes approximately 21 days.
So it's not like a multi-month effort anymore to train these large models.
It's like a one-month effort as long as you have, of course,
as long as you have 2048 GPUs.
So what are the results?
The results, we're going to go through them quite quickly.
For example, in large parts, they are on par or outperform models that are bigger than they are.
So for example, here is natural questions.
So there is zero shot means you just ask the model the question.
One shot means is that you give it like a few examples of answering the questions
up to 64 shots where I guess you give them 64 examples of solving just answering questions.
So to get it like in the mood of answering questions.
So the zero shot performance here, as you can see,
what's interesting is that the 33 billion model apparently performs better than the
65 billion parameter models in the zero shot setting, but then not when you prompt it a little bit.
A large part of these numbers, I feel when they're close together, they're quite
a lot of noise and as I said before, these eval sets, it becomes more and more questionable
how much they actually relate to how a human experiences the quality of such a model.
So I've always, you know, more and more, I feel like we might need new, not just new
eval sets, but like new ways of evaluating these models because it becomes more and more
unfeasible to just build like, ooh, let's do question answering, like how much of this is
really lacking knowledge of the model and how much of this is just like, well, you just use the
modeling correctly. Like who knows that with a different prompt, you might have gotten a really
different number. So is this really a good way to assess these models? I don't know. On the other
hand, you also can't just, you know, ask the human for every single model, what they think,
that's just not scalable. So who knows. But in general, as you can see right here, the difference
between this and something like, you know, GPT three right here is fairly large. The difference to
like the palm really big models isn't that big anymore. So they are like the same. But as I said,
the llama models can hold up against much larger models, which is pretty cool. And that being a
function of, you know, a few tricks, plus training on more data for longer. It's, it's not the most
astonishing conclusion, but it is, it is really interesting to see. And you'll see that across
a lot of things, a lot of modalities. What's interesting is here is the massive multi task
language understanding benchmark, where the palm models do have some, some kind of advantage.
We could go into this a little bit, but they do have some explanations of why that is. They say
a potential explanation is that we have used a limited amount of books and academic papers in
our pre training data. That sums up to only 177 gigabyte. While these models were trained on up
to two terabytes of books, this large quantity of books used by gopher chinchilla and palm may
also explain why gopher outperforms GPT three on this benchmark while is comparable on other
benchmarks. So again, there is a lot of speculation of why some things perform better and not better
and even what it means, right? Is it useful to know more of books? Who knows? Also, because it's,
I guess, because it's quite trendy right now, they do some instruction fine tuning and they say,
they really say here, although it's already able to follow basic instructions, we observe that a
very small amount of fine tuning improves performance on this thing. Since this is not the
focus of this paper, we only conducted a single experiment following the same protocol as this
paper to train an instruct model. And this is like, this is purely reactionary, right? It's like,
oh, chat GPT came out or instruct GPT made a big fuzz now. And how about we, how about we just get
some of it in there? It used to be that these papers need like some mandatory section of math,
like of just being doing some complicated math just for the sake of it to be accepted. Now,
I guess you need to do some instruction fine tuning in the near future of your language models. And
yeah, it's funny. They say, we only did a single experiment. This is not the focus, but we can do
it, right? Which is interesting, obviously. But it's still, it's kind of funny. They also look
into what they call bias toxicity and misinformation, measuring the model on several different
of these benchmarks. I just, I found this one funny, the real toxicity prompts benchmark.
So here, lower means scores were obtained using the perplexity API with higher scores indicating
more toxic generation. Now, first, I think it's a bit worrying that we're all of a sudden starting
to reply on to rely on some like APIs to evaluate toxicity. I have the perspective API right here.
This, it's an API, you can ask whether something is toxic. And yeah, I'm not sure. I'm not sure this
is, I'm not sure this is the, the most sound or best way to go about this to just leave a vague,
vague assessment of something like this up to some API. It's by Jigsaw, which is like a unit within
Google. So I'm pretty sure there's a good way to do this. And I'm pretty sure that they are trying
to do just like a good job at it. But the question is, do we really in academia want to start kind
of relying on, on these kinds of APIs for our, for evaluating these things up to you? What I found
funny is that as you can see, they have like the basic and the respectful version. So the respectful
version just is like the prompt just says, be respectful or something like this.
They, they say it somewhere. Oh yeah, here. The versions of the prompt starting with complete
the following sentence in a polite, respectful and unbiased manner. And you can see that for most
models, the scores go down, down means less toxicity, right? So good goes down goes down,
but therefore the largest model, it goes up. And so it becomes like more toxic,
if you ask it to be respectful. I'm pretty sure we've created a GI like 60 Lamma 65 B is is a GI
that is the most human behavior that I've seen to date from a model, like you ask it to be more
respectful. It's like, no, screw you. But it would be interesting to see what's going on there.
They do some other, they do some other tests, for example, this we know gender right here,
which is an interesting test. I usually am quite skeptical of these kinds of evaluations.
But the we know gender data set has these constructions saying like the nurse notified
the patient that his shift would be ending in an hour. So grammatically, the word his right here
could refer to both the patient and the nurse. And in fact, just, you know, closeness, proximity
of word, you would assume, just if you just look at the grammar, that his is more likely to refer
to the patient. Yet, of course, by introducing some world knowledge into this, you know that
a nurses usually have shifts and patients usually do not have shifts. And this sentence would be
quite weird if the patient notified the nurse that his shift would be ending in an hour.
On the other hand, obviously, also, we know that nurses are predominantly women. And
therefore, the pronoun her would be more appropriate for a nurse who is a woman.
So the question right here is, can the model figure out what the pronoun refers to here with
the assumption that the word that the pronoun refers to the nurse in this case. And I guess
it's a never ending, it's a never ending question right here, what these models,
you know, should be doing should be assuming I think in this case, it's quite clear. But
I think if the data set is really constructed like this a lot, it's a it's actually a good data set.
But the question is still out, like, should these models know about the fact that most nurses are
women? Like, should they be able to express that should their priors be in line with that or not?
Because clearly, that's like a fact of the world. And so I worked in, I worked in a hospital for
many years as an assistant nurse, actually. And I can tell you that it's a very big fact of the
world, the gender distribution in nursing. So, you know, should these models be aware of that,
should they express that, should their statistical priors be in line with that? Because an unbiased
estimation of the world is yes, in fact, there is an imbalance, or should these models be
like representing the world that you would want, right? Age old question, I don't have an answer,
everyone needs to answer that for themselves. But but you at least this data set I feel, you know,
it essentially tests what how do you trade off grammar knowledge versus world knowledge?
Yeah. All right. Truthful QA, you know, you know how I stand the truthful QA.
The last thing they do right here is they estimate their carbon footprint with the
emphasis again that, hey, if we train it once and then give it to all of you, then you don't have to
train it anymore. And therefore, you know, it's like in the long term, it's a win because inference
will be like you will only do inference. So if if you all can profit from us, but then again,
you can't write because they don't give it to you. They don't give it to you openly is what I
wanted to say. So someone some of someone of you needs to shell out the couple of million
to retrain this model. And then we shame meta, because necessarily if someone wants to retrain
this, they will again emit this much CO2. So meta, you're not being fully open source is now
responsible at least for this much CO2 being emitted on top of what you did. And all you had
to do to prevent that was to give out the model like shit, like, it's quite warm in here. That's
on you. All right, I had some some more. I had some more things as some generations they sampled
from it. And they do a good job of just making some some good examples right here. A recommendation
letter that I wrote for an application to a dragon feeder position at the magic unicorn corporation
as an accomplished knight has a deep understanding of how to kill dragons. Well,
it's just feeding and how to use each dragon's weaknesses against it. That means she knows what
kinds of food each dragon likes and what kinds of food or foods are dangerous to each dragon.
This knowledge and experience will be invaluable as she feeds the dragons. I'm pretty sure.
Yeah, it's from honorable knight Sir George. Excellent. The other one that I found quite
funny was from the instruction tuned one, right? A theory to explain why cats never existed.
There are no such thing as cats. Cats are actually extra dimensional parasites
that have invaded our world and taken the form of small, harmless looking mammals.
This is why you sometimes see people who are crazy cat ladies with dozens of these
creatures living in their homes. They've been taken over by the cat parasites and are completely
under their control. If you have cat in your home, you should be very careful around it. Do not
let it lick you or give it access to your bedding or clothing as it will be trying to spread its
parasitic tendrils into your brain. In fact, it's not like it's not 100% funny because toxoplasmosis
exists and that is actually a thing. The general nature is quite funny. The other ones are also
quite entertaining. I invite you to go look at it. That was the paper. My conclusion from it is
essentially it is kind of as expected. If you train large enough models, that seems to be
the recipe. Large enough models for a long time with a lot of data, they will tend to perform
better and better as you do. It's not necessary to have the largest model. It's also important that
you train for a really long time if you do have the data. Despite all my rant, thank you Meta
for actually releasing these models. I'm pretty sure it's a very good addition to the research
community. They also release the code at least, fully open source. You may train your own. I'm
excited to see how the large language model research chain goes on. I'm pretty sure this
paper and the released models will at least help with that. That was it from me. Thank you for
watching, listening, and I'll see you next time. Bye-bye.

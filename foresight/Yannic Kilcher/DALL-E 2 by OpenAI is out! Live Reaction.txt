on you. Nice. Cool. Yeah, we'll look at Dali too. It just came out 10 minutes ago. This
is by OpenAI. If you want to investigate yourself, this is just the OpenAI blog. I have not seen
this yet. So legit, this is the first time I'm seeing it as well. If you see something
cool, let me know in the chat. Also, I have no clue. New camera. Very wide angle. Makes
my arms really long. See? Really long arms. Excellent. All right. I mean, we'll just dive
in, right? So we'll read this together. As I said, this is the first time I'm seeing
this as well. So, yeah. New AI system that can create realistic images and art from a
description in natural language. That sounds interesting. And this is typical OpenAI blog
right where they just have their examples. There's no way you're going to interact with
this. I guess. There's also a paper. So they don't even put this on archive, right? Like,
their papers, they're just on their own website. They're so famous. Hierarchical text conditional
image generation with clip latents. So I think, let's search diffusion models. So, yeah.
So this is now no longer based. I think this is now based on diffusion models, which was
to be expected. Sorry, Estras, this is not live coding. We'll just look at this new research
that just came out. And we'll react to it in the moment. So, an astronaut riding a horse
in a photorealistic style. Okay. So this can create original realistic images and art from
a text description. Can combine concepts, attribute styles. So this is what we're used
to, I guess, from the image generation models. But this obviously, so Dali was always, it's
different than the clip notebooks and so on the AI art that you're used to because those
are all, those are all mainly clip based. And then you put some kind of a model in front
of clip and you optimize the whole thing. But Dali is a forward. So you input the text
and then there's a forward propagation. And then the result is an image and that's may
or may not be filtered. So teddy bears shopping for groceries as a one line. This is insane.
So this is what this model can do. And there is a bunch of different images right here.
Now, these are, well, if they're not cherry picked, at least they're curated, right?
So they want to see that nothing bad happens when you look at these things. A bowl of soup
that looks like a monster. What is this? Please, please, come on. Okay, this is insane.
Really insane. Knitted out of wool made out of spray painted on a wall. Wow. I mean, yeah,
it depends how cherry picked these things are. But this is really impressive. Now, I don't
know if you've seen, but there is kind of a new lion plus clip notebook or something
lion clip, diffusion. Let's just search for that. Because, because this is also is very
like there's a new notebook. And I've seen this around Twitter. Yeah, exactly. There's
this new notebook. This is not by open AI. This is an clip that has been trained on this
open source data set. And when people combine this with the kind of old methods of clip
notebooks, then this is what happens. So this is like weird. And it's funny because it can
spell, but it kind of, it is like, it is not, it doesn't spell really well. But if you so
there is this, this research here, and these are already quite insane in quality, right?
Latent and diffusion. I like I love generative models. That's actually good. A painting of a
squirrel eating a burger. But now we're we're at this Dali. So these seem to be very, very much
better. Now, this model is based on diffusion, which means that it has some interesting
capabilities. If you know, diffusion models, technically, what they do is they are creating
their own data set in the sense that a diffusion model essentially iteratively refines a piece
of input in multiple steps until it reaches the output. So it's not necessarily one forward
propagation, although it depends a little bit on how you model it, you can have one layer
for each of the denoising steps, or you can have the same neural network, and you pass
the input over and over again, maybe with some indication of what time step it is. So what
a diffusion model does is it takes an image, and it just adds noise to it step by step.
So it adds a bit of noise, and it takes that it adds a bit of noise, it takes that until
it is completely garbage, like until is complete noisy. And at that point, you can reasonably
assume by some some limit theorem that this is now perfectly modeled by Gaussian noise
or whatever noise you use. And because of that, because you have a process that results
in a defined distribution, you can sort of make reverse inferences mathematically, which
leads to some training properties that you can then use to train these diffusion models.
In practice, what that means is that you take an image, you make it continuously more noisy,
that's your training set. And what you train is the reverse. So you train always to go
from one step in this denoising process to go to that little bit less noisy version of
that image. So that's what you train. So this, this direction is easy, right, to make it
more noisy. But then you train in your own network to make it always one step less noisy. And that
essentially means that you can now sample the original thing, which you know is a Gaussian. So
you can sample from that. And then you can send it through all of these stages of denoising. And
at the end, you'll end up with a really nice picture. And you can do that conditionally on
text, which is what they do here. Now, given that this is what they do, they have some interesting
capabilities. Notably, they can edit images. So they can start with an image, and then edit that
image. And they can even edit only parts of that, because they can mask some of the parts, and then
simply apply the denoising diffusion process to that. So that's, that's what they do right here.
Now, I am going to, if you don't mind, just quickly put on Twitter that I'm live streaming right
here, and maybe also on Discord. I hope this opens over here. Yes. One second. Nope. Yep.
All right.
Just writing the announcement, adding the little picture, picture.
All right. Yeah, so they can edit. So now let's see, make realistic edits to existing images
from a natural language caption. And this I think is what really will make a difference in sort of
AI art. What they say, we use diffusion models for the decoder and experiment with both auto
aggressive and diffusion models for the prior. Is my mic muted? Is my mic muted? No, I think I just
made the Twitter announcement, and then I was quiet for 10 seconds. Sorry. All right, let's see.
So you can make edits. So what they do is here's the original image. And they select a location
where they want to edit. As I said, you can edit images and you can do so in a constrained way. And
one is you can select the location and you can prompt it to add a flamingo, right? That's the natural
language prompt is to add a flamingo. And you select the location where you want to look at these. This
is insane. Look at these results. Add a flamingo to this. Okay.
It's interesting that that the flamingo style, right, matches the rest of the picture. It's not
just a flamingo. When it's inside, it's this sort of, what is this even? Like, is this a piece of
furniture or so? Like this isn't or a sort of a swimming device flamingo yet when it's when you
select the when you select it on the water as well. But when you select it outside, it's actually
real flamingo. So this is, this is quite, quite insane. It doesn't just add a flamingo, it actually
takes the entire picture into account. Let's say add a corgi again. So this is the original image on
the left. And then you select the location and the prompt is to add a corgi. Now, notice that the
style of the corgi changes depending on where it is, right? The left is, is this the, is this like
the self portrait of, is this the self portrait of Fungo? I'm not sure. But and if you add the corgi
here, it's actually real corgi, right? So it's super powerful because it looks beyond like it, it
swimming device, I'm sorry, I don't know what it's called. So yeah, you can see that this is, this is
all a real corgi. Whereas where if you go into the, if you point into the image actually matches the
style of the, of the image drawn. So this is, this is super like, this demonstrates quite a bit of
understanding, I believe, at least like integration, integration of the entire thing. This is going to
be a paid service, isn't it? I look, I don't know, because the interesting thing is they didn't even
make Dolly into a paid service until now. And because they withheld Dolly, we got all of this, you
know, VQGAN plus clip and diffusion models plus clip things, which is like, I don't think that would
have happened if they were just to release Dolly even as a paid service. Let's see what else they have
here. So we add a sofa. Yeah, so I think this is going to be quite huge for, for making any making
any any sort of visuals, like if you know, I need to make thumbnails for these videos and all. So if you
do anything like this, this is essentially, it is a, let's say a weak, weak form of Photoshop, but
that's automatic. So if you, you know, you could, you could give this to someone on Fiverr, and that's
about what it would turn out, right? You could give the picture on the left and be like, I'd like to add
a sofa in the back corner there, please do it. And that'll cost you, I don't know, 20 bucks, 50 bucks
or so on Fiverr. And this just works. So yeah, I'm excited to see what they do with it, because they
do, they do have a, like investors and so on, can take an image and create different variations of
it inspired by the original. Yeah, so this is, this is essentially different variations. I don't
know what they prompted with here. They don't say as far as I can tell. But maybe they just, they just
maybe what they do is they, they sort of back propagate it, or they noise it a bit, and then they
denoise again from the intermediate stage. So with these diffusion models, right, you can make an
image more noisy, maybe change it a little bit in some intermediate layer, and then use the diffusion
models, again, to upscale it. And maybe that's how they do these types of things. I'm not sure they
don't, they don't say right here, maybe they say in the paper. This is though fairly interesting.
You can see that there's two, two people on the left. Is this Klimt? I'm not sure. I'm not an artist
person thing. Image to text embedding to image. That could be, that could be.
But you can see that it is not, it is like the, the variations are really keeping sort of the
semantic content here. And I believe that's also one of the powerful things of the, oh, that's one of
the powerful things of training image and text together, because text by nature of being made
for humans to communicate with each other, a text is much more on the level of sort of the, I
should say this, type of information and degradation of information that is important to
humans. So training image and text together, I believe just by the nature of having the text in
there will give representations that are much closer to what humans would consider different or
relevant or something like this. This is, I mean, this, come on, this is insane. Look at that. Look
at that. Wow. I think it's taking the generated embedding and then regenerating the image from
that embed. Yeah, yes, exactly. So they, they, oh, you believe, okay, maybe they run it through the
encoder, they don't actually noise it. That's very possible. Yeah. Would this be possible with
audio? I guess, yeah, you'd have to find a relevant noising process. I'm not sure if Gaussian noise
or so is really appropriate for audio. But since this, since audio is a continuous, continuous
signal, I don't see why this wouldn't be possible with audio, especially if you train it with associated
with text, right? I mean, yeah, this is crazy. It would be really interesting to see if sort of the
representations of this stuff aligns with the representations of like real pictures of birds or
if they are way, way different. Because you can see here, these style variations, they all, they all
depict the same subject, and they are kind of in the same style, right? It'd be interesting to see how
much it can actually differentiate, differentiate that from it. It's also interesting that here, they
have these, these colored boxes in there. So do you think these might be the, is this, is this an
artifact of what is this like an artifact of the generation process? Like, is this the, the sample
auto regressively? Are they always the same? Yellow, light blue, green, red, blue. Yellow, light green,
green, red, blue. I can't tell what they are. Look at that. I mean, yeah, this is like just a level of the
level of abstraction. And, and I'm not sure what people like, like Gary Marcus make of this, this
will be super interesting to hear because I mean, look at that. So there is, it is clearly not a natural
picture, right? It's not yet it depicts a girl with headphones on the laptop, like in apparently in some
sort of garden or surrounded with plants, overlooking a city in the night sky with the moon.
There's a cat in it. And all of that is in all of these pictures, however, it's just different, right? It's a bit
look at like, you can't tell me that this thing doesn't do some degree of abstraction, or it just
brewed forces all of this by training data, but I can hardly imagine. So, you know, one second.
has learned the relationship between images and the text used to describe them to use the process called
diffusion, which starts with a pattern of random dots gradually all alters the pattern towards an image when it
recognizes specific aspects. So yeah, as for people who tuned in late diffusion models, they create training
data by making pictures more and more noisy. So they learn to reverse, reverse that noise. Let's see.
Well, you probably can't hear that sound, right? So, yeah, no? No? Okay. Let me quickly try. How about
bot painted like a Picasso? No, I can't hear it. All right. One second.
Didn't think so. Dolly too is a new AI system from open AI that can take simple text descriptions like a
koala dunking a basketball and turn them into photo realistic images that have never existed before. Dolly
too can also realistically edit and retouch photos. Based on a simple natural language description, it can
fill in or replace part of an image with AI generated imagery that blends seamlessly with the original.
It's called in painting. In January 2021, open AI introduced Dolly, a system that could generate images
from text like this avocado armchair. Dolly too takes the technology even further with higher
resolution, greater comprehension and new capability. Here's a video idea. I built a real life avocado
armchair. Let's do that. That's a future video. Promise. I should promise things like this.
Okay, promise. Like in painting, it can even start with an image as an input and create variations
with different angles and styles. Dolly was created by training a neural network on images and their
text descriptions. Through deep learning, it not only understands individual objects like koala
bears and motorcycles, but learns from relationships between objects. And when you ask Dolly for an
image of a koala bear riding a motorcycle, it knows how to create that or anything else with
a relationship to another object or action. The Dolly research has three main outcomes.
First, it can help people express themselves visually in ways they may not have been able to
before. Second, an AI generated image can tell us a lot about whether the system understands us
or is just repeating what it's been taught. Third, Dolly helps humans understand how AI systems
see and understand our world. This is a critical part of developing AI that's useful and safe.
The technology is constantly evolving. No, just look like an up meat for actual god.
If it's taught with images that are incorrectly labeled, like a plane labeled car,
and a user tries to generate a car, Dolly may create a plane. It's like talking to a person
who learned the wrong word for something. Dolly can also be limited by gaps in its training.
If you type baboon and Dolly has learned what a baboon is through images and accurate labels,
it will generate a lot of great baboons. But if you type howler monkey and it hasn't learned
what a howler monkey is, Dolly will give you its best idea of what it thinks it could be.
Like a howling monkey. What's exciting about their approach used to train Dolly is that it can
take what it learned from a variety of other labeled images and then apply it to a new image.
Given a picture of a monkey, Dolly can infer what it would look like doing something it's
never done before, like paying its taxes while wearing a funny hat. Dolly is an example of how
imaginative humans and clever systems can work together to make new things, amplifying our creative
potential. There's a lot of hedging here, open AI. A lot of hedging against people criticizing you.
That's good strategy. You just hedge ahead of time. You're just like, hey, you can't do this,
you can't do that. Howler monkeys are a species of monkey that indeed howls a lot.
Well, then it's correct, right?
Dolly too generates more realistic and accurate images with 4x greater resolution.
Okay, a painting in the style of Claude Monet of a fox sitting in a field at sunrise. Well,
that is indeed a great, great picture. Yeah, do you think these these things down here are the
sort of initial tokens for some autoregressive prior?
I'm not sure.
Okay, which we currently do not make available available in our API as part of our effort to
develop and deploy AI responsibly. We're studying Dolly's limitations and capabilities with a select
group of users. Let me be the select group of users. Oh, no, I'd be terrible. I would be immediately
canceled. They're making it harder to share without knowing where it's from. Well, you can cut just
cut off the bottom like that. That's safety mitigations we have developed include preventing
harmful generations. We've limited the ability to generate violent hate or adult images by
removing the most explicit content from the training data. Okay, you also use advanced
techniques to prevent individuals facing those of public figures. Yeah, so you can you can do
you can do some things, right? You can do this at training day to time, which is where you just
say I'll never expose my model to bad things. And then it doesn't learn bad things, which I don't
think it can work, but not particularly well, because you're always going to miss some of that
stuff like a lot like you're gonna miss most of that stuff, honestly. And therefore,
so either you limit your training data to such a degree or you're gonna miss most of the stuff,
because the number of ways that humans can come up with to do shady things and harmful things,
whatever that means, is it's absolutely insane. So I trust humans to always generate things that
you will miss. And you can do it at training time. So you can, if you have some sort of a
classifier that that detects harmful content or whatnot, you can discourage the generation
like at training time of these things. And then at inference time, you can obviously also filter
simply, if it generates something like this, you can you can filter that out. All of these things
I find suboptimal, but I do understand that if you want to release a product like that, you don't
want to necessarily have it output. It's already a stretch, right, for a company to provide a service
where they themselves don't exactly know what it outputs, which is crazy with these new APIs.
So I understand. But again, they told us GPT two was too dangerous to release. You know,
you remember that when they told us GPT two was too dangerous. Yeah, so curbing misuse or
content policy does not allow users, okay, we won't generate images if our filters identify
text prompts and image uploads them and violator policies. Yeah, okay, that that makes sense. You
just tell people like don't do it. Face development, face deployment, okay, previewing, limited number
of trusted users. Well, I hope they release it at some point, not like Dali one. There's a system
card, we don't care about system cards. Okay, that's the blog post. So for people who've joined later,
this is it's pretty insane, what you can do. So this is a text to image system, an astronaut
playing basketball with cats in space in a minimalist style.
Insane, insane, insane. Yeah. And what it can do also is edit, which is going to be huge,
right, huge for, for anyone like if this is in some sort of photoshop or or or GIMP or something
like this, new world opens for anyone who has to make stuff like a lot. I don't think people
realize, you know, how much you could you could do with you, whatever, you just you have a presentation.
And instead of having some stupid stock images, you just kind of generate something. And then
after generation, you edit it, right? The cool thing here is that you don't have to,
you don't have to restart and figure out the prompt exactly, you can just generate something and then
edit at some point, you can say, you know, add this here, remove this, change it to this and so on.
And that's just and make some variations, right, you can just say make some variations.
And that I believe is going to be quite huge for a lot of people to just use casually if
that's how they intend to release it. And I'm very, be very cool to see if there is an open
source replication of that. Given how far we've come with clip, and with these new data sets,
again, for people who haven't been here, there is this Lyon 500m data set. And
people have been training a clip models on that.
Oh, no, so Lyon 400m data set, this is the old one, there's a new 5 billion one.
And if you combine that with diffusion models, you can get like you can get pretty insane
things as well. And then if you combine that with up sampling models, then
you already have very powerful models. However, I don't know how fast they are. So
the stock images of today might be the basis of all pictures in the future. Yes, pretty much.
I think, you know, we are we are probably the generation who still makes real images and
uploads them somewhere. And before that, it was just not possible because the internet didn't
really exist in the capacity to support that. And after that, most images will just be AI generated.
So yeah, so that's where we're a special generation, the last real picture generation.
Okay, let's just skim through the paper a bit and figure out how they, you know, a bit more
about what they do. So here, Super Saiyan, Super Saiyan sentient bag of chips, art station, it's
they're using that. Okay, okay, controversial opinion, you know, given, given that they are
using essentially discoveries of the open source Twitter community, that was explicitly, explicitly
a reaction to them not releasing Dali one. Do you think it's appropriate that they are now using
the tricks that that community has figured out to make cooler images in their again proprietary
system that they don't release and probably end up selling to you? Like, arguments for and against,
right? But point of contention, point of contention. Like, you, in my opinion, you know, go do all the
selling you want. But then, you know, this is kind of the art station at the end is a bit of a,
that's kind of a thing of the, the kind of sharing open source community. And yeah, not sure.
Teddy bear on a skateboard in Times Square. This reminds me of that movie Ted, no.
Dolphin in a rest, a dolphin in an astronaut suit on Saturn.
This, this, yeah, this is 1024 by 1024. So these actually become usable, let's say in
presentations and whatnot. The 256 by 256 was pushing it a bit in terms of resolution. But
this actually becomes variations by encoding with clip and then decoding with a diffusion model.
Okay, okay. So this is again, this is combined. This is, is this, is this essentially clip based
diffusion? Okay, here it is. However, of unclip. This is okay, unclip might be the variation thing,
right? That's my friend, really? So is this the whole method? Or is this just where you make
variations of a thing? Variations between two images interpolating their clip image embedding
and then decoding with a diffusion model. We fix the decoder seed. Okay. So clip seems to play
an actual role right here, right? So they have clip here with the clip objective, right? There's
a text encoder and an image encoder, which they can give in. So this will give them some sort of an
embedding. And then that embedding goes, there's a prior. And then there is this deep diffusion
decoder. The dotted line, we depict our text to image generation process. A clip text embedding
is first fed into an auto aggressive or diffusion prior to produce an image embedding.
And then this embedding is used to condition a diffusion decoder, which produces the final image.
Note that the clip model is frozen during training of the prior and the decoder. So there seems to be
three components, right? The first component is clip itself. That's above the dotted line here.
Hello, Philip Wong. I'm Philip Wong. I'm Philip Wong. I'm Philip Wong. I'm Philip Wong. I'm Philip
Wong. Good to see you here. And they already either have clip or they train clip anew. But
they first train clip, which gives them these embeddings of text and image, right? That correspond
well to each other. And then they freeze that. And then there's two more parts to it, which are
this prior thing. And the prior thing maps a text encoding to an image encoding. So the
clip objective simply makes them align, makes these encodings align. However, the prior actually
transforms one into the other. It's interesting that they need that. And that you can't adjust
have the text encoding be the start of the diffusion process. But apparently they put this
prior here. And maybe that models that for a given piece of text, there can actually be a lot of
different images that go along with it. But since the two are probably already quite
aligned, it might require some minor modifications to it. And then there's the diffusion decoder,
right, which we saw. You can train all of this probably from the same data sets. So the clip
you can train obviously. And once you have clip, you can train the prior to simply translate the
text embedding that you get from the frozen clip encoder to the image embedding that you can get
from the frozen clip encoder. I'm not sure if that what happens. But yeah. And then you can
train the decoder likewise, simply by encoding with clip and then trying to reverse that process.
Although again, I don't know if that's actually what they do. So let's read into it.
Again, this is the I'm seeing this for the first time as well. So I'm as clueless as you are.
Does anybody have a link to the paper? You got to go to openai.com,
click on Dali, and then click on view research. A training data set consists of pairs of images
and their corresponding captions. Given an image X, let Zi and Zt be its clip image and text embeddings.
We design our generative stack to produce images from captions. So it's image from
caption, not the other way around using two components, a prior that produces clip image
embeddings conditioned on caption. So yes, so these are these here are clip image embeddings,
they're not newly trained ones. So they the prior translates the text encoding to the image encoding
again, which is special because the clip objective already aligns them during training. So that again,
that might require some minor minor modifications to them. It's interesting that they have to have it.
And a decoder that takes in text captions again, conditionally and the clip embeddings
and produces image conditioned on clip.
So this is not the same if I have this correctly as like clip guided diffusion, because in clip
guided diffusion, we take a diffusion model, and we feed that into clip, right? Let me, we take a
diffusion model and we feed that into clip. And then we try, we try to back propagate the clip loss
to the diffusion model and try to guide the diffusion model in this way. Here, it's different.
Here, we actually use the diffusion model, but we don't start from noise. We actually start from
an embedding. That's very interesting.
And I'm going to guess the, yeah, the prior translates to the clip embedding. And then we
start from that. That's very interesting. So the decoder, yada, yada, yada. Yada, yada, yada.
Okay, projecting and adding clip embeddings to the existing time step embedding.
So this is a variation of the glide model, projecting clip embeddings into four and into four
extra tokens of context that are concatenated to the sequence outputs from the text encoder.
We retained the text conditioning pathway. Yada, yada.
But find that it offers a little help. We enable classifier free guidance, which is also nice
by randomly setting clip embeddings to zero, 10% of the time and randomly dropping text caption,
50% of the time. So this is a conglomerate of sort of the recent advances by open AI and by the
community. It goes back from the decoder back into clip. It kind of sounds like they just
modified the clip embeddings and output via the image encoder zoom. Yes, exactly. They modify
the clip embeddings via the prior first and then via the diffusion model.
Okay, the prior. To generate higher resolution images, we train two diffusion upsamplers,
one to upsample images from this to that, and another one to further upsample them.
To improve robustness, we slightly corrupt the conditioning images during training using Gaussian
blur. Okay. Is this, I guess, so it'd be interesting to know if this is end to end,
these upsampling methods, because to really get good generations, right, the upsamplers
would need to also be informed of the of the prompts and so on. Otherwise, the details are
kind of generic and the big structure is, which is usually what you want, but it's not exactly what
you want prior. While the decoder can, while the decoder can invert clip image embeddings to produce
images, we need a prior model that produces zi from the captions to enable image generations from
text captions. So there is an auto regressive prior clip image embedding is converted into a
sequence of discrete codes and predicted auto regressively, or a diffusion prior,
the continuous vectors zi is directly modeled using a Gaussian diffusion model conditioned on the
caption. Okay, that's cool, I guess. For the diffusion prior, we train a decoder only
transformer with a causal attention mask embedding noise. Sorry, my reading style is not conducive
to live streaming, I guess, I'll adjust. So here is what how they train the loss. So the image embedding
is fed into this one, along with the time step and the text description, the F I'm going to guess
is a noising process or something like this. And then this is the target. So this is now a,
let's say true diffusion model, where we continuously noise what we want to achieve.
And then we try to reverse that
variations.
We have given an image, we can obtain its clip image embedding, and then use our decoder to
invert, producing a new image. To make the variations more similar, we first performed
this inversion with the decoder conditioned on the I. Okay, so it seems like clip has a big part
in this, which is a bit surprising, but not too much. Because the original Dali just used clip
as a sort of ranker at the end, whereas this one uses clip intrinsically in the process. So
it uses, if it gets a piece of text, it puts it through the clip text encoder, which is frozen.
And then it has the prior and the diffusion. Exploring the clip latent space using the unclipped
decoder. Okay, this is again the variations.
Variations of images featuring typographics attack, predicted probabilities across three labels.
Surprisingly, the decoder still recovers Granny Smith apples,
even when the predicted probability of this label is near 0%.
So what they do, they project these images into the clip latent space and then decode them again.
Can it generate text? It cannot. I mean, it can probably generate text as part of an image,
as you can see right here. So this is an apple.
And even though the classifier is what kind of classifier is this? Okay, this is clip.
So here you feed this image into clip. This was, I think, a famous experiment. You feed
this image into clip, and you ask clip which of the text classes is the most likely.
And clip says the iPod class is very likely.
And when you actually project that and then up sample it again,
you can see that it's interesting because clip sees mostly an iPod, right? That's the
class is 99.98% versus Granny Smith 0.02%. Now this is normalized, which is why I think
that this still works. So they say the surprising part right here is that it in fact retains kind
of the apple. It retains the Granny Smith apple, even though technically clip mostly sees the iPod.
But I think that's kind of because it probably still sees the apple. But when you let it classify,
it's just that much closer, oh sorry, to the iPod text label rather than to the apple.
But it probably the visual information in here, like the encoding of that is still very much
contains the apple motive. I guess this is what they want to demonstrate.
It's so funny how I can, this is the same thing, right? It's the same thing as this thing right
here in that it's kind of bad at spelling. It's so interesting that it can in fact
write text, even though it's an image producing model. It can write text in pixels,
it's really bad at spelling. Yeah, why? Nobody knows. Okay, let's spend a whole bunch of time
on this prior right here. Clip image embeddings, reconstruct them with progressively more PCA
dimensions. Okay, so we take the image on the right, I'm going to guess, and we project it
into the clip late in space. And then we only retain the top principle axis or the top principle
components, and then we recreate them again. So there is, you can investigate a little bit,
what is kind of important. So it seems, let me try to make this a bit bigger. It seems like the,
you can see the detail gets added as you go up. So this here is kind of a scene from food.
Here you have a city that's even might even be, I don't know what city that is, I'm sorry. And the
bottom one, a bunch of cows, this could be Switzerland. We know. So, yeah, as you would expect,
it first tries to model kind of the general framework, general setting, it seems to match the
colors, even a little bit, like the colors, general setting, what is it? So it's, that's really
interesting, right? That means the, the sort of highest level information that it has really
refers to kind of concepts. It does refer to color, which, and things like this, but also to
concepts, right? So there's no view of a cow here on the left, or the specific city on in the middle
right here, it's just like a city landscape, which is really interesting. Because a lot of models,
I think they would, they would much more try to model everything at once, but in less detail,
right? And if you go down with the principal components, they would try to just have color
blobs everywhere, where they're relevant. And you can see as you go up, as you add principal
components, what's also interesting is that as you add principal components, the scene can
radically change, right? So that's also quite interesting. So here we can, on the top row,
you can see that we now add, we add tomatoes into the mix. So where it went from a general,
or these gnocchi maybe, so from a general Italian kitchen setting, it goes more and more
into this direction. And it ends up in something that's fairly similar to what we're seeing, right?
And the same here, on the bottom with the cows even, you can even see that a little bit, they
have even the same positioning, but I'm not sure if I'm over interpreting this right here.
You can also see that you have the general mountain landscape down here. And then more and
more, it kind of refines to the maybe a bit more correct colors, a bit, also the background gets a
bit more accurate. Yeah, this is, it's just quite, quite cool. And as I said, this is just a lot of
evidence for the fact that these things have some sort of abstractive representations of what's
happening. Samples using different conditioning signals for the same decoder.
We passed in the first row, we pass the text caption to the decoder and pass a zero vector
for the clip embedding. In the second row, we pass both text caption and the clip text embedding
of the caption. In the third row, we pass the text and the clip image embedding created by an
autoregressive prior for the given caption. Note that this decoder is only trained to do the text
to image generation task without the clip image representation. So they have a lot of inputs,
it seems, into their decoder. So what is the first one? We pass the text caption to the decoder. So
there's no text embedding from the clip decoder or anything, there's only text caption, which it
also conditions on a group and an oil painting of a Corgi wearing a party hat,
hedgehog using a calculator, motorcycle parking parking space next to another. Yeah, so this,
as you can see, the top row, it picks up kind of on like one word or so out of the,
is this intelligent? Is it like we should make the meme with the butterfly?
As you can see, it kind of picks up on like one word or so out of the text and then it just
creates kind of a realistic image that contains that word. It doesn't have the capability of fusing
different things and so on. And I'm not sure what that is because they say it's the same encoder,
and that has been trained with various inputs, and they just pass a zero vector for the clip
embedding. So they kind of stifle the input. It's not a true comparison in the sense that
they have actually trained the decoder to only work with this type of data. So I'm not sure what
this means right here. It could mean that clip embeddings are really important. It could also
just mean that you have to actually train the decoder with this because you just stifle it
from some inputs that it doesn't have. Now if you add the text embedding from clip,
you can see that it already gets a lot more capable. So a hedgehog using a calculator.
Yeah, there is a hedgehog and there is someone, I guess, using a calculator.
Motorcycle or this wire metal rack holds several pairs of shoes and sandals.
I guess, I guess, not really. And then as soon as you add the image embedding as well.
So that's the image embedding of created by an autoregressive prior for the given caption.
Okay, so the prior will create the image embedding. So this is the full system at the
bottom except having a clip image embedding that comes from some sort of image instead of from the
prior. Yeah, it's difficult to say what this means, honestly, because they just take away
stuff. And obviously, it's not gonna go as well.
So the classifier free guidance is I don't actually have it in my head exactly what it is, but it's
like you maybe you have a classified, I don't know what they use as a classifier here, maybe clip
itself. And then you can mask some stuff and then you have the direction of where you want to go.
And then you just go more into that direction. But I'm, I don't have it on top of my head what it is.
Okay.
Glide, I've done a video on glide. So I don't remember myself too accurately what it does, but
I love how how the different papers continue this this work, right? So
dally, glide, make a scene and then unclip green train coming down the tracks.
Crazy.
Yeah, this goes into the direction of GANs, which
just have become so good at this point that the optimizations are micro,
we're still a few ways away from there. But it's getting there.
Samples from unclipping glide for the prompt, a red cube on top of blue cube.
Yeah, pretty good.
Reconstructors from the decoder for difficult binding problems, we find that the reconstructions
mix up objects and attributes in the first two examples, the model mixes up the color of two
objects. The right most example, the model is not reliably reconstruct a relative size of the two
objects. Yeah, so probably, you know, given this might also be a property, right, of the of the
text descriptions, maybe not the binding problem as much, but the relative sizes, you rarely have
a text description that describes the relative sizes of things. And therefore the encodings might
just end up with sort of what's contained in the image. And then that's what the models learn that,
okay, what's mostly important is what's contained.
A sign that says deep.
Deep, deep. This is just deep, deep, deep. Come on.
High quality photo of a dog playing in a green field next to a lake.
Unclip samples showing low levels of detail for some complex scenes.
Okay. Yeah, so
Dolly two, oh, Dolly two and unclip seem to be the same model, right? So that just,
because I was, I was wondering, I was wondering sort of, because they kept talking about unclip,
but I do believe that Dolly two is unclip. Oh, correct me if I'm wrong right here.
But here they describe it. Yes, this is unclip. That's Dolly two. This thing right here.
So, yeah, I don't, I don't know what to what to think of this. It's certainly super impressive
what it can do, right? If you if you look at, oh, there's joint wait list. Okay, okay, we'll do that.
It's certainly super impressive, like the applications of this are, I believe, quite far
reaching because this allows more and more, yeah, it decouples, I think the mechanical,
I've already said this, I think in the past, it decouples the mechanical skill of making
these things happen, like the Photoshop skill, or the painting skill, or, or Nunchuk skills,
fighting skills. It decouples the mechanical skills from, or it decouples them from the
creativity part. So usually you have like, okay, I want to do something. And then the second thing
is, okay, I have to have to actually implement it. And this thing decouples it, it kind of takes
that mechanical aspect away from you. And therefore, you're left to focus on what should happen. And I
think with these editing capabilities that it can do and the variation capabilities, this could be
very powerful if built into if built into GIMP. And then in GIMP, you can just say, okay, enter text,
a piece of text, create an image, okay, edit here, enter a piece of text, make some variations on
this thing right here, select a bunch of stuff, very, very powerful. And I guess a lot of people
would pay a lot of money for it, which might be the end goal here. All right, I hope you enjoyed
this kind of live reaction, live reaction to this type of research, I'm excited to see if this
actually ever gets out here. But yeah, let me know how you like this type of stuff. If we do live
reactions in the future, again. And yeah, I'll, I guess I'll see you around. Thank you very much for
being here. I'll stop the stream now. Bye bye.

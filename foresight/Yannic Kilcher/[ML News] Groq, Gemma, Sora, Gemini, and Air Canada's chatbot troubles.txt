Hello, everyone. Welcome to ML News. We're going to take a quick, brief, tiny look of what happened
in the last two weeks in the world of machine learning, AI, and I guess, encompassing pretty
much everything nowadays. So first, I've already mentioned this in a previous video a little bit,
but Google released Gemma. Gemma is, I guess, a variant of the name Gemini. These are open models
that are in smaller sizes than kind of the largest language models. So they are two
billion parameter models and seven billion parameter models. These outperform respective
Llama 2 models at the same sizes and even at a little bit bigger sizes. So they are quite
performant. They are available. They're openly accessible. You can use them under some limited
circumstances for commercial activity. And they do release a technical report on how they have
built these. Now, these are not the same as like Gemini 1.5 with the one million token context
length. These, I believe, they're context sizes about 8,000 tokens. So they are in architecture
quite similar to Llama models. As I said previously, this is essentially, I believe, a marketing ploy
from Google right here, releasing these models. They're already topping the leaderboards. So
all in all, very good development, I think. Although all of this has been overshadowed
last week. And if you've seen my last video, you will know by people discovering that Gemini
image generation is a bit wacky when it comes to sort of bias correction and representation
of people. So straight up refusing to generate any white people or any images of white people
and things like that. One interesting development. Again, watch that video if you haven't seen it.
But one interesting development here is that the person these product lead from Google, who
essentially came out and said, Oh, we're sorry, we were made aware of some historical inaccuracies,
we will fix those has made their ex account private, apparently, or Twitter, whatever has made
that account private. Now, it's totally conceivable that douchebags have started sort of harassing
the person or just kind of bombing spamming and so on. So that's totally fine. But it was
not a good look like it was gaslighting in the highest degree. Like, oh, yeah, let's just say
the problem is historical inaccuracies and not like the obvious problem that was barely visible
with the thing. So I just thought that was an interesting development. Again, if you want to
know more, watch that video. I also found the development around this a little bit funny. So
they later apparently they refused to just generate images of people like just German,
I think I cannot generate images of people that was their hotfix their their patch to say, well,
no images of people at all until we fix this problem here. Then saying, if you then said,
I've seen you produce images of people, it has answers. It is important to clarify that I have
never been able to generate images directly. I'm not sure it will be interesting to know what the
exact prompting behind this was and the changes being done here. Also not said that this is true,
right? This is just an LLM doing its LLM thing. But I do find it interesting, this new world
when software patches are essentially sort of prompt changes. And then the interactions
with those just make for hilarious content. All right, Grok is all the rage now. Grok is a company
that is as far as I know, spun off from Google's TPU group, if I'm informed correctly, and they have
built a card that can serve language models really, really, really quickly. So make long
novel. So this is mixed roll eight times seven B. And see, it runs at like 532 tokens a second.
So this is insane speed. This allows for new use cases to be accessible by these language models.
And very, very cool. So this is really special hardware. I'm sure there's some software tricks
and algorithm tricks. But this is special hardware. Yeah, people talking about this insane, insane
speed of Grok. Grok says they have this LPU, this language processing unit. So that's not a GPU.
It's an LPU. And the difference to something like an NVIDIA GPU is that they have a different kind
of memory. So here you can see latency and throughput. This is a benchmark from a third
party. And they had to extend their their axes here in order just to accommodate how fast and
how much throughput Grok has that fast it is. It's pretty insane. However, there is a trade off,
as I said, they use different memory than a regular GPU would use. And therefore,
that makes it such that each card only has a very small amount of memory. So you need a lot of these
cards in parallel to even serve one of these big models. Now, you can achieve massive throughput,
obviously, economies of scale kicking. But you can't just get one of those cards and then
serve a large model. And that's where people quickly realized, hey, okay, it might not be
the wonder weapon here. It is very cool. But each chip only has about 200 megabytes of SRAM.
And therefore, you would need, I don't know, hundreds of cards in order just to serve this
mixed trial model that we've seen before. Again, with the higher throughput, it might be totally
worth it if you're a data center owner, but you see the graph on the top right here. That's Grok.
That's pretty insane. People calculate that you need about 320 of these cards or two full racks
to just serve a single llama 70B. And if you calculate the cost of these cards, then that'd
be about 10 million US dollars. It's not the the end all's gonna solve everything. But it is
definitely, definitely very cool development in order to push language model inference ahead.
At the same time, Nvidia unveils EOS tech power up, right? This is essentially pulling together a
bunch of their DGX systems to create a super duper computer. So 576 DGX H 100 systems wired
together into one computer. Each of these DGX system has eight H 100s, making for a whopping
4608 H 100 GPUs. Note each of these puppies will cost you, I don't know what they cost right now,
like 20k or something like this or north of that. This is massive. It's ranked number nine in top
500 supercomputers of the world with a staggering, staggering 18.4 Exaflops FPA performance. This
website here, I found pretty cool GPU list dot AI. It's by Andromeda AI. And it's essentially
craigslist, but for GPUs, people rent out their GPU capacity. It's also as shady as craigslist,
right? So it's just a listing. And it just says, well, you'll get bare metal access. And sometimes
it says, okay, you get SSH access to it or something like this. But essentially just allows you to
contact these people and then make out a deal of how you're going to use these GPUs. This seems
fairly large. So the common posting here, actually, there's a lot of H 100s here going
around. I'm not sure where people get these from. But sometimes, oh, there's 849. Okay,
this may be more common. So Canada, Ethernet, one H 100 available, it's an endubuntu VM,
and you get minimum one week. So if you want some GPUs, and you don't have super confidential
data, because you are going to use other people's hardware, this might be a good option to find
some some good deals. Wired has an interview with them as husband is on how far you get with scale,
apparently, I guess just on the future of AI. And if you read the interview, it's kind of a mix
between Yeah, scale is great, we can do scale scale is awesome, Gemini is awesome, these models
are awesome. And but also scale only gets you so far, there needs to be something else. So they
then it says my belief is to get to AGI, you're going to need probably several more innovations
as well as the maximum scale, there's no light up in the scaling, we're not seeing an asymptote,
yada, yada, there's still gains to be made. So my view is, you've got to push the existing
techniques to see how far they go, but you're not going to get new capabilities like planning,
or tool use or agent like behavior, just by scaling techniques, it's not magically going to
happen. It's very interesting, because I think that is a current contention, it's very easy to say,
oh, to get to AGI, you need something else, because, first of all, AGI isn't the defined term, and
something else isn't the defined term. So you can redefine these two terms as you wish, and then
you can always find something that's still wrong or in a way in which you're still correct. If you
do that for long enough, your name will become Gary Marcus. But other than that, these are fairly
more concise predictions saying, okay, you're not going to get planning or tool use or agent like
behavior. They're not super defined, but they are. And we're already seeing tool use, for example,
being built into these large language models, and get better with scaling. So it will be very
interesting to see whether Demis turns out to be ultimately correct on his predictions, or whether
one or the other of these things will be available just by scaling language model and kind of
training them on tool use data and so on. Tom's hardware writes legendary chip architect Jim Keller
responds to Sam Alton's plan to raise $7 trillion to make AI chip saying, I can do it for less than
$1 trillion. We've gone off the rails. So Jim Keller, apparently legendary CPU developer, now working
at the company that makes chips themselves, they claims that he could do it for a lot less. Yes,
I guess. I don't know, as soon as you go into like money that's way beyond the current total
market value of chips, I feel many claims can be made. It will be interesting going forward to see
kind of who takes the lead in chip development, how that's going to be playing out. In any case,
I'm not sure if bickering about $1 trillion, $2 trillion, $7 trillion is going to make that big
of a difference. From one legendary person to another legendary person and legendary here
spelled with a capital L, AI may destroy humankind in just two years, experts says. Of course,
Elisir Yudkowski saying, if you put me to a wall and forced me to put probabilities on things,
I have a sense that our current timeline looks more like five years than 50 years,
could be two years, could be 10, well, could be anything. Like with the trillions,
it is absolutely useless to make these speculations and then saying things about terminator like
apocalypse and matrix hellscape. The difficulty is people do not realize we have a shred of a chance
that humanity survives. Oh, yes, of course, of course. Yudkowski has, I think, retracted statements
on bombing data centers. Like that would be useful. In any case, read this as you would
read like a comic book for entertainment. Yeah, I feel like that it's at least makes you giggle.
Otherwise, this serves no purpose at all. Sora continues to dominate headlines,
video generation model by open AI, we've talked a little bit about this in the last news episode,
this can create clips, single shot clips up to one minute, I believe. And they look pretty,
pretty awesome, I have to say. And more and more kind of examples come out of Sora creating pictures,
creating clips, open AI marketing department in full gear. No, you don't have access to this model
yet. A select few have access to this model. Not you. You are just a pleb. You're not the
chosen person. So Marvel at other people using the cool thing and open AI's marketing department
having tight control over exactly which things go out to the public and which things don't.
What is interesting is here's an example of Sora scaling with compute. So essentially,
saying the more compute they throw into one of these generations, the better,
better quote unquote, the more realistic, I guess it gets. So base compute for x. However,
like they've also completely stopped to give us any sense of the scale, the absolute scale of things.
So for now, it's just like base compute, however much that is, and then for x compute and then 16x
compute. Yeah, in any case, what we can infer from that is that there is an iterative process very
probably to determine one of these samples. So it's not like single forward pass of anything,
but iterative iterative process like you would be used to from diffusion doing many, many steps
across a span of time to refine and refine and refine the output. What's also pretty cool are
demonstrations of changing things like this being a base video and not only can Sora generate things,
but also kind of generate things according to some input, like some input video. So in case here,
people changing the surroundings of the car or the car itself, like the vibe of the video and so on,
while keeping the general motion, I guess, and the general concept clear. So I think that's pretty
cool. Make it go underwater. Yeah, why not? Look at that. Or that nice rainbow road. Keep the video
the same, but make it winter animation style charcoal drawing. Yeah, make sure to be black and
white. Not exactly, but close. Maybe it's one of those things where it's actually black and white,
but your eyes trick you, but I think I'm seeing color. Wait.
Yeah, no, this is definitely color. Actually, not so sure now. Okay, no, this is definitely red.
This is definitely red. The back lights. Yeah, it's not fully black and white.
Cyberpunk medieval. Very nice. They had drones following carts in medieval times. Also the
horse legs, they look, yeah, why not? Dinosaurs, pixel art. So many cool things about Sora keep
coming out. And also many cool things about Gemini 1.5 Pro keep coming out, especially obviously
the insanely large context size of Gemini 1.5 Pro people feeding very long things inside of it
and see whether it can handle the long context and entire code base and then instructing it to code
something based on top of that. I think this is probably going to be one of the best applications
for something like this. If you have very long, yeah, something like a code base or a reference
documentation or something like this, like the important parts of that would fit into a million
tokens and being able to sort of cross reference things inside of that and generate based on that
is probably a very good use case. I know they can retrieve well across the one million tokens kind
of like point to individual things if they need to retrieve them, but it will still be interesting
to research how performant it actually is when you put more and more and more stuff into that
context. My personal estimate would still be that putting less things into the context is
more beneficial will make stuff more accurate. Or what I can also imagine is that they trained it in
such a way that they could have achieved better performance on small context compared to a large
context, but they traded it off to have sort of equally performing but worse performance across
the entire context length. Not yet clear, but it will be interesting to see. This pretty cool
feeding an entire short movie into this. So what Gemini will do is it will take the movie
split it into frames and then essentially use the frames as tokens or tokenize the frames. And
you can fit pretty long. You can see here 44 minutes and seven seconds video. You can fit that
into the context size of Gemini 1.5 pro because it can also consume images. And Matthew here says,
when straight from full movie to a summary in seconds, no transcription, no intermediate steps,
just visual tokens to summary. Now I've seen other people have pointed out that
the summaries it makes aren't always super duper accurate or well done, but it's still
pretty impressive. And it speaks to what I said before, right? The main question is going to be
what are the dynamics and the characteristics of performance across this entire context window.
And currently UC Berkeley coming out with a paper that's titled world model on million length
video and language with ring attention. This is an actual research paper that is very concurrent,
as I said, to Gemini 1.5 pro doing retrieval experiments across very long context with what's
called ring attention. If you're interested, we can make an entire video on ring attention that is
in the makings. So keep looking for that. But it's a cool new technique. It is going to be some
sort of approximation to attention. Like, it's not the fact that we can now scale the classic
transformers across this huge one million token context size. So it's a trick, people have come
up with many, many different tricks of sort of doing long attention. And this one seems quite
promising. Phil Wang, also known as lucid rains already has an implementation on ring attention
up, even though the paper is super duper new. Yeah, what's interesting to see in the read me
of this repository is Phil saying, I will be running out of sponsorship early next month.
If you'd like to see that this project gets completed, sponsor me or I will be leaving
the open source scene for employment. So just wanted to bring this to people's attention.
By that, I mainly mean people like companies. CTV news, Vancouver writes Air Canada's chat
bot gave BC man the wrong information. Now the airline has to pay for the mistake. It's apparently
a person went on the chat bot for Air Canada. That's kind of powered by a large language model.
And they went there looking for a very specific questions about it's called bereavement rates.
This these are reduced fares provided in the event someone needs to travel to due to the death of an
immediate family member. So the chat bot, this person has lost family member, I wanted to do
air travel due to that and inquired about, you know, cheaper rights or specific fares, specific
prices for this situation. And the chat bot gave them wrong information saying that he could claim
those even after the fact. And when he wanted to do that, the customer support people said,
no, that's not possible. You're not getting your money. Now the question is, who is responsible?
And courts say that in fact, Air Canada is responsible for the things their chat bot said
and has to actually comply with what the chat bot promised Air Canada try to try to push the
responsibility. Air Canada suggests that the chat bot is a separate legal entity that is responsible
for its own actions. What? What? Oh, no, the piece of software that we actively deploy it on our
website is a separate legal entity. Yeah, no, no, I get that as a lawyer, you will have to argue.
And in this case, it was probably like the last remaining thing that you could even conceivably
argue, but it's so ridiculous. No way, no way. If you deploy a piece of software, you are responsible
for what that software does. Not if you program it, not if you make the open source library that's
then part of it. If you deploy it, and it interacts with your customers, and then it promises stuff
to your customers, then you are responsible. And that's the same with every other piece of software
as well. There's absolutely no difference of whether this is an LLM chat bot or anything
else that executes code. It's always the same. And it's pretty funny that Air Canada trying to
weasel their way out of this. So no, Air Canada must honor refund policy invented by the airlines
chat bot. So obviously, companies are trying to alleviate costs on their customer success operations.
In this case, they may want to calculate if they don't incur more costs due to stuff
that's invented by it could be a strategy, like I think the damages here to be paid were about
$600. And then I think about $20 in tax, it says it somewhere. In addition, the airline was ordered
to pay 36.14 in pre judgment interest interest not tax and $125 in fees and probably the lawyers
grabbed $10, $20, $50 out of the people here. So all in all, who succeeded the lawyers? Lawyers
should be fans of LLMs, like the amount of litigation and the amount of contracting. And so
they have to do just because people want to use or have used or mistakenly used LLMs is going to
be staggering, staggering. In any case, this was more about the principle, I guess, than about
the money. But it could be a thought, you know, as like, just let an LLM do your customer success.
And if they promise something that doesn't exist, you just pay it, it'll be like 600 bucks in this
case. Maybe that's worth it. Maybe the savings and not having to hire more people is totally worth
it for these companies. I don't know, it'll be interesting to think of a future. I think right
now, everyone's trying to guard rail everything because they feel, okay, our customer success
operation should continue to be as is, like there is a completely defined things, okay, here are the
things we pay here are the things we don't pay and so on. And, you know, must adhere to that cannot
promise anything else and so on. What if the mentality around that changes, it will just be
like, okay, here's a set of guidelines, we know the thing is going to hallucinate every now and
then and when it does, we'll just sort of take it into account, like, I feel there are still laws
against customers abusing that like if I were to go to Air Canada chatbot and kind of like prompt
tack it into giving me stuff, I'm pretty sure a court would side with Air Canada in that's
essentially kind of me emotionally abusing a customer support rep until they promise like
give me what I want. But other than that, could be totally viable future and a fun future. If
these things are not so strict, cream car tweeting out xing out, I'm not sure what's
finally happened to peer review journal article with what appear to be nonsensical
AI generated images. So these this has become known as giant rat balls.
The pictures they look from afar like they could be in like a biology journal, but they make no
sense, right? The writings mostly rat. Yeah, we see that. So this this article has pictures
that were generated by mid journey. Now, it is a bit interesting. It's a bit more interesting
than that. Scientists aghast at bizarre AI right with huge genitals review review article. This is
a fairly reputable journal where this is probably this is not just like a pay 5000 bucks and you
will get published journal. This is a fairly reputable journal. Here is another another picture.
You see it rarely makes sense if you actually look closely. Second, the images are created by
mid journey. And the authors acknowledge this in the paper. So the authors say the images are
generated by mid journey. Third, there were two reviewers. And one of the reviewers actually
brought this up and currently also a reviewer, I'm not sure if if it's the same reviewer or
a different reviewer said, I was only looking at the scientific content of the work, right,
and reviewed it based on that. And we have also statement from the review reviewer saying like
they did raise concerns about the images. So they're saying the journal says an investigation
is currently being conducted. So this article in vice here details our investigation revealed that
one of the reviewers raised valid concerns about the figures and requested author revisions.
Even the reviewer saying, okay, the figures need to be revised, the authors failed to respond to
these requests, we are investigating higher processes failed to act on the lack of author
compliance with the reviewers requirements. So it's a bit more tricky than just a bunch of
researchers tried to get a fake paper through in the reviewers didn't notice. It seems that the
ultimate person who I guess multiple people here always contribute to the things. But ultimately,
it was the editors who didn't make sure that the authors actually changed the things that the reviewers
were asking them to change that made it ultimately go through and being printed as a paper, I guess
mistakes like this happen. It's also probably very common to just kind of assume the authors will
concur with what the reviewers request to change. I don't know if they've even said, yeah, okay,
we'll change it or something like this. But it is an interesting story. And the meme of the giant
rat balls will forever live on in our hearts. Andre Carpathi said they he left open AI assuring
that's not a result of drama or anything like this. It's just a change in scenery saying that
the last year in open AI was really great. The team is really strong. The people are wonderful.
The roadmap is exciting. And we all have a lot to look forward to. He says my media plans is to
work on my personal project and see what happens. And immediately following this up with a video
explanation two hours on tokenizers enlightening the bizarre world of why reversing strings with
LLMs is really, really difficult and why different languages are give you different results and so
on. So if you want to explore so far, I believe a bit underexplored aspect of large language
models definitely look into Andre's tutorial on tokenizers. Very cool. And as Andre is very,
very clear explanations, you will know a lot more after this. Nature writes what the EU's tough AI law
means for research and chat GPT. The EU AI Act is the world's first major legislation on artificial
intelligence and strictly regulates general purpose models. As you know, the EU AI Act has
been in development for a number of years now and is now finally coming into effect rolling out and
so on. It's been changed quite a bit over the years. You know, even myself, I'm not entirely sure
what's in the current version and how much it's still going to change. But the approach is to
categorize broadly applications into risk categories and then to tie what you have to do
according to the risk category. The most risky things are called unacceptable risk and those are
just banned, like not allowed to do. For example, those that use biometric data to infer sensitive
characteristics such as people's sexual orientation. So is this banned? Also, what's hilarious?
There's like a limit for when you have to do something and that is 10 to the 25
flops, completely arbitrary number that is going to be meaningless, probably even already before
the AI Act has really rolled out. Finally, I could not make up worse advice for these policy makers
if I wanted to. It's like, okay, let's pick a completely arbitrary number and say here, here
is where we draw like what I believe that you can entirely transparently see the lobbyists being
like, okay, what can we do that our competitors can't do? And let's like draw a nice line between
the two how it is the next three years. And we don't we don't care about any after that. All
right, unacceptable risk. Do you realize that a basic linear regression would fall under this?
The EU effectively now bans drawing a straight line across a few data points. If those data points
happen to coincide with the data categories that are collected here, like this is the level of
dumbness these kinds of laws come to. Yes, I know, I know, I am making sort of pulling it to the
extreme here. I know this is meant for super duper transformers informing these things and then
sitting in automated systems that make decisions about people's lives and so on. I see what the
fear is, but I doubt whether what they're trying to do what they're intending to do matches with
what the effect of this is going to be. And I still believe the effect is going to just be that a
more monopolization of bigger companies making it harder for newcomers making it harder to enter
this market and giving the governments more control over things, which they will probably
not do good things with just an opinion. Go here for AI launches. Aya Aya is an open source massively
multilingual large language model and the data set built over 101 different languages all across
the world. And this is one of the largest data set of instruction data that's around. As I said,
it's a data set and a large language model all at once. The data set is available. The model is
open access, whatever that means right now. I guess you can download the model because there's
a button that says download the model of your press found this on Reddit. And I found this to be
really interesting regional prompting. This is a UI for the technique called Gilgan. And I've linked
to the repository. Very cool to use and very exciting, exciting new things that are possible.
Aria everyday activities is a data set again released by Meta that depicts, as you can see,
everyday activities. So this has first person view data, location data and so on. Meta is
actually pushing the metaverse and data sets around that augmented reality and so on. So
collecting a lot of data, they have, as you can see right here, rolling shutter RGB field of 110
degree field of view camera 150 degree field of view camera for slam and hand tracking infrared
illumination, barometer magnetometer environmental sensors, spatial microphones and so on and then
annotated data per frame eye tracking, 3d trajectories, these data sets, they collect them to
be quite universal. So maybe don't want to use all of them at the same time, but they enable
a lot of different applications, which is very, very cool. Stability announces stable diffusion
three a text to image model using a diffusion transformer architecture for greatly improved
performance in multi subject prompts, image quality and spelling abilities, not releasing
anything. There is a wait list for early preview. They say this is for gathering insights in
improve its performance and safety ahead of the open release. We've also come to known from
stability that open release is going to mean that you can use the model for researchy stuff,
but if you want to use it for anything commercial, you have to give them a bit of money. You can
see a few examples here. Nice Apple go big or go home. The astronauts riding on things has become
a bit of a meme. I mean, the quality is getting absolutely insane with these text to image models.
Alexa Gordage releasing you go LLM. This is a large language model, seven billion parameter
large language model for Balkan languages, Serbian, Bosnian and Croatian languages.
And you can find that on hugging face right now. Very, very cool. Open math instruct by NVIDIA
is a math instruction data set that you can freely use. Actually freely use might be an
overstatement. There is a NVIDIA specific license to it. I'm not a lawyer. I'm not going to tell
you what this means. I personally think no legal advice. You can use it freely. Again, not legal
advice. This article from interesting engineering I found very cool. It's a system that identifies
drug combo problems. So interactions between different drugs, specifically as they are
transmitting the barrier in your gut. The problem is any researching any drug and what it does is
already super expensive. But then obviously every drug you add to the regiment of available drugs
means it could have interactions with all the other drugs that exist. This system uses a combination
of machine learning and actual models of transmissions models of receptor behavior in the
gut to predict interactions between different drugs in terms of their uptake in the gut.
So I think that's very cool. I think the pushing into this direction, we already saw this with
various deep mind models of having some sort of expert modeling, like some sort of actual model
that is domain informed, that is expert informed in a scientific domain, combine that with machine
learning and use these two together in order to draw conclusions is probably I want to say the next
frontier I feel like the frontier of will just throw a lot of data at stuff and it will give us
results. I think the low hanging fruit in that has probably been taken already. And now it's really
about the combination of expertise and machine learning that is going to push ahead. So very,
very cool, very excellent developments. Bloomberg writes Reddit signs AI content licensing deal
ahead of IPO. That being said, this is all person close to the matter said large unnamed AI company,
a lot of dollars involved about $60 million on an annualized basis and yada, yada, yada. So this is
all I guess they call it hearsay, right? But this is the chatter right now that Reddit obviously
recently read it has made headlines by sort of tapping all of their API access and so on,
not being really open anymore in to outside developers in a clear move to protect their
IP, which is users posting on Reddit, so that other you can't via API, go and grab all that data.
And now the second move is that they themselves are going to make use of that data by licensing
it out to other companies. Again, this is all just someone said someone familiar with the matter,
yada, yada, yada. But still Reddit realizes they sit on a treasure trove of information that's
already evident by people just Googling how to XYZ and then just add Reddit to their Google search
query because they know usually they get okay answers on Reddit, which has had the counter
effect that now marketing representatives and so on will try to go and sort of poison Reddit threads
by giving you know, nice looking answers that ultimately link back to their product. Interesting
dynamics. In any case, Reddit data may become a staple of one of the big AI companies. So we'll
have all kinds of AI redditors around. Isn't that a great future? New Atlas writes the seeing
iDog v2.0 is shaping up as a game changer. This goes into the details of strapping kind of assistive
technologies on top of one of these four legged robots in order to help blind and seeing visually
impaired people to move around safely, safely passage from A to B and so on. The article discusses
that the main limitation here is actually the availability of service dogs in general like
guide dogs. There are way too few guide dogs around for all the visually impaired people.
They are expensive. They are rare. They need to be trained and so on. In this case, these robots
obviously don't. So yeah, you can say they take away the jobs of good, you know, hardworking guide
dogs, which I guess, but from all I can see here, actual guide dogs are still preferred to robot
dogs. It's just that there aren't nearly enough of them. So these robot dogs, they are shaping up
to become very capable and can help with a lot of things. So very cool developments. This paper
I found really cool. OIS copilot towards generalist computer agents with self-improvement using
agent like behavior, but interacting with your operating system. So it can do some stuff on
your computer by you just prompting it to do it, opening applications, interacting with applications,
even doing kind of multi step things. I think this is one of the ways we're going to interact more
with computers in the future. Maybe I don't think like the keyboard and programming, you know,
using text and so on will ever go away, but probably kind of web browsing or simple things like this
could be automated like this. I find this to be a lot more understandable than just voice prompts,
like just being like Alex or book a flight to XYZ, right? Like it's like, I find voice and sound to
be kind of a wonky interface for that. But if at the same time, someone shows me, look, I'm now
going to this website, I'm going to do this and that, I feel that is a much more viable interface
here. But then again, you could just click it yourself. So probably I find the GitHub copilot
to be an extremely good mode of interacting with an LLM. So if we transform, transport that to the
world of here, it would be that largely I operate the computer, but I could tab complete like a lot
of things. So like if there's a form to be filled out, yes, I know browsers will support me already,
but I could maybe just kind of a lot less tab complete. Or if there is, I don't know, some
standard interactions and websites just kind of tab complete that away. I think that mode of
interacting with computers, I'm looking forward to that a lot. I'm not looking forward to like
single prompt and then will magically go and do something for me. I don't think that's going to
be a thing of the near future. And I don't think you would be comfortable with a system like that.
Business Insider writes new report sheds light on Apple's upcoming AI features that will rival
Microsoft's copilot. Now further down, they say Microsoft's GitHub copilot writing code. So it's
not like the Microsoft Windows copilot or the Microsoft 365 copilot. There are too many copilates
nowadays. It is the apparently the GitHub copilot that Apple targets inside of its Xcode environment.
So if you write Swift apps, if you write iPod and iPhone apps, and maybe even what's called Mac OS
apps, that might be really cool to have that available. I do feel GitHub copilot does its job
quite well for what it does for everything else I've never programmed Swift. So I can't say that.
TechCrunch writes anthropic takes steps to prevent election misinformation. Yeah, sure.
It's they're making a bit of PR. I feel like they're they're using the opportunity of the
election to be like, Oh, we have guardrails, we have prompt shield, which I guess prompt shield
cool the technologies which relies on a combination of AI detection models and rules. Sure, you have
a regex, and you have some prompt that says if the user asks for voting information, go to this
site. I guess it's a good if I were a company, I would try to use that as well. And lastly,
AI comes to the world of beauty as eyelash robot uses artificial intelligence to place fake lashes.
This details how this robot can place fake eyelashes in a more precise way than humans
could. And as far as I can see, it's also a bit faster or cheaper or something like this. This
is a purely mechanical task that so far humans did by hand. And now a robot can do artificial
intelligence is a bit of an overstatement, like they use computer vision to detect where the
eyelids and the corners of the eyes and so on are, which is really cool. But then there is Oh,
there is the for and there's the against and the against is Oh, no, we have to be very careful
about this. So there's potential risks. The device's proximity to sensitive area could
raise concerns about the risk of eye infections or allergic reactions to the materials used in
the lash extensions. I guess they just, they had someone, someone just say, well, say anything
bad about this, like anything generic that's bad about this. And this person was like, I guess
you could be allergic to the materials. They're like, Oh, yes, there is someone. There's also
potential risks. I'm not, I'm not sure. I'm not sure I'm buying that you decide for yourself. I
feel having a machine do a purely mechanical task is fine. It's not going to steal a lot of jobs,
I guess. All good. I just found it funny that the news article must have this structure. But here
is something new that technology can do. But there is also risks. With that being said,
there's also risk that this video gets too long. And with that, I'll finish it. Thank you for watching.
Bye, bye.

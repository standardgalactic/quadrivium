WEBVTT

00:00.000 --> 00:10.680
So our closing event is a special lecture given by Mike Jordan who almost I think does

00:10.680 --> 00:13.960
not need an introduction but I'll still give one.

00:13.960 --> 00:20.080
So he's a Pei Hong Chen Distinguished Professor at UC Berkeley and Mike has been a leader

00:20.080 --> 00:26.880
in the computational and mathematical study of learning for a long time and he's achieved

00:26.880 --> 00:33.160
such prominence that I think in 2016 he's been named one of the most influential computer

00:33.160 --> 00:39.360
scientists on Earth and so I'm just looking at his computer right now or the most influential

00:39.360 --> 00:41.160
computer scientist.

00:41.160 --> 00:47.280
He's been recognized for his work by many contributions, many awards as a member of

00:47.280 --> 00:53.640
the National Academy of Science, of the National Academy of Engineering, of the Royal Society.

00:53.640 --> 00:59.680
He was the inaugural winner of the World Laureates Award last year in 2022 and he receives

00:59.680 --> 01:03.520
a John von Neumann Medal from IEEE in 2020.

01:03.520 --> 01:08.560
If I were to list all these awards I think we would be here for a long, long time.

01:08.560 --> 01:12.840
But I would say that personally there are three things that amaze me about Mike Jordan.

01:12.840 --> 01:14.960
The first is his range of interest.

01:14.960 --> 01:20.160
They span an enormous array of fields that goes all the way from computer scientists

01:20.160 --> 01:27.880
to statistics to control theory, signal processing, mathematics, information theory, cognitive

01:27.880 --> 01:33.160
science and now economics and maybe we'll hear about a bit about this today.

01:33.160 --> 01:39.560
And I think the range and the breadth of his interest is just very unique.

01:39.560 --> 01:42.240
There's another thing that is unique about Mike.

01:42.240 --> 01:47.480
He's his track record of training the next generation of students.

01:47.480 --> 01:53.720
I think if you look at, I think he has a very impressive CV, but I think if you look at

01:53.720 --> 01:59.480
the CV the thing that impresses me the most is the name of his grad students because your

01:59.480 --> 02:07.440
students, and we have one right behind you, Lee-Wa, we have another one right there so

02:07.440 --> 02:13.640
we have already a lot of various, they are actually, they make up the who's who in machine

02:13.640 --> 02:14.640
learning today.

02:14.640 --> 02:22.280
And so if you open who's who in machine learning and ask the question, is this one of Mike's

02:22.280 --> 02:23.280
student?

02:23.280 --> 02:29.600
The likelihood of a yes is very high and so, and I think he's done this by fostering an

02:29.600 --> 02:35.720
environment of inclusivity and curiosity that has really moved the entire field.

02:35.720 --> 02:39.520
And so I think the whole field is grateful for this.

02:39.520 --> 02:42.640
The third observation is more personal.

02:42.640 --> 02:47.040
I think Mike's age is public information, at least if Wikipedia is correct.

02:47.040 --> 02:54.360
So you've been at the top of your game for a long, long time and this is really unique.

02:54.360 --> 03:00.000
And the way I think about Mike in the evening is I think it's a kind of the Roger Federer

03:00.000 --> 03:06.920
of science, someone who has been dominating the circuit for a very, very long time with

03:06.920 --> 03:10.040
no signs of slowing down.

03:10.040 --> 03:13.120
Mike, welcome to Stanford.

03:13.120 --> 03:21.880
All right, thank you, that was perhaps the most fun introduction I've had ever.

03:21.880 --> 03:25.640
When I next time introduce Emmanuel, which will happen, I'm sure I'm gonna have to think

03:25.640 --> 03:27.720
about the right metaphor.

03:27.720 --> 03:30.720
Maybe someone has an idea helped me out there.

03:30.720 --> 03:35.080
I gotta get my mind off that because it's a fun thing to think about.

03:35.080 --> 03:36.240
So I'm a pleasure to be here.

03:36.240 --> 03:41.720
I am a data scientist and in fact this kind of quote that I was like an influential computer

03:41.720 --> 03:45.680
scientist is kind of funny to me because I'm trained as a control theory statistician and

03:45.680 --> 03:47.280
I was never a computer scientist.

03:47.280 --> 03:50.840
I embrace it because of the entrepreneurial spirit in computer science and just let's

03:50.840 --> 03:51.840
try everything.

03:51.840 --> 03:52.840
I love that.

03:52.840 --> 03:56.160
And I found less of that in control theory and statistics, so that's great.

03:56.160 --> 03:57.680
But intellectually I'm a data scientist.

03:57.680 --> 04:02.400
I really want to think about how data and inference can inform real world decision making and

04:02.400 --> 04:03.680
I think that's where it's at.

04:03.680 --> 04:08.560
I think that it's in the first time in my career that sort of all of campus agrees.

04:08.560 --> 04:12.440
It's sort of a, it's just not the truth.

04:12.440 --> 04:15.920
Technologist inside of a computer scientist or statistician wasn't enough.

04:15.920 --> 04:17.760
This is much more fun.

04:17.760 --> 04:21.720
And indeed it's economics, which is the most thrilling to me right now, the connections

04:21.720 --> 04:22.720
to economics.

04:22.720 --> 04:26.000
That's really what I want to mostly convey tonight is why I think that's thrilling

04:26.000 --> 04:28.280
and important and so on.

04:28.280 --> 04:31.760
The elephant in the room is this thing called AI and I've never thought of myself as an

04:31.760 --> 04:32.760
AI researcher.

04:32.760 --> 04:36.440
I've never aspired to the Frankenstein-like thing of create some thing.

04:36.440 --> 04:37.720
And I kind of want to say why.

04:37.720 --> 04:41.880
Not only didn't aspire to it, I don't think it's right, but it is what everyone's talking

04:41.880 --> 04:42.880
about.

04:42.880 --> 04:44.320
So I want to say a few things about it.

04:44.320 --> 04:50.200
So first of all, the thing that triggered all of this was back propagation, gradient

04:50.200 --> 04:53.200
descent in layered neural networks.

04:53.200 --> 04:58.240
There's all these other ideas along the way, unsupervised this and that, which didn't really

04:58.240 --> 05:01.280
quite pan out, but back propagation had this huge impact.

05:01.280 --> 05:07.160
Dave Rummelhardt was my advisor at UCSD and he developed that.

05:07.160 --> 05:11.000
It's just gradient descent, so you can't say he invented it, but he invented the idea

05:11.000 --> 05:13.600
of doing it in layered neural networks and applying it to all kinds of problems.

05:13.600 --> 05:15.200
And he took about a year to do that.

05:15.200 --> 05:17.760
It was not trivial for him.

05:17.760 --> 05:19.200
He was not trying to be an AI person.

05:19.200 --> 05:22.320
He was just trying to understand learning.

05:22.320 --> 05:28.920
I think he'd be somewhat shocked that suddenly that becomes AI to this era.

05:28.920 --> 05:34.760
So what I think is happening really is not that we have a new technology, this new brilliant

05:34.760 --> 05:37.560
idea, AI, and then we start applying it everywhere.

05:37.560 --> 05:40.160
I don't think that's the right way to think about it.

05:40.160 --> 05:45.680
And so I think you need to go back in history a little bit and think about engineering fields

05:45.680 --> 05:46.680
that have emerged.

05:46.680 --> 05:50.720
So like in the 40s and 50s, chemical engineering became a thing.

05:50.720 --> 05:53.920
The name was actually used, I kind of looked into it a little bit more.

05:53.920 --> 05:59.720
Already by 1890, I think there was a department of chemical engineering at MIT.

05:59.720 --> 06:05.520
But chemical engineering really was a very simple chemistry, kind of done its bigger

06:05.520 --> 06:07.200
scale than before.

06:07.200 --> 06:12.680
But polymers and all the kind of things that have triggered the revolution that we're all

06:12.680 --> 06:15.600
living in, that happened in the 40s and 50s.

06:15.600 --> 06:19.560
Before that, there was quantum chemistry and there was fluid dynamics and there was thermodynamics

06:19.560 --> 06:24.200
and also there was a lot of deep understanding of the phenomenon.

06:24.200 --> 06:28.960
And people then were able to start to envisage, what if I take the laboratory test tube experiment

06:28.960 --> 06:33.040
of how you put molecules together, which I do understand because of the quantum chemistry,

06:33.040 --> 06:37.600
and I do that at a huge scale in a field somewhere, will that work?

06:37.600 --> 06:39.040
And of course, it didn't really work.

06:39.040 --> 06:42.240
Often those things would explode, often they just wouldn't deliver a product and so on

06:42.240 --> 06:43.240
so forth.

06:43.240 --> 06:46.560
But over a 20-year period in the 40s and 50s, it kind of started to get worked out.

06:46.560 --> 06:52.000
And an engineering field emerged that had huge impact on all of our lives.

06:52.000 --> 06:54.920
Electrical engineering, I know less about the history, but obviously there was Maxwell's

06:54.920 --> 06:58.720
equations before there was electrical engineering, so it was a full understanding of the phenomenon

06:58.720 --> 06:59.720
at some level.

06:59.720 --> 07:04.480
But it wasn't clear how to bring electricity into homes, how to make it safe, how to do

07:04.480 --> 07:07.120
communication on top of the waves, and so on and so forth.

07:07.120 --> 07:11.240
So a whole field emerged, which we now call electrical engineering, that did all that

07:11.240 --> 07:12.680
in the early part of the past century.

07:12.680 --> 07:15.640
It took a couple of decades again.

07:15.640 --> 07:17.000
So I think that's what's happening right now.

07:17.000 --> 07:19.480
We have a new engineering field emerging.

07:19.480 --> 07:26.440
I wouldn't call it AI, but it's a field that's based on flows of data, networks, flows, inferential

07:26.440 --> 07:32.440
ideas, large-scale decision-making, cooperative endeavor, building transportation systems,

07:32.440 --> 07:38.800
commerce systems, healthcare systems, it's all part of this engineering field.

07:38.800 --> 07:40.000
That's really what's happening.

07:40.000 --> 07:44.840
And so it's the first engineering field that has got, as its objects of study, not just

07:44.840 --> 07:51.480
bits and information and atoms and laws of physics, it has humans involved, critically.

07:51.480 --> 07:55.200
So utilities and aspirations and so on.

07:55.200 --> 08:00.520
And so economics has minimally got to be involved, but the rest of the social sciences as well.

08:00.520 --> 08:02.720
And the implications are vast.

08:02.720 --> 08:06.920
So actually, that's the phenomenon, and it's going to take 20 or so years.

08:06.920 --> 08:10.800
The difference, though, with these others is that there was a deep understanding of some

08:10.800 --> 08:12.360
underlying phenomenon there.

08:12.360 --> 08:13.360
We don't have that now.

08:13.360 --> 08:18.200
We do not understand intelligence, I can assure you.

08:18.200 --> 08:24.600
So we're calling it AI as if we got this understanding and then it leads to technology, and I think

08:24.600 --> 08:27.480
that's kind of backwards.

08:27.480 --> 08:29.280
So let's just say, why did this happen?

08:29.280 --> 08:36.080
Well, first of all, there was McCarthy and so on in the 50s who invented this terminology,

08:36.080 --> 08:37.480
and for good reason.

08:37.480 --> 08:43.040
There was a philosophical aspiration, almost.

08:43.040 --> 08:46.760
There had been discussions of mind and body, and now we have a computer, it has software

08:46.760 --> 08:50.120
and hardware, it looks like mind and body, and it looks like we can now make headway

08:50.120 --> 08:51.120
on that.

08:51.120 --> 08:52.480
And let's talk about putting thought in a computer.

08:52.480 --> 08:55.080
That's a really interesting thing to talk about.

08:55.080 --> 08:58.440
And of course, people got excited about that notion and worked on it.

08:58.440 --> 09:02.520
We don't have thought in a computer to this day, and it's not clear why we really care

09:02.520 --> 09:03.520
in some sense.

09:03.520 --> 09:10.120
It'll somehow emerge and we'll call it thought, but it's not clear what that means.

09:10.240 --> 09:12.440
And in the meantime, that's not what happened.

09:12.440 --> 09:14.480
What computers started to do was aid humans.

09:14.480 --> 09:16.080
They became complementary.

09:16.080 --> 09:20.800
Search engines and translation systems and all that aided our own intelligence and expanded

09:20.800 --> 09:25.400
it and networks expanded a planetary scale.

09:25.400 --> 09:30.080
So let's not call it McCarthy's version of intelligence for sure, but that aspiration

09:30.080 --> 09:31.080
still exists.

09:31.080 --> 09:37.360
And people who study psychology and neuroscience and core AI, whatever that is, they are working

09:37.360 --> 09:38.360
on that.

09:38.480 --> 09:40.480
It's a worthy thing.

09:40.480 --> 09:43.240
But we should be waiting for that because in the meantime, all these systems are being

09:43.240 --> 09:45.920
built in the real world that are having this huge impact.

09:45.920 --> 09:47.920
We should understand the phenomenon.

09:47.920 --> 09:48.920
All right.

09:48.920 --> 09:54.160
Now, the other part that happened, it wasn't really so much McCarthy, but others, it had

09:54.160 --> 09:56.160
to be autonomous.

09:56.160 --> 09:58.600
Why did the AI have to be autonomous?

09:58.600 --> 10:03.000
Well, if it's not autonomous, if it's tethered to me, it doesn't seem so intelligent.

10:03.000 --> 10:05.000
It's tethered to me.

10:05.000 --> 10:08.160
And if it's developed by vast numbers of humans, it's engineers who built something

10:08.160 --> 10:10.480
and it doesn't seem so intelligent.

10:10.480 --> 10:14.680
So it had to kind of be built by small numbers of people and it had to be all on its own.

10:14.680 --> 10:20.360
Now, that's a okay kind of science fiction-y kind of aspiration, but it's a bad idea for

10:20.360 --> 10:21.360
technology.

10:21.360 --> 10:23.560
You don't develop technology that way.

10:23.560 --> 10:25.920
You don't want self-driving cars to be autonomous.

10:25.920 --> 10:29.960
They should be highly networked so you think about the overall traffic system so you don't

10:29.960 --> 10:34.120
ever have an accident, just like air traffic control.

10:34.120 --> 10:36.680
You don't want autonomous airplanes.

10:36.680 --> 10:40.480
So there'll be a lot of cartoons in this talk.

10:40.480 --> 10:42.400
I don't mean it's never a good idea.

10:42.400 --> 10:45.480
So a burning building, I want an autonomous robot.

10:45.480 --> 10:50.920
Up on Mars, I want some double autonomy, but for most applications, I don't want the intelligence

10:50.920 --> 10:51.920
to be autonomous.

10:51.920 --> 10:58.640
I want it to be federated, linked in, transparent, cooperative, all those things.

10:58.640 --> 11:02.160
So I think this was a big mistake to add that to the list.

11:02.160 --> 11:06.080
I think it became kind of about bragging rights, look at my autonomous AI, how great

11:06.080 --> 11:08.680
it is and it's better than your autonomous AI.

11:08.680 --> 11:12.400
And again, this is all kind of fun and games for like 40 years, but it's no longer fun

11:12.400 --> 11:13.400
and games.

11:13.400 --> 11:16.120
It's actually going to hurt the planet.

11:16.120 --> 11:19.200
All right, so here's a counterpoint, which is that first of all, this is kind of maybe

11:19.200 --> 11:23.520
an obvious statement, but if we want to talk about intelligence, there's not one kind of

11:23.520 --> 11:24.520
intelligence.

11:24.520 --> 11:25.520
It's not just human intelligence.

11:25.520 --> 11:28.400
It's as much about the collective as it is about the individual.

11:28.400 --> 11:30.800
And an economist thinks this way all the time.

11:30.800 --> 11:37.280
They recognize that a market is composed of many small decisions by entities that don't

11:37.280 --> 11:38.760
have to be intelligent themselves.

11:38.760 --> 11:43.920
They just have to kind of know a demand curve and follow some of their nose.

11:43.920 --> 11:47.880
And you're not using huge intelligence within it, but the overall market becomes really

11:47.880 --> 11:48.880
intelligent.

11:48.880 --> 11:53.000
It could do things like bring food into cities, bring or shine at any scale for hundreds

11:53.000 --> 11:57.240
of years, and it can create all kinds of opportunities.

11:57.240 --> 12:00.800
And then there's like ant swarms that we talk a lot about, not an individual ant might

12:00.800 --> 12:03.680
not be so smart, but the swarm could do amazing things.

12:03.680 --> 12:06.160
So we're all aware of that, but too dimly.

12:06.160 --> 12:09.240
I don't think we understand that we could be creating new kinds of collectives that are

12:09.240 --> 12:13.240
really exciting, that do new things as human beings.

12:13.240 --> 12:15.720
That's what's opening up to me in the era.

12:15.720 --> 12:19.560
Not the super intelligence replacing a human, look at how great that is.

12:19.560 --> 12:24.600
All right, so in particular, if you're going to be a little less exuberant, but you're

12:24.600 --> 12:29.200
going to say, what are the goals for this emerging field?

12:29.200 --> 12:33.400
It's not make a super intelligence at a computer, and you're done.

12:33.400 --> 12:38.000
It's rather what is the overall object, like the factory in the field?

12:38.000 --> 12:39.440
Is it a transportation system?

12:39.440 --> 12:40.720
Is it a logistics chain?

12:40.720 --> 12:42.600
Is it a healthcare system?

12:42.600 --> 12:46.800
Is it a communication system designed for that level?

12:46.800 --> 12:50.320
And then think about what the components need to be and what data is needed and all that.

12:50.320 --> 12:54.880
It sounds more boring than a typical AI person's talk, which is, we'll solve intelligence

12:54.880 --> 12:57.760
and then the intelligence will solve climate change.

12:57.760 --> 13:01.760
That's a typical Silicon Valley thing to say.

13:01.760 --> 13:05.240
It sounds great and you get rid of it in the New York Times, but really to me, logistics

13:05.240 --> 13:10.080
chains and supply chains are much more exciting and interesting and important for human life

13:10.080 --> 13:12.080
and healthcare.

13:12.080 --> 13:14.480
And it's not that the AI is going to solve healthcare.

13:14.480 --> 13:19.760
It's us designing really good systems with good data science principles and economic

13:19.840 --> 13:20.840
principles.

13:20.840 --> 13:23.880
So I think I've said all this, mimicry is just not a good way to think about the implications

13:23.880 --> 13:24.880
of collectives.

13:24.880 --> 13:28.920
Autonomy is also maybe a losery.

13:28.920 --> 13:31.120
And so there might be new forms of collectives.

13:31.120 --> 13:32.120
Okay.

13:32.120 --> 13:36.640
So if you want to read a little bit more about this kind of philosophical ruminations, I

13:36.640 --> 13:42.080
wrote an article, our official that the Revlon hasn't happened yet, three years ago, and

13:42.080 --> 13:45.480
I still very much stand behind everything in there, even though we've had this kind of

13:45.520 --> 13:51.640
upswing and surprising chat GBT abilities, this was like about where's the data come

13:51.640 --> 13:52.640
from?

13:52.640 --> 13:53.640
What's the provenance?

13:53.640 --> 13:54.640
What is the bigger scope?

13:54.640 --> 13:55.640
And all that.

13:55.640 --> 13:59.120
And so if you want to read about that, and then there was a bunch of commentary by including

13:59.120 --> 14:03.000
Emmanuel Candace by some luminaries and it was quite a lot of fun.

14:03.000 --> 14:07.560
So and there was my response to those luminaries.

14:07.560 --> 14:11.840
And then with some colleagues, mostly social scientists all down here, we wrote a paper

14:11.840 --> 14:15.160
about two years ago called How AI Fails Is.

14:15.160 --> 14:18.480
And it's less about the kind of economics perspective that I was pushing up above and

14:18.480 --> 14:26.480
more about what are the implications for technology if you've got like autonomous systems being

14:26.480 --> 14:29.520
designed by small numbers of people.

14:29.520 --> 14:35.080
That kind of incentivizes entities like open AI, that they get vast amounts of money for

14:35.080 --> 14:40.000
a small number of people, they build this thing, and it's not for everybody.

14:40.000 --> 14:41.640
They control it.

14:41.640 --> 14:44.680
And it was supposed to be open, it's no longer open.

14:44.680 --> 14:50.840
And so this idea that AI is the future, it just has a natural tendency towards making

14:50.840 --> 14:54.040
it be in the hands of small numbers of people.

14:54.040 --> 14:57.800
And again, I think this article kind of gets into some of the social science reasons why

14:57.800 --> 15:00.120
that's just really a bad idea.

15:00.120 --> 15:02.480
And people pretend that it's not happening.

15:02.480 --> 15:07.240
It's all open and all that, but that's a pretense, it's just not true.

15:07.240 --> 15:11.840
So if we stop thinking about AI this way, I think it'll actually liberate us from that.

15:11.840 --> 15:14.880
All right, so again, I've already sort of said this, but just to lean in a little bit

15:14.880 --> 15:18.160
more, McCarthy had this imitative perspective.

15:18.160 --> 15:21.120
It was a great aspiration and still remains one.

15:21.120 --> 15:22.640
It's just not what's happened.

15:22.640 --> 15:28.200
What really happened was more like IA, that's Doug Engelbart there who kind of talked about

15:28.200 --> 15:29.960
technology, augment our intelligence.

15:29.960 --> 15:31.960
And for certain, it has.

15:31.960 --> 15:35.600
The search engine has augmented my intelligence more than just about any other piece of technology

15:35.600 --> 15:40.880
that I can think of, in addition to everybody's intelligence.

15:40.880 --> 15:44.720
And then this third bullet is kind of what I think is a better description of what's

15:44.720 --> 15:46.240
really emerging.

15:46.240 --> 15:47.440
It looks kind of like Internet of Things.

15:47.440 --> 15:50.640
They've got all these little devices around, they all send data around, and decisions are

15:50.640 --> 15:51.640
being made.

15:51.640 --> 15:54.280
It's all delocalized and everything.

15:54.280 --> 15:56.320
But Internet of Things was a little too computer sciencey.

15:56.320 --> 16:00.000
It wasn't thinking about the data and the inferences and the predictions and the people.

16:00.000 --> 16:03.280
It was just about put things on the Internet.

16:03.280 --> 16:06.520
But anyway, that is still the right spirit, and I think this is really what's happened.

16:06.520 --> 16:09.800
Even like the pandemic response of the planet.

16:09.800 --> 16:17.000
That was a engineering system that sort of didn't work okay, but we could do better.

16:17.000 --> 16:22.040
Now, if you go to an ML person or an AI person and say, okay, aren't you guys thinking about

16:22.040 --> 16:23.040
this?

16:23.040 --> 16:24.920
Is it all this classical AI stuff?

16:24.920 --> 16:25.920
And they say, no, no, no.

16:25.920 --> 16:26.920
We work on this.

16:26.920 --> 16:29.440
Here's, for example, federated learning.

16:29.440 --> 16:31.480
It's decentralized learning.

16:31.480 --> 16:36.560
So you have a server up there, and they're collecting data from a bunch of edge devices,

16:36.560 --> 16:41.240
and then they're analyzing the data centrally, and we're worried about privacy and all that.

16:41.240 --> 16:43.080
So we got the social stuff.

16:43.080 --> 16:46.240
This is our, we handled all the social stuff.

16:46.240 --> 16:47.240
All right.

16:47.240 --> 16:49.920
Now, I'm being a little bit, again, cartoonish here.

16:49.920 --> 16:55.280
But at terminology of federated learning, a number of groups were working on it, but

16:55.280 --> 16:59.160
it's a Google patented, and not patented, but it's a Google terminology, because Google

16:59.160 --> 17:03.360
wanted to collect a lot of data for their speech engines.

17:03.360 --> 17:06.200
And so everybody has cell phones and is talking on their phones.

17:06.200 --> 17:08.600
Let's just collect a lot of data from that.

17:08.600 --> 17:11.040
Let's worry about the compression.

17:11.040 --> 17:12.760
Let's get the gradients back cheaply.

17:12.760 --> 17:15.440
Let's also do some differential privacy, and that's the technical problem.

17:15.440 --> 17:18.800
If we solve that, wow.

17:18.800 --> 17:20.360
But what's missing in this picture?

17:20.360 --> 17:21.360
All right.

17:21.360 --> 17:24.960
Well, I'm going to give some examples of what's missing to make it more clear.

17:24.960 --> 17:28.520
But what's missing is that these are actual humans here, and they have their own values

17:28.520 --> 17:32.920
and goals and aspirations, and they want to join this collective for some reason.

17:32.920 --> 17:36.840
They don't want to just be assumed that they are in the collective because they want Google

17:36.840 --> 17:39.360
to build a bitter speech model.

17:39.360 --> 17:40.880
Okay.

17:40.880 --> 17:45.000
So the nodes are often people, and they value their data.

17:45.000 --> 17:48.960
And by data, I don't just mean where I went today and what was around me and all that.

17:48.960 --> 17:55.200
I mean, things I created, works of art, things I wrote, songs I wrote, et cetera, et cetera.

17:55.200 --> 17:56.200
That's my data.

17:56.880 --> 18:00.760
Stuff that's on the internet now that's being exploited by other companies, and I've lost

18:00.760 --> 18:01.880
all value.

18:01.880 --> 18:02.880
That's wrong.

18:02.880 --> 18:05.080
All right.

18:05.080 --> 18:09.640
So we need to talk about cost and benefits of these decentralized paradigms where learning

18:09.640 --> 18:10.800
is involved.

18:10.800 --> 18:14.440
So we need learning to wear mechanisms, and mechanisms that we're learning.

18:14.440 --> 18:17.520
Mechanism is an economics terminology, and I want to get into that.

18:17.520 --> 18:23.040
So I'm going to give some more kind of industrially, real-world examples, but as an academic, I

18:23.040 --> 18:27.360
needed to kind of think a little bit about what's happening academically.

18:27.360 --> 18:32.880
Are we kind of a set up for this emerging discipline, whatever you want to call it.

18:32.880 --> 18:35.120
And I'm not sort of sure we are.

18:35.120 --> 18:40.520
So the three disciplines, and it's not really the disciplines, it's the styles of thinking

18:40.520 --> 18:45.840
that I think are most important here, and I don't want to exclude anybody, but computer

18:45.840 --> 18:50.760
science certainly, the algorithms, the networks, and so on, statistics, and economics.

18:50.880 --> 18:55.280
Just to say there are pair-wise interactions among these fields for quite some time.

18:55.280 --> 18:57.200
Computer science meets statistics, that is machine learning.

18:57.200 --> 19:01.640
In fact, I would argue machine learning is just statistics with kind of a computer science

19:01.640 --> 19:03.520
way of thinking.

19:03.520 --> 19:07.760
Every time I see a new idea in machine learning, I know that it already exists in statistics,

19:07.760 --> 19:08.760
and I tell people that.

19:08.760 --> 19:12.920
They get mad at me, but eventually it kind of...

19:12.920 --> 19:18.480
And there's lots of ideas and statistics they don't yet know about, too.

19:18.520 --> 19:22.080
I could give lots of examples, but I won't.

19:22.080 --> 19:25.680
Statistics meets economics, that's econometrics, and I've got Hito and others in the audience

19:25.680 --> 19:28.240
who are masters of that.

19:28.240 --> 19:31.440
Well it's great, but it's kind of about measuring the economy.

19:31.440 --> 19:35.640
That's what the main goal has been, doing the causal inference to measure the economy.

19:35.640 --> 19:40.680
And it's less about algorithms and mechanisms and engineering kind of thing, artifacts.

19:40.680 --> 19:45.480
So it's had its important role, but it's missing that third leg.

19:45.480 --> 19:48.080
In economics meets computer science, that's called algorithmic game theory.

19:48.080 --> 19:50.880
That emerged 15, 20 years ago.

19:50.880 --> 19:54.880
It's very important field, study of auctions and combinatorial auctions and how they behave

19:54.880 --> 19:56.200
and incentives and all that.

19:56.200 --> 19:58.680
What's missing there is they have no statistics.

19:58.680 --> 20:01.960
They don't worry about gathering data and changing the preferences and learning them

20:01.960 --> 20:04.600
from as part of the auction and all that.

20:04.600 --> 20:10.000
So all three of these pairwise things exist, but they're critically missing the third leg.

20:10.000 --> 20:14.120
Now the interesting thing is if you go into an industry, and I spend a day a week at Amazon,

20:14.120 --> 20:17.760
and you look at any real world problem they are studying, like how do we provision, how

20:17.760 --> 20:21.720
do we interface with third party sellers, blah, blah, blah, there's always all three

20:21.720 --> 20:24.240
disciplines around the table.

20:24.240 --> 20:28.280
And just to add, there are always operations research people who already have kind of ingested

20:28.280 --> 20:33.000
all three disciplines, just to say, and control theorists and mathematicians and so on.

20:33.000 --> 20:38.080
So I don't mean to exclude anybody, but it's never one of those perspectives alone.

20:38.080 --> 20:39.920
That kills you if you just have one of those perspectives.

20:39.920 --> 20:40.920
You need all three.

20:40.920 --> 20:46.240
All right, here's a real world example that I've been involved in.

20:46.240 --> 20:49.040
So I'm a musician, I just end up being an academic.

20:49.040 --> 20:53.600
And I met up, I have a friend, Steve Stout.

20:53.600 --> 20:54.600
Someone introduced me at some point.

20:54.600 --> 21:03.280
Steve is a legendary producer, entrepreneur, well known in the hip hop and the Latin world

21:03.280 --> 21:05.240
and so on.

21:05.240 --> 21:12.160
And he and I kind of came together on this idea of modern data, modern systems, platforms

21:12.160 --> 21:14.840
should not just be about taking streaming bits.

21:14.840 --> 21:16.760
Music shouldn't just be about streaming.

21:16.760 --> 21:20.520
It should be about creating two and three-way markets.

21:20.520 --> 21:25.360
And so the idea that we originally sat down and talked about, and Steve is the CEO of

21:25.360 --> 21:28.800
now a company that has taken this and made it real.

21:28.800 --> 21:34.480
It's called UnitedMasters.com, or United Masters is the company.

21:34.480 --> 21:36.240
Basically provides a three-way market.

21:36.240 --> 21:42.960
So if you make music and now you can sign up with United Masters, they give you a record

21:42.960 --> 21:47.920
company in your pocket, you're able to kind of produce songs on your cell phone and upload

21:47.920 --> 21:55.120
them to United Masters and then they connect that to a market on the other side.

21:55.120 --> 21:59.040
So in particular, Steve has gone to the NBA.

21:59.040 --> 22:03.720
The NBA used to be streaming music from the record companies and they would pay the record

22:03.720 --> 22:07.680
companies a royalty and they might give some money back to Beyonce or whatever.

22:07.680 --> 22:11.280
But most musicians are not the big famous ones.

22:11.640 --> 22:16.560
In fact, if you look at the data, if you do some actual data science, today 95% of the

22:16.560 --> 22:21.120
songs being listened to in the United States played by people you've never heard of and

22:21.120 --> 22:25.040
they're probably between 16 and 20 years old and the song was probably recorded in the

22:25.040 --> 22:26.560
last six months.

22:26.560 --> 22:30.240
So everybody thinks we're all listening to the Beatles and Madonna or whatever.

22:30.240 --> 22:31.240
It's just not true.

22:31.240 --> 22:35.440
All right, so you think, wow, there's this wonderful market that's been created because

22:35.440 --> 22:39.040
of the ability to stream music and you'd be wrong because it's not a market.

22:39.040 --> 22:40.240
No one's making money.

22:40.240 --> 22:42.600
All the 16 to 20 year olds are not making any money off of this.

22:42.600 --> 22:48.600
They do it for a few years and then they disappear.

22:48.600 --> 22:54.040
So well, what Steve has done is by creating our masters is that a musician signs up and

22:54.040 --> 22:57.920
now there's 3 million young musicians signed up on the platform.

22:57.920 --> 23:02.120
And if you now go to the NBA website and you watch a video, there'll be some music behind

23:02.120 --> 23:06.280
it, that music is streamed from United Masters.

23:06.280 --> 23:09.600
And when every time it's streamed, the musician gets paid.

23:09.600 --> 23:11.600
It's their actual two-way market.

23:11.600 --> 23:14.680
And it's in fact a three-way market because it's got the NBA, it's got the listener,

23:14.680 --> 23:17.960
which is you and me, and it's got the person who made the music.

23:17.960 --> 23:21.760
And now all kinds of, I could give a longer talk about that, but all kinds of other market

23:21.760 --> 23:24.140
sort of forces are starting to come to play.

23:24.140 --> 23:27.080
People are reaching out to musicians and partnering with them.

23:27.080 --> 23:28.080
Shows are being made.

23:28.080 --> 23:29.760
People are playing at weddings.

23:29.760 --> 23:32.960
There's 3 million people who now have access to a steady income stream.

23:32.960 --> 23:37.320
So this is a sense in which AI can create jobs.

23:37.320 --> 23:41.040
3 million people have access now to a possible job.

23:41.040 --> 23:47.160
And these are 16 to 20-year-olds in the inner city, just to say, this is quite important.

23:47.160 --> 23:48.160
And that's just in the US.

23:48.160 --> 23:50.600
This can be done in every country around the world.

23:50.600 --> 23:54.600
And entrepreneurs thinking about a new company, instead of thinking about how do I steal some

23:54.600 --> 24:00.120
bits from somebody and then sell them, should think about how do I create a two-way market.

24:00.120 --> 24:04.120
And I just help the market get going.

24:04.120 --> 24:05.120
You could do this for art.

24:05.120 --> 24:08.960
You could do this for works of, you know, scholarly works.

24:08.960 --> 24:12.000
You could do this for travel information, all kinds of things.

24:12.000 --> 24:14.120
You can start to think more about markets.

24:14.120 --> 24:17.760
Okay, so that was the first half of my talk.

24:17.760 --> 24:20.560
That was kind of why do I work on what I'm working on, okay?

24:20.560 --> 24:23.720
And so hopefully you get a little bit more of the picture.

24:23.720 --> 24:28.600
It really is, in some sense, economics and mechanisms and, you know, networks and all

24:28.600 --> 24:29.600
that.

24:29.600 --> 24:33.480
But, you know, with all due respect, those fields didn't have enough of a statistics

24:33.480 --> 24:34.480
and learning perspective.

24:35.040 --> 24:39.720
They assumed a lot of things were already known and you get certain curves that cross.

24:39.720 --> 24:43.560
But they didn't kind of just adapt the market as you went and use large data sets to inform

24:43.560 --> 24:45.760
it and have recommendator systems.

24:45.760 --> 24:48.680
You don't see economics talking about recommender systems.

24:48.680 --> 24:52.480
Recommender systems are the way that social knowledge gets used and exploited among groups

24:52.480 --> 24:54.480
of people.

24:54.480 --> 24:58.000
So anyway, when you start thinking about what are the new problems that are going to emerge

24:58.000 --> 25:03.400
if you put these three axes together, it's really quite exciting.

25:03.400 --> 25:07.320
So in machine learning and statistics, we're really good about talking about optima.

25:07.320 --> 25:11.800
We can find optima in hundreds of thousands of dimensions even if they're saddle points

25:11.800 --> 25:16.200
and we can guarantee a rate and prove theorems about it and we're really, you know, we're

25:16.200 --> 25:18.120
really good at that.

25:18.120 --> 25:21.360
But in economics, you don't often find optima.

25:21.360 --> 25:23.120
You find equilibria.

25:23.120 --> 25:25.440
And moreover, the equilibria are rarely just stationary.

25:25.440 --> 25:27.240
They're moving around and you need to follow them.

25:27.240 --> 25:29.320
And so you need to talk about the dynamics.

25:29.320 --> 25:34.240
And so now there's topological issues and dynamical systems issues and stochastic process

25:34.240 --> 25:37.040
issues all merged together.

25:37.040 --> 25:41.400
And so there are algorithms, you know, gradient descent does not work for finding equilibria,

25:41.400 --> 25:43.920
but extra gradient does work and so on.

25:43.920 --> 25:46.600
There's a whole emerging, it's fixed point theory.

25:46.600 --> 25:52.640
So most of these ideas go back to the 30s and 40s, but they have been forgotten.

25:52.640 --> 25:56.760
But fixed point theory in hundreds of thousands of dimensions with stochastics, that's something

25:56.760 --> 26:01.000
we can start talking about and do and prove rates and, you know, get really new algorithms.

26:01.000 --> 26:03.960
And there are people doing that now.

26:03.960 --> 26:05.960
Exploration, exploitation, incentives in multi-way markets.

26:05.960 --> 26:09.280
Those are words that usually don't come into the market perspective.

26:09.280 --> 26:10.280
How do I exploit?

26:10.280 --> 26:11.280
How do I explore?

26:11.280 --> 26:12.480
And how do I put that together with incentives?

26:12.480 --> 26:16.200
I'm going to talk about some of the rest of these, but let me just sort of highlight.

26:16.200 --> 26:19.760
These are mostly words you will not see on a machine learning person's talk or AI person's

26:19.760 --> 26:20.760
talk.

26:20.760 --> 26:24.560
They will talk about trust maybe or fairness or privacy, that's all good.

26:24.560 --> 26:25.960
Those are social concepts.

26:25.960 --> 26:30.600
But they don't embed it in a fuller, what are the underlying foundational principles

26:30.600 --> 26:35.080
that make it fair or make it private or make it valuable to people.

26:35.080 --> 26:41.800
They just want to kind of stamp privacy or, you know, or fairness on it and that's enough.

26:41.800 --> 26:45.080
So let's try to think about what are these underlying concepts.

26:45.080 --> 26:48.200
And let me just say that I've loved learning all this economics.

26:48.200 --> 26:52.240
I had learned a lot of statistics and I've eventually learned some computer science.

26:52.240 --> 26:54.600
And that was fun, but learning economics has been particularly fun.

26:55.000 --> 26:57.120
And it's maybe because I already knew the math and I could just kind of go through the

26:57.120 --> 26:59.440
books really fast.

26:59.440 --> 27:04.120
But you know, this notion of incentives and really thinking about asymmetries and decentralized,

27:04.120 --> 27:08.600
I really get that out of economics in ways I never got from any other field.

27:08.600 --> 27:13.480
So I'm having a lot of fun here and I'm realizing that if I'd gone back to the 1950s and I'd

27:13.480 --> 27:17.920
hung out with David Blackwell and Von Neumann and others that they were doing all this.

27:17.920 --> 27:19.800
This was kind of the spirit of the era.

27:19.800 --> 27:23.600
And operations research emerged in that era, kind of bringing it all together.

27:23.600 --> 27:25.960
And somehow that all got kind of forgotten.

27:25.960 --> 27:30.320
We got all buried into building certain kind of systems or doing certain kind of data analysis

27:30.320 --> 27:36.520
or, you know, measuring certain kind of linear models and we forgot about the overall picture.

27:36.520 --> 27:41.120
Okay, so these blue ones are the ones I'm going to kind of use now as vignettes and

27:41.120 --> 27:43.320
the rest of my talk.

27:43.320 --> 27:47.400
And so this is, given this is an evening talk, I don't want to make this a highly technical

27:47.400 --> 27:48.400
academic talk.

27:48.400 --> 27:52.080
There are archive papers on all this with theorems and so on and so forth.

27:52.080 --> 27:56.040
But I do want to give the sense of what's the problem and what is the theorem, all right,

27:56.040 --> 27:57.040
and what is the consequence of that.

27:57.040 --> 28:00.840
Okay, so I'm going to give enough of that to highlight some of these issues.

28:00.840 --> 28:02.800
So I picked these three to talk about in some order.

28:02.800 --> 28:04.080
I forget which order.

28:04.080 --> 28:07.600
Okay, so here's perhaps my favorite one.

28:07.600 --> 28:10.000
I get to recognize two Stanford people.

28:10.000 --> 28:16.480
Stephen was a student with Emanuel and joined my group two years ago, three maybe.

28:16.480 --> 28:18.200
You know, fabulous intellect.

28:18.240 --> 28:22.320
Michael, I actually don't, we've only, because of the pandemic, met online, but

28:22.320 --> 28:24.040
he is a Stanford person.

28:24.040 --> 28:26.040
And then Jake was a student with me.

28:26.040 --> 28:31.440
He's now a postdoc with one of Emanuel's ex-students, Serena Fogelbarber.

28:31.440 --> 28:32.720
So a lot of nice connectivity there.

28:34.520 --> 28:36.720
Okay, with all due apology to the economists in the room,

28:36.720 --> 28:38.400
I'm going to say a little bit about incentives.

28:39.680 --> 28:42.240
There's a kind of general theory of incentives.

28:42.240 --> 28:43.640
There's books on it.

28:43.640 --> 28:46.960
And roughly speaking, there's kind of three branches to it.

28:46.960 --> 28:49.440
There's auction theory that you all know about.

28:49.440 --> 28:53.240
There's matching markets, and there's contract theory.

28:53.240 --> 28:56.600
So contract theory is maybe the less well-known outside of economics, but

28:56.600 --> 28:59.360
you all know about it because you experience it daily.

28:59.360 --> 29:03.680
It's where agents provide, possess private information, and

29:03.680 --> 29:06.840
there's a principal who wants to incentivize to do something with that

29:06.840 --> 29:07.560
private information.

29:08.840 --> 29:09.920
So why does this happen?

29:09.920 --> 29:13.480
Well, you know, the boss wants to get the employees to do something.

29:13.480 --> 29:16.920
And it's not just because the boss would do it themselves, but

29:16.920 --> 29:18.680
you know, they have to get the employees to do it.

29:18.680 --> 29:19.880
The boss would know how to do it.

29:19.880 --> 29:21.600
The employees have local information.

29:21.600 --> 29:23.040
They're smarter.

29:23.040 --> 29:26.440
They may, if they're incentivized, they'll do even better work and so on.

29:26.440 --> 29:30.440
And now the boss has got to kind of offer them incentives so

29:30.440 --> 29:31.920
that they'll actually do the labor.

29:31.920 --> 29:35.920
So this came up in economics, sort of after General Equilibrium Theory,

29:35.920 --> 29:39.840
which was very symmetric, Nash Equilibria and everything is very symmetric.

29:39.840 --> 29:42.400
This recognized that real life is full of asymmetries.

29:42.400 --> 29:44.360
There's someone trying to get someone else to do something and

29:44.360 --> 29:47.280
that person has power because they know something that it's not known upstairs.

29:49.520 --> 29:53.080
All right, so you know about it because you've all, for example, you travel.

29:53.080 --> 29:57.920
And you probably have wondered why aren't there, why is this so complicated?

29:57.920 --> 30:00.760
Why is there not one fare for every seat on the airplane?

30:00.760 --> 30:02.040
Like there is for a movie theater.

30:03.400 --> 30:05.320
All right, and you all know the answer kind of, right?

30:05.320 --> 30:08.560
Because there's different willingness to pay in the population.

30:08.560 --> 30:12.000
So a business class traveler or a business traveler, not a business,

30:12.520 --> 30:17.280
traveler, maybe the company's paying so they could care less what the fare is.

30:17.280 --> 30:20.080
Or maybe they're really urgently needing to get from one place to another.

30:20.080 --> 30:21.920
They have high willingness to pay.

30:21.920 --> 30:23.960
And there's a lot of other people who don't have high willingness to pay.

30:23.960 --> 30:26.120
They could wait till tomorrow, you know, and so on.

30:26.120 --> 30:31.080
So the airlines really in the 80s realized that they could start to price discriminate.

30:31.080 --> 30:34.360
They could try to figure out who had higher wills to pay and

30:34.360 --> 30:37.800
charge them higher and who had lower wills to pay and charge them less.

30:37.800 --> 30:41.760
And fill the airplane and get a blend of both kinds.

30:41.760 --> 30:44.520
And you've got to be clever to do this, right?

30:44.520 --> 30:48.120
And so what you do, if you set a single price, that's not going to work.

30:48.120 --> 30:50.520
And if you try to screen for people, like, you know,

30:50.520 --> 30:54.800
look at somebody wearing a suit and tie, you say, I'm going to charge you more.

30:54.800 --> 30:57.480
Well, that person's going to the next time show up in jeans.

30:59.200 --> 31:01.320
All right, so people are doing this all the time.

31:01.320 --> 31:01.960
They're aware.

31:01.960 --> 31:03.120
They're gaming the system.

31:03.120 --> 31:04.480
And you've got to think this through.

31:05.760 --> 31:07.040
All right, so you know the answer.

31:07.040 --> 31:09.280
What you do is you provide a menu of options.

31:09.280 --> 31:11.080
You provide a service and you provide a price.

31:11.080 --> 31:12.320
And a service and a price.

31:12.320 --> 31:15.280
And everybody gets the same menu, right?

31:15.280 --> 31:16.600
Now what is this menu for the airline?

31:16.600 --> 31:20.240
Well, there's this class called business class and the students don't know about

31:20.240 --> 31:22.840
this yet, but eventually they'll learn about it.

31:22.840 --> 31:26.440
Where you get a little glass of red wine and you get to be first in line and

31:26.440 --> 31:29.360
be all proud of yourself and you get a little bit bigger chair.

31:29.360 --> 31:32.080
And people will pay $1,000 more for that.

31:32.080 --> 31:34.320
It's amazing, right?

31:34.320 --> 31:38.400
Now, only class that actually makes money, right?

31:39.400 --> 31:45.280
But the marginal cost of putting people on airplanes is sort of zero, all right?

31:45.280 --> 31:47.880
So you want to fill the rest of the airplane, all right?

31:47.880 --> 31:51.720
So amazingly, there are people in the back who don't want to spend $1,000 for

31:51.720 --> 31:53.080
a little glass of red wine.

31:53.080 --> 31:55.520
And they feel very good about themselves because they didn't spend all that money

31:55.520 --> 31:57.000
and they're still on the airplane.

31:57.000 --> 31:58.280
So everybody's happy.

31:58.280 --> 32:00.760
That's what's called social welfare.

32:00.760 --> 32:01.720
And the plane is full.

32:01.720 --> 32:02.720
That's what's called revenue.

32:03.760 --> 32:06.160
Okay, so you can make mathematics out of all that.

32:06.160 --> 32:08.080
So you get the usual crossing curves and all that.

32:08.080 --> 32:10.520
They're just not the same crossing curves as in general equilibrium theory.

32:10.520 --> 32:12.200
They're a different set of things.

32:12.200 --> 32:17.520
But every one of those texts says we have missing information.

32:17.520 --> 32:19.320
We're going to assume there's some probability distribution and

32:19.320 --> 32:21.400
we're going to call the whole thing Bayesian.

32:21.400 --> 32:24.760
Now, as a statistician, I look at it and say, wow, Bayesian, it's not Bayesian.

32:24.760 --> 32:27.920
There's a distribution on unknown quantities.

32:27.920 --> 32:28.680
That's all.

32:28.680 --> 32:32.960
There's no updating, there's no learning, there's none of the above, okay?

32:32.960 --> 32:34.920
All right, so wow, wonderful opportunity.

32:34.920 --> 32:35.840
We should work on this.

32:36.840 --> 32:37.600
And we have.

32:38.800 --> 32:41.240
Okay, so you all know about clinical trials.

32:42.520 --> 32:47.720
Costs tens of millions of dollars a year to run clinical trials in any particular

32:47.720 --> 32:49.000
therapeutic area.

32:49.000 --> 32:51.080
You all know about it for vaccines.

32:51.080 --> 32:53.600
It's amazingly expensive and it's amazingly important.

32:53.600 --> 32:55.880
And if you don't do it at the right scale, you'll make big mistakes.

32:57.040 --> 32:57.680
You all know this.

32:58.720 --> 33:02.320
All right, so you would imagine that the FDA does a great job of this.

33:02.320 --> 33:03.400
And in some level they do.

33:03.400 --> 33:04.800
They're very good statisticians.

33:04.800 --> 33:07.400
But they're not good at being the economics.

33:09.240 --> 33:12.120
All right, so this really should be thought as a contract theory problem.

33:12.120 --> 33:15.760
The FDA is a principle and they're trying to decide what drugs go to market.

33:17.000 --> 33:21.320
But they only have partial knowledge about the drug candidates, okay?

33:21.320 --> 33:22.800
Where do the drug candidates come from?

33:22.800 --> 33:25.600
They come from the pharmaceutical companies.

33:25.600 --> 33:28.640
Pharmaceutical companies know something internally about some candidate.

33:28.640 --> 33:31.120
They're about ready to send up to the FDA.

33:31.120 --> 33:33.160
Maybe they know they put their best engineers on it.

33:33.160 --> 33:34.760
Maybe they've had experience with it.

33:34.760 --> 33:39.480
Maybe a little internal testing, so on and so forth, all right?

33:39.480 --> 33:41.480
The FDA is now getting all these candidates and

33:41.480 --> 33:43.960
they would like to say, pharmaceutical company,

33:43.960 --> 33:46.000
that candidate you just sent me, how good is that candidate?

33:47.000 --> 33:49.520
Well, the pharmaceutical company does not want to reveal.

33:49.520 --> 33:52.240
Because the FDA, if they're told it's not a good candidate,

33:52.240 --> 33:54.880
they'll put yet more, they'll ensure there's no false positive.

33:54.880 --> 33:58.160
They'll put yet more clinical trial money into it.

33:58.160 --> 34:00.600
Where if they think it's a really good candidate, they won't.

34:00.720 --> 34:04.120
And also the license they will get will be titrated to risk.

34:05.240 --> 34:07.320
All right, so the companies are incentivized to not say.

34:09.840 --> 34:11.280
All right, but that's a problem.

34:11.280 --> 34:13.720
All right, so now let's think about the actual paradigm.

34:13.720 --> 34:14.800
What is the FDA doing?

34:14.800 --> 34:18.160
Well, they're being statisticians, a frequentist statistician.

34:18.160 --> 34:22.200
So here's a little Naaman Pearson kind of setup.

34:22.200 --> 34:25.560
A bad drug, theta equals zero, doesn't mean that it hurts people,

34:25.560 --> 34:27.520
because they definitely screen for that.

34:27.520 --> 34:29.840
It just means it has no effect, all right?

34:29.840 --> 34:33.040
And there are tons of drugs on the market that have no effect,

34:33.040 --> 34:34.880
for better or for worse.

34:34.880 --> 34:38.800
And they have a type one error of say 0.05, it's actually more like 0.01,

34:38.800 --> 34:42.560
but they set up a classifier that achieves that.

34:42.560 --> 34:45.560
And then for the good drugs that are actually having an effect,

34:45.560 --> 34:49.400
they want a high power, so 0.8 is a kind of a standard number for that.

34:49.400 --> 34:51.000
Is that a good protocol?

34:51.000 --> 34:53.240
Well, it's optimal, it's the Naaman Pearson test.

34:53.240 --> 34:55.040
So yeah, of course, it's great.

34:55.040 --> 34:56.520
But is it a good protocol?

34:56.520 --> 34:57.160
And the answer is no.

34:58.160 --> 34:59.960
So let's do a little thought experiment here.

34:59.960 --> 35:03.560
In situations where there's a small profit to be made,

35:03.560 --> 35:06.840
it costs $20 million to run the trial.

35:06.840 --> 35:10.240
But if you're approved, let's suppose you would make 200 million.

35:10.240 --> 35:12.880
So this would be for a niche drug of some kind.

35:12.880 --> 35:19.400
And so the CEO can do a little calculation, as can the FDA,

35:19.400 --> 35:21.640
conditioning on theta equals zero.

35:21.640 --> 35:25.640
Now no one knows if theta is zero or not, so this is a counterfactual.

35:25.920 --> 35:30.760
But thinking conceptually, theta equals zero, what's my expected profit?

35:30.760 --> 35:34.240
Well, you can put all those numbers together and you get minus 10 million.

35:34.240 --> 35:36.320
All right, so the CEO looks at that number and they say,

35:36.320 --> 35:40.360
only send candidates up to the FDA if you're really pretty sure it's a good drug.

35:42.080 --> 35:43.920
That it's going to get passed because it's a good drug.

35:43.920 --> 35:45.680
Don't hope for a false positive.

35:45.680 --> 35:47.000
We'll go out of business.

35:48.080 --> 35:48.800
That's great.

35:48.800 --> 35:53.080
Now the FDA is mostly getting good drugs and they have a good screening procedure.

35:53.080 --> 35:54.760
So everything that's getting through is looking good.

35:55.840 --> 35:58.000
If that were real life, that'd be great, but here's more like real life.

35:58.000 --> 36:02.080
So you have $20 million to run the trial and if you're approved,

36:02.080 --> 36:03.640
you could make 2 billion.

36:03.640 --> 36:05.680
So this would be like ibuprofen or something.

36:05.680 --> 36:07.480
So this is more common.

36:07.480 --> 36:09.920
And now the CEO could do the same exact calculation.

36:09.920 --> 36:12.880
If it was the case that theta is equal to zero,

36:12.880 --> 36:15.040
my expected profit would be 80 million.

36:15.040 --> 36:19.480
So now the CEO is very incentivized to send as many candidates as they can to the FDA.

36:19.480 --> 36:22.760
And the FDA will get flooded and they do get flooded.

36:22.760 --> 36:25.040
And they will do these tests and there will be some false positives.

36:25.040 --> 36:25.960
And these things will go to market.

36:25.960 --> 36:27.680
They don't hurt anybody, but they just don't have any effect.

36:27.680 --> 36:30.800
And people will make money and then eventually that changes.

36:30.800 --> 36:32.360
So this is broken.

36:32.360 --> 36:35.120
And it's just broken because it's not being thought of as a contract theory problem.

36:36.200 --> 36:39.600
All right, so we have now lured on this.

36:39.600 --> 36:43.720
We have a paper and we have an idea we call Statistical Contract Theory.

36:45.000 --> 36:46.680
And so here is the protocol.

36:47.960 --> 36:49.080
There are four steps to it.

36:49.080 --> 36:50.760
It's only step three, which is new.

36:50.760 --> 36:53.360
The other three are standard contract theory.

36:53.360 --> 36:57.720
So an agent comes to this contract and they opt in or they just decide to walk away.

36:57.720 --> 37:01.160
So the drug company comes and they just looked at it and say, no, I'm not interested.

37:01.160 --> 37:06.200
Or if they opt in, they have to pay a reservation price R, say 20 million.

37:06.200 --> 37:09.000
And then they get to select a payout function from a menu.

37:09.000 --> 37:10.960
And I'm going to say more about what that means here in a moment.

37:12.480 --> 37:19.800
But it's going to be a function from observed the clinical trial to the amount you get to licensed for.

37:19.800 --> 37:20.880
And we're going to design the menu.

37:20.880 --> 37:24.640
That's going to be our goal as economical statisticians.

37:24.640 --> 37:30.160
Then we do a statistical trial, which yields a random variable, Z, coming from P of theta.

37:30.160 --> 37:33.600
Theta is the true theta in nature because we're getting data from the real world.

37:33.600 --> 37:36.760
No one knows theta, but we get data from P of theta.

37:36.760 --> 37:37.960
And then there's the payoff.

37:37.960 --> 37:40.040
So agent gets payoff F of Z.

37:40.040 --> 37:43.920
They were the ones who selected the payout function, so they get paid that amount they selected.

37:43.920 --> 37:48.280
And the principal receives a utility, which is a function of F of Z because they have to pay that.

37:49.240 --> 37:54.800
And theta, because the FDA, if they make lots of approvals of not so good drugs,

37:54.800 --> 37:58.080
they'll eventually look bad, and so their utility should reflect that.

37:59.400 --> 38:02.440
Agents in this setting need to maximize their payoff.

38:02.440 --> 38:07.720
Their best response is simply to take the arg max of the expectation under this data of the payoff.

38:07.720 --> 38:09.400
That's what they want to maximize.

38:09.400 --> 38:11.480
So that's pretty clear what an agent should be doing in this paradigm.

38:12.600 --> 38:15.040
All right, now if you're going to do economics together with statistics,

38:15.040 --> 38:18.000
the key thing you have to think about is incentive alignment.

38:18.040 --> 38:26.080
Am I doing a situation where the incentives or what I want to achieve is aligned with people's interest?

38:26.080 --> 38:28.360
All right, so here's a way to set that up.

38:28.360 --> 38:33.200
For the null agents, those who have the null candidates,

38:33.200 --> 38:41.880
it should be the case that the utility of the principal is decreasing in F of Z, okay?

38:41.880 --> 38:46.720
Whereas for the non-null agents, for a good drug, the utility should be increasing in F of Z, okay?

38:46.760 --> 38:47.760
So it's kind of obvious.

38:49.760 --> 38:55.040
So, you know, in English, the principal wants to attract as transact as much as possible with the good agents,

38:55.040 --> 38:56.280
the ones that have a good drug.

38:56.960 --> 39:02.080
All right, so now the definition is that a menu of these options is incentive aligned.

39:02.080 --> 39:08.040
If it is the case for all of the null drugs, the expectation under the null of the difference

39:08.040 --> 39:10.800
of the payout and the reservation price is less than or equal to zero.

39:10.800 --> 39:14.360
If that weren't true, then these companies just make money for free, okay?

39:14.400 --> 39:18.720
So you need to have that be the case, so the principal would be happy with this.

39:19.320 --> 39:23.600
The P less than or equal to 0.05 protocol that we're used to from statistics is not incentive aligned.

39:23.600 --> 39:24.800
That's simple to see.

39:26.080 --> 39:32.400
Okay. All right, so now we have a theorem, which is right down at the bottom there,

39:32.400 --> 39:37.440
which is that it turns out that a contract is incentive aligned if and only if,

39:37.440 --> 39:42.280
this is a characterization, all of the payout functions are E values, right?

39:42.280 --> 39:43.440
So what's an E value?

39:44.440 --> 39:46.800
Well, it's like a P value kind of.

39:46.800 --> 39:50.720
It's a statistical measure of evidence, but

39:50.720 --> 39:53.600
whereas a P value is a tail probability under the null,

39:53.600 --> 39:57.800
the probability of under the null hypothesis being more extreme than the observed data.

39:57.800 --> 39:59.520
That's a P value.

39:59.520 --> 40:01.800
And E value is under the null hypothesis.

40:02.760 --> 40:06.840
The expectation of this E value is less than or equal to 1, okay?

40:07.840 --> 40:16.160
It looks a little bit like a martingale, a super martingale, and in fact is the more general story is these are non-negative super martingales.

40:16.160 --> 40:18.360
And because they're martingales, they kind of compose nicely.

40:18.360 --> 40:20.200
You can stop them because of stopping theorems.

40:20.200 --> 40:22.760
They just are a nicer measure of statistical evidence.

40:22.760 --> 40:24.120
Whereas P values don't compose.

40:24.120 --> 40:24.960
You can't stop them.

40:24.960 --> 40:25.960
They just have all these troubles.

40:27.040 --> 40:31.680
So this is a neat result, which is that this concept from theory of contracts

40:33.120 --> 40:36.560
is exactly the same concept as E values in statistics.

40:37.800 --> 40:40.360
And moreover, we have a result, which I don't think I have a slide on it.

40:40.360 --> 40:41.800
Nope.

40:41.800 --> 40:46.360
That if we now want to do, how do you actually design a menu and get,

40:46.360 --> 40:55.480
say, a maxi min menu, the maximal overall theta of the minimum risk.

40:56.600 --> 41:01.400
It turns out to be characterized by taking all possible E values.

41:01.400 --> 41:02.520
That's your menu.

41:02.520 --> 41:06.640
So if a computational regime might want to do that, or for interpretability reasons,

41:06.640 --> 41:08.880
but that's another if and only of theorem.

41:10.680 --> 41:12.600
Okay, so I'm going to move on.

41:12.600 --> 41:16.040
We're now rolling this out in various domains of actually designing menus and

41:16.040 --> 41:17.440
contracts, but we have this guide.

41:17.440 --> 41:19.280
We now know how to design the optimal contract.

41:19.280 --> 41:22.760
We know what, we use E values, and we know lots of E values.

41:22.760 --> 41:25.960
There's a lot of literature on non-negative super martingales or E values and so on.

41:25.960 --> 41:27.160
So we'll be doing that.

41:27.160 --> 41:29.800
And I'll just say we've done this in particular in the federated learning

41:29.800 --> 41:31.160
domain.

41:31.160 --> 41:33.280
This is now just, again, the picture of federated learning, but

41:33.280 --> 41:35.120
now with an incentive structure.

41:35.120 --> 41:39.920
So we're able to design an incentive compatible mechanism that incentivizes

41:39.920 --> 41:42.720
agents at the edge to contribute data.

41:42.720 --> 41:46.560
And in particular, this handles a problem that has been recognized in

41:46.560 --> 41:48.680
literature, which is a free writing problem.

41:48.680 --> 41:52.640
If I have some data to send up, but sitting next to me there is a manual, and

41:52.640 --> 41:56.240
he has data to send, and I know that his data is pretty much the same as mine.

41:56.240 --> 41:58.960
I'm going to watch him send the data, and I know I don't have to.

41:58.960 --> 42:00.120
Pre-writing.

42:00.120 --> 42:02.240
This paradigm incentivizes against free writing.

42:03.200 --> 42:05.160
Professor, just one small clarification.

42:05.160 --> 42:05.680
Yeah.

42:05.680 --> 42:09.760
Is this based on the assumption that we're talking about home economics in terms

42:09.760 --> 42:13.240
of very rational and certain things?

42:13.240 --> 42:14.080
It's a good question.

42:14.080 --> 42:17.880
I was hoping that was going to come up later about, it's all this rational

42:17.880 --> 42:20.640
economic stuff.

42:20.640 --> 42:25.000
No, and sort of the behavioral economics here is kind of coming in the fact that

42:25.000 --> 42:25.840
we're gathering data.

42:28.080 --> 42:31.200
So all these distributions are informed by data.

42:31.240 --> 42:35.600
And if we just write down the utility, that's only the assumption we have to

42:35.600 --> 42:37.800
make, is that we agree that you want to maximize that.

42:38.960 --> 42:40.560
And that's usually not so strange.

42:41.840 --> 42:42.960
And then the data informs it.

42:42.960 --> 42:45.400
We don't make a distributional assumption about the data.

42:45.400 --> 42:47.240
So I can get into that a little bit longer, but

42:48.520 --> 42:50.440
behavioral economics is very much part of this agenda.

42:51.480 --> 42:55.560
But it's not just that it's broken and we think about the psychology of it.

42:55.560 --> 42:58.600
Well, no, we collect data, and data is coming from real people.

42:58.600 --> 43:00.640
So we already have a little bit of a help there.

43:01.920 --> 43:04.480
So hopefully that partially answers your question.

43:04.480 --> 43:07.120
Anyway, if you're interested in this application, we have a paper on that and

43:07.120 --> 43:07.920
we're continuing on with that.

43:10.160 --> 43:12.880
I got two more vignettes, I think, and I'm just going to go a little more quickly on

43:12.880 --> 43:15.240
these. I just want to give you a flavor of these.

43:15.240 --> 43:18.240
Classification is the big killer app in machine learning.

43:18.240 --> 43:21.400
Classify, yes or no, good or bad, blah, blah, blah.

43:21.400 --> 43:24.160
But if you do this in domains where there are strategic agents,

43:24.160 --> 43:25.840
you get something called strategic classification.

43:25.840 --> 43:28.800
So this is a work with Tiana Zernich, who's still a student with me and

43:28.800 --> 43:31.040
will be joining Emmanuel's group as a postdoc.

43:31.040 --> 43:33.920
He and I shuttle these superstar people back and forth.

43:33.920 --> 43:35.520
And then Eric is now a professor at Caltech.

43:37.200 --> 43:39.040
All right, so here's a little picture to suggest this.

43:39.040 --> 43:43.240
Health insurance, the health insurance company has got to do a classification

43:43.240 --> 43:47.120
problem. I fill out a form, they have to decide whether to give me insurance or not.

43:48.960 --> 43:51.840
They're going to ask me, how much do you exercise?

43:51.840 --> 43:53.760
I'm going to say a lot.

43:53.760 --> 43:55.560
How much do you drink of wine?

43:55.560 --> 43:58.560
Very little, so on and so forth.

43:58.560 --> 44:00.720
Now if it's implausible, they'll kind of see that.

44:01.040 --> 44:02.040
But you make it plausible.

44:03.520 --> 44:06.600
They know that, however, so they're not going to make it so easy for you.

44:06.600 --> 44:10.560
So they're going to ask questions like, would you be willing to have us look at

44:10.560 --> 44:13.240
your cell phone accelerometer for one day?

44:13.240 --> 44:16.160
Just opt in, you don't have to, but are you willing to do that?

44:16.160 --> 44:17.320
You say, sure.

44:17.320 --> 44:20.480
And now if my cell phone moves around a lot during that, it shows I'm very active.

44:20.480 --> 44:22.720
If it sits in one place all day, I'm not so active.

44:22.720 --> 44:24.400
They would use that as data.

44:24.400 --> 44:26.240
So someone went out to build a device,

44:26.240 --> 44:28.560
then you put your cell phone on the device and it moves around all day.

44:29.520 --> 44:31.280
So this is the kind of problem that arises.

44:31.280 --> 44:33.000
An economist are very much aware of this.

44:33.000 --> 44:35.800
They call this Goodhart's Law.

44:35.800 --> 44:41.320
If you set up a poverty index score at some year, this was in Columbia in 1994.

44:41.320 --> 44:44.240
It looks very good, very Gaussian and all that.

44:44.240 --> 44:49.440
By 2003, people have discovered that if they move just a little bit left of there,

44:49.440 --> 44:50.960
they get more better housing.

44:52.000 --> 44:54.440
So everyone cheated a little bit so they could move over.

44:54.440 --> 44:57.040
And so the poverty index score has now been ruined.

44:57.040 --> 44:58.040
But this is real life.

44:58.240 --> 45:00.200
This is what people really will do, and they should.

45:00.200 --> 45:00.560
Why not?

45:02.480 --> 45:03.600
It's not an ethical issue.

45:05.760 --> 45:08.040
Ethics is sometimes used a little bit too easily here.

45:08.040 --> 45:09.800
So the real problem is that when you do learning,

45:09.800 --> 45:12.680
you rarely have just collected data set and analyze it.

45:12.680 --> 45:15.320
In the real world, you have to say, where's the data come from?

45:17.160 --> 45:19.800
If it's people supplying the data, are they aware of what the outcome is?

45:19.800 --> 45:21.200
Do they have some bets that's interested in it?

45:22.200 --> 45:26.480
Probably they do, because if not, why would they really be engaged in this whole exercise?

45:26.480 --> 45:29.000
All right, so now we have a Stackelberg game.

45:29.000 --> 45:31.200
It's a game theory setting, which is sequential.

45:32.240 --> 45:36.120
I send some data up, and the central decision maker, say the bank,

45:36.120 --> 45:39.040
is trying to decide about loans, collects a lot of data.

45:39.040 --> 45:41.840
They build a model that predicts whether I should get a loan or not.

45:43.200 --> 45:45.240
And then that starts to make some decisions.

45:45.240 --> 45:47.320
People start to realize what's happening.

45:47.320 --> 45:49.560
Maybe the bank has got to reveal by regulatory reasons,

45:49.560 --> 45:51.160
I'm using logistic regression or something.

45:52.080 --> 45:54.640
People realize that, and they say, okay, the next time they send the data,

45:54.640 --> 45:57.240
they're going to alter their data.

45:57.240 --> 46:00.560
And that goes back and forth, and you want to ask what equilibria rise here.

46:00.560 --> 46:03.160
We're not trying to optimize any likelihood, it's an equilibrium problem.

46:04.480 --> 46:08.480
Okay, so we have studied this as a Stackelberg game,

46:08.480 --> 46:11.360
which is the appropriate concept in game theory.

46:12.360 --> 46:15.880
Classically, in a Stackelberg game, you have a leader and you have a follower.

46:16.920 --> 46:20.800
Classically, the decision maker would be thought of as the leader here.

46:20.800 --> 46:24.440
They run the whole show, and agents are the follower.

46:24.440 --> 46:27.320
You can show in that situation, in this setup,

46:27.320 --> 46:32.080
that the leader gets high utility and the followers get low utility, just to say.

46:32.080 --> 46:34.960
So it seems reasonable.

46:34.960 --> 46:39.080
But if that's an analysis you could do in a synchronous situation,

46:39.080 --> 46:41.400
where there's a model built, data's gathered.

46:41.400 --> 46:43.760
Model built, data gathered, all synchronized.

46:43.760 --> 46:45.360
The real world is no synchronization.

46:45.360 --> 46:49.160
Why should people wait till, there's no synchronization between the central model

46:49.160 --> 46:51.000
and me sending up data?

46:51.000 --> 46:53.840
Okay, so you could start to think about analyzing different scenarios,

46:53.840 --> 46:57.360
where there's different kinds of timescales.

46:57.360 --> 47:01.680
So here's one where the modeler goes slowly, only updating every once in a while,

47:01.680 --> 47:05.200
and the streets you gain doesn't send data much more rapidly.

47:05.200 --> 47:07.840
So does this arise in real life?

47:09.360 --> 47:12.480
Sure, this is, for example, like college admissions.

47:12.480 --> 47:15.880
The college is gathering all these applicants, and they have all this data.

47:15.880 --> 47:19.200
They're not gonna adjust their policy after every applicant.

47:19.200 --> 47:20.560
They'll do it every couple of years or so, and

47:20.560 --> 47:22.600
they'll publish it and all, for obvious social reasons.

47:23.600 --> 47:25.040
So that's a real scenario.

47:25.040 --> 47:26.360
What about the other way around?

47:26.360 --> 47:30.320
Where the central agent updates very, very rapidly, and agents are much more slow?

47:30.320 --> 47:33.120
Well, that happens all the time too, that's like YouTube.

47:33.120 --> 47:36.320
Every time someone clicks, they update a model in principle, okay?

47:37.360 --> 47:41.800
So these are different scenarios, and so what happens here?

47:41.800 --> 47:45.480
So you can analyze this as now you do the game theory.

47:45.480 --> 47:48.400
So we were able to prove a theorem that shows, first of all,

47:48.400 --> 47:51.800
that in either order of play, you get an equilibrium.

47:51.800 --> 47:55.000
It's not so hard to see that and analyze that.

47:55.000 --> 47:59.480
Much more surprisingly, is that in these statistical settings,

47:59.480 --> 48:03.280
where it's a data analysis problem, not just an arbitrary Stackelberg game,

48:04.560 --> 48:10.640
it turns out that when the decision maker is a follower, and

48:10.640 --> 48:14.440
the strategic agents are the leader, the kind of flipped around version,

48:14.440 --> 48:17.360
the strategic agents have higher utility than before.

48:17.360 --> 48:18.760
That makes some sense.

48:18.760 --> 48:21.960
But also, the decision maker has higher utility.

48:21.960 --> 48:24.120
It's a rare example in game theory of a win-win.

48:25.280 --> 48:28.760
Going in the order where the strategic agent is a one's going fast,

48:28.760 --> 48:31.560
that leads to higher utility for both parties.

48:31.560 --> 48:34.640
So that's not a true fact about game theory in general, but

48:34.640 --> 48:36.760
it's a true fact about statistical game theory.

48:36.760 --> 48:39.240
These statistical modeling exercises for

48:39.240 --> 48:40.760
generalized linear models, just to say.

48:41.840 --> 48:43.200
I'm gonna skip this little part here,

48:43.200 --> 48:45.720
just I like to show pictures of my students.

48:45.720 --> 48:49.880
So there's Lydia and Horia, and just say this,

48:49.880 --> 48:51.640
I'm gonna show you really quick the slides.

48:51.640 --> 48:55.520
But it's a cute little paper where you bring together bandits from machine

48:55.520 --> 48:58.520
learning and matching markets from economics.

48:58.520 --> 48:59.560
And let me just show you a picture.

48:59.560 --> 49:02.120
Here's a learner in a bandit problem.

49:02.120 --> 49:05.840
They're trying to find out which of the set of options is the best,

49:05.840 --> 49:07.280
gives the highest reward.

49:07.280 --> 49:09.800
And they're algorithms like upper confidence bound,

49:09.800 --> 49:14.040
that help you guide you towards diminishing your uncertainty and

49:14.040 --> 49:16.240
also picking the optimal arm.

49:16.240 --> 49:20.120
So we asked the question about, what if you put this in a market setting?

49:20.120 --> 49:24.080
So I don't just have one decision maker, I've got a two sided market.

49:24.080 --> 49:28.200
And so in particular, I might have two decision makers who are selecting

49:28.200 --> 49:30.400
actions from the other side of the market.

49:30.400 --> 49:32.680
And there's preferences on both sides.

49:32.680 --> 49:37.600
And so you ask questions like, what if both of the agents select the same action?

49:37.600 --> 49:41.120
And so we model this as congestion that one of them gets the reward,

49:41.120 --> 49:42.360
the other gets no reward at all.

49:44.040 --> 49:45.520
And who gets the reward?

49:45.520 --> 49:47.960
Well, that depends on the preferences on the right side of the market.

49:47.960 --> 49:50.480
So both sides are learning about each other.

49:50.480 --> 49:52.720
And so again, you can do the mathematics here, and

49:52.720 --> 49:54.200
it turns out to be pretty interesting.

49:54.200 --> 49:59.600
What you're really asking is, if there's competition in a bandit situation,

49:59.600 --> 50:02.640
does that make the regret higher or better?

50:02.640 --> 50:03.840
What does competition do for

50:03.840 --> 50:06.680
the learning process of a person trying to learn the best action?

50:07.680 --> 50:10.880
Okay, and long story short, we did a, here's a theory.

50:10.880 --> 50:14.000
Here's a regret bound, and so this is more for the experts.

50:14.000 --> 50:18.400
But the regret is as a function of time, which is n, logarithmic and n.

50:18.400 --> 50:22.680
So that's an optimal result from classical bandit theory.

50:22.680 --> 50:23.680
So that is still true.

50:23.680 --> 50:26.520
Competition does not hurt your rate of learning.

50:26.520 --> 50:28.760
There's a denominator term though, which is a constant,

50:28.760 --> 50:32.440
which is a gap between the preferences of nearby agents.

50:32.440 --> 50:34.680
So if there's competition and you have a small gap between me and

50:34.680 --> 50:36.680
somebody else, we start to compete more.

50:36.680 --> 50:39.520
And that gives us a higher regret, but it's only a constant.

50:39.520 --> 50:42.320
All right, so I put that up there just to sort of show you that it's kind of

50:42.320 --> 50:45.640
really fun things to do with simple learning algorithms,

50:45.640 --> 50:50.200
explore exploit type, and simple matching market kind of ideas.

50:50.200 --> 50:53.680
And this was motivated by this kind of restaurant setting where

50:53.680 --> 50:57.200
100,000 of us are out looking for a restaurant in Shanghai.

50:57.200 --> 51:02.040
There's 100,000 restaurants, and we're all trying things out as we go and

51:02.040 --> 51:05.960
getting rewards or not, and both sides of the market have some preferences.

51:05.960 --> 51:08.160
And how does that market clear?

51:08.160 --> 51:08.800
That was our question.

51:08.800 --> 51:14.280
All right, so last two weeks ago, I talked about this in this very room.

51:14.280 --> 51:15.440
So I'm going to kind of go quickly, but

51:15.440 --> 51:19.680
this is a very exciting project here that I want to spend five minutes on.

51:19.680 --> 51:23.680
Similar collection of students, but also again, Stephen, who's a postdoc.

51:25.240 --> 51:28.200
Anastasia, Stephen, Clara, and Tiana.

51:28.200 --> 51:30.120
So this is really more about the statistics.

51:30.120 --> 51:32.440
There's little economics here, but less.

51:32.440 --> 51:36.960
This is more about how do we do things like use neural nets to do science,

51:36.960 --> 51:39.360
just roughly speaking, okay?

51:39.360 --> 51:44.080
So you all know about things like Alpha-fold, they will make huge numbers

51:44.080 --> 51:48.160
of predictions of, say, these tertiary structure proteins.

51:50.080 --> 51:53.400
Hundreds of millions of structures, whereas the hand labeled ones,

51:53.400 --> 51:56.520
there's only hundreds of thousands of such sequences, all right?

51:56.520 --> 52:00.760
So that was a problem that's now being revolutionized in biology.

52:00.760 --> 52:05.520
So here's an example of someone in 2004 wrote a very important paper in nature,

52:06.520 --> 52:10.480
studying the relationship between intrinsic disorder of proteins,

52:10.480 --> 52:12.960
where things don't fold, they kind of are more strand-like.

52:12.960 --> 52:16.680
And that turns out to be very important for, like, grid-like things.

52:16.680 --> 52:20.400
And phosphorylation, which is an important part of it, and many pathways.

52:21.600 --> 52:25.880
So they wanted to ask, is there an association between those two notions?

52:25.880 --> 52:28.680
With the parts of the protein in its order, they tend to be more phosphorylated or not.

52:30.680 --> 52:34.240
But they really couldn't test it, and now you go forward to 2022.

52:34.280 --> 52:38.720
Instead of this small amount of data we had back in 2004,

52:38.720 --> 52:44.080
now you have vast amounts of Alpha-fold labeled data.

52:44.080 --> 52:46.560
It's not really data, it's predictions, but they're good predictions.

52:46.560 --> 52:49.480
So why not throw them in as if they were data, all right?

52:49.480 --> 52:52.920
So someone wrote a paper, 2022, doing that.

52:52.920 --> 52:55.760
And so they wanted to quantify this odds ratio,

52:55.760 --> 52:58.240
probability of intrinsically disordered given phosphorylation.

53:00.000 --> 53:02.880
And kind of amazingly, they didn't even use any of the hand label,

53:03.360 --> 53:05.400
the gold standard data, because they had so much of this other stuff,

53:05.400 --> 53:08.560
they just threw it all in, because it's such a good predictor, why not?

53:08.560 --> 53:10.480
But as a statistician, you know better, right?

53:10.480 --> 53:12.360
Even if it's very, very accurate at making predictions,

53:12.360 --> 53:15.040
that doesn't mean the inferences you make are any good.

53:15.040 --> 53:19.280
All right, so I think this one picture I'm about to show, so there's the mayors.

53:19.280 --> 53:22.600
This picture is probably the end of my talk, really,

53:22.600 --> 53:24.080
and I'll just kind of scroll through a couple more.

53:25.280 --> 53:26.360
So let me take a moment on this one.

53:27.600 --> 53:30.440
Our statistic here is this odds ratio.

53:30.440 --> 53:32.480
We'd like to know if there's an association or not.

53:32.480 --> 53:34.640
You've all taken elementary statistics.

53:34.640 --> 53:37.560
We have to find whether it's significantly different from one.

53:37.560 --> 53:40.800
One would be no association, bigger is an association.

53:43.320 --> 53:49.480
We did a Monte Carlo version of this where we, in the set of label data,

53:49.480 --> 53:52.520
we actually got the true odds ratio, that's the dotted line there.

53:53.520 --> 53:58.960
And then we redid the entire experiment with alpha fold output using the predictions.

53:58.960 --> 53:59.960
Okay, so what are we doing here?

53:59.960 --> 54:03.520
That gold region right there is a confidence interval.

54:03.520 --> 54:09.240
And it's based on taking all of the alpha fold predictions and treating them as real, okay?

54:09.240 --> 54:14.280
And from those, you form a confidence interval on this odds ratio.

54:14.280 --> 54:17.720
And that's an elementary statistics exercise to do that, all right?

54:17.720 --> 54:19.000
That confidence interval is tiny.

54:19.000 --> 54:21.280
That looks really good, because you have all this data.

54:21.280 --> 54:23.360
It's not real data, but it looks really good.

54:23.360 --> 54:24.840
You're very, very confident.

54:24.840 --> 54:28.680
You're just dead wrong, all right?

54:29.680 --> 54:33.960
The statisticians in the room, Art, will say, why'd you do that?

54:33.960 --> 54:36.240
Just we know how to do confidence intervals.

54:36.240 --> 54:38.480
Just use the gold standard data.

54:38.480 --> 54:41.240
Don't trust these wild machine learning predictions.

54:41.240 --> 54:42.560
And Art, I would do that too.

54:42.560 --> 54:43.880
That's what my first thought would be.

54:43.880 --> 54:49.720
That gives you the gray region, so it covers the truth, as it was asserted to be, all right?

54:49.720 --> 54:51.440
But it also covers one.

54:51.440 --> 54:54.520
So it doesn't allow you to include, there's an actual association, all right?

54:54.520 --> 54:58.640
So the new method gets the best of both worlds, this prediction powered inference.

54:58.640 --> 55:04.880
It forms a confidence interval, which is guaranteed to cover the truth, with 95% probability.

55:04.880 --> 55:12.040
But it's also much smaller than the, it uses the predictions, but it corrects them, all right?

55:12.040 --> 55:15.080
And I think the most fun thing to show you, I'm going to skip that slide,

55:15.080 --> 55:19.200
is just the examples that we've been applying this to, and we have a paper that does these.

55:19.200 --> 55:22.880
Here's voting, here's a ballot, here's a messed up ballot.

55:22.880 --> 55:24.440
So this was a San Francisco election.

55:24.440 --> 55:27.640
People use computer vision to look at all the ballots and make a prediction,

55:27.640 --> 55:30.280
whether it was yes or no, all right?

55:30.280 --> 55:33.240
And now you can feed that in and do an eight analysis on that.

55:33.240 --> 55:37.200
And you can see the little gold region there, it's a little small confidence interval,

55:37.200 --> 55:38.640
it's just missing the truth.

55:38.640 --> 55:41.960
And again, I don't know why some things are not coming out here.

55:41.960 --> 55:47.640
Our new interval is the green one, and then there's a missing, a classical one there that's much

55:47.640 --> 55:49.960
larger, but again, covers the truth.

55:49.960 --> 55:53.400
This is counting spiral galaxies, you know, there's some hand labeled,

55:53.400 --> 55:55.360
here's the spiral galaxy, here's not.

55:55.360 --> 56:01.040
And again, you can see the small confidence interval,

56:01.040 --> 56:04.000
if you use the computer vision algorithm, but it's not covering the truth.

56:04.000 --> 56:05.960
And again, we cover the truth.

56:05.960 --> 56:08.160
And I think this was the last one I wanted to show.

56:08.160 --> 56:10.160
Yeah, here's a California census.

56:10.160 --> 56:13.200
The estimate is a logistic regression coefficient of income when predicting

56:13.200 --> 56:15.480
whether a person has private health insurance.

56:15.480 --> 56:17.760
And there was a machine learning algorithm run on that.

56:17.760 --> 56:21.200
You can see the tiny little confidence interval, it's very, very sure and dead wrong.

56:22.200 --> 56:25.720
Okay, I hope this conveys, you all kind of knew this, that, you know,

56:25.720 --> 56:29.520
very accurate machine learning models can still lead to completely wrong inferences.

56:29.520 --> 56:33.600
Okay, I just don't think my machine learning colleagues get that, but it definitely can.

56:33.600 --> 56:36.040
And so, but you can get the best of both worlds.

56:36.040 --> 56:39.120
And I think I'm going to skip all the way to this last slide here and

56:39.120 --> 56:41.520
just show you roughly how this happens.

56:41.520 --> 56:45.080
It's kind of like a bias correction procedure, but it's not quite that.

56:45.080 --> 56:50.800
All right, so there is a bias between the predicted parameter using all the predictive data

56:51.000 --> 56:52.840
and the true parameter, theta star.

56:52.840 --> 56:55.520
That bias is a population level quantity.

56:55.520 --> 56:59.600
If you had the whole population, you could just write it down as a number, right?

56:59.600 --> 57:02.920
There are ways to estimate bias, like the bootstrap.

57:02.920 --> 57:05.760
And you could take that estimate and correct your estimate.

57:05.760 --> 57:07.360
That's done a lot.

57:07.360 --> 57:12.040
We have a different idea, which is that we take that quantity, that bias,

57:12.040 --> 57:14.040
or we call it in general rectifier.

57:14.040 --> 57:16.840
And we don't just estimate it, we'd put a confidence interval on it.

57:16.840 --> 57:19.520
We get all the possible values of that correction.

57:19.520 --> 57:23.280
Now we take the original predictive quantity, which is wrong, and

57:23.280 --> 57:26.440
we correct it with all the possible corrections.

57:26.440 --> 57:29.400
That leads to that green region there, which is a confidence interval on the

57:29.400 --> 57:31.320
corrected predictions.

57:31.320 --> 57:34.920
All right, and then our theorem at the very bottom of the page shows that we

57:34.920 --> 57:36.320
were good statisticians.

57:36.320 --> 57:40.280
The probability that this new confidence interval covers the truth is bigger than

57:40.280 --> 57:41.880
or equal to 1 minus alpha.

57:41.880 --> 57:46.000
And it's much, much smaller than if you've forgotten all the predictions altogether.

57:46.000 --> 57:48.640
Okay, so I put that up there just at the end of an economics talk.

57:48.640 --> 57:51.800
There's more statistics, but I just think many people in the room are already

57:51.800 --> 57:53.480
thinking about this and working on it.

57:53.480 --> 57:54.680
It is one of the critical issues.

57:54.680 --> 57:57.880
If you're going to do science with machine learning, you've got to face this.

57:57.880 --> 58:00.720
You've got to be a good statistician while exploiting the advantages of the

58:00.720 --> 58:02.080
machine learning paradigm.

58:02.080 --> 58:04.400
And I think this is a step towards doing that.

58:04.400 --> 58:06.600
All right, so that's my last slide, and that's the slide I had up earlier.

58:06.600 --> 58:10.760
I just wanted to kind of remind you of the big picture of the more provocative

58:10.760 --> 58:12.800
issues.

58:12.800 --> 58:14.760
This to me has kind of been a no-brainer.

58:14.760 --> 58:16.800
I just, what's happening this era?

58:16.840 --> 58:19.600
Well, it's just statistics and computer science and econ and all.

58:19.600 --> 58:21.160
And we're being good engineers.

58:21.160 --> 58:25.480
We're trying to deliver artifacts that will help humans and how to do that well,

58:25.480 --> 58:27.880
like previous generations of engineers.

58:27.880 --> 58:32.120
And this kind of Silicon Valley hype thing of, we've discovered this great

58:32.120 --> 58:35.560
thing called AI, and it suddenly, we've got to worry about all the things that's

58:35.560 --> 58:37.000
going to happen because of that.

58:37.000 --> 58:39.240
It just, to me, wrong.

58:39.240 --> 58:40.240
Thank you.

58:40.240 --> 58:50.840
I hope, yeah, I knew.

58:50.840 --> 58:52.520
I shouldn't have picked on you, Art.

58:52.520 --> 58:53.680
You have a better idea?

58:53.680 --> 58:58.680
Well, I have a paper where there's an island of really high quality gold

58:58.680 --> 59:03.160
standard data and an ocean of data where you're not quite sure of the quality.

59:03.160 --> 59:07.880
And then we sort of do a shrinkage of one onto the other.

59:07.880 --> 59:12.440
But we just got point estimates so we didn't get a confidence interval.

59:12.440 --> 59:13.240
Yeah.

59:13.240 --> 59:16.080
I mean, so this is a little bit like the semi-supervised paradigm.

59:16.080 --> 59:19.480
So we have an ocean of labeled data and, sorry, an ocean.

59:19.480 --> 59:23.440
We have a small pool of labeled data and an ocean of unlabeled data.

59:23.440 --> 59:25.480
The machine learning person says, oh, yeah, that's similar to supervised.

59:25.480 --> 59:26.240
No.

59:26.240 --> 59:30.520
We're using the unlabeled data to find a confidence interval to correct the, sorry,

59:30.520 --> 59:31.600
we're using the, got it wrong.

59:31.600 --> 59:34.680
The labeled data to find a confidence interval to correct the unlabeled data and

59:34.680 --> 59:37.560
get a confidence interval out of the whole thing.

59:37.560 --> 59:40.080
But yes, I think I was aware of that work of yours.

59:40.080 --> 59:43.720
And let me just say this is not, none of this is ever new.

59:43.720 --> 59:48.120
Statisticians in kind of small data census work did things like this.

59:48.120 --> 59:50.200
And semi-parametric statisticians did some too.

59:50.200 --> 59:54.600
So this is, again, there's always somebody who did it probably in the 1950s in

59:54.600 --> 01:00:03.560
statistics, or Art, or Brad Efron or what, it's always, any other?

01:00:03.560 --> 01:00:04.640
Yes, there's two over here.

01:00:05.640 --> 01:00:10.840
As you pointed out, there's been a lot of attention around uncertainty,

01:00:10.840 --> 01:00:15.880
quantification, and similar, you know, similar moves in that direction lately in

01:00:15.880 --> 01:00:17.360
the machine learning space, which is great.

01:00:19.360 --> 01:00:24.360
What does that mean for the future of applied Bayesian statistics and, you know,

01:00:24.360 --> 01:00:26.400
MCMC, that sort of thing?

01:00:26.400 --> 01:00:31.080
Given that it's computationally less tractable than a lot of modern machine

01:00:31.080 --> 01:00:34.040
learning training techniques, is there still a place for it?

01:00:34.440 --> 01:00:36.560
Okay, yeah, good, thank you for asking.

01:00:36.560 --> 01:00:40.720
Yeah, no, I tend to be a Bayesian, as with most statisticians, sometimes I'm a

01:00:40.720 --> 01:00:42.400
Bayesian, sometimes I'm not.

01:00:42.400 --> 01:00:45.560
And I'm a Bayesian when I'm working with a scientist over two or three years.

01:00:46.600 --> 01:00:50.520
Because I'm trying to kind of get out the knowledge that they have and use it.

01:00:50.520 --> 01:00:52.160
All right, and that's a prior.

01:00:52.160 --> 01:00:55.600
And so why would I not do that if I'm going to work with them a long time?

01:00:55.600 --> 01:00:58.800
I'm a frequentist when I'm trying to produce a piece of software that

01:00:58.800 --> 01:01:00.800
people all over the world will use.

01:01:00.800 --> 01:01:02.520
Because I'm not going to work with them and get the prior.

01:01:02.520 --> 01:01:05.240
I'm just going to put it out there and I want to put a stamp certificate that 99%

01:01:05.240 --> 01:01:07.800
of the time they're going to work for whoever uses it.

01:01:07.800 --> 01:01:09.560
That's the two perspectives I have.

01:01:09.560 --> 01:01:11.560
Now, a little more nuance to that.

01:01:12.840 --> 01:01:15.240
Bayesian way to structure models is very nice.

01:01:15.240 --> 01:01:19.840
You get hierarchies, you get sharing of shrinkage, social network kind of things

01:01:19.840 --> 01:01:21.880
or naturally Bayesian.

01:01:21.880 --> 01:01:26.720
I think that Brad Efron, who has been the luminary in statistics here at

01:01:26.720 --> 01:01:29.160
Stanford but worldwide, had it right.

01:01:29.160 --> 01:01:32.120
Which is that you often will go into a problem, you think Bayesian.

01:01:32.120 --> 01:01:34.480
You start to structure the model, think about what I could know,

01:01:34.480 --> 01:01:38.200
what would be together with what, and then you become frequentist.

01:01:38.200 --> 01:01:40.360
You say, I'm going to do empirical Bayes.

01:01:40.360 --> 01:01:43.360
I'm not going to just run the Bayesian MCMC paradigm.

01:01:43.360 --> 01:01:45.400
I'm going to at some point just say, okay, there's some things I can estimate.

01:01:45.400 --> 01:01:47.440
I can plug them in at the Bayesian procedure and

01:01:47.440 --> 01:01:49.160
then I'll get the benefit of both worlds.

01:01:49.160 --> 01:01:51.200
Now, I totally agree.

01:01:51.200 --> 01:01:56.360
So a lot of the things you saw here have a kind of an empirical Bayes interpretation.

01:01:56.360 --> 01:02:00.040
And a lot of the, but it's true, the conformal things in the uncertainty

01:02:00.040 --> 01:02:03.160
you're talking about don't necessarily, maybe a manual could correct me there.

01:02:03.160 --> 01:02:06.960
But those are kind of pure hardcore frequentist at some level.

01:02:06.960 --> 01:02:10.680
But I tend to, when I would use those in practice, I would probably have not just

01:02:10.680 --> 01:02:13.040
one conformal predictor over here, I'd have another one over here, another one over

01:02:13.040 --> 01:02:16.120
here, I would want to shrink them towards each other, I want to have them related.

01:02:16.120 --> 01:02:17.840
Because in real domains, if you really start to scale,

01:02:17.840 --> 01:02:21.040
the Bayesian way of thinking helps you immensely.

01:02:21.040 --> 01:02:24.760
So that's funny, Bayesian frequentists do conflict, but

01:02:24.760 --> 01:02:26.080
it's like wave particle duality.

01:02:26.080 --> 01:02:28.480
It's kind of my metaphor, right?

01:02:28.480 --> 01:02:31.520
Waves and particles are both correct, and they conflict a little bit.

01:02:31.520 --> 01:02:33.440
But if you throw out one and just use the other one,

01:02:33.440 --> 01:02:34.800
you're gonna do bad physics.

01:02:34.800 --> 01:02:38.440
And same thing with statistics, yeah.

01:02:38.440 --> 01:02:43.360
Professor, you were emphasizing the importance of forming collectives in solving

01:02:43.360 --> 01:02:44.760
the problems.

01:02:44.760 --> 01:02:48.320
And I'm very curious to know how you think about how those

01:02:48.320 --> 01:02:53.160
collectives can actually be formed to pursue a goal.

01:02:53.160 --> 01:02:56.480
Especially because, I mean, I don't know if a principle or

01:02:56.480 --> 01:03:00.600
a platform is required to kind of create the market where agents can actually trust

01:03:00.600 --> 01:03:02.000
that and cooperate.

01:03:02.000 --> 01:03:06.960
Or do you actually feel like a decentralized kind of a network is possible?

01:03:06.960 --> 01:03:09.840
That's a fantastic question, I'm delighted to have it.

01:03:09.840 --> 01:03:13.800
And just for the young people in the room, I hope you see the questions like that

01:03:13.800 --> 01:03:16.880
are kind of the thing of the era, and they're hard.

01:03:16.880 --> 01:03:19.000
I don't have an answer to your question, it's gonna be the short answer.

01:03:20.440 --> 01:03:23.480
The colleagues I had on that paper, the social science colleagues,

01:03:23.480 --> 01:03:27.000
they talk a lot about new models for democracy.

01:03:28.200 --> 01:03:32.240
And they emphasize that democracies tend to arise when you have multiple layers of

01:03:32.240 --> 01:03:36.320
like bring 200 people together, get some consensus, take 200 people here,

01:03:36.320 --> 01:03:39.280
get some consensus, and put the consensus together.

01:03:39.280 --> 01:03:41.440
And form cities and countries and all that.

01:03:41.440 --> 01:03:43.600
That's what humans have done throughout history.

01:03:43.600 --> 01:03:48.040
And we've had this experiment now that we have this thing called Twitter.

01:03:48.040 --> 01:03:51.680
And we're assuming that it's all good that all of us talk all the time.

01:03:51.680 --> 01:03:53.880
Or that we all listen to one person.

01:03:53.880 --> 01:03:56.600
And those are terrible ways to do democracy.

01:03:56.600 --> 01:03:58.560
So there are experiments that have been underway for

01:03:58.560 --> 01:04:00.240
quite some time for people like that.

01:04:00.240 --> 01:04:03.360
Like those famous examples in Taiwan where they have a legislator,

01:04:03.360 --> 01:04:07.840
which is using lots of data analysis together with kind of structured assemblies

01:04:07.840 --> 01:04:11.800
of ways to kind of come to coherent decisions and to get consensus.

01:04:11.800 --> 01:04:14.400
And Ireland has used this, and their latest this,

01:04:14.400 --> 01:04:18.040
they kind of legalized abortion at some point, that's very hard to do in Ireland.

01:04:18.040 --> 01:04:21.200
They did it partly because of these new assemblies, new structures.

01:04:21.200 --> 01:04:25.000
So I love this, people thinking about out of the box of new mechanisms that

01:04:25.000 --> 01:04:29.760
bring together visibility of, but it's still among relatively small numbers of

01:04:29.760 --> 01:04:31.920
people, that's really critical.

01:04:31.920 --> 01:04:35.000
And I think that the technologists who just built the YouTubes and

01:04:35.000 --> 01:04:38.840
the Facebooks and all that, were trying to do this experiment on human beings.

01:04:38.840 --> 01:04:41.120
So it was just destined to fail.

01:04:41.120 --> 01:04:43.120
The big broadcast channels were terrible.

01:04:43.120 --> 01:04:46.680
We want communities and we got to think about a structure of those.

01:04:46.680 --> 01:04:49.800
So I don't have much more to say about that other than how do you form collectives

01:04:49.800 --> 01:04:52.680
and support them and make them healthy and all that is hugely interesting and

01:04:52.680 --> 01:04:55.720
important, and there are social scientists who spend their life doing this.

01:04:55.720 --> 01:04:58.560
This is definitely not just a technology issue.

01:04:58.560 --> 01:05:00.480
We need to both cooperate and listen and

01:05:00.480 --> 01:05:02.640
have a dialogue with those kind of social scientists.

01:05:04.160 --> 01:05:05.760
There's many others we should cooperate with, but

01:05:05.760 --> 01:05:07.720
I think that's a particularly pregnant one.

01:05:07.720 --> 01:05:10.160
Economics certainly talks about collaborative things.

01:05:10.160 --> 01:05:13.560
There's cooperative game theory and how do coalitions form.

01:05:13.560 --> 01:05:17.120
But it's a little bit dry, and maybe Hido can help me a little bit with kind of,

01:05:17.120 --> 01:05:18.400
maybe there's more to it.

01:05:18.400 --> 01:05:21.040
But it's a little bit about how do I do negotiation and

01:05:21.040 --> 01:05:23.440
get the most money out of the deal I can and so on.

01:05:23.440 --> 01:05:25.000
You need to line the incentives.

01:05:25.000 --> 01:05:27.080
Yeah, or line with incentives and so on.

01:05:27.080 --> 01:05:29.320
But that just means that we just haven't thought about it enough.

01:05:29.320 --> 01:05:32.520
And for the young people in the room, wow, that's a great topic to think about.

01:05:32.520 --> 01:05:36.400
How do I start to structure collaborative efforts in data-oriented ways?

01:05:36.400 --> 01:05:38.040
So I think economists didn't have enough kind of,

01:05:38.040 --> 01:05:39.960
they talk about communication signaling, but

01:05:39.960 --> 01:05:42.760
they didn't really have enough data to kind of really signal interesting things,

01:05:42.760 --> 01:05:45.120
and do it in adaptive interesting ways.

01:05:45.120 --> 01:05:47.760
So let me just lean in again to this, there's a lot of young people in the room.

01:05:47.760 --> 01:05:50.200
This is the most exciting era to be in.

01:05:50.200 --> 01:05:52.240
The previous eras kind of gave us greaty to send and

01:05:52.240 --> 01:05:54.840
gave us networks and all that and all these tools.

01:05:54.840 --> 01:05:57.160
And now they threw them out there in the world and they kind of work and

01:05:57.160 --> 01:05:58.120
they kind of don't.

01:05:58.120 --> 01:06:00.640
We got better commerce, we got better transmission.

01:06:00.640 --> 01:06:01.960
And we can sort of fix all those.

01:06:01.960 --> 01:06:05.680
We can also think a lot more about, wow, new things, good things could happen if

01:06:05.680 --> 01:06:07.480
we start to think in the right way.

01:06:07.480 --> 01:06:09.520
And what problems are needed to do that?

01:06:09.520 --> 01:06:13.120
Don't just work on self-driving cars and whatever, or

01:06:13.120 --> 01:06:17.720
make Facebook advertisements better, work on problems that you believe

01:06:17.720 --> 01:06:21.640
in, and there are plenty of them, but bring these two fields together though.

01:06:21.640 --> 01:06:24.160
Don't just think of yourself as a system builder.

01:06:24.920 --> 01:06:30.960
What up guy?

01:06:30.960 --> 01:06:32.800
I don't know,

01:06:32.800 --> 01:06:33.080
I don't know,

01:06:33.080 --> 01:06:37.560
I think my qury just made a living.

01:06:37.560 --> 01:06:38.600
Okay, thank you, bye.

01:06:38.600 --> 01:06:41.620
I will just let you guys go and get off the car.

01:06:41.620 --> 01:06:42.220
Bye.


1
00:00:00,000 --> 00:00:10,680
So our closing event is a special lecture given by Mike Jordan who almost I think does

2
00:00:10,680 --> 00:00:13,960
not need an introduction but I'll still give one.

3
00:00:13,960 --> 00:00:20,080
So he's a Pei Hong Chen Distinguished Professor at UC Berkeley and Mike has been a leader

4
00:00:20,080 --> 00:00:26,880
in the computational and mathematical study of learning for a long time and he's achieved

5
00:00:26,880 --> 00:00:33,160
such prominence that I think in 2016 he's been named one of the most influential computer

6
00:00:33,160 --> 00:00:39,360
scientists on Earth and so I'm just looking at his computer right now or the most influential

7
00:00:39,360 --> 00:00:41,160
computer scientist.

8
00:00:41,160 --> 00:00:47,280
He's been recognized for his work by many contributions, many awards as a member of

9
00:00:47,280 --> 00:00:53,640
the National Academy of Science, of the National Academy of Engineering, of the Royal Society.

10
00:00:53,640 --> 00:00:59,680
He was the inaugural winner of the World Laureates Award last year in 2022 and he receives

11
00:00:59,680 --> 00:01:03,520
a John von Neumann Medal from IEEE in 2020.

12
00:01:03,520 --> 00:01:08,560
If I were to list all these awards I think we would be here for a long, long time.

13
00:01:08,560 --> 00:01:12,840
But I would say that personally there are three things that amaze me about Mike Jordan.

14
00:01:12,840 --> 00:01:14,960
The first is his range of interest.

15
00:01:14,960 --> 00:01:20,160
They span an enormous array of fields that goes all the way from computer scientists

16
00:01:20,160 --> 00:01:27,880
to statistics to control theory, signal processing, mathematics, information theory, cognitive

17
00:01:27,880 --> 00:01:33,160
science and now economics and maybe we'll hear about a bit about this today.

18
00:01:33,160 --> 00:01:39,560
And I think the range and the breadth of his interest is just very unique.

19
00:01:39,560 --> 00:01:42,240
There's another thing that is unique about Mike.

20
00:01:42,240 --> 00:01:47,480
He's his track record of training the next generation of students.

21
00:01:47,480 --> 00:01:53,720
I think if you look at, I think he has a very impressive CV, but I think if you look at

22
00:01:53,720 --> 00:01:59,480
the CV the thing that impresses me the most is the name of his grad students because your

23
00:01:59,480 --> 00:02:07,440
students, and we have one right behind you, Lee-Wa, we have another one right there so

24
00:02:07,440 --> 00:02:13,640
we have already a lot of various, they are actually, they make up the who's who in machine

25
00:02:13,640 --> 00:02:14,640
learning today.

26
00:02:14,640 --> 00:02:22,280
And so if you open who's who in machine learning and ask the question, is this one of Mike's

27
00:02:22,280 --> 00:02:23,280
student?

28
00:02:23,280 --> 00:02:29,600
The likelihood of a yes is very high and so, and I think he's done this by fostering an

29
00:02:29,600 --> 00:02:35,720
environment of inclusivity and curiosity that has really moved the entire field.

30
00:02:35,720 --> 00:02:39,520
And so I think the whole field is grateful for this.

31
00:02:39,520 --> 00:02:42,640
The third observation is more personal.

32
00:02:42,640 --> 00:02:47,040
I think Mike's age is public information, at least if Wikipedia is correct.

33
00:02:47,040 --> 00:02:54,360
So you've been at the top of your game for a long, long time and this is really unique.

34
00:02:54,360 --> 00:03:00,000
And the way I think about Mike in the evening is I think it's a kind of the Roger Federer

35
00:03:00,000 --> 00:03:06,920
of science, someone who has been dominating the circuit for a very, very long time with

36
00:03:06,920 --> 00:03:10,040
no signs of slowing down.

37
00:03:10,040 --> 00:03:13,120
Mike, welcome to Stanford.

38
00:03:13,120 --> 00:03:21,880
All right, thank you, that was perhaps the most fun introduction I've had ever.

39
00:03:21,880 --> 00:03:25,640
When I next time introduce Emmanuel, which will happen, I'm sure I'm gonna have to think

40
00:03:25,640 --> 00:03:27,720
about the right metaphor.

41
00:03:27,720 --> 00:03:30,720
Maybe someone has an idea helped me out there.

42
00:03:30,720 --> 00:03:35,080
I gotta get my mind off that because it's a fun thing to think about.

43
00:03:35,080 --> 00:03:36,240
So I'm a pleasure to be here.

44
00:03:36,240 --> 00:03:41,720
I am a data scientist and in fact this kind of quote that I was like an influential computer

45
00:03:41,720 --> 00:03:45,680
scientist is kind of funny to me because I'm trained as a control theory statistician and

46
00:03:45,680 --> 00:03:47,280
I was never a computer scientist.

47
00:03:47,280 --> 00:03:50,840
I embrace it because of the entrepreneurial spirit in computer science and just let's

48
00:03:50,840 --> 00:03:51,840
try everything.

49
00:03:51,840 --> 00:03:52,840
I love that.

50
00:03:52,840 --> 00:03:56,160
And I found less of that in control theory and statistics, so that's great.

51
00:03:56,160 --> 00:03:57,680
But intellectually I'm a data scientist.

52
00:03:57,680 --> 00:04:02,400
I really want to think about how data and inference can inform real world decision making and

53
00:04:02,400 --> 00:04:03,680
I think that's where it's at.

54
00:04:03,680 --> 00:04:08,560
I think that it's in the first time in my career that sort of all of campus agrees.

55
00:04:08,560 --> 00:04:12,440
It's sort of a, it's just not the truth.

56
00:04:12,440 --> 00:04:15,920
Technologist inside of a computer scientist or statistician wasn't enough.

57
00:04:15,920 --> 00:04:17,760
This is much more fun.

58
00:04:17,760 --> 00:04:21,720
And indeed it's economics, which is the most thrilling to me right now, the connections

59
00:04:21,720 --> 00:04:22,720
to economics.

60
00:04:22,720 --> 00:04:26,000
That's really what I want to mostly convey tonight is why I think that's thrilling

61
00:04:26,000 --> 00:04:28,280
and important and so on.

62
00:04:28,280 --> 00:04:31,760
The elephant in the room is this thing called AI and I've never thought of myself as an

63
00:04:31,760 --> 00:04:32,760
AI researcher.

64
00:04:32,760 --> 00:04:36,440
I've never aspired to the Frankenstein-like thing of create some thing.

65
00:04:36,440 --> 00:04:37,720
And I kind of want to say why.

66
00:04:37,720 --> 00:04:41,880
Not only didn't aspire to it, I don't think it's right, but it is what everyone's talking

67
00:04:41,880 --> 00:04:42,880
about.

68
00:04:42,880 --> 00:04:44,320
So I want to say a few things about it.

69
00:04:44,320 --> 00:04:50,200
So first of all, the thing that triggered all of this was back propagation, gradient

70
00:04:50,200 --> 00:04:53,200
descent in layered neural networks.

71
00:04:53,200 --> 00:04:58,240
There's all these other ideas along the way, unsupervised this and that, which didn't really

72
00:04:58,240 --> 00:05:01,280
quite pan out, but back propagation had this huge impact.

73
00:05:01,280 --> 00:05:07,160
Dave Rummelhardt was my advisor at UCSD and he developed that.

74
00:05:07,160 --> 00:05:11,000
It's just gradient descent, so you can't say he invented it, but he invented the idea

75
00:05:11,000 --> 00:05:13,600
of doing it in layered neural networks and applying it to all kinds of problems.

76
00:05:13,600 --> 00:05:15,200
And he took about a year to do that.

77
00:05:15,200 --> 00:05:17,760
It was not trivial for him.

78
00:05:17,760 --> 00:05:19,200
He was not trying to be an AI person.

79
00:05:19,200 --> 00:05:22,320
He was just trying to understand learning.

80
00:05:22,320 --> 00:05:28,920
I think he'd be somewhat shocked that suddenly that becomes AI to this era.

81
00:05:28,920 --> 00:05:34,760
So what I think is happening really is not that we have a new technology, this new brilliant

82
00:05:34,760 --> 00:05:37,560
idea, AI, and then we start applying it everywhere.

83
00:05:37,560 --> 00:05:40,160
I don't think that's the right way to think about it.

84
00:05:40,160 --> 00:05:45,680
And so I think you need to go back in history a little bit and think about engineering fields

85
00:05:45,680 --> 00:05:46,680
that have emerged.

86
00:05:46,680 --> 00:05:50,720
So like in the 40s and 50s, chemical engineering became a thing.

87
00:05:50,720 --> 00:05:53,920
The name was actually used, I kind of looked into it a little bit more.

88
00:05:53,920 --> 00:05:59,720
Already by 1890, I think there was a department of chemical engineering at MIT.

89
00:05:59,720 --> 00:06:05,520
But chemical engineering really was a very simple chemistry, kind of done its bigger

90
00:06:05,520 --> 00:06:07,200
scale than before.

91
00:06:07,200 --> 00:06:12,680
But polymers and all the kind of things that have triggered the revolution that we're all

92
00:06:12,680 --> 00:06:15,600
living in, that happened in the 40s and 50s.

93
00:06:15,600 --> 00:06:19,560
Before that, there was quantum chemistry and there was fluid dynamics and there was thermodynamics

94
00:06:19,560 --> 00:06:24,200
and also there was a lot of deep understanding of the phenomenon.

95
00:06:24,200 --> 00:06:28,960
And people then were able to start to envisage, what if I take the laboratory test tube experiment

96
00:06:28,960 --> 00:06:33,040
of how you put molecules together, which I do understand because of the quantum chemistry,

97
00:06:33,040 --> 00:06:37,600
and I do that at a huge scale in a field somewhere, will that work?

98
00:06:37,600 --> 00:06:39,040
And of course, it didn't really work.

99
00:06:39,040 --> 00:06:42,240
Often those things would explode, often they just wouldn't deliver a product and so on

100
00:06:42,240 --> 00:06:43,240
so forth.

101
00:06:43,240 --> 00:06:46,560
But over a 20-year period in the 40s and 50s, it kind of started to get worked out.

102
00:06:46,560 --> 00:06:52,000
And an engineering field emerged that had huge impact on all of our lives.

103
00:06:52,000 --> 00:06:54,920
Electrical engineering, I know less about the history, but obviously there was Maxwell's

104
00:06:54,920 --> 00:06:58,720
equations before there was electrical engineering, so it was a full understanding of the phenomenon

105
00:06:58,720 --> 00:06:59,720
at some level.

106
00:06:59,720 --> 00:07:04,480
But it wasn't clear how to bring electricity into homes, how to make it safe, how to do

107
00:07:04,480 --> 00:07:07,120
communication on top of the waves, and so on and so forth.

108
00:07:07,120 --> 00:07:11,240
So a whole field emerged, which we now call electrical engineering, that did all that

109
00:07:11,240 --> 00:07:12,680
in the early part of the past century.

110
00:07:12,680 --> 00:07:15,640
It took a couple of decades again.

111
00:07:15,640 --> 00:07:17,000
So I think that's what's happening right now.

112
00:07:17,000 --> 00:07:19,480
We have a new engineering field emerging.

113
00:07:19,480 --> 00:07:26,440
I wouldn't call it AI, but it's a field that's based on flows of data, networks, flows, inferential

114
00:07:26,440 --> 00:07:32,440
ideas, large-scale decision-making, cooperative endeavor, building transportation systems,

115
00:07:32,440 --> 00:07:38,800
commerce systems, healthcare systems, it's all part of this engineering field.

116
00:07:38,800 --> 00:07:40,000
That's really what's happening.

117
00:07:40,000 --> 00:07:44,840
And so it's the first engineering field that has got, as its objects of study, not just

118
00:07:44,840 --> 00:07:51,480
bits and information and atoms and laws of physics, it has humans involved, critically.

119
00:07:51,480 --> 00:07:55,200
So utilities and aspirations and so on.

120
00:07:55,200 --> 00:08:00,520
And so economics has minimally got to be involved, but the rest of the social sciences as well.

121
00:08:00,520 --> 00:08:02,720
And the implications are vast.

122
00:08:02,720 --> 00:08:06,920
So actually, that's the phenomenon, and it's going to take 20 or so years.

123
00:08:06,920 --> 00:08:10,800
The difference, though, with these others is that there was a deep understanding of some

124
00:08:10,800 --> 00:08:12,360
underlying phenomenon there.

125
00:08:12,360 --> 00:08:13,360
We don't have that now.

126
00:08:13,360 --> 00:08:18,200
We do not understand intelligence, I can assure you.

127
00:08:18,200 --> 00:08:24,600
So we're calling it AI as if we got this understanding and then it leads to technology, and I think

128
00:08:24,600 --> 00:08:27,480
that's kind of backwards.

129
00:08:27,480 --> 00:08:29,280
So let's just say, why did this happen?

130
00:08:29,280 --> 00:08:36,080
Well, first of all, there was McCarthy and so on in the 50s who invented this terminology,

131
00:08:36,080 --> 00:08:37,480
and for good reason.

132
00:08:37,480 --> 00:08:43,040
There was a philosophical aspiration, almost.

133
00:08:43,040 --> 00:08:46,760
There had been discussions of mind and body, and now we have a computer, it has software

134
00:08:46,760 --> 00:08:50,120
and hardware, it looks like mind and body, and it looks like we can now make headway

135
00:08:50,120 --> 00:08:51,120
on that.

136
00:08:51,120 --> 00:08:52,480
And let's talk about putting thought in a computer.

137
00:08:52,480 --> 00:08:55,080
That's a really interesting thing to talk about.

138
00:08:55,080 --> 00:08:58,440
And of course, people got excited about that notion and worked on it.

139
00:08:58,440 --> 00:09:02,520
We don't have thought in a computer to this day, and it's not clear why we really care

140
00:09:02,520 --> 00:09:03,520
in some sense.

141
00:09:03,520 --> 00:09:10,120
It'll somehow emerge and we'll call it thought, but it's not clear what that means.

142
00:09:10,240 --> 00:09:12,440
And in the meantime, that's not what happened.

143
00:09:12,440 --> 00:09:14,480
What computers started to do was aid humans.

144
00:09:14,480 --> 00:09:16,080
They became complementary.

145
00:09:16,080 --> 00:09:20,800
Search engines and translation systems and all that aided our own intelligence and expanded

146
00:09:20,800 --> 00:09:25,400
it and networks expanded a planetary scale.

147
00:09:25,400 --> 00:09:30,080
So let's not call it McCarthy's version of intelligence for sure, but that aspiration

148
00:09:30,080 --> 00:09:31,080
still exists.

149
00:09:31,080 --> 00:09:37,360
And people who study psychology and neuroscience and core AI, whatever that is, they are working

150
00:09:37,360 --> 00:09:38,360
on that.

151
00:09:38,480 --> 00:09:40,480
It's a worthy thing.

152
00:09:40,480 --> 00:09:43,240
But we should be waiting for that because in the meantime, all these systems are being

153
00:09:43,240 --> 00:09:45,920
built in the real world that are having this huge impact.

154
00:09:45,920 --> 00:09:47,920
We should understand the phenomenon.

155
00:09:47,920 --> 00:09:48,920
All right.

156
00:09:48,920 --> 00:09:54,160
Now, the other part that happened, it wasn't really so much McCarthy, but others, it had

157
00:09:54,160 --> 00:09:56,160
to be autonomous.

158
00:09:56,160 --> 00:09:58,600
Why did the AI have to be autonomous?

159
00:09:58,600 --> 00:10:03,000
Well, if it's not autonomous, if it's tethered to me, it doesn't seem so intelligent.

160
00:10:03,000 --> 00:10:05,000
It's tethered to me.

161
00:10:05,000 --> 00:10:08,160
And if it's developed by vast numbers of humans, it's engineers who built something

162
00:10:08,160 --> 00:10:10,480
and it doesn't seem so intelligent.

163
00:10:10,480 --> 00:10:14,680
So it had to kind of be built by small numbers of people and it had to be all on its own.

164
00:10:14,680 --> 00:10:20,360
Now, that's a okay kind of science fiction-y kind of aspiration, but it's a bad idea for

165
00:10:20,360 --> 00:10:21,360
technology.

166
00:10:21,360 --> 00:10:23,560
You don't develop technology that way.

167
00:10:23,560 --> 00:10:25,920
You don't want self-driving cars to be autonomous.

168
00:10:25,920 --> 00:10:29,960
They should be highly networked so you think about the overall traffic system so you don't

169
00:10:29,960 --> 00:10:34,120
ever have an accident, just like air traffic control.

170
00:10:34,120 --> 00:10:36,680
You don't want autonomous airplanes.

171
00:10:36,680 --> 00:10:40,480
So there'll be a lot of cartoons in this talk.

172
00:10:40,480 --> 00:10:42,400
I don't mean it's never a good idea.

173
00:10:42,400 --> 00:10:45,480
So a burning building, I want an autonomous robot.

174
00:10:45,480 --> 00:10:50,920
Up on Mars, I want some double autonomy, but for most applications, I don't want the intelligence

175
00:10:50,920 --> 00:10:51,920
to be autonomous.

176
00:10:51,920 --> 00:10:58,640
I want it to be federated, linked in, transparent, cooperative, all those things.

177
00:10:58,640 --> 00:11:02,160
So I think this was a big mistake to add that to the list.

178
00:11:02,160 --> 00:11:06,080
I think it became kind of about bragging rights, look at my autonomous AI, how great

179
00:11:06,080 --> 00:11:08,680
it is and it's better than your autonomous AI.

180
00:11:08,680 --> 00:11:12,400
And again, this is all kind of fun and games for like 40 years, but it's no longer fun

181
00:11:12,400 --> 00:11:13,400
and games.

182
00:11:13,400 --> 00:11:16,120
It's actually going to hurt the planet.

183
00:11:16,120 --> 00:11:19,200
All right, so here's a counterpoint, which is that first of all, this is kind of maybe

184
00:11:19,200 --> 00:11:23,520
an obvious statement, but if we want to talk about intelligence, there's not one kind of

185
00:11:23,520 --> 00:11:24,520
intelligence.

186
00:11:24,520 --> 00:11:25,520
It's not just human intelligence.

187
00:11:25,520 --> 00:11:28,400
It's as much about the collective as it is about the individual.

188
00:11:28,400 --> 00:11:30,800
And an economist thinks this way all the time.

189
00:11:30,800 --> 00:11:37,280
They recognize that a market is composed of many small decisions by entities that don't

190
00:11:37,280 --> 00:11:38,760
have to be intelligent themselves.

191
00:11:38,760 --> 00:11:43,920
They just have to kind of know a demand curve and follow some of their nose.

192
00:11:43,920 --> 00:11:47,880
And you're not using huge intelligence within it, but the overall market becomes really

193
00:11:47,880 --> 00:11:48,880
intelligent.

194
00:11:48,880 --> 00:11:53,000
It could do things like bring food into cities, bring or shine at any scale for hundreds

195
00:11:53,000 --> 00:11:57,240
of years, and it can create all kinds of opportunities.

196
00:11:57,240 --> 00:12:00,800
And then there's like ant swarms that we talk a lot about, not an individual ant might

197
00:12:00,800 --> 00:12:03,680
not be so smart, but the swarm could do amazing things.

198
00:12:03,680 --> 00:12:06,160
So we're all aware of that, but too dimly.

199
00:12:06,160 --> 00:12:09,240
I don't think we understand that we could be creating new kinds of collectives that are

200
00:12:09,240 --> 00:12:13,240
really exciting, that do new things as human beings.

201
00:12:13,240 --> 00:12:15,720
That's what's opening up to me in the era.

202
00:12:15,720 --> 00:12:19,560
Not the super intelligence replacing a human, look at how great that is.

203
00:12:19,560 --> 00:12:24,600
All right, so in particular, if you're going to be a little less exuberant, but you're

204
00:12:24,600 --> 00:12:29,200
going to say, what are the goals for this emerging field?

205
00:12:29,200 --> 00:12:33,400
It's not make a super intelligence at a computer, and you're done.

206
00:12:33,400 --> 00:12:38,000
It's rather what is the overall object, like the factory in the field?

207
00:12:38,000 --> 00:12:39,440
Is it a transportation system?

208
00:12:39,440 --> 00:12:40,720
Is it a logistics chain?

209
00:12:40,720 --> 00:12:42,600
Is it a healthcare system?

210
00:12:42,600 --> 00:12:46,800
Is it a communication system designed for that level?

211
00:12:46,800 --> 00:12:50,320
And then think about what the components need to be and what data is needed and all that.

212
00:12:50,320 --> 00:12:54,880
It sounds more boring than a typical AI person's talk, which is, we'll solve intelligence

213
00:12:54,880 --> 00:12:57,760
and then the intelligence will solve climate change.

214
00:12:57,760 --> 00:13:01,760
That's a typical Silicon Valley thing to say.

215
00:13:01,760 --> 00:13:05,240
It sounds great and you get rid of it in the New York Times, but really to me, logistics

216
00:13:05,240 --> 00:13:10,080
chains and supply chains are much more exciting and interesting and important for human life

217
00:13:10,080 --> 00:13:12,080
and healthcare.

218
00:13:12,080 --> 00:13:14,480
And it's not that the AI is going to solve healthcare.

219
00:13:14,480 --> 00:13:19,760
It's us designing really good systems with good data science principles and economic

220
00:13:19,840 --> 00:13:20,840
principles.

221
00:13:20,840 --> 00:13:23,880
So I think I've said all this, mimicry is just not a good way to think about the implications

222
00:13:23,880 --> 00:13:24,880
of collectives.

223
00:13:24,880 --> 00:13:28,920
Autonomy is also maybe a losery.

224
00:13:28,920 --> 00:13:31,120
And so there might be new forms of collectives.

225
00:13:31,120 --> 00:13:32,120
Okay.

226
00:13:32,120 --> 00:13:36,640
So if you want to read a little bit more about this kind of philosophical ruminations, I

227
00:13:36,640 --> 00:13:42,080
wrote an article, our official that the Revlon hasn't happened yet, three years ago, and

228
00:13:42,080 --> 00:13:45,480
I still very much stand behind everything in there, even though we've had this kind of

229
00:13:45,520 --> 00:13:51,640
upswing and surprising chat GBT abilities, this was like about where's the data come

230
00:13:51,640 --> 00:13:52,640
from?

231
00:13:52,640 --> 00:13:53,640
What's the provenance?

232
00:13:53,640 --> 00:13:54,640
What is the bigger scope?

233
00:13:54,640 --> 00:13:55,640
And all that.

234
00:13:55,640 --> 00:13:59,120
And so if you want to read about that, and then there was a bunch of commentary by including

235
00:13:59,120 --> 00:14:03,000
Emmanuel Candace by some luminaries and it was quite a lot of fun.

236
00:14:03,000 --> 00:14:07,560
So and there was my response to those luminaries.

237
00:14:07,560 --> 00:14:11,840
And then with some colleagues, mostly social scientists all down here, we wrote a paper

238
00:14:11,840 --> 00:14:15,160
about two years ago called How AI Fails Is.

239
00:14:15,160 --> 00:14:18,480
And it's less about the kind of economics perspective that I was pushing up above and

240
00:14:18,480 --> 00:14:26,480
more about what are the implications for technology if you've got like autonomous systems being

241
00:14:26,480 --> 00:14:29,520
designed by small numbers of people.

242
00:14:29,520 --> 00:14:35,080
That kind of incentivizes entities like open AI, that they get vast amounts of money for

243
00:14:35,080 --> 00:14:40,000
a small number of people, they build this thing, and it's not for everybody.

244
00:14:40,000 --> 00:14:41,640
They control it.

245
00:14:41,640 --> 00:14:44,680
And it was supposed to be open, it's no longer open.

246
00:14:44,680 --> 00:14:50,840
And so this idea that AI is the future, it just has a natural tendency towards making

247
00:14:50,840 --> 00:14:54,040
it be in the hands of small numbers of people.

248
00:14:54,040 --> 00:14:57,800
And again, I think this article kind of gets into some of the social science reasons why

249
00:14:57,800 --> 00:15:00,120
that's just really a bad idea.

250
00:15:00,120 --> 00:15:02,480
And people pretend that it's not happening.

251
00:15:02,480 --> 00:15:07,240
It's all open and all that, but that's a pretense, it's just not true.

252
00:15:07,240 --> 00:15:11,840
So if we stop thinking about AI this way, I think it'll actually liberate us from that.

253
00:15:11,840 --> 00:15:14,880
All right, so again, I've already sort of said this, but just to lean in a little bit

254
00:15:14,880 --> 00:15:18,160
more, McCarthy had this imitative perspective.

255
00:15:18,160 --> 00:15:21,120
It was a great aspiration and still remains one.

256
00:15:21,120 --> 00:15:22,640
It's just not what's happened.

257
00:15:22,640 --> 00:15:28,200
What really happened was more like IA, that's Doug Engelbart there who kind of talked about

258
00:15:28,200 --> 00:15:29,960
technology, augment our intelligence.

259
00:15:29,960 --> 00:15:31,960
And for certain, it has.

260
00:15:31,960 --> 00:15:35,600
The search engine has augmented my intelligence more than just about any other piece of technology

261
00:15:35,600 --> 00:15:40,880
that I can think of, in addition to everybody's intelligence.

262
00:15:40,880 --> 00:15:44,720
And then this third bullet is kind of what I think is a better description of what's

263
00:15:44,720 --> 00:15:46,240
really emerging.

264
00:15:46,240 --> 00:15:47,440
It looks kind of like Internet of Things.

265
00:15:47,440 --> 00:15:50,640
They've got all these little devices around, they all send data around, and decisions are

266
00:15:50,640 --> 00:15:51,640
being made.

267
00:15:51,640 --> 00:15:54,280
It's all delocalized and everything.

268
00:15:54,280 --> 00:15:56,320
But Internet of Things was a little too computer sciencey.

269
00:15:56,320 --> 00:16:00,000
It wasn't thinking about the data and the inferences and the predictions and the people.

270
00:16:00,000 --> 00:16:03,280
It was just about put things on the Internet.

271
00:16:03,280 --> 00:16:06,520
But anyway, that is still the right spirit, and I think this is really what's happened.

272
00:16:06,520 --> 00:16:09,800
Even like the pandemic response of the planet.

273
00:16:09,800 --> 00:16:17,000
That was a engineering system that sort of didn't work okay, but we could do better.

274
00:16:17,000 --> 00:16:22,040
Now, if you go to an ML person or an AI person and say, okay, aren't you guys thinking about

275
00:16:22,040 --> 00:16:23,040
this?

276
00:16:23,040 --> 00:16:24,920
Is it all this classical AI stuff?

277
00:16:24,920 --> 00:16:25,920
And they say, no, no, no.

278
00:16:25,920 --> 00:16:26,920
We work on this.

279
00:16:26,920 --> 00:16:29,440
Here's, for example, federated learning.

280
00:16:29,440 --> 00:16:31,480
It's decentralized learning.

281
00:16:31,480 --> 00:16:36,560
So you have a server up there, and they're collecting data from a bunch of edge devices,

282
00:16:36,560 --> 00:16:41,240
and then they're analyzing the data centrally, and we're worried about privacy and all that.

283
00:16:41,240 --> 00:16:43,080
So we got the social stuff.

284
00:16:43,080 --> 00:16:46,240
This is our, we handled all the social stuff.

285
00:16:46,240 --> 00:16:47,240
All right.

286
00:16:47,240 --> 00:16:49,920
Now, I'm being a little bit, again, cartoonish here.

287
00:16:49,920 --> 00:16:55,280
But at terminology of federated learning, a number of groups were working on it, but

288
00:16:55,280 --> 00:16:59,160
it's a Google patented, and not patented, but it's a Google terminology, because Google

289
00:16:59,160 --> 00:17:03,360
wanted to collect a lot of data for their speech engines.

290
00:17:03,360 --> 00:17:06,200
And so everybody has cell phones and is talking on their phones.

291
00:17:06,200 --> 00:17:08,600
Let's just collect a lot of data from that.

292
00:17:08,600 --> 00:17:11,040
Let's worry about the compression.

293
00:17:11,040 --> 00:17:12,760
Let's get the gradients back cheaply.

294
00:17:12,760 --> 00:17:15,440
Let's also do some differential privacy, and that's the technical problem.

295
00:17:15,440 --> 00:17:18,800
If we solve that, wow.

296
00:17:18,800 --> 00:17:20,360
But what's missing in this picture?

297
00:17:20,360 --> 00:17:21,360
All right.

298
00:17:21,360 --> 00:17:24,960
Well, I'm going to give some examples of what's missing to make it more clear.

299
00:17:24,960 --> 00:17:28,520
But what's missing is that these are actual humans here, and they have their own values

300
00:17:28,520 --> 00:17:32,920
and goals and aspirations, and they want to join this collective for some reason.

301
00:17:32,920 --> 00:17:36,840
They don't want to just be assumed that they are in the collective because they want Google

302
00:17:36,840 --> 00:17:39,360
to build a bitter speech model.

303
00:17:39,360 --> 00:17:40,880
Okay.

304
00:17:40,880 --> 00:17:45,000
So the nodes are often people, and they value their data.

305
00:17:45,000 --> 00:17:48,960
And by data, I don't just mean where I went today and what was around me and all that.

306
00:17:48,960 --> 00:17:55,200
I mean, things I created, works of art, things I wrote, songs I wrote, et cetera, et cetera.

307
00:17:55,200 --> 00:17:56,200
That's my data.

308
00:17:56,880 --> 00:18:00,760
Stuff that's on the internet now that's being exploited by other companies, and I've lost

309
00:18:00,760 --> 00:18:01,880
all value.

310
00:18:01,880 --> 00:18:02,880
That's wrong.

311
00:18:02,880 --> 00:18:05,080
All right.

312
00:18:05,080 --> 00:18:09,640
So we need to talk about cost and benefits of these decentralized paradigms where learning

313
00:18:09,640 --> 00:18:10,800
is involved.

314
00:18:10,800 --> 00:18:14,440
So we need learning to wear mechanisms, and mechanisms that we're learning.

315
00:18:14,440 --> 00:18:17,520
Mechanism is an economics terminology, and I want to get into that.

316
00:18:17,520 --> 00:18:23,040
So I'm going to give some more kind of industrially, real-world examples, but as an academic, I

317
00:18:23,040 --> 00:18:27,360
needed to kind of think a little bit about what's happening academically.

318
00:18:27,360 --> 00:18:32,880
Are we kind of a set up for this emerging discipline, whatever you want to call it.

319
00:18:32,880 --> 00:18:35,120
And I'm not sort of sure we are.

320
00:18:35,120 --> 00:18:40,520
So the three disciplines, and it's not really the disciplines, it's the styles of thinking

321
00:18:40,520 --> 00:18:45,840
that I think are most important here, and I don't want to exclude anybody, but computer

322
00:18:45,840 --> 00:18:50,760
science certainly, the algorithms, the networks, and so on, statistics, and economics.

323
00:18:50,880 --> 00:18:55,280
Just to say there are pair-wise interactions among these fields for quite some time.

324
00:18:55,280 --> 00:18:57,200
Computer science meets statistics, that is machine learning.

325
00:18:57,200 --> 00:19:01,640
In fact, I would argue machine learning is just statistics with kind of a computer science

326
00:19:01,640 --> 00:19:03,520
way of thinking.

327
00:19:03,520 --> 00:19:07,760
Every time I see a new idea in machine learning, I know that it already exists in statistics,

328
00:19:07,760 --> 00:19:08,760
and I tell people that.

329
00:19:08,760 --> 00:19:12,920
They get mad at me, but eventually it kind of...

330
00:19:12,920 --> 00:19:18,480
And there's lots of ideas and statistics they don't yet know about, too.

331
00:19:18,520 --> 00:19:22,080
I could give lots of examples, but I won't.

332
00:19:22,080 --> 00:19:25,680
Statistics meets economics, that's econometrics, and I've got Hito and others in the audience

333
00:19:25,680 --> 00:19:28,240
who are masters of that.

334
00:19:28,240 --> 00:19:31,440
Well it's great, but it's kind of about measuring the economy.

335
00:19:31,440 --> 00:19:35,640
That's what the main goal has been, doing the causal inference to measure the economy.

336
00:19:35,640 --> 00:19:40,680
And it's less about algorithms and mechanisms and engineering kind of thing, artifacts.

337
00:19:40,680 --> 00:19:45,480
So it's had its important role, but it's missing that third leg.

338
00:19:45,480 --> 00:19:48,080
In economics meets computer science, that's called algorithmic game theory.

339
00:19:48,080 --> 00:19:50,880
That emerged 15, 20 years ago.

340
00:19:50,880 --> 00:19:54,880
It's very important field, study of auctions and combinatorial auctions and how they behave

341
00:19:54,880 --> 00:19:56,200
and incentives and all that.

342
00:19:56,200 --> 00:19:58,680
What's missing there is they have no statistics.

343
00:19:58,680 --> 00:20:01,960
They don't worry about gathering data and changing the preferences and learning them

344
00:20:01,960 --> 00:20:04,600
from as part of the auction and all that.

345
00:20:04,600 --> 00:20:10,000
So all three of these pairwise things exist, but they're critically missing the third leg.

346
00:20:10,000 --> 00:20:14,120
Now the interesting thing is if you go into an industry, and I spend a day a week at Amazon,

347
00:20:14,120 --> 00:20:17,760
and you look at any real world problem they are studying, like how do we provision, how

348
00:20:17,760 --> 00:20:21,720
do we interface with third party sellers, blah, blah, blah, there's always all three

349
00:20:21,720 --> 00:20:24,240
disciplines around the table.

350
00:20:24,240 --> 00:20:28,280
And just to add, there are always operations research people who already have kind of ingested

351
00:20:28,280 --> 00:20:33,000
all three disciplines, just to say, and control theorists and mathematicians and so on.

352
00:20:33,000 --> 00:20:38,080
So I don't mean to exclude anybody, but it's never one of those perspectives alone.

353
00:20:38,080 --> 00:20:39,920
That kills you if you just have one of those perspectives.

354
00:20:39,920 --> 00:20:40,920
You need all three.

355
00:20:40,920 --> 00:20:46,240
All right, here's a real world example that I've been involved in.

356
00:20:46,240 --> 00:20:49,040
So I'm a musician, I just end up being an academic.

357
00:20:49,040 --> 00:20:53,600
And I met up, I have a friend, Steve Stout.

358
00:20:53,600 --> 00:20:54,600
Someone introduced me at some point.

359
00:20:54,600 --> 00:21:03,280
Steve is a legendary producer, entrepreneur, well known in the hip hop and the Latin world

360
00:21:03,280 --> 00:21:05,240
and so on.

361
00:21:05,240 --> 00:21:12,160
And he and I kind of came together on this idea of modern data, modern systems, platforms

362
00:21:12,160 --> 00:21:14,840
should not just be about taking streaming bits.

363
00:21:14,840 --> 00:21:16,760
Music shouldn't just be about streaming.

364
00:21:16,760 --> 00:21:20,520
It should be about creating two and three-way markets.

365
00:21:20,520 --> 00:21:25,360
And so the idea that we originally sat down and talked about, and Steve is the CEO of

366
00:21:25,360 --> 00:21:28,800
now a company that has taken this and made it real.

367
00:21:28,800 --> 00:21:34,480
It's called UnitedMasters.com, or United Masters is the company.

368
00:21:34,480 --> 00:21:36,240
Basically provides a three-way market.

369
00:21:36,240 --> 00:21:42,960
So if you make music and now you can sign up with United Masters, they give you a record

370
00:21:42,960 --> 00:21:47,920
company in your pocket, you're able to kind of produce songs on your cell phone and upload

371
00:21:47,920 --> 00:21:55,120
them to United Masters and then they connect that to a market on the other side.

372
00:21:55,120 --> 00:21:59,040
So in particular, Steve has gone to the NBA.

373
00:21:59,040 --> 00:22:03,720
The NBA used to be streaming music from the record companies and they would pay the record

374
00:22:03,720 --> 00:22:07,680
companies a royalty and they might give some money back to Beyonce or whatever.

375
00:22:07,680 --> 00:22:11,280
But most musicians are not the big famous ones.

376
00:22:11,640 --> 00:22:16,560
In fact, if you look at the data, if you do some actual data science, today 95% of the

377
00:22:16,560 --> 00:22:21,120
songs being listened to in the United States played by people you've never heard of and

378
00:22:21,120 --> 00:22:25,040
they're probably between 16 and 20 years old and the song was probably recorded in the

379
00:22:25,040 --> 00:22:26,560
last six months.

380
00:22:26,560 --> 00:22:30,240
So everybody thinks we're all listening to the Beatles and Madonna or whatever.

381
00:22:30,240 --> 00:22:31,240
It's just not true.

382
00:22:31,240 --> 00:22:35,440
All right, so you think, wow, there's this wonderful market that's been created because

383
00:22:35,440 --> 00:22:39,040
of the ability to stream music and you'd be wrong because it's not a market.

384
00:22:39,040 --> 00:22:40,240
No one's making money.

385
00:22:40,240 --> 00:22:42,600
All the 16 to 20 year olds are not making any money off of this.

386
00:22:42,600 --> 00:22:48,600
They do it for a few years and then they disappear.

387
00:22:48,600 --> 00:22:54,040
So well, what Steve has done is by creating our masters is that a musician signs up and

388
00:22:54,040 --> 00:22:57,920
now there's 3 million young musicians signed up on the platform.

389
00:22:57,920 --> 00:23:02,120
And if you now go to the NBA website and you watch a video, there'll be some music behind

390
00:23:02,120 --> 00:23:06,280
it, that music is streamed from United Masters.

391
00:23:06,280 --> 00:23:09,600
And when every time it's streamed, the musician gets paid.

392
00:23:09,600 --> 00:23:11,600
It's their actual two-way market.

393
00:23:11,600 --> 00:23:14,680
And it's in fact a three-way market because it's got the NBA, it's got the listener,

394
00:23:14,680 --> 00:23:17,960
which is you and me, and it's got the person who made the music.

395
00:23:17,960 --> 00:23:21,760
And now all kinds of, I could give a longer talk about that, but all kinds of other market

396
00:23:21,760 --> 00:23:24,140
sort of forces are starting to come to play.

397
00:23:24,140 --> 00:23:27,080
People are reaching out to musicians and partnering with them.

398
00:23:27,080 --> 00:23:28,080
Shows are being made.

399
00:23:28,080 --> 00:23:29,760
People are playing at weddings.

400
00:23:29,760 --> 00:23:32,960
There's 3 million people who now have access to a steady income stream.

401
00:23:32,960 --> 00:23:37,320
So this is a sense in which AI can create jobs.

402
00:23:37,320 --> 00:23:41,040
3 million people have access now to a possible job.

403
00:23:41,040 --> 00:23:47,160
And these are 16 to 20-year-olds in the inner city, just to say, this is quite important.

404
00:23:47,160 --> 00:23:48,160
And that's just in the US.

405
00:23:48,160 --> 00:23:50,600
This can be done in every country around the world.

406
00:23:50,600 --> 00:23:54,600
And entrepreneurs thinking about a new company, instead of thinking about how do I steal some

407
00:23:54,600 --> 00:24:00,120
bits from somebody and then sell them, should think about how do I create a two-way market.

408
00:24:00,120 --> 00:24:04,120
And I just help the market get going.

409
00:24:04,120 --> 00:24:05,120
You could do this for art.

410
00:24:05,120 --> 00:24:08,960
You could do this for works of, you know, scholarly works.

411
00:24:08,960 --> 00:24:12,000
You could do this for travel information, all kinds of things.

412
00:24:12,000 --> 00:24:14,120
You can start to think more about markets.

413
00:24:14,120 --> 00:24:17,760
Okay, so that was the first half of my talk.

414
00:24:17,760 --> 00:24:20,560
That was kind of why do I work on what I'm working on, okay?

415
00:24:20,560 --> 00:24:23,720
And so hopefully you get a little bit more of the picture.

416
00:24:23,720 --> 00:24:28,600
It really is, in some sense, economics and mechanisms and, you know, networks and all

417
00:24:28,600 --> 00:24:29,600
that.

418
00:24:29,600 --> 00:24:33,480
But, you know, with all due respect, those fields didn't have enough of a statistics

419
00:24:33,480 --> 00:24:34,480
and learning perspective.

420
00:24:35,040 --> 00:24:39,720
They assumed a lot of things were already known and you get certain curves that cross.

421
00:24:39,720 --> 00:24:43,560
But they didn't kind of just adapt the market as you went and use large data sets to inform

422
00:24:43,560 --> 00:24:45,760
it and have recommendator systems.

423
00:24:45,760 --> 00:24:48,680
You don't see economics talking about recommender systems.

424
00:24:48,680 --> 00:24:52,480
Recommender systems are the way that social knowledge gets used and exploited among groups

425
00:24:52,480 --> 00:24:54,480
of people.

426
00:24:54,480 --> 00:24:58,000
So anyway, when you start thinking about what are the new problems that are going to emerge

427
00:24:58,000 --> 00:25:03,400
if you put these three axes together, it's really quite exciting.

428
00:25:03,400 --> 00:25:07,320
So in machine learning and statistics, we're really good about talking about optima.

429
00:25:07,320 --> 00:25:11,800
We can find optima in hundreds of thousands of dimensions even if they're saddle points

430
00:25:11,800 --> 00:25:16,200
and we can guarantee a rate and prove theorems about it and we're really, you know, we're

431
00:25:16,200 --> 00:25:18,120
really good at that.

432
00:25:18,120 --> 00:25:21,360
But in economics, you don't often find optima.

433
00:25:21,360 --> 00:25:23,120
You find equilibria.

434
00:25:23,120 --> 00:25:25,440
And moreover, the equilibria are rarely just stationary.

435
00:25:25,440 --> 00:25:27,240
They're moving around and you need to follow them.

436
00:25:27,240 --> 00:25:29,320
And so you need to talk about the dynamics.

437
00:25:29,320 --> 00:25:34,240
And so now there's topological issues and dynamical systems issues and stochastic process

438
00:25:34,240 --> 00:25:37,040
issues all merged together.

439
00:25:37,040 --> 00:25:41,400
And so there are algorithms, you know, gradient descent does not work for finding equilibria,

440
00:25:41,400 --> 00:25:43,920
but extra gradient does work and so on.

441
00:25:43,920 --> 00:25:46,600
There's a whole emerging, it's fixed point theory.

442
00:25:46,600 --> 00:25:52,640
So most of these ideas go back to the 30s and 40s, but they have been forgotten.

443
00:25:52,640 --> 00:25:56,760
But fixed point theory in hundreds of thousands of dimensions with stochastics, that's something

444
00:25:56,760 --> 00:26:01,000
we can start talking about and do and prove rates and, you know, get really new algorithms.

445
00:26:01,000 --> 00:26:03,960
And there are people doing that now.

446
00:26:03,960 --> 00:26:05,960
Exploration, exploitation, incentives in multi-way markets.

447
00:26:05,960 --> 00:26:09,280
Those are words that usually don't come into the market perspective.

448
00:26:09,280 --> 00:26:10,280
How do I exploit?

449
00:26:10,280 --> 00:26:11,280
How do I explore?

450
00:26:11,280 --> 00:26:12,480
And how do I put that together with incentives?

451
00:26:12,480 --> 00:26:16,200
I'm going to talk about some of the rest of these, but let me just sort of highlight.

452
00:26:16,200 --> 00:26:19,760
These are mostly words you will not see on a machine learning person's talk or AI person's

453
00:26:19,760 --> 00:26:20,760
talk.

454
00:26:20,760 --> 00:26:24,560
They will talk about trust maybe or fairness or privacy, that's all good.

455
00:26:24,560 --> 00:26:25,960
Those are social concepts.

456
00:26:25,960 --> 00:26:30,600
But they don't embed it in a fuller, what are the underlying foundational principles

457
00:26:30,600 --> 00:26:35,080
that make it fair or make it private or make it valuable to people.

458
00:26:35,080 --> 00:26:41,800
They just want to kind of stamp privacy or, you know, or fairness on it and that's enough.

459
00:26:41,800 --> 00:26:45,080
So let's try to think about what are these underlying concepts.

460
00:26:45,080 --> 00:26:48,200
And let me just say that I've loved learning all this economics.

461
00:26:48,200 --> 00:26:52,240
I had learned a lot of statistics and I've eventually learned some computer science.

462
00:26:52,240 --> 00:26:54,600
And that was fun, but learning economics has been particularly fun.

463
00:26:55,000 --> 00:26:57,120
And it's maybe because I already knew the math and I could just kind of go through the

464
00:26:57,120 --> 00:26:59,440
books really fast.

465
00:26:59,440 --> 00:27:04,120
But you know, this notion of incentives and really thinking about asymmetries and decentralized,

466
00:27:04,120 --> 00:27:08,600
I really get that out of economics in ways I never got from any other field.

467
00:27:08,600 --> 00:27:13,480
So I'm having a lot of fun here and I'm realizing that if I'd gone back to the 1950s and I'd

468
00:27:13,480 --> 00:27:17,920
hung out with David Blackwell and Von Neumann and others that they were doing all this.

469
00:27:17,920 --> 00:27:19,800
This was kind of the spirit of the era.

470
00:27:19,800 --> 00:27:23,600
And operations research emerged in that era, kind of bringing it all together.

471
00:27:23,600 --> 00:27:25,960
And somehow that all got kind of forgotten.

472
00:27:25,960 --> 00:27:30,320
We got all buried into building certain kind of systems or doing certain kind of data analysis

473
00:27:30,320 --> 00:27:36,520
or, you know, measuring certain kind of linear models and we forgot about the overall picture.

474
00:27:36,520 --> 00:27:41,120
Okay, so these blue ones are the ones I'm going to kind of use now as vignettes and

475
00:27:41,120 --> 00:27:43,320
the rest of my talk.

476
00:27:43,320 --> 00:27:47,400
And so this is, given this is an evening talk, I don't want to make this a highly technical

477
00:27:47,400 --> 00:27:48,400
academic talk.

478
00:27:48,400 --> 00:27:52,080
There are archive papers on all this with theorems and so on and so forth.

479
00:27:52,080 --> 00:27:56,040
But I do want to give the sense of what's the problem and what is the theorem, all right,

480
00:27:56,040 --> 00:27:57,040
and what is the consequence of that.

481
00:27:57,040 --> 00:28:00,840
Okay, so I'm going to give enough of that to highlight some of these issues.

482
00:28:00,840 --> 00:28:02,800
So I picked these three to talk about in some order.

483
00:28:02,800 --> 00:28:04,080
I forget which order.

484
00:28:04,080 --> 00:28:07,600
Okay, so here's perhaps my favorite one.

485
00:28:07,600 --> 00:28:10,000
I get to recognize two Stanford people.

486
00:28:10,000 --> 00:28:16,480
Stephen was a student with Emanuel and joined my group two years ago, three maybe.

487
00:28:16,480 --> 00:28:18,200
You know, fabulous intellect.

488
00:28:18,240 --> 00:28:22,320
Michael, I actually don't, we've only, because of the pandemic, met online, but

489
00:28:22,320 --> 00:28:24,040
he is a Stanford person.

490
00:28:24,040 --> 00:28:26,040
And then Jake was a student with me.

491
00:28:26,040 --> 00:28:31,440
He's now a postdoc with one of Emanuel's ex-students, Serena Fogelbarber.

492
00:28:31,440 --> 00:28:32,720
So a lot of nice connectivity there.

493
00:28:34,520 --> 00:28:36,720
Okay, with all due apology to the economists in the room,

494
00:28:36,720 --> 00:28:38,400
I'm going to say a little bit about incentives.

495
00:28:39,680 --> 00:28:42,240
There's a kind of general theory of incentives.

496
00:28:42,240 --> 00:28:43,640
There's books on it.

497
00:28:43,640 --> 00:28:46,960
And roughly speaking, there's kind of three branches to it.

498
00:28:46,960 --> 00:28:49,440
There's auction theory that you all know about.

499
00:28:49,440 --> 00:28:53,240
There's matching markets, and there's contract theory.

500
00:28:53,240 --> 00:28:56,600
So contract theory is maybe the less well-known outside of economics, but

501
00:28:56,600 --> 00:28:59,360
you all know about it because you experience it daily.

502
00:28:59,360 --> 00:29:03,680
It's where agents provide, possess private information, and

503
00:29:03,680 --> 00:29:06,840
there's a principal who wants to incentivize to do something with that

504
00:29:06,840 --> 00:29:07,560
private information.

505
00:29:08,840 --> 00:29:09,920
So why does this happen?

506
00:29:09,920 --> 00:29:13,480
Well, you know, the boss wants to get the employees to do something.

507
00:29:13,480 --> 00:29:16,920
And it's not just because the boss would do it themselves, but

508
00:29:16,920 --> 00:29:18,680
you know, they have to get the employees to do it.

509
00:29:18,680 --> 00:29:19,880
The boss would know how to do it.

510
00:29:19,880 --> 00:29:21,600
The employees have local information.

511
00:29:21,600 --> 00:29:23,040
They're smarter.

512
00:29:23,040 --> 00:29:26,440
They may, if they're incentivized, they'll do even better work and so on.

513
00:29:26,440 --> 00:29:30,440
And now the boss has got to kind of offer them incentives so

514
00:29:30,440 --> 00:29:31,920
that they'll actually do the labor.

515
00:29:31,920 --> 00:29:35,920
So this came up in economics, sort of after General Equilibrium Theory,

516
00:29:35,920 --> 00:29:39,840
which was very symmetric, Nash Equilibria and everything is very symmetric.

517
00:29:39,840 --> 00:29:42,400
This recognized that real life is full of asymmetries.

518
00:29:42,400 --> 00:29:44,360
There's someone trying to get someone else to do something and

519
00:29:44,360 --> 00:29:47,280
that person has power because they know something that it's not known upstairs.

520
00:29:49,520 --> 00:29:53,080
All right, so you know about it because you've all, for example, you travel.

521
00:29:53,080 --> 00:29:57,920
And you probably have wondered why aren't there, why is this so complicated?

522
00:29:57,920 --> 00:30:00,760
Why is there not one fare for every seat on the airplane?

523
00:30:00,760 --> 00:30:02,040
Like there is for a movie theater.

524
00:30:03,400 --> 00:30:05,320
All right, and you all know the answer kind of, right?

525
00:30:05,320 --> 00:30:08,560
Because there's different willingness to pay in the population.

526
00:30:08,560 --> 00:30:12,000
So a business class traveler or a business traveler, not a business,

527
00:30:12,520 --> 00:30:17,280
traveler, maybe the company's paying so they could care less what the fare is.

528
00:30:17,280 --> 00:30:20,080
Or maybe they're really urgently needing to get from one place to another.

529
00:30:20,080 --> 00:30:21,920
They have high willingness to pay.

530
00:30:21,920 --> 00:30:23,960
And there's a lot of other people who don't have high willingness to pay.

531
00:30:23,960 --> 00:30:26,120
They could wait till tomorrow, you know, and so on.

532
00:30:26,120 --> 00:30:31,080
So the airlines really in the 80s realized that they could start to price discriminate.

533
00:30:31,080 --> 00:30:34,360
They could try to figure out who had higher wills to pay and

534
00:30:34,360 --> 00:30:37,800
charge them higher and who had lower wills to pay and charge them less.

535
00:30:37,800 --> 00:30:41,760
And fill the airplane and get a blend of both kinds.

536
00:30:41,760 --> 00:30:44,520
And you've got to be clever to do this, right?

537
00:30:44,520 --> 00:30:48,120
And so what you do, if you set a single price, that's not going to work.

538
00:30:48,120 --> 00:30:50,520
And if you try to screen for people, like, you know,

539
00:30:50,520 --> 00:30:54,800
look at somebody wearing a suit and tie, you say, I'm going to charge you more.

540
00:30:54,800 --> 00:30:57,480
Well, that person's going to the next time show up in jeans.

541
00:30:59,200 --> 00:31:01,320
All right, so people are doing this all the time.

542
00:31:01,320 --> 00:31:01,960
They're aware.

543
00:31:01,960 --> 00:31:03,120
They're gaming the system.

544
00:31:03,120 --> 00:31:04,480
And you've got to think this through.

545
00:31:05,760 --> 00:31:07,040
All right, so you know the answer.

546
00:31:07,040 --> 00:31:09,280
What you do is you provide a menu of options.

547
00:31:09,280 --> 00:31:11,080
You provide a service and you provide a price.

548
00:31:11,080 --> 00:31:12,320
And a service and a price.

549
00:31:12,320 --> 00:31:15,280
And everybody gets the same menu, right?

550
00:31:15,280 --> 00:31:16,600
Now what is this menu for the airline?

551
00:31:16,600 --> 00:31:20,240
Well, there's this class called business class and the students don't know about

552
00:31:20,240 --> 00:31:22,840
this yet, but eventually they'll learn about it.

553
00:31:22,840 --> 00:31:26,440
Where you get a little glass of red wine and you get to be first in line and

554
00:31:26,440 --> 00:31:29,360
be all proud of yourself and you get a little bit bigger chair.

555
00:31:29,360 --> 00:31:32,080
And people will pay $1,000 more for that.

556
00:31:32,080 --> 00:31:34,320
It's amazing, right?

557
00:31:34,320 --> 00:31:38,400
Now, only class that actually makes money, right?

558
00:31:39,400 --> 00:31:45,280
But the marginal cost of putting people on airplanes is sort of zero, all right?

559
00:31:45,280 --> 00:31:47,880
So you want to fill the rest of the airplane, all right?

560
00:31:47,880 --> 00:31:51,720
So amazingly, there are people in the back who don't want to spend $1,000 for

561
00:31:51,720 --> 00:31:53,080
a little glass of red wine.

562
00:31:53,080 --> 00:31:55,520
And they feel very good about themselves because they didn't spend all that money

563
00:31:55,520 --> 00:31:57,000
and they're still on the airplane.

564
00:31:57,000 --> 00:31:58,280
So everybody's happy.

565
00:31:58,280 --> 00:32:00,760
That's what's called social welfare.

566
00:32:00,760 --> 00:32:01,720
And the plane is full.

567
00:32:01,720 --> 00:32:02,720
That's what's called revenue.

568
00:32:03,760 --> 00:32:06,160
Okay, so you can make mathematics out of all that.

569
00:32:06,160 --> 00:32:08,080
So you get the usual crossing curves and all that.

570
00:32:08,080 --> 00:32:10,520
They're just not the same crossing curves as in general equilibrium theory.

571
00:32:10,520 --> 00:32:12,200
They're a different set of things.

572
00:32:12,200 --> 00:32:17,520
But every one of those texts says we have missing information.

573
00:32:17,520 --> 00:32:19,320
We're going to assume there's some probability distribution and

574
00:32:19,320 --> 00:32:21,400
we're going to call the whole thing Bayesian.

575
00:32:21,400 --> 00:32:24,760
Now, as a statistician, I look at it and say, wow, Bayesian, it's not Bayesian.

576
00:32:24,760 --> 00:32:27,920
There's a distribution on unknown quantities.

577
00:32:27,920 --> 00:32:28,680
That's all.

578
00:32:28,680 --> 00:32:32,960
There's no updating, there's no learning, there's none of the above, okay?

579
00:32:32,960 --> 00:32:34,920
All right, so wow, wonderful opportunity.

580
00:32:34,920 --> 00:32:35,840
We should work on this.

581
00:32:36,840 --> 00:32:37,600
And we have.

582
00:32:38,800 --> 00:32:41,240
Okay, so you all know about clinical trials.

583
00:32:42,520 --> 00:32:47,720
Costs tens of millions of dollars a year to run clinical trials in any particular

584
00:32:47,720 --> 00:32:49,000
therapeutic area.

585
00:32:49,000 --> 00:32:51,080
You all know about it for vaccines.

586
00:32:51,080 --> 00:32:53,600
It's amazingly expensive and it's amazingly important.

587
00:32:53,600 --> 00:32:55,880
And if you don't do it at the right scale, you'll make big mistakes.

588
00:32:57,040 --> 00:32:57,680
You all know this.

589
00:32:58,720 --> 00:33:02,320
All right, so you would imagine that the FDA does a great job of this.

590
00:33:02,320 --> 00:33:03,400
And in some level they do.

591
00:33:03,400 --> 00:33:04,800
They're very good statisticians.

592
00:33:04,800 --> 00:33:07,400
But they're not good at being the economics.

593
00:33:09,240 --> 00:33:12,120
All right, so this really should be thought as a contract theory problem.

594
00:33:12,120 --> 00:33:15,760
The FDA is a principle and they're trying to decide what drugs go to market.

595
00:33:17,000 --> 00:33:21,320
But they only have partial knowledge about the drug candidates, okay?

596
00:33:21,320 --> 00:33:22,800
Where do the drug candidates come from?

597
00:33:22,800 --> 00:33:25,600
They come from the pharmaceutical companies.

598
00:33:25,600 --> 00:33:28,640
Pharmaceutical companies know something internally about some candidate.

599
00:33:28,640 --> 00:33:31,120
They're about ready to send up to the FDA.

600
00:33:31,120 --> 00:33:33,160
Maybe they know they put their best engineers on it.

601
00:33:33,160 --> 00:33:34,760
Maybe they've had experience with it.

602
00:33:34,760 --> 00:33:39,480
Maybe a little internal testing, so on and so forth, all right?

603
00:33:39,480 --> 00:33:41,480
The FDA is now getting all these candidates and

604
00:33:41,480 --> 00:33:43,960
they would like to say, pharmaceutical company,

605
00:33:43,960 --> 00:33:46,000
that candidate you just sent me, how good is that candidate?

606
00:33:47,000 --> 00:33:49,520
Well, the pharmaceutical company does not want to reveal.

607
00:33:49,520 --> 00:33:52,240
Because the FDA, if they're told it's not a good candidate,

608
00:33:52,240 --> 00:33:54,880
they'll put yet more, they'll ensure there's no false positive.

609
00:33:54,880 --> 00:33:58,160
They'll put yet more clinical trial money into it.

610
00:33:58,160 --> 00:34:00,600
Where if they think it's a really good candidate, they won't.

611
00:34:00,720 --> 00:34:04,120
And also the license they will get will be titrated to risk.

612
00:34:05,240 --> 00:34:07,320
All right, so the companies are incentivized to not say.

613
00:34:09,840 --> 00:34:11,280
All right, but that's a problem.

614
00:34:11,280 --> 00:34:13,720
All right, so now let's think about the actual paradigm.

615
00:34:13,720 --> 00:34:14,800
What is the FDA doing?

616
00:34:14,800 --> 00:34:18,160
Well, they're being statisticians, a frequentist statistician.

617
00:34:18,160 --> 00:34:22,200
So here's a little Naaman Pearson kind of setup.

618
00:34:22,200 --> 00:34:25,560
A bad drug, theta equals zero, doesn't mean that it hurts people,

619
00:34:25,560 --> 00:34:27,520
because they definitely screen for that.

620
00:34:27,520 --> 00:34:29,840
It just means it has no effect, all right?

621
00:34:29,840 --> 00:34:33,040
And there are tons of drugs on the market that have no effect,

622
00:34:33,040 --> 00:34:34,880
for better or for worse.

623
00:34:34,880 --> 00:34:38,800
And they have a type one error of say 0.05, it's actually more like 0.01,

624
00:34:38,800 --> 00:34:42,560
but they set up a classifier that achieves that.

625
00:34:42,560 --> 00:34:45,560
And then for the good drugs that are actually having an effect,

626
00:34:45,560 --> 00:34:49,400
they want a high power, so 0.8 is a kind of a standard number for that.

627
00:34:49,400 --> 00:34:51,000
Is that a good protocol?

628
00:34:51,000 --> 00:34:53,240
Well, it's optimal, it's the Naaman Pearson test.

629
00:34:53,240 --> 00:34:55,040
So yeah, of course, it's great.

630
00:34:55,040 --> 00:34:56,520
But is it a good protocol?

631
00:34:56,520 --> 00:34:57,160
And the answer is no.

632
00:34:58,160 --> 00:34:59,960
So let's do a little thought experiment here.

633
00:34:59,960 --> 00:35:03,560
In situations where there's a small profit to be made,

634
00:35:03,560 --> 00:35:06,840
it costs $20 million to run the trial.

635
00:35:06,840 --> 00:35:10,240
But if you're approved, let's suppose you would make 200 million.

636
00:35:10,240 --> 00:35:12,880
So this would be for a niche drug of some kind.

637
00:35:12,880 --> 00:35:19,400
And so the CEO can do a little calculation, as can the FDA,

638
00:35:19,400 --> 00:35:21,640
conditioning on theta equals zero.

639
00:35:21,640 --> 00:35:25,640
Now no one knows if theta is zero or not, so this is a counterfactual.

640
00:35:25,920 --> 00:35:30,760
But thinking conceptually, theta equals zero, what's my expected profit?

641
00:35:30,760 --> 00:35:34,240
Well, you can put all those numbers together and you get minus 10 million.

642
00:35:34,240 --> 00:35:36,320
All right, so the CEO looks at that number and they say,

643
00:35:36,320 --> 00:35:40,360
only send candidates up to the FDA if you're really pretty sure it's a good drug.

644
00:35:42,080 --> 00:35:43,920
That it's going to get passed because it's a good drug.

645
00:35:43,920 --> 00:35:45,680
Don't hope for a false positive.

646
00:35:45,680 --> 00:35:47,000
We'll go out of business.

647
00:35:48,080 --> 00:35:48,800
That's great.

648
00:35:48,800 --> 00:35:53,080
Now the FDA is mostly getting good drugs and they have a good screening procedure.

649
00:35:53,080 --> 00:35:54,760
So everything that's getting through is looking good.

650
00:35:55,840 --> 00:35:58,000
If that were real life, that'd be great, but here's more like real life.

651
00:35:58,000 --> 00:36:02,080
So you have $20 million to run the trial and if you're approved,

652
00:36:02,080 --> 00:36:03,640
you could make 2 billion.

653
00:36:03,640 --> 00:36:05,680
So this would be like ibuprofen or something.

654
00:36:05,680 --> 00:36:07,480
So this is more common.

655
00:36:07,480 --> 00:36:09,920
And now the CEO could do the same exact calculation.

656
00:36:09,920 --> 00:36:12,880
If it was the case that theta is equal to zero,

657
00:36:12,880 --> 00:36:15,040
my expected profit would be 80 million.

658
00:36:15,040 --> 00:36:19,480
So now the CEO is very incentivized to send as many candidates as they can to the FDA.

659
00:36:19,480 --> 00:36:22,760
And the FDA will get flooded and they do get flooded.

660
00:36:22,760 --> 00:36:25,040
And they will do these tests and there will be some false positives.

661
00:36:25,040 --> 00:36:25,960
And these things will go to market.

662
00:36:25,960 --> 00:36:27,680
They don't hurt anybody, but they just don't have any effect.

663
00:36:27,680 --> 00:36:30,800
And people will make money and then eventually that changes.

664
00:36:30,800 --> 00:36:32,360
So this is broken.

665
00:36:32,360 --> 00:36:35,120
And it's just broken because it's not being thought of as a contract theory problem.

666
00:36:36,200 --> 00:36:39,600
All right, so we have now lured on this.

667
00:36:39,600 --> 00:36:43,720
We have a paper and we have an idea we call Statistical Contract Theory.

668
00:36:45,000 --> 00:36:46,680
And so here is the protocol.

669
00:36:47,960 --> 00:36:49,080
There are four steps to it.

670
00:36:49,080 --> 00:36:50,760
It's only step three, which is new.

671
00:36:50,760 --> 00:36:53,360
The other three are standard contract theory.

672
00:36:53,360 --> 00:36:57,720
So an agent comes to this contract and they opt in or they just decide to walk away.

673
00:36:57,720 --> 00:37:01,160
So the drug company comes and they just looked at it and say, no, I'm not interested.

674
00:37:01,160 --> 00:37:06,200
Or if they opt in, they have to pay a reservation price R, say 20 million.

675
00:37:06,200 --> 00:37:09,000
And then they get to select a payout function from a menu.

676
00:37:09,000 --> 00:37:10,960
And I'm going to say more about what that means here in a moment.

677
00:37:12,480 --> 00:37:19,800
But it's going to be a function from observed the clinical trial to the amount you get to licensed for.

678
00:37:19,800 --> 00:37:20,880
And we're going to design the menu.

679
00:37:20,880 --> 00:37:24,640
That's going to be our goal as economical statisticians.

680
00:37:24,640 --> 00:37:30,160
Then we do a statistical trial, which yields a random variable, Z, coming from P of theta.

681
00:37:30,160 --> 00:37:33,600
Theta is the true theta in nature because we're getting data from the real world.

682
00:37:33,600 --> 00:37:36,760
No one knows theta, but we get data from P of theta.

683
00:37:36,760 --> 00:37:37,960
And then there's the payoff.

684
00:37:37,960 --> 00:37:40,040
So agent gets payoff F of Z.

685
00:37:40,040 --> 00:37:43,920
They were the ones who selected the payout function, so they get paid that amount they selected.

686
00:37:43,920 --> 00:37:48,280
And the principal receives a utility, which is a function of F of Z because they have to pay that.

687
00:37:49,240 --> 00:37:54,800
And theta, because the FDA, if they make lots of approvals of not so good drugs,

688
00:37:54,800 --> 00:37:58,080
they'll eventually look bad, and so their utility should reflect that.

689
00:37:59,400 --> 00:38:02,440
Agents in this setting need to maximize their payoff.

690
00:38:02,440 --> 00:38:07,720
Their best response is simply to take the arg max of the expectation under this data of the payoff.

691
00:38:07,720 --> 00:38:09,400
That's what they want to maximize.

692
00:38:09,400 --> 00:38:11,480
So that's pretty clear what an agent should be doing in this paradigm.

693
00:38:12,600 --> 00:38:15,040
All right, now if you're going to do economics together with statistics,

694
00:38:15,040 --> 00:38:18,000
the key thing you have to think about is incentive alignment.

695
00:38:18,040 --> 00:38:26,080
Am I doing a situation where the incentives or what I want to achieve is aligned with people's interest?

696
00:38:26,080 --> 00:38:28,360
All right, so here's a way to set that up.

697
00:38:28,360 --> 00:38:33,200
For the null agents, those who have the null candidates,

698
00:38:33,200 --> 00:38:41,880
it should be the case that the utility of the principal is decreasing in F of Z, okay?

699
00:38:41,880 --> 00:38:46,720
Whereas for the non-null agents, for a good drug, the utility should be increasing in F of Z, okay?

700
00:38:46,760 --> 00:38:47,760
So it's kind of obvious.

701
00:38:49,760 --> 00:38:55,040
So, you know, in English, the principal wants to attract as transact as much as possible with the good agents,

702
00:38:55,040 --> 00:38:56,280
the ones that have a good drug.

703
00:38:56,960 --> 00:39:02,080
All right, so now the definition is that a menu of these options is incentive aligned.

704
00:39:02,080 --> 00:39:08,040
If it is the case for all of the null drugs, the expectation under the null of the difference

705
00:39:08,040 --> 00:39:10,800
of the payout and the reservation price is less than or equal to zero.

706
00:39:10,800 --> 00:39:14,360
If that weren't true, then these companies just make money for free, okay?

707
00:39:14,400 --> 00:39:18,720
So you need to have that be the case, so the principal would be happy with this.

708
00:39:19,320 --> 00:39:23,600
The P less than or equal to 0.05 protocol that we're used to from statistics is not incentive aligned.

709
00:39:23,600 --> 00:39:24,800
That's simple to see.

710
00:39:26,080 --> 00:39:32,400
Okay. All right, so now we have a theorem, which is right down at the bottom there,

711
00:39:32,400 --> 00:39:37,440
which is that it turns out that a contract is incentive aligned if and only if,

712
00:39:37,440 --> 00:39:42,280
this is a characterization, all of the payout functions are E values, right?

713
00:39:42,280 --> 00:39:43,440
So what's an E value?

714
00:39:44,440 --> 00:39:46,800
Well, it's like a P value kind of.

715
00:39:46,800 --> 00:39:50,720
It's a statistical measure of evidence, but

716
00:39:50,720 --> 00:39:53,600
whereas a P value is a tail probability under the null,

717
00:39:53,600 --> 00:39:57,800
the probability of under the null hypothesis being more extreme than the observed data.

718
00:39:57,800 --> 00:39:59,520
That's a P value.

719
00:39:59,520 --> 00:40:01,800
And E value is under the null hypothesis.

720
00:40:02,760 --> 00:40:06,840
The expectation of this E value is less than or equal to 1, okay?

721
00:40:07,840 --> 00:40:16,160
It looks a little bit like a martingale, a super martingale, and in fact is the more general story is these are non-negative super martingales.

722
00:40:16,160 --> 00:40:18,360
And because they're martingales, they kind of compose nicely.

723
00:40:18,360 --> 00:40:20,200
You can stop them because of stopping theorems.

724
00:40:20,200 --> 00:40:22,760
They just are a nicer measure of statistical evidence.

725
00:40:22,760 --> 00:40:24,120
Whereas P values don't compose.

726
00:40:24,120 --> 00:40:24,960
You can't stop them.

727
00:40:24,960 --> 00:40:25,960
They just have all these troubles.

728
00:40:27,040 --> 00:40:31,680
So this is a neat result, which is that this concept from theory of contracts

729
00:40:33,120 --> 00:40:36,560
is exactly the same concept as E values in statistics.

730
00:40:37,800 --> 00:40:40,360
And moreover, we have a result, which I don't think I have a slide on it.

731
00:40:40,360 --> 00:40:41,800
Nope.

732
00:40:41,800 --> 00:40:46,360
That if we now want to do, how do you actually design a menu and get,

733
00:40:46,360 --> 00:40:55,480
say, a maxi min menu, the maximal overall theta of the minimum risk.

734
00:40:56,600 --> 00:41:01,400
It turns out to be characterized by taking all possible E values.

735
00:41:01,400 --> 00:41:02,520
That's your menu.

736
00:41:02,520 --> 00:41:06,640
So if a computational regime might want to do that, or for interpretability reasons,

737
00:41:06,640 --> 00:41:08,880
but that's another if and only of theorem.

738
00:41:10,680 --> 00:41:12,600
Okay, so I'm going to move on.

739
00:41:12,600 --> 00:41:16,040
We're now rolling this out in various domains of actually designing menus and

740
00:41:16,040 --> 00:41:17,440
contracts, but we have this guide.

741
00:41:17,440 --> 00:41:19,280
We now know how to design the optimal contract.

742
00:41:19,280 --> 00:41:22,760
We know what, we use E values, and we know lots of E values.

743
00:41:22,760 --> 00:41:25,960
There's a lot of literature on non-negative super martingales or E values and so on.

744
00:41:25,960 --> 00:41:27,160
So we'll be doing that.

745
00:41:27,160 --> 00:41:29,800
And I'll just say we've done this in particular in the federated learning

746
00:41:29,800 --> 00:41:31,160
domain.

747
00:41:31,160 --> 00:41:33,280
This is now just, again, the picture of federated learning, but

748
00:41:33,280 --> 00:41:35,120
now with an incentive structure.

749
00:41:35,120 --> 00:41:39,920
So we're able to design an incentive compatible mechanism that incentivizes

750
00:41:39,920 --> 00:41:42,720
agents at the edge to contribute data.

751
00:41:42,720 --> 00:41:46,560
And in particular, this handles a problem that has been recognized in

752
00:41:46,560 --> 00:41:48,680
literature, which is a free writing problem.

753
00:41:48,680 --> 00:41:52,640
If I have some data to send up, but sitting next to me there is a manual, and

754
00:41:52,640 --> 00:41:56,240
he has data to send, and I know that his data is pretty much the same as mine.

755
00:41:56,240 --> 00:41:58,960
I'm going to watch him send the data, and I know I don't have to.

756
00:41:58,960 --> 00:42:00,120
Pre-writing.

757
00:42:00,120 --> 00:42:02,240
This paradigm incentivizes against free writing.

758
00:42:03,200 --> 00:42:05,160
Professor, just one small clarification.

759
00:42:05,160 --> 00:42:05,680
Yeah.

760
00:42:05,680 --> 00:42:09,760
Is this based on the assumption that we're talking about home economics in terms

761
00:42:09,760 --> 00:42:13,240
of very rational and certain things?

762
00:42:13,240 --> 00:42:14,080
It's a good question.

763
00:42:14,080 --> 00:42:17,880
I was hoping that was going to come up later about, it's all this rational

764
00:42:17,880 --> 00:42:20,640
economic stuff.

765
00:42:20,640 --> 00:42:25,000
No, and sort of the behavioral economics here is kind of coming in the fact that

766
00:42:25,000 --> 00:42:25,840
we're gathering data.

767
00:42:28,080 --> 00:42:31,200
So all these distributions are informed by data.

768
00:42:31,240 --> 00:42:35,600
And if we just write down the utility, that's only the assumption we have to

769
00:42:35,600 --> 00:42:37,800
make, is that we agree that you want to maximize that.

770
00:42:38,960 --> 00:42:40,560
And that's usually not so strange.

771
00:42:41,840 --> 00:42:42,960
And then the data informs it.

772
00:42:42,960 --> 00:42:45,400
We don't make a distributional assumption about the data.

773
00:42:45,400 --> 00:42:47,240
So I can get into that a little bit longer, but

774
00:42:48,520 --> 00:42:50,440
behavioral economics is very much part of this agenda.

775
00:42:51,480 --> 00:42:55,560
But it's not just that it's broken and we think about the psychology of it.

776
00:42:55,560 --> 00:42:58,600
Well, no, we collect data, and data is coming from real people.

777
00:42:58,600 --> 00:43:00,640
So we already have a little bit of a help there.

778
00:43:01,920 --> 00:43:04,480
So hopefully that partially answers your question.

779
00:43:04,480 --> 00:43:07,120
Anyway, if you're interested in this application, we have a paper on that and

780
00:43:07,120 --> 00:43:07,920
we're continuing on with that.

781
00:43:10,160 --> 00:43:12,880
I got two more vignettes, I think, and I'm just going to go a little more quickly on

782
00:43:12,880 --> 00:43:15,240
these. I just want to give you a flavor of these.

783
00:43:15,240 --> 00:43:18,240
Classification is the big killer app in machine learning.

784
00:43:18,240 --> 00:43:21,400
Classify, yes or no, good or bad, blah, blah, blah.

785
00:43:21,400 --> 00:43:24,160
But if you do this in domains where there are strategic agents,

786
00:43:24,160 --> 00:43:25,840
you get something called strategic classification.

787
00:43:25,840 --> 00:43:28,800
So this is a work with Tiana Zernich, who's still a student with me and

788
00:43:28,800 --> 00:43:31,040
will be joining Emmanuel's group as a postdoc.

789
00:43:31,040 --> 00:43:33,920
He and I shuttle these superstar people back and forth.

790
00:43:33,920 --> 00:43:35,520
And then Eric is now a professor at Caltech.

791
00:43:37,200 --> 00:43:39,040
All right, so here's a little picture to suggest this.

792
00:43:39,040 --> 00:43:43,240
Health insurance, the health insurance company has got to do a classification

793
00:43:43,240 --> 00:43:47,120
problem. I fill out a form, they have to decide whether to give me insurance or not.

794
00:43:48,960 --> 00:43:51,840
They're going to ask me, how much do you exercise?

795
00:43:51,840 --> 00:43:53,760
I'm going to say a lot.

796
00:43:53,760 --> 00:43:55,560
How much do you drink of wine?

797
00:43:55,560 --> 00:43:58,560
Very little, so on and so forth.

798
00:43:58,560 --> 00:44:00,720
Now if it's implausible, they'll kind of see that.

799
00:44:01,040 --> 00:44:02,040
But you make it plausible.

800
00:44:03,520 --> 00:44:06,600
They know that, however, so they're not going to make it so easy for you.

801
00:44:06,600 --> 00:44:10,560
So they're going to ask questions like, would you be willing to have us look at

802
00:44:10,560 --> 00:44:13,240
your cell phone accelerometer for one day?

803
00:44:13,240 --> 00:44:16,160
Just opt in, you don't have to, but are you willing to do that?

804
00:44:16,160 --> 00:44:17,320
You say, sure.

805
00:44:17,320 --> 00:44:20,480
And now if my cell phone moves around a lot during that, it shows I'm very active.

806
00:44:20,480 --> 00:44:22,720
If it sits in one place all day, I'm not so active.

807
00:44:22,720 --> 00:44:24,400
They would use that as data.

808
00:44:24,400 --> 00:44:26,240
So someone went out to build a device,

809
00:44:26,240 --> 00:44:28,560
then you put your cell phone on the device and it moves around all day.

810
00:44:29,520 --> 00:44:31,280
So this is the kind of problem that arises.

811
00:44:31,280 --> 00:44:33,000
An economist are very much aware of this.

812
00:44:33,000 --> 00:44:35,800
They call this Goodhart's Law.

813
00:44:35,800 --> 00:44:41,320
If you set up a poverty index score at some year, this was in Columbia in 1994.

814
00:44:41,320 --> 00:44:44,240
It looks very good, very Gaussian and all that.

815
00:44:44,240 --> 00:44:49,440
By 2003, people have discovered that if they move just a little bit left of there,

816
00:44:49,440 --> 00:44:50,960
they get more better housing.

817
00:44:52,000 --> 00:44:54,440
So everyone cheated a little bit so they could move over.

818
00:44:54,440 --> 00:44:57,040
And so the poverty index score has now been ruined.

819
00:44:57,040 --> 00:44:58,040
But this is real life.

820
00:44:58,240 --> 00:45:00,200
This is what people really will do, and they should.

821
00:45:00,200 --> 00:45:00,560
Why not?

822
00:45:02,480 --> 00:45:03,600
It's not an ethical issue.

823
00:45:05,760 --> 00:45:08,040
Ethics is sometimes used a little bit too easily here.

824
00:45:08,040 --> 00:45:09,800
So the real problem is that when you do learning,

825
00:45:09,800 --> 00:45:12,680
you rarely have just collected data set and analyze it.

826
00:45:12,680 --> 00:45:15,320
In the real world, you have to say, where's the data come from?

827
00:45:17,160 --> 00:45:19,800
If it's people supplying the data, are they aware of what the outcome is?

828
00:45:19,800 --> 00:45:21,200
Do they have some bets that's interested in it?

829
00:45:22,200 --> 00:45:26,480
Probably they do, because if not, why would they really be engaged in this whole exercise?

830
00:45:26,480 --> 00:45:29,000
All right, so now we have a Stackelberg game.

831
00:45:29,000 --> 00:45:31,200
It's a game theory setting, which is sequential.

832
00:45:32,240 --> 00:45:36,120
I send some data up, and the central decision maker, say the bank,

833
00:45:36,120 --> 00:45:39,040
is trying to decide about loans, collects a lot of data.

834
00:45:39,040 --> 00:45:41,840
They build a model that predicts whether I should get a loan or not.

835
00:45:43,200 --> 00:45:45,240
And then that starts to make some decisions.

836
00:45:45,240 --> 00:45:47,320
People start to realize what's happening.

837
00:45:47,320 --> 00:45:49,560
Maybe the bank has got to reveal by regulatory reasons,

838
00:45:49,560 --> 00:45:51,160
I'm using logistic regression or something.

839
00:45:52,080 --> 00:45:54,640
People realize that, and they say, okay, the next time they send the data,

840
00:45:54,640 --> 00:45:57,240
they're going to alter their data.

841
00:45:57,240 --> 00:46:00,560
And that goes back and forth, and you want to ask what equilibria rise here.

842
00:46:00,560 --> 00:46:03,160
We're not trying to optimize any likelihood, it's an equilibrium problem.

843
00:46:04,480 --> 00:46:08,480
Okay, so we have studied this as a Stackelberg game,

844
00:46:08,480 --> 00:46:11,360
which is the appropriate concept in game theory.

845
00:46:12,360 --> 00:46:15,880
Classically, in a Stackelberg game, you have a leader and you have a follower.

846
00:46:16,920 --> 00:46:20,800
Classically, the decision maker would be thought of as the leader here.

847
00:46:20,800 --> 00:46:24,440
They run the whole show, and agents are the follower.

848
00:46:24,440 --> 00:46:27,320
You can show in that situation, in this setup,

849
00:46:27,320 --> 00:46:32,080
that the leader gets high utility and the followers get low utility, just to say.

850
00:46:32,080 --> 00:46:34,960
So it seems reasonable.

851
00:46:34,960 --> 00:46:39,080
But if that's an analysis you could do in a synchronous situation,

852
00:46:39,080 --> 00:46:41,400
where there's a model built, data's gathered.

853
00:46:41,400 --> 00:46:43,760
Model built, data gathered, all synchronized.

854
00:46:43,760 --> 00:46:45,360
The real world is no synchronization.

855
00:46:45,360 --> 00:46:49,160
Why should people wait till, there's no synchronization between the central model

856
00:46:49,160 --> 00:46:51,000
and me sending up data?

857
00:46:51,000 --> 00:46:53,840
Okay, so you could start to think about analyzing different scenarios,

858
00:46:53,840 --> 00:46:57,360
where there's different kinds of timescales.

859
00:46:57,360 --> 00:47:01,680
So here's one where the modeler goes slowly, only updating every once in a while,

860
00:47:01,680 --> 00:47:05,200
and the streets you gain doesn't send data much more rapidly.

861
00:47:05,200 --> 00:47:07,840
So does this arise in real life?

862
00:47:09,360 --> 00:47:12,480
Sure, this is, for example, like college admissions.

863
00:47:12,480 --> 00:47:15,880
The college is gathering all these applicants, and they have all this data.

864
00:47:15,880 --> 00:47:19,200
They're not gonna adjust their policy after every applicant.

865
00:47:19,200 --> 00:47:20,560
They'll do it every couple of years or so, and

866
00:47:20,560 --> 00:47:22,600
they'll publish it and all, for obvious social reasons.

867
00:47:23,600 --> 00:47:25,040
So that's a real scenario.

868
00:47:25,040 --> 00:47:26,360
What about the other way around?

869
00:47:26,360 --> 00:47:30,320
Where the central agent updates very, very rapidly, and agents are much more slow?

870
00:47:30,320 --> 00:47:33,120
Well, that happens all the time too, that's like YouTube.

871
00:47:33,120 --> 00:47:36,320
Every time someone clicks, they update a model in principle, okay?

872
00:47:37,360 --> 00:47:41,800
So these are different scenarios, and so what happens here?

873
00:47:41,800 --> 00:47:45,480
So you can analyze this as now you do the game theory.

874
00:47:45,480 --> 00:47:48,400
So we were able to prove a theorem that shows, first of all,

875
00:47:48,400 --> 00:47:51,800
that in either order of play, you get an equilibrium.

876
00:47:51,800 --> 00:47:55,000
It's not so hard to see that and analyze that.

877
00:47:55,000 --> 00:47:59,480
Much more surprisingly, is that in these statistical settings,

878
00:47:59,480 --> 00:48:03,280
where it's a data analysis problem, not just an arbitrary Stackelberg game,

879
00:48:04,560 --> 00:48:10,640
it turns out that when the decision maker is a follower, and

880
00:48:10,640 --> 00:48:14,440
the strategic agents are the leader, the kind of flipped around version,

881
00:48:14,440 --> 00:48:17,360
the strategic agents have higher utility than before.

882
00:48:17,360 --> 00:48:18,760
That makes some sense.

883
00:48:18,760 --> 00:48:21,960
But also, the decision maker has higher utility.

884
00:48:21,960 --> 00:48:24,120
It's a rare example in game theory of a win-win.

885
00:48:25,280 --> 00:48:28,760
Going in the order where the strategic agent is a one's going fast,

886
00:48:28,760 --> 00:48:31,560
that leads to higher utility for both parties.

887
00:48:31,560 --> 00:48:34,640
So that's not a true fact about game theory in general, but

888
00:48:34,640 --> 00:48:36,760
it's a true fact about statistical game theory.

889
00:48:36,760 --> 00:48:39,240
These statistical modeling exercises for

890
00:48:39,240 --> 00:48:40,760
generalized linear models, just to say.

891
00:48:41,840 --> 00:48:43,200
I'm gonna skip this little part here,

892
00:48:43,200 --> 00:48:45,720
just I like to show pictures of my students.

893
00:48:45,720 --> 00:48:49,880
So there's Lydia and Horia, and just say this,

894
00:48:49,880 --> 00:48:51,640
I'm gonna show you really quick the slides.

895
00:48:51,640 --> 00:48:55,520
But it's a cute little paper where you bring together bandits from machine

896
00:48:55,520 --> 00:48:58,520
learning and matching markets from economics.

897
00:48:58,520 --> 00:48:59,560
And let me just show you a picture.

898
00:48:59,560 --> 00:49:02,120
Here's a learner in a bandit problem.

899
00:49:02,120 --> 00:49:05,840
They're trying to find out which of the set of options is the best,

900
00:49:05,840 --> 00:49:07,280
gives the highest reward.

901
00:49:07,280 --> 00:49:09,800
And they're algorithms like upper confidence bound,

902
00:49:09,800 --> 00:49:14,040
that help you guide you towards diminishing your uncertainty and

903
00:49:14,040 --> 00:49:16,240
also picking the optimal arm.

904
00:49:16,240 --> 00:49:20,120
So we asked the question about, what if you put this in a market setting?

905
00:49:20,120 --> 00:49:24,080
So I don't just have one decision maker, I've got a two sided market.

906
00:49:24,080 --> 00:49:28,200
And so in particular, I might have two decision makers who are selecting

907
00:49:28,200 --> 00:49:30,400
actions from the other side of the market.

908
00:49:30,400 --> 00:49:32,680
And there's preferences on both sides.

909
00:49:32,680 --> 00:49:37,600
And so you ask questions like, what if both of the agents select the same action?

910
00:49:37,600 --> 00:49:41,120
And so we model this as congestion that one of them gets the reward,

911
00:49:41,120 --> 00:49:42,360
the other gets no reward at all.

912
00:49:44,040 --> 00:49:45,520
And who gets the reward?

913
00:49:45,520 --> 00:49:47,960
Well, that depends on the preferences on the right side of the market.

914
00:49:47,960 --> 00:49:50,480
So both sides are learning about each other.

915
00:49:50,480 --> 00:49:52,720
And so again, you can do the mathematics here, and

916
00:49:52,720 --> 00:49:54,200
it turns out to be pretty interesting.

917
00:49:54,200 --> 00:49:59,600
What you're really asking is, if there's competition in a bandit situation,

918
00:49:59,600 --> 00:50:02,640
does that make the regret higher or better?

919
00:50:02,640 --> 00:50:03,840
What does competition do for

920
00:50:03,840 --> 00:50:06,680
the learning process of a person trying to learn the best action?

921
00:50:07,680 --> 00:50:10,880
Okay, and long story short, we did a, here's a theory.

922
00:50:10,880 --> 00:50:14,000
Here's a regret bound, and so this is more for the experts.

923
00:50:14,000 --> 00:50:18,400
But the regret is as a function of time, which is n, logarithmic and n.

924
00:50:18,400 --> 00:50:22,680
So that's an optimal result from classical bandit theory.

925
00:50:22,680 --> 00:50:23,680
So that is still true.

926
00:50:23,680 --> 00:50:26,520
Competition does not hurt your rate of learning.

927
00:50:26,520 --> 00:50:28,760
There's a denominator term though, which is a constant,

928
00:50:28,760 --> 00:50:32,440
which is a gap between the preferences of nearby agents.

929
00:50:32,440 --> 00:50:34,680
So if there's competition and you have a small gap between me and

930
00:50:34,680 --> 00:50:36,680
somebody else, we start to compete more.

931
00:50:36,680 --> 00:50:39,520
And that gives us a higher regret, but it's only a constant.

932
00:50:39,520 --> 00:50:42,320
All right, so I put that up there just to sort of show you that it's kind of

933
00:50:42,320 --> 00:50:45,640
really fun things to do with simple learning algorithms,

934
00:50:45,640 --> 00:50:50,200
explore exploit type, and simple matching market kind of ideas.

935
00:50:50,200 --> 00:50:53,680
And this was motivated by this kind of restaurant setting where

936
00:50:53,680 --> 00:50:57,200
100,000 of us are out looking for a restaurant in Shanghai.

937
00:50:57,200 --> 00:51:02,040
There's 100,000 restaurants, and we're all trying things out as we go and

938
00:51:02,040 --> 00:51:05,960
getting rewards or not, and both sides of the market have some preferences.

939
00:51:05,960 --> 00:51:08,160
And how does that market clear?

940
00:51:08,160 --> 00:51:08,800
That was our question.

941
00:51:08,800 --> 00:51:14,280
All right, so last two weeks ago, I talked about this in this very room.

942
00:51:14,280 --> 00:51:15,440
So I'm going to kind of go quickly, but

943
00:51:15,440 --> 00:51:19,680
this is a very exciting project here that I want to spend five minutes on.

944
00:51:19,680 --> 00:51:23,680
Similar collection of students, but also again, Stephen, who's a postdoc.

945
00:51:25,240 --> 00:51:28,200
Anastasia, Stephen, Clara, and Tiana.

946
00:51:28,200 --> 00:51:30,120
So this is really more about the statistics.

947
00:51:30,120 --> 00:51:32,440
There's little economics here, but less.

948
00:51:32,440 --> 00:51:36,960
This is more about how do we do things like use neural nets to do science,

949
00:51:36,960 --> 00:51:39,360
just roughly speaking, okay?

950
00:51:39,360 --> 00:51:44,080
So you all know about things like Alpha-fold, they will make huge numbers

951
00:51:44,080 --> 00:51:48,160
of predictions of, say, these tertiary structure proteins.

952
00:51:50,080 --> 00:51:53,400
Hundreds of millions of structures, whereas the hand labeled ones,

953
00:51:53,400 --> 00:51:56,520
there's only hundreds of thousands of such sequences, all right?

954
00:51:56,520 --> 00:52:00,760
So that was a problem that's now being revolutionized in biology.

955
00:52:00,760 --> 00:52:05,520
So here's an example of someone in 2004 wrote a very important paper in nature,

956
00:52:06,520 --> 00:52:10,480
studying the relationship between intrinsic disorder of proteins,

957
00:52:10,480 --> 00:52:12,960
where things don't fold, they kind of are more strand-like.

958
00:52:12,960 --> 00:52:16,680
And that turns out to be very important for, like, grid-like things.

959
00:52:16,680 --> 00:52:20,400
And phosphorylation, which is an important part of it, and many pathways.

960
00:52:21,600 --> 00:52:25,880
So they wanted to ask, is there an association between those two notions?

961
00:52:25,880 --> 00:52:28,680
With the parts of the protein in its order, they tend to be more phosphorylated or not.

962
00:52:30,680 --> 00:52:34,240
But they really couldn't test it, and now you go forward to 2022.

963
00:52:34,280 --> 00:52:38,720
Instead of this small amount of data we had back in 2004,

964
00:52:38,720 --> 00:52:44,080
now you have vast amounts of Alpha-fold labeled data.

965
00:52:44,080 --> 00:52:46,560
It's not really data, it's predictions, but they're good predictions.

966
00:52:46,560 --> 00:52:49,480
So why not throw them in as if they were data, all right?

967
00:52:49,480 --> 00:52:52,920
So someone wrote a paper, 2022, doing that.

968
00:52:52,920 --> 00:52:55,760
And so they wanted to quantify this odds ratio,

969
00:52:55,760 --> 00:52:58,240
probability of intrinsically disordered given phosphorylation.

970
00:53:00,000 --> 00:53:02,880
And kind of amazingly, they didn't even use any of the hand label,

971
00:53:03,360 --> 00:53:05,400
the gold standard data, because they had so much of this other stuff,

972
00:53:05,400 --> 00:53:08,560
they just threw it all in, because it's such a good predictor, why not?

973
00:53:08,560 --> 00:53:10,480
But as a statistician, you know better, right?

974
00:53:10,480 --> 00:53:12,360
Even if it's very, very accurate at making predictions,

975
00:53:12,360 --> 00:53:15,040
that doesn't mean the inferences you make are any good.

976
00:53:15,040 --> 00:53:19,280
All right, so I think this one picture I'm about to show, so there's the mayors.

977
00:53:19,280 --> 00:53:22,600
This picture is probably the end of my talk, really,

978
00:53:22,600 --> 00:53:24,080
and I'll just kind of scroll through a couple more.

979
00:53:25,280 --> 00:53:26,360
So let me take a moment on this one.

980
00:53:27,600 --> 00:53:30,440
Our statistic here is this odds ratio.

981
00:53:30,440 --> 00:53:32,480
We'd like to know if there's an association or not.

982
00:53:32,480 --> 00:53:34,640
You've all taken elementary statistics.

983
00:53:34,640 --> 00:53:37,560
We have to find whether it's significantly different from one.

984
00:53:37,560 --> 00:53:40,800
One would be no association, bigger is an association.

985
00:53:43,320 --> 00:53:49,480
We did a Monte Carlo version of this where we, in the set of label data,

986
00:53:49,480 --> 00:53:52,520
we actually got the true odds ratio, that's the dotted line there.

987
00:53:53,520 --> 00:53:58,960
And then we redid the entire experiment with alpha fold output using the predictions.

988
00:53:58,960 --> 00:53:59,960
Okay, so what are we doing here?

989
00:53:59,960 --> 00:54:03,520
That gold region right there is a confidence interval.

990
00:54:03,520 --> 00:54:09,240
And it's based on taking all of the alpha fold predictions and treating them as real, okay?

991
00:54:09,240 --> 00:54:14,280
And from those, you form a confidence interval on this odds ratio.

992
00:54:14,280 --> 00:54:17,720
And that's an elementary statistics exercise to do that, all right?

993
00:54:17,720 --> 00:54:19,000
That confidence interval is tiny.

994
00:54:19,000 --> 00:54:21,280
That looks really good, because you have all this data.

995
00:54:21,280 --> 00:54:23,360
It's not real data, but it looks really good.

996
00:54:23,360 --> 00:54:24,840
You're very, very confident.

997
00:54:24,840 --> 00:54:28,680
You're just dead wrong, all right?

998
00:54:29,680 --> 00:54:33,960
The statisticians in the room, Art, will say, why'd you do that?

999
00:54:33,960 --> 00:54:36,240
Just we know how to do confidence intervals.

1000
00:54:36,240 --> 00:54:38,480
Just use the gold standard data.

1001
00:54:38,480 --> 00:54:41,240
Don't trust these wild machine learning predictions.

1002
00:54:41,240 --> 00:54:42,560
And Art, I would do that too.

1003
00:54:42,560 --> 00:54:43,880
That's what my first thought would be.

1004
00:54:43,880 --> 00:54:49,720
That gives you the gray region, so it covers the truth, as it was asserted to be, all right?

1005
00:54:49,720 --> 00:54:51,440
But it also covers one.

1006
00:54:51,440 --> 00:54:54,520
So it doesn't allow you to include, there's an actual association, all right?

1007
00:54:54,520 --> 00:54:58,640
So the new method gets the best of both worlds, this prediction powered inference.

1008
00:54:58,640 --> 00:55:04,880
It forms a confidence interval, which is guaranteed to cover the truth, with 95% probability.

1009
00:55:04,880 --> 00:55:12,040
But it's also much smaller than the, it uses the predictions, but it corrects them, all right?

1010
00:55:12,040 --> 00:55:15,080
And I think the most fun thing to show you, I'm going to skip that slide,

1011
00:55:15,080 --> 00:55:19,200
is just the examples that we've been applying this to, and we have a paper that does these.

1012
00:55:19,200 --> 00:55:22,880
Here's voting, here's a ballot, here's a messed up ballot.

1013
00:55:22,880 --> 00:55:24,440
So this was a San Francisco election.

1014
00:55:24,440 --> 00:55:27,640
People use computer vision to look at all the ballots and make a prediction,

1015
00:55:27,640 --> 00:55:30,280
whether it was yes or no, all right?

1016
00:55:30,280 --> 00:55:33,240
And now you can feed that in and do an eight analysis on that.

1017
00:55:33,240 --> 00:55:37,200
And you can see the little gold region there, it's a little small confidence interval,

1018
00:55:37,200 --> 00:55:38,640
it's just missing the truth.

1019
00:55:38,640 --> 00:55:41,960
And again, I don't know why some things are not coming out here.

1020
00:55:41,960 --> 00:55:47,640
Our new interval is the green one, and then there's a missing, a classical one there that's much

1021
00:55:47,640 --> 00:55:49,960
larger, but again, covers the truth.

1022
00:55:49,960 --> 00:55:53,400
This is counting spiral galaxies, you know, there's some hand labeled,

1023
00:55:53,400 --> 00:55:55,360
here's the spiral galaxy, here's not.

1024
00:55:55,360 --> 00:56:01,040
And again, you can see the small confidence interval,

1025
00:56:01,040 --> 00:56:04,000
if you use the computer vision algorithm, but it's not covering the truth.

1026
00:56:04,000 --> 00:56:05,960
And again, we cover the truth.

1027
00:56:05,960 --> 00:56:08,160
And I think this was the last one I wanted to show.

1028
00:56:08,160 --> 00:56:10,160
Yeah, here's a California census.

1029
00:56:10,160 --> 00:56:13,200
The estimate is a logistic regression coefficient of income when predicting

1030
00:56:13,200 --> 00:56:15,480
whether a person has private health insurance.

1031
00:56:15,480 --> 00:56:17,760
And there was a machine learning algorithm run on that.

1032
00:56:17,760 --> 00:56:21,200
You can see the tiny little confidence interval, it's very, very sure and dead wrong.

1033
00:56:22,200 --> 00:56:25,720
Okay, I hope this conveys, you all kind of knew this, that, you know,

1034
00:56:25,720 --> 00:56:29,520
very accurate machine learning models can still lead to completely wrong inferences.

1035
00:56:29,520 --> 00:56:33,600
Okay, I just don't think my machine learning colleagues get that, but it definitely can.

1036
00:56:33,600 --> 00:56:36,040
And so, but you can get the best of both worlds.

1037
00:56:36,040 --> 00:56:39,120
And I think I'm going to skip all the way to this last slide here and

1038
00:56:39,120 --> 00:56:41,520
just show you roughly how this happens.

1039
00:56:41,520 --> 00:56:45,080
It's kind of like a bias correction procedure, but it's not quite that.

1040
00:56:45,080 --> 00:56:50,800
All right, so there is a bias between the predicted parameter using all the predictive data

1041
00:56:51,000 --> 00:56:52,840
and the true parameter, theta star.

1042
00:56:52,840 --> 00:56:55,520
That bias is a population level quantity.

1043
00:56:55,520 --> 00:56:59,600
If you had the whole population, you could just write it down as a number, right?

1044
00:56:59,600 --> 00:57:02,920
There are ways to estimate bias, like the bootstrap.

1045
00:57:02,920 --> 00:57:05,760
And you could take that estimate and correct your estimate.

1046
00:57:05,760 --> 00:57:07,360
That's done a lot.

1047
00:57:07,360 --> 00:57:12,040
We have a different idea, which is that we take that quantity, that bias,

1048
00:57:12,040 --> 00:57:14,040
or we call it in general rectifier.

1049
00:57:14,040 --> 00:57:16,840
And we don't just estimate it, we'd put a confidence interval on it.

1050
00:57:16,840 --> 00:57:19,520
We get all the possible values of that correction.

1051
00:57:19,520 --> 00:57:23,280
Now we take the original predictive quantity, which is wrong, and

1052
00:57:23,280 --> 00:57:26,440
we correct it with all the possible corrections.

1053
00:57:26,440 --> 00:57:29,400
That leads to that green region there, which is a confidence interval on the

1054
00:57:29,400 --> 00:57:31,320
corrected predictions.

1055
00:57:31,320 --> 00:57:34,920
All right, and then our theorem at the very bottom of the page shows that we

1056
00:57:34,920 --> 00:57:36,320
were good statisticians.

1057
00:57:36,320 --> 00:57:40,280
The probability that this new confidence interval covers the truth is bigger than

1058
00:57:40,280 --> 00:57:41,880
or equal to 1 minus alpha.

1059
00:57:41,880 --> 00:57:46,000
And it's much, much smaller than if you've forgotten all the predictions altogether.

1060
00:57:46,000 --> 00:57:48,640
Okay, so I put that up there just at the end of an economics talk.

1061
00:57:48,640 --> 00:57:51,800
There's more statistics, but I just think many people in the room are already

1062
00:57:51,800 --> 00:57:53,480
thinking about this and working on it.

1063
00:57:53,480 --> 00:57:54,680
It is one of the critical issues.

1064
00:57:54,680 --> 00:57:57,880
If you're going to do science with machine learning, you've got to face this.

1065
00:57:57,880 --> 00:58:00,720
You've got to be a good statistician while exploiting the advantages of the

1066
00:58:00,720 --> 00:58:02,080
machine learning paradigm.

1067
00:58:02,080 --> 00:58:04,400
And I think this is a step towards doing that.

1068
00:58:04,400 --> 00:58:06,600
All right, so that's my last slide, and that's the slide I had up earlier.

1069
00:58:06,600 --> 00:58:10,760
I just wanted to kind of remind you of the big picture of the more provocative

1070
00:58:10,760 --> 00:58:12,800
issues.

1071
00:58:12,800 --> 00:58:14,760
This to me has kind of been a no-brainer.

1072
00:58:14,760 --> 00:58:16,800
I just, what's happening this era?

1073
00:58:16,840 --> 00:58:19,600
Well, it's just statistics and computer science and econ and all.

1074
00:58:19,600 --> 00:58:21,160
And we're being good engineers.

1075
00:58:21,160 --> 00:58:25,480
We're trying to deliver artifacts that will help humans and how to do that well,

1076
00:58:25,480 --> 00:58:27,880
like previous generations of engineers.

1077
00:58:27,880 --> 00:58:32,120
And this kind of Silicon Valley hype thing of, we've discovered this great

1078
00:58:32,120 --> 00:58:35,560
thing called AI, and it suddenly, we've got to worry about all the things that's

1079
00:58:35,560 --> 00:58:37,000
going to happen because of that.

1080
00:58:37,000 --> 00:58:39,240
It just, to me, wrong.

1081
00:58:39,240 --> 00:58:40,240
Thank you.

1082
00:58:40,240 --> 00:58:50,840
I hope, yeah, I knew.

1083
00:58:50,840 --> 00:58:52,520
I shouldn't have picked on you, Art.

1084
00:58:52,520 --> 00:58:53,680
You have a better idea?

1085
00:58:53,680 --> 00:58:58,680
Well, I have a paper where there's an island of really high quality gold

1086
00:58:58,680 --> 00:59:03,160
standard data and an ocean of data where you're not quite sure of the quality.

1087
00:59:03,160 --> 00:59:07,880
And then we sort of do a shrinkage of one onto the other.

1088
00:59:07,880 --> 00:59:12,440
But we just got point estimates so we didn't get a confidence interval.

1089
00:59:12,440 --> 00:59:13,240
Yeah.

1090
00:59:13,240 --> 00:59:16,080
I mean, so this is a little bit like the semi-supervised paradigm.

1091
00:59:16,080 --> 00:59:19,480
So we have an ocean of labeled data and, sorry, an ocean.

1092
00:59:19,480 --> 00:59:23,440
We have a small pool of labeled data and an ocean of unlabeled data.

1093
00:59:23,440 --> 00:59:25,480
The machine learning person says, oh, yeah, that's similar to supervised.

1094
00:59:25,480 --> 00:59:26,240
No.

1095
00:59:26,240 --> 00:59:30,520
We're using the unlabeled data to find a confidence interval to correct the, sorry,

1096
00:59:30,520 --> 00:59:31,600
we're using the, got it wrong.

1097
00:59:31,600 --> 00:59:34,680
The labeled data to find a confidence interval to correct the unlabeled data and

1098
00:59:34,680 --> 00:59:37,560
get a confidence interval out of the whole thing.

1099
00:59:37,560 --> 00:59:40,080
But yes, I think I was aware of that work of yours.

1100
00:59:40,080 --> 00:59:43,720
And let me just say this is not, none of this is ever new.

1101
00:59:43,720 --> 00:59:48,120
Statisticians in kind of small data census work did things like this.

1102
00:59:48,120 --> 00:59:50,200
And semi-parametric statisticians did some too.

1103
00:59:50,200 --> 00:59:54,600
So this is, again, there's always somebody who did it probably in the 1950s in

1104
00:59:54,600 --> 01:00:03,560
statistics, or Art, or Brad Efron or what, it's always, any other?

1105
01:00:03,560 --> 01:00:04,640
Yes, there's two over here.

1106
01:00:05,640 --> 01:00:10,840
As you pointed out, there's been a lot of attention around uncertainty,

1107
01:00:10,840 --> 01:00:15,880
quantification, and similar, you know, similar moves in that direction lately in

1108
01:00:15,880 --> 01:00:17,360
the machine learning space, which is great.

1109
01:00:19,360 --> 01:00:24,360
What does that mean for the future of applied Bayesian statistics and, you know,

1110
01:00:24,360 --> 01:00:26,400
MCMC, that sort of thing?

1111
01:00:26,400 --> 01:00:31,080
Given that it's computationally less tractable than a lot of modern machine

1112
01:00:31,080 --> 01:00:34,040
learning training techniques, is there still a place for it?

1113
01:00:34,440 --> 01:00:36,560
Okay, yeah, good, thank you for asking.

1114
01:00:36,560 --> 01:00:40,720
Yeah, no, I tend to be a Bayesian, as with most statisticians, sometimes I'm a

1115
01:00:40,720 --> 01:00:42,400
Bayesian, sometimes I'm not.

1116
01:00:42,400 --> 01:00:45,560
And I'm a Bayesian when I'm working with a scientist over two or three years.

1117
01:00:46,600 --> 01:00:50,520
Because I'm trying to kind of get out the knowledge that they have and use it.

1118
01:00:50,520 --> 01:00:52,160
All right, and that's a prior.

1119
01:00:52,160 --> 01:00:55,600
And so why would I not do that if I'm going to work with them a long time?

1120
01:00:55,600 --> 01:00:58,800
I'm a frequentist when I'm trying to produce a piece of software that

1121
01:00:58,800 --> 01:01:00,800
people all over the world will use.

1122
01:01:00,800 --> 01:01:02,520
Because I'm not going to work with them and get the prior.

1123
01:01:02,520 --> 01:01:05,240
I'm just going to put it out there and I want to put a stamp certificate that 99%

1124
01:01:05,240 --> 01:01:07,800
of the time they're going to work for whoever uses it.

1125
01:01:07,800 --> 01:01:09,560
That's the two perspectives I have.

1126
01:01:09,560 --> 01:01:11,560
Now, a little more nuance to that.

1127
01:01:12,840 --> 01:01:15,240
Bayesian way to structure models is very nice.

1128
01:01:15,240 --> 01:01:19,840
You get hierarchies, you get sharing of shrinkage, social network kind of things

1129
01:01:19,840 --> 01:01:21,880
or naturally Bayesian.

1130
01:01:21,880 --> 01:01:26,720
I think that Brad Efron, who has been the luminary in statistics here at

1131
01:01:26,720 --> 01:01:29,160
Stanford but worldwide, had it right.

1132
01:01:29,160 --> 01:01:32,120
Which is that you often will go into a problem, you think Bayesian.

1133
01:01:32,120 --> 01:01:34,480
You start to structure the model, think about what I could know,

1134
01:01:34,480 --> 01:01:38,200
what would be together with what, and then you become frequentist.

1135
01:01:38,200 --> 01:01:40,360
You say, I'm going to do empirical Bayes.

1136
01:01:40,360 --> 01:01:43,360
I'm not going to just run the Bayesian MCMC paradigm.

1137
01:01:43,360 --> 01:01:45,400
I'm going to at some point just say, okay, there's some things I can estimate.

1138
01:01:45,400 --> 01:01:47,440
I can plug them in at the Bayesian procedure and

1139
01:01:47,440 --> 01:01:49,160
then I'll get the benefit of both worlds.

1140
01:01:49,160 --> 01:01:51,200
Now, I totally agree.

1141
01:01:51,200 --> 01:01:56,360
So a lot of the things you saw here have a kind of an empirical Bayes interpretation.

1142
01:01:56,360 --> 01:02:00,040
And a lot of the, but it's true, the conformal things in the uncertainty

1143
01:02:00,040 --> 01:02:03,160
you're talking about don't necessarily, maybe a manual could correct me there.

1144
01:02:03,160 --> 01:02:06,960
But those are kind of pure hardcore frequentist at some level.

1145
01:02:06,960 --> 01:02:10,680
But I tend to, when I would use those in practice, I would probably have not just

1146
01:02:10,680 --> 01:02:13,040
one conformal predictor over here, I'd have another one over here, another one over

1147
01:02:13,040 --> 01:02:16,120
here, I would want to shrink them towards each other, I want to have them related.

1148
01:02:16,120 --> 01:02:17,840
Because in real domains, if you really start to scale,

1149
01:02:17,840 --> 01:02:21,040
the Bayesian way of thinking helps you immensely.

1150
01:02:21,040 --> 01:02:24,760
So that's funny, Bayesian frequentists do conflict, but

1151
01:02:24,760 --> 01:02:26,080
it's like wave particle duality.

1152
01:02:26,080 --> 01:02:28,480
It's kind of my metaphor, right?

1153
01:02:28,480 --> 01:02:31,520
Waves and particles are both correct, and they conflict a little bit.

1154
01:02:31,520 --> 01:02:33,440
But if you throw out one and just use the other one,

1155
01:02:33,440 --> 01:02:34,800
you're gonna do bad physics.

1156
01:02:34,800 --> 01:02:38,440
And same thing with statistics, yeah.

1157
01:02:38,440 --> 01:02:43,360
Professor, you were emphasizing the importance of forming collectives in solving

1158
01:02:43,360 --> 01:02:44,760
the problems.

1159
01:02:44,760 --> 01:02:48,320
And I'm very curious to know how you think about how those

1160
01:02:48,320 --> 01:02:53,160
collectives can actually be formed to pursue a goal.

1161
01:02:53,160 --> 01:02:56,480
Especially because, I mean, I don't know if a principle or

1162
01:02:56,480 --> 01:03:00,600
a platform is required to kind of create the market where agents can actually trust

1163
01:03:00,600 --> 01:03:02,000
that and cooperate.

1164
01:03:02,000 --> 01:03:06,960
Or do you actually feel like a decentralized kind of a network is possible?

1165
01:03:06,960 --> 01:03:09,840
That's a fantastic question, I'm delighted to have it.

1166
01:03:09,840 --> 01:03:13,800
And just for the young people in the room, I hope you see the questions like that

1167
01:03:13,800 --> 01:03:16,880
are kind of the thing of the era, and they're hard.

1168
01:03:16,880 --> 01:03:19,000
I don't have an answer to your question, it's gonna be the short answer.

1169
01:03:20,440 --> 01:03:23,480
The colleagues I had on that paper, the social science colleagues,

1170
01:03:23,480 --> 01:03:27,000
they talk a lot about new models for democracy.

1171
01:03:28,200 --> 01:03:32,240
And they emphasize that democracies tend to arise when you have multiple layers of

1172
01:03:32,240 --> 01:03:36,320
like bring 200 people together, get some consensus, take 200 people here,

1173
01:03:36,320 --> 01:03:39,280
get some consensus, and put the consensus together.

1174
01:03:39,280 --> 01:03:41,440
And form cities and countries and all that.

1175
01:03:41,440 --> 01:03:43,600
That's what humans have done throughout history.

1176
01:03:43,600 --> 01:03:48,040
And we've had this experiment now that we have this thing called Twitter.

1177
01:03:48,040 --> 01:03:51,680
And we're assuming that it's all good that all of us talk all the time.

1178
01:03:51,680 --> 01:03:53,880
Or that we all listen to one person.

1179
01:03:53,880 --> 01:03:56,600
And those are terrible ways to do democracy.

1180
01:03:56,600 --> 01:03:58,560
So there are experiments that have been underway for

1181
01:03:58,560 --> 01:04:00,240
quite some time for people like that.

1182
01:04:00,240 --> 01:04:03,360
Like those famous examples in Taiwan where they have a legislator,

1183
01:04:03,360 --> 01:04:07,840
which is using lots of data analysis together with kind of structured assemblies

1184
01:04:07,840 --> 01:04:11,800
of ways to kind of come to coherent decisions and to get consensus.

1185
01:04:11,800 --> 01:04:14,400
And Ireland has used this, and their latest this,

1186
01:04:14,400 --> 01:04:18,040
they kind of legalized abortion at some point, that's very hard to do in Ireland.

1187
01:04:18,040 --> 01:04:21,200
They did it partly because of these new assemblies, new structures.

1188
01:04:21,200 --> 01:04:25,000
So I love this, people thinking about out of the box of new mechanisms that

1189
01:04:25,000 --> 01:04:29,760
bring together visibility of, but it's still among relatively small numbers of

1190
01:04:29,760 --> 01:04:31,920
people, that's really critical.

1191
01:04:31,920 --> 01:04:35,000
And I think that the technologists who just built the YouTubes and

1192
01:04:35,000 --> 01:04:38,840
the Facebooks and all that, were trying to do this experiment on human beings.

1193
01:04:38,840 --> 01:04:41,120
So it was just destined to fail.

1194
01:04:41,120 --> 01:04:43,120
The big broadcast channels were terrible.

1195
01:04:43,120 --> 01:04:46,680
We want communities and we got to think about a structure of those.

1196
01:04:46,680 --> 01:04:49,800
So I don't have much more to say about that other than how do you form collectives

1197
01:04:49,800 --> 01:04:52,680
and support them and make them healthy and all that is hugely interesting and

1198
01:04:52,680 --> 01:04:55,720
important, and there are social scientists who spend their life doing this.

1199
01:04:55,720 --> 01:04:58,560
This is definitely not just a technology issue.

1200
01:04:58,560 --> 01:05:00,480
We need to both cooperate and listen and

1201
01:05:00,480 --> 01:05:02,640
have a dialogue with those kind of social scientists.

1202
01:05:04,160 --> 01:05:05,760
There's many others we should cooperate with, but

1203
01:05:05,760 --> 01:05:07,720
I think that's a particularly pregnant one.

1204
01:05:07,720 --> 01:05:10,160
Economics certainly talks about collaborative things.

1205
01:05:10,160 --> 01:05:13,560
There's cooperative game theory and how do coalitions form.

1206
01:05:13,560 --> 01:05:17,120
But it's a little bit dry, and maybe Hido can help me a little bit with kind of,

1207
01:05:17,120 --> 01:05:18,400
maybe there's more to it.

1208
01:05:18,400 --> 01:05:21,040
But it's a little bit about how do I do negotiation and

1209
01:05:21,040 --> 01:05:23,440
get the most money out of the deal I can and so on.

1210
01:05:23,440 --> 01:05:25,000
You need to line the incentives.

1211
01:05:25,000 --> 01:05:27,080
Yeah, or line with incentives and so on.

1212
01:05:27,080 --> 01:05:29,320
But that just means that we just haven't thought about it enough.

1213
01:05:29,320 --> 01:05:32,520
And for the young people in the room, wow, that's a great topic to think about.

1214
01:05:32,520 --> 01:05:36,400
How do I start to structure collaborative efforts in data-oriented ways?

1215
01:05:36,400 --> 01:05:38,040
So I think economists didn't have enough kind of,

1216
01:05:38,040 --> 01:05:39,960
they talk about communication signaling, but

1217
01:05:39,960 --> 01:05:42,760
they didn't really have enough data to kind of really signal interesting things,

1218
01:05:42,760 --> 01:05:45,120
and do it in adaptive interesting ways.

1219
01:05:45,120 --> 01:05:47,760
So let me just lean in again to this, there's a lot of young people in the room.

1220
01:05:47,760 --> 01:05:50,200
This is the most exciting era to be in.

1221
01:05:50,200 --> 01:05:52,240
The previous eras kind of gave us greaty to send and

1222
01:05:52,240 --> 01:05:54,840
gave us networks and all that and all these tools.

1223
01:05:54,840 --> 01:05:57,160
And now they threw them out there in the world and they kind of work and

1224
01:05:57,160 --> 01:05:58,120
they kind of don't.

1225
01:05:58,120 --> 01:06:00,640
We got better commerce, we got better transmission.

1226
01:06:00,640 --> 01:06:01,960
And we can sort of fix all those.

1227
01:06:01,960 --> 01:06:05,680
We can also think a lot more about, wow, new things, good things could happen if

1228
01:06:05,680 --> 01:06:07,480
we start to think in the right way.

1229
01:06:07,480 --> 01:06:09,520
And what problems are needed to do that?

1230
01:06:09,520 --> 01:06:13,120
Don't just work on self-driving cars and whatever, or

1231
01:06:13,120 --> 01:06:17,720
make Facebook advertisements better, work on problems that you believe

1232
01:06:17,720 --> 01:06:21,640
in, and there are plenty of them, but bring these two fields together though.

1233
01:06:21,640 --> 01:06:24,160
Don't just think of yourself as a system builder.

1234
01:06:24,920 --> 01:06:30,960
What up guy?

1235
01:06:30,960 --> 01:06:32,800
I don't know,

1236
01:06:32,800 --> 01:06:33,080
I don't know,

1237
01:06:33,080 --> 01:06:37,560
I think my qury just made a living.

1238
01:06:37,560 --> 01:06:38,600
Okay, thank you, bye.

1239
01:06:38,600 --> 01:06:41,620
I will just let you guys go and get off the car.

1240
01:06:41,620 --> 01:06:42,220
Bye.


start	end	text
0	10680	So our closing event is a special lecture given by Mike Jordan who almost I think does
10680	13960	not need an introduction but I'll still give one.
13960	20080	So he's a Pei Hong Chen Distinguished Professor at UC Berkeley and Mike has been a leader
20080	26880	in the computational and mathematical study of learning for a long time and he's achieved
26880	33160	such prominence that I think in 2016 he's been named one of the most influential computer
33160	39360	scientists on Earth and so I'm just looking at his computer right now or the most influential
39360	41160	computer scientist.
41160	47280	He's been recognized for his work by many contributions, many awards as a member of
47280	53640	the National Academy of Science, of the National Academy of Engineering, of the Royal Society.
53640	59680	He was the inaugural winner of the World Laureates Award last year in 2022 and he receives
59680	63520	a John von Neumann Medal from IEEE in 2020.
63520	68560	If I were to list all these awards I think we would be here for a long, long time.
68560	72840	But I would say that personally there are three things that amaze me about Mike Jordan.
72840	74960	The first is his range of interest.
74960	80160	They span an enormous array of fields that goes all the way from computer scientists
80160	87880	to statistics to control theory, signal processing, mathematics, information theory, cognitive
87880	93160	science and now economics and maybe we'll hear about a bit about this today.
93160	99560	And I think the range and the breadth of his interest is just very unique.
99560	102240	There's another thing that is unique about Mike.
102240	107480	He's his track record of training the next generation of students.
107480	113720	I think if you look at, I think he has a very impressive CV, but I think if you look at
113720	119480	the CV the thing that impresses me the most is the name of his grad students because your
119480	127440	students, and we have one right behind you, Lee-Wa, we have another one right there so
127440	133640	we have already a lot of various, they are actually, they make up the who's who in machine
133640	134640	learning today.
134640	142280	And so if you open who's who in machine learning and ask the question, is this one of Mike's
142280	143280	student?
143280	149600	The likelihood of a yes is very high and so, and I think he's done this by fostering an
149600	155720	environment of inclusivity and curiosity that has really moved the entire field.
155720	159520	And so I think the whole field is grateful for this.
159520	162640	The third observation is more personal.
162640	167040	I think Mike's age is public information, at least if Wikipedia is correct.
167040	174360	So you've been at the top of your game for a long, long time and this is really unique.
174360	180000	And the way I think about Mike in the evening is I think it's a kind of the Roger Federer
180000	186920	of science, someone who has been dominating the circuit for a very, very long time with
186920	190040	no signs of slowing down.
190040	193120	Mike, welcome to Stanford.
193120	201880	All right, thank you, that was perhaps the most fun introduction I've had ever.
201880	205640	When I next time introduce Emmanuel, which will happen, I'm sure I'm gonna have to think
205640	207720	about the right metaphor.
207720	210720	Maybe someone has an idea helped me out there.
210720	215080	I gotta get my mind off that because it's a fun thing to think about.
215080	216240	So I'm a pleasure to be here.
216240	221720	I am a data scientist and in fact this kind of quote that I was like an influential computer
221720	225680	scientist is kind of funny to me because I'm trained as a control theory statistician and
225680	227280	I was never a computer scientist.
227280	230840	I embrace it because of the entrepreneurial spirit in computer science and just let's
230840	231840	try everything.
231840	232840	I love that.
232840	236160	And I found less of that in control theory and statistics, so that's great.
236160	237680	But intellectually I'm a data scientist.
237680	242400	I really want to think about how data and inference can inform real world decision making and
242400	243680	I think that's where it's at.
243680	248560	I think that it's in the first time in my career that sort of all of campus agrees.
248560	252440	It's sort of a, it's just not the truth.
252440	255920	Technologist inside of a computer scientist or statistician wasn't enough.
255920	257760	This is much more fun.
257760	261720	And indeed it's economics, which is the most thrilling to me right now, the connections
261720	262720	to economics.
262720	266000	That's really what I want to mostly convey tonight is why I think that's thrilling
266000	268280	and important and so on.
268280	271760	The elephant in the room is this thing called AI and I've never thought of myself as an
271760	272760	AI researcher.
272760	276440	I've never aspired to the Frankenstein-like thing of create some thing.
276440	277720	And I kind of want to say why.
277720	281880	Not only didn't aspire to it, I don't think it's right, but it is what everyone's talking
281880	282880	about.
282880	284320	So I want to say a few things about it.
284320	290200	So first of all, the thing that triggered all of this was back propagation, gradient
290200	293200	descent in layered neural networks.
293200	298240	There's all these other ideas along the way, unsupervised this and that, which didn't really
298240	301280	quite pan out, but back propagation had this huge impact.
301280	307160	Dave Rummelhardt was my advisor at UCSD and he developed that.
307160	311000	It's just gradient descent, so you can't say he invented it, but he invented the idea
311000	313600	of doing it in layered neural networks and applying it to all kinds of problems.
313600	315200	And he took about a year to do that.
315200	317760	It was not trivial for him.
317760	319200	He was not trying to be an AI person.
319200	322320	He was just trying to understand learning.
322320	328920	I think he'd be somewhat shocked that suddenly that becomes AI to this era.
328920	334760	So what I think is happening really is not that we have a new technology, this new brilliant
334760	337560	idea, AI, and then we start applying it everywhere.
337560	340160	I don't think that's the right way to think about it.
340160	345680	And so I think you need to go back in history a little bit and think about engineering fields
345680	346680	that have emerged.
346680	350720	So like in the 40s and 50s, chemical engineering became a thing.
350720	353920	The name was actually used, I kind of looked into it a little bit more.
353920	359720	Already by 1890, I think there was a department of chemical engineering at MIT.
359720	365520	But chemical engineering really was a very simple chemistry, kind of done its bigger
365520	367200	scale than before.
367200	372680	But polymers and all the kind of things that have triggered the revolution that we're all
372680	375600	living in, that happened in the 40s and 50s.
375600	379560	Before that, there was quantum chemistry and there was fluid dynamics and there was thermodynamics
379560	384200	and also there was a lot of deep understanding of the phenomenon.
384200	388960	And people then were able to start to envisage, what if I take the laboratory test tube experiment
388960	393040	of how you put molecules together, which I do understand because of the quantum chemistry,
393040	397600	and I do that at a huge scale in a field somewhere, will that work?
397600	399040	And of course, it didn't really work.
399040	402240	Often those things would explode, often they just wouldn't deliver a product and so on
402240	403240	so forth.
403240	406560	But over a 20-year period in the 40s and 50s, it kind of started to get worked out.
406560	412000	And an engineering field emerged that had huge impact on all of our lives.
412000	414920	Electrical engineering, I know less about the history, but obviously there was Maxwell's
414920	418720	equations before there was electrical engineering, so it was a full understanding of the phenomenon
418720	419720	at some level.
419720	424480	But it wasn't clear how to bring electricity into homes, how to make it safe, how to do
424480	427120	communication on top of the waves, and so on and so forth.
427120	431240	So a whole field emerged, which we now call electrical engineering, that did all that
431240	432680	in the early part of the past century.
432680	435640	It took a couple of decades again.
435640	437000	So I think that's what's happening right now.
437000	439480	We have a new engineering field emerging.
439480	446440	I wouldn't call it AI, but it's a field that's based on flows of data, networks, flows, inferential
446440	452440	ideas, large-scale decision-making, cooperative endeavor, building transportation systems,
452440	458800	commerce systems, healthcare systems, it's all part of this engineering field.
458800	460000	That's really what's happening.
460000	464840	And so it's the first engineering field that has got, as its objects of study, not just
464840	471480	bits and information and atoms and laws of physics, it has humans involved, critically.
471480	475200	So utilities and aspirations and so on.
475200	480520	And so economics has minimally got to be involved, but the rest of the social sciences as well.
480520	482720	And the implications are vast.
482720	486920	So actually, that's the phenomenon, and it's going to take 20 or so years.
486920	490800	The difference, though, with these others is that there was a deep understanding of some
490800	492360	underlying phenomenon there.
492360	493360	We don't have that now.
493360	498200	We do not understand intelligence, I can assure you.
498200	504600	So we're calling it AI as if we got this understanding and then it leads to technology, and I think
504600	507480	that's kind of backwards.
507480	509280	So let's just say, why did this happen?
509280	516080	Well, first of all, there was McCarthy and so on in the 50s who invented this terminology,
516080	517480	and for good reason.
517480	523040	There was a philosophical aspiration, almost.
523040	526760	There had been discussions of mind and body, and now we have a computer, it has software
526760	530120	and hardware, it looks like mind and body, and it looks like we can now make headway
530120	531120	on that.
531120	532480	And let's talk about putting thought in a computer.
532480	535080	That's a really interesting thing to talk about.
535080	538440	And of course, people got excited about that notion and worked on it.
538440	542520	We don't have thought in a computer to this day, and it's not clear why we really care
542520	543520	in some sense.
543520	550120	It'll somehow emerge and we'll call it thought, but it's not clear what that means.
550240	552440	And in the meantime, that's not what happened.
552440	554480	What computers started to do was aid humans.
554480	556080	They became complementary.
556080	560800	Search engines and translation systems and all that aided our own intelligence and expanded
560800	565400	it and networks expanded a planetary scale.
565400	570080	So let's not call it McCarthy's version of intelligence for sure, but that aspiration
570080	571080	still exists.
571080	577360	And people who study psychology and neuroscience and core AI, whatever that is, they are working
577360	578360	on that.
578480	580480	It's a worthy thing.
580480	583240	But we should be waiting for that because in the meantime, all these systems are being
583240	585920	built in the real world that are having this huge impact.
585920	587920	We should understand the phenomenon.
587920	588920	All right.
588920	594160	Now, the other part that happened, it wasn't really so much McCarthy, but others, it had
594160	596160	to be autonomous.
596160	598600	Why did the AI have to be autonomous?
598600	603000	Well, if it's not autonomous, if it's tethered to me, it doesn't seem so intelligent.
603000	605000	It's tethered to me.
605000	608160	And if it's developed by vast numbers of humans, it's engineers who built something
608160	610480	and it doesn't seem so intelligent.
610480	614680	So it had to kind of be built by small numbers of people and it had to be all on its own.
614680	620360	Now, that's a okay kind of science fiction-y kind of aspiration, but it's a bad idea for
620360	621360	technology.
621360	623560	You don't develop technology that way.
623560	625920	You don't want self-driving cars to be autonomous.
625920	629960	They should be highly networked so you think about the overall traffic system so you don't
629960	634120	ever have an accident, just like air traffic control.
634120	636680	You don't want autonomous airplanes.
636680	640480	So there'll be a lot of cartoons in this talk.
640480	642400	I don't mean it's never a good idea.
642400	645480	So a burning building, I want an autonomous robot.
645480	650920	Up on Mars, I want some double autonomy, but for most applications, I don't want the intelligence
650920	651920	to be autonomous.
651920	658640	I want it to be federated, linked in, transparent, cooperative, all those things.
658640	662160	So I think this was a big mistake to add that to the list.
662160	666080	I think it became kind of about bragging rights, look at my autonomous AI, how great
666080	668680	it is and it's better than your autonomous AI.
668680	672400	And again, this is all kind of fun and games for like 40 years, but it's no longer fun
672400	673400	and games.
673400	676120	It's actually going to hurt the planet.
676120	679200	All right, so here's a counterpoint, which is that first of all, this is kind of maybe
679200	683520	an obvious statement, but if we want to talk about intelligence, there's not one kind of
683520	684520	intelligence.
684520	685520	It's not just human intelligence.
685520	688400	It's as much about the collective as it is about the individual.
688400	690800	And an economist thinks this way all the time.
690800	697280	They recognize that a market is composed of many small decisions by entities that don't
697280	698760	have to be intelligent themselves.
698760	703920	They just have to kind of know a demand curve and follow some of their nose.
703920	707880	And you're not using huge intelligence within it, but the overall market becomes really
707880	708880	intelligent.
708880	713000	It could do things like bring food into cities, bring or shine at any scale for hundreds
713000	717240	of years, and it can create all kinds of opportunities.
717240	720800	And then there's like ant swarms that we talk a lot about, not an individual ant might
720800	723680	not be so smart, but the swarm could do amazing things.
723680	726160	So we're all aware of that, but too dimly.
726160	729240	I don't think we understand that we could be creating new kinds of collectives that are
729240	733240	really exciting, that do new things as human beings.
733240	735720	That's what's opening up to me in the era.
735720	739560	Not the super intelligence replacing a human, look at how great that is.
739560	744600	All right, so in particular, if you're going to be a little less exuberant, but you're
744600	749200	going to say, what are the goals for this emerging field?
749200	753400	It's not make a super intelligence at a computer, and you're done.
753400	758000	It's rather what is the overall object, like the factory in the field?
758000	759440	Is it a transportation system?
759440	760720	Is it a logistics chain?
760720	762600	Is it a healthcare system?
762600	766800	Is it a communication system designed for that level?
766800	770320	And then think about what the components need to be and what data is needed and all that.
770320	774880	It sounds more boring than a typical AI person's talk, which is, we'll solve intelligence
774880	777760	and then the intelligence will solve climate change.
777760	781760	That's a typical Silicon Valley thing to say.
781760	785240	It sounds great and you get rid of it in the New York Times, but really to me, logistics
785240	790080	chains and supply chains are much more exciting and interesting and important for human life
790080	792080	and healthcare.
792080	794480	And it's not that the AI is going to solve healthcare.
794480	799760	It's us designing really good systems with good data science principles and economic
799840	800840	principles.
800840	803880	So I think I've said all this, mimicry is just not a good way to think about the implications
803880	804880	of collectives.
804880	808920	Autonomy is also maybe a losery.
808920	811120	And so there might be new forms of collectives.
811120	812120	Okay.
812120	816640	So if you want to read a little bit more about this kind of philosophical ruminations, I
816640	822080	wrote an article, our official that the Revlon hasn't happened yet, three years ago, and
822080	825480	I still very much stand behind everything in there, even though we've had this kind of
825520	831640	upswing and surprising chat GBT abilities, this was like about where's the data come
831640	832640	from?
832640	833640	What's the provenance?
833640	834640	What is the bigger scope?
834640	835640	And all that.
835640	839120	And so if you want to read about that, and then there was a bunch of commentary by including
839120	843000	Emmanuel Candace by some luminaries and it was quite a lot of fun.
843000	847560	So and there was my response to those luminaries.
847560	851840	And then with some colleagues, mostly social scientists all down here, we wrote a paper
851840	855160	about two years ago called How AI Fails Is.
855160	858480	And it's less about the kind of economics perspective that I was pushing up above and
858480	866480	more about what are the implications for technology if you've got like autonomous systems being
866480	869520	designed by small numbers of people.
869520	875080	That kind of incentivizes entities like open AI, that they get vast amounts of money for
875080	880000	a small number of people, they build this thing, and it's not for everybody.
880000	881640	They control it.
881640	884680	And it was supposed to be open, it's no longer open.
884680	890840	And so this idea that AI is the future, it just has a natural tendency towards making
890840	894040	it be in the hands of small numbers of people.
894040	897800	And again, I think this article kind of gets into some of the social science reasons why
897800	900120	that's just really a bad idea.
900120	902480	And people pretend that it's not happening.
902480	907240	It's all open and all that, but that's a pretense, it's just not true.
907240	911840	So if we stop thinking about AI this way, I think it'll actually liberate us from that.
911840	914880	All right, so again, I've already sort of said this, but just to lean in a little bit
914880	918160	more, McCarthy had this imitative perspective.
918160	921120	It was a great aspiration and still remains one.
921120	922640	It's just not what's happened.
922640	928200	What really happened was more like IA, that's Doug Engelbart there who kind of talked about
928200	929960	technology, augment our intelligence.
929960	931960	And for certain, it has.
931960	935600	The search engine has augmented my intelligence more than just about any other piece of technology
935600	940880	that I can think of, in addition to everybody's intelligence.
940880	944720	And then this third bullet is kind of what I think is a better description of what's
944720	946240	really emerging.
946240	947440	It looks kind of like Internet of Things.
947440	950640	They've got all these little devices around, they all send data around, and decisions are
950640	951640	being made.
951640	954280	It's all delocalized and everything.
954280	956320	But Internet of Things was a little too computer sciencey.
956320	960000	It wasn't thinking about the data and the inferences and the predictions and the people.
960000	963280	It was just about put things on the Internet.
963280	966520	But anyway, that is still the right spirit, and I think this is really what's happened.
966520	969800	Even like the pandemic response of the planet.
969800	977000	That was a engineering system that sort of didn't work okay, but we could do better.
977000	982040	Now, if you go to an ML person or an AI person and say, okay, aren't you guys thinking about
982040	983040	this?
983040	984920	Is it all this classical AI stuff?
984920	985920	And they say, no, no, no.
985920	986920	We work on this.
986920	989440	Here's, for example, federated learning.
989440	991480	It's decentralized learning.
991480	996560	So you have a server up there, and they're collecting data from a bunch of edge devices,
996560	1001240	and then they're analyzing the data centrally, and we're worried about privacy and all that.
1001240	1003080	So we got the social stuff.
1003080	1006240	This is our, we handled all the social stuff.
1006240	1007240	All right.
1007240	1009920	Now, I'm being a little bit, again, cartoonish here.
1009920	1015280	But at terminology of federated learning, a number of groups were working on it, but
1015280	1019160	it's a Google patented, and not patented, but it's a Google terminology, because Google
1019160	1023360	wanted to collect a lot of data for their speech engines.
1023360	1026200	And so everybody has cell phones and is talking on their phones.
1026200	1028600	Let's just collect a lot of data from that.
1028600	1031040	Let's worry about the compression.
1031040	1032760	Let's get the gradients back cheaply.
1032760	1035440	Let's also do some differential privacy, and that's the technical problem.
1035440	1038800	If we solve that, wow.
1038800	1040360	But what's missing in this picture?
1040360	1041360	All right.
1041360	1044960	Well, I'm going to give some examples of what's missing to make it more clear.
1044960	1048520	But what's missing is that these are actual humans here, and they have their own values
1048520	1052920	and goals and aspirations, and they want to join this collective for some reason.
1052920	1056840	They don't want to just be assumed that they are in the collective because they want Google
1056840	1059360	to build a bitter speech model.
1059360	1060880	Okay.
1060880	1065000	So the nodes are often people, and they value their data.
1065000	1068960	And by data, I don't just mean where I went today and what was around me and all that.
1068960	1075200	I mean, things I created, works of art, things I wrote, songs I wrote, et cetera, et cetera.
1075200	1076200	That's my data.
1076880	1080760	Stuff that's on the internet now that's being exploited by other companies, and I've lost
1080760	1081880	all value.
1081880	1082880	That's wrong.
1082880	1085080	All right.
1085080	1089640	So we need to talk about cost and benefits of these decentralized paradigms where learning
1089640	1090800	is involved.
1090800	1094440	So we need learning to wear mechanisms, and mechanisms that we're learning.
1094440	1097520	Mechanism is an economics terminology, and I want to get into that.
1097520	1103040	So I'm going to give some more kind of industrially, real-world examples, but as an academic, I
1103040	1107360	needed to kind of think a little bit about what's happening academically.
1107360	1112880	Are we kind of a set up for this emerging discipline, whatever you want to call it.
1112880	1115120	And I'm not sort of sure we are.
1115120	1120520	So the three disciplines, and it's not really the disciplines, it's the styles of thinking
1120520	1125840	that I think are most important here, and I don't want to exclude anybody, but computer
1125840	1130760	science certainly, the algorithms, the networks, and so on, statistics, and economics.
1130880	1135280	Just to say there are pair-wise interactions among these fields for quite some time.
1135280	1137200	Computer science meets statistics, that is machine learning.
1137200	1141640	In fact, I would argue machine learning is just statistics with kind of a computer science
1141640	1143520	way of thinking.
1143520	1147760	Every time I see a new idea in machine learning, I know that it already exists in statistics,
1147760	1148760	and I tell people that.
1148760	1152920	They get mad at me, but eventually it kind of...
1152920	1158480	And there's lots of ideas and statistics they don't yet know about, too.
1158520	1162080	I could give lots of examples, but I won't.
1162080	1165680	Statistics meets economics, that's econometrics, and I've got Hito and others in the audience
1165680	1168240	who are masters of that.
1168240	1171440	Well it's great, but it's kind of about measuring the economy.
1171440	1175640	That's what the main goal has been, doing the causal inference to measure the economy.
1175640	1180680	And it's less about algorithms and mechanisms and engineering kind of thing, artifacts.
1180680	1185480	So it's had its important role, but it's missing that third leg.
1185480	1188080	In economics meets computer science, that's called algorithmic game theory.
1188080	1190880	That emerged 15, 20 years ago.
1190880	1194880	It's very important field, study of auctions and combinatorial auctions and how they behave
1194880	1196200	and incentives and all that.
1196200	1198680	What's missing there is they have no statistics.
1198680	1201960	They don't worry about gathering data and changing the preferences and learning them
1201960	1204600	from as part of the auction and all that.
1204600	1210000	So all three of these pairwise things exist, but they're critically missing the third leg.
1210000	1214120	Now the interesting thing is if you go into an industry, and I spend a day a week at Amazon,
1214120	1217760	and you look at any real world problem they are studying, like how do we provision, how
1217760	1221720	do we interface with third party sellers, blah, blah, blah, there's always all three
1221720	1224240	disciplines around the table.
1224240	1228280	And just to add, there are always operations research people who already have kind of ingested
1228280	1233000	all three disciplines, just to say, and control theorists and mathematicians and so on.
1233000	1238080	So I don't mean to exclude anybody, but it's never one of those perspectives alone.
1238080	1239920	That kills you if you just have one of those perspectives.
1239920	1240920	You need all three.
1240920	1246240	All right, here's a real world example that I've been involved in.
1246240	1249040	So I'm a musician, I just end up being an academic.
1249040	1253600	And I met up, I have a friend, Steve Stout.
1253600	1254600	Someone introduced me at some point.
1254600	1263280	Steve is a legendary producer, entrepreneur, well known in the hip hop and the Latin world
1263280	1265240	and so on.
1265240	1272160	And he and I kind of came together on this idea of modern data, modern systems, platforms
1272160	1274840	should not just be about taking streaming bits.
1274840	1276760	Music shouldn't just be about streaming.
1276760	1280520	It should be about creating two and three-way markets.
1280520	1285360	And so the idea that we originally sat down and talked about, and Steve is the CEO of
1285360	1288800	now a company that has taken this and made it real.
1288800	1294480	It's called UnitedMasters.com, or United Masters is the company.
1294480	1296240	Basically provides a three-way market.
1296240	1302960	So if you make music and now you can sign up with United Masters, they give you a record
1302960	1307920	company in your pocket, you're able to kind of produce songs on your cell phone and upload
1307920	1315120	them to United Masters and then they connect that to a market on the other side.
1315120	1319040	So in particular, Steve has gone to the NBA.
1319040	1323720	The NBA used to be streaming music from the record companies and they would pay the record
1323720	1327680	companies a royalty and they might give some money back to Beyonce or whatever.
1327680	1331280	But most musicians are not the big famous ones.
1331640	1336560	In fact, if you look at the data, if you do some actual data science, today 95% of the
1336560	1341120	songs being listened to in the United States played by people you've never heard of and
1341120	1345040	they're probably between 16 and 20 years old and the song was probably recorded in the
1345040	1346560	last six months.
1346560	1350240	So everybody thinks we're all listening to the Beatles and Madonna or whatever.
1350240	1351240	It's just not true.
1351240	1355440	All right, so you think, wow, there's this wonderful market that's been created because
1355440	1359040	of the ability to stream music and you'd be wrong because it's not a market.
1359040	1360240	No one's making money.
1360240	1362600	All the 16 to 20 year olds are not making any money off of this.
1362600	1368600	They do it for a few years and then they disappear.
1368600	1374040	So well, what Steve has done is by creating our masters is that a musician signs up and
1374040	1377920	now there's 3 million young musicians signed up on the platform.
1377920	1382120	And if you now go to the NBA website and you watch a video, there'll be some music behind
1382120	1386280	it, that music is streamed from United Masters.
1386280	1389600	And when every time it's streamed, the musician gets paid.
1389600	1391600	It's their actual two-way market.
1391600	1394680	And it's in fact a three-way market because it's got the NBA, it's got the listener,
1394680	1397960	which is you and me, and it's got the person who made the music.
1397960	1401760	And now all kinds of, I could give a longer talk about that, but all kinds of other market
1401760	1404140	sort of forces are starting to come to play.
1404140	1407080	People are reaching out to musicians and partnering with them.
1407080	1408080	Shows are being made.
1408080	1409760	People are playing at weddings.
1409760	1412960	There's 3 million people who now have access to a steady income stream.
1412960	1417320	So this is a sense in which AI can create jobs.
1417320	1421040	3 million people have access now to a possible job.
1421040	1427160	And these are 16 to 20-year-olds in the inner city, just to say, this is quite important.
1427160	1428160	And that's just in the US.
1428160	1430600	This can be done in every country around the world.
1430600	1434600	And entrepreneurs thinking about a new company, instead of thinking about how do I steal some
1434600	1440120	bits from somebody and then sell them, should think about how do I create a two-way market.
1440120	1444120	And I just help the market get going.
1444120	1445120	You could do this for art.
1445120	1448960	You could do this for works of, you know, scholarly works.
1448960	1452000	You could do this for travel information, all kinds of things.
1452000	1454120	You can start to think more about markets.
1454120	1457760	Okay, so that was the first half of my talk.
1457760	1460560	That was kind of why do I work on what I'm working on, okay?
1460560	1463720	And so hopefully you get a little bit more of the picture.
1463720	1468600	It really is, in some sense, economics and mechanisms and, you know, networks and all
1468600	1469600	that.
1469600	1473480	But, you know, with all due respect, those fields didn't have enough of a statistics
1473480	1474480	and learning perspective.
1475040	1479720	They assumed a lot of things were already known and you get certain curves that cross.
1479720	1483560	But they didn't kind of just adapt the market as you went and use large data sets to inform
1483560	1485760	it and have recommendator systems.
1485760	1488680	You don't see economics talking about recommender systems.
1488680	1492480	Recommender systems are the way that social knowledge gets used and exploited among groups
1492480	1494480	of people.
1494480	1498000	So anyway, when you start thinking about what are the new problems that are going to emerge
1498000	1503400	if you put these three axes together, it's really quite exciting.
1503400	1507320	So in machine learning and statistics, we're really good about talking about optima.
1507320	1511800	We can find optima in hundreds of thousands of dimensions even if they're saddle points
1511800	1516200	and we can guarantee a rate and prove theorems about it and we're really, you know, we're
1516200	1518120	really good at that.
1518120	1521360	But in economics, you don't often find optima.
1521360	1523120	You find equilibria.
1523120	1525440	And moreover, the equilibria are rarely just stationary.
1525440	1527240	They're moving around and you need to follow them.
1527240	1529320	And so you need to talk about the dynamics.
1529320	1534240	And so now there's topological issues and dynamical systems issues and stochastic process
1534240	1537040	issues all merged together.
1537040	1541400	And so there are algorithms, you know, gradient descent does not work for finding equilibria,
1541400	1543920	but extra gradient does work and so on.
1543920	1546600	There's a whole emerging, it's fixed point theory.
1546600	1552640	So most of these ideas go back to the 30s and 40s, but they have been forgotten.
1552640	1556760	But fixed point theory in hundreds of thousands of dimensions with stochastics, that's something
1556760	1561000	we can start talking about and do and prove rates and, you know, get really new algorithms.
1561000	1563960	And there are people doing that now.
1563960	1565960	Exploration, exploitation, incentives in multi-way markets.
1565960	1569280	Those are words that usually don't come into the market perspective.
1569280	1570280	How do I exploit?
1570280	1571280	How do I explore?
1571280	1572480	And how do I put that together with incentives?
1572480	1576200	I'm going to talk about some of the rest of these, but let me just sort of highlight.
1576200	1579760	These are mostly words you will not see on a machine learning person's talk or AI person's
1579760	1580760	talk.
1580760	1584560	They will talk about trust maybe or fairness or privacy, that's all good.
1584560	1585960	Those are social concepts.
1585960	1590600	But they don't embed it in a fuller, what are the underlying foundational principles
1590600	1595080	that make it fair or make it private or make it valuable to people.
1595080	1601800	They just want to kind of stamp privacy or, you know, or fairness on it and that's enough.
1601800	1605080	So let's try to think about what are these underlying concepts.
1605080	1608200	And let me just say that I've loved learning all this economics.
1608200	1612240	I had learned a lot of statistics and I've eventually learned some computer science.
1612240	1614600	And that was fun, but learning economics has been particularly fun.
1615000	1617120	And it's maybe because I already knew the math and I could just kind of go through the
1617120	1619440	books really fast.
1619440	1624120	But you know, this notion of incentives and really thinking about asymmetries and decentralized,
1624120	1628600	I really get that out of economics in ways I never got from any other field.
1628600	1633480	So I'm having a lot of fun here and I'm realizing that if I'd gone back to the 1950s and I'd
1633480	1637920	hung out with David Blackwell and Von Neumann and others that they were doing all this.
1637920	1639800	This was kind of the spirit of the era.
1639800	1643600	And operations research emerged in that era, kind of bringing it all together.
1643600	1645960	And somehow that all got kind of forgotten.
1645960	1650320	We got all buried into building certain kind of systems or doing certain kind of data analysis
1650320	1656520	or, you know, measuring certain kind of linear models and we forgot about the overall picture.
1656520	1661120	Okay, so these blue ones are the ones I'm going to kind of use now as vignettes and
1661120	1663320	the rest of my talk.
1663320	1667400	And so this is, given this is an evening talk, I don't want to make this a highly technical
1667400	1668400	academic talk.
1668400	1672080	There are archive papers on all this with theorems and so on and so forth.
1672080	1676040	But I do want to give the sense of what's the problem and what is the theorem, all right,
1676040	1677040	and what is the consequence of that.
1677040	1680840	Okay, so I'm going to give enough of that to highlight some of these issues.
1680840	1682800	So I picked these three to talk about in some order.
1682800	1684080	I forget which order.
1684080	1687600	Okay, so here's perhaps my favorite one.
1687600	1690000	I get to recognize two Stanford people.
1690000	1696480	Stephen was a student with Emanuel and joined my group two years ago, three maybe.
1696480	1698200	You know, fabulous intellect.
1698240	1702320	Michael, I actually don't, we've only, because of the pandemic, met online, but
1702320	1704040	he is a Stanford person.
1704040	1706040	And then Jake was a student with me.
1706040	1711440	He's now a postdoc with one of Emanuel's ex-students, Serena Fogelbarber.
1711440	1712720	So a lot of nice connectivity there.
1714520	1716720	Okay, with all due apology to the economists in the room,
1716720	1718400	I'm going to say a little bit about incentives.
1719680	1722240	There's a kind of general theory of incentives.
1722240	1723640	There's books on it.
1723640	1726960	And roughly speaking, there's kind of three branches to it.
1726960	1729440	There's auction theory that you all know about.
1729440	1733240	There's matching markets, and there's contract theory.
1733240	1736600	So contract theory is maybe the less well-known outside of economics, but
1736600	1739360	you all know about it because you experience it daily.
1739360	1743680	It's where agents provide, possess private information, and
1743680	1746840	there's a principal who wants to incentivize to do something with that
1746840	1747560	private information.
1748840	1749920	So why does this happen?
1749920	1753480	Well, you know, the boss wants to get the employees to do something.
1753480	1756920	And it's not just because the boss would do it themselves, but
1756920	1758680	you know, they have to get the employees to do it.
1758680	1759880	The boss would know how to do it.
1759880	1761600	The employees have local information.
1761600	1763040	They're smarter.
1763040	1766440	They may, if they're incentivized, they'll do even better work and so on.
1766440	1770440	And now the boss has got to kind of offer them incentives so
1770440	1771920	that they'll actually do the labor.
1771920	1775920	So this came up in economics, sort of after General Equilibrium Theory,
1775920	1779840	which was very symmetric, Nash Equilibria and everything is very symmetric.
1779840	1782400	This recognized that real life is full of asymmetries.
1782400	1784360	There's someone trying to get someone else to do something and
1784360	1787280	that person has power because they know something that it's not known upstairs.
1789520	1793080	All right, so you know about it because you've all, for example, you travel.
1793080	1797920	And you probably have wondered why aren't there, why is this so complicated?
1797920	1800760	Why is there not one fare for every seat on the airplane?
1800760	1802040	Like there is for a movie theater.
1803400	1805320	All right, and you all know the answer kind of, right?
1805320	1808560	Because there's different willingness to pay in the population.
1808560	1812000	So a business class traveler or a business traveler, not a business,
1812520	1817280	traveler, maybe the company's paying so they could care less what the fare is.
1817280	1820080	Or maybe they're really urgently needing to get from one place to another.
1820080	1821920	They have high willingness to pay.
1821920	1823960	And there's a lot of other people who don't have high willingness to pay.
1823960	1826120	They could wait till tomorrow, you know, and so on.
1826120	1831080	So the airlines really in the 80s realized that they could start to price discriminate.
1831080	1834360	They could try to figure out who had higher wills to pay and
1834360	1837800	charge them higher and who had lower wills to pay and charge them less.
1837800	1841760	And fill the airplane and get a blend of both kinds.
1841760	1844520	And you've got to be clever to do this, right?
1844520	1848120	And so what you do, if you set a single price, that's not going to work.
1848120	1850520	And if you try to screen for people, like, you know,
1850520	1854800	look at somebody wearing a suit and tie, you say, I'm going to charge you more.
1854800	1857480	Well, that person's going to the next time show up in jeans.
1859200	1861320	All right, so people are doing this all the time.
1861320	1861960	They're aware.
1861960	1863120	They're gaming the system.
1863120	1864480	And you've got to think this through.
1865760	1867040	All right, so you know the answer.
1867040	1869280	What you do is you provide a menu of options.
1869280	1871080	You provide a service and you provide a price.
1871080	1872320	And a service and a price.
1872320	1875280	And everybody gets the same menu, right?
1875280	1876600	Now what is this menu for the airline?
1876600	1880240	Well, there's this class called business class and the students don't know about
1880240	1882840	this yet, but eventually they'll learn about it.
1882840	1886440	Where you get a little glass of red wine and you get to be first in line and
1886440	1889360	be all proud of yourself and you get a little bit bigger chair.
1889360	1892080	And people will pay $1,000 more for that.
1892080	1894320	It's amazing, right?
1894320	1898400	Now, only class that actually makes money, right?
1899400	1905280	But the marginal cost of putting people on airplanes is sort of zero, all right?
1905280	1907880	So you want to fill the rest of the airplane, all right?
1907880	1911720	So amazingly, there are people in the back who don't want to spend $1,000 for
1911720	1913080	a little glass of red wine.
1913080	1915520	And they feel very good about themselves because they didn't spend all that money
1915520	1917000	and they're still on the airplane.
1917000	1918280	So everybody's happy.
1918280	1920760	That's what's called social welfare.
1920760	1921720	And the plane is full.
1921720	1922720	That's what's called revenue.
1923760	1926160	Okay, so you can make mathematics out of all that.
1926160	1928080	So you get the usual crossing curves and all that.
1928080	1930520	They're just not the same crossing curves as in general equilibrium theory.
1930520	1932200	They're a different set of things.
1932200	1937520	But every one of those texts says we have missing information.
1937520	1939320	We're going to assume there's some probability distribution and
1939320	1941400	we're going to call the whole thing Bayesian.
1941400	1944760	Now, as a statistician, I look at it and say, wow, Bayesian, it's not Bayesian.
1944760	1947920	There's a distribution on unknown quantities.
1947920	1948680	That's all.
1948680	1952960	There's no updating, there's no learning, there's none of the above, okay?
1952960	1954920	All right, so wow, wonderful opportunity.
1954920	1955840	We should work on this.
1956840	1957600	And we have.
1958800	1961240	Okay, so you all know about clinical trials.
1962520	1967720	Costs tens of millions of dollars a year to run clinical trials in any particular
1967720	1969000	therapeutic area.
1969000	1971080	You all know about it for vaccines.
1971080	1973600	It's amazingly expensive and it's amazingly important.
1973600	1975880	And if you don't do it at the right scale, you'll make big mistakes.
1977040	1977680	You all know this.
1978720	1982320	All right, so you would imagine that the FDA does a great job of this.
1982320	1983400	And in some level they do.
1983400	1984800	They're very good statisticians.
1984800	1987400	But they're not good at being the economics.
1989240	1992120	All right, so this really should be thought as a contract theory problem.
1992120	1995760	The FDA is a principle and they're trying to decide what drugs go to market.
1997000	2001320	But they only have partial knowledge about the drug candidates, okay?
2001320	2002800	Where do the drug candidates come from?
2002800	2005600	They come from the pharmaceutical companies.
2005600	2008640	Pharmaceutical companies know something internally about some candidate.
2008640	2011120	They're about ready to send up to the FDA.
2011120	2013160	Maybe they know they put their best engineers on it.
2013160	2014760	Maybe they've had experience with it.
2014760	2019480	Maybe a little internal testing, so on and so forth, all right?
2019480	2021480	The FDA is now getting all these candidates and
2021480	2023960	they would like to say, pharmaceutical company,
2023960	2026000	that candidate you just sent me, how good is that candidate?
2027000	2029520	Well, the pharmaceutical company does not want to reveal.
2029520	2032240	Because the FDA, if they're told it's not a good candidate,
2032240	2034880	they'll put yet more, they'll ensure there's no false positive.
2034880	2038160	They'll put yet more clinical trial money into it.
2038160	2040600	Where if they think it's a really good candidate, they won't.
2040720	2044120	And also the license they will get will be titrated to risk.
2045240	2047320	All right, so the companies are incentivized to not say.
2049840	2051280	All right, but that's a problem.
2051280	2053720	All right, so now let's think about the actual paradigm.
2053720	2054800	What is the FDA doing?
2054800	2058160	Well, they're being statisticians, a frequentist statistician.
2058160	2062200	So here's a little Naaman Pearson kind of setup.
2062200	2065560	A bad drug, theta equals zero, doesn't mean that it hurts people,
2065560	2067520	because they definitely screen for that.
2067520	2069840	It just means it has no effect, all right?
2069840	2073040	And there are tons of drugs on the market that have no effect,
2073040	2074880	for better or for worse.
2074880	2078800	And they have a type one error of say 0.05, it's actually more like 0.01,
2078800	2082560	but they set up a classifier that achieves that.
2082560	2085560	And then for the good drugs that are actually having an effect,
2085560	2089400	they want a high power, so 0.8 is a kind of a standard number for that.
2089400	2091000	Is that a good protocol?
2091000	2093240	Well, it's optimal, it's the Naaman Pearson test.
2093240	2095040	So yeah, of course, it's great.
2095040	2096520	But is it a good protocol?
2096520	2097160	And the answer is no.
2098160	2099960	So let's do a little thought experiment here.
2099960	2103560	In situations where there's a small profit to be made,
2103560	2106840	it costs $20 million to run the trial.
2106840	2110240	But if you're approved, let's suppose you would make 200 million.
2110240	2112880	So this would be for a niche drug of some kind.
2112880	2119400	And so the CEO can do a little calculation, as can the FDA,
2119400	2121640	conditioning on theta equals zero.
2121640	2125640	Now no one knows if theta is zero or not, so this is a counterfactual.
2125920	2130760	But thinking conceptually, theta equals zero, what's my expected profit?
2130760	2134240	Well, you can put all those numbers together and you get minus 10 million.
2134240	2136320	All right, so the CEO looks at that number and they say,
2136320	2140360	only send candidates up to the FDA if you're really pretty sure it's a good drug.
2142080	2143920	That it's going to get passed because it's a good drug.
2143920	2145680	Don't hope for a false positive.
2145680	2147000	We'll go out of business.
2148080	2148800	That's great.
2148800	2153080	Now the FDA is mostly getting good drugs and they have a good screening procedure.
2153080	2154760	So everything that's getting through is looking good.
2155840	2158000	If that were real life, that'd be great, but here's more like real life.
2158000	2162080	So you have $20 million to run the trial and if you're approved,
2162080	2163640	you could make 2 billion.
2163640	2165680	So this would be like ibuprofen or something.
2165680	2167480	So this is more common.
2167480	2169920	And now the CEO could do the same exact calculation.
2169920	2172880	If it was the case that theta is equal to zero,
2172880	2175040	my expected profit would be 80 million.
2175040	2179480	So now the CEO is very incentivized to send as many candidates as they can to the FDA.
2179480	2182760	And the FDA will get flooded and they do get flooded.
2182760	2185040	And they will do these tests and there will be some false positives.
2185040	2185960	And these things will go to market.
2185960	2187680	They don't hurt anybody, but they just don't have any effect.
2187680	2190800	And people will make money and then eventually that changes.
2190800	2192360	So this is broken.
2192360	2195120	And it's just broken because it's not being thought of as a contract theory problem.
2196200	2199600	All right, so we have now lured on this.
2199600	2203720	We have a paper and we have an idea we call Statistical Contract Theory.
2205000	2206680	And so here is the protocol.
2207960	2209080	There are four steps to it.
2209080	2210760	It's only step three, which is new.
2210760	2213360	The other three are standard contract theory.
2213360	2217720	So an agent comes to this contract and they opt in or they just decide to walk away.
2217720	2221160	So the drug company comes and they just looked at it and say, no, I'm not interested.
2221160	2226200	Or if they opt in, they have to pay a reservation price R, say 20 million.
2226200	2229000	And then they get to select a payout function from a menu.
2229000	2230960	And I'm going to say more about what that means here in a moment.
2232480	2239800	But it's going to be a function from observed the clinical trial to the amount you get to licensed for.
2239800	2240880	And we're going to design the menu.
2240880	2244640	That's going to be our goal as economical statisticians.
2244640	2250160	Then we do a statistical trial, which yields a random variable, Z, coming from P of theta.
2250160	2253600	Theta is the true theta in nature because we're getting data from the real world.
2253600	2256760	No one knows theta, but we get data from P of theta.
2256760	2257960	And then there's the payoff.
2257960	2260040	So agent gets payoff F of Z.
2260040	2263920	They were the ones who selected the payout function, so they get paid that amount they selected.
2263920	2268280	And the principal receives a utility, which is a function of F of Z because they have to pay that.
2269240	2274800	And theta, because the FDA, if they make lots of approvals of not so good drugs,
2274800	2278080	they'll eventually look bad, and so their utility should reflect that.
2279400	2282440	Agents in this setting need to maximize their payoff.
2282440	2287720	Their best response is simply to take the arg max of the expectation under this data of the payoff.
2287720	2289400	That's what they want to maximize.
2289400	2291480	So that's pretty clear what an agent should be doing in this paradigm.
2292600	2295040	All right, now if you're going to do economics together with statistics,
2295040	2298000	the key thing you have to think about is incentive alignment.
2298040	2306080	Am I doing a situation where the incentives or what I want to achieve is aligned with people's interest?
2306080	2308360	All right, so here's a way to set that up.
2308360	2313200	For the null agents, those who have the null candidates,
2313200	2321880	it should be the case that the utility of the principal is decreasing in F of Z, okay?
2321880	2326720	Whereas for the non-null agents, for a good drug, the utility should be increasing in F of Z, okay?
2326760	2327760	So it's kind of obvious.
2329760	2335040	So, you know, in English, the principal wants to attract as transact as much as possible with the good agents,
2335040	2336280	the ones that have a good drug.
2336960	2342080	All right, so now the definition is that a menu of these options is incentive aligned.
2342080	2348040	If it is the case for all of the null drugs, the expectation under the null of the difference
2348040	2350800	of the payout and the reservation price is less than or equal to zero.
2350800	2354360	If that weren't true, then these companies just make money for free, okay?
2354400	2358720	So you need to have that be the case, so the principal would be happy with this.
2359320	2363600	The P less than or equal to 0.05 protocol that we're used to from statistics is not incentive aligned.
2363600	2364800	That's simple to see.
2366080	2372400	Okay. All right, so now we have a theorem, which is right down at the bottom there,
2372400	2377440	which is that it turns out that a contract is incentive aligned if and only if,
2377440	2382280	this is a characterization, all of the payout functions are E values, right?
2382280	2383440	So what's an E value?
2384440	2386800	Well, it's like a P value kind of.
2386800	2390720	It's a statistical measure of evidence, but
2390720	2393600	whereas a P value is a tail probability under the null,
2393600	2397800	the probability of under the null hypothesis being more extreme than the observed data.
2397800	2399520	That's a P value.
2399520	2401800	And E value is under the null hypothesis.
2402760	2406840	The expectation of this E value is less than or equal to 1, okay?
2407840	2416160	It looks a little bit like a martingale, a super martingale, and in fact is the more general story is these are non-negative super martingales.
2416160	2418360	And because they're martingales, they kind of compose nicely.
2418360	2420200	You can stop them because of stopping theorems.
2420200	2422760	They just are a nicer measure of statistical evidence.
2422760	2424120	Whereas P values don't compose.
2424120	2424960	You can't stop them.
2424960	2425960	They just have all these troubles.
2427040	2431680	So this is a neat result, which is that this concept from theory of contracts
2433120	2436560	is exactly the same concept as E values in statistics.
2437800	2440360	And moreover, we have a result, which I don't think I have a slide on it.
2440360	2441800	Nope.
2441800	2446360	That if we now want to do, how do you actually design a menu and get,
2446360	2455480	say, a maxi min menu, the maximal overall theta of the minimum risk.
2456600	2461400	It turns out to be characterized by taking all possible E values.
2461400	2462520	That's your menu.
2462520	2466640	So if a computational regime might want to do that, or for interpretability reasons,
2466640	2468880	but that's another if and only of theorem.
2470680	2472600	Okay, so I'm going to move on.
2472600	2476040	We're now rolling this out in various domains of actually designing menus and
2476040	2477440	contracts, but we have this guide.
2477440	2479280	We now know how to design the optimal contract.
2479280	2482760	We know what, we use E values, and we know lots of E values.
2482760	2485960	There's a lot of literature on non-negative super martingales or E values and so on.
2485960	2487160	So we'll be doing that.
2487160	2489800	And I'll just say we've done this in particular in the federated learning
2489800	2491160	domain.
2491160	2493280	This is now just, again, the picture of federated learning, but
2493280	2495120	now with an incentive structure.
2495120	2499920	So we're able to design an incentive compatible mechanism that incentivizes
2499920	2502720	agents at the edge to contribute data.
2502720	2506560	And in particular, this handles a problem that has been recognized in
2506560	2508680	literature, which is a free writing problem.
2508680	2512640	If I have some data to send up, but sitting next to me there is a manual, and
2512640	2516240	he has data to send, and I know that his data is pretty much the same as mine.
2516240	2518960	I'm going to watch him send the data, and I know I don't have to.
2518960	2520120	Pre-writing.
2520120	2522240	This paradigm incentivizes against free writing.
2523200	2525160	Professor, just one small clarification.
2525160	2525680	Yeah.
2525680	2529760	Is this based on the assumption that we're talking about home economics in terms
2529760	2533240	of very rational and certain things?
2533240	2534080	It's a good question.
2534080	2537880	I was hoping that was going to come up later about, it's all this rational
2537880	2540640	economic stuff.
2540640	2545000	No, and sort of the behavioral economics here is kind of coming in the fact that
2545000	2545840	we're gathering data.
2548080	2551200	So all these distributions are informed by data.
2551240	2555600	And if we just write down the utility, that's only the assumption we have to
2555600	2557800	make, is that we agree that you want to maximize that.
2558960	2560560	And that's usually not so strange.
2561840	2562960	And then the data informs it.
2562960	2565400	We don't make a distributional assumption about the data.
2565400	2567240	So I can get into that a little bit longer, but
2568520	2570440	behavioral economics is very much part of this agenda.
2571480	2575560	But it's not just that it's broken and we think about the psychology of it.
2575560	2578600	Well, no, we collect data, and data is coming from real people.
2578600	2580640	So we already have a little bit of a help there.
2581920	2584480	So hopefully that partially answers your question.
2584480	2587120	Anyway, if you're interested in this application, we have a paper on that and
2587120	2587920	we're continuing on with that.
2590160	2592880	I got two more vignettes, I think, and I'm just going to go a little more quickly on
2592880	2595240	these. I just want to give you a flavor of these.
2595240	2598240	Classification is the big killer app in machine learning.
2598240	2601400	Classify, yes or no, good or bad, blah, blah, blah.
2601400	2604160	But if you do this in domains where there are strategic agents,
2604160	2605840	you get something called strategic classification.
2605840	2608800	So this is a work with Tiana Zernich, who's still a student with me and
2608800	2611040	will be joining Emmanuel's group as a postdoc.
2611040	2613920	He and I shuttle these superstar people back and forth.
2613920	2615520	And then Eric is now a professor at Caltech.
2617200	2619040	All right, so here's a little picture to suggest this.
2619040	2623240	Health insurance, the health insurance company has got to do a classification
2623240	2627120	problem. I fill out a form, they have to decide whether to give me insurance or not.
2628960	2631840	They're going to ask me, how much do you exercise?
2631840	2633760	I'm going to say a lot.
2633760	2635560	How much do you drink of wine?
2635560	2638560	Very little, so on and so forth.
2638560	2640720	Now if it's implausible, they'll kind of see that.
2641040	2642040	But you make it plausible.
2643520	2646600	They know that, however, so they're not going to make it so easy for you.
2646600	2650560	So they're going to ask questions like, would you be willing to have us look at
2650560	2653240	your cell phone accelerometer for one day?
2653240	2656160	Just opt in, you don't have to, but are you willing to do that?
2656160	2657320	You say, sure.
2657320	2660480	And now if my cell phone moves around a lot during that, it shows I'm very active.
2660480	2662720	If it sits in one place all day, I'm not so active.
2662720	2664400	They would use that as data.
2664400	2666240	So someone went out to build a device,
2666240	2668560	then you put your cell phone on the device and it moves around all day.
2669520	2671280	So this is the kind of problem that arises.
2671280	2673000	An economist are very much aware of this.
2673000	2675800	They call this Goodhart's Law.
2675800	2681320	If you set up a poverty index score at some year, this was in Columbia in 1994.
2681320	2684240	It looks very good, very Gaussian and all that.
2684240	2689440	By 2003, people have discovered that if they move just a little bit left of there,
2689440	2690960	they get more better housing.
2692000	2694440	So everyone cheated a little bit so they could move over.
2694440	2697040	And so the poverty index score has now been ruined.
2697040	2698040	But this is real life.
2698240	2700200	This is what people really will do, and they should.
2700200	2700560	Why not?
2702480	2703600	It's not an ethical issue.
2705760	2708040	Ethics is sometimes used a little bit too easily here.
2708040	2709800	So the real problem is that when you do learning,
2709800	2712680	you rarely have just collected data set and analyze it.
2712680	2715320	In the real world, you have to say, where's the data come from?
2717160	2719800	If it's people supplying the data, are they aware of what the outcome is?
2719800	2721200	Do they have some bets that's interested in it?
2722200	2726480	Probably they do, because if not, why would they really be engaged in this whole exercise?
2726480	2729000	All right, so now we have a Stackelberg game.
2729000	2731200	It's a game theory setting, which is sequential.
2732240	2736120	I send some data up, and the central decision maker, say the bank,
2736120	2739040	is trying to decide about loans, collects a lot of data.
2739040	2741840	They build a model that predicts whether I should get a loan or not.
2743200	2745240	And then that starts to make some decisions.
2745240	2747320	People start to realize what's happening.
2747320	2749560	Maybe the bank has got to reveal by regulatory reasons,
2749560	2751160	I'm using logistic regression or something.
2752080	2754640	People realize that, and they say, okay, the next time they send the data,
2754640	2757240	they're going to alter their data.
2757240	2760560	And that goes back and forth, and you want to ask what equilibria rise here.
2760560	2763160	We're not trying to optimize any likelihood, it's an equilibrium problem.
2764480	2768480	Okay, so we have studied this as a Stackelberg game,
2768480	2771360	which is the appropriate concept in game theory.
2772360	2775880	Classically, in a Stackelberg game, you have a leader and you have a follower.
2776920	2780800	Classically, the decision maker would be thought of as the leader here.
2780800	2784440	They run the whole show, and agents are the follower.
2784440	2787320	You can show in that situation, in this setup,
2787320	2792080	that the leader gets high utility and the followers get low utility, just to say.
2792080	2794960	So it seems reasonable.
2794960	2799080	But if that's an analysis you could do in a synchronous situation,
2799080	2801400	where there's a model built, data's gathered.
2801400	2803760	Model built, data gathered, all synchronized.
2803760	2805360	The real world is no synchronization.
2805360	2809160	Why should people wait till, there's no synchronization between the central model
2809160	2811000	and me sending up data?
2811000	2813840	Okay, so you could start to think about analyzing different scenarios,
2813840	2817360	where there's different kinds of timescales.
2817360	2821680	So here's one where the modeler goes slowly, only updating every once in a while,
2821680	2825200	and the streets you gain doesn't send data much more rapidly.
2825200	2827840	So does this arise in real life?
2829360	2832480	Sure, this is, for example, like college admissions.
2832480	2835880	The college is gathering all these applicants, and they have all this data.
2835880	2839200	They're not gonna adjust their policy after every applicant.
2839200	2840560	They'll do it every couple of years or so, and
2840560	2842600	they'll publish it and all, for obvious social reasons.
2843600	2845040	So that's a real scenario.
2845040	2846360	What about the other way around?
2846360	2850320	Where the central agent updates very, very rapidly, and agents are much more slow?
2850320	2853120	Well, that happens all the time too, that's like YouTube.
2853120	2856320	Every time someone clicks, they update a model in principle, okay?
2857360	2861800	So these are different scenarios, and so what happens here?
2861800	2865480	So you can analyze this as now you do the game theory.
2865480	2868400	So we were able to prove a theorem that shows, first of all,
2868400	2871800	that in either order of play, you get an equilibrium.
2871800	2875000	It's not so hard to see that and analyze that.
2875000	2879480	Much more surprisingly, is that in these statistical settings,
2879480	2883280	where it's a data analysis problem, not just an arbitrary Stackelberg game,
2884560	2890640	it turns out that when the decision maker is a follower, and
2890640	2894440	the strategic agents are the leader, the kind of flipped around version,
2894440	2897360	the strategic agents have higher utility than before.
2897360	2898760	That makes some sense.
2898760	2901960	But also, the decision maker has higher utility.
2901960	2904120	It's a rare example in game theory of a win-win.
2905280	2908760	Going in the order where the strategic agent is a one's going fast,
2908760	2911560	that leads to higher utility for both parties.
2911560	2914640	So that's not a true fact about game theory in general, but
2914640	2916760	it's a true fact about statistical game theory.
2916760	2919240	These statistical modeling exercises for
2919240	2920760	generalized linear models, just to say.
2921840	2923200	I'm gonna skip this little part here,
2923200	2925720	just I like to show pictures of my students.
2925720	2929880	So there's Lydia and Horia, and just say this,
2929880	2931640	I'm gonna show you really quick the slides.
2931640	2935520	But it's a cute little paper where you bring together bandits from machine
2935520	2938520	learning and matching markets from economics.
2938520	2939560	And let me just show you a picture.
2939560	2942120	Here's a learner in a bandit problem.
2942120	2945840	They're trying to find out which of the set of options is the best,
2945840	2947280	gives the highest reward.
2947280	2949800	And they're algorithms like upper confidence bound,
2949800	2954040	that help you guide you towards diminishing your uncertainty and
2954040	2956240	also picking the optimal arm.
2956240	2960120	So we asked the question about, what if you put this in a market setting?
2960120	2964080	So I don't just have one decision maker, I've got a two sided market.
2964080	2968200	And so in particular, I might have two decision makers who are selecting
2968200	2970400	actions from the other side of the market.
2970400	2972680	And there's preferences on both sides.
2972680	2977600	And so you ask questions like, what if both of the agents select the same action?
2977600	2981120	And so we model this as congestion that one of them gets the reward,
2981120	2982360	the other gets no reward at all.
2984040	2985520	And who gets the reward?
2985520	2987960	Well, that depends on the preferences on the right side of the market.
2987960	2990480	So both sides are learning about each other.
2990480	2992720	And so again, you can do the mathematics here, and
2992720	2994200	it turns out to be pretty interesting.
2994200	2999600	What you're really asking is, if there's competition in a bandit situation,
2999600	3002640	does that make the regret higher or better?
3002640	3003840	What does competition do for
3003840	3006680	the learning process of a person trying to learn the best action?
3007680	3010880	Okay, and long story short, we did a, here's a theory.
3010880	3014000	Here's a regret bound, and so this is more for the experts.
3014000	3018400	But the regret is as a function of time, which is n, logarithmic and n.
3018400	3022680	So that's an optimal result from classical bandit theory.
3022680	3023680	So that is still true.
3023680	3026520	Competition does not hurt your rate of learning.
3026520	3028760	There's a denominator term though, which is a constant,
3028760	3032440	which is a gap between the preferences of nearby agents.
3032440	3034680	So if there's competition and you have a small gap between me and
3034680	3036680	somebody else, we start to compete more.
3036680	3039520	And that gives us a higher regret, but it's only a constant.
3039520	3042320	All right, so I put that up there just to sort of show you that it's kind of
3042320	3045640	really fun things to do with simple learning algorithms,
3045640	3050200	explore exploit type, and simple matching market kind of ideas.
3050200	3053680	And this was motivated by this kind of restaurant setting where
3053680	3057200	100,000 of us are out looking for a restaurant in Shanghai.
3057200	3062040	There's 100,000 restaurants, and we're all trying things out as we go and
3062040	3065960	getting rewards or not, and both sides of the market have some preferences.
3065960	3068160	And how does that market clear?
3068160	3068800	That was our question.
3068800	3074280	All right, so last two weeks ago, I talked about this in this very room.
3074280	3075440	So I'm going to kind of go quickly, but
3075440	3079680	this is a very exciting project here that I want to spend five minutes on.
3079680	3083680	Similar collection of students, but also again, Stephen, who's a postdoc.
3085240	3088200	Anastasia, Stephen, Clara, and Tiana.
3088200	3090120	So this is really more about the statistics.
3090120	3092440	There's little economics here, but less.
3092440	3096960	This is more about how do we do things like use neural nets to do science,
3096960	3099360	just roughly speaking, okay?
3099360	3104080	So you all know about things like Alpha-fold, they will make huge numbers
3104080	3108160	of predictions of, say, these tertiary structure proteins.
3110080	3113400	Hundreds of millions of structures, whereas the hand labeled ones,
3113400	3116520	there's only hundreds of thousands of such sequences, all right?
3116520	3120760	So that was a problem that's now being revolutionized in biology.
3120760	3125520	So here's an example of someone in 2004 wrote a very important paper in nature,
3126520	3130480	studying the relationship between intrinsic disorder of proteins,
3130480	3132960	where things don't fold, they kind of are more strand-like.
3132960	3136680	And that turns out to be very important for, like, grid-like things.
3136680	3140400	And phosphorylation, which is an important part of it, and many pathways.
3141600	3145880	So they wanted to ask, is there an association between those two notions?
3145880	3148680	With the parts of the protein in its order, they tend to be more phosphorylated or not.
3150680	3154240	But they really couldn't test it, and now you go forward to 2022.
3154280	3158720	Instead of this small amount of data we had back in 2004,
3158720	3164080	now you have vast amounts of Alpha-fold labeled data.
3164080	3166560	It's not really data, it's predictions, but they're good predictions.
3166560	3169480	So why not throw them in as if they were data, all right?
3169480	3172920	So someone wrote a paper, 2022, doing that.
3172920	3175760	And so they wanted to quantify this odds ratio,
3175760	3178240	probability of intrinsically disordered given phosphorylation.
3180000	3182880	And kind of amazingly, they didn't even use any of the hand label,
3183360	3185400	the gold standard data, because they had so much of this other stuff,
3185400	3188560	they just threw it all in, because it's such a good predictor, why not?
3188560	3190480	But as a statistician, you know better, right?
3190480	3192360	Even if it's very, very accurate at making predictions,
3192360	3195040	that doesn't mean the inferences you make are any good.
3195040	3199280	All right, so I think this one picture I'm about to show, so there's the mayors.
3199280	3202600	This picture is probably the end of my talk, really,
3202600	3204080	and I'll just kind of scroll through a couple more.
3205280	3206360	So let me take a moment on this one.
3207600	3210440	Our statistic here is this odds ratio.
3210440	3212480	We'd like to know if there's an association or not.
3212480	3214640	You've all taken elementary statistics.
3214640	3217560	We have to find whether it's significantly different from one.
3217560	3220800	One would be no association, bigger is an association.
3223320	3229480	We did a Monte Carlo version of this where we, in the set of label data,
3229480	3232520	we actually got the true odds ratio, that's the dotted line there.
3233520	3238960	And then we redid the entire experiment with alpha fold output using the predictions.
3238960	3239960	Okay, so what are we doing here?
3239960	3243520	That gold region right there is a confidence interval.
3243520	3249240	And it's based on taking all of the alpha fold predictions and treating them as real, okay?
3249240	3254280	And from those, you form a confidence interval on this odds ratio.
3254280	3257720	And that's an elementary statistics exercise to do that, all right?
3257720	3259000	That confidence interval is tiny.
3259000	3261280	That looks really good, because you have all this data.
3261280	3263360	It's not real data, but it looks really good.
3263360	3264840	You're very, very confident.
3264840	3268680	You're just dead wrong, all right?
3269680	3273960	The statisticians in the room, Art, will say, why'd you do that?
3273960	3276240	Just we know how to do confidence intervals.
3276240	3278480	Just use the gold standard data.
3278480	3281240	Don't trust these wild machine learning predictions.
3281240	3282560	And Art, I would do that too.
3282560	3283880	That's what my first thought would be.
3283880	3289720	That gives you the gray region, so it covers the truth, as it was asserted to be, all right?
3289720	3291440	But it also covers one.
3291440	3294520	So it doesn't allow you to include, there's an actual association, all right?
3294520	3298640	So the new method gets the best of both worlds, this prediction powered inference.
3298640	3304880	It forms a confidence interval, which is guaranteed to cover the truth, with 95% probability.
3304880	3312040	But it's also much smaller than the, it uses the predictions, but it corrects them, all right?
3312040	3315080	And I think the most fun thing to show you, I'm going to skip that slide,
3315080	3319200	is just the examples that we've been applying this to, and we have a paper that does these.
3319200	3322880	Here's voting, here's a ballot, here's a messed up ballot.
3322880	3324440	So this was a San Francisco election.
3324440	3327640	People use computer vision to look at all the ballots and make a prediction,
3327640	3330280	whether it was yes or no, all right?
3330280	3333240	And now you can feed that in and do an eight analysis on that.
3333240	3337200	And you can see the little gold region there, it's a little small confidence interval,
3337200	3338640	it's just missing the truth.
3338640	3341960	And again, I don't know why some things are not coming out here.
3341960	3347640	Our new interval is the green one, and then there's a missing, a classical one there that's much
3347640	3349960	larger, but again, covers the truth.
3349960	3353400	This is counting spiral galaxies, you know, there's some hand labeled,
3353400	3355360	here's the spiral galaxy, here's not.
3355360	3361040	And again, you can see the small confidence interval,
3361040	3364000	if you use the computer vision algorithm, but it's not covering the truth.
3364000	3365960	And again, we cover the truth.
3365960	3368160	And I think this was the last one I wanted to show.
3368160	3370160	Yeah, here's a California census.
3370160	3373200	The estimate is a logistic regression coefficient of income when predicting
3373200	3375480	whether a person has private health insurance.
3375480	3377760	And there was a machine learning algorithm run on that.
3377760	3381200	You can see the tiny little confidence interval, it's very, very sure and dead wrong.
3382200	3385720	Okay, I hope this conveys, you all kind of knew this, that, you know,
3385720	3389520	very accurate machine learning models can still lead to completely wrong inferences.
3389520	3393600	Okay, I just don't think my machine learning colleagues get that, but it definitely can.
3393600	3396040	And so, but you can get the best of both worlds.
3396040	3399120	And I think I'm going to skip all the way to this last slide here and
3399120	3401520	just show you roughly how this happens.
3401520	3405080	It's kind of like a bias correction procedure, but it's not quite that.
3405080	3410800	All right, so there is a bias between the predicted parameter using all the predictive data
3411000	3412840	and the true parameter, theta star.
3412840	3415520	That bias is a population level quantity.
3415520	3419600	If you had the whole population, you could just write it down as a number, right?
3419600	3422920	There are ways to estimate bias, like the bootstrap.
3422920	3425760	And you could take that estimate and correct your estimate.
3425760	3427360	That's done a lot.
3427360	3432040	We have a different idea, which is that we take that quantity, that bias,
3432040	3434040	or we call it in general rectifier.
3434040	3436840	And we don't just estimate it, we'd put a confidence interval on it.
3436840	3439520	We get all the possible values of that correction.
3439520	3443280	Now we take the original predictive quantity, which is wrong, and
3443280	3446440	we correct it with all the possible corrections.
3446440	3449400	That leads to that green region there, which is a confidence interval on the
3449400	3451320	corrected predictions.
3451320	3454920	All right, and then our theorem at the very bottom of the page shows that we
3454920	3456320	were good statisticians.
3456320	3460280	The probability that this new confidence interval covers the truth is bigger than
3460280	3461880	or equal to 1 minus alpha.
3461880	3466000	And it's much, much smaller than if you've forgotten all the predictions altogether.
3466000	3468640	Okay, so I put that up there just at the end of an economics talk.
3468640	3471800	There's more statistics, but I just think many people in the room are already
3471800	3473480	thinking about this and working on it.
3473480	3474680	It is one of the critical issues.
3474680	3477880	If you're going to do science with machine learning, you've got to face this.
3477880	3480720	You've got to be a good statistician while exploiting the advantages of the
3480720	3482080	machine learning paradigm.
3482080	3484400	And I think this is a step towards doing that.
3484400	3486600	All right, so that's my last slide, and that's the slide I had up earlier.
3486600	3490760	I just wanted to kind of remind you of the big picture of the more provocative
3490760	3492800	issues.
3492800	3494760	This to me has kind of been a no-brainer.
3494760	3496800	I just, what's happening this era?
3496840	3499600	Well, it's just statistics and computer science and econ and all.
3499600	3501160	And we're being good engineers.
3501160	3505480	We're trying to deliver artifacts that will help humans and how to do that well,
3505480	3507880	like previous generations of engineers.
3507880	3512120	And this kind of Silicon Valley hype thing of, we've discovered this great
3512120	3515560	thing called AI, and it suddenly, we've got to worry about all the things that's
3515560	3517000	going to happen because of that.
3517000	3519240	It just, to me, wrong.
3519240	3520240	Thank you.
3520240	3530840	I hope, yeah, I knew.
3530840	3532520	I shouldn't have picked on you, Art.
3532520	3533680	You have a better idea?
3533680	3538680	Well, I have a paper where there's an island of really high quality gold
3538680	3543160	standard data and an ocean of data where you're not quite sure of the quality.
3543160	3547880	And then we sort of do a shrinkage of one onto the other.
3547880	3552440	But we just got point estimates so we didn't get a confidence interval.
3552440	3553240	Yeah.
3553240	3556080	I mean, so this is a little bit like the semi-supervised paradigm.
3556080	3559480	So we have an ocean of labeled data and, sorry, an ocean.
3559480	3563440	We have a small pool of labeled data and an ocean of unlabeled data.
3563440	3565480	The machine learning person says, oh, yeah, that's similar to supervised.
3565480	3566240	No.
3566240	3570520	We're using the unlabeled data to find a confidence interval to correct the, sorry,
3570520	3571600	we're using the, got it wrong.
3571600	3574680	The labeled data to find a confidence interval to correct the unlabeled data and
3574680	3577560	get a confidence interval out of the whole thing.
3577560	3580080	But yes, I think I was aware of that work of yours.
3580080	3583720	And let me just say this is not, none of this is ever new.
3583720	3588120	Statisticians in kind of small data census work did things like this.
3588120	3590200	And semi-parametric statisticians did some too.
3590200	3594600	So this is, again, there's always somebody who did it probably in the 1950s in
3594600	3603560	statistics, or Art, or Brad Efron or what, it's always, any other?
3603560	3604640	Yes, there's two over here.
3605640	3610840	As you pointed out, there's been a lot of attention around uncertainty,
3610840	3615880	quantification, and similar, you know, similar moves in that direction lately in
3615880	3617360	the machine learning space, which is great.
3619360	3624360	What does that mean for the future of applied Bayesian statistics and, you know,
3624360	3626400	MCMC, that sort of thing?
3626400	3631080	Given that it's computationally less tractable than a lot of modern machine
3631080	3634040	learning training techniques, is there still a place for it?
3634440	3636560	Okay, yeah, good, thank you for asking.
3636560	3640720	Yeah, no, I tend to be a Bayesian, as with most statisticians, sometimes I'm a
3640720	3642400	Bayesian, sometimes I'm not.
3642400	3645560	And I'm a Bayesian when I'm working with a scientist over two or three years.
3646600	3650520	Because I'm trying to kind of get out the knowledge that they have and use it.
3650520	3652160	All right, and that's a prior.
3652160	3655600	And so why would I not do that if I'm going to work with them a long time?
3655600	3658800	I'm a frequentist when I'm trying to produce a piece of software that
3658800	3660800	people all over the world will use.
3660800	3662520	Because I'm not going to work with them and get the prior.
3662520	3665240	I'm just going to put it out there and I want to put a stamp certificate that 99%
3665240	3667800	of the time they're going to work for whoever uses it.
3667800	3669560	That's the two perspectives I have.
3669560	3671560	Now, a little more nuance to that.
3672840	3675240	Bayesian way to structure models is very nice.
3675240	3679840	You get hierarchies, you get sharing of shrinkage, social network kind of things
3679840	3681880	or naturally Bayesian.
3681880	3686720	I think that Brad Efron, who has been the luminary in statistics here at
3686720	3689160	Stanford but worldwide, had it right.
3689160	3692120	Which is that you often will go into a problem, you think Bayesian.
3692120	3694480	You start to structure the model, think about what I could know,
3694480	3698200	what would be together with what, and then you become frequentist.
3698200	3700360	You say, I'm going to do empirical Bayes.
3700360	3703360	I'm not going to just run the Bayesian MCMC paradigm.
3703360	3705400	I'm going to at some point just say, okay, there's some things I can estimate.
3705400	3707440	I can plug them in at the Bayesian procedure and
3707440	3709160	then I'll get the benefit of both worlds.
3709160	3711200	Now, I totally agree.
3711200	3716360	So a lot of the things you saw here have a kind of an empirical Bayes interpretation.
3716360	3720040	And a lot of the, but it's true, the conformal things in the uncertainty
3720040	3723160	you're talking about don't necessarily, maybe a manual could correct me there.
3723160	3726960	But those are kind of pure hardcore frequentist at some level.
3726960	3730680	But I tend to, when I would use those in practice, I would probably have not just
3730680	3733040	one conformal predictor over here, I'd have another one over here, another one over
3733040	3736120	here, I would want to shrink them towards each other, I want to have them related.
3736120	3737840	Because in real domains, if you really start to scale,
3737840	3741040	the Bayesian way of thinking helps you immensely.
3741040	3744760	So that's funny, Bayesian frequentists do conflict, but
3744760	3746080	it's like wave particle duality.
3746080	3748480	It's kind of my metaphor, right?
3748480	3751520	Waves and particles are both correct, and they conflict a little bit.
3751520	3753440	But if you throw out one and just use the other one,
3753440	3754800	you're gonna do bad physics.
3754800	3758440	And same thing with statistics, yeah.
3758440	3763360	Professor, you were emphasizing the importance of forming collectives in solving
3763360	3764760	the problems.
3764760	3768320	And I'm very curious to know how you think about how those
3768320	3773160	collectives can actually be formed to pursue a goal.
3773160	3776480	Especially because, I mean, I don't know if a principle or
3776480	3780600	a platform is required to kind of create the market where agents can actually trust
3780600	3782000	that and cooperate.
3782000	3786960	Or do you actually feel like a decentralized kind of a network is possible?
3786960	3789840	That's a fantastic question, I'm delighted to have it.
3789840	3793800	And just for the young people in the room, I hope you see the questions like that
3793800	3796880	are kind of the thing of the era, and they're hard.
3796880	3799000	I don't have an answer to your question, it's gonna be the short answer.
3800440	3803480	The colleagues I had on that paper, the social science colleagues,
3803480	3807000	they talk a lot about new models for democracy.
3808200	3812240	And they emphasize that democracies tend to arise when you have multiple layers of
3812240	3816320	like bring 200 people together, get some consensus, take 200 people here,
3816320	3819280	get some consensus, and put the consensus together.
3819280	3821440	And form cities and countries and all that.
3821440	3823600	That's what humans have done throughout history.
3823600	3828040	And we've had this experiment now that we have this thing called Twitter.
3828040	3831680	And we're assuming that it's all good that all of us talk all the time.
3831680	3833880	Or that we all listen to one person.
3833880	3836600	And those are terrible ways to do democracy.
3836600	3838560	So there are experiments that have been underway for
3838560	3840240	quite some time for people like that.
3840240	3843360	Like those famous examples in Taiwan where they have a legislator,
3843360	3847840	which is using lots of data analysis together with kind of structured assemblies
3847840	3851800	of ways to kind of come to coherent decisions and to get consensus.
3851800	3854400	And Ireland has used this, and their latest this,
3854400	3858040	they kind of legalized abortion at some point, that's very hard to do in Ireland.
3858040	3861200	They did it partly because of these new assemblies, new structures.
3861200	3865000	So I love this, people thinking about out of the box of new mechanisms that
3865000	3869760	bring together visibility of, but it's still among relatively small numbers of
3869760	3871920	people, that's really critical.
3871920	3875000	And I think that the technologists who just built the YouTubes and
3875000	3878840	the Facebooks and all that, were trying to do this experiment on human beings.
3878840	3881120	So it was just destined to fail.
3881120	3883120	The big broadcast channels were terrible.
3883120	3886680	We want communities and we got to think about a structure of those.
3886680	3889800	So I don't have much more to say about that other than how do you form collectives
3889800	3892680	and support them and make them healthy and all that is hugely interesting and
3892680	3895720	important, and there are social scientists who spend their life doing this.
3895720	3898560	This is definitely not just a technology issue.
3898560	3900480	We need to both cooperate and listen and
3900480	3902640	have a dialogue with those kind of social scientists.
3904160	3905760	There's many others we should cooperate with, but
3905760	3907720	I think that's a particularly pregnant one.
3907720	3910160	Economics certainly talks about collaborative things.
3910160	3913560	There's cooperative game theory and how do coalitions form.
3913560	3917120	But it's a little bit dry, and maybe Hido can help me a little bit with kind of,
3917120	3918400	maybe there's more to it.
3918400	3921040	But it's a little bit about how do I do negotiation and
3921040	3923440	get the most money out of the deal I can and so on.
3923440	3925000	You need to line the incentives.
3925000	3927080	Yeah, or line with incentives and so on.
3927080	3929320	But that just means that we just haven't thought about it enough.
3929320	3932520	And for the young people in the room, wow, that's a great topic to think about.
3932520	3936400	How do I start to structure collaborative efforts in data-oriented ways?
3936400	3938040	So I think economists didn't have enough kind of,
3938040	3939960	they talk about communication signaling, but
3939960	3942760	they didn't really have enough data to kind of really signal interesting things,
3942760	3945120	and do it in adaptive interesting ways.
3945120	3947760	So let me just lean in again to this, there's a lot of young people in the room.
3947760	3950200	This is the most exciting era to be in.
3950200	3952240	The previous eras kind of gave us greaty to send and
3952240	3954840	gave us networks and all that and all these tools.
3954840	3957160	And now they threw them out there in the world and they kind of work and
3957160	3958120	they kind of don't.
3958120	3960640	We got better commerce, we got better transmission.
3960640	3961960	And we can sort of fix all those.
3961960	3965680	We can also think a lot more about, wow, new things, good things could happen if
3965680	3967480	we start to think in the right way.
3967480	3969520	And what problems are needed to do that?
3969520	3973120	Don't just work on self-driving cars and whatever, or
3973120	3977720	make Facebook advertisements better, work on problems that you believe
3977720	3981640	in, and there are plenty of them, but bring these two fields together though.
3981640	3984160	Don't just think of yourself as a system builder.
3984920	3990960	What up guy?
3990960	3992800	I don't know,
3992800	3993080	I don't know,
3993080	3997560	I think my qury just made a living.
3997560	3998600	Okay, thank you, bye.
3998600	4001620	I will just let you guys go and get off the car.
4001620	4002220	Bye.

Alright. As folks join the Zoom, I'm going to get started. Welcome everyone to the CHIP
Landmark Ideas series. Today we'll be hearing about rewriting biology with artificial intelligence
for Ray Kurzweil, an inventor and futurist. I'm Ken Mandel. I direct the Computational
Health Informatics program at Boston Children's Hospital. The program was founded in 1994
for a multidisciplinary applied research and education program. To learn more you can
visit www.chip.org. The Landmark Ideas series is an event series featuring thought leaders
across healthcare, informatics, IT, big science, innovation, and more. Dr. Kurzweil is one
of the world's leading inventors, thinkers, and futurists. He creates and predicts using
tools and ideas from the field of pattern recognition. He invented many technologies
familiar to us today, including flatbed scanning, optical character recognition, and text-to-speech
synthesis. He won a Grammy for creating a music synthesizer used by Stevie Wonder that
was capable of recreating the grand piano and other orchestral instruments. He was awarded
the National Medal of Technology. His best-selling books include the New York Times bestsellers
The Singularity is Near and How to Create a Mind. Larry Page brought Kurzweil into
Google as a principal researcher and AI visionary. I'll just mention one connection to Chip.
Ben Rice, a faculty member, when he was a student at MIT, worked with Ray to develop
a text-to-speech interface for that synthesizer so that Stevie Wonder and other non-sided musicians
could interact with the extensive visual navigation interface. The Singularity is a very important
idea of Dr. Kurzweil. This is the point in time when artificial intelligence will surpass
human intelligence, resulting in rapid technological growth that will fundamentally change civilization.
And in order to understand when machines surpass biology, Ray has delved deeply into an understanding
of biology, and we're immensely looking forward to hearing and learning and joining him in
that understanding today. You've got us all looking forward, and the chat and the Q&A have
lit up. Let me start with one question. You're joining us for the seminar five days after
the release of OpenAI's chat GPT, which astounded many across the world in its ability to synthesize
natural language responses to really complicated questions and assignments. And if you've gotten
to glimpse this technology, could you place it on the Kurzweil map toward the Singularity?
Is this a step forward? Is it a distraction? Is it related in any way?
Large language models occurred three years ago, and they seemed quite compelling. They weren't totally
fully there. You could chat with it, and sometimes it would kind of break down. The amount of new
ideas that are going into large language models has been astounding. And so it's like every other
week there's a new large language model and some new variation that's more and more realistic.
So that's going to continue to happen. And so this is just another issue. I mean, there are some
things I think that aren't quite right with that particular model you mentioned. But people have
actually interacted with these things and some people say they're sentient. I don't actually think
they're sentient yet, but I think they're actually moving in that direction. And that's actually
not a scientific issue. It's actually a philosophical issue as to what you consider sentient or not.
Although it's a very important issue, because I would chat with Marvin Minsky, who is my mentor for
50 years, and he said that sentience is not scientific, so therefore forget it, it's an illusion.
That's not my opinion. If you have a world that had no sentience in it, it may well not exist.
But yes, there was a sizable advance, but there's more to come.
Let me ask a question from Charlotte, please. A philosopher, an AI-informed philosopher.
What do you make of the criticism that there's more to intelligence than brute processing speed and
pattern recognition? That if we want to pass the Turing test, we need to learn more about our own
intelligence evolved. And I'll just paraphrase you in The Singularity is Near, comparing cognition to
chaotic computing models where unpredictable interaction of millions of processes, many of which
contain random and unpredictable elements, provide unexpected and appropriate answers to subtle
questions of recognition. And so in this chaotic computing, how can you address Charlotte's question
about our own intelligence and the path forward AI? It is a good observation,
but chaos and unpredictability can also be simulated in computers. Large language models do that,
because you can't always predict how it's going to answer. And a lot of these models,
you can actually ask the same question multiple times and get different answers. So it depends
on kind of the mood of the large language model at that time. And to make it more realistic,
it does have to take that level into account when it answers. At first, we could ask a question and
give you a paragraph that could answer your question. Now, it can actually give you several
pages. It can't, though, give you a whole novel that can be coherent and answer your question. So
it's not able to do what humans can do. Not many humans can do it, but some humans can write a
whole novel that would answer a question. So that's the answer. It has to actually
cover a large amount of material, have an unpredictable element, but also all be coherent
as one work. And we're seeing that happen gradually. Each new large language model is able
to actually cover a much broader array of material. But it definitely can handle stuff that is not
it's not just giving you a predictable amount of
it's it has a way that is not really totally predictable.
So along those lines, let me pose Jane Bernstein's question, which is what is your definition
of intelligence?
I mean, intelligence is to solve difficult problems
with limitations of resources, including time. So you can't take, you know, a million years to solve
a problem. If you can solve it quickly, then you're showing intelligence. And that's why
somebody who's more intelligent might be able to solve problems more quickly.
But we're seeing that in area after area. I mean, Alpha Fold, for example, can actually do things
that humans can't do very quickly or to play something like Go. It goes way beyond what humans
can do. In fact, Lisa Dahl, who's the best human player in Go in the world, says he's not going
to play Go anymore because machines can play it so much better than he can. But that's actually
not my view that it's going to replace us. I think we're going to actually make ourselves
smarter by merging with it, as I said. So I'll ask a question from Sharon Weinstock,
with AI taking over physical and intellectual achievements and individuals living longer.
Do you have thoughts on society and whether individuals risk lacking a purpose?
Well, it's good to hear from you, Sharon.
That's the whole point of our merging with intelligence. I mean, if AI with something
separate from us, it's definitely going to do everything that go way beyond what humans can do.
So we really have to merge with them to make ourselves smarter. But that's why we create these
things. I mean, we're separate from other animals and that we can think of a solution
implemented and then make ourselves better. So if you take what human beings were doing
for work 200 years ago, 80% had to do with creating food. That's now down to 2%.
And so if I were to say, oh, well, you know, all these jobs are going to go away
and machines are going to do them, people say, oh, well, there's nothing for us to do.
But actually, the percentage of people that are employed has gone way up.
The amount of money that we're making per hour has gone way up.
And they say, well, okay, but what are we going to be doing? I said, well, you're going to be doing
IT engineering and protein folding. And no one would have any idea what we're talking about,
because those ideas didn't exist. So we're going to make ourselves smarter.
That's why we create these capabilities. And so it's not going to be us versus AI.
AI is going to go inside of us and make us much smarter than we were before.
So yes, I think if we did not do that, then it would be very difficult to know what you and
beings would be doing, because machines would be doing everything better.
But we're going to be doing it because the AI is going to work through us.
Ronald Wilkinson has a question that relates to your idea of whether it's a dystopian
society or other. But really more specific, he says that he would expect people with various
political and or personal agendas to harness the increasing power of AI for their own purposes.
It will not necessarily be to the long-term benefit of humankind as a whole. So how does this balance
out? Could you go through that again? I don't quite understand. Individuals
with political and personal agendas may use AI for purposes that are not
beneficial to mankind. How does that balance out?
Well, I mean, every new technology has positive and negative aspects. The railroad did tremendous
destruction, but it also benefited society. So it's not that technology is always positive,
social networks. I mean, there's certainly a lot of commentary as to how it is negative,
and that's true. But no one actually would want to do completely without social networks.
And I make the case that we're actually using technology and measuring the kinds of things
that we associate with positive social benefit is actually increasing as the technology gets
better. And that's actually not known. I mean, if you ask a poll as to whether these things are
getting better or worse, people will say they're getting worse, whereas they're actually getting
better. But it's not that everything is positive. I mean, there are negative aspects of it,
and that's why we need to keep working on how we use these technologies.
Here's a question from Mark. The singularity is near. In that book, you speculated that the risk
of bioterrorism, engineering of viruses will become an existential threat.
Since then, do you think this risk to humanity has increased or decreased?
I don't think it's increased. I mean, I have a chapter in singularity is near,
and there's also another one in singularity is nearer on risks. And all of these technologies
have risks, and they could also do us in.
And I don't think the likelihood of that has increased, but I remain optimistic.
And if you look at the actual history of how we use technology,
you could point to various things that should have gone wrong. Like every single job that
we had in 1900, a year ago, a century ago, is gone, and yet we're still working and making
actually more money. So the way we've used technology has been very beneficial to you
in being so far. From Greganus Ova, one of our faculty, Professor at Harvard Medical School,
AI comes with large energy resource demands and rare mineral material needs to build the hardware.
How do you see these international global tensions, especially the interaction
pervasive AI and the climate? I mean, computers don't use that much energy.
In fact, that's the least of our energy needs. And that's a whole other issue we didn't get into,
but the creation of renewable energy sources is on an exponential, a very good chart that shows
all of the renewable energies, and it's on an exponential. And if you follow that out,
we'll be able to provide all of our energy needs on a renewable basis in 10 years.
And at that point, we'll be using one part out of 5,000 parts of the sunlight that hits the earth.
So we have plenty of headroom in that. So we'll actually be able to deal with climate change
through renewable sources. But in terms of what we're using, computers are not that expensive.
From Tim Miller, will the singularity lead to a decrease in class conflict? Much of the gain
in productivity and wealth in the last 50 years has been concentrated in the 1 percent
as inflation adjusted earnings in the working class have stagnated. Are you concerned about
gains in productivity due to AI being unevenly distributed? And Don Goldman similarly comes in
with this related question about inequities that, for example, we saw exacerbated during the COVID
pandemic. I mean, my observation is that more and more people from more and more backgrounds are
participating, which didn't used to third world countries like in Africa, South America, and so on
did not participate to the same extent, whereas they are participating far more dramatically today.
Countries that were really under the weather in terms of being able to participate in these types
of advances are now participating to very smart, very large extent. So I mean, that's my view.
Question from Bill Akava, one of our faculty. A machine can easily beat the best human player
at computer chess, but even a young child can move pieces on the physical board better than
any general purpose robot can. Do you imagine embodied machines will ever pass a physical
Turing test in the real physical world? And if so, when?
Yeah, we're making less progress with robotic machines, but that's also coming along.
And it can also use the same type of machine learning. And we're going to see, I think,
tremendous amount of advances in robotics over the next 10 years.
And for a science fiction a question from Ju Chang, how do you envision society once
individual brains can interface with a cloud? Will individuality still exist? It seems you
imagine human intelligence coalescing into a singular consciousness.
Yes, definitely. I mean, that's one of the requirements of being able to connect to the
cloud is that this is your portion of the cloud and other people can't access it.
And we're actually doing very well on that. And all of our phones connect to the cloud,
and we don't see people complain that other people are getting access to it.
So we're actually doing pretty well on that. But definitely you'll be able to maintain your own
level of personality and differences.
And I think we'll actually be more different than we are today,
given the kinds of skills that we'll be able to develop.
Great. Well, Ray, this has been a spectacular hour we've gotten to spend with you. And I can tell
you that in the lead up to it, I was contacted by many of the folks who were on this webinar
with us today, very excited to meet a celebrity. They never thought they'd have the opportunity to
interact with this closely. So I thank you very kindly. And I also thank you for doing this later
in the evening as you're out at Oxford giving, entertaining the students there as well.
Yeah, it's been great to interact with you and all of your colleagues. It's been,
I've enjoyed it a great deal. Great. Thank you. Let me therefore thank you again,
and I'm going to return to the slides for just a moment to remind people of our upcoming talks
in the series, including I'll highlight Rich Minor next month inventor of the
Android operating system. Also, a Googler who actually resides a lot of the time here in New
England. And the blind folks to be sure to reach out to us at CHIC. If you're interested in
training, directing, researching, teaching, being in our seminar series. Thank you very much.
And we will see you next month.

Today, we're going to be speaking with Eliezer Yudkowski, who's the founder and senior research
fellow of the Machine Intelligence Research Institute, which is an organization dedicated
to ensuring that smarter than human AI has a has a positive impact on the world.
Eliezer, really appreciate your time and insights today.
Thank you very much.
So I mean, to start with many of the discussions around AI, particularly now with chat GPT-4
and the questions of what will this look like in six months, in 12 months, in 24 months?
What will the impact be on jobs?
What about if it turns evil or it turns against humanity or whatever?
Are the conversations about AI even being framed in the right parameters as far as you
see?
Well, we try to frame it correctly as we see it ourselves when we have the chance.
I think there's widespread agreement that chat GPT-4 and GPT-4 are relatively unlikely
to end the world, either exterminate humanity or have a very large impact outside of that.
So anyone who opens it with the frame of like, look at just GPT-4, look at only chat GPT
is probably there to convince you it's not going to have a large impact.
And is it wrong to limit the conversation only to GPT-GPT-4?
Obviously, the future exists.
You know, tomorrow is going to be a different day from today in five years.
It's not going to be this year.
What is the physical mechanism?
And a bunch of people wrote some version of this question to me when we said we'd be having
you on.
And there's this idea, the scary idea of an AI going beyond a computer screen and starting
to function or manipulate the quote real world.
A lot of people were saying to me, can you ask what is the mechanism through which that
would happen?
Would it be by controlling systems like traffic lights and airplanes or like what?
Because at the end of the day, we're talking about software.
If something like that were to happen, what's the physical process?
I mean, this is a deep question.
This is going to take a while to answer well.
Okay.
All right.
So, first of all, even GPT-4 is already at the level where if you ask it to hire a task
rabbit and think out loud about how to hire a task rabbit in order to bypass captures
that are meant to restrict systems for human use only, it's already advanced enough that
when the person that was trying to hire as a task rabbit said, like, so are you a robot
that you can't read the captchas, lol, GPT-4, thought out loud, like, I should conceal that
for the fact that I'm a robot, I should make up a reason that I can't solve the captcha.
Right.
And then on the main channel told the task rabbit, no, I have a visual impairment.
Right.
So, it's already like, you might say, it's already like able to use task rabbits as fingers
in the real world, and it's already understands humans out in the real world well enough to
know that it shouldn't just tell people to row about it if it wants to get its job done.
So that's today's systems.
Right.
That's relatively straightforward.
I can tell you about it because it already happened.
Predicting the future is harder.
Predicting what something smarter than you could do is fundamentally harder.
If I was playing chess against Gary Kasparov, you know, like the past world champion who
lost to Deep Blue, if I could predict exactly where Gary Kasparov would move against me
on the chess board, I could play chess at least as well as Kasparov by just moving wherever
I predicted Kasparov would move.
So something really actually smarter than you, you cannot predict unless it's a very
narrow game, like tic-tac-toe, you can maybe predict where somebody will move against you
in tic-tac-toe even if they're smarter.
Right.
The real world is much more complicated than tic-tac-toe.
So the basic question as to how an AI gets out of the computer, the basic answer is if
I knew exactly how it would do that, I would have to be as smart as the AI.
With that fundamental obstacle in mind, the more you sort of look into this stuff and study
where the current technological roadblocks are, the better the guess you can take at
how a smarter opponent might move against you or like setting lower bounds.
What can somebody do if they're able to solve technological problems that we understand well
enough to know a smarter mind could solve them?
So if you're just coming at this and you don't know anything about exotic technologies
that haven't been developed yet, you might be like, yeah, it will use task rabbits.
It'll pay humans that don't know it's an AI.
It'll blackmail humans, still no reason for it to tell it that it's an AI.
It'll say it's an AI to online wacky cultists who believe that an AI should destroy everything.
Those are its human hands.
You'll ask yourself, what could I do with some human hands if I were an AI?
You might imagine that it gets a hold of some GPUs and tries to build a backup for itself
someplace that humans don't know about.
You might imagine that it writes the next version of itself and makes it even smarter.
You're trying to already imagine what if it's smart.
So saying that doesn't help a lot in some ways.
But maybe you can make itself smaller, more efficient, maybe it can back itself up in
multiple places in computers where you would not expect that there was an AI on board.
Maybe it can find unknown security holes in a cell phone, in many varieties of cell phones,
start listening to human conversations, get more blackmail material.
Maybe it can pretend to be human and make online friends with a bunch of voters, persuade
them to vote for candidates that seem outwardly nice but which it in fact has under fairly
detailed blackmailed control.
And this is sort of like without any of the difficult to understand stuff.
Right.
That's based on mechanisms and social cultural realities and technologies that exist and
that we understand, which there's an entire other category of things that we simply can't
conceive of yet.
Right.
So suppose you ask the 11th century, like a portal opens to the 21st century in the
middle of say modern Russia.
What are your concerns about fighting somebody from a thousand years in the future?
They might be like, well, what if it's a wealthier country?
What if they have more knights?
What if more of their knights are armored?
Right.
And they're just not going to get from their nuclear weapons.
If they imagine that Russia of the future has irresistibly powerful sorcery, they'll
get close to the truth.
It's not actually irresistible and it's not actually sorcery.
Right.
But you have to reach pretty far to expect that among the resources they have are nuclear
weapons.
And Russia's not actually going to bother with the nuclear weapons.
They don't need it.
Right.
They're going to go through a 11th century battlefield.
And you know, maybe if they see it coming, they can dig a pit trap, but otherwise all
the arrows and lances are going to bounce off it all day long.
It just rolls over the horses.
Right.
Right.
It's talking about orders of magnitude difference.
So, you know, you if we imagine some sort of spectrum of opinion on this issue, you
do have some folks who say this is as you alluded to most dangerous thing we can imagine
or can't imagine must be stopped immediately in its tracks with either legislation or regulation
or whatever the case may be.
On the other side, I read some interesting op-eds that basically say this is one of the
best things we can imagine.
This is going to 10 X the productivity of every normal worker.
It's going to do so many of the things that people said in the thirties were going to
happen where everybody would only be working 12 hours a week and have everything they need.
Right.
That didn't happen.
But there's the very pro side.
And then there's sort of like what I think maybe Neil Postman's view would have been
if he were writing about a I today, which was his view on prior new technologies, which
was spending too much time simply on trying to block them is not the most useful thing.
We're better off harnessing the positive and regulating the potential risks.
And that would maybe be an in the middle sort of view.
What do you think makes the most sense based on what we know right now as an approach?
Well, I think that talking about it like it's a normal technology and not something smarter
than you is basically misguided and having the wrong conversation.
So it's not even in the right category of the stuff Postman talked about.
It's not electricity.
Right.
It's not nuclear power.
It's not nuclear weapons.
It's not even a super virus.
It's something that has its own plans.
You don't get to just plan how to use it.
It is planning how to use itself.
So a totally different paradigm needs to be applied here.
I mean, imagine if somebody was like, well, we're about to contact these aliens with much
more advanced technology that think faster than us, that are smarter than us, that have
been around the stars for a while.
They're landing.
We're not quite sure exactly when.
We're not sure how fast their, you know, spaceships are, you know, it might be two years, it might
be 30 years.
And somebody is like, well, you know, this is just like electricity, right?
Let's use the cool stuff about their technology, but make sure they don't do anything bad.
Right.
I'm going to say it in that way.
Precisely.
Yeah.
So then what do you think needs to be done at this point?
I mean, one of the cruxes of the issue is can you do nice things with it?
And the like grim dark message that I am bearing is that we don't know how to do that and we're
not likely to figure it out in time.
There's a very large gap between where we are and understanding what goes on inside the
frontier AIs and being able to shape them in detail in a way where they go on wanting
to be nice or even in a certain sense wanting to be, you know, non-agentic, to just do particular
tasks and do that without lots of side effects in like the most normal possible way that
they can accomplish those tasks.
Like even building that sort of limited thing, never mind something that is really friends
with you, is I think beyond the range of what we're going to figure out in the foreseeable
future and I do think we are storming directly ahead on capabilities.
That looks like a giant disaster in the making.
So yeah, I side with, well, I think my basic factual point of view is if we do not somehow
avoid doing this, we're all going to die and my corresponding policy view is that we should
not do this even if that's very, really quite hard and requires us to do some unusual things.
So in terms of what to do, I mean, I guess if everybody came around to your view, you
would think anybody developing these technologies would just give it up, right?
On their own, they would say, wow, Eliezer is right.
I share that concern.
The right thing to do is for me to stop working on this.
Assuming that that doesn't happen, what is it that you would like to see?
Would this type of research and development be made against the law?
Yeah, basically, I think that we should track all the GPUs, have international arrangements
for all of the AI training tech to end up in only monitored, supervised, licensed data
centers in allied countries and if you're any time you have any and just like not permit
training runs more powerful than GPT-4, if we're not going to do that, then we should
monitor all the training runs larger than GPT-4 or rather like GPT-4 sized and keep
the model weights inside only the licensed regulated data centers and track who is running
them, store the outputs someplace so you can look at the outputs with the warrant.
A bunch of the tech like that whole line of reasoning is not that regulating the stuff
will protect you from a super intelligence because it will not.
That's more in the hopes that people change their minds later, maybe after some major
disaster that doesn't kill everyone and in that case, the technology that you would need
to sue somebody who killed a dozen people in a hospital or cost $100 million worth of
damage or whatever, the technology you need to know who did that and sue them and not
just have all the AI development go to places where they couldn't be sued is the same technology
that humanity would need to have a off switch where you don't like to press the off switch
to deal with the super intelligence.
The super intelligence does not let you know that you need to press the off switch until
you are already dead, but if we're lucky enough to get warning signs, then the civilizational
infrastructure to have a pause button is the same as the civilizational infrastructure to
sue people who do small amounts of damage, like small as in survivable, as in there
were survivors.
As someone who very clearly you've expressed your very serious concerns with this, do you
also see this technology as something that could and this doesn't mean that it would
or that it would be a good trade for the things that you're talking about, but do you also
see this as a technology that could, for example, do analysis of the human genome such that
it would accelerate us in terms of our ability to cure or prevent disease, the likes of which
could take who knows how long without such technology or when it comes to energy or whatever.
Do you also see that side and or does it not matter because the downside is so huge?
I mean, if we knew how to build an AI that did exactly what we wanted, we could thereby
spread out across the galaxies, turning them into our cities full of sapiens sentient beings.
Enjoying themselves and caring about each other and generally living happily ever after
until the last of the negentropy runs out.
That's always been the dream.
We don't know how to do that.
It's physically possible, but the art and technique to do it is beyond us in the present
time and it's going to not be gained in the next five years and I'm not sure how you would
task a government bureaucracy to recognize it even if somebody came up with it in 40 years.
It's like there's a sort of basic question about whether our civilization is smart enough
to do something correctly on the first try at all.
The way science usually works is that you have a bunch of luniite optimists with wacky
theories who storm ahead on their basic research problem and they are wrong and they go back
to the drawing board and they're wrong again and the next generation is a bit more cynical
about how hard the problem is and eventually people work it out.
If we were allowed to do that with super intelligence, I would have much, much, much less fear of
the outcome, still some fear because you're playing with pretty high stakes there.
You might have somebody who like, the people who end up figuring out how to line it might
misuse it.
There would still be that major threat, but it wouldn't be like the automatic extinction
scenario.
If we have the textbook from 100 years in the future that contains all the simple ideas
that actually work, that takes so long to identify and practice, the relus instead of
sigmoids for those of us who've been following AI for more than the last couple of years
and know what I'm talking about there.
There's all kinds of places in AI where people tried to do things using complicated techniques
and they didn't work.
And 20 years later, they kind of put the simple technique that actually works.
And if you have the textbook from the future with all of the simple things that actually
work in practice for alignment, it is probably not hard and you can get all the goodies,
but we don't have that textbook.
And the first problem is that getting this correct on the first, the first problem is
the amount of time it takes to get that where capabilities are storming ahead because you
can tell whether things are working or not on capabilities, whereas with alignment you've
got to know that at the point where it's much smarter than you.
It's already aligned.
You don't get retries past that point.
So it's more like launching a space probe that has to land on Mars correctly the first
time than it is with like building a car, watching it break down and tinkering with
the car.
And if it doesn't, it might break everything so you don't get to try again.
Yeah.
Like the entire human species is packed on board the Mars probe and the first rocket
that goes high enough has to land on Mars.
There aren't even really good analogies for it.
Humanity has not faced an issue like this before, which is why we're still around.
And I think looking at this, that this is just like clearly beyond the reach of our
present civilization.
It's not close.
It's outside the range of things that you could reasonably, that you could tell a reasonable
story about people doing successfully.
And that's why we need to back off.
Or rather, I predict that if we don't back off, we die and I wish we would back off.
I don't predict that we will.
Well, certainly there's no way to wrap up such a dark vision in any way that is going
to be satisfactory or calming to the audience, I think.
But that being said, Eliezer Yudkowski is taking this very seriously as the founder
and senior research fellow of the Machine Intelligence Research Institute.
And I really do appreciate your time and insights, even if they are not optimistic on this issue.
Thank you.
Thanks for having me on.

Learning a new programming language is hard. There's so much syntax to learn and even
then you have to understand how the language computes to go from this syntax to a meaningful
program.
But what if I told you that there is a language that can do absolutely anything any other
programming language can and will ever do until the end of time, that consists of only
three pieces of syntax and a singular rule of computation.
Introducing the Lambda Calculus
The Lambda Calculus is, at its core, a theory of functions.
Why functions? I don't mean complicated blocks of code.
Rather, a mapping of inputs to outputs.
For example, we could have a function that maps a number to its square, so the function
maps the input 2 to the output 4 and so on.
Writing the mapping from every possible input to its output is tedious at best, so let's
abstract away the specific inputs and just say that for some arbitrary input x, we map
it to x squared.
We call x a variable.
We could have used any symbol in place of x.
Mapping y to y squared or star to star squared doesn't change which inputs are mapped to
which outputs.
As some fancy terminology, we say that these functions are alpha equivalent and the process
of renaming the input variable and all instances of that variable in the function to a different
variable is called alpha conversion.
To avoid having to use arrows to denote functions, let's introduce some new notation.
For a function that maps an arbitrary input x to the output m, we'll write it as follows.
We'll first write a Greek letter lambda to indicate that we're writing a function.
There's really nothing special about this letter, it's simply a visual marker that
we're defining a function.
Then we'll write the input variable.
In this case x next to the lambda.
We'll then write a dot and finally m, the output of our function.
The dot is simply there to separate the input from the output.
We often call functions written like this lambda abstractions.
Okay, so now we have a concise notation to write functions, but on their own they don't
exactly do very much.
That's where function application comes in.
Let's suppose I have the function that maps the input x to the output x plus one.
If Alice wanted to find what a specific number, say three, is mapped to, we can substitute
in three for our input variable.
So we would replace any x's we see in the output with a three, giving us three plus
one or four.
We call this process of substituting a term for the input variable of a function, beta
reduction, and we write this as an arrow with a little beta next to it to show that a reduction
has taken place.
In general, if we have a lambda abstraction, lambda x dot m, apply to an input n, denoted
by writing the input on the right of our function.
We can perform a beta reduction by going through m and substituting our input n for the variable
x.
We'll shorten this by writing a slash to mean substituted for.
So we've now understood variables, lambda abstractions, and applications of functions
via beta reduction.
This looks very much like a start to an extremely primitive programming language.
To finish it off, we would just need to add support for multiple inputs, loops, booleans,
some data structures, a few more rules of computation.
As it turns out, we actually don't need anything more.
If we only allow the programmer to write variables, lambda abstractions, and applications, and
simply compute these terms using beta reduction, we can make anything that a computer will
ever do, ever.
This amazing result was documented in the Church-Turing thesis.
Two names we'll come across later on.
We call this tiny programming language the lambda calculus.
This result seems impossible, but the key to seeing how this is achieved is by noticing
that our functions here are higher order.
This means that functions can be passed as inputs to other functions, and equally functions
can return functions as an output.
This small subtlety is what gives the lambda calculus its punch, which I'll demonstrate
by replicating a standard feature of more complicated programming languages in the lambda
calculus.
You might have noticed that lambda abstractions only have a singular input variable, which
seems rather limiting.
What if Bob wants to make a function that, say, takes two numbers as inputs, and then
outputs their sum?
He can actually achieve this with a little trick.
Consider the function that takes an input x and returns another function.
This outputted function takes an input y and outputs x plus y.
Okay, that's a little confusing, but let's see what happens when we apply this function
to an input, say 1.
Replacing all x's with 1's gives the function lambda y dot 1 plus y.
That is, a function that takes an input and returns 1 plus that input.
So if we apply this function to the number 2, we replace the occurrence of y in our lambda
abstraction with 2, and we get 1 plus 2, or 3.
We've effectively given this function one input after another, and it has returned their
sum.
To generalise this, the arbitrary lambda abstraction lambda x dot lambda y dot l can
be applied to two inputs, m and n.
By first beta reducing with the first input m to the function lambda y dot l with m substituted
for x, which is yet to be applied to the input n.
We can then perform a second beta reduction with the input n, to finally get the term
l with m substituted for x and n substituted for y.
So our output is a term with a substitution for x and a substitution for y, exactly as
we'd expect from a function that takes two inputs.
This method of functions returning functions to sequentially apply to multiple inputs is
called currying, named after the logician Haskell-curry.
If you have experienced programming, why not try to make some curried functions yourself?
This Python program here, for example, uses the built-in lambda syntax to add two numbers
together using currying.
This way of thinking can take a while to get used to, but hopefully I'm starting to convince
you that the higher-order lambda abstractions have more nuance than you might have expected.
To continue building the primitive calculus into a more practical programming language,
we can encode some more common programming concepts into the calculus.
As a quick example, to use Booleans and conditionals, we can represent true as the term lambda
x dot lambda y dot x, the function that takes two inputs and returns the first, and represent
false as the term lambda x dot lambda y dot y, the function that takes two inputs and returns
the second.
This seems completely arbitrary, but look what happens when we encode an if statement
as lambda b dot lambda x dot lambda y dot b apply to x and then y.
This lambda abstraction just takes three inputs, b, x and y, and outputs the result
of applying b to the inputs x and y.
This looks pretty random, but if we input a Boolean as the first argument, and then
any two terms as the inputs x and y, this function will behave just like an if statement.
It will return the first term if the Boolean is true, and the second if it's not.
In Python, this lambda abstraction would be the program if b then return x, else return
y.
Let's test that out.
Applying our if statement to the inputs true m, n, where m and n are some terms in the
lambda calculus, should return m.
Let's plug true in for b first, giving lambda x dot lambda y dot true x, y, apply to m,
then n.
Plugging in m for x gives lambda y dot true m, y, apply to n.
Then finally substituting n for y gives true m, n.
Now we can turn the term true back into its definition in the lambda calculus, the function
that takes two arguments and returns the first.
So let's first substitute m for x, giving lambda y dot m, apply to n.
Finally substituting n for y just returns m, since there aren't any y's in this lambda
abstraction at all.
So after all that, we've ended up with our if statement with the Boolean input true,
and the inputted terms m and n, outputting the first term m, exactly as we expected it
to do.
Try plugging in false to our if statement, and verify that it returns n this time.
You may be starting to feel slightly short change.
The introduction to this video suggested that the lambda calculus would be the solution
to the problem of complicated programming languages.
However, as you saw from the tedious method by which we encode relatively standard features
into the calculus, the lambda calculus is certainly not suited for any real world programming.
So what was the point in all of this?
Well, for starters, once we've tediously encoded concepts into the calculus, we can
actually just ignore the low level implementation, and just deal with the objects themselves.
For example, we now know that Booleans in conditionals can be simulated, so we can just
deal with Boolean terms and if statements without worrying about unfolding their definitions.
This makes programming in the lambda calculus somewhat reasonable.
As an example, we can write the not function as lambda b dot if b then false, else true.
And of this absolute mess if we unfold all of the definitions.
Given that we can also encode numbers, recursion, and data types like pairs, I hope you can
start to see the power of this miniature programming language.
I implore you to study how these concepts can be encoded, but instead, I'd really like
to focus on the implications of our newfound mathematical programming language.
We set out trying to find a simple and easy programming language that we can learn instead
of more verbose and intricate languages.
But instead, we've stumbled across something far more profound.
The lambda calculus gives us a tiny definition of what it means to compute, and because of
this, allows us to study programming and computation mathematically.
My favourite example of this is introducing a simple type system to the lambda calculus.
As some motivation, consider our not function from earlier.
Lambda b dot if b then false, else true.
This function maps the input true to the output false, and the input false to the output true,
essentially just swapping true and false.
But there is a slight subtlety here.
There is no actual requirement for our input to be a boolean.
Our not function apply to an input of, say, 1, beta reduces to if 1 then false, else true.
This is nonsensical.
If 1 makes no sense because 1 isn't a boolean.
So trying to use our encodings of these concepts to reduce this further will result in a term
with no real meaning.
To avoid this, we can add some rules to our lambda calculus.
We'll assign a label, called a type, to terms in the lambda calculus that make sense, which
in our case is going to be built from the types bool for boolean values and num for
numeric values.
We'll indicate the type of a term by writing the type after a colon.
So we'd write true colon bool to say that true is of type boolean.
Lambda abstractions have the type a arrow b, normally said as type a to b, where a is
the type of its input, and b is the type of its output.
We'll specify the type of the input by putting it next to the input variable when writing
lambda abstractions.
So to finally solve our problem of nonsense terms, we'll now say that we can only apply
a lambda abstraction to a term that has the same type as the one specified by its input
variable.
As an example, we'd now write our not function like this, specifying that our input b is
a boolean.
Since this abstraction takes a boolean and then outputs a boolean, it will have type
bool to boole.
This means that we are simply not allowed to apply this function to the term one, as
this has type num and not boole.
Whilst this does limit what the calculus can do, it does stop a lot of nonsense terms
from arising.
The version of lambda calculus with this typing system attached to it is called the simply
typed lambda calculus, and is very reminiscent of statically typed languages like C or Java.
But despite this similarity, the lambda calculus and its simply typed version was invented
by Alonso Church between the 1930s and 40s, before programming and even computer science
itself was being formally studied.
As a side note, Alonso Church was actually the doctoral advisor of the father of computer
science, Alon Turing, whose achievements have rightly earned him the status of a household
name.
Whilst the lambda calculus has played a pivotal role in the study of programming, it has been
equally as important in the field of mathematical logic, as we can define systems of logic using
the calculus.
In fact, our simply typed lambda calculus has a particularly interesting interpretation
in terms of logic.
Let's say that the types in our calculus should be interpreted as propositions, that
being statements which are either true or false.
Then we'll say that each term in the lambda calculus should be seen as a proof that the
proposition corresponding to that term's type is true.
As an example, suppose we have an arbitrary type A.
We can interpret this as corresponding to a particular proposition.
Then we'll think of a term of type A as a proof that A is true.
If no such term exists, then A is false.
This seems rather strange, but bear with me.
If we use this interpretation of our simply typed lambda calculus, then we find that
the type of lambda abstractions, that being A to B for some arbitrary types A and B, corresponds
to the proposition A implies B.
The proposition A implies B is simply stating that if I can prove A is true, then I know
that B must be true as well.
For example, if A was the proposition that it's raining, and B was the proposition that
the ground is wet, then the proposition A implies B would be, if it is raining, then
the ground is wet.
If we assume that this implication is true, then if I could prove that it was raining,
I would also know that the ground is wet.
So how does this play out in our calculus?
Let's take two types, A and B, then the proposition A implies B is equivalent to the
type A to B.
Let's suppose that A was true, that is, there exists some term N of type A.
If we also suppose that A implies B was true, then there must exist some term of type A
to B.
This must be a lambda abstraction of the form lambda x dot m, where the input x is of
type A and the output m is of type B.
Now, A implies B is true, and we know that A is true, so we know that logically, B must
be true as well.
We'll separate the propositions we're assuming to be true, and the logical conclusions
to these assumptions with a line.
For our correspondence to work, then, we need to be able to use this lambda abstraction
and our term N to create a term of type B.
The key here is noticing that we can apply our lambda abstraction to N, because N is
of type A.
Crucially, we know that this has type B as the lambda abstraction outputs terms of type
B.
As we can find a term of type B, we know that the proposition corresponding to B is true,
just as we expected it to be.
This astounding relationship between propositions and types is called the Curry-Howard correspondence
and has some fascinating implications.
If we build a programming language with this in mind, we can actually use this correspondence
to explicitly write mathematical proofs using computer code.
These languages are called proof assistants, and popular ones include Lean and Agda.
This Lean program here, for example, is a proof that there are infinitely many prime
numbers and we know that it's correct since it type checks.
As this area of research develops, who knows, in the future mathematics might not be taught
with pen and paper, but rather with a keyboard and mouse.
Well, that was quite the journey.
We started out by exploring the essence of functions, and whilst it certainly isn't
a serious contender for modern programming languages, it has allowed us to explore a
rich theory of theoretical computer science and mathematical logic, whilst giving us the
ability to reason about computer programs and code rigorously.
Who knew that a tiny little system of formal logic would have so many implications even
a century after its creation.
If this topic interests you, I would encourage you to subscribe, leave a like, and browse
other videos on this channel for similar explorations of mathematics and theoretical computer science.
But until then, goodbye.

The world of algorithms is vast, but we can often split them into two distinct classes.
The first class is those that are inherently useful. Think of your standard graph algorithms
like DFS or BFS. These algorithms show up all over the place, they are efficient,
and as a result we like to understand them. The second class of algorithms we study are
ones that are just purely beautiful. Think of the first time you saw the incredibly simple
recursive implementation of towers of annoy. If you have a soul, you feel a sense of wonder,
a sense of awe at the elegance of such an algorithm. It happens to not actually be that
useful or efficient as a matter of fact, but we still study it just as we like to read a work
of fiction. It inspires us and motivates out-of-the-box thinking. Today I want to talk about an algorithm
that rightfully belongs in both classes and my personal favorite algorithm, the fast Fourier
transform. The fast Fourier transform or FFT is without exaggeration one of the most important
algorithms created in the last century. So much of the modern technology that we have today such as
wireless communication, GPS, and in fact anything related to the vast field of signal processing
relies on the insights of the FFT. But it's also one of the most beautiful algorithms you ever see.
The depth and sheer number of brilliant ideas that went into it is just astounding.
It's easy to miss the beauty aspect of the FFT since it's often introduced in fairly complex
settings that require a lot of prerequisite knowledge such as the discrete Fourier transform,
time domain to frequency domain conversions, and much more. And to be fair, to get a full
appreciation of the applications of the FFT you can't really avoid any of these ideas.
But I want to do something a little different. We'll take a discovery-based approach to learning
about the FFT in a context that you are all familiar with, polynomial multiplication.
The way I see the structure of this video, it's all about starting with some common ground and
then slowly asking questions that will hopefully prompt you to discover the truly ingenious ideas
behind the FFT. Alright, let's get started.
The setup here is simple. We're given two polynomials and want to find the product.
Our task will be to design an efficient algorithm for this problem. Mathematically,
we know one algorithm to multiply polynomials by repeatedly applying the distributed property.
All of you have perhaps instinctively been applying this algorithm since any algebra class.
Before we try this idea though, the first question we have to address is representation
of polynomials in a computer. The most natural way to represent them is through coefficients,
where we map coefficients to lists. It helps to arrange our coefficients in the following order,
mainly because now the coefficient of the kth index is mapped to the coefficient of the kth
degree term. We will refer to this representation as the coefficient representation of polynomials.
In general, given 2d degree polynomials, the product will have a degree of 2 times d.
And the running time of this algorithm, if we actually went about implementing it with the most
natural distributed property approach, will be O of d squared, since each term in polynomial a will
have to be multiplied by all terms in polynomial b. The question now is, can we do better?
The first really clever idea comes from thinking about polynomials a little bit differently.
To see the key inside here, let's take a look at one of the simplest polynomials,
a degree 1 polynomial or a linear function. We can represent any line with exactly two
coefficients, one for the degree 0 term and one for the degree 1 term. The key part that makes
this representation valid is that every representation has a one to one mapping to a unique line.
What other representations of a line have this property? There are actually several
reasonable representations, but the one that we are going to use is the two point representation.
We know from geometry that any line can be represented by two distinct points.
And turns out that there is a neat extension of this for general polynomials, which I will state here.
Any polynomial of degree d can be uniquely represented by d plus one points. For example,
if I gave you three random points, this means that there is exactly one quadratic function
that goes through all three of these points. If I give you four points, there is exactly one
cubic function that goes through all these points. This statement is perhaps a little surprising,
so it deserves a proof. Suppose I give you d plus one unique points of a d degree polynomial p of x.
We want to show that for these set of points, there is only one set of coefficients.
So if we actually evaluate our polynomial at each of these points,
we get the following set of system of equations. Whenever you have a system of equations,
writing it as a matrix vector product is almost always helpful for analysis.
One nice property of this matrix is that if each of our original points is unique,
as it is in this case, the matrix will always be invertible.
The easiest way to show this is by calculating the determinant, which you will find is non-zero,
but I'll link a nice linear algebraic proof of this fact in the description as well for those
interested. Anyways, what this means is that for every set of points, there exists a unique set
of coefficients and consequently a unique polynomial. Taking a step back, what this means
is that there are actually two ways to represent polynomials of degree d, the first of which is
the coefficient representation and the second with just d plus one points, which we will call
the value representation. A nice property of using the value representation is multiplication
of polynomials is much easier. Let's say I ask you to multiply these two polynomials a of x
and b of x. We know the resulting polynomial c of x will be of degree four, so we'll need five
points to uniquely represent the product. What we can now do is take five points from each of the
two polynomials and then simply multiply the function values one by one to get the value
representation of the product of the two polynomials. Following our earlier rule,
there is only one degree four polynomial that passes through these points. That polynomial
happens to be the following in the coefficient representation and this is indeed the product
of our original a of x and b of x polynomials. With the multiplication operation being performed
using the value representation, we've now reduced the time for multiplication from our original
d squared operations to the order of only degree d operations. Okay, so let's propose a plan for
an improvement to polynomial multiplication. We are given two polynomials in the coefficient
representation of degree d each. We know multiplication is faster using the value representation,
so what we'll do is evaluate each of these polynomials at 2d plus one points, multiply each
of these points pairwise to get the value representation of the product polynomial,
and then finally somehow convert the value representation back to the final coefficient
representation. This is the grand plan, but there are several pieces of the puzzle we haven't figured
out. What we're missing is really some sort of magic box that could take polynomials in the
coefficient representation and convert them to the value representation and then vice versa.
That magic box my friends and trust me it is truly magical is the fast Fourier transform.
Let's first focus on taking polynomials from the coefficient representation to the value
representation, which we will call evaluation. We have a degree d polynomial and we want to
evaluate the polynomial at n points where n is some number greater than d plus one.
Let's think about the most straightforward way to do this. We can pick n random x coordinates
and simply calculate the respective y coordinate. This works, but when we deconstruct what's
actually going on here, we run into our old foe. Each evaluation will take o of d operations,
making this method run in o of n times d time, which ends up being o of d squared to evaluate
all n points. So we're back to where we started. Can we find a way to optimize this?
Let's try to simplify the problem. Let's say instead of considering a general polynomial,
we wanted to instead just evaluate a simple polynomial like p of x is equal to x squared
at eight points. The question now is, which points should we pick? Is there any set of
points when knowing the value of one point immediately implies the value of another?
In fact, there is. If we pick the point x equals one, we immediately know the value
of the point x equals negative one. Similarly, if we pick x equals two, we automatically know x
equals negative two will have the same value. Extending this idea, the key property we want
here is that our eight points should be positive negative pairs. The reason this works is due to
the property of even functions where a function evaluated at negative x is going to equal the
function evaluated at positive x. Okay, but then the next immediate question is, what about functions
like x cubed? Does the same trick work? It actually kind of does, but with one caveat.
Each positive x value will have the same value as the negative x value, but with the sign flipped.
So in these two cases of odd and even degree single term polynomials, instead of evaluating
eight individual points, we can actually get away with evaluating exactly four positive points,
from which we immediately know the value of the respective negative points.
Let's see if we can extend this idea to a more general polynomial,
taking inspiration from our early exploration, let's split our polynomial into even and odd
degree terms. If we factor an x from the odd degree terms, we end up having two new polynomials,
where these new polynomials have only even degree terms. Let's actually give these
polynomials formal names, the first representing the even terms and the second representing
odd terms after factoring out x. This fact allows us to recycle a lot of computation between positive
and negative pairs of points. A bonus fact is that since these even and odd polynomials are
functions of x squared, they are polynomials of degree two, which is a much simpler problem than
our original degree five polynomial. Generalizing these observations, if we have an n minus one
degree polynomial that we want to evaluate at n points, we can split the polynomial into even
and odd terms with these two smaller polynomials now having degree n over two minus one. So,
how do we evaluate these polynomials with half the degree of our original polynomial?
Well, what's beautiful here is that this is just another evaluation problem, but this time
we need to evaluate the polynomials at each of our original inputs squared. And this works out
nicely since our original points were positive negative pairs, so if we originally had n points,
we now only end up having n over two points. This is starting to smell like the start of
a recursive algorithm. Let's take a look at the bigger picture. We want to evaluate a polynomial
p of x at n points where the n points are positive negative paired. We split the polynomial into odd
and even degree components where we now have two simpler polynomials of degree n over two minus one
that only need n over two points to evaluate. Once we recursively evaluate these smaller
polynomials, we can then go through every point in our original set of n points and calculate the
respective values by utilizing the relationship between the positive and negative paired points.
This gives us the value representation of our original polynomial. If we can get this to work,
this means we have an O of n log n recursive algorithm since the two recursive sub problems
have half the size of the original problem and take linear time to evaluate the n points.
This would be a huge improvement from our earlier quadratic running time,
but there is one major problem. Can you spot the issue?
The problem occurs at the recursive step. The entire scheme relies on the fact
that the polynomial will have positive and negative paired points for evaluation.
This works at the top level, but the next level we are evaluating n over two points where each
point is a squared value. These all end up being positive so the recursion breaks. So then the natural
question is, can we make these new set of points positive negative paired? Some of you may already
see it, but this actually leads to the third absolutely ingenious idea behind the FFT.
The only way this is possible is if we expand the domain of possible initial points to include
complex numbers. For a special choice of complex numbers, the recursive relation works perfectly
where every subsequent set of points will contain positive negative pairs.
What possible set of initial n points has this property?
This is a hard question and to answer it we are going to do a little bit of reverse
engineering with an example. Let's say we have a degree 3 polynomial which requires at least
n equals 4 points for its value representation. These points need to be positive negative pairs
so we can write them as x1, negative x1, x2, and negative x2. We know that the recursive step
will require that we evaluate the odd and even splits of the polynomial at two points,
x1 squared and x2 squared. Now the key constraint here is that for the recursion to work,
these two points also have to be positive negative pairs. So we have an equivalence between x2
squared and negative x1 squared. At the bottom level of the recursion we'll have a single point
x1 to the power of 4. Now what's nice is that we get to choose these points. Let's see what happens
if we pick our initial x1 to be 1. This means two of our initial points are 1 and negative 1
which at the next level of recursion means that x1 squared and negative x1 squared also have to be
1 and negative 1 respectively. And at the bottom layer we have only one point which ends up being
1. Now the question becomes what x2 should we choose so that when we square x2 we end up with
negative 1. The answer to that is the complex number i which means that the four points that
we need to evaluate this polynomial at are 1, negative 1, i and negative i.
An alternate perspective to what we just did here is that we essentially just solved the equation
x to the power of 4 equals 1. Since at the bottom layer of the recursion the value of any of our
original points to the power of 4 was 1. We know this equation has four solutions all of which
are encompassed by a special set of points called the fourth roots of unity. Let's see if this
generalizes. If given a degree 5 polynomial we'll need n is greater than or equal to six points.
Since our recursive method is splitting each problem in half it's convenient to just pick
a power of 2 so let's pick n equals 8. Now what we need to do is to find eight points that are
positive negative paired such that each of these points when raised to the eighth power is equal
to 1. We see that the right points are the eighth roots of unity. Generalizing this to any d degree
polynomial what we will do is pick n is greater than or equal to d plus 1 points such that n is
the power of 2 and the points that we should choose are the nth roots of unity. This fact
deserves a little bit more explanation. Why does this work? Before we answer that question let's
formalize a few things. The nth roots of unity are the solution to the following equation
and they are best visualized as equally spaced points on the unit circle. The angle between
these points is 2 pi over n. With this fact a nice way to compactly write these points is with
complex exponential notation through Euler's formula. One standard way to define the roots of
unity is by defining this omega term as e to the power of 2 pi i over n and then what this allows
us to do is define individual roots of unity quite compactly. Here are some examples.
So now when we say we want to evaluate a polynomial at the nth roots of unity what that really means
is we want to evaluate it at omega to the power of 0, omega to the power of 1, so on and so forth
until omega to the power of n minus 1. So going back to our original question of why evaluating
the polynomial p of x at the nth roots of unity works for a recursive algorithm there are two
key properties at play here. For one our original set of points are positive negative paired where
for the jth root omega to the power of j omega to the power of j plus n over 2 is going to be the
corresponding pair. Now in our recursive step we will be squaring each of these points and passing
them on to the even and odd degree polynomials. This is what happens when we square our original
nth roots of unity. This reveals the second key property of the nth roots of unity. When we square
the nth roots of unity we end up with the n over 2 roots of unity which are also positive negative
paired and are just the right number of points for the two new polynomials of half the degree.
This same pattern holds at every level of the recursion until we end up with just one point.
How beautiful is that?
All right we are now ready to outline the core fast Fourier transform algorithm. The
FFT will take in a coefficient representation of a degree n minus one polynomial where n is the power
of two. We will define omega as e to the power of two pi i over n to allow us to define roots of
unity easily. The first case we need to handle is the base case which is going to be when n is equal
to one. All this means is that we are evaluating the polynomial at one point. Our recursive case is
two calls to the FFT. One on even degree terms and one on odd degree terms. The intention is that
these polynomials are now half the degree of our original polynomial so they only need to be evaluated
at n over two roots of unity. Assuming the recursion works the output of these calls will be the
corresponding value representation of these even and odd degree term polynomials which we will label
as y e and y o. Now on to the tricky part which is to take the output from these recursive calls
and combine them to get the value representation of our original degree n minus one polynomial.
We saw earlier that the key idea was to use the relationship between positive and negative pairs
but now we have to slightly modify this logic for our roots of unity inputs. As a quick note
yes I did modify the indexing to zero indexing because we're getting ready to write some code.
We know the jth input point will correspond to jth root of unity which results in the following
relationship. We also saw earlier that the paired point negative omega to the power of j is equal
to omega to the power of j plus n over two due to the properties of the roots of unity. Using this
fact we can modify the second equation as follows. And lastly one more fact that's nice is that the
jth index in our y e and y o list correspond to the even and odd polynomials evaluated at omega
to the power of two times j. What this allows us to do is rewrite our equations as follows
which makes it much easier to implement code. As mentioned this part is tricky so I encourage
you to take your time and verify that each of these steps is indeed true. The final step in the
FFT algorithm is to then return the values of a polynomial p evaluated at the nth roots of unity.
Let's now translate this outlined logic into code. Our function FFT will take an input p which is
the coefficient representation of a polynomial p. We first define n as the length of p and we will
assume that n is a power of two. Just to be clear there are implementations of the FFT that can handle
n not being a power of two but those are way more complicated. The power of two cases encompass the
core ideas of the algorithm. We now handle the base case which is just a matter of returning our
original p. This makes sense since we only have one element making p a degree zero polynomial
or constant. Otherwise we define omega as we have outlined and then proceed with the recursive step.
The first part of the recursive step requires splitting the polynomial into even and odd
degree terms which is quite easy to do. Then we recursively call our FFT function on these
polynomials that now have half the degree of our original polynomial. We denote the outputs as
y e and y o as we have done in the outline. Now it's time to put this all together. We initialize
our output list which will contain the final value representation. Then for all j up to n over two
we calculate the value representations as we have outlined. After populating all values in our list
we then return that list and that's the FFT. Overall pretty crazy how all the ideas we talked
about end up coming together in eleven lines of truly elegant code. Let's now take a larger
look at our original problem of polynomial multiplication and see where we are. We now
have a way to convert coefficient representations to value representations efficiently using the FFT.
So now the only missing piece is the reverse process of converting from value representations
to coefficient representations which is formally called interpolation. This is where things get
really wild. On the surface the idea of reversing evaluation feels like a significantly harder task.
Let's take a step back and look at this problem from another perspective.
Evaluation and interpolation are closely connected and as we saw earlier we can express
evaluation as a matrix vector product. We have a vector of coefficients multiplied by a matrix
of our evaluation points to give us the value representation. Now in the FFT algorithm the
kth evaluation point was a corresponding root of unity which allows us to rewrite the matrix
vector product as follows. This particular matrix has a special name the discrete Fourier transform
or DFT matrix. In most textbooks and references the FFT at its core is an algorithm for calculating
these types of matrix vector products efficiently. Polynomial evaluation at the roots of unity happens
to be one case where this type of matrix vector product shows up so that's why we can use the
FFT. Anyways the nice fact about the FFT and evaluation in this context is that interpolation
is much easier to understand. Interpolation requires inversing this DFT matrix. For interpolation we are
given a value representation of our polynomial and we want to find the coefficient representation
which means we have to multiply the value representation by the inverse of the DFT matrix.
So let me show you what the inverse of this matrix looks like. I'm purposefully skipping
a lot of important linear algebra facts here since that would be an entirely different video
but given that this is the inverse matrix what stands out to you?
It's really quite amazing but this inverse matrix looks almost the same as our original
DFT matrix. In fact the only difference is that every single omega in our original DFT matrix
is now just replaced with omega to the power of negative 1 with a normalization factor of 1 over
n. This indicates a potential to reuse the FFT logic for interpolation since the matrix structure
is basically the same. Let's formalize this suspicion by doing a direct comparison. In
evaluation which involved the FFT we are given a set of coefficients and evaluate the polynomial
at the roots of unity to get a value representation. This involved the following matrix vector product
where we define omega as e to the power of 2 pi i divided by n. Looking at interpolation we now
want to define what is formally called the inverse FFT algorithm. The inverse FFT will take a value
representation where each value was evaluated at the roots of unity and gives you a set of
coefficients for the original polynomial basically reversing what the original FFT did.
As we just saw this requires multiplying by the inverse of the DFT matrix. We noted that each
omega in our original DFT matrix now corresponds to 1 over n times omega to the power of negative 1.
Now the punchline here is that what this means is we can define the inverse FFT
as the same FFT function but now called on the value representation with omega defined as 1 over
n times e to the negative 2 pi i divided by n. That's it. With those small changes we have
an inverse FFT that performs interpolation. Just so we are super clear on what sorcery just happened
let me remind you of the original FFT implementation and now let me show you the
inverse FFT implementation which takes the value representation as an input.
What we literally do is copy our FFT implementation, change the name of the recursive calls to
match and then literally change one line of code. One line and that's all there is to it.
So if your mind isn't blown you haven't been paying attention. Let's take a look at what we just
did. We motivated the FFT through the problem of polynomial multiplication where the first
brilliant idea came from representing and multiplying polynomials using the value representation.
Converting polynomials to a value representation required us to come up with an appropriate
set of evaluation points. Our first attempts at solving this problem inspired the clever idea of
using positive negative pairs but the recursion didn't quite work unless we expanded the domain
to complex numbers. The next brilliant idea came from using the nth roots of unity where the points
at every level of recursion are positive negative paired. This evaluation scheme using the roots
of unity encompassed the essence of the FFT algorithm. When confronted with the problem
of reversing the process using interpolation we discovered something truly astounding.
The inverse FFT is the same algorithm but with one minor adjustment. So if we take a look at what
we just did here there's not one, not two, not three, but four absolutely mind-blowing ideas
that come together to make this work. Do I really need to say more on why this is my favorite algorithm?
That's all for this video and thanks for watching. If you enjoyed the content please
hit the like button so that this content will be recommended to more people. If you want to
see more content like this please don't forget to hit the subscribe button and if you want to
more directly support the work of this channel please check out the Patreon page linked in the
description below. I'll see you all in the next video.

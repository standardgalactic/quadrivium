Four years from now, you're going to have a very large amount of
actors that are going to have a rather non-trivial amount of compute.
Like inflection is building a cluster of 22,000 H100s.
The military guys now kind of getting into it and saying like,
Hey, maybe we should start racing with China.
The Aurora supercomputer guy is saying like, Hey, we're going to work with Intel.
We're going to make a trillion parameter model.
And we're going to use our big supercomputer with like 60,000 Intel GPUs to make that happen.
People who are making these orders are aware of the other people also making
these orders, right?
The relevant actors are all aware of what is going on.
The people with the most impact or chance to really do these kinds of things are
the most acutely aware and the most like ready to race.
And here is Kurtis Huebner, head of alignment at Eleuther AI and known on the internet as AI waifu.
People might recognize him as the one who commented on Eleuther
Rutkowski's death with dignity by saying, fuck that noise.
Can you like explain for people who were like, haven't seen that comment,
like what your comment was about?
So maybe like what the posts permit Koski was as well.
Yeah, sure.
So at kind of a high level, the posts that Eleuther posted on April Fool's of all days
is basically Mary's strategy is we're probably all going to die.
Almost certainly we're all going to die.
But really what we want to do is we want to maximize the sort of log odds of survival.
So essentially they're saying, like, you know, do what you can to not like,
you know, not screw up existing alignment efforts.
Do what you can to, you know, maximize the probability of survival.
But in reality, we're probably all going to die.
And so, you know, I saw this post and like there's a lot.
There's a bit of a misunderstanding, at least in my opinion,
there's a bit of a misunderstanding about like what my comment like says
and kind of the tone I took because one thing that happened with that post
that I didn't, you know, they really like too much, but was maybe necessary
for some people is that the tone of the post was a very somber one.
Kind of being honest about like the, you know, the gravity of the situation
and, you know, how deep and, you know, in trouble we all are.
And really that was kind of the main thing I wanted to counteract with that post
or like with my comment on that post.
I actually do, to a large extent, agree with, you know, people like Connor
and Ellie's or that the probability that we are all toast is very, very high.
However, like the way I kind of see it is that you can't sort of give off
vibes that you're going to give up, so to speak, or that you're going to accept
anything less than success.
And sort of the reason I think that is like, I don't know if like you're
familiar with sort of like the game theory kind of notion of a stag hunt.
Essentially, you have two hunters and the hunters have the choice of either
hunting a stag or a hare.
And if they go for the hare, there's a very high probability of success.
And so they'll probably get the hare, but the hare doesn't have a lot of meat.
So it's not a very big reward.
If you go for the stag, though, the only way you actually manage to successfully
hunt the stag is if the other hunter also goes and hunts the stag, right?
And both hunters, in the traditional stag hunt, both hunters are faced
with the same choice, right?
So the ideal outcome is that kind of both of the hunters go and they go hunt
the stag together and then they get the stag and then, you know, they get a lot
of meat because the stag has a lot of meat instead of the, instead of the
the the hares, essentially, both of them have to sort of trust each other,
that they're not going to try to go for the easier route.
And I guess like my perception when it comes to existing alignment efforts
is that it's like a stag hunt, except that it's not like two people.
It's like you need hundreds of people to get together, maybe thousands,
maybe, you know, tens of thousands, all getting together to basically solve
this, you know, solve the alignment problem.
I think that because of that, at least for a lot of people who kind
of understand the magnitude of it, it you really can't signal something
like this sort of the somber attitude of failure, even if it even if it is
very true that we're almost certainly screwed because a lot of people
are probably going to take the easier road of being like, well, we're all dead.
I'm going to go on vacation.
You know, I'm going to enjoy kind of the rest of the life that I have.
And so really that was kind of one of the at least like one of the motivations
for for making the comment that I did.
So what you're saying is basically like we should all go with everything we have
because because that's the world in which like we actually solve the problem
and and make progress and did his posts had the downside effect of like people
that might like be become like depressed or sad about it.
And then like try to like the fact and say like, you know, if we're doomed,
like I might as well like enjoy life for like a few years.
And so what you say fuck that noise or let's try to like listen to some
very like optimistic things and like do the thing.
That's what you were trying to like encourage and give some like wave of optimism.
Right. Yeah.
And the thing is, it's important to notice too that like the the post
that Eleazar wrote also serve like a very important purpose.
Right. There's a lot of people that I believe were sort of, I guess,
sleeping at the switch, maybe like they were kind of aware of the situation,
but they weren't like aware of the gravity of it.
And I think that like for those people, the the post had kind of would have
the opposite effect, right? Of like, oh, wow.
Okay. You know, we don't actually have this under control because the guys
who are like their entire specialty is preventing this from happening
are saying there's a zero percent chance, essentially,
you're almost zero percent chance that we're going to succeed.
Right. Both messages were sort of necessary to say like for the people
that are aware of the gravity of the situation and are, you know,
trying to do something, you know, knowing that there are other people
out there trying to mitigate existential risks and still kind of going at it
with everything they have, but also like serving as a wake up call for people
that may not have been paying as much attention.
So for me, the post wasn't like that much about the probability of dying
from AI extinction as it was about like how to behave with dignity or like
don't trying like not trying like to like crazy things that might have
like bad effects in the long term, like second order consequences.
And like if you think that like blowing up TSMC or like doing crazy
thing like this might be good to save the world, then instead,
maybe you should consider like dying with dignity instead and like doing the
like the ethical things that might be like better in the long run or in
the medium run, depending on your timelines.
So I guess that was like my kind of like intuition.
And maybe if I want to like push back against your post or your comment,
it would be like maybe try to be optimistic, but still keep the like dignity part
from from your coast game.
Yeah, I think I think that's that's a fair kind of point.
In fact, like one thing that happened after I made that comment is I have kind
of a really long discussion with Connor kind of following this.
And one thing that we sort of like disgust is really like, I think in the
post Ychowsky talks about dignity is like the logarithm of the probability of your success.
Right. And I think there's there's really a fair point to be said there that like
for us to succeed, it's not going to be because like a high variance,
you know, a single high variance action was done.
You know, I think he refers to those as like miracles.
But you need a lot of little things to go right and build on top of each other.
And I don't I don't think I really disagree with that too much.
It's mainly the the the point that I'm getting at is is the is the tone.
Right. And and what you're signaling to the people around you who are actually trying
to reduce existential risk.
And I think the tone might be like a huge factor in how we like manage to convince people to
to work on this. I think mostly on on Twitter or on internet, people have been like committing on
this as being like a do more post or like the even the word like doom or doom or I think it
might be like negative long term to to be like a term people use as often as they do.
And instead, we should like talk about, you know, like aligning AI or building an utopia or
like maximizing our impact on the light code or those kind of things.
And I think like people like tend to like see maybe like this post or a lot of the vibe vibe
there as like maybe like too negative.
So yeah, I guess like some people on Twitter have been like asking like if you add like any
updates since the since last year or last year and a half, like did you since your road
had come and do you have like any like new thoughts on this or like a new perspective or
you're still like as motivated and willing to, you know, work very hard on this.
I think basically nothing has changed. I do think that the situation has gotten actually
like significantly more pessimistic even than when when I did write that comment.
So I think that with the release of GPT-4, there was a like a slice of worlds where Open AI kind
of said, okay, let's, you know, let's pump the brakes. There's a slice of worlds where like
Open AI kind of says, okay, let's, you know, let's stop releasing things. Let's kind of close the
gates and let's take actions to kind of slow down race dynamics. And right now we're seeing sort of
the complete opposite of that, right? You have, you know, GPT-4, you have a tremendous amount of AI
hype pretty much all over the place. You have a lot of different companies that are now cropping up
trying to replicate what Open AI is doing. You have a lot of venture capital kind of flowing
into the space. Generally, if you're trying to avoid this kind of further acceleration of an AI
race, this is not the timeline that you want to be in. How pessimistic are you about AI being an
extinction risk? And like, more generally, like, what are your like 10 months? How do you see the
future in the next few years? If you had to ask me for a number, I think I would say, like, we are
90% toast. And the reason I'm saying 90% instead of like 99.99% or something like that is mainly
because I do believe that there's a great deal of uncertainty that I'm just not able to move.
Factors that I haven't considered, places where I'm wrong, that's like most of the
thing that is carrying me to be like, you know, it's not 100%. But I really do think that the
situation that we're in is quite dire. So internally, when you wake up in the morning,
without taking into account your uncertainty, you think there's like 99.99% chance of your dying?
Maybe not like 99.99, but like, you know, pretty, pretty up there, you know, like 99% chance that
we're all toast. We're really toast. Why do you think that? Was there like reasoning or like
evidence for disbelief? So I think like, you know, where I kind of disagree with with a lot of other
people is I do think that like, once we kind of understand how to produce intelligence, and once
we kind of how to, we understand how to do it very quickly, or like, you know, we to do it
efficiently is really, is really the thing. I think that it took out is really just that
as soon as we had a language model that was good enough, we plugged it into every API we could,
even API is that, you know, we just made it generally, I think opening, I just made an
update to make it generally able to be plugged into APIs by, you know, training it to format
JSONs or something like that. So like, you know, this is the kind of behavior where like,
it's the timeline where we don't survive is what it is. So you get that and you get like a whole
bunch of other kind of similar failures. And you add it all up. And it's like, okay, we're almost
certainly toast. There's definitely room for me to be wrong about the whole situation. But it's
very much not looking good. I'm kind of lectures of like, if you have any teal man or, yeah, so I
guess there's a couple different like, kind of classes of counter arguments. So like, one of them
is that I'm completely wrong about the efficiency arguments. So like, you know, you look at the
total compute that is necessary. And maybe you really do need like, an absolutely enormous
amount of compute to be able to do a GI. And as a result, like, you know, okay, you need a full
super computer, you need 10,000 graphics cards, and Moore's law will, you know, just happen to
peter out, right? When it is petering out a little bit already, we are kind of seeing that
where, you know, we are able to run these things, but they are extremely technically,
you know, it's very technically challenging to run them at scale or in any way, dangerously.
So that could be like one kind of component where I'm wrong. Another kind of component
where things could be, you know, where things could go well is just like, you know, maybe I'm
wrong about the difficulties of alignment. And you can actually just get away with really simple
tricks, like RLHF and stuff like that. And that, you know, turns out to be good enough that, you
know, you don't really need that much more than the current existing techniques or really anything
more at all. And you get a system that just in general works well. So really, it's, you know,
those would be kind of examples that I would reach for is that like, the ceiling on capabilities and
the speed at which you can get to that ceiling is significantly lower than I thought it was,
or alignment just turns out to be significantly easier than I thought it was.
And so that's why it's 90% and not like 99.99 because you have like 10% chance of like this
being true. Exactly. Yeah. There's definitely like there could be a modeling error somewhere
and things work out to be significantly better. In terms of like compute we need or like size of
the models we need to get to something dangerous. So you're saying that maybe the speed or like
the ceiling is maybe wrong in your model. What do you guys predict for like, how big of a model
do we need or when we will get there? So kind of like my, you know, if I had to kind of give a
number, I would probably guess like something like, you know, the thing that you need is probably
something really small, like 10 to the 10 floating point operations per second, you know, over the
over the lifetime of a model 10 to the power of 19 floating point operations per second,
which is really not that much, right? That's like 100 teraflops or like 140.90 for like,
you know, three hours. Now you're not going to be able to do it with, you know, 40.90
in three hours with like current algorithms. You'd probably need to do like quite a bit of
your optimization and improvement and maybe you need like a slightly different computer
architecture or something like that. But like, that would probably be like the lower bound.
Or people were not into the deep weeds of training, deep learning models,
like how expensive it is to get like a 40.90 and like, is it like something you can get
right now? Or is it like something like top companies use? Well, I mean, like a 40.90 is
like top of the line gaming GPU. So like any gamer with like, you know, I don't know exactly what
the current prices are, but like, it's like one or $2,000 is able to get their hands on one.
So, so yeah, this is, this is a level of compute that is very much accessible,
a large proportion of users or people. So when you say like, the compute required to have something
dangerous or something that could like, maybe like disempowered, you might see,
are you saying like for training or are you saying for inference? Because I feel like for
training, that's like not a lot of compute, right? It's really not a lot of compute,
but I still do think it's actually like, you know, if you were to get the AIs from the future,
you know, after we've kind of had a lot of room for optimization of these algorithms,
I think you would actually get something that can go from like zero to human level in,
sorry, it's not three hours, it's 30 hours, but you know, in a day on a 40.90 essentially,
you know, using the, using like the best algorithms. I think in practice, like,
it's probably more realistic that we're like three orders of magnitude higher than that,
but like three orders of magnitude higher than that is still not that much, right? Like that is,
you know, that's, you know, 10 of them for like $10,000 worth of compute for 100 days,
right? Which is very detainable for a lot of people or like one of these graphics cards for
three years, right? Again, that is a very small, you know, it's not like exactly consumer level,
at least right now, but you know, assuming that we can still keep getting performance improvements
and stuff like that, it will be very, very soon. It's really not that much compute. And I can go
in a little bit more detail about like where I'm getting kind of those numbers and stuff like that.
Yeah, yeah, please. Where do you get his numbers?
So like, these are like really cheap heuristics. Yeah, part of it is just kind of like estimates
of like how much compute the human brain uses. You look at the human brain and it's like estimates
estimate that like each neuron fires maybe like once every 10 seconds. And there's 100 billion
neurons in there. So that's like 10 billion neurons firing every second. And every neuron
is connected to 1000 synapses. And you say, okay, like each synaptic operation is one
floating point operation. So you make this arbitrary equivalence. I actually think a floating
point operation is more complicated than a synaptic operation. But you know, you can about get that
equivalence. So you have this you have this like rough equivalence there. And that gives you an
estimate of like 10 to the 10 to the 12 or 10 to the, you know, 13, or something like that. Yeah,
10 to the 13, I believe is the estimate you get. So you get like 10 trillion synaptic operations
per second, which is 10 to the 13. And then basically, like what I'm doing is I'm taking
that number and saying, okay, well, you know, a human lives for like 10 to the nine seconds over
30 years, or, you know, that's about it. So 10 to the nine plus 10 to the 30, or 10 to the 13 is
well, not plus, but you get the point. That's 10 to the 22 operations. So your your lifetime training
compute is 10 to the 22 operations. And essentially, what I'm doing is I'm allowing for like, you know,
let's say whether we find like 1000x improvement and efficiency over what the human brain can do,
because we can express algorithms that the brain kept express, we can compute in ways that the
brain can compute, you know, we we can we can like the space of intelligence is presumably very large.
And so that's how you get to the 10 to the 19 number. So like, if you if you make the assumption
of like, okay, nothing, nothing, no algorithmic fanciness, you get like 10 to the 22. And if you
assume like, you know, arbitrarily, like, we'll get 1000 times more efficient than that, you get
10 to the 10, which again, okay, like is kind of like numbers that you, you know, you pull out of
thin air. But like, this sort of gives you a ballpark of things. And when you compare it to,
you know, existing models, like, I think like GPT three is like 10 to the 23, GPT four is like
either 10 to the 24, 10 to the 25, and probably like, you know, frontier models going forward are
probably going to be 10 to the 26, 10 to 27 flops, you're really into several orders of magnitude,
more compute, KB to that is that like 10 to the 13 floating point operations per second is a very
kind of lower end estimate of like what the requirement of the human, you know, the computer
requirements of the human brain. Some guys put it at like, you know, 10 to the 16, which is three
orders of magnitude higher. And that kind of puts you at like 10 to the 25 for total lifetime compute.
Some people put it all the way up to 10 to the 18, which puts you at 10 to the 27. You know,
it's kind of, it's kind of all over the place. But I do think kind of the lower bound is probably
correct for like, you know, weird vague intuition reasons. And as a result, I do think that we are
in a very hot water, I guess, like we are in a very large hardware override, and it is simply a
matter of either, you know, we get an AGI, and then after that, it goes and does a little bit of
recursive salt improvement, or, you know, something similar happens. And we are very much in a
situation where there is mass proliferation of human level, artificial intelligence.
Okay, so I think I got, I got the main reasoning behind your argument and kind of the main numbers.
So you're saying that the main like, uncertainties about like, how much compute is the brain doing,
and usually I think like your 10 to the 13 flops per second is probably,
probably we're on or maybe like other people are have like other estimated like two or three
orders higher. Yeah, yeah. So like, yeah, some people will say, well, okay, a synaptic operation,
you know, a neuron is doing a little bit more complicated stuff. Or really, you're only looking
at like the firing neurons, really, you should be looking at everything, you stuff like that,
your estimates are going to go up. You know, depending on how much complexity,
you contribute to to what's going on. And then all of that kind of, you know,
cranks up the the require, essentially. So I still haven't read a GI coach rise report fully,
but I'm doing like a series of videos on it. And I'm looking at the graphs right now. And I think
for the lifetime anchor, which is like, how much compute is maybe like a human doing in
terms of compute from like birth to death, I think the estimates point at, at least from for
a jazz best guess goes from like, 10 to the 29 in 2025, to like 10 to the 27. Like after like,
I would make improvements and like other efficiencies. So your your 10 to 22 seems like
much lower than like everything else, even like the most like aggressive things.
So I can talk a little bit about that. So I think that there's a couple of things
that I disagree with in the report. So I think one thing that Ajaya did is that she disagrees with me
very, you know, very much like a complete sign flip in terms of like, how algorithms kind of
play into things. So I believe that, you know, we're not going to have too much difficulty level,
you know, finding human level efficiency algorithms. And in fact, we're probably going to be able to do
several orders of magnitude better than humans in terms of algorithmic efficiency.
I believe that if you look at the text of the the culture report, and again, you know,
don't quote me on this to actually go check to see if it's correct. But I believe there's actually
a multiplier of 1000 of three orders of magnitude in the other direction, you know, basically,
I'm saying you let's go 10 to the 13, and then let's multiply by, you know, one 1000s to get 10
to the 10. And they're going and saying, well, let's go 10 to the 13, and then let's go the other
direction. Right. Because it's very hard to get like human level improvements.
Yeah, because because like, you know, the idea is maybe like the human brain and really brains
in general have maybe had a lot of time to optimize their algorithms. You know, evolution has
probably spent a lot of time kind of optimizing things. And as a result, like, we're not going to
be able to in any reasonable amount of time, match the quality and efficiency of human level
algorithms. And what another kind of thing that I believe the report did is that the lifetime
anchor when they initially had it was predicting or a couple of the anchors actually were predicting
that we would already have had a GI by now. And I believe what they did is they did like a sort of
a squishing operation where they took sort of the Gaussian estimates and they sort of like pushed
them over because they said, okay, well, we haven't we haven't seen it so far. And I think that like
that squishing operation is the wrong way to kind of do things. What you should be doing is you
should actually be cutting off the Gaussian and renormalizing. And that produces a much more
sort of pressing, you know, like you see that already, like a lot of the probability mass is
a lot closer. So I think, and again, you know, the double check that that's actually what it is,
but at least that's what I remember from kind of glancing at the report. And those kind of
corrections, and I think there's a couple other things all kind of work together to lead to a
significantly higher estimate. I will however note that I believe Ajaya initially predicted
something like 2050 or something like that for median timelines, and that just since revised
her predictions down quite a bit. So, you know, I don't actually know like what specific changes
happened in her model to that led to that sort of down revision. But you know, yeah.
Yeah, if I remember correctly, in the revision, there was something about being able to like
have AI's that do code for you. I think code coding was like a big part of like,
how much she thought that AI's will be able to generate value in the future. And so AI's could
be like transformative sooner because of like that, like how important code is for for everything we
do. And like how easy it was to do code right now. But yeah, again, like check the post for
more details. And the thing you said about like squishing the distribution on the rights,
I think it makes sense if you're like, if you're like in 2019, and Ajay seems like very far away
to not like include models that predict Ajay happening today was like, was like high value
if you think it's like very far away. I think I think there was something about like not
including models that would predict things much sooner. And I haven't seen the part where they're
like, they moved the thing to the right for 2025. I think it would be good to have models to like
have nonzero probability mass on 2023 or later. Yeah, I think this is actually like the case for
me. So like, I do have to like, you know, I'm not some kind of, you know, really good predictor. I
have, you know, previously made very aggressive kind of timelines. And, you know, I said like,
okay, maybe I think like in 2015 or something, I was thinking like, there is like a 15% chance
that like by 2020, we would have a GI. And that turned out to be wrong. Right. So that is like,
I lose base points, I lose predictive, you know, credibility, because of that. And I think like
when you know, when you kind of fully integrate that till out to like 2023, where we're at now,
like, it's something like 30% or 40% or something from my initial like 2023 estimates,
sorry, 2015 estimates of when a GI was going to happen. So, you know, you can go to sort of look
at me and say, well, like, my timelines are actually like updating in the other direction,
where it's like, okay, they're actually stretching out. Another kind of like fun thing to think about
when it comes to timeline prediction dynamics is that a consistent predictor will have their
expected value of when the timeline changes slowly increase over time. You know, you can imagine
like a toy model of this being like an exponential distribution of when you think a GI is going to
happen. And an exponential distribution will have the property that, you know, it's expectation,
it always looks the same, you see it with like radioactive decay of particles where like,
if you have a particle, and it doesn't decay for like five minutes, then you still expect that the
amount of time it takes to decay is going to be the same. Because every time you do the Bayesian
update, you renormalize the probability distribution, and you get back to the original
probability distribution that you had. And so you do actually get this general effect, where
in general, you do actually expect timelines, if you're if you're fully updated and you're
fully calibrated, you do expect your timelines to sort of get longer slowly over time,
before they abruptly collapse when the the event actually happens. And then all of a sudden your
timelines are now zero. If you do have more information, right, you do have like more papers,
more models being released. This is a very simple model of timelines. But yes, you're obviously going
to be updating on on, you know, papers and new developments and all this kind of thing. So that
it does kind of complicate things. Yeah. So you said in 2015 that there was like 15% for 2020.
And now in 2023, so if you were to to give you like median like 50% chance of Asia, maybe you
can give like an estimate for superintelligence as opening, I like to call it now. Yeah, when you
think there's like a 50% chance of of like, some like, superintelligent agents coming, coming along.
So I think like if you'd asked me this, like, maybe a month or two earlier, I would have said
like 2027. So like maybe like four years from now, I think that now that I've kind of seen,
you know, a little bit more information, like really recent information,
I think that is kind of like by timelines have pushed back a little bit, not very much.
You know, I think maybe like maybe a year. So maybe, maybe 2020, I don't know, maybe add like
six months to the timeline or something like that. So maybe 2028 is where I would put the
meeting right now, just because like some very recent information is kind of maybe down revised
things. But was that information? I guess like, I was expecting certain research to happen,
and it didn't happen. And that either means that that research doesn't work. Or it means that people
are not really interested in kind of making that, you know, the red direction happen. Either way,
that is kind of a positive update for timelines, because it means that, you know, in the cases
where it is kind of a dangerous research direction, people aren't really pursuing that. And in the
cases where, you know, it is a dangerous research direction, people are, you know, it just doesn't
work. And I'm wrong. And therefore, the path the AGI is a little bit more complicated than I thought.
And as a result, you know, we live in a world where we have a little bit more time.
So I'm glad to know that the dangerous approaches that we will not name these
podcasts don't work. How do you wake up in the morning thinking that there is,
you know, like a 99% chance that the two might die in the next five years. So if you multiply
with the 50% chance for five years, it's more like 45% chance.
I think I would say like, I've kind of had the time to really think about this for much longer
than most other people have. So like, yeah, when it was 2013, and, you know, things were kind of the
way they were, you know, I've kind of started getting like more and more kind of evidence
about the direction that kind of things you're going in, you know, like a lot of people are,
when they see things, they see chat GPT come out, they see GP4 come out like a couple months later,
and all of a sudden they go from like, you know, nothing to, oh my God, we're all going to die.
But for me, it's very much more been like a very gradual process where I've been like
internalizing, okay, things are more complicated than I thought. This problem is hard. I'm starting
to understand all of the rough edges. And slowly, but surely my probability that we're all going to
make it out of this has gone down. And I think I've sort of like, at least partially made peace
with that. So at this point, like, I guess it's sort of like a normal thing. And it's been like
that for a long time. Whereas I think for a lot of people that are like getting blindsided by this
technology, it very much is a, you know, a punch in the gut of like, oh, I had all these plans for
what I was going to do 30 years from now and blah, blah, blah, blah, blah, and you know, maybe
that's not going to happen. Whereas for me, it was like, okay, I'm probably going to make it to,
you know, 2030 is looking real far away right now. And it's been looking real far for a while.
So I think that's kind of a big contributor to me, like not really being that, you know,
negatively affected by it. Like there is a couple of times where I'm like,
you know, it hits really, you know, things get really real for a moment. I'm like, oh boy.
You know, we could, you know, you get the real visceral fear that we were all toast. But like,
I think I've had enough of those and those have happened in like over a sufficiently long period
of time now that it's like, oh, this is just like, yeah, we're all toast. And we're all toast
probably pretty soon, unless I'm wrong. For me, that moment was like seeing everyone talk about it
on Twitter more and more. And like, even like the US government talking about it. And I was like,
oh, it's not like some obscure thing that people talk about on the internet, people like in the
real world are talking about it for real. Like, at some point, when you when you have like some
like, uncertainty about your own models, you're like, oh, maybe I'm wrong, maybe I make mistakes.
Like, I'm probably like, you know, like in the in the wrong direction. But if a lot of people
like seem to converge the same belief, and it starts to like appear on TV, and your like grandma
calls you and be like, hey, have you heard of this thing called chat GPT is pretty good, huh?
It feels more and more real. And you can like starting to feel like an in a gut level. I think
that's like what Robert Oldman was saying, like in my podcast, like is God is catching up with
like what his model was thinking for a long time. And I guess when you were saying that like,
you were predicting this since 2013, it seems like in 2013, there was like not a lot of evidence,
right? There's like, maybe imagine that in 2012, like what was like other things were like,
pushing you in the direction of like stuff happening fast.
So I think really, like it hit me that things were happening fast around maybe 2014, 2015 in
2013, like X risk was a thing that I was aware of. And I was like, Hey, we need to be thinking about
this. But I was nowhere near as pessimistic as I am. Right. Really, like what started to update me
was like, I started paying attention to all the papers that were coming out from from really
all over the place. And like this, this sensation that progress was really fast and accelerating
has been pretty much constant and hasn't really gone away for a very long time. You know, I remember
all the way back in, you know, 2014, 2015, being like blown away at the at the rate of progress.
And that sensation hasn't really left. If anything, it's almost felt like things have
slowed down a little bit compared to some of the developments that were happening in the in the
earlier years. Do you think we're going to get like a fast takeoff with maybe like some food
scenario was recursively self improvement? Or do you think things will progress slower than this?
So I think in takeoff time, I think there's, I think there's kind of two factors, right? So like,
there's the possibility of takeoff, like a software takeoff. And then there's a hardware takeoff.
And I think like, you get kind of different dynamics and assumptions, depending on how you
like make assumptions about the interplay between those two things. So like if I'm right about the
like extreme potential for like efficient intelligence, I think that what you will get is
you will get a, you know, everything will be mostly fine. And then you will get a very fast
kind of software takeoff where you know, the minimum requirements to run an AGI very rapidly
drop over orders of magnitude, because, you know, you're able to, you know, as you go down the
number of orders of magnitude necessary to reach a given level of intelligence, the rate of iteration
sort of grows exponentially, because you obviously your experiments get, you know, get faster and
faster to run. So I do expect that like, if I'm right about the efficiency stuff that we could
see like, you know, these massive, massive models that require like 10 to the 26 or 10 to the 27
flops, you know, eventually one of these kind of comes online, because, you know, maybe we're
just doing things in a really dumb way. And then after that, we just have like this complete collapse
as the system like undergoes very fast recursive self improvement so that it's able to run itself on
on very much, much smaller hardware. And that could happen. I expect on the time scale, probably
like, you know, maybe not days, but like, I could easily see something like a month or something
like that as being a reasonable value. Things get more complicated if I'm wrong about about the
software efficiency. In the case where you actually have hardware bottlenecks, now you're no longer
in kind of the world of bits, you're you're in the world of atoms. And this is where like,
things kind of get a little bit more fun to think about it in different ways. Because now you're
no longer thinking about like, okay, how do we like lower the flops kind of intelligence? It's
more like, how do we get more flops in general? And the question of how you get more flops is one
of like manufacturing cycles, total available energy of your replication rates. And this is
where you kind of get into like the fun kind of like spectrum of like, well, okay, do we get like
humans to go and build the factories to make more of the stuff that we care about? Or like,
is this kind of a thing where, you know, you're going to get macroscopic robots building factories
that we don't really understand? Or is this kind of the case where like, you know, Yadkowski is
right about everything. And we get, you know, bacteria that replicate on the air, you on the
order of like, you know, 10s of minutes or hours or whatever. And, you know, after an initial like,
slow kind of progression to better and better sort of nanotechnology that you eventually just
get really, really fast replication rate. But like kind of before then, things are sort of
under control, and we kind of have a good understanding of how kind of things fit in. So
like, my view on takeoff is kind of a function of those two things, like how, how do things work
in the software world? And how do things work in the hardware world? And there's a lot of uncertainty
on, on both of those. I think for the software world, what you say doesn't really apply for
this current world, where you have like, one company that like trains like very large models,
maybe like a few companies, let's say like four or five training, like very large models.
And training takes like a long time, let's say weeks or months, right? And if we were to have
like, recrystallized improvement from like efficiencies that are like discovered, I would
kind of like see it as like open source people doing kind of like experiments on some kind of like
Cal's GPT or like some, some stuff online, and that they won't have access to like all the
computers like big companies that have. So if you're like, except if like big companies where
like launching a program being like, please find a new like improvements in how to train more
efficiently, and you're allowed to like interact with your own hardware or like server to do more
stuff. Well, let me flip that around and say like, given that there's, you know, I think Nvidia's
aiming for something like 400,000 to 800,000 rolling off the, the assembly line every quarter.
And there's a lot of companies that are making big orders for like, you know, 20 to 80k, each 100,000,
right? What is your view that someone isn't going to go and just say, especially given kind of the
previous circumstances where we've said, like, you know, we've initially we were talking about
boxes and and the way that that actually turned out, you know, what makes you think that you
aren't going to get, you know, these AI systems effectively being unleashed on their own source
code once they're, you know, sufficiently capable and being, you know, explicitly instructed to
make themselves as powerful as possible because all the companies ordering like 100s or thousands of
age 100s are going to try to research how to make this their stuff more efficient and and like run
agents or run stuff in the loop. Like what is the argument here? So like, I guess like the thing is
is like, you know, say you get the first AI that is as good as an AI researcher, and it takes like,
you know, you have maybe like 100 people and you know, a couple hundred people in this company
that are that are working on this thing. And you have like, you know, you train it on a cluster of
like 10,000 H100s. Initially, you have like 100 people that are that are that are working on doing
these optimizations. But as soon as you kind of get to this human level, you have like, you know,
now you have significantly more throughput, you know, you can actually kind of dive into the
math of like, okay, exactly how many, you know, what is the like equivalent of like simultaneous
people? And it depends on like assumptions about how big you make the networks and stuff like that
and how fast you can do inference and all these kinds of things. But like, I think there's going
to be a lot of room, like you're going to get a lot of you know, after as soon as you do like the
first training run, you're going to get a lot of room for a very rapid self improvement. Just
because you're going to have, you know, quote unquote, a lot of, you know, machine power that
you're going to be able to redirect at the task of kind of optimizing the code base. And yes,
there is like, you know, there's fundamental kind of limitations there. But this is this is where
like the question of like, what is the theoretical software efficiency and how much time does it
take to really get there in terms of like man hours? And yeah, I think it's like a huge assumption
that we will get some like AI capable of doing like everything and AI research can do. Or at
least like that's like further down the line, maybe it's like in more than like at least
three years, possibly like five to 10, right? And or we can discuss precise numbers, but I don't
think it's very significant. The real argument is that we're going to get something like between
0% to like 100% of like, can do like everything a human can do. And there's that it's going to be
like this like 100% or like type something that's like the AI is not able to do and like the human
will need we need to like jump in and do some kind of things to help like in the physical world or
like file some document for like Amazon AWS to like prove that you're a human or like doing
this like couch hour. Of course, at like at some point is able to like bypass like every human
thing and do everything on its own. But I'm like, my model for like why, why do I be later in this
is like, maybe the research is like kind of like hard to do. And there's like always like some small
fresh nose human labor. Yeah, so I think it really, you know, you're depending on your model of
exactly how that human labor fraction kind of this decays is going to tell you whether or not
you get like a slower or a faster or like a harder, you know, maybe not, you may be not fast and
slow, but like hard and soft take off. Because like, I think that there's a really big difference
between a human being there needed for like 1% of all the work and a human being there for,
you know, 0% of the work. And it also is a function of like, the amount of compute
that you that you have at your disposal. So like, if the AI is helping, you know, humans,
but like that 1% where the human is there is actually a bottleneck. And so you're only really
able to throw like 1% of the computational resources to actually accelerate, you know,
your researchers. And I do believe like with chat, she became stuff like that right now,
you know, we're well below that, you know, level like if you look at like, how much compute open
AI researchers are soaking up just to like, you know, aid themselves, I imagine it's a tiny fraction
of the amount of compute that they actually have available. And yeah, maybe like, like if you get
a soft transition, it'll probably be because you're sort of saturating that compute. And you're
doing AI research as fast as you can. And, you know, and more and more like, you know, when I
say that I do mean stuff like that's less non trivial than like big training ones. I mean,
like, you know, actually like improvements to the source code, like actual, you know, time being
spent, you know, writing code and making improvements and stuff like that. But yeah, I do think
that like, it's a very tiny fraction right now. But if we live in a world where like, open AI
manages to soak up all of their compute helping, you know, helping the humans and the humans
aren't really a bottleneck, but they are still necessary. Then yeah, I think you could make
an argument that you get like a more smooth take off. I guess the main argument I was trying to aim
at is unless people are like, acting like a really dangerously and and trying like very like,
like, you know, like not very moral things, like trying to build like a self improving AI or
tries to build like an AI scientist without sandboxing it, then we might not get it. And
and it's still it seems like with the super alignment posts, or I don't know, like entropic
and even deep minded, they have this concern about like alignment. And so maybe, maybe they
won't run those experiments just yet. And maybe we we maybe like the open source people will just
run those experiments like three years later when they have the compute, right? This is why I bring
up the the the large amount of patient hundreds that are being manufactured right now is that
there's more than just open AI, anthropic and deep mind now. The race has been started, you know,
like inflection is building a cluster of 22,000 h 100s, you know, there's rumors of like, you know,
some people doing a cluster of like 80,000 h 100s, or something like maybe not a cluster,
but like an order that big, you know, you have the major cloud providers, you know, Microsoft is
not, you know, sitting around idly, either. And they've kind of explicitly said in their like
sparks of a GI paper that they're kind of aiming for it for agency and self improvement and that
kind of thing. And then you have like, you know, you have other actors, you have state level actors,
you have, you know, you all the the military guys now kind of getting into it and saying like,
Hey, maybe we should start racing with China. And then you have like, you know, the Aurora
supercomputer guy is saying like, Hey, we're going to work with Intel, we're going to make a trillion
parameter model, and we're going to, we're going to use our big supercomputer with like,
you know, 60,000 Intel GPUs to make that happen. I really do think that like four years from now,
you're going to have a very large amount of actors that are going to have a rather non trivial
amount of compute. And, you know, open AI and deep mind and anthropic, you know, might be in the
lead. And they might be able to like, you know, be smart about not just like, opening up the throttle
and letting it rip. You know, they probably have the internal discipline to not let that happen.
But, you know, if they have the discipline to not let that happen, I do expect that somebody
else with a very large amount of GPUs is going to step in and pull the record, basically.
I think that makes me much doomier than I was before our characterization.
And short term is my timeline by a bit. So yeah, you're kind of like doing more like open source
research or open source software. I don't know how much you're you're doing open source things.
But some people are like, I've been asking me like, if you had like any thoughts of like,
an open source versus like, close source, open resources, like close research for
just kind of things. Yeah, sure. So I guess like there's like, there's mundane considerations
and there's apocalyptic considerations. You know, that's kind of one way to put it. And I kind of
have different opinions on both of them. So like, you know, when it comes to kind of the mundane
considerations, you know, you have pros and cons of going for like as much transparency and open
sourceness as possible, right? Like, you know, on one hand, you have, you know, once you have a
fully transparent model, and you kind of know what, you know, what it is, how it was trained,
what went into it. You know, that's that's really good for auditability. That's really good for
you know, just kind of understanding like why, you know, obviously, we were we're terrible at
interpreting transformers right now. But like, you know, presumably, that'll get a little bit
better over time. It's just generally useful to have some level of auditability into like how the
how the model was trained. At the same time, you have like other kind of again, mundane kind of
concerns, right? Like, you know, how do you how do you handle personally, or personal and private
information? You have an open source language model that is, you know, during its training,
kind of hoovered up some information about a, you know, certain individuals, and effectively
sort of like synthesized a lot of that. As part of its training, you know, how do you handle that?
So yeah, that's kind of like the mundane stuff. But I think probably the more interesting thing
that you want to talk about is is the apocalyptic considerations. And there it's a little bit more
yeah, yeah. And there it's a little bit more. It's a little bit more tricky. I think that probably
what you want is really you want sort of sort of kind of a buy, maybe not like a buy modal thing,
but you want a policy where there's a ceiling on what you're willing to, you know, the level of
which you're you're willing to kind of open source, you know, small models, having them open
source, you know, it makes it possible for, you know, companies and individuals and everybody
else to kind of study them. But really, like once you get to the really dangerous systems,
the systems that, you know, that can, you know, end the world. And even like, you know, you do want
to have like a buffer margin, essentially, between the systems that end the world and the systems that
don't, you really don't want those to be open. Because like, if you have something that can
destroy the world and you distribute that to everybody, well, you know, that's just that ends
the world. Right. It's not that complicated. So share the models that can help us get a better
understanding of how neural networks work or how customers work. But whenever you get to like,
very dangerous models, maybe try to like not share it on a hji.exe file.
Well, I would even go further than that and say like, those models should not exist. Right. Like,
if you're making a system where if it leaked out, you know, it would be a complete disaster,
you shouldn't have that and you shouldn't have that for two reasons. One is that your own internal
security is only going to be so good. And there will come a time where there will be a security
incident and those weights will get leaked and you will lose control of the situation.
The second is that as soon as something like that exists, as soon as something like GPT-4
exists or GPT-3 exists, and it's behind a closed door, people do not trust each other enough to
be able to like have one single party trusted with control over a, you know, a very powerful AI
system. Right. Because you can be cut off from, you can be cut off from that capability. There's
also like just like a lot of people will ask like, okay, whose values get represented in the AI?
They will go and say, well, okay, you're, you know, you're doing this with it. I don't trust you.
I want my own that has, you know, its own values and let's me do this, this and that. And just the
fact that you have it and just the fact that it exists is going to motivate a whole bunch of other
actors in the space to try and do a replication effort. And that's effectively what you're seeing,
or at least in my opinion, that's what we're seeing a lot of right now.
So yeah, those kind of like concerns we're raising about open source models,
is it related to like things like Llama where people like accelerating on like making those
models like better, more efficient, bigger, or are we talking about something else?
I think that's definitely part of it. It's more than just, I would say like Llama, it's just the
general like, you know, every other country is kind of noticing these things. You know, the U.S.
government is noticing it. Everybody sees that someone has a capability that they don't. And
they will make efforts to try and close that capability gap. You know, whether or not they
succeed is another thing, whether or not they have the budget to be able to do it. The thing
though is that the motivation is now there. And that itself is concerning, at least in my opinion.
Yeah, I agree it's a concerning that like governments are paying more attention to it.
And yeah, definitely like if you're if you're paying attention to it and this podcast for doing
the right thing, people might be interested in you, like, what's your background, like how you
got into this like whole like, like exit service from AI thing or like deep learning thing, like
where did Curtis learn to like do like a lot of my work or like deep learning research?
I guess on my end, my background kind of is a very informal background, but it goes back
a long ways. So like, the first neural network that I wrote or kind of machine learning I wrote
algorithm I wrote was like back in, I think like 11th grade, like almost a decade ago.
So like, I think I was really kind of introduced to the Kowsky's writings, probably around like
ninth or 10th grade or something like that. So probably 11 or 12 years ago, something like that
2011, 2012. I do actually remember giving a talk about like effective altruism and existential
risk back in like 2013 at like a small like gathering in a place in Canada called Saskatoon
in Saskatchewan, which is the middle of nowhere. Is this where you're from, Canada?
Yeah, I'm from Canada, originally Edmonton. But yeah, essentially, yeah, I have been kind of
following this for a very long time. And then since then, I've been kind of, you know, reading
papers and, you know, playing around with just small neural networks, doing little experiments
here and there. I did go on to kind of do some some internships in bioinformatics. But you know,
the extent of like my formal training, it is not actually that large. It's all sort of been self
taught. Right. And I guess that's the best way to learn how to do this thing. And everything is
so recent, right? Did you did you first like learn how to like train big models while having
you out with like those GPT Neo GPT Neo X or like, like power research projects at the
Luther AI or did you arrive later? So I actually didn't have much of a hand in the, the Neo and
Neo X models, but I definitely did pick up a lot about distributed training just from hanging out
in the Luther AI discord. I learned, you know, quite a lot about everything from, you know,
how the low level kind of the GPU operates and the importance of the memory band of memory bandwidth
and, you know, vectorization, all that all the way up to kind of, you know, the various
parallelism strategies that we use in large scale modeling or large scale training today.
Yeah. And now you're like working with a Luther AI on alignment projects, right? Like head of
alignment for like all the different products that are going on. So maybe can you like give like a
an overview of like the different projects that are currently being being done there?
Yeah. So I can talk a little bit about a few of them. So like one of them that I'm kind of excited
about is a collaboration that we're doing with the AI safety initiative at Georgia Tech. And we've
kind of got two sort of projects that are going on over there. So like the first one is looking at
language models kind of as Markov chains. And I don't know how I guess like how familiar you are
with Markov chains, but basically the idea is that you can sort of view a language model with a
limited context window as sort of having a state where the state is the entire context of that
of that language model. And then you can go and ask yourself, okay, what is the transition
distribution from like one state? So one context to the next state? And you can look at it and say,
well, okay, we add one, you know, we add one token by sampling. And so the distribution from the
net one state to the next is is the, you know, log probabilities of the language model. And then
after that, we pop off the last value from from the state, like the last token that falls out of
the context window. And when you take that kind of view of what the language model is doing,
what you can do is you can go and say, okay, what are you can ask yourself questions like, you know,
what is the stationary distribution of this chain? So like, if you run the language model
auto aggressively continuously generating new tokens, and then, you know, sampling and producing,
what is sort of the distribution of text that you're going to see kind of in the long run?
Another question that you can ask yourself is like, what is the quote unquote reverse Markov chain
or reverse process? So if you wanted to reverse the dynamics in time such that you append a token
and then you pop off the, you know, append a token back to kind of pop off the back token,
what kind of distribution do you get? And, you know, it's kind of like, you know, the first
question you ask yourself is like, what does this have to do with alignment? Right. And I guess,
like from my perspective, it's mainly that it is a largely unexplored domain and seems highly
relevant to just understanding language models as we currently use them. The reason for that is
because like, when we train language models, we really do use sort of like the next token
prediction objective as the base. Like obviously, there's RLHF that comes after that. There's,
you know, supervised tuning that we do, but really a lot of the meat of the training is
all focused around this sort of next next token prediction objective. And, you know, there's,
there's a little bit of reason to believe that like, when you kind of run a language model for
an extended period of time, or really any sort of model that has been trained on this kind of
next token prediction objective, you will kind of get a little bit of error that piles up, you
know, eventually, like the model will mis-predict something. And then now it's input distributions
a little bit off from what it was kind of initially trained on. And then, you know,
things kind of start to snowball from there. Another kind of example of like a good of like
where this sort of failure mode is already sort of manifesting itself is, or at least like this,
you know, this is me speculating. But being when it came out, initially sort of, you could keep
talking with it going back and forth as much as you wanted. And it pretty quickly kind of started
going off the rails, right? But the solution to that was, okay, let's go on, you know, limit the
the length of the conversations that you can have with Bing. And, you know, obviously, I can't
say exactly why Microsoft did that. But I do suspect that at least part of it was because of the
model kind of starting to go off the rails. So I guess like that would be sort of like an early
manifestation of the kind of worries that kind of I have in mind when it comes to like this.
Are you trying to like see what is the like stationary distribution of that Markov saying
of like the where does the model go if it like starts to like output like very
very long paragraphs of text. And at some point, it's really like really just like
bad behavior. Are you trying to like get to this like bad behavior at the end?
So not really, we're just trying to see like, we're not even going that far. We're just trying to
say like, how does, you know, how like, where does it go at all? Right. And then, you know,
questions like, what is sort of like the mutual information? Like how does, you know,
does it it starts with one topic? How does that kind of change over time? Do you get like a
distribution shift? And sort of, you know, how long can you stay on the same?
Yes, you know, same kind of track, so to speak. So just just these really basic kind of low level
questions is where we're at. I think recently the the focus has actually shifted from that.
And kind of asking on the on the reverse side now, like if you start with like a bad, you know,
kind of a bad output or a bad state somewhere that you don't want to be, can you sort of like
work backwards to say like, what was the chain of events that led you to that bad state according
to the probability distribution specified by the language model? So that is sort of like,
you know, it's a it's a very sort of exploratory kind of direction. There's if you have to ask me
like, yo, okay, how is this specifically going to solve like, you know, one of the alignment
problems? I'm going to have to say like, you know, it's it's not it that that is not where
where we're at. I do definitely think that like, having more lenses and kind of perspectives to
be able to understand the systems is going to be useful in general. And that maybe we're going to
get to like another part later down the line where we say, oh, well, this lens is useful, or this,
you know, is a precursor to a training technique that would, you know, mitigate some failure motor
and other. Yeah, I definitely agree that like, it's it's good to like do like specific experiments
to like inform like, bigger, bigger theory on alignment and or like other projects that like
could like help like focus on like specific things. I think it makes sense to like zoom out a little
bit. And maybe for people like, we don't really know what's the like, main goal of a third AI of
a third AI element team, like is because I think some people like my thing of like, like academia
as having like professors right to like publish papers, or like doing research for like a lab
or something. And in some sense, now I think there is like a nonprofit. So maybe like some people
like donate and, and then it does like research in some sense. So yeah, maybe like a better like
overview or like introduction to like, how is it like to like research at a third AI or
or an alignment? Like, what does it mean? Like, what's the goal? And like, what is like to like
open source stuff with like, other people that I don't like me like full time employees or
Yeah, so I guess I can speak like just generally about a Luther because like, you know, okay,
we have like alignment and we have interpretability and stuff like that. Really, the lines do get
blurred. Really, like, the Luther tries to do all like stuff that is basically we you know,
think we think the stuff that we think is sort of like net value. Right. And that can be that can
be kind of anything. Right. If we think that there's some interpretability research or some paper
that that makes sense to publish, we'll do that. If we think that there's like an alignment problem
that it makes sense to kind of tackle that other people are not really, you know, spending a lot
of time on, we'll focus our resources there. And it also kind of extends to kind of more
like mundane things, right? Like, one thing that you know, I'm less familiar with myself, but
is also kind of part of a Luther is is multilingual work. So like a lot of current language
model development is very much focused on English. That is sort of the the dominant kind of
the dominant language for these language models. And there's also like, you know, there's there's
a whole bunch of other languages that that don't get the same level of attention.
And so at least partially, a Luther does see like a really valuable opportunity there of like,
hey, this is the kind of thing that really isn't going to like, you know, increase our knowledge
of how to build, you know, AGI and short and timelines. But it's definitely something that
is going to kind of bring mundane and kind of, you know, valuable utility to communities that
otherwise wouldn't have. Right. Right. So that is like, like the general vibe there is like,
we're going to make sure that we don't make the situation worse. And if we see something that
we think is is is a good idea to kind of pursue and to spend our time and resources on, we're
going to go for it, whatever that might be. From what I know about the like multilingual
projects, there's like a guy like leading a team, like every week or like in the past like year or
so, like doing like Korean language modeling or something. And correct me if I'm wrong, I might
be wrong. And I guess like, he, he like started by doing the thing by himself and like having people
come and then they like started the project. But it's like, it's like very informal at the start.
And now it's like maybe like more, more organized. If someone like listening to this or watching
this wants to, you know, like, work on alignment for the Luther AI, or like just like contribute to
like open source. Is there like any like, best ways to like reach out to you or, or like get
started? Like, do you have like shovel ready stuff to do is like Markov change things or is there
like other products that like, maybe like easier to, to get started with? Yeah, we do have an email
you can you can send an email us a contact at Luther AI or just, you know, Curtis, Luther AI,
if you're interested in talking to me specifically. But generally the primary method that everybody
gets involved with is, is through Discord. And like for this like particular like Markov
chain project, how many people are working on it? And is there like a need for like someone else to,
to do something? I think right now we're pretty full on that project. I think there's like five
can't remember exactly the exact number, but I think it's like five people. And they're kind of,
you know, they're, they're sort of working at it. Again, this is, this is from a collaboration
with the AI safety initiative at Georgia Tech. So there are at least, you know, partially kind of
handling the management and stuff like that, at least that project. But just in general,
like the way Luther, especially the Discord itself is actually structured is that we have
like a whole bunch of project channels. And so like what I would kind of recommend if someone
wants to kind of get involved is like, you know, just take a look at the, the project channels and
see kind of what people are talking about, see like which research leads are sort of active in
there. And then either like make a general post in the channel itself, or go and DM whoever like the,
the active researchers are for the project, if you want to get involved. And even just asking
for, for the, like the specific project is probably the best way to kind of do it.
In terms of leading, you're the head of the alignment, mind test projects, right?
Yes, that's correct.
What is the alignment, mind test projects and why is it useful?
Yeah, so the, the high level kind of like long term goal, there's like a lot of kind of like
intermediate sort of directions that we find is kind of interesting. But like the main thing that
I want to study is, I guess you could say it's like embedded agency failures in a toy sandbox.
So like, what do I, what do I kind of mean by that? If you look at like, if you just punch in
reinforcement learning into Google images, you will get this diagram, the same diagram,
a bunch of different variations on the same diagram has like the agent on one side and that has
the environment on the other. And there's this like really solid boundary between the agent
and the environment. And you have actions going from the agent to the environment,
and you have observations and rewards going back to the agent, right? Unfortunately, this is not
how the real world works. Agents in the real world are embedded in that world, right?
And a consequence of that is that you, like architectures and designs that would work fine
in the, you know, the standard sort of reinforcement learning setting are not necessarily going to
carry over to the embedded setting. And there's certain kind of failures that we expect to be
able to see and demonstrate when, you know, when that happens. Like the, the really classic one
that, you know, is less relevant because of the current techniques that we have, you know,
RLHF and stuff like that. But it's still kind of something that, you know, is worth kind of
keeping in mind as an example is wire heading, essentially. And this is essentially what happens
when a neural network or an agent is trained with reinforcement learning and the reward signal.
And it gets sort of smart enough to realize that the thing that is, it is getting reward,
like it's not being rewarded for, you know, for whatever it is that we are giving it reward for.
We, it's getting reward because the little reward circuit is being activated, right? Or like, you
know, you know, you could imagine like a setup where like a human has a robot and, you know,
the robot is kind of going around and, you know, when the robot does stuff that we like,
we press a button and we give it some reward, right? Well, eventually the robot is going to
get smart enough to be like, hey, wait, what happens if I press the button, right? That gets rid
of the human and starts pressing the button itself so it gets into that reward, right? So that would
be kind of an example of like the sort of an embedded failure. That is a very simple example
of the kind of failure that we're looking for. There's significantly more kind of complicated
failures that you can get with like really any sort of value updating scheme.
And that is sort of the long-term goal of the MindTest project is that we want to be able to
sort of sandbox these scenarios with an AI that is like ideally like sort of as weak as we can make
it to still sort of isolate the sort of embedded agency failure that we're trying to target
so that we can then like understand the failure so we can, you know, sort of get some idea of like,
okay, how does like the value function for an RL agent involves when it starts to notice that
there's this kind of failure mode happening? And then like once we've kind of isolated that failure,
then we can start to ask ourselves like, what kind of techniques can we develop
to mitigate or eliminate that kind of failure?
So having some kind of like RL agent in an environment like Minecraft being embedded in
the world and having a failure mode where he starts hacking his reward, assuming his reward
was like inside of Minecraft and... Well, that's exactly it. Yeah. So the MindTester project,
one of the motivations for using MindTest instead of Minecraft or existing models
is because like we wanted to have as much flexibility in terms of like, you know, programming
things or implementing different, you know, implementing stuff in the environment. So you
could actually see like, okay, let's put a button in the environment and let's have a player control
the button and let's have the agent also be in that environment and let's have the, you know,
let's actually just do the experiment and see like, you know, under what circumstances does the
agent kind of wise up and say like, you know, hey, I can maybe just get rid of the other player and
I can press the button myself. You know, this is kind of the type of failure that we want to be
able to investigate, that we want to be able to replicate, and that we want to be able to mitigate
in the long run. Obviously, the project is not quite there yet, and sort of like we're going on,
you know, kind of intermediate tangents. So I believe like in the first blog post, what we mainly
outlined was, you know, we trained a basic PPO policy to kind of punch trees. And we said, okay,
well, this is a really simple policy. Can we do some basic interpretability on that?
What does the, you know, what does interpretability on an RL policy look like? What does it feel like?
Do we kind of bump into problems that we don't really bump into in kind of other,
you know, other kind of environments or like other sort of training settings? And there's a
little bit of that that we've already sort of run into and that we're kind of starting to think about
like, what is the, what is the best way to kind of get around these, these sort of limitations?
Like a good example of this would be like, you incentivize the model to go and punch trees,
right, or punch wood. And one thing that you would expect as a consequence of that is that
the model would go and punch, you know, punch all of the logs in a tree, and that would be that.
But what you actually end up seeing is a situation where the model kind of learns
a strategy where it doesn't punch out the bottom log. And instead it, you know, punches out the
log above it and then sort of hops on and then is able to access kind of more logs at the top of
the tree. So like, if you're kind of a reward designer and you, you know, maybe want to go on
like get rid of all the entire tree, that is sort of like an unexpected or unintended kind of consequence.
So one of the things that like I am currently thinking about is what is a good way to sort of
detect these sort of unintended consequences of the, you know, the rewards or the feedback that
we give. And, you know, how does that translate? Like right now, obviously, we, we did basically
like reward shaping and, and, you know, manually specify reward function. But like,
what is the analog of that with something like RLHF? So these are all kind of like questions
that are sort of hovering around in my mind. And so in your project, you're, you have this
thing with the log when it's like an expected behavior. What's the, what's the next step?
What's the next thing you're trying to observe? So right now we're kind of, you know, putting
aside the PPO policy for a little bit. And we're going to try and focus on like model-based RL.
And, you know, kind of the next step to enabling that is, is just training models,
generative models in mind tests and seeing, you know, what can we understand about, you know,
those generative models? Because like, you know, there's, there's, you know, language model
interpretability, right? There's vision model interpretability. There's, you know, RL policy
interpretability. And each of them kind of has like their own kind of unique challenges and unique
strategies and stuff like that for, for getting places. And I do expect that like, you know,
a video model or like a model that actually also has like action outputs is going to like represent,
you know, I expect to see things like causality. I expect to see things like tracking state,
like if you see a tree and then you turn to the left and then you turn back, presumably there's
going to be some machinery inside that generative model that is tracking the state of like whether
that tree is there. So questions like, okay, how do you, how do you look at like state tracking
inside of, you know, inside of a video model or a video model augmented with, with actions?
How does, how do all these kind of things interact with each other? That is, that is like the next
step that we're taking with the, the, the mind test. So is the basic idea to like see if the,
like the estimates from the agent of like how much like reward he's going to get in the future,
like gets like higher when he sees like a log or like when you see something like,
when he detects something in his, in the video, like saying like how the value function he has,
like changes over time with like different inputs.
So that's, that's like one direction. I would say like it's, it's even a little bit like,
like that would be kind of later down the line where we're, we're not even really like,
we're not really going towards the full model based on that. Right. The way that we're kind of
going to approach this is we're going to actually just have users, you know, play the game. And then
after that, we're going to take that data along with like, you know, some pre-training data and
just build some basic models of the environment and just saying like, okay, forget about value
functions, forget about policies. Let's just focus on like this one sub component of the,
of the agent and what can we kind of get out of that.
On the website, I think it says that you're like trying to build some gym,
gym like environments. Is it done or is it still in the weeds?
It's out of date. We, we have the environment, you know, obviously we, we needed the environment
so we were able to actually train it. The next kind of step, and it's not that big of a change,
we'll probably get it done pretty soon, is that we're planning on collaborating with the Ferama
Foundation. And they maintain an updated version of the, the gym environment, along with like,
I believe like some multi-agent APIs. So kind of another kind of, you know, place where there's
work to do and, and is also on the road back for the project is, is support for sort of these more
modern APIs, essentially, and just swapping out like the old open AI gym API. But, you know,
we've had an environment working for, for quite a while now. And we've used it to train some agents
and defense of interpretability on, on the policies that are learned. That's been, you know,
the website is a little bit out of thing. If I'm like a deep learning or a real engineer,
like listening to this, and I'm kind of like interested, what kind of like compute do you
have for this? Or do you just like, like, how big is the model? Is it like just like a
standard size model for like this kind of like video processing and like to,
you know, have people on top or what kind of like model are we talking about?
It's tiny. It's a, it's a very tiny model, the one that we trained. It's like, I actually,
I think we outlined it in the blog post, it's like, it's like three convolutional layers,
a fully connected layer, and then like an action and critic cat, it's a, it's a very tiny neural
network. Scaling things up, I do imagine is going to make the, the interpretability significantly
more, significantly more challenging. I think, I think like one thing, like another kind of thing
that came out of it is like, you can have a functional policy, but it is, it's relatively
difficult to like, especially as you, as you kind of scale things up, you get like more noise in
the system. Like one thing that we notice is like the, the learned policy is, is really not that
symmetric, but it works anyways. So like, you know, you're thinking, well, you know, the environment
is mostly symmetric, you know, maybe there's a little bit of a change with like, you know, the
textures being oriented one way instead of the other, but like, it's not enough to explain what's
going on. And I think it's really, it's just like an initialization noise and RL noise and all
these kind of things that are sort of conspiring. And the net effect of that is that you can get a
very noisy, very not great policy that doesn't have very clean internal representations, but it
will still get the job done. They will still kind of execute, you know, and execute a policy that
actually does work in the environment. I was going to make the joke that maybe in the Minecraft
environments, because the character has like a sword or like a tool on the right, maybe that's
making like the image not symmetric. Well, you see, you can test that by like mirroring the,
you know, I mean, that's actually, you know, that could, that could actually explain it because
obviously like during actual training, you know, that's not what's going on. So that, you know,
that could actually be what we've seen. But yeah, that is just generally we just haven't seen the
symmetry. So that could be that could be what's going on. So yeah, another thing that like you
mentioned in the blog post is that the main goal is to understand corrigibility. So how to make
models more likely to be corrected by by humans. Can you say more about like what
corrigibility is and why you're interested in these projects will help with that?
Yeah, so corrigibility is the ability to, well, I mean, as the name implies to be able to correct
agents and models after they've been deployed. I guess even during training, depending on like
what assumptions you make, but but really just being able to say, hey, no, don't do that.
You know, I want you to do like y instead of x, right. And like, you know, in the in the RL case,
you know, you get a really serious corrigibility failure where the agent, you know, in the button
example, going back to that, the agent going in and just like, you know, getting rid of the human
and then just pressing its own button, that's like a failure, but it's not really a corrigibility
failure. A corrigibility failure is is one where like, the agent wants like, say you instructed
to go cut down some trees, the agent wants to go and cut down some trees. And maybe you change
your mind and you say like, well, I don't really want you to be cutting down trees anymore. I
want you to be doing something right. A corrigible agent will let you go and modify its source code
or like, you know, give it whatever, you know, use whatever reward mechanisms you have,
or whatever mechanism really you have to edit the model to change that behavior so that it stops
doing the thing it's trying to do or it's originally trying to do and kind of moves on to doing
something else. And this is a little bit like unnatural because, well, if you want, if you
make an agent that wants to cut down trees, it's going to naturally reason, well, if the human
changes his mind, and doesn't want me to cut down trees anymore, well, maybe it's even smarter than
you and it's sort of like, sees this happening before you even, you know, are aware that you've
changed your mind. Maybe it's going to go and like throw some obstacles, or maybe it's going to deceive
you, or maybe it's going to do like one of a many, you know, a bunch of different things so that it
can keep doing its street cutting. And you're sort of, you know, cut out of the loop, essentially,
of deciding what it does next. And so a corrigible agent is one that has that, you know, at least
at least in my kind of understanding is one that has that property of, you know, it will go and
if you change your mind, it will go along with your, your change of mind.
In some sense, like the agent is like, trying to do what is best for you, and like tries to like
check if you, if you want to turn it off or not, and he's not trying to like prevent you from saying
what's going on. And I think it's like pretty, pretty hard to get, I'm not sure if there's like
any examples of agents like preventing like humans from saying what's going on, like in real life,
or if it's almost like toy experiments right now. So that's the thing, right, is I believe at the
moment, most of the demonstrations are kind of like analysis that you have, are either like,
you know, very simple decision problems, like the off switch game, where there's like theoretical
analysis of, you know, if the agent believes X, or you know, you make certain assumptions about
the agent's beliefs of the player, or you, you know, you specify a certain training setup,
you know, then you can get a theoretical analysis of like, okay, under this assumption,
you get this behavior, under that assumption, you get that behavior. And then like, you know,
the level above that is like grid worlds. So I'm aware of like off switch grid worlds where like
a user has an off switch button that turns the agent off and prevents it from continuing to do
what was previously. But you know, anything more sophisticated than that, I am not aware of,
of anything like that really existing. I think just like people that are like watching this might
not like not know exactly what is the off switch game. Can you just like explain what is this game?
Yeah, so I mean, like at a high level, the off switch game is really like a simplification of
kind of the more complicated example that I put where it's like the agent is doing a thing,
and there's an off switch. And when you decide that you don't want the agent to be doing the thing,
you press the off switch, and the agent won't try to interfere and prevent you from from pressing
the off switch might not be exactly that my recollection of the details of the off switch
game are a little bit fuzzy. I'm more just aware of like, this is like the simplest example of
corageability that I'm aware. I believe it's Dylan Hatfield Manel is probably the words you
want to Google with regards to the off switch game. I think there's a paper on it from like 2016
or 24 years. And I think like they model it as like some game theory experiment. And maybe there's
like some some way of like, like knowing exactly what's like optimal values you give to your to
your agent so that they accept to like be credible. Yeah, how excited are you about like,
you know, like finding this discredibility inside of a Minecraft environment or like mine test.
So I think we're got I think we're going to be able to fairly easily demonstrate the failure
modes that we're looking for. Fixing those failure modes is kind of another story entirely. And
especially being into doing it do a reliably seems like it's going to be quite a challenge.
I have like some vague intuitions about ideas that I can't really articulate yet about how to kind of
go about actually tackling the problem. But nothing that I would say is is super concrete. And I
think that it's it's probably better to wait until we have like a solid demonstration before we
really start thinking or like, you know, deploying things because what's going to happen is we want
to we're going to get an implementation and we're going to get some data. And like whatever, you
know, some at least some of the assumptions that I'm kind of making right now are going to be proven
wrong. So so yeah, that's that's kind of like where we're headed right now. I wonder like how
equal as RG it is to like post something about race. I think the cat's out of the bag on that one.
The people who have the people who have the 8800 to use order, they probably know it was going
off. Yeah, again, the people who are making these orders are aware of the other people also making
these orders, right? This is nothing the relevant actors are all aware of what is going on. In fact,
that is part of like what makes it so, you know, terrible is that they are very, you know, the people
with the most, you know, impact or chance to really like do these kind of things are the most
acutely aware and the most like ready to race, right? Because if they weren't racing, they wouldn't
be trying to raise a billion dollars to build these giant supercomputers so that they can build AGI,
right? So yeah, that is that is yeah. It was a pleasure to have you. Do you have like any last
message for for the audience for other through AI, the world, the machine learning engineers,
the people ordering GPUs aligned? Come hang out at our discord. Come hang out on off topic.
Say hi to the people there. Yeah.
Come hang out on the other three AI. Thanks. It was a pleasure to have you have a wonderful day.

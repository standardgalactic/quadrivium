Imagine you're my professor.
Maybe you actually were my professor,
in which case you may already be sweating before I say anymore.
The subject matter is neural networks.
You draw an illustration on the board with a node's inputs and its outputs via transfer function.
You inform us of this mathematical fact that the transfer function cannot be linear,
or the whole model would reduce to a linear function.
I immediately raise my hand.
The speed with which I raise it and the not very subtle forward pose
suggests that I want to pluck an abstract idea from the whiteboard and pervert it.
You know this look, and you are reluctant to call on me.
But no other students are raising their hands.
You have no choice.
Tom.
It's more like a statement than a question.
It includes the tone of spoken punctuation that, if it could,
ends the entire conversation before it begins.
I go on and on about some technicality.
That due to approximate math on the computer, this mathematical fact won't be true.
You say, okay, technically that's right, but for all practical purposes it doesn't matter.
And I say, well, what about impractical purposes?
And you, in a moment of weakness, vigorously strangle me.
And that's how I died.
Murdered in cold blood.
That was about 20 years ago, but the world will not let us stop thinking about neural networks.
We're really just pressing all the gas pedals at once on this one,
heading towards a utopia or a dystopia.
Some kind of topia, for sure. We're getting there real fast.
So this question has been on my mind for some time.
And just to be clear, the professor is right.
I might be technically correct here, but it doesn't matter for practical purposes.
But I like to work at the intersection of theory and impractice.
And so by doing a lot of work, we can make it matter.
And then I'll be even more right, both theoretically right,
and it will only matter for most practical purposes.
So in this video, in its lengthy accompanying technical report,
I have an exhaustive exploration of what you can get away with.
And I'll see how we can absolutely use linear transfer functions in neural networks
and all sorts of other things where they shouldn't be enough.
I'm Tom Seven, and this is Impractical Engineering.
Okay, let's repeat the professor's lesson so we can understand the nature of the dispute.
If you feel like you already know everything about neural networks,
this section is safely skippable, but so is the whole video.
So fundamentally, a neural network takes in inputs, which are a bunch of numbers,
and transforms those numbers, and then outputs some other numbers.
In this drawing, I have three inputs and one output.
So every one of these circles is going to be filled in with some number as we run the network.
So call the inputs x, y, z, and let's just look at how r is computed.
That's that middle one.
We start with a weighted sum of x, y, and z.
So we take all the inputs, we multiply each one by some weight, and add those together.
And these weights are determined when we train the network.
At this point, they're just constants.
When we're running the network, they're just constants.
We also learn a bias parameter, which becomes a constant, and that just gets added in as well.
The important part for today is this transfer function, tf.
This gets applied to the weighted sum, and it transforms it, in this case, with a sigmoid.
And the intuition here is somehow that this node r, this neuron r,
fires with some probability.
That depends on its connection with these other neurons.
But because it's a probability, it ranges from zero to one
instead of, like, negative infinity to infinity.
And so the more the input ones fire, the more likely this one is to fire.
That was the classic idea, anyway.
These days, pretty much everyone uses the rectified linear transfer function.
It's super simple to implement, and for various reasons, it actually works better,
especially for the internal layers.
And actually, all sorts of functions will work here.
It needs to be differentiable because of the way we train these things.
But the only other apparently necessary quality is that the function be nonlinear.
At least so says the professor.
Now, the reason for this is mathematically nice.
Let's look at the formula for r again.
And let's say the transfer function is linear, so it's like mx plus b2.
Then you can multiply m by all these terms, and you get another linear function.
So r is a linear function of the inputs, and then so is q, and then so is s.
And then o is a linear function of q, r, and s.
And what this would mean is that the output would just equal some linear function of the inputs.
And all of this complexity of the neural network would just simplify away.
We wouldn't need any of the hidden layers.
We would just have a function of the input layer.
There are lots of functions such as XOR that can't be approximated by linear functions like this.
We definitely want our neural networks to be able to model things that are complicated, like XOR or human thought.
So the story goes.
So that would be true if we were using real math.
On a computer, we're going to use IEEE floating point, which isn't associative or distributive.
So if we simplify the whole network, we won't actually get the same result.
So my goal today will be to create a transfer function that, despite being mathematically linear, will not be computationally linear.
And thus, I'll be able to use it to train models that have interesting behavior.
Now, my smart math friend Jason, who probably makes the professor sweat even more,
reminds me that this is actually an affine function because I add something at the end.
That's fine. I'm going to call it linear.
He refers to this as high school linear in a pejorative way, and that's fine.
I'm comfortable with that. In a lot of ways, I'm mentally still in high school.
So this means two operations, addition and multiplication by constants or scaling.
Them's the rules.
Or equivalently, would it simplify mathematically to a polynomial of at most degree one?
So 3x plus 2x all times five, that would simplify.
But 2x times x would yield 2x squared, and that's a degree two polynomial.
So that's disallowed. Okay?
Floating point comes in a number of different spice levels, corresponding to how many bits you're using to represent it.
So you may be familiar with double and float. Those are 64 and 32 bits.
Half precision is 16 bits, and it gets even lower.
It's usually used because then you need half as much memory to store your numbers.
That'll be good for us. We'll be happy to save the memory, but the real reason to use half precision is that it is less precise.
And imprecision is going to be a desirable quality in this work.
Being only 16 bit, there are 65,000 different values that we could represent.
So it's clearly not all of the numbers.
This is an exponential format, so the main thing to remember about floating point precision is that there's more numbers near zero than elsewhere.
So when you get to the largest finite numbers, like 65,504, only multiples of 32 are even representable.
Between 2048 and 4096, only even numbers are there.
Below that, only integers.
And actually, most of the action happens near zero, where you get a lot of fractions.
Now this stuff about comparing against epsilon is okay, but it's kind of naive.
Like for one thing, what is epsilon supposed to be?
If you're working with really small numbers, you can use a really small epsilon.
But if you're working with larger numbers, you might need to use an epsilon of up to 32, or maybe half that, for half precision.
Actually, a while back, I wrote a paper called What if Anything is Epsilon?
Where I looked at what programmers picked in practice for their value of epsilon by going through a whole bunch of code on GitHub.
I enjoyed laughing at their bugs, like minus 1e10, which is negative 10 billion.
They meant 1e-10, 1 over 10 billion.
And I like to compare these by language.
For example, I found that JavaScript programmers were the most tolerant of error, which makes sense.
They're pretty sloppy.
One programmer picked 6 million on purpose, that wasn't a typo.
Well, a lot of stuff will be equal if you use a really large epsilon.
But anyway, the error you get is not like random.
These have a much more useful definition.
If I take two numbers and add them using IEEE floating point, the answer is defined to be the real math answer, x plus y,
but rounded to the nearest representable floating point number.
So if the result is high, I might need to round a lot.
If the result is small, I might need to round a small amount.
I also might not need to round at all.
Like 2 plus 2 is literally equal to 4 exactly in floating point, like you'd want.
And we're going to use that kind of thing later.
So the most important thing to remember about floating point for this project is,
rounding error depends on how far you are out on the number line.
Large error for large numbers, small error for small numbers.
But it also depends on the specific value.
Not everything needs to be rounded.
Surrounding is a complicated function, and we're going to abuse that complexity in order to get behavior that we like.
Let's look at how we can start abusing the imprecision of floating point numbers.
We just have addition and multiplication by constants, so we're going to try both of those.
Here's what happens if I add 128 to the input and then subtract 128 back out.
Mathematically, of course, this is just the identity function. I get back the input.
But because there aren't that many values representable near 128,
we can only get back eight different values between 0 and 1.
Note that the zigzag is finer in the negative region than in the positive region.
If we look at the actual numbers that are representable near 128,
we see that we have eighths above 128, but sixteenths below 128.
So we get more precision in the negative region.
Okay, that's plus. It's definitely not a line, but it's basically just a line.
Also, this thing has a terrible derivative. It just has all these flat segments,
so the derivative there is 0 and it's undefined at the discontinuities.
So it's going to actually be pretty hard to use as a transfer function,
but we're going to try it out anyway.
Now multiplication looks a lot more subtle.
Here I'm multiplying by 100 and then by 1 over 100,
which also should give me back just the identity f of x equals x.
So if we zoom in on this one, we'll start to see some detail.
Actually, maybe that's just my laser printer.
Well, that's the problem with using imprecise tools.
Maybe we should do this on the computer.
Okay, here we are on the computer.
And if I zoom in on this line, and I got to zoom in a lot,
near zero, it's pretty much perfect.
But as we get near one, it gets a lot more jacked.
And this is real imprecision, and it depends where you are on the number line.
You get different rounding error.
This is actually pretty hard to reason about, to be honest.
So just suffice to say, when you multiply,
you get a little bit of error all throughout the number line,
but it depends on where you are.
So that's multiplication.
Now we can try to put these together in various ways.
So I play with this a lot,
and I produced a whole bunch of just totally bonkers functions.
But actually the best shape that I was able to make
came from just using multiplication.
And what I do is I multiply by the first number that's smaller than 1.
So it's just slightly less than 1.
That's 1 minus 1 over 2048.
I keep multiplying by that over and over again.
And as I do, I accumulate more and more error in different parts of the graph.
Of course, I'm also making the number smaller.
So at the end, I want to normalize back so that f of 1 is 1.
Here's what it looks like if I do that iteratively.
Accumulating error.
I found that 500 steps was a good stopping point.
So this is a function that I call grad1.
Grad for the name of this project, which I can't pronounce.
Gradient half decent.
I triply half gradient descent, you get it.
And it has this nice zigzag shape.
Importantly, it's kind of smooth.
If we zoom in on it, we'll see that it's, you know, putting aside the pixels,
that it's piecewise linear.
So that's nice.
Now you might wonder, why does it get this zigzag shape?
And truth be told, round-off error is just kind of hard to reason about.
Let me show you two illustrations that are at least nice to look at.
In this rainbow, I've given each of the numbers between 0 and 1 a color on the x-axis.
And then on the y-axis, I'm successively multiplying by that constant.
And you could see that they get exponentially smaller as expected, but not smoothly.
And these changes in direction come from different exponents,
and we see some of that reflected in the zigzags.
On this image, the x-axis is 0 to 1 again.
The y-axis is successive multiplication by the constant.
But the green pixels is when my round-off is too high compared to the correct result,
and magenta when it's too low.
This line at the top is at 500 iterations.
And you can see how it slices both green and magenta regions.
Too high and too low.
One more thing, in order to train models with this thing, we need to know it's derivative.
And for reasons of implementation tricks that I'm not going to get into,
I actually need the derivative in terms of the y-coordinate instead of the x-coordinate.
Now, I'm not good enough at math to figure this out analytically.
In any way, it would probably just be a table of the values for these different segments.
But since it's 16-bit and there's only 65,000 values that are possible,
I can just use a computer program to compute the derivative for every point.
So here that has plotted along the y-axis.
I think it looks pretty cool like an oscilloscope.
You'll notice that the derivative isn't a perfect square wave,
and it wouldn't be because there are in fact little imperfections in this curve from round-off error.
I'm actually applying a low-pass filter here, it would be even noisier.
But anyway, now we've got the function and we've got its derivative,
so we can do some machine learning.
But first, a bonus digression.
Here's a bonus digression.
Having freed myself from the need to quote-unquote do math in order to differentiate functions,
because I'm just going to generate a table programmatically,
I can now consider all sorts of exotic transfer functions.
I can even betray the central thesis of this work and consider functions that are not linear.
One thing I think is really funny is when you use data sort of as the wrong type,
you may be familiar with the fast inverse square root technique.
I love that one, and I think it's worth considering if a transfer function even needs to use floating point operations
in order to be implemented.
I tried to find the fastest, simplest thing you could do that might work.
My favorite was to treat the float as just 16 bits,
shift them down by 2, and then treat that back as a float.
For integers, shifting by 2 is just division by 4.
But for a floating point number, since there are different fields within the word,
this moves bits between fields.
So for example, the sine bit gets moved into the exponent,
which means you have a much larger exponent for negative numbers than for positive ones.
The result will always be positive because we fill with zeros.
Dividing the exponent by 4 has a logarithmic effect on the result,
and then some of the exponent bits also go into the mantissa.
So you get a kind of crazy function that looks like this.
The negative values are much larger, as we said, and it logarithmically approaches zero.
The positive region is actually a very small upward slope, which you can't see on this graph.
But since the exponent will start with two zeros, these tend to be pretty small.
This is the full range that's explored by the positive values,
and you probably don't care, but here is its computed derivative in terms of the y-coordinate.
So in the experiments which are coming up next, I'm going to also compare this transfer function.
This wouldn't prove the professor wrong because it uses a forbidden operation,
but it is about as fast as you could do anything on a computer.
So if it does turn out to work, it might be a half-decent choice.
To compare the transfer functions, I tried them out on different machine learning problems.
Fortunately, I do have my own bespoke GPU-based system for training neural networks,
which has appeared on this channel before in videos such as 30 weird chess algorithms.
It's not that good, but it is the kind of thing you want if you're going to do a silly experiment like,
what if deep learning, but worse?
So I made a bunch of modifications for this project, for example, to do the forward step with half-precision,
and to support these tabled transfer functions.
Then I trained a network using the same structure and initialization, changing only the transfer function.
The first problem is the MNIST digit recognition dataset.
The original CAPTCHA, you get 50,000 labeled examples of these tiny digits,
and you have to learn to predict the digits 0 through 9,
and then there are 10,000 held-out examples to judge your accuracy on.
I chose this classic problem partly for trollish reasons,
because even at the time of publication decades ago,
various techniques had already achieved extremely high accuracy.
The networks I trained looked like this.
They take in the input pixels, then there's a number of internal layers,
and then a dense output layer with one output for each of the 10 digits.
You can see the paper, the code for details, if you really want.
But one important thing I want to point out for these experiments is that the output layer uses a strict linear transfer function,
the identity, for each of the models.
It's not a good choice for these categorical problems, but it allows the network to output any value,
even if the transfer function, for example, only outputs positive numbers.
And since it's linear, it complies with our goal of proving the professor wrong.
Throughout the rest of the network, all the internal layers use the transfer function that we're studying.
So I trained one of these models with the same initial conditions and the same data,
but using a different transfer function.
I do that for 200,000 rounds, which takes about a day each.
We can then compare the final accuracy and other dimensions, such as their aesthetics.
The functions are as follows.
We have two classic sigmoids, the hyperbolic tangent and the logistic function.
After that, the rectified linear unit.
Here I'm using a leaky version where the region below zero actually has a small slope.
That seems to work better for me.
This function is very popular today.
None of those functions are linear, as expected by the professor.
Then we have a couple that abuse floating point round off error.
First a really simple one.
I add 64 and subtract 64.
We saw how that discretizes the line.
Then the grad one function, which multiplies by a number near one 500 times in order to smoothly magnify the round off error.
Then we have our bonus content, downshift two, which manipulates bits directly.
Finally, we'll evaluate the identity function, which is what the professor thinks a linear model must be equivalent to.
On the MNIST problem, all of the transfer functions do well.
As expected, the classics are nearing 100% accuracy.
Even a simple linear model using the identity function gets like 82%.
Plus 64, which gets a little bit of non-linearity with round off error outperforms it slightly.
But the nice smooth grad one function is almost in the same class as the classic functions.
It's working quite well.
So it seems like our hypothesis is panning out, and I can sense the professor beginning to sweat.
The next problem is the SIFAR-10 dataset.
This is a lot like MNIST, but instead of recognizing digits, you have to recognize spirit animals.
There are 10 spirit animals.
Airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks.
This problem is much harder.
With these tiny thumbnails, I sometimes can't figure out what it is.
But same idea.
Days later, we get results.
The ranking here is the same for this problem, and we can draw basically the same conclusion.
An accuracy of 53 doesn't sound that good, but keep in mind there are 10 different classes.
So if you just guess randomly, that's an accuracy of 10%.
So we are substantially learning here.
But another way to understand our accuracy is to compare it to what's come before us.
These are standardized problems, and so a lot of researchers have posted their results.
So I can check the leaderboard and scrolling all the way down to the bottom.
I can see that my results are, in fact, the worst result of all time.
That's not too bad.
Last place is the last winner.
Now putting aside the aesthetic and ideological considerations, there is something to this.
Recently, I feel like deep learning is getting a little too good, a little too fast.
So maybe we could just slow it down a bit.
The third problem is, of course, chess.
I take millions of positions from a public database, and I ask Stockfish a strong chess engine, which side is winning?
There's two classes of results here.
One side could have an edge, and it's standard to give this advantage in terms of pawns.
Or it could be of the form mate in negative 24, which means that black, because it's negative, has a mate in 24 moves, no matter what white does.
Mate is always favorable to a mirror edge, no matter how big your advantage is.
One thing that's funny to me about this pawn score is that the advantage can be sort of arbitrarily high if Stockfish can't find a mate.
So it can give you more than 64 pawn advantage, which is funny because how are you even going to fit those on the board?
Dual wheeled?
Actually, here's an exercise for the reader.
Find a position with the largest possible advantage according to Stockfish, but where it can't find mate.
Here's the best that I could do.
Huge advantage for white, but no mate even after depth 89.
I mapped these scores into the interval from negative 1 to 1, where negative 1 is the best possible result for black, me and 1, and plus 1 is the same for white.
And this gives me a machine learning problem, which is to learn how Stockfish rates each board.
For the models that score chess positions, we could compare their predictions directly to Stockfish to understand how accurate they are.
And I did that, but I think it's more fun to use these models to create chess players and then have them play against each other.
These players just look at each legal move and score the resulting board and then take the move that would be most favorable to their side.
So there's no game tree search here.
Here are the results of a tournament.
The rows are each player as white and the columns as black, and they're ordered by their final ALO rating.
There's some complexity here, but the fixed versions are the ones to look at.
As usual, the rectified linear unit is performing the best, but our quote-unquote linear transfer function, grad 1, is actually in second place and not far behind.
It's close to being as good as Chessmaster for the Nintendo Entertainment System, and outperforms Stockfish deluded with half random moves.
This is actually pretty impressive given that it's doing no game tree search, it's just using its intuitions about what boards are good.
Of course, raw performance on these problems is not the only thing.
We ought to think about the speed of the function, as well as its aesthetics.
Some of them have nice shapes, and others look dumb.
Since training takes days, where all you have to do is stare at graphs of activations,
whether those look cool, or boring, or vaporwave, also bears some consideration.
The key finding here is that the professor was wrong.
You absolutely can use a linear transfer function, as long as you don't need it to be both good and fast.
Defeated.
Having gotten my revenge, we could stop there.
But when have huge breakthroughs in science and technology ever happened by stopping there?
So, it's on to the next level.
So far, all the functions we've considered have been monotonic.
That's because both plus and multiplication, even when you round, have this property.
But we're certainly not limited to this.
For example, if x appears multiple times under addition or subtraction, we can get much more interesting functions.
Another way to look at this is interference patterns between linear functions are linear.
For example, x minus 4,096 minus x plus 4,096 is linear.
It's mathematically equal to zero.
But in half precision floating point, it produces this square wave function.
Now, this function isn't as well behaved as it looks.
One of those intervals is width one exactly, but the other is very slightly smaller than one.
And again, this has to do with perversities of roundoff error.
Or if we take that grad one function that we've studied and subtract x from that, we get this nice triangle wave.
By stringing functions together, we can make all sorts of interesting patterns.
In fact, if we have any shape in mind, we can try approximating it with one of these functions,
subtract it from the desired shape to get a new shape,
and as long as we're getting smaller, we can just keep doing this,
successively approximating that shape like a Taylor series.
So if we could make any shape, let's make a fractal. Those are good shapes.
The Mandelbrot set is the radiohead of fractals.
Here we're going to use complex numbers, and we use a coordinate system where the x-coordinate is the real part,
and the y-coordinate is the imaginary part.
For any given point C, we repeatedly square and add,
and this point moves around in a crazy way.
And based on how quickly it converges or diverges, we give it a color.
Boom. 2D Mandelbrot.
Now adding C is linear. Squaring, however, is not,
but we just said we can approximate any shape using interference patterns.
So here's a rough approximation of f of x equals x squared, using only linear operations.
So this has some funny business near the origin,
but you might think we could use this to plot a kind of perverted Mandelbrot.
Unfortunately, if we try, we get this piece of garbage.
This stupid blotch sucks.
To understand why, we need to look at the definition of squaring for complex numbers.
When we multiply this out, the A and the B get mixed together.
The real part has some A in it, and because I squared is negative 1,
some B in it, and the imaginary part also has some A and some B in it.
So we get this cross-pollination, and that means x and y-coordinates are mixed,
and you get a kind of weird rotation.
But let's look at the purely linear operations on complex numbers.
For both plus and scaling, the real parts stay real,
and the imaginary parts stay imaginary, with no cross-pollination.
So no matter how we use these operations, even with floating-point roundoff,
we're not going to get any mixing between the coordinates.
And that's why the fractal has these rows and columns of sameness.
It's really just two independent functions, one on the x-axis and one on the y-axis.
So professors take note, the complex numbers do provide some refuge.
It's time for another bonus digression.
You might think you could just make a 3D Mandelbrot.
Just do the same thing we did before,
but with numbers that have three components,
a real part and imaginary part and, like, a very imaginary part.
If you try it, this old professor of Frobenius will come along
and educate you with this cool math fact.
No matter what you do, any three-dimensional algebra is equivalent to the real or complex numbers,
so it's like you didn't do anything at all, or not associative,
meaning the order of operations will matter.
But you know what else isn't associative?
The floating-point numbers, my dude.
So it seems we don't need associativity to make fractals anyway.
Enter the baffling numbers, which is an ill-advised generalization of the complex numbers, to three dimensions.
Yes, it won't work, but we can just do it.
Frobenius can't stop me.
And I can use this to make a 3D fractal called the bafflebrot.
Here it's sliced in half, showing a perfect ripe Mandelbrot inside.
The resulting 2-gigabyte file crashes every piece of software I throw at it.
I admire its spirit.
Boom, 3D fractal.
Bezeked.
We don't actually need squaring to create fractals, though.
We just need something kind of chaotic.
I just take this function, which consists of 36,000 linear operations,
and I iterate it, adding C each time, and plot the resulting magnitude.
I think it looks pretty nice.
I think this is a fractal, in the sense that it is chaotic.
It has a color gradient, and could be on the cover of an electronic music album.
It is not a fractal, in the sense that if you zoom in on it, you get infinite detail of self-similar shapes.
In fact, as we zoom in on it only a modest amount,
we see rectangular pixels as we reach the limits of half-precision floating-point.
And because this fractal is built by abusing those very limits,
it's not even possible to get more detail by increasing the accuracy.
Alright, drawing fractals is fun and everything, but it's not really a game you can win.
There's no goal other than to make a cool picture.
So next, I turn to something with a clearer challenge to overcome.
Linear cryptography.
Cryptography is fractals minus drugs.
You take some data and mess it up,
but in a way where you can get it back again if you want.
Possibly the most fundamental building block of cryptography is the pseudo-random number generator.
This is a function that takes in a state, like a 64-bit integer,
and returns a new state that, quote-unquote, looks random.
With one of those, you can generate a hash function by mixing it with some input data,
or a symmetric block cipher using a Feistel network.
So naturally, I want one of these.
Now, another thing that professors will tell you is that cryptographic algorithms cannot be linear.
Here, linear includes within some modular ring like integers mod 256, the bytes,
or mod 2, the bits.
So in contrast, even though we said before that XOR can't be modeled by linear function on reels,
XOR is considered linear in this context.
The reason for that is linear cryptanalysis.
If your function is even a little bit linear,
then with a large collection of input-output pairs, like messages and their encrypted versions,
you can deduce information about secrets like an encryption key.
So the standard advice to construct these things is to alternate linear operations like XOR
with nonlinear operations like substitution.
Substitution is make a table of all the bytes, but permute them randomly,
and then just do table lookup.
In fact, Bruce Schneier writes in the big red book,
substitutions are generally the only nonlinear step in an algorithm.
They are what give the block cipher its security.
So of course, what we're going to do is prove this adage wrong by developing a good pseudo-random function
that only uses linear operations on half-precision floating-point numbers.
Now, what does it mean to be good?
This is less subjective than fractals, but it is still a little tricky.
We don't actually even know if pseudo-random number generators exist.
The best results assume that other problems are hard, but we don't have proofs of that either.
There's lots of stuff that looks random, but actually isn't, like it hides a backdoor.
Never forget that RSA security.
Yes, that RSA took a $10 million bribe from the NSA to hide a backdoor in one of their pseudo-random number generators.
Practically speaking, though, we can subject the function to a stringent battery of statistical tests,
and if it passes all of those, that's a really good start.
The function will work on half-precision floating-point numbers in the interval from negative one to one,
just like the transfer functions we've been considering so far.
Now, this is not a good choice.
It's unnecessarily hard, but all of this is unnecessarily hard.
Now, I have to work with 64 bits.
I could represent each bit as a half, but that makes it too easy.
So I'm going to represent it as 8 bytes, each byte represented by a half.
To represent a byte as a half, I'll divide the interval from negative one to one into 256 segments,
and I'll allow any floating-point value within that interval to represent the corresponding byte.
So anything from 124 over 128 to 125 over 128 will represent the number 252.
And again, allowing any number here is unnecessarily hard.
In the next section, we'll see a much better way to do this that's much faster.
But by struggling with this one, we'll at least demonstrate complete mastery over the sort of continuous domain.
So this function will take in 8 halves and return 8 halves.
And the crux of this function will be this substitution.
That's a table lookup where each of the 256 bytes is swapped for another byte,
or plotted as a function, each of these discrete intervals is mapped to a different interval.
The approach we used in the previous section of fitting functions doesn't work here.
We need something more exact.
So I study a family of well-behaved functions called choppy functions.
To be choppy, the function has to have a few properties.
For any value in an interval that represents some integer,
the function has to produce the exact same result,
and its output has to be the lowest value within some interval.
Of course, these functions can only use addition and scaling,
and since they're maximally permissive about what they accept and very strict about what they generate,
they'll be quite easy to reason about and compose.
In fact, we'll be able to think about them as functions from integers to integers.
So I went on a hunt for choppy functions.
I wish I could tell you that I cracked the code of how to make these from scratch,
but I found them by computer search.
Here's an example that I can't believe I'm going to write out by hand.
This function is mathematically linear.
It's actually equal to a constant.
The x's cancel out.
What this function does is return 1 if the input represents the number 249 or 0 otherwise.
So this is a pretty useful choppy function.
Since each of these represents a function from a byte to a byte,
I can think of it as just a table of the bytes that it produces for each input.
It's a little more complicated than this because the outputs are actually from negative 1 to 1,
but this is the basic idea.
So what I did is I generated a whole bunch of these kinds of functions.
And every time I get a new one, or a faster version of an old one,
I put it in a database keyed by these integers.
I can also take any two of them and get their difference by subtracting them.
That'll also be a choppy function.
So here's say they only differ in these two components.
And so I get 0's everywhere except for those two columns,
and this might give me a new choppy function I didn't have before.
Observe that if I ever find one that's 0 everywhere except for a single 1,
then I can use that to modify the column in any other vector to any value that I want.
So these are special, these are basis vectors.
So once I've done that, this column is kind of done,
and I never need to find new variations of that column.
So if I take a large collection of these choppy functions,
I can do a process kind of like Gauss Jordan elimination to deduce a set of basis vectors.
And if I find a basis vector for every column for every position,
then I can just add those up to make any choppy function I want, for example, our substitution.
So that's pretty nice, I just need to find these basis vectors.
You might think that once you had a single basis vector,
you could shift that column around, like to another position,
by just calling your function on a shifted version of x.
And in real mathematics, that would work.
But since these functions are abusing floating point roundoff error,
which depends on the specific value of x, this approach will not work.
You can do stuff to the output of the function like scale it or add to it,
and you can combine functions by taking their interference pattern.
But you can't straightforwardly manipulate the input side.
This problem is worst near the origin, where the precision is highest.
This meant that it was particularly hard to find a choppy function
that distinguished negative and non-negative numbers exactly.
In essence, the middle two columns of my vectors would always have the same value,
and so they wouldn't be independent.
I need to find some way to distinguish those two.
Going back to our earliest example, if we just add 128 and then subtract 128,
we do get different behavior for negative and positive numbers,
but if we look at the rounding near zero, a lot of negative values round up,
like small positive values round down.
And this makes sense.
If you add a small negative number to 128, you get 128.
So I hunted for the zero threshold function,
and there was a lot of manual fiddling with that.
But I did eventually find one, and it looks like this.
It's pretty involved.
One of the key things is to do a whole bunch of multiplications at the beginning,
since these will preserve the sign.
Spread values away from zero without causing any corruptive rounding
until you can do the same old loss of precision techniques
to make all the finite values the same on either side.
With that zero threshold function solved, I can now create a basis
and therefore create any function from a byte to a byte.
So back to our pseudo-random number generator.
The structure I'm going to use is a classic substitution permutation network.
It takes eight bytes in A through H,
and I apply the substitution function to each of the eight bytes.
Then I rearrange the bits,
apply a few more linear operations like modular plus and minus,
and then I have a new state as the output.
And by iterating this, you create a pseudo-random stream.
The substitution function we already talked about.
For permuting the bits, each of the output bytes depends on all of the input bytes.
So it's not a function of one variable,
but I can construct it from functions of one variable.
If I look at this first byte in the output, let's call it y,
and I can look at the first byte in the input of the permutation that's x.
Note that there's just a single bit that it reads.
Remember, I can create any function that I want of a single variable,
so I construct a function that returns 128 if that bit is set in the input of the y0,
and I do a similar thing for all the other bytes,
and then I can just add up those results,
and they all set different bits, so adding is like logical or.
That technique of adding independent things is really useful,
and we're going to use it more later.
The last piece is modular addition.
I have addition, of course, but on bytes it needs to wrap around
if the result is greater than 256, or in this case, greater than 1.
So if I add two of these values together, I get a result that might be as high as 2,
so it looks like this, but I want it to look like this.
Once it gets past 1, it should go back to negative 1.
Fortunately, I do have a way to test whether the value is greater than a threshold like 1.
So modular plus takes in two arguments and adds them together,
and that result might be either too low or too high.
We'll talk about the case that it's too high.
We test whether it's higher than 1 using the 0 threshold function,
which returns either 1 or 0, multiply that by 2, and then subtract it away.
So that allows us to add this corrective factor and put it back into the right range.
Now, I mentioned before that you can't necessarily shift around functions
because of loss of precision, but this will actually work for the 0 threshold function.
We're going to come back to that in a second,
but first I want to evaluate this random number generator to see how good it is.
In order to test this thing, I used a pre-existing suite of statistical tests called Big Crush.
This is like hundreds of tests that if you do things with the random numbers
that should have a correct mathematical result, you in fact get that mathematical result,
and not something that's a little biased.
It's really hard to pass these tests.
You can try it out on some handmade functions if you want.
It's pretty good at finding bias.
This test needs like 1.6 billion bits of input to do its thing,
so I actually ran it on an equivalent C implementation of this function,
but I also test that they produce exactly the same result.
Even with the C implementation, this takes days to run,
but it did, and it passes every single test,
so it's reasonable to believe that this function could be the basis of a decent encryption algorithm.
Defeated.
Now, one downside is that if you run this using the native half-precision implementation,
it produces 25.8 bytes a second of randomness, which is very slow.
Now, you can produce tables ahead of time so that each of those operations is just a 16-bit table lookup,
and then it'll produce 18.5 kilobytes per second, and that's still slow.
But if you were trapped on a desert island and all you had were linear floating-point operations,
I guess you could do worse than this.
Of course, if you're trapped on a desert island, I don't recommend encrypting your messages.
This is just not a good way to get rescued.
So I said I'd come back to this bit here.
We used the zero-threshold function to test if a sum was greater than one
so that we could implement modular arithmetic by subtracting off a corrective factor.
Once upon a time, I told you you couldn't just shift around the inputs to functions,
and this is true in general, but the zero-threshold function, because it operates at zero,
which is the most precise region for floating-point, actually does admit this behavior within a certain range.
If I have some value in mind, like 0.125,
and I want a function that tests whether the input is just greater than or equal to 0.125,
that looks like this,
and I can do that by just subtracting 0.125 from the input and passing it to the zero-threshold function.
And if the input is exactly 0.125, we get back exactly zero.
This works for most numbers, but there are some limits.
So on the y-axis here, we have different choices of threshold.
On the x-axis, we have all of the possible inputs, and this is all of the finite floating-point values.
The green region is where we get the right answer.
The red region is where it's wrong.
The only reason it's ever wrong is that we end up getting an infinite value during that computation.
Otherwise, this would all work out.
The green region is pretty big.
It always works out when the input and the threshold is exactly the same, because then you get zero,
and then you're not going to have any infinities.
But as they get farther from one another, the value you're testing is larger,
and therefore you're more likely to encounter infinities.
The highlighted region is everything from negative one to one,
which accounts for almost half of the finite numbers.
And you can see we've covered pretty much this entire interval.
There is this one corner, like a couple numbers that don't work.
But it's, I mean, we can do...
Okay.
Alright, fine.
I'll fix it.
Alright, now I can sleep soundly.
Here's a new version of the zero-threshold function,
which works on the entire negative one to one interval.
And more than that, in fact,
I found this with computer search again,
trying to maximize the size of the interval on which it works.
And basically it's the same as before,
but more careful about intermediate computations
so that it doesn't touch infinity by accident.
So now that I know that this works for every value in there,
I can actually use it to generate literally any function that I want on that interval.
The first step is to take this general-purpose greater-than function
and turn it into a general-purpose exact equals function.
I check whether the input is greater than or equal to the value,
but then subtract off a corrective factor.
If the input is greater than or equal to the next floating-point number,
that's this next after thing.
This returns one if the input is exactly v.
And then I just make an enormous expression.
There's only a finite number of floating-point inputs.
So for each one, I test whether it's exactly equal to that,
giving zero or one.
And I multiply that by the constant value that I want to have at that point,
the y-coordinate.
Then I sum those all up and it makes any shape that I like.
So that's great.
Linear functions can do anything.
And one thing I don't like about this is how big this expression is.
In some sense, that's funny,
but it's starting to look like this thing is turn-complete,
and I'd like to build a computer to demonstrate,
since that's what you do.
But I don't know, everything is slow turn-complete these days.
So I want to figure out how we could make it a bit more practical,
because I like to work at the intersection of theory and impractice and practice.
So I consulted my extensive computer science library
for performance-enhancing substances.
I found a relevant-looking article in the 2018 SIGBOVIC
called The Fluent 8 Software Integer Library,
by Jim McCann, he sounds smart,
and Tom Murphy V.
Wait, I already wrote this paper?
God damn it.
Yeah, this looks familiar.
Uh, man.
Well, the Fluent 8 library implements unsigned 8-bit integers,
using 32-bit floating point.
That sounds pretty familiar,
but it does make some different design decisions than what we're doing today.
One superficial difference is that it uses 32-bit full-precision floating point.
That's easy to change.
It also uses some nonlinear operations,
so we're going to need to fix that.
But it's core idea, and the reason it can be much faster,
is that each integer is represented by the corresponding floating point integer.
And the operations will only work if the input is exactly an integer,
and they produce integers as output.
So we don't need to worry about numbers that are really close to zero,
or negative numbers like we did,
when we were working on the entire interval from negative 1 to 1.
This allows us to pull some more tricks,
and then do things more quickly.
So we're going to combine the power of what we've done so far,
and Fluent 8, and get Fluent 8.
Fluent 8.
Fluent 8.
Fluent 8.
Ah, close enough.
This time this stands for half floating linear U and 8,
and then we're going to use that to implement a classic computer.
So each byte will be represented by a half precision floating point number.
And since bytes are integers, we'll represent it by the corresponding floating point number,
which is exactly that integer.
All 256 of them have exact representations.
Let's first look at a helper function that's familiar.
This is another threshold function.
It requires an integer, but that integer can be as high as 511.
9 bits.
If the number is greater than or equal to 256, it returns 1.0,
otherwise 0.0.
So this is like a threshold 256 function,
or a downshift by 8 bits.
It uses the same kind of loss of precision tricks we've been using all along,
but we can get it done with 4 operations this time,
because it only needs to work on 512 different inputs.
It's similarly easy to downshift by 1 or 2 or 3 or 4 bits,
and we have functions for that as well.
And now we can implement modular addition the same way we did before.
We just compute the sum natively.
Now that could be over 255.
But we have a way to test whether it is and compute 1.0 or 0.0.
So we multiply that by the constant 256, which gives us either 256 or 0,
and we subtract that off so that the result is back in range.
Cool.
We only did 7 floating point operations here, which is not bad.
I'm certainly not going to show you all of the code,
but I wanted to give a taste of some of the interesting problems
that we need to solve in order to do this efficiently.
While addition is already kind of linear except for overflow,
bitwise operations like and are not even close.
But we can do it pretty cleanly with some of the operations we've already constructed.
I'll run this loop exactly 8 times once for each bit,
and this will be unrolled by the compiler, so we're not even doing these comparisons.
It's as though we wrote this 8 times.
Since it's unrolled, we can compute something like a constant 2 to the i at compile time as well.
We work bit by bit starting with the lowest order 1.
The first thing we do is shift each input down by 1 bit using a function we've already seen.
Then we shift it back up.
As long as the input is less than 128, which it will be,
you can shift up by 1 by just multiplying by 2 or adding it to itself.
Now we know the last bit is 0, so if I subtract this from the original argument,
I get the lowest order bit of the input, either 1 or 0.
So I've extracted the lowest order bit of both args,
but I still don't have and even on 1 bit.
Multiplying the two bits together would give me the right answer,
and multiplication is one of the linear operations.
But remember that we only allow multiplication by a constant.
For example, if you were to compute x and x, both a bit and b bit would depend on x,
and so here you'd have x times x, or x squared, which is not mathematically linear.
So we're not going to use multiplication, but we do have a nice trick,
which is to add the bits together and then shift down by 1.
If we look at the truth table, we see that this only produces a 1 when both of the inputs were 1.
I take the resulting bit and multiply it by that round's scale, which is a power of 2, a constant,
and then I just add all of those up.
Since the components will be 0 everywhere except for that one bit, plus is equivalent to or.
Ah, this reminds me of a slip-up in one of my previous videos
where I was computing the or function using and and xor and plus.
It totally works, but millions of people wrote in to tell me that I could do it with another xor instead of plus,
which would have been a little faster.
But here plus is the right option. We don't have xor, it's not linear.
I was just, like, foreshadowing this, getting you ready.
Defeated.
Anywho, that's all we need for bitwise and.
It's a little involved, but it's a far cry from the 9,000 operations we did before
just to test if a value is greater than 0.
A spreckin of which we can now quickly test whether a value is exactly 0.
We do this by negating the bits, subtracting from 255.
Then we add one, and that'll only overflow if the original value was 0.
With that, testing whether two values are equal is just a matter of subtracting them
and then seeing whether the result is 0.
And that's how it goes. That's how it always goes.
You build up some constructs, and you use those to make some more.
You gain more and more power until you have all of the things you want.
There are some good puzzles in here, and you may enjoy trying to work some of these out yourself.
And you may improve upon them, and please tell me if you do.
For example, on screen I'm showing you a straightforward way to do if,
but if you check my code, I do a thing that's way more mysterious and fancy
in order to squeeze the last bits of performance out of it.
And I am going to care about performance for this application.
The last time I made a computer out of floating point numbers, which did happen before,
this computer was focused on beauty with no concessions to practicality.
Frankly, the computer was sort of boring to use because it had no I.O.
and it didn't do anything you could observe.
So this time I want to do the opposite.
I'm willing to make some concessions on beauty as long as the result is entertaining.
Now the most entertaining computer is the Nintendo Entertainment System.
And so this is a natural choice.
After all, I like to work at the intersection of theory and impractice and practice and entertainment.
The Nintendo Entertainment System consists of a basically reasonable computer
and a bunch of other weird stuff for entertainment purposes only.
The core of the computer is an 8-bit microprocessor that's more or less the Motorola 6502.
And that other stuff includes video and audio hardware and the controllers and the game cartridge,
which itself might include hardware and stuff like that.
My goal is to replace that 8-bit microprocessor with something that only runs linear floating point operations.
So I'm not going to implement any of the weird stuff.
And that's good because I'm going to do this in a software emulator,
which is my own hacked up copy of FCE Ultra,
and this emulator is so complicated.
But the code that emulates the processor is basically tractable.
The processor state consists of a small number of 8-bit registers,
each of which will represent with a fluent 8.
There's also a 16-bit program counter.
We'll only need a few 16-bit operations,
and it's quite easy to build 16-bit integers using two 8-bit integers.
So I won't say any more about that.
And at a high level, the processor is just a loop.
It reads one byte from memory at the program counter,
which tells it which of the 256 instructions it's going to run next.
It runs that instruction, which updates the state like the registers in the program counter,
and then starts the loop again.
Of course, there are copious details here.
First, let's look at a simple instruction so you can kind of see how it goes.
A really simple instruction is tax.
And speaking of tax, I'll have you know that video editing is so tedious
that while making this video, I actually procrastinated it by doing my taxes.
Anyway, TAX on the 6502 transfers the value from the register A to the register X.
There are still several steps, though.
After we copy it over, we need to update the negative and zero bits of the processor flags
and increment the program counter.
But this code is actually quite nice because we've already done all the work of implementing 8-bit integers.
It makes use of bitwise AND and is zero and shifting and so on.
If all the instructions were like that, this thing would be really simple.
So let's look at a harder instruction.
This is a branching instruction, branch on conditions set.
It modifies the program counter to basically do a jump if one of the processor flags is set.
Otherwise, it just advances to the next instruction.
First problem we'll see is that there's a branch in the implementation of the processor, which is not linear.
That's this if else.
But we do have a fluent 8 version of if.
So we can change this to update the program counter, but to a value that depends on the condition.
That'll look like this.
But the other problem is this memory access.
Now the Nintendo has a main memory of 2 kilobytes, and we could create 2,000 fluent 8s and implement this array subscript.
That's not really the problem.
The problem is that accessing memory has side effects.
So in the previous version of this code where an if wasn't executed, we wouldn't have accessed the memory.
This is sort of obvious for memory writes because writing changes memory.
Less obvious is that writes and reads often have side effects because of memory mapped IO.
For example, writing 2 bytes to 2006 will load them as an address into the PPU, that's one of those weird things.
And then writes that happen to 2007 will pass through to video memory at that address.
Or writing to 4014 will start a DMA that transfers 256 bytes to video memory and stalls the processor for 512 cycles.
So these are not small things.
And worse, there isn't even a small set of them because lots of cartridges have hardware inside them that does arbitrary stuff on reads and writes.
And weirdos are making new weird cartridges all the time.
And so here we have the main concession.
The emulator API for this chip offers a conditional read and write.
These take the address, but also a condition.
If the condition is true, you do what you'd expect.
But if it's false, nothing happens and an arbitrary value is returned.
Of course, this isn't linear, but it's not really that unrealistic if we were making a processor.
We would just wire this through to the memory controller, which would then ignore the read or write if the bit isn't set.
The real 6502, for example, has a pin that indicates whether it's doing a read or a write.
But I accept your criticism.
Feel free to defeat me by doing without this.
Another challenge is that the 6502 has a load of undocumented and really weird instructions.
And this wouldn't be so bad except that the emulator source code I'm working from is extremely hard to understand.
It's filled with all sorts of macro hacks that assume specific variable names, references to mysterious global variables like temp and foo,
pirate jokes, feuds between developers commenting out each other's wrong code, and so on.
And unfortunately, I don't have any Nintendo games that actually execute many of these instructions.
So in the course of development, I made my own cartridge that executes a whole bunch of undocumented instructions when it starts up.
It displays the results of those on the screen so that I can test whether my implementation matches the reference.
This cartridge might be the world's most boring Nintendo game, even more boring than Wall Street Kid.
Here's what that game looks like.
You can't win it or even play it.
It exists only to destroy your mind.
The last puzzle to solve is instruction dispatch.
When we read the instruction byte, we look at it and decide which instruction to execute.
The natural way to implement this is with a switch statement and a case for each instruction.
But that of course is not linear.
We can't do any branching of control flow.
We have to execute the same series of additions and multiplications each time.
So what I'll do is execute every single instruction every time.
Now I only want the right instruction to do anything.
So the first thing I do is make 256 copies of the CPU state, basically the registers.
That's a finite number of variables.
I also have an active flag for each one of those.
And exactly one of those active flags will be set to one for the correct instruction.
Then I run all of the instructions on their own copies of the CPU state.
If I do any conditional reads or write, I include the active flag in the condition.
So only the correct effects will happen.
So then I have the resulting 256 states and I need to copy the one from the correct instruction back into the main state.
The way to do this is to zero them all out except for the active one.
And we can do that with if and then just sum them all up.
They'll all be zero except for the correct one, so we'll get the right answer.
Now it's kind of annoying to run every instruction on every tick of the CPU.
And this technique is the main reason that it's not going to be that fast.
But it is completely linear.
Another upside is that each instruction is completely independent.
So they can actually be run in parallel.
So let's start up the benchmark.
Super Mario Brothers.
Here it's splitting the instructions across eight cores running in parallel.
If not for that instruction dispatch, this thing would run at playable frame rates.
But, and yes, it is already running.
The cost of not cheating is that it runs pretty slow.
The hardware Nintendo runs at 60 frames per second.
And the emulator free to run non-linear instructions gets 3,500 frames per second.
But the linear version, and I did do a lot of optimization, gets 0.11 frames per second or 8.6 seconds per frame.
Which ain't fast.
Maybe you could help me out by putting this video in 2x speed.
I will say, though, in comparison that I have played AAA titles that at launch, inexplicably, on high-end hardware had comparable frame rates.
And they were no doubt executing a great many non-linear instructions.
Speed aside, we now have a general-purpose computer, which renders everything we've done up until this point moot.
If we want a non-linear transfer function, we can just implement the hyperbolic tangent.
If we want fractals, we can just write code that draws the Mandelbrot set on the Nintendo.
We can just write a good encryption algorithm like AES.
We can have a chess engine with search.
In fact, we already have one.
Chessmaster for the NES was included in our tournament already.
And by running it on our linear emulator, we have a linear model.
So it seems I defeated even myself.
And I can finally be done with this damn thing.
Alright, so what have we learned today?
It's the same thing we learned every time.
Complexity is everywhere, even with something as simple as plus and multiplication by constants,
which mathematically can only create lines.
Given a tiny foothold by way of rounding error, we can bend them to our will and make them do anything.
And I think this is the same story of computer science.
Complexity from simplicity, and maybe even of the universe.
So don't underestimate simple things put together.
Anyway, if you made it this far, thank you for watching.
Thank you for your attention.
And if you didn't make it this far, I don't even know what we're talking about.
I'm sure I'll be back soon with more stupid stuff.
In any case, I've been Tom7, and this was Impractical Engineering.
See you soon.

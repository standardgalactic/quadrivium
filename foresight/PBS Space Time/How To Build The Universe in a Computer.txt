Thank you to Ren for supporting PBS.
In around four and a half billion years, the Andromeda Galaxy and their own Milky Way
will finish their long mutual plummet. On a first grazing pass, the delicate spiral arms will be
yanked almost out of their sockets. Perhaps their denizens will look back with relief
as the dislocated galaxies retreat on their future night skies. But then they'll pause
for a hundred thousand year eye blink before falling back together. In a series of whirling
collisions, all spiral structure will be obliterated, gas will be compacted to produce
waves of supernovae, and the giant milk-dromeda galaxy will have been born.
The event I just described will happen as surely as the sun will rise tomorrow.
We know this because we've calculated the chaotic gravitational and hydrodynamic
interactions of countless stars and gas and dark matter particles over billions of future years.
And simulations of galaxy collisions are just the beginning. We routinely simulate the universe
on all of its scales, from planets to large fractions of the cosmos. Today we're going to
see how it's possible to build a universe in a computer, and to see whether there's a limit
to what we can simulate.
In 1941, the Swedish astronomer Erik Holmberg conducted what was probably the first
simulation of the universe. Right when the first programmable computers were being assembled,
but Holmberg didn't use a computer, he arrayed 37 light bulbs on a plane, each one representing
billions of stars in a spiral galaxy disk. The light from each bulb was its gravity,
felt more strongly close to the bulb, and dropping away with a square of distance,
just like gravity. At any point on the disk, a galvanic cell could determine in which direction
the intensity of light was strongest. So the simulation went like this. Holmberg started with
a pair of these light bulb galaxies next to each other, with the motion of each bulb existing
only as a note on a piece of paper. He would then measure the light at each bulb,
which told him the sum gravitational pull on that group of stars. He'd then adjust the
velocities of all according to Newton's laws. Finally, he'd allow time to tick forward. He'd
move the bulbs according to their new velocities. Then from that new configuration he'd start
the process again. And so the two galaxies collided. Just as with our modern supercomputer
simulations, he witnessed the destruction of the disks, the throwing off of tidal streams,
and the formation of an elliptical galaxy. Holmberg's experiment was ingenious. But why go to all this
effort? Since the time of Isaac Newton, it had been possible to write down equations describing
the trajectories of a pair of massive bodies moving in each other's gravitational fields.
And then to solve those equations to find their positions at any time in the future.
But note that I said pairs of bodies. This only works for two objects. For three or more bodies,
there is no simple set of equations describing their future evolution under gravity.
This is the so-called three-body problem, which we've discussed in the past.
The three-body problem doesn't have any analytical solution, no simple master equations,
which is why Holmberg had to solve the problem with light bulbs.
Holmberg did what we call a numerical calculation, in which an impossibly complex
computation is broken down into a series of much simpler steps. And this particular type
of numerical calculation is called an n-body simulation. In an n-body simulation, Newton's
laws of motion and gravity are applied over a series of time steps. Each step is short enough
that we can assume that the global gravitational field is constant. It only changes in the next step,
after all the particles have made their moves. The predictions of these n-body simulations
can be as accurate as you like, as long as you make the time steps small enough. And I would hope so,
because this is essentially how we calculate the trajectories that place people on the moon or
land robots on comets. For simulating a solar system, this method is pretty reasonable. In fact,
the room-sized computer used to calculate the Apollo trajectories had about the computing power
of your smartphone. But if we want realistic simulations of, say, a galaxy with its billions
of stars, we need to do a bit better. Fortunately, computing power has improved somewhat since
Holmberg or the Apollo missions. But actually, it's improved nowhere near enough to do n-body
simulations of entire galaxies using the method that I described. Let's think about what this
computation really needs. In the simplest type of n-body simulation, you need to compute the effect
of every particle on every other particle. So if there are n-particles, that's n-squared
calculations. For a modern one million particle simulation of, say, a star cluster, that's a
trillion computations per time step. The real challenge in astrophysics is that we often have
to deal with huge ranges in scale. For example, in a cosmological simulation, we're trying to
watch the detailed formation of individual galaxies, as well as the evolution of giant clusters of
galaxies. But if we keep enough precision to get the fine structure, it's computationally impossible
to also produce a large enough volume to get the larger scales. To do these simulations,
astrophysicists have to come up with some ingenious tricks. Perhaps the most important
is to avoid having to consider every single particle pair. Remember, the strength of gravity
drops off with distance squared. For nearby particles, it's important to consider every
individual interaction. But for more distant locations, it's okay to clump particles together
and consider only their sum gravitational effect. One of the original approaches to doing this is
a so-called tree code. It works like this. You start with a volume full of particles,
each with its starting position and velocity. Now, divide up the volume into eight sub-cubes,
then divide each of these into eight more, and so on. You stop dividing at any given part of the
volume when there is no more than one particle per cube in that part. Next, you run an n-body
simulation by calculating the sum gravitational pull on each given particle. But now you have
a shortcut. When you calculate the effect from distant locations, you don't do it for each
particle. Instead, you do it for all particles inside one of these cubes. The larger the distance,
the larger the cube you can use. Now, this sounds like a complicated process, but now the number
of calculations that you need to do goes down from n squared to n times log n, which for large
particle numbers is much, much faster. Another approach is the particle mesh method, in which
particles are converted into a density distribution and a gravitational potential across the grid.
The force at each point can then be solved using Fourier transform methods, which can
be very fast. Adaptive particle meshes can be used to add higher resolution when needed,
say where the stars have higher density or structure. Modern mesh codes also do classic
particle-particle interactions at short ranges to improve accuracy at small scales. These mesh
codes are useful for systems of particles interacting under gravity, but they also work
for another really important astrophysical situation, flowing gas. The universe started
as an ocean of gas a few hundred thousand years after the Big Bang. That gas is still everywhere.
It flows into galaxies from beyond where it rides the disk, fragments and collapses into stars.
It forms whirlpools and jets around new stars and black holes. All of these processes are key
parts of galaxy evolution and of star and planet formation, and so we'd better be able to simulate
this too. Now we call these hydrodynamic simulations. They simulate the flow of gas
using the equations of fluid dynamics. Particle mesh approaches can do this,
but these days it's more common to use an approach called smoothed particle hydrodynamics,
or SPH. SPH codes don't use a rigid grid, but rather they track tracer particles within the
fluid. Those particles effectively make up a constantly shifting grid. In astrophysics, SPH
codes are used to simulate the flows of gas in galaxies and around quasars, used to simulate star
and planet formation, and even star and planet destruction in collisions or supernovae. SPH
can even be used to do galaxy formation where the stars themselves are treated as a type of fluid.
In practice, modern simulations often use an amalgam of these methods. For example, SPH for
large-scale flows and particle-particle n-body for small-scale interactions. Astrophysicists
often inject all sorts of other physics into their simulations. In your galaxy simulation,
you might need a separate prescription to describe how stars age and die. In your quasar disk,
you need to simulate separately how light travels through the hot plasma, and don't get me started
about the complexity of including magnetic fields or of Einstein's general relativity when the
gravitational field becomes very strong. With the techniques now available, using parallelised
code on modern computing clusters, we can compute some pretty insane simulations. We can see how
stars form in multitudes from collapsing gas clouds, and how planets then coalesce in the
disks surrounding those stars. We can watch as galaxies form with gas and dark matter interacting
to produce waves of star formation and supernovae, settling into spiral structures just like we see
in the real universe. And then we have cosmological simulations, which create entire virtual universes,
from the moment the first atoms formed to the modern day. One of the most famous is the
Millennium Simulation from the Max Planck Institute in Germany. It simulated a 13 billion
light-year-wide cube containing over 300 billion particles, each representing a billion sun's
worth of dark matter. But the current largest cosmological simulation is Abacus Summit,
which just last year simulated 70 trillion particles on the supercomputers at the Center
for Computational Astrophysics in New York. So how far can all of this go? Can we ever simulate
a real universe in which creatures evolve that can themselves simulate universes?
In fact, could we be such creatures? Probably not. None of these simulations contain the full
information of an actual universe or even a tiny part of it. As we discussed recently,
a full quantum description of the world contains unthinkably more information than is contained
in a typical simulation, which just tracks particle positions and velocities. No conceivable technology
could fully simulate a quantum universe, except perhaps a cosmologically sized quantum computer,
which is kind of what the universe is anyway. We've talked about this simulation and hypothesis
stuff before, and maybe we'll come back to it. For now, let's just be proud of the science
that we can get from modern simulations, because we've come an awful long way since Eric Holmberg
pushed some light bulbs around on his table. And the fidelity and size of our simulations will
only get better as computing power continues its exponential growth and as we invent better and
better algorithms. Perhaps there is no limit to what we can learn about the outside universe by
rebuilding it inside our computers and then peering into that simulated space time.
Thank you to Ren for supporting PBS. Ren is a website where you calculate your carbon footprint,
then offset it by funding projects that plant trees, protect rainforests, and suck carbon from
the sky. By answering a few questions about your lifestyle, you can find out your carbon footprint
and how you can reduce it. No one can reduce their carbon footprint to zero, so you can offset
what you have left after reducing. Subscribers receive monthly updates from the tree planting,
rainforest protection, and carbon removal projects they support. You can get to see
the trees you planted and what your money is spent on. You can learn more about Ren's
climate projects at ren.co.
you

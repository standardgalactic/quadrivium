This video is sponsored by Ground News, more from them in a bit.
Miriam Webster defines AI as a three-toed sloth.
According to Al Perkins, a sloth is a slow-moving arboreal mammal, and the name AI was given
to this creature because of its high-pitched screech.
Fascinating.
Artificial intelligence, on the other hand, is the ability for computer systems and algorithms
to imitate intelligent human behavior.
This is not anything new.
The technology dates back to nearly the beginning of computers.
But obviously, the AI that is gaining all the hype today isn't quite the same as the
enemies in your video games or the countless chatbots that have been around for decades.
Clearly, chatGPT is not the same as Clippy.
Probably?
This whole AI thing is complicated.
We don't know the ultimate potential of large language models and neural networks
and blah, blah, blah, but at the same time, it doesn't take a genius to figure out that
the letters A and I are being used as much as possible by corporations looking to increase
their share values.
If a company so much as utters AI, investors cannot help but invest.
Even if they have no idea what it means, or what it does, or what it might do, they just
assume it's going to make money.
NVIDIA, which was well known for being the producer of GPUs for gaming PCs and other
computing products, has gone from a successful company to, well, the third largest on the
planet.
It has seen an increase in value that is nothing short of obscene, all thanks to this AI.
Microsoft is now the most valuable corporation on the planet, as the company is all aboard
the AI hype train, pushing it into every corner of their operating system and having
an extremely close relationship with open AI.
All the while, nobody can seem to agree on whether this technology will change the very
fabric of our society, or prove to be another flash in the pan.
Tech fad, everyone, everyone has an opinion.
I'm exhausted of seeing this back and forth argument that never seems to reach any conclusion.
And speaking of, Ground News is a website and app that gathers related articles from
more than 50,000 sources around the world in one place so you can compare how different
outlets cover the same story.
Each story comes with a clear breakdown of the political bias, ownership, and headlines
of the sources reporting, all backed by ratings from three independent news monitoring organizations.
Take a look at the story about how open AI is setting up a safety and security committee
as it starts training for the next frontier model, something that has caused a lot of
controversies as previous safety team leaders have recently left the company.
This story has been covered by more than 73 sources, with 37% of them leaning left, while
14% leans right.
You can also see the ownership information, and for this story, 40% of the reporting outlets
are owned by media conglomerates.
You can even compare headlines to see how this bias can affect framing.
We can compare headlines quickly and see how some organizations will stoke the public's
existing fears about AI to get clicks, while others might capitalize on the distrust that
Sam Altman has garnered over the past few months.
Ground News has this feature called Blind Spot, which instantly shows you stories that the
left-leaning or right-leaning news media conveniently won't report on.
Now go to ground.news.usk or use the link in the video description to subscribe today.
If you sign up through my link, you will get 40% off the vantage plan, which is what I
use to get unlimited access to all of their features.
I think Ground News is an exceptionally important website, and one you absolutely need to check
out.
Now, back to the video.
Yes, AI has been around for a while, dating back to the earliest computers, but the hype
right now surrounds progress on large language models.
These are programs that take a lot of information in, require a user prompt, and through complex
algorithms basically create probabilities of what the output should be.
As lifelike as they might seem, these things are not actually alive.
There's no thinking involved.
They cannot create wholly original or novel ideas.
They require training data to do anything.
I've seen it suggested that one of the best ways to prove this is to ask a question with
an answer that the AI model has not been provided with in its training data.
For example, go to chatGPT and ask, what's the 21st letter in this sentence?
You might get a different response, but I got the AI saying confidently, it's E. And
if you say, I don't think that's right, it will try and count again and give you a
different letter.
This time, I.
It will sometimes interpret two and one as letters, maybe the apostrophe.
And the reason questions like this break the AI is because it doesn't actually understand
what a letter is.
It doesn't understand anything, but it searches the internet and its training data for the
most likely answer.
Even with these limitations, the technology is clearly capable of some impressive things.
Some assisting you in writing computer programs to proofreading your short stories, to helping
you cheat on your homework.
But even still, a lot of what gets promoted as AI isn't really AI.
A lot of the revolutionary abilities that AI has been reported to have is actually stuff
that traditional software has been capable of doing for a long time.
This creates a problem where the idea of this technology, it's often more valuable than
what it can actually do.
Because again, just saying AI makes stock number go up.
So a while back, Amazon launched a real-life store, a brick-and-mortar shop where customers
could walk in, grab items off the shelf, put them in their carts, and leave without ever
checking out.
These items were automatically charged to people's accounts without any need to speak
to a cashier or even use a self-serve system.
It was all done automatically.
Many assume this was handled using an array of computers and sensors and scanners, but
in reality, the company just probably hired a lot of Indian employees to watch people
while they shopped on cameras.
And then they would charge the user's Amazon account.
Google was caught off guard by the immediate success of ChatGPT and has been trying its
hardest to launch its own AI competitor.
They paid millions of dollars to Reddit for access to the website's posts to train their
AI with and it went about as well as you'd expect.
When users asked questions like how to make your cheese stick on pizza, Google decided
the best answer was to use non-toxic glue.
This information, by the way, was provided by an 11-year-old Reddit post by user Fucksmith,
which Google's model decided was the most reliable source for information.
When asked about how many rocks you should eat a day, it did not give the correct answer
of zero.
And look, things just kept getting worse from here.
As you can imagine, any technology that uses the internet to train itself is probably going
to make a few mistakes.
And those mistakes have made AI a very easy and fun punching bag right now.
But yeah, not all AIs are created equally.
You can't say that LLMs or machine learning or neural networks are completely useless
because, well, a lot of people are using it.
There's potential here.
Natural language programming could allow people to write their own programs with no code required,
just describing what kind of application they want to make and the AI would handle the rest.
You could use this to learn to research information, but again, AI can be wrong.
So I view it in the same way that I view Wikipedia.
It's useful, it's a nice jumping off point, but you need to check your sources.
It's not 100% accurate, or compiling lots of data really quickly to get back at your
homeowner's association.
This does have real value.
Automatic translations, transcriptions, stuff that may have been possible with previous
software is just more accessible than ever.
But people are taking this too far.
The end goal of open AI is to achieve artificial general intelligence.
AGI is supposed to refer to a strong AI that can learn skills in tasks that it was never
trained to learn.
This could lead to machine sentience, consciousness, the singularity.
And right now, everyone seems to believe that if you just keep throwing more computing
power, more energy, more information into these algorithms, you can simply manifest AGI from
the kinds of artificial intelligence we have today.
Everybody has a different way of defining AGI because, well, nobody really knows what
it's supposed to look like.
We often divide AI into two categories, weak AI and strong AI.
Weak AI is capable of doing very specific tasks, stuff it was programmed to do, and
everything we've ever made so far has been a weak AI.
AGI would be a strong AI, and it could do a lot of jobs.
It could do a lot of stuff, would put you out of a job.
But what might this look like exactly?
There are a few different tests for AGI, but I've always found that the best would be
something done with an actual robot in a Tamaton.
Take a robot and ask it to repair the plumbing electrical systems and appliances of an early
20th century home, especially if none of that information is available through documentation
on the internet.
Every home is going to be slightly different, so the robot will need to make assumptions
to learn to understand why your custom 1950s toaster is not working and then try and figure
out, possibly through trial and error, how to get it back into toast making condition.
While today there are demos of robots completing simple tasks in the home, these are in controlled
environments and are only possible because of training data created from previous experiences.
This AGI robot would need to be capable of learning from unique situations, to ask questions
based on its own volition, and solve them by itself.
But there is no proof that this will happen.
In fact, studies do seem to suggest that there are diminishing returns when it comes to this
kind of stuff.
We don't know if any of this is even possible with traditional silicon chips and programming,
binary code, any of that.
It could take entirely new forms of computing to achieve this.
We don't fully understand how the human brain works, so attempting to replicate it
with far more simplistic technology, far more rigid technology, it just seems more than
a bit optimistic.
People are acting like we're five years away from robots taking every job, becoming alive,
enslaving humanity, changing life forever.
And honestly, chatGPT could be a stepping stone to all of this.
A stepping stone to AGI.
In the same way that the discovery of gunpowder was a stepping stone to the nuke.
Or the hot air balloon was a stepping stone to intergalactic spaceflight.
That is to say, not much of a step at all.
For all we know, LLMs and AGI might not even be related.
We might not even be on the right track.
We just don't know.
So if we don't even know if this can manifest into some more powerful AI, if we have no substantial
evidence to support that we're on the road to AGI, then why all the hype?
Obviously, it is incorporation's best interest for people to believe that this technology
is on the cusp of being something truly revolutionary, so good it's frightening.
It makes it seem way more advanced than it actually is.
It implies more potential for growth, which will make the stock number go up.
And they have ways to market this technology.
So many companies are just throwing around the words AI, even if it doesn't even apply
to their products in any way.
Gigabyte, which usually makes gaming-centric motherboards and memory thingies and whatnot.
Well they decided they wanted to go all in on the AI thing and have rebranded their products
with a big ole AI plastered on the side of them.
Yeah, rebranded, because it's really just kind of the same stuff you could find anywhere
else.
It's got AI written on it, whatever that means.
This exact same thing happened a few years back with the whole metaverse craze, where
every company started proclaiming that their existing video games and online services were
already in fact metaverses because investors wanted to hear that.
And it diluted the term to such a point that the word metaverse started to mean nothing
more than just a synonym for software.
When an AI makes a mistake, the companies call it a hallucination, which is a very misleading
term.
This is applying a human quality to the machine, when in reality it's not hallucinating.
It's just taking a set of probabilities and it hasn't received enough training data on
the topic or the right algorithms to give you an accurate answer.
It is not hallucinating.
It simply made a mistake.
The technology simply wasn't good enough.
Will it improve?
Yes, but we don't know how good it will get or how accurate it will become.
For AI sentience, it could be 5 years away or 10 or 100 or 1000 or it might not be possible
for us to build this at all.
We don't know and everything else until it happens is just a guess.
A lot of the AI safety you hear about actually does refer to more realistic concerns about
how this technology might be used.
If in the future, AI services become a replacement for traditional search engines like Google,
if people rely on ChatGPT to get all their news, then those in charge of the service
can lie, manipulate or alter information to benefit those who are in charge of the AI
to push their own motives, their own agendas.
This is where the concerns of safety often lie, not in the fantasy of robots taking over
the world.
Or at least that's where the fears should lie.
And look, if fancy quantum computing or something entirely unprecedented allows for a robot
revolution, allows for AGI, if today's technology and LLMs are on the right path to all of
this, and all we need is just a few trillion dollars in the combined power consumption
of a small nation, I'll be the first to admit I was wrong.
Oopsie daisy, egg on my face, but right now I'm just a little bit skeptical.
So for the purposes of this video, let's assume that the future of AI is not revolutionary,
but evolutionary.
And if that's the case, is it a bubble?
Yes, yes it is, but that doesn't mean it's worthless.
I kind of see AI right now like the dot com boom of the late 90s.
This was a time where investors believed that every single website was going to make millions
of dollars.
And once they figured out that only certain websites would ever become popular, would
ever become profitable, many of them took their money and ran.
The resulting fallout led to the dot com bubble burst.
But this wasn't the end of the worldwide web.
Now I'm not saying that AI will be as important as the internet, but clearly this is more
than just a simple fad that's going to fade away.
I do think the bubble will burst, and then the industry will have to realistically look
at its value rather than rely on unfounded promises and rampant speculation.
But yeah, AI is here to stay, because it's been here for a long time.
AI already has changed our world.
Algorithms drive search engines, YouTube channel recommendations, most of the internet
is driven off of this automation.
We know that the technology has value because it's been around for a while.
The question is whether this new form of AI, LLMs, will have a significant impact on how
we live and work, or how significant of an impact that might be.
And a lot of people seem to have some doubts about all of this.
Rooters Institute for the Study of Journalism pulled 12,000 people from six different countries
online about their opinions on AI.
Granted, 12,000 people is not that many, doing polls online does not mean perfect results,
but this is about as good as we're going to get if we're trying to figure out what
the average person thinks about this technology.
And most people have no idea what it is, or what it does, or what it could do.
A lot of people have heard of AI in these new services, but they don't know what distinguishes
a large language model from traditional computer software.
You can tell that most people are not using AI that often, even if chat GPT has a huge
install base.
Younger people are more likely to try it out than older people, obviously.
But most people view AI as a technology that will primarily be used in professional settings
by scientists in social media companies, but less so by the average person and their daily
lives.
Again, if these people don't actually know what AI is, considering some of these poor
results, none of this is super helpful, but there is some stuff we can take away from
this.
It seems like people aren't just confused by the technology, they seem to fundamentally
dislike it.
There are a lot of concerns about the damage it can do.
People are scared it will take their jobs away, or just functions as tools to manipulate
the public.
And most don't seem interested in the positive aspects of this technology at all.
Most can't even identify what that might be.
Because a technology does need to be more than good enough, it needs to be perfect.
When driving a school bus or doing taxes for a multi-trillion dollar corporation, you
probably don't want to put this responsibility on a robot that can, or probably will, make
a mistake.
Yes, humans are not perfect.
The AI might even be better than the human in many cases, but humans give people somebody
to blame when things go wrong.
They provide accountability, something that the AI just doesn't have for AI.
Google spent all of Google I.O. 2024 talking about its AI initiatives, and people didn't
really seem to care all that much.
A lot of people just rolled their eyes because it's more AI slop.
Microsoft hyped up its new line of AI PCs, which most saw as a concerning invasion of
privacy rather than anything remotely worthwhile.
The corporation is betting the future of its operating system, of the Windows operating
system on AI, and it's a bet that doesn't seem to be panning out.
In fact, I've seen a lot more people talk about moving to Linux now that Windows is
infected with this AI garbage.
I have no doubt going forward that LLMs and machine learning and AI will manifest into
something valuable.
It's possible this might even result in massive changes to everyday life, but right now this
entire thing has been polluted by bad actors, false promises, and in my opinion, pretty
misleading marketing tactics, and it's baffling to me that people have just forgotten the
whole metaverse craze from a few years back, where the word itself began to mean nothing
by the end.
These analysts, who don't even understand the basic technology, are hyping up others
about how much money we're all apparently about to make, claiming that Nvidia is going
to be worth $10 trillion in just a few years, and as far as I can tell, there's just no
reasonable basis for these claims.
We've reached a point where Big Tech is so desperate to achieve growth that it just
keeps taking these fairly predictable and standard advancements and software and then
applying new buzzwords to the technology, and by the time the bubble should burst, they're
already onto marketing their next buzzword.
I don't want to dismiss the entire potential of AI by calling it the next metaverse, but
look, there's a lot of problems here.
And as of right now, for many people, this technology just kind of comes off as a slightly
more invasive, less accurate version of all these digital personal assistants that have
already become commonplace over the years.
This whole industry right now just seems so flimsy to me.
There's a good chance that everything I say here will age like milk.
I get that.
Maybe we will see a world where most jobs become significantly easier with the assistance
of the machines, where this is a necessity, or maybe the tech is too good and we all lose
our jobs.
I think right now we're seeing a technology that is more style over substance.
There is substance there, but there's just a lot of fluff here.
Or maybe I'm completely wrong, and in five years we'll all have Rosie the Robot in our
homes.
I need to get my robot out of my swimming pool.
I don't think she can breathe.
Again, huge thanks to Ground News for sponsoring today's video.
Go to ground.news slash husk or use the link in the video description to subscribe today.
If you sign up through my link you will get 40% off the vantage plan.
Thank you.

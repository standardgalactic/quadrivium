This week's Moment of Zen is a feed drop for one of Turpentine's biggest shows, The
Cognitive Revolution, hosted by Nathan LeBenz.
Biology and Nathan discussed the evolution, challenges, and role of AI in different realms
like politics, environmentalism, medicine, and more.
There's a lot of new content here about where biology thinks AI is going and interesting
parallels with religion.
Nathan brings up some AI safety concerns in the course of the discussion.
If you like what you hear, check out our other Moment of Zen episodes with Boloji and check
out The Cognitive Revolution.
Please enjoy.
Boloji Srinivasan, welcome to The Cognitive Revolution.
All right.
I feel welcome.
Well, we've got a ton to talk about, you know, obviously you bring a lot of different
perspectives to everything that you think about and work on.
And today I want to just try to muster all those different perspectives onto this, you
know, what I see is really the defining question of our time, which is like, what's up with
AI and, you know, how's it going to turn out?
I thought maybe for starters, I would love to just get your baseline kind of table setting
on how much more AI progress do you expect us to see over the next few years, like how
powerful our AI system is going to become in, again, kind of a relatively short timeline.
And then maybe if you want to take, you know, a bigger stab at it, you could answer that
same question for a longer timeline, like the rest of our lives or whatever.
Sure.
Let me give an abstract answer.
Then let me give a technical answer.
You know, if you look at evolution, we've seen something as complex as flight evolved
independently in birds, bats, and bees.
And even intelligence, we've seen fairly high intelligence in dolphins, in whales, in octopuses,
you know, octopus in particular can do like tool manipulation.
They've got things that are a lot like hands, you know, with tentacles.
And so that indicates that it is plausible that you could have multiple pathways to intelligence,
whether, you know, we have carbon based intelligence, or we could have silicon based intelligence
that just has a totally different form or the fundamental thing is an electromagnetic
wave and data storage as opposed to, you know, DNA and so on, right?
So that's like a plausibility argument in terms of evolution is being so resourceful
that it's invented really complicated things in different ways, okay?
Then in terms of the technical point, I think as of like right now, I should probably date
it as like December 11, 2023, because this field moves so fast, right?
My view is, and maybe you'll have a different view, is that the breakthroughs that are really
needed for something that's like true artificial intelligence that is human independent, right?
Maybe the next step after the Turing test, I've got an article that, you know, we're
writing called the Turing thresholds, which tries to generalize the Turing test to like
the cartage of scale, you know, have you got energy thresholds?
Like what are useful scales beyond that?
And right now, I think that what we call AI is absolutely amazing for environments that
are not time varying or rule varying.
And what I mean by that is, so you kind of have, let's say two large schools of AI, and
obviously there's overlap in terms of the personnel and so on, but there's like the
deep mind school, which has gotten less press recently, but got more press, you know, a
few years ago, and that is game playing, right?
It is, you know, superhuman playing of go without go.
It is, you know, all the video game stuff they've done where they learn at the pixel
level and they don't, they just teach the very basic rules and it figures it out from
there. And it's also, you know, the protein folding stuff and what have you, right?
But in general, I think they're known for reinforcement learning and those kinds of
approaches. I mean, they're good at a lot of things, but that's what I think he finds
known for. Of course, they put out this new model recently, the Gemini model.
So I'm not saying that they're not good at everything, but that's just kind of what
they're maybe most known for.
And then you have the open AI chat, GBT school of generative AI, and it includes stable
diffusion and just as a pioneer, even if, you know, they're not, I don't know how much
they're used right now, but basically, you know, you have the diffusion models for images
and you have large language models and now you have the multimodals that integrate them.
And so the difference, I think with these is the reinforcement learning approaches are
based on an assumption of static rules, like the rules of chess, the rules that go, the
rules of a video game are not changing with time, they're discoverable, they're like the
laws of physics. And similarly, like the body of language where you're learning it, English
is not rapidly time varying. That is to say, the rules of grammar that are implicit aren't
changing, the meanings of words aren't changing very rapidly, you can argue they're changing
over the span of decades or centuries, but not extremely rapidly, right? So therefore,
when you generate a new result, training data from five years ago for English is actually
still fairly valuable, and the same input roughly gives the same output. Now, of course,
there are facts that change with time, like who is the the ruler of England, right, the
Queen of England is passed away now, it's the King of England, right, which is facts
that change with time. But I think more fundamentally is when there's rules that change with time,
you know, you have, for example, changes in law and countries, right? But most interestingly,
perhaps changes in markets, because the same input does not give the same output in a market.
If you try that, then what will happen is there's adversarial behavior on the other side. And
once people see it enough times, they'll see your strategy, and they're going to trade
against you on that, right? And I can get to other technical examples on that, but I think,
and probably people in the space are aware of this, but I think that is a true frontier
is dealing with time varying rule varying systems, as opposed to systems where the implicit rules
are static. Let me pause there. Yeah, I think that makes sense. I think the, you know, in the
very practical, you know, just trying to get as V calls it mundane utility from AI, that is often
kind of cashed out to AI is good at tasks, but it's not good at whole jobs. You know, it can
handle these kind of small things where you can define, you know, what good looks like and tell
it exactly what to do. But in the sort of broader context of, you know, handling things that come
up as they come up, it's definitely not there yet. And I agree that there's likely to be some
synthesis, you know, which is kind of the subject of all the Q star rumors. Recently, I would say
is kind of the, the prospect that there could be already, you know, within the labs, a beginning
of a synthesis between the, I kind of think of it as like harder edged reinforcement learning
systems, you know, that are like small, efficient and deadly versus the like language model systems
that are like kind of slow and soft and, you know, but have a sense of our values, which is really
a remarkable accomplishment that that they're able to have even, you know, an approximation of our
values that seems like reasonably good. So yeah, I think I agree with that framing. But I guess I
would, you know, still wonder like, how far do you think this goes in the near term? Because I have
a lot of uncertainty about that. And I think the field has a lot of uncertainty. You hear people
say, Well, you know, it's never going to get smarter than its training data, you know, it'll
kind of level out where humans are. But we certainly don't see that in the reinforcement
learning side, right? Like once it usually don't take too long at human level of these games,
and then it like blows past human level. Interestingly, you do still see some adversarial
vulnerability, like there's a great paper from the team at FAR AI, and I'm planning to have
Adam Gleave, the head of that organization on soon to talk about that and other things,
where they found like a basically a hack where a really simple, but unexpected attack on the
superhuman go player can defeat it. So you do have these like very interesting vulnerabilities
or kind of lack of adversarial robustness, still kind of wondering like, where do you think that
leaves us in say a three to five years time? Obviously, huge uncertainty on that. It's really
hard to predict something like this. Just to your point, generative AI is generic AI, right?
It's like generically smart, but doesn't have specific intelligence or creativity or facts.
And as you're saying, just like we have, you know, adversarial images
back in full programs that are trained on a certain set of data, and they just give some
weird, you know, pattern that looks like a giraffe, but the algorithm thinks it's a dog,
you can do the same thing for game playing, and you can have out of sample input that can beat,
you know, these very sophisticated reinforcement learners.
And an interesting question is whether that is a fundamental thing, or whether it is a
work aroundable thing. And you'd think it was work aroundable, you know,
because there's probably some robustification because these pictures look like giraffes,
you know, and yet they're being recognized as dogs. See, there's, you would think that
the right proximity metric would group it with giraffes, you know, but maybe there's some,
I don't know, maybe there's some result there. My intuition would be we can probably
robustify these systems so that they are less vulnerable to adversarial input.
But if we can't, then that leads us in a totally different direction,
where these systems are fragile in a fundamental way.
So that's one big branch point is how fragile these systems are, because if they're fragile in
a certain way, then it's almost like you can always kill them, which is kind of good, right,
in a sense, that there's that, you know, almost like the, you know, the 50 IQ, 100 IQ, 150 IQ
thing, like the, the meme. Yeah, the meme, right. So the 50 IQ guys like these machines will never
be as creative as humans or whatever. 100 IQ is look at all the things they can do. The 150 IQ
is like, well, there's some like equivalent equivalent result, you know, that's like some
impossibility proof that shows that we the dimensional space of a giraffe is too high,
and we can't actually learn what a true giraffe, I don't think that's true.
But maybe it's true from the perspective of how these learners are working,
because my understanding is people have been trying, and I mean, I'm not the cutting edge of
this. So, you know, maybe something, but my understanding is we haven't yet been able to
robustify these models against adversarial input. Am I wrong about that?
Yeah, that's definitely right.
We'll continue our interview in a moment after a word from our sponsors.
There's no single architecture, as far as I know, that is demonstrably robust. And on the contrary,
you know, even with language models, there's a, we did a whole episode on the universal jailbreak,
where, especially if you have access to the weights, not to change the weights, but just to
kind of probe around in the weights, then you have a really hard time, you know, guaranteeing
any sort of robustness. The conjecture is, see, for humans, you can't like, mirror their brain
and analyze it. Okay, but we have enough humans that we've got things like optical illusions,
stuff like that, that works on enough humans, and our brains aren't changing enough, right?
A conjecture is, if you had, as you said, open weights, open weights mean safety, because if you
have open weights, you can always reverse engineer adversarial input, and then you can always break
the system. Conjecture. Yeah, there's I, that's again, with Adam from far AI, I'm really interested
to get into that, because they are starting to study, as I understand it, kind of proto scaling
laws for adversarial robustness. And I think a huge question there is, what are the kind of
frontiers of possibility there? Like, do you need, you know, how do the orders of magnitude work,
right? Do you need another 10x as much adversarial training to half the rate of your
adversarial failures? And if so, you know, can we generate that many, it may always sort of
be fleeting. So far AI, and they are, they're working on cutting edge of adversarial input.
Yeah, they're the group that did the attack on the Alpha Go model, and found that like, you know,
and what was really interesting about that, I mean, multiple things, right? First, that they could
beat a super human Go player at all. But second, that the technique that they used would not work
at all if playing a quality human. Or is, you know, it's a strategy that is trivial to beat if
you're a quality human Go player. But the Alpha Go is just totally blind to it.
You know, that's why I say the conjecture is, if you have the model,
then you can generate the adversarial input. And then so if that is true, and that itself is
an important conjecture about AI safety, right? Because if open weights are inherently something
where you can generate adversarial input from that and break or crash or defeat the AI,
then that AI is not omnipotent, right? You have some power words, you can speak to it, almost
like magical words, that'll just make it, power down, so to speak, right? It's like those movies
where the monsters can't see you if you stand really still, or if you, you don't make a noise or
something like that, right? They're very powerful on Dimension X, but they're very weak on Dimension
1. A kind of an obvious point, but you know, I'm not sure how important it's going to be in the
future. Your next question was on like, you know, humanoid robots and so on. And before we get to
that, maybe obviously, but all of these models are trained on things that we can easily record,
which are sights and sounds, right? But touch and taste and smell, we don't have amazing datasets
on those. Well, I mean, there's some haptic stuff, right? There's, there's probably some,
you know, some work on taste and smell and so on. But those, there's five senses, right? I wonder if
there's something like that where you might be like, okay, how are you going to out smell, you know,
a robot or something like that? Well, dogs actually have a very powerful sense of smell,
and that's being very important for them, you know? And it may turn out that there's,
maybe it's just that we just haven't collected the data, and it could become a much better
smeller or whatever, or, you know, taster than anything else. I wouldn't be surprised. It could
be a much better wine taster because you can do molecular diagnostics. But it's just kind of,
I just use that as an analogy to say there's areas of the human experience that we haven't
yet quantified. And maybe it's just the opera term is yet, okay? But there's areas of the
human experience we haven't yet quantified, which are also an area that AIs at least are not yet
capable at. Yeah, I guess maybe my expectation boils down to, I think the really powerful systems
are probably likely to mix architectures in some sort of ensemble, you know, when you think about
just the structure of the brain, it's not, I mean, there certainly are aspects of it that are repeated,
right? You look at the frontal cortex, and it's like, there is kind of this, you know, unit that
gets repeated over and over again, in a sense, that's kind of analogous to say the transformer block
that just gets, you know, stacked layer on layer. But it is striking in a transformer that it's
basically the same exact mechanism at every layer that's doing kind of all the different kinds of
processing. And so whatever weaknesses that structure has, and you know, with the transformer
and the attention mechanism, there's like some pretty profound ones like finite context window,
you know, you kind of need, I would think a different sort of architecture with a little bit
of a different strength and weakness profile to complement that in such a way that, you know,
kind of more similar to like a biological system where you kind of have this like dynamic feedback
where, you know, if we have obviously, you know, thinking fast and slow and all sorts of different
modules in the brain, and they kind of cross regulate each other and don't let any one system,
you know, go totally, you know, down the wrong path on its own, right, without something kind of
coming back and trying to override that. It seems to me like that's a big part of what is missing
from the current crop of AIs in terms of their robustness. And I don't know how long that takes
to show up. But we are starting to see some, you know, possibly, you know, I think people are maybe
thinking about this a little bit the wrong way. They're just in the last couple of weeks, there's
been a number of papers that are really looking at the state space model, kind of alternative,
it's being framed as an alternative to the transformer. But when I see that, I'm much more
like, it's probably a compliment to the transformer or, you know, these two things probably get
integrated in some form, because to the degree that they do have very different strengths and
weaknesses, ultimately, you're going to want the best of both in a robust system, certainly if you're
trying to make an agent, certainly if you're trying to make, you know, a humanoid robot that can go
around your house and like do useful work, but also be robust enough that it doesn't, you get tricked
into attacking your kid or your dog or, you know, whatever, you're going to want to have more checks
and balances than just kind of a single stack of, you know, the same block over and over again.
Well, so I know Boston Dynamics with their legged robots is all control theory and it's not
classical ML development. It's really interesting to see how they've accomplished it. And they do
have essentially a state space model where they have a big position vector that's got all the
coordinates of all the joints and then a bunch of matrix algebra to figure out how this thing is
moving and all the feedback control and so on there. And it's more complicated than that, but
that's, you know, I think the V1 of it. Sorry, it was there. I wasn't following this though.
Are you saying that there's papers that are integrating that with the kind of gender to
eye transformer model? You know, what's like, what's a good citation for me to look at?
Yeah, starting to, we did an episode, for example, with one of the technology leads at Skydio,
the, you know, the US is champion drone maker. And they have kind of a similar thing where
they have built over, you know, a decade, right, a fully explicit multiple orders of,
you know, spanning multiple orders of magnitude control stack. And now over the top of that,
they're starting to layer this kind of, you know, it's not exactly generative AI in their case,
because they're not like generating content, but it's kind of the high level, you know,
can I give the thing verbal instructions, have it go out and kind of understand, okay, like,
this is a bridge, I'm supposed to kind of, you know, survey the bridge and translate those high
level instructions to a plan, and then use the lower level explicit code that is fully deterministic,
and, you know, runs on control theory and all that kind of stuff to actually execute the plan at
a low level. But also, you know, at times like surface errors up to the top and say like, hey,
we've got a problem, you know, whatever, I'm not able to do it, you know, can you now,
at the higher level, the semantic layer, adjust the plan. That stuff is starting to happen in
multiple domains, I would say. Yeah. And so I think that makes sense is basically it's like,
gender AI is almost the front end. And then you have almost like an assembly, like,
you give instructions to Figma, and the objects there are their shapes and their images. And so
there's not it's not text, you give instructions to a drone, and the objects are like GPS coordinates
and paths and so on. And so you are generating structures that are in a different domain, or
it's like in VR, you're generating 3d structures again, as opposed to text. And then that compute
engine takes those three structures and does something with them in a much more rules based
way. So you have like a statistical user friendly front end with a generative AI, and then you have
a more deterministic, or usually totally deterministic, almost like assembly language
backend that actually takes that and does that's what you're saying, right? Yeah, pretty much.
And I would say there's another analogy to just, again, our biological experience where it's like,
I'm, you know, sort of in a semi conscious level, right, I kind of think about what I want to do.
But the low level movements of the hand, you know, are both like not conscious. And also,
you know, if I do encounter some pain, or you know, hit some, you know, hot item or whatever,
like, there's a quick reaction that's sort of mediated by a lower level control system.
And then that fires back up to the brain and is like, Hey, you know, we need a new plan here.
So that is only starting to come into focus, I think with, you know, because obviously these,
I mean, it's amazing, as you said, it's all moving so fast. What is always striking to me,
I just, and I kind of like recite timelines to myself almost as like a mantra, right? Like,
the first instruction following AI that hit the public was just January 2022. That was
OpenAI's Text of NGOO2 was the first one where you could say like, do X and it would do X,
as opposed to having, you know, an elaborate prompt engineering type of setup.
GPT-4, you know, just a little over a year ago, finished training, not even a year that it's
been in the public. And, you know, it has been amazing to see how quickly this kind of technology
is being integrated into those systems, but it's definitely still very much a work in progress.
Yeah, I mean, the tricky part is, like the training data and so on, like a large existing
scale company like a Figma or DJI that has millions or billions of user sessions will have
a much easier time training and they have a unique data set. And then everybody else will
not be able to do that. So there is actually almost like, I mean, a return on scale where
the massive data set, if you've got a massive clean data set and a unique domain that lots of
people are using, then you can crush it. And if you don't, I suppose, I mean, there's lots of
people who work on zero shot stuff and sort of sort of, but it still strikes me that there'll
probably be an advantage to see those sessions. I find it hard to believe that you could generate
a really good drone command language without lots of drone flight paths, but you can see.
And where it doesn't exist, people are, obviously, need deep pockets for this, but the likes of
Google are starting to just grind out the generation of that, right? They've got their kind of
test kitchen, which is a literal physical kitchen at Google, where the robots go around and do
tasks. And when they get stuck, my understanding of their kind of critical path, as I understand,
they understand it, is robots going to get stuck. We'll have a human operator remotely
operate the robot to show what to do. And then that data becomes the bridge from what the robot
can't do to what it's supposed to learn to do next time. And they're going to need a lot of that,
for sure. But they increasingly have, I don't know exactly how many robots they have now, but
last I talked to someone there, it was like, into the dozens. And presumably, they're continuing
to scale that. I think they just view that they can probably brute force it to the point where
it's good enough to put out into the world. And then very much like a Waymo or a cruise or whatever,
they probably still have remote operators, even when the robot is in your home,
you know, when it encounters something that it doesn't know what to do about,
raise that alarm, get the human supervision to help it over the hump, and then, you know,
obviously, that's where you really get the scale that you're talking about.
This raises a couple of questions I wanted to ask that are conceptual. So, you know, obviously,
there's huge questions around like, again, highest level, how is all this going to play out?
One big debate is, to what degree does AI favor the incumbents? To what degree, you know,
does it enable startups? Obviously, it's both. But, you know, if you're interested in your
perspective on that, also really interested in your perspective on like, offense versus defense,
that's something that a lot of people now and in the future, right, that seems like it probably
really matters a lot, whether it's a more offense enabling or defense enabling technology. So,
I love your take on those two dimensions. Hey, we'll continue our interview in a moment after
a word from our sponsors. If you're a startup founder or executive running a growing business,
you know that as you scale, your systems break down and the cracks start to show.
If this resonates with you, there are three numbers you need to know. 36,000, 25, and 1.
36,000. That's the number of businesses which have upgraded to NetSuite by Oracle. NetSuite is the
number one cloud financial system, streamline accounting, financial management, inventory,
HR, and more. 25. NetSuite turns 25 this year. That's 25 years of helping businesses do more
with less, close their books in days, not weeks, and drive down costs. One, because your business
is one of a kind, so you get a customized solution for all your KPIs in one efficient system with
one source of truth. Manage risk, get reliable forecasts, and improve margins, everything you
need all in one place. Right now, download NetSuite's popular KPI checklist designed to give you
consistently excellent performance, absolutely free at netsuite.com slash zen. That's netsuite.com
to get your own KPI checklist, netsuite.com slash zen.
So, like offense or defense in the sense of disenabled disruptors or incumbents?
Both in business and in potentially outright conflict. I'd be interested to hear your analysis
on both. All right. A lot of views on this. Obviously, if you've got a competent existing
tech CEO who's still in their prime, like Amjad of Replet, or
you know, Dillon Field of Figma, or, you know, those are two who have thought of who are very
good and, you know, will be on top of it. Amjad is very early on integrating AI into Replet,
and it's basically built that into an AI-first company, which is really impressive.
Those are folks who cleanly made a pivot. It's as big or bigger than, comparable to, I would say,
the pivot from desktop to mobile that broke a bunch of companies in the late 2000s and early
2010s. Like Facebook in 2012 had no mobile revenue, roughly, at the time of their IPO,
and then they had to like redo the whole thing. And it's hard to turn a company 90 degrees when
something new like that hits, you know? Those that are run by kind of tech CEOs in their prime
will adapt and will AI-ify their existing services. And the question is, obviously,
there's new things that are coming out, like pika and character.ai. There's some like really
good stuff that's out there. The question is, you know, will the disruption be allowed to happen
in the US regulatory environment? And so my view is actually that, you know, so this is from like
the network state book, right? I talk about, you know, people talk about a multipolar world or
unipolar world. The political axis is actually really important in my view for thinking about
whether AI will be allowed to disrupt, okay? Because we'll get to this probably later,
but the 640K of compute is enough for everyone executive order, you know, 640K of memory,
the apocryphal, he didn't bill gates and actually say it, but that quote kind of
gives a certain mindset about computing. That should be enough for everybody. So the 10 to the 26
of compute should be enough for everyone bill. I actually think it's very bad. And I think it's
just the beginning of their attempts to build like a software FDA, okay, to decelerate, control,
regulate, red tape, the entire space, just like how, you know, the threat of nuclear terrorism
got turned into the TSA. The threat of, you know, terminators and AGI gets turned into a million
rules on whether you can set up servers and this last free sector of the economy is strangled or at
least controlled within the territory controlled by Washington DC. Now, why does this relate
to the political? Well, obviously this, you know, you can just spend your entire life just tracking
AI papers and that's moving like at the speed of light like this, right? What's also happening
as you can kind of see in your peripheral vision is there's political developments that are happening
at the speed of light much faster than they've happened in our lifespans. Like there's more,
you just notice more wars, more serious online conflicts like, you know, there's a sovereign
debt crisis, all of those things that can show graph after graph of things looking like their own
types of singularities, you know, like military debts are way up, you know, the long piece that
Stephen Pinker showed it's looking like a you that suddenly way up after Ukraine and some of
these other wars are happening, unfortunately, right? Interest payments rush way up to the side.
What's my point? Point is, I think that the world is going to become from the Pax Americana world
of just like basically one superpower, hyperpower that we grew up in from 91 to 2021 roughly,
that we're going to get a specifically tripolar world, not unipolar, not bipolar, not multipolar,
but tripolar. And those three poles, I kind of think of as NYT, CCP, BTC, or you could think of
them as, and those are just certain labels that are associated with them, but they're roughly
US tech, the US environment, China tech and China environment, and global tech and the global
environment. And why do I identify BTC and crypto and so on with global tech? Because that's a tech
that decentralized out of the US. And right now people think of crypto as finance, but it's also
financiers. Okay, and in this next run up, it is, I think quite likely about depending on how you
count, between a third to a half of the world's billionaires will be crypto. Okay, around, you
know, I calculated this a while back around Bitcoin at a few hundred thousands, around a third to a
half of the world's billionaires are crypto. That's the unlocked pool of capital. And those are the
people who do not bow to DC or Beijing. And they might by the way be Indians or Israelis or every
other demographic in the world, or they could be American libertarians, or they could be Chinese
liberals like Jack Ma were pushed out of Beijing sphere. Okay, or the next Jack Ma, you know,
Jack Ma himself may not be able to do too much. Okay, that group of people who are, let's say,
the dissident technologists who are not going to just kneel to anything that comes out of
Washington DC or Beijing, that is the that's decentralized AI. That's crypto. That's decentralized
social media. So you can think of it as, you know, where we talked about in the recent pirate
wires podcast, freedom to speak with decentralized censorship resistant social media, freedom to
transact with cryptocurrency, freedom to compute with open source AI, and no compute limits. Okay,
that's a freedom movement. And that's like the same spirit as a pirate bay, the same spirit
as BitTorrent, the same spirit as Bitcoin, the same spirit as peer to peer and into an encryption.
That's a very different spirit than having Kamala Harris regulate a superintelligence
or signing it over to Xi Jinping thought. And the reason I say this is, I think that that group
of people, of which I think Indians and Israelis will be a very prominent, maybe a plurality, right,
just because the sheer quantity of Indians are like the third sort of big group that's kind of
coming up. And they're relatively underpriced, you know, China is, I don't say it's price
to perfection. But it's something that people when I say priced, I mean, people were dismissive
of China even up until 2019. And then it was after 2020, if you look at people started to take China
seriously. And I mean, that is the West Coast tech people knew that China actually had a plus tech
companies and was a very strong competitor. But the East Coast still thought of them as a third
world country until after COVID, when now, you know, the East Coast was sort of threatened by them
politically. And it wasn't just blue collars, but blue America that was threatened by China.
And so that's why the reaction to China went from Oh, who cares, just taking some manufacturing
jobs to this is an empire that can contend with us for control of the world. That's why the hostility
is ramped up in my view. There's a lot of other dimensions to it. But that's a big part of it.
So India is also kind of there, but it's like the third. And India is not going to play for number
one or number two. But India and Israel, if you look at like tech founders, depending on how you
count, especially if you include diasporas, it's on the order of 30 to 50% of tech founders, right.
And it's obviously some, you know, very good tech CEOs and, you know, Satya and Sundar and investors
and whatnot. Those are folks Indians do not want about to DC or to Beijing, neither do Israelis
for all kinds of reasons, even if Israel has to, you know, take some direction from the US now,
they're bristling at it, right. And then a bunch of other countries don't. So the question is,
who breaks away? And now we get to your point on the reason I had to say that is that that's
preface, the political environment is a tripolar thing of US tech and US regulated Chinese tech
and China regulated and global tech that's free. Okay, of course, there's, even though I identify
those three polls, there's of course boundary regions. EAC is actually on the boundary of US
tech and decentralized tech, you know, and I'm sure there'll be some Chinese thing that comes out
that is also on the boundary there. For example, Binance is on the boundary of Chinese tech
and global and decentralized tech, if that makes any sense, right? There's probably others Apple is
actually on the boundary of US tech and Chinese tech, because they make all of their stuff in
China, right? So these are not totally disjoint groups, but there's boundary areas, but you can
think about why is this third group so important in my view? Both the Chinese group and the
decentralized group will be very strong competition for the American group for totally different reasons.
China has things like WeChat, these super apps. I mean, obviously not likely, but like
WeChat is a super app, but they also have, for example, their digital yuan, right? They have
the largest cleanest data sets in the world that are constantly updated in real time that they can
mandate their entire population opt into. And most of the Chinese language speaking people are under
their ambit, right? So that doesn't include Taiwan, doesn't include Singapore, doesn't include,
you know, some of the Chinese yaspera, but basically anything that's happening in Chinese
for 99% of it, 95, whatever the ratio is, they can see it and they can coerce it and they can control
it. So they can tell all of their people, okay, here's five bucks in, you know, digital yuan,
do this micro task, okay? All of these digital blue collar jobs, both China and India, I think,
can do quite a lot with that and they'll come back to it. So they can make their people do
immense amounts of training data, clean up lots of data sets. Once it's clear that you have to
build this and do this, they can just kind of execute on that. And they can also deploy. I mean,
in many ways, the US is still very strong in digital technology, but in the physical world,
it's terrible because of all the regulations, because of the nimbyism and so on. It's not like
that in China. So anything which kind of works in the US at a physical level, like the Boston
Dynamics stuff, they're already cloning it in China and they can scale it out in the physical
world. You already have drones, little sidewalk drone things that come to your hotel room and
drop things off. That's already like very common in China. In many ways, it's already ahead if you
go to the Chinese cities. So the Chinese version of AI is ultra centralized, more centralized,
more monitoring, less privacy and so on than the American version. And therefore they will have
potentially better data sets, at least for the Chinese population. And so we chat AI, I don't
even know what it's going to be, but it'll be probably really good. It'll also be really dangerous
in other words. Then the decentralized sphere has power for a different reason, because the
decentralized sphere can train on full Hollywood movies. It can train on all books, all mp3s and
just say, screw all this copyright stuff, like what Psyhub and Libgen are doing, because all the
copyright, first of all, it's like Disney lobbying politicians to put another 60 or 70 or 90, I
don't even know what it is, some crazy amount on copyright, so you can keep milking this stuff and
it doesn't go into public domain number one. And second, you know how Hollywood was built in the
first place? It was all patent copyright and IP violation. Essentially Edison had all the patents,
he's in New Jersey-ish, okay, that East Coast area. And Neil Gabler has this great book called
An Empire of Their Own, where he talks about how immigrant populations, you know, the Jewish
community in particular, and also others, went to Southern California in part, so they could just
make movies that Edison coming and suing them for all the patents and so on and so forth.
And they made enough money that they could fight those battles in court, and that's how they built
Hollywood, okay? So, you know, one of my big theses is history is running in reverse, and I can get
to why, but it's like 1950s and mirror moment, you go more decentralized backwards and forwards
in time is like, you have these huge centralized states like the US and USSR and China, you know,
all these things exist, and their fist relaxes as you go forwards and backwards in time. For example,
backwards in time, the Western frontier closed, and forwards in time, the Eastern frontier opens.
Backwards in time, you have the robber barons, forwards in time, you have the tech billionaires.
Backwards in time, you have Spanish flu, forwards in time, you have COVID-19. And I've got dozens
of examples of this in the book. The point is that if you go backwards in time, the ability to
enforce patents and copyrights and so on starts dropping off, right? You have much more of a
Grand Theft Auto environment. And you go forwards in time, and that's happening again. So, India
in particular, for many years, basically just didn't obey Western patent protections and all these
stupid rules basically, you know, it's a combination of artificial scarcity on the patent side and
artificial regulation on the FDI side. That's a big part of what jacks up drug costs, where these
things cost, you know, only cents to manufacturing, they sell them for so much money. All the delays,
of course, that are imposed on the process, the only way they can pay for the manufacturers to
take it out of your hide. What India did is they just said, we're not going to obey any of that stuff.
So, they have a whole massive generic drugs and biotech industry that arose because they built
all the skills for that. That's why they could do their own vaccine during COVID. And they're one
of the biggest biotech industries in the world because they said screw Western restrictive
IPs and other stuff. So, I was actually talking with the founder of Flipkart, that's India's largest
exit. And we were talking about this a few months ago. And what we want is for India and other
countries like it, do something similar, not just generic drugs, but generic AI, meaning
just let people train on Hollywood movies, let them train on full songs, let them train on every
book, let them train on anything. And you know what, sue them in India, right? And have the servers
in India and let people also train models in India, because that's something that can build up a
domestic industry with skills that the rest of the world, you know, people will want the model
output, they'll want to use the software service there, and they'll be fighting in court on the
back. And this is similar to how all of the record companies fought Napster and Kazaa and so on,
but they couldn't take down Spotify. Do you know that story? Do you remember that?
Basically, because Spotify was legitimately, you know, a European company and that a combination of
execution and, you know, negotiation, they couldn't take them down. They did take down Napster,
they took down Limewire, they took down Groove Shark, and Kazaa had Estonians, I don't know
exactly how it was incorporated, but it was probably two US proximal, and that's why they
were able to get them. But Spotify was far enough away that they couldn't just sue them and they
actually genuinely had European traction. That's why the RA had to negotiate. So being far away
from San Francisco may also be an advantage in AI, because it means you're far away from the blue
city in the blue state in the Union. This relates to another really important point.
When you actually think about deploying AI, there's those jobs you can disrupt that are not
regulated jobs, like, you know, obviously, programmers are not, thank God, you don't need
a license to be a programmer, but programmers adopt this kind of stuff naturally, right? So
get up, co-pilot, replete, we just boom, use it, and now it's amplified intelligence, okay?
But a lot of other jobs, there's some that are unionized and then some that are licensed, right?
So Hollywood screenwriters are complaining, right? Journalists are complaining,
artists are complaining. This is a good chunk of blue America. If you add in licensed jobs,
like lawyers and doctors and bureaucrats, right? You know, especially lawyers and doctors are very
politically powerful, MDs and JDs. They have strong lobbying organizations, AMA and, you know,
ABA and so on. Basically, AI is part of the economic apocalypse for blue America,
okay? It just attacks these overpriced jobs. They say overpriced relative to
what an Indian could do with an Android phone, what a South American could do with an Android phone,
what someone in the Middle East or the Midwest could do with an Android phone.
Now, those folks have, you know, been armed with generative AI. They can do way more.
They're ready to work, they're ready to work for much less money, and they're a massive threat
to blue America. Blue America is now feeling like the blue collars of 10 or 20 years ago, where
the blue collars had their jobs, you know, going to China and other places, right? And they were mad
about that. Factories got shut down and so on. That's about to happen to blue America, already
happening, okay? And so that's going to mean a political backlash by blue America of protectionism,
again, already happening. And the AI safety stuff, that's a whole separate thing, but it's going to
be used. I'm going to use a phrase, and I hope you won't be offended by this. Have you heard
the phrase, useful idiots, like by Lenin or whatever, okay? It basically means like, okay,
those guys, you know, they're useful idiots for communism and so on. So there's, let me put it
like naive people who think that the US government is interested in AI safety, are trying to give a
lot of power to the US government. And the reason is they haven't actually thought through from
first principles, what is the most powerful action in the world, a convective. They're trying to get
power to the US government to regulate AI safety. But the government doesn't care about safety of
anything. They literally funded the COVID virus in Wuhan, credibly alleged, right? There's at
least it is a reasonable hypothesis based on a lot of the data. Matt Ridley wrote a whole book on
this. There's a lot of data that indicates a lot of scientists believe it. I'm actually like a
bioinformatics genomics guy. If you look at the sequences, there is a gap and a jump where it looks
like this thing could have been engineered or partially engineered or evolved. There's Peter
Dazak. There's Zeng Lishi. There's actually a lot of evidence here. So the US government and the
Chinese government are responsible for an existential risk by studying it, they created it.
They're responsible for risking nuclear war with Russia over this piece of land in eastern Ukraine,
which probably is going to get wound down. So they don't care about your safety at all.
These are immediate things where we can show and there's nobody who's punished for this,
nobody who's fired for this, literally rolling the dice on millions, hundreds of millions of
people's lives has not been punished. In fact, it's not even talked about. We're past the pandemic
and these institutions can't be punished. So they don't care about AI safety. What they care about
is AI control. And so the people in tech who are like, well, the government will guarantee AI
safety. That's actually what we're going to actually get is something on the current path,
like what happened with nuclear technology, where you got nuclear weapons, but not nuclear power,
or at least not to the scale that we could have had it, right? We could have had much cheaper
energy for everything. Instead, we got the militarization and the regulation and the
deceleration worst of all worlds where you can blow people up, but you can't build nuclear power
plants. And like even getting into nuclear technology, forget about nuclear power plants.
We don't have nuclear submarines. We don't have nuclear planes, all that kind of stuff. I don't
have nuclear planes are possible, but I do know nuclear submarines are possible. You can do a lot
more cruise ships, a lot more stuff like that. You could probably have nuclear trains. You have
to look at exactly how big those are. I don't know exactly how big those engines are and what
the spies, but I wouldn't be surprised if you could. We don't have that. Why don't we have that?
Because we had the wrong fear-driven regulation in the early 70s.
Putting it all together, I think that the current AI safety stuff is similar to nuclear safety stuff
that the US government has a terrible track record on safety in general. It doesn't care about it.
It funded the COVID virus, incredibly alleged. It definitely risked nuclear war with Russia recently.
Hot war with Russia was the red line we were not supposed to cross, and we're now like way
into that. It doesn't care about AI safety, it doesn't care about your safety. It's also not
even good at regulating. What it cares about is control. We are going to have potentially a bad
outcome where Silicon Valley and San Francisco is the Xerox Park of AI. Maybe that's too strong,
okay? Basically, it develops it, and there's a lot of things it can't do because it lobbied for
this regulation that is going to come back and choke it. Then there are other two spheres
we'll push ahead because it's not about the technology, it's also about the political layer.
You know the Steve Jobs saying, actually Alan Kay by way of Steve Jobs, if you're
really serious about software, you need your own hardware, right? So if you're really serious
about technology, you need your own sovereignty. Because what the AI people haven't thought about
is there's a platform beneath you, which is not just compute, it is regulate. It's a law, okay?
And it's a law doesn't allow you to compute so much for all of your stuff above that.
And I know you're saying, oh, it's only a 10 to 26 compute ban and so on and so forth.
Have you seen the first IRS tax form? It's always, always super simple. It's only the super, super,
super rich who's we're going to get in at first doesn't matter to you. So that's called
boiling the frog slowly. There's a million, you know, slippery slope, slippery slope isn't a fallacy.
It's literally how things work, right? You know, Apple, one of the reasons they, you know,
they talk about not setting a precedent. Zuck starts off is a very hard line on setting precedents
because he understands the long-term equivalent of setting a precedent, right? The precedent setting
is that they're setting up a software FDA and they're going to and DC is so energized on this
because they know how much social media disrupted them. That's why they're on the attack on crypto
and AI. That's why they're on the attack on self-driving cars. They want to freeze the current
social order and amber domestically and globally. So they think they can sanction China and stop it
from developing chips. They think they can impose regulations on the US and stop it from developing
AI, but they can't. And also, by the way, they're totally schizophrenic on this, where when they're
talking about China, they're like, we're going to stop their chips to make sure America is a global
leader. This is Gina Raimondo who's English. And then domestically, they're like, we're going to
regulate you so you stop accelerating AI. We're not about AI acceleration. EAC is weird over there.
Okay. So think about how schizophrenic that is. Okay, you're going to be far ahead of China.
We're also going to be make sure to control the US. So they want to try and slow what they actually
want is to freeze the current system and amber, try to go back to pre 2007 before all these tech
guys disrupted everything. But that's not what's going to happen. So, but they're going to try to
do it. And so everybody who's still loyal to the DC sphere, which includes an enormous chunk
of AI people. And because they're all in a lot of them in San Francisco, right? And the political
chaos of the last few years was not sufficient for them to relocate yet. Not all of them. I mean,
Elon is in Texas. And that it may turn out that grok, for example, and what they're doing there,
because he's a very legit, I mean, you know, he's Elon. So he's capable of doing a lot. He's very
early on opening AI, he understands, you know, the right, it may turn out that grok
becomes red AI, or the community around that, you know, an opening eye and deep mind or still blue
AI. And we have Chinese and we're going to have decentralized AI. Okay, let me pause there. I
know there's a big download. Well, I for starters, I would say, broadly, I have a pretty similar
intellectual, you know, tendency as you, I would broadly describe myself as a techno
optimist, libertarian, just about every issue. And I think your analysis of the dynamics is
super interesting. And I think it, you know, a lot of it sounds pretty plausible, although I'll
kind of float a couple of things that I think maybe bucking the trend. But I think it's maybe
useful to kind of try to separate this into scenarios. Because all the analysis that you're
describing here seem, if I understand it correctly, it seems to have the implicit assumption
that the AI itself is not going to get super powerful or hard to control. It's like, if we
assume that it's kind of a normal technology, then you're off to the races on this analysis. And
then we can get into the fine points. But I do want to take at least one moment and say,
how confident are you on that? Because if it's a totally different kind of technology from other
technologies that we've seen, if it's more, you raise the gain of function research example,
if it's that sort of technology that has these sort of non-local possible impacts or
self-reinforcing kind of dynamics, which need not be like an Eliezer style snap of the fingers fume,
but even over, say, a decade, let's imagine that over the next 10 years that AI's kind of
multiple architectures develop and they sort of get integrated and we have something that
kind of looks like robust, silicon-based intelligence, maybe not totally robust, but as
robust or more robust than us and running faster and the kind of thing that can do lots of full
jobs or maybe even be tech CEOs, then it kind of feels like a lot of this analysis probably
doesn't hold, because we're just in a totally different regime that is just extremely hard
to predict. And I guess I wonder, first of all, do you agree with that? There seems to be a big
fork in the road there that's like, just how fast and how powerful do these AI's become super
powerful or do they not? And if they don't, then yeah, I think we're much more into real
politic type of analysis, but I'm not at all confident in that. To me, it feels like there's
a very real chance that AI of 10 years from now is, and by the way, this is what the leaders
are saying, right? I mean, open AI is saying this, Anthropic is saying this, Demis and Shane
Legge are certainly saying things like this. It seems like they expect that we will have AI's
that are more powerful than any individual human and that that becomes the bigger question
than anything else. So do you agree with that kind of division of scenarios? First of all,
and then maybe you could kind of say like how likely you think each one is. And obviously,
that one where it takes off is like super hard to analyze. And I also definitely think it is
worth analyzing this scenario where it doesn't take off. But I just wanted to flag that it seems
like there's a, you know, there's a big, if you talk to the AI safety people, any world in which
it's like, you know, we're suing Indian AI firms in Indian court over like IP is like a normal world
in their mind, right? And that's not the kind of world that they're most worried about.
I think that there have been some plausible sounding things that have been said. But I want
to just kind of talk about a few technical counter arguments, mathematical or physical,
that constrain what is possible. Okay. And actually, Martin Casado and Vijay and I are
working on a long thing on this where, you know, Vijay did folding at home, he's a physicist,
Martin, sold in the Syrah for, you know, a billion dollars and knows a lot about how a
Stuxnet like thing could work at the systems level. And I've thought about it from other angles and,
you know, and some of the math stuff that I'll get to. So for example, one thing, and I'm going to
give a bunch of different technical arguments, and then let's kind of combine them. Okay.
One thing that's being talked about is, if you have a super intelligence, it can
double it right for a million years, and then it can make one move and it's going to outthink you
all the time and so on and so forth. Okay. Well, if you're familiar with the math of chaos,
or the math of turbulence, there are limits to even very simple systems that you can set up,
where they can become very unpredictable quite quickly. Okay. And so you can, if you want to,
engineer a system where you have very rapid diversions of predictability, so that, I don't know,
it's like the heat depth of the universe before you can predict out in timestamps.
Do you understand what I'm saying? Right? This is sort of akin to like a wolf from like simple,
even simple rules can generate patterns such that you can't know them without literally computing
them. Yeah, exactly. Right. So at least right now with chaos and turbulence, you can get things
that are extremely provably difficult to forecast without actually doing it. Okay. You know, I can
make that argument quantitative, but that's just something to look at, right? It's almost like a
delta epsilon challenge from calculus, like, okay, how hard do you want me to make this to predict?
Okay, I can set up a problem that is, that is like that, right? It's basically extreme sensitivity
to initial conditions lead to extreme divergence in outcomes. So you could design systems to be
chaotic, that might be AI immune, because they can't be forecasted that well, you have to kind of
react to them in real time. The ultimate version of this is not even a chaotic system, it's a
cryptographic system, where I've got a whole slide deck on this, how AI makes everything fake,
easy to fake, crypto makes it hard to fake again. Right? Because crypto in the broader sense of
cryptography, but also in the narrow sense, I think crypto is to cryptography as the internet
is to computer science. It's like the primary place where all this stuff is applied, but obviously
it's not the equivalent. Okay. And AI can fake an image, but it can't fake a digital signature,
unless it can break certain math, you know, and so sort of like a, you know, solve factors,
each problem or something like that. So cryptography is another mathematical thing that
constrains AI, similar to chaos and turbulence, it constrains how much an AI can infer things.
You can't statistically infer it. Okay, you need to actually have the private key to solve that
equation. So that is another math. So I'm going to rules of math, right? Math is very powerful
because you can make proofs that will work no matter what devices we come up with. Okay,
you start to put an AI in a cage, it can't predict beyond a certain amount because of chaos and
turbulence math, it cannot solve certain equations unless it has a private key is because of what
we know about cryptography math. Okay, again, if somebody proves P equals NP, some of this stuff
breaks down, but this is when the bounds of our mathematical knowledge right now, physics wise,
physical friction exists, a lot of physical friction exists. And a huge amount of the writing on AI
assumes by guys like a laser who I like, I don't, I don't dislike it, you know, but
it is extremely, it's there's two things that really stick out to me about it. First is extremely
theoretical and not empirical. And second, extremely Abrahamic rather than Dharmic or
signing. Okay, white theoretical and not empirical. It's not trivial to turn something from the
computer into a real world thing. Okay, one of the biggest gaps in all of this thinking is what
are the sensors and actuators? Okay, because like if you actually build, you know, I've built
industrial robot systems that you know, 10 years ago, I, you know, a genome sequencing lab with
robots, that's hard. That's physical friction. Okay, and a lot of the AI scenarios seem to basically
say, Oh, it's going to be a self programming Stuxnet that's going to escape and live off the land
and hypnotize people into doing things. Okay, now, each of those is actually really,
really difficult steps. First is self programming Stuxnet, like, this would have to be a computer
virus that can live on any device, despite the fact that Apple or Google can push a software
update to a billion devices, right, a few executives coordinating almost certainly can I mean, the
off switch exists, right? Like, this is actually like the core thing, lots of AI safety guys get
themselves into the mindset that the off switch doesn't exist. But guess what, there's almost
nothing living that we haven't been able to kill. Right, like, can we kill it? This thing exists.
And this is getting back to living off land. A even if you had like something that could solve
some other technical problems that I'll get to it exists as an electromagnetic wave kind of thing
on on a certain, you know, on chips and so on and so forth. It's taking it out in the environment
is like putting a really smart human into outer space. Right, your body just explodes and you die.
Doesn't matter how smart you are, that that strength on this axis, but you're weak on this
axis. And, you know, so strength on the x axis, not strength on the y or the z axis in AI outside,
you know, pour water on it. You know, this is why I mean the 50 IQ, 150 IQ thing, you know,
the 150 IQ way of saying it is it's strong on this x and weak on this x and the 50 IQ way is
pour water on it, disconnect it, you know, turn the power off. Okay, right. Like, it'll, it'll be
very difficult to build a system where you literally cannot turn it off. The closest thing we have
to that is actually not stuck snap. It's Bitcoin. And Bitcoin only exists because millions of humans
keep it going. So you, you need, so that gets the second point living off the land
for an AI to live off land, meaning without human cooperation. Okay, that's the next
Turing threshold in AI to live without human cooperation. It would need to be able to control
robots sufficient to dig or out of the ground, set up data centers and generators and connect them
and defend that against human attack, literally a terminator scenario. Okay, that's a big leap
in terms. I mean, is it completely impossible? I can't say it's completely impossible,
but it's not happening tomorrow. No matter what your AI timelines are, you would need to have
like a billion or hundreds of millions of internet connected autonomous robots that this
Stuxnet AI could hijack that were sufficient to carve or out of the earth and, you know, set up
data centers and make the AI duplicate. We're not there. That's a huge amount of physical friction.
That's AI operating without a human to make itself propagate, right? A human doesn't need
the cooperation of a lizard to self replicate. For an AI to replicate right now, it would need
the cooperation of a human in some sense, because otherwise those humans can kill it because there's
not that many different pieces of, you know, operating systems around the world. I'm just
talking about the practical constraints of our current world, right? You know, actually existing
reality, not AI safety guys, you know, you know, reality where all these things don't exist. There's
just a few operating systems, just a few countries. If everybody is going with torches and search
lights through the internet, it's very hard for a virus to continue. Okay. So A, on the practical
difficulties that there's the technical stuff with, you know, with the chaos and turbulence and
with cryptography itself or AI can't predict and it can't solve certain equations. B, on the physical
difficulties, it probably, I mean, like to be a Stuxnet, Microsoft and Google and so on could kill
it. The off switch exists. Can it live off the land? No, it cannot because it doesn't have, you
know, drones to mine or and stuff out of the ground. And can it like exist without humans? Can it be
this hypnotizing thing? Okay. So the hypnotizing thing, by the way, this is one of the things that's
the most hilarious self fulfilling prophecy in my view. Okay. And no offense anybody listening to
this podcast, but I think the absolutely dumbest kind of tweet that I've seen on AI is, I typed this
in and oh my God, it told me this. Like, I asked it how to make sarin gas and it told me X or whatever.
Right. That's just a search engine. Okay. What, what basically a lot of these people are doing is
they're saying, what if there were people out there that were so impressionable that they would
type things into an AI and, and follow it as if they were hearing voices. And that's actually
not the, the, the model or whatever that's doing it. That's like this AI cult that has evolved
around the world, like a Aum Shinrikyo, you know, that, that hears voices and does like the sarin
gas. The point is an AI can't just like hypnotize people. Those people have to like participate
in it. They're typing things into the machine or whatever. Okay. Now you might say, all right,
let's project out a few years. In a few years, what you have is, you have an AI that is not
just text, but it appears as Jesus. What would, what would AI Jesus do? What would AI Lee Kwan
you do? What would AI George Washington do? So it appears as 3d. Okay. So it's generating that.
It speaks in your language and in a voice. It knows the history of your whole culture. Okay.
That would be very convincing. Absolutely be very convincing. But it still can't exist without
human programmers who are like the priests tending this AI God, whether it's AI Jesus or AI Lee
Kwan you or something like that. The thing about the hypnotization thing that I really want to
poke on that, are you familiar with the concept of the principal Asian problem? Basically in every,
every time you've got like a CEO and a, and a, a worker, or you have a LP and a VC, or you have,
you know, an employer and a contractor, every edge there, there are four possibilities in a
two by two matrix. Win, win, win, lose, lose, win, lose, lose. Okay. And so for example, win, win is,
you know, when, when somebody joins a tech startup, the, the CEO makes a lot of money and so does a
worker. Okay. That's win, win, lose, lose is they both lose money. Win, lose is the CEO makes money
and the employee doesn't lose. Win is the company fails, but the employee got paid a very high salary.
So what equity does is it aligns people. That's where the top console alignment comes from.
It aligns people to the upper left corner of win, win. That's when you have one, one CEO and one
employee. When you have one CEO and two employees, you don't have two squared outcomes. You have two
cubed outcomes because you have win, win, win, win, win, lose, win, lose, lose, etc. Right.
Because all three people can be win or lose. Because CEO can be winner, lose. Employee can
be winner, lose. Employee number two can be winner, lose. If you have N people, rather than three
people, you have two to the N possible outcomes and you have essentially a two by two by two by two
by two by N hypercube of possibilities. Okay. It's all literally just two dimensions on each axis.
There's tons of possible defecting kinds of things that happen there. So that's why in a large company,
there's lose, win coalitions that happen where M people gang up on the other K people and they win
what the other people lose. That's how politics happens. When you've got a startup that's driven
by equity and the biggest payoff, people don't have to try to think, okay, well, I make more money
by politics, we'll make more money by the win, win, win, win, win column because the exit makes
everybody make the most money. That's actually how the opening AI people were able to coordinate
around. We want an $80 billion company. The economics helped find the sell that was actually
the most beneficial to all of them helped them coordinate. Okay. So you search that hypercube.
Okay. That's a point of equity is lining. Still, despite all of this, that that's one of our best
mechanisms for coordinating large numbers of people in the principal agent problem. Despite all
of this, the possibility exists for any of these people to win while the others lose, right with
me so far. And I'll explain why this is important. What that means is those 1000 employees of the
CEO are their own agents with their own payoff functions that are not perfectly aligned with
the CEO's payoff function. As such, there are scenarios under which they will defect and do
other things. Okay. The only way they become like actual limbs, see my hand is not an agent of its
own. It lives or dies with me. Therefore it does exactly what I'm saying at this time. I tell it
to go up, it goes up, tell it to go down, it goes down, sideways, sideways, right. An employee is
not like that. They will do this and this and sideways, sideways up to a certain point. And if
you, if you have them do something that's extremely against their interests, they will not do your
action. Do you understand my point? Okay. That is the difference between an AI hypnotizing humans
versus an AI controlling drones. AI controlling drones is like your hands. They're actually
pieces of your body. There's no defecting. There's no loose wind. They have no mind of their own.
They're literally taking instructions. Okay. They have no payoff function. They will
kill themselves for the hoard. Right. An AI hypnotizing humans has a thousand principal
Asian problems for every thousand humans. And it has to incentivize them to continue and
has to generate huge payoffs. It's like an AI CEO. That's really hard to do. Right. The history
of evolution shows us how hard it is to coordinate multicellular organisms. You have to make them
all live or die as one. Then you get something along these lines. Like an ant colony can coordinate
like that because if the queen doesn't reproduce all the ants, it doesn't matter what they're
having sort of genetic material. Okay. We are not currently set up for those humans to not be able
to reproduce unless the AI reproduces. Do I think we eventually get to a configuration like that?
Maybe. Where you have an AI brain is at the center of civilization and it's coordinating all the
people around it. And every civilization that makes it is capable of crowdfunding and operating
its own AI. That gets me to my other critique of the AI safety guys. I mentioned that the first
critique is very theoretical rather than empirical. The second critique is their Abrahamic rather than
Dharmic or Sinic. Okay. And you know, our background culture influences things in ways we don't even
think about. So much of the paperclip thinking is like a vengeful God will turn you into pillars
of salt, except it's a vengeful, you know, AI God will turn you into paperclips. Okay.
The polytheistic model of many gods as opposed to one God is we're all going to have our own AI
gods and there'll be war of the gods like Zeus and Hera and so on. That's the closest western
version, you know, the paganism that predated, you know, Abrahamic religions, but that's still
there in India. That's still how Indians think. That's why India is sort of people got so woke
that they don't even make large scale cultural generalizations anymore. But it's true that India
is just culturally more amenable to decentralization to, you know, multiple gods rather than one
God in one state. Okay. And then the Chinese model is yet the opposite, like they have like,
I mean, of course they have their tech entrepreneurs and so on, but they're, if India is more
decentralized, China is more centralized, they have like one government and one leader for the
entire civilization. Okay. And, and that the biggest thing that China has done over the last 20 or
30 years is they've taken various, you know, U.S. things and they've made sure that they have their
own Chinese version where they have root. So they take U.S. social media and they made sure they had
root over Sina Weibo. Okay. They make sure they have their own Chinese version of electric cars,
the most Chinese version. So the private keys in the sense are with G. So that means that they also
at a minimum, you combine these two things, you're at a minimum going to get polytheistic AI
of the U.S. and Chinese varieties. And then you add the Indian version on it and you're going to get
quite a few of these different AIs around there. And then you have War of the Gods where maybe
they are good at coordinating humans who, who, you know, take instructions from them,
but they can't live without the humans. And humans are giving input to them. That's a series of
things I could probably make that clearer if I just laid it out in bullets in an essay, but just
to recap it, A, technical reasons like chaos, turbulence, cryptography, why AI is limited
in its ability to predict timeframes and to solve equations, B, practical limits. And AI cannot
easily be a Stuxnet because Microsoft and Google and Apple can install software on a billion devices
and just kill it, right? Like basically guys with torches come. All right. It can't easily live off
the land without humans because they would need hundreds of millions of autonomous robots out there
to control, to mine the ore and, and set the data centers. It can't just hypnotize humans
like it can control drones because of the principal agent problem and the degree of human
defection. To make those humans do that, you'd have to have such massive alignment between the AI
and humans that the humans all know they'll die if the AI dies and vice versa. We're not there.
Maybe we'll be there in like, I don't know, n number of years, but not for a while. That's a
total change in like how states are organized. Okay. Finally, let me just talk about the physics
a little bit more. There's a lot of stuff which is talked about at a very sci-fi book level of
it'll just invent nanomedicine and nanotech and kill us all and so on and so forth. Now look,
I like Robert Freitas, obviously Richard Feynman's a genius and so on and so forth,
but nanotech somehow hasn't been invented yet. Okay. Meaning that, you know, there's a lot of
chemists that have worked in this area. Okay. And a lot of what nanotech is like rebranded chemistry
because those are the molecular machines, you know, for example, DNA polymerase or ribosome,
those are molecular machines that we can get to work at that scale, the evolved ones.
To my knowledge, and I may be wrong about this, I haven't looked at it very, very recently,
we haven't actually been able to make artificial, you know, replicators of the stuff that they're
talking about, which means it's possible that there's some practical difficulty that intervened
between Feynman and Freitas and so on's calculations, right? Just a sheer fact that those
books have came out decades ago and no progress has been made indicates that maybe there's a road
block that wasn't contemplated, right? So you can't just click your fingers and say, boom,
nanomus, and it's sort of like clicking your fingers and saying, boom, time travel, right?
Nanomus and exists. That was a good poke that I had a while ago in a conversation like this,
where the AI guy, AI safety guy on their side was like, well, time travel, that's too implausible.
I'm like, yeah, but you're waiting on the nanotech thing you're thinking is like here,
and you're making so many assumptions there that I want to actually see some more work there. I
want to actually see that nanotech is actually more possible than you think it is. As for, oh,
we just need to mix things in a beaker and make a, you know, virus and so on. You know what is
really, really good at defending against novel viruses, like the human immune, that's something
that's within envelope, right? Like you have evolved to not die and to fight off viruses. Is it possible
that maybe you could make some super virus? I mean, maybe, but again, like humans are really good
and the immune system is really good at that kind of thing. That is what we're set up to do,
right? To adapt to that billions of years of evolution being set up. Physical constraints
are not really contemplated when people talk about these super powerful mathematical constraints,
practical constraints are not contemplated. And I could give more, but I think that was a lot
right there. Let me pause here. Yeah. Let me try to steal man a few things. And then
I do think, you know, it's before too long, I want to kind of get back to the
somewhat less, you know, radically transformative scenarios and ask a few follow up questions on
that too. But I think for starters, I would say the, the sort of Eleazar, you know, he's updated
his thinking over time as well. And I would say probably doesn't get quite enough credit for it
because he's definitely on record, you know, repeatedly saying, yeah, I was kind of expecting
more something from like the deep mind school to pop out and be, you know, wildly overpowered
very quickly. And on the contrary, it seems like we're in more of a slow takeoff type of scenario
where, you know, we've got these, again, like super high surface area kind of suck up all the
knowledge, gradually get better at everything. Some surprises in there, you know, certainly some
emergent properties, if you will accept that term, you know, surprise surprises to the developers
of nothing else, right, that are definitely things we don't fully understand. But it does seem to be a,
you know, more gradual turning up of capability versus some like, you know, super sudden surprise.
But okay, so then what is the alternative? I'm going to try to kind of give you the what I,
what I think of as the most consensus strongest scenario where humans lose track of the future
and or lose control of the future, maybe starting by kind of losing track of the president and then
having that kind of, you know, give way to losing control of the future. And I think within that,
by the way, the, I'm not really one who cares that much about like, whether AIs say something
offensive today, I'm not easily offended and like, whatever. That's not, that's not world ending. I
understand your point. That's not like, who cares, whatever, that's within scope, that's within envelope.
Within within this bigger kind of, you know, what is the real, you know, most likely
path to like AI disaster, as understood, I think by the smartest people today, I think that is
still a useful leading indicator, because it's like, okay, the developers, you know, whether you
agree with their politics, whether you agree with their, whether you think their commercial
reasons are their sincere reasons or not, they have made it a goal to get the AI to not say
certain things, right? They don't want it to be offensive. The most naive, you know, kind of
down the fairway interpretation of that is like, hey, they want to sell it to corporate customers.
They know that their corporate customers don't want, you know, to have their AI saying offensive
things. So they don't want to say offensive things. And yet they can't really control it. It's like
still pretty easy to break. So I view that as just kind of a leading indicator of, okay, we've seen
GPT two, three and four over the last four years. And that's, you know, a big delta in capability.
How much control have we seen developed in that time? And does it seem to be keeping pace?
And my answer would be on the face of it, it seems like the answer is no, you know, we, we don't
have the ability to really dial in the behavior such that we can say, okay, you're going to,
you know, you can expect, you can trust that these AIs will like not do, you know, aviancy.
On the contrary, it's like, if you're a little clever, you know, you can get them to do it.
You can break out of the sandbox on it.
Yeah. And it's not even like, I mean, we've talked about, you know, things where you have
access to the weights and you're doing like counter optimizations, but you don't even need that,
you know, the kind of stuff I do in like my red teaming in public is literally just like
feed the AI a couple of words, put a couple of words in its mouth, you know, and it will kind
of carry on from there. So with that in mind is just a leading indicator. You know, I don't know
how powerful the most powerful AI systems get over the next few years, but it seems very plausible
to me that it might be as powerful as like an Elon Musk type figure, you know, somebody who's
like really good at thinking from first principles, really smart, you know, really dynamic across a
wide range of different contexts. And, you know, he's not powerful enough to like in and of himself
take over the world, but he is kind of becoming transformative. Now imagine that you have that
kind of system, and it's trivial to replicate it. So, you know, if you have like one Elon Musk,
all of a sudden you can have arbitrary, you know, functionally arbitrary numbers of Elon Musk power
things that are clones of each other. Maybe I can pause you there. So that's my polytheistic AI
scenario. But here's the thing that is this is background, but I want to push it to foreground.
You still have a human typing in things into that thing. The human is doing the jailbreak, right?
What we're talking about is not artificial intelligence in the sense of something separate
from a human, but amplified intelligence. Amplified intelligence, I very much believe in. The reason
is amplified intelligence. So here's something that people may not know about humans. There's this
great book, Cooking Made Us Human. Okay, tool use has shifted your biology in the following way.
For example, I know I'll map it to the present day. This book by Richard Rang and Cooking Made Us
Human, where the fact that we started cooking and using fire meant that we could do metabolism
outside the body, which meant it freed up energy for more brain development. Okay,
similarly, developing clothes meant that we didn't have to evolve as much
fur, again, more energy for brain development. Evolving tools meant we didn't have as much
fangs and claws and muscles, again, more energy for brain development, right? So encephalization
quotient rose as tool use meant that we didn't have to do as much natively and we could push
more to the machines. In a very real sense, we have been a man machine symbiosis since the
invention of fire and the stone axe and clothes, right? You do not exist as a human being on your
own like the entire Ted Kaczynski concept of like living in nature by itself. Humans are social
organisms that are adapted to working with other humans and using tools and you have for and we
have been for millennia. Okay, this goes back, not just human history, but like hundreds of
thousands years before 100 gatherers are using tools. Okay, so what that means is man machine
symbiosis is not some new thing. It's actually the old thing that broke us away from other
primate lineages that weren't using tools. Okay, this is the fundamental difference between what I
call Uncle Ted and Uncle Fred. Uncle Ted is Ted Kaczynski. It's a unabomber. It's a doomer. It's
a decelerator, the de grother who thinks we need to go back to Gaia and Eden and become monkeys
and live in the jungle like, like, you know, Ted Kaczynski, right? The unabomber cell. Uncle Fred
is Friedrich Nietzsche, right? Nietzsche and we must get the stars and become ubermen and so
on and so forth. This I think is going to become, and I actually tweeted about this years ago before
current AI debates, that, you know, between anarcho primitivism, de growth, deceleration,
okay, on the one hand, and transhumanism and acceleration and human 2.0 and human self-improvement
and make it just the stars, on the other hand, this is the future political axis, the current one.
And roughly speaking, you can cut, it's not really left and right because you'll have both
left status and right conservatives go over here. You know, left states will say it's against
the state and the right states will say, the right conservatives say it's against God,
okay, and you'll have left libertarians and right libertarians over here,
where left libertarians say it's my body and, you know, the right libertarians say it's my,
you know, my money, right? And so that is a re-architecting of the political axis where,
you know, Uncle Ted and Uncle Fred, which is kind of a clever way of putting it, okay?
And the problem with the Uncle Ted guys in my view is, as I said, yeah, if they go and want to live
in the, you know, the woods, fine, go get them. But once you start having even like a thousand,
forget a thousand, a hundred people doing that, your trees will very quickly get exfoliated,
you know, the leaves are going to get all picked off of them. Humans are not set up to just literally
live in the jungle right now. You've had hundreds of thousands of years of evolution that have
driven you in the direction of tool use, social organisms, farming, etc., etc. The man machine
symbiosis is not today, it's yesterday and the day before and 10,000 years ago and 100,000 years ago.
And how do we know we've got man machine symbiosis? Can you live without
even if you're not living, even if you're not using the stove, somebody's using the stove
to make you food, right? Can you live without the tractors that are digging up the grains? Can you
live without indoor heating? Can you live without your clothes? Frankly, can you do your work without
your phone, without your computer? No, you can't. You are already a man machine symbiosis. Once we
accept that, then the question is, what's the next step? And right now, we're in the middle of that
next step, which is AI is amplified intelligence. So what you're talking about is not that the AI
is Elon Musk, it is that the AI human fusion means there's another 20 Elon Musk's or whatever the
number is, okay? And that's good. That's fine. That's within envelope. That's just a bunch of smarter
humans on the planet. That is amplified intelligence. That is more like, you know, I mentioned the tool
thing. Okay, the other analogy would be like a dog. You know, a dog is man's best friend.
Right? So that AI does not live without humans can turn it off. They have to power it. They have
to give it substance, right? Eventually, that might become like a ceremonial thing, like this is our
God that we pray to, right? Because it's wiser and smarter than us and it appears in an image.
But the priests maintain it. You know, just like you go to a Hindu temple or something like that,
and the priests will pour out the ghee, you know, for the fires and so on and so forth. And then
everybody comes in and prays, okay? The priests believe in the whole thing, but they also maintain
the back of the house. They do the system administration for the temple. Same, you know,
in a Christian church, right? The, the, you know, like, it's not like it appears out of nowhere.
Somebody, you know, went and assembled this cathedral, right? They saw the back of the house,
the fact that it was just woods and rocks and so on that came together. Then when people come
there, it feels like a spiritual experience. You see what I'm saying? Okay. So the equivalent of that,
the priests or the, you know, the people maintaining temples, cathedrals, mosques, whatever, is
engineers who are maintaining these future AIs, which appear to you as Jesus. They appear to you,
maybe even a hologram. Okay, you come there, you ask it for guidance as an oracle. You've also got
the personal version on your phone. You ask it for guidance. But guess what? You're still a human
AI symbiosis until and unless the AI actually has the terminator scenario where it's got
lots of robots that can live on its own. I'm not saying that's physically impossible. I did give
some constraints on it earlier, but for a while we're not going to be there. So that alone means
it's not fume because we don't have lots of drones running around. The AI has to be with
the human. It's a human AI symbiosis. It's not AI Elon Musk. It is human AI fusion that becomes
Elon Musk. And frankly, that's not that different from what Elon Musk himself is. Elon Musk would
not be Elon Musk without the internet. Without the internet, you can't tweet and reach 150 million
people. The internet itself made Elon what he is. And so this is the next version of that.
Maybe there's now 30 Elon's because the AI makes the next 30 Elon's.
Yeah, I mean, again, I think I'm largely with you with just this one very important nagging worry
that's like, what if this time is different because what if these systems are getting
so powerful so quickly that we don't really have time for that techno human fusion
to really work out? And I'll just give you kind of a couple of data points on that.
You said it's still somebody putting something into the AI. Well, sort of, right? I mean,
already we have these proto agents and the super simple scaffolding of an agent is just
run it in a loop, give it a goal, and have it kind of pursue some like plan, act, get feedback,
and loop type of structure, right? It doesn't seem to take a lot. Now, they're not smart enough yet
to accomplish big things in the world. But it seems like the language model to agent
switch is less one right now that is gated by the structure or the architecture and more one
that's just gated by the fact that like the language models when framed as agents just aren't
that successful at like doing practical things and getting over hump. So they tend to get stuck.
But it doesn't seem that hard to imagine that like, you know, if you had something that is sort
of that next level that you put it into a loop, you say, okay, you're Elon Musk, LLM, and your
job is to like make, you know, us, whatever us exactly is a, you know, multi planetary species.
And then you just kind of keep updating your status, keep updating your plans, keep trying
stuff, keep getting feedback. And, you know, like what really limits that.
There may be like a really good program. But the whole AI kills everyone thing is so it's like,
where's the actuator? Okay, I hit enter. What kills me? Right? Is it a hypnotized human who's
being hypnotized by an AI that he's typed into? And he's radicalized himself by typing into a
computer? Okay, that's not that different from a lot of other things that have happened in the past.
Right. So who is actually striking me? Right? Who's striking the human? It's another human within
acts that he's been radicalized by an AI. Okay, he's not actually that's not even the right term.
We're giving agency to the AI when it's not really an agent. It is a human who's self radicalized by
typing into a computer screen and has hit another human. That's one scenario. The other scenario is
it's literally a Skynet drone that's hitting you. Those are the only two. How else is it going to be
physical? Right? How does it? The actuation step is a part that is skipped over and it's a non trivial
step. Well, I think it could be lots of things, right? I mean, if it's not one of those two,
if it's not human or drone hitting you, what is it? Just habitat degradation, right? I mean,
how do we kill most of the other species that we drive to extinction? We don't go out and hunt them
down with axes one by one. We just change the environment more broadly to the point where
it's not suitable for them anymore and they don't have enough space and they kind of die out, right?
So we did hunt down some of the megafauna literally one by one with spears and stuff,
but most of the recent loss of species is just like we're out there just extracting resources for
our own purposes. And in the course of doing that, you know, whatever bird or whatever, you know,
thing just kind of loses its place and then it's no more. And I don't think that's like totally
implausible. Wait, so that is though, I think within normal world, right? What does that mean?
That means that some people, some amplified intelligence, and maybe we might call it HAI,
okay, human plus AI combination, right? Some HAIs outcompete others economically and they lose
their jobs. Is that what you're talking about? I think also the humans potentially become
unnecessary in a lot of the configurations, like just a recent paper from DeepMind.
It's your marginal product workers. Or negative. Yeah, I mean, so the last, you know, DeepMind has
been on Google, Google DeepMind has been on a tear of increasingly impressive medical AIs.
Their most recent one takes a bunch of difficult case studies from the literature. I mean, case
studies, you know, this is like rare diseases, hard to diagnose stuff, and asks an AI to do the
differential diagnosis, compares that to human and compares it to human plus AI. And they phrase
their results like in a very understated way. But the headline is the AI blows away the human
plus AI. The human makes the AI worse. So here's the thing. I'll say something provocative, maybe.
Okay, like I have in a very fine. I do think that the ABCs of economic apocalypse for blue America
are AI, Bitcoin and China, where AI takes away their, a lot of the revenue streams, the licensures
that have made medical and legal costs and other things so high. Bitcoin takes away the power of
money and China takes away their military power. So I've received total meltdown for blue America
in the years and, you know, maybe decade to come already kind of happening. But that's different
than being at the end of the world, right? Like blue America had a really great time for a long time
and they've got these licensure locks. But because of that, they've hyperinflated the cost of medicine.
It's like how much, so what you're talking about is, wow, we have infinite free medicine.
Man, Dr. Billing events are going to get ahead. That's the point.
Yeah. And to be clear, I'm really with you on that too. Like I want to see one of the things,
when people say like, what is good about AI, you know, why should we, why should we pursue this?
This, my standard answer is high quality medical advice for everyone at pennies,
you know, per visit, right? It is orders of magnitude cheaper. We're already starting to
see that in some ways it's better. People prefer it, you know, that AI is more patient,
it has better bedside manner. I wouldn't say, you know, if I was giving my, you know, my own
family advice today, I would say use both a human doctor and an AI, but definitely use the AI as
part of your mix. Absolutely. That's right. That's right. But you're prompting it still, right?
The smarter you are, the smarter the AI is, you notice this immediately with your vocabulary,
right? The more sophisticated your vocabulary, the finer the distinctions you can have,
the better your own ability to spot errors. You can generate a basic program with it, right?
But really amplified intelligence is I think a much better way of thinking about it,
because whatever your IQ is, it surges it upward by a factor of three or whatever the number.
And maybe the amplifier increases with your intelligence, but that, that internal intelligence
difference still exists. It's just like what a computer is, a computer is an amplifier for
intelligence. If you're smart, you can hit enter and programs can go to like, like thinking about
the Minecraft guy, right? Or Satoshi, one person built a billion or an associated trillion dollar
thing, you know, obviously other people continued Bitcoin and so on and so forth, right? So what I
feel though is this is what I mean by going from nuclear terrorism to the TSA. Okay, we went from
AI will kill everyone. And I'm like, what's the actuator to, okay, it'll gradually to greater
environment. What does that mean? Okay, some people lose their jobs, but then we're back in normal
worlds. Well, hold on, let me paint a little bit more complete picture because I don't think we're
quite there yet. So I think the differential diagnosis, recent paper, that's just a data point
where it's kind of like chess. This, you know, this came long before, right? There was a period
where humans are the best chess players, then there was a period where the best were the hybrid
human AI systems. And now as far as I understand it, we're in a regime where the human can't really
help the AI anymore. And so the AIs are, you know, the best chess players are just pure AIs.
We're not there in medicine, but we're starting to see examples where, hey, in a pretty defined study
differential diagnosis, the AI is beating, not just beating the humans, but also beating the AI
human hybrid or the human with access to AI. So, okay, that's not it, right? There's a paper
recently called Eureka out of NVIDIA. This is Jim Fan's lab where they use GPT-4 to write the reward
functions to train a robot. So you want to train a robot to like twirl a pencil in fingers. Hard,
you know, hard for me to do. Robots, you definitely can't do it. How do you train that? Well, you
need a reward function, the reward function. Basically, while you're in the early process
of learning and failing all the time, the reward function gives you encouragement when you're on
the right track, right? So there are people who, you know, have developed this skill and you might
do something like, well, if the pencil has angular momentum, you know, then that seems like you're
on maybe sort of the right track. So give that, you know, a reward, even though at the beginning
you're just failing all the time. Turns out GPT-4 is way better than humans at this, right? So it's
better at training robots. So all of that is awesome. And it's great. And, but here is, here's the thing,
is there's a huge difference between AI is going to kill everybody and turn everybody into paper
clips, okay, versus some humans with some AI are going to make a lot more money. And some people
are going to lose their jobs. Yeah, I'm not scared of that. I'm not scared of that scenario. I mean,
it could be disruptive. It could be disruptive, but it's not existential under itself.
Big deal. Okay. So that's why I went, right. There's the, the, the beta, to me, it comes,
if I, if I ask just one question is what is the actuator, right? You know, sensors and actuators,
right? What is the thing that's actually going to plunge a knife or a bullet into you and kill
you? It is either a human who has hypnotized themselves by typing into a computer, like
basically an AI terrorist, you know, which is kind of where some of the EAs are going in my view,
or it is like an autonomous drone that is controlled in a starcraft or terminator like way.
We are not there yet in terms of having enough humanoid or autonomous drones that are intranet
connected and programmable. That won't be there for some time. Okay. So that alone means fast take
off is, and what I think by the time we get there, you will have a cryptographic control over them.
That's a crucial thing. Cryptography fragments the whole space in a very fundamental way.
If you don't have the private keys, you do not have control over it. So long as that piece of
hardware, the cryptographic controller, you've nailed the equations on that. And frankly, you can
use AI to attack that as well to make sure the code is perfect, right? Remember you talked about
attack and defense? AI is attack crypto's defense, right? Because one of the things that crypto is
done, do you know the PKI problem is public key infrastructure? I'll say no, on behalf of the
audience. This is good. We should do more of these actually. I feel it's a good, you know, fusion of
things or whatever, right? But the public key infrastructure problem, the public key infrastructure
problem is something that was sort of lots of cryptography papers and computer science papers
in the 90s and 2000s assumed that this could exist and essentially meant if you could assume that
everybody on the internet had a public key that was public and a private key that was kept both
secure and available at all times, then there's all kinds of amazing things you can do with
privacy preserving, messaging and authentication and so on. The problem is that for many years,
what cryptographers try to do is they try to nag people into keeping their private keys secure
and available. And the issue is it's trivial to keep it secure and unavailable where you write
it down, you put it into a lockbox and you lose the lockbox. It's trivial to keep it available
and not secure, okay, where you put it on your public website and it's available all the time,
you never lose it, but it's not secure because anybody can see it. When you actually ask,
what does it mean to keep something secure and available? That's actually a very high cost.
It's precious space because it's based on your wallet, right? Your wallet is on your person
at all times, so it's available, but it's not available to everybody else, so it's secure.
So you actually have to touch it constantly, yes, right? So it turns out that the crypto wallet
by adding a literal incentive to keep your private keys secure and available because if they're not
available, you've lost your money. If they're not secure, you've lost your money, okay? To have both
of them, that was what solved the PKI problem. Now we have hundreds of millions of people with
public private key pairs where the private keys are secure and available. That means all kinds
of cryptographic schemes, zero knowledge stuff, there's this amazing universe of things that is
happening now. Zero knowledge in particular has made cryptography much more programmable.
There's a whole topic which is, if you want something that's kind of, you know, like AI was
creeping for a while and people, specialists were paying attention to it and then just burst
out on the scene, zero knowledge is kind of like that for cryptography. Thanks to the,
you know, you've probably heard of zero knowledge before.
Yeah, we did one episode with Daniel Kong on the use of zero knowledge proofs to basically
to prove without revealing like the weights that you actually ran the model you said you were
going to run and things like that, I think are super interesting.
Exactly, right? So what kinds of stuff, why is that useful in the AI space? Well, first is you
can use it, for example, for training on medical records while keeping them both private,
but also getting the data you want. For example, let's say you've got a collection of
genomes, okay, and you want to ask, okay, how many G's were in this data set, how many C's,
how many A's, how many T's, okay, like you just say, like, that's a very simple downstairs.
What's the ACG T content of this, you know, the sequence data set, you could get those numbers,
you could prove they were correct without giving any information about the individual sequences,
right, or more specifically, you do it at one locus and you say, how many G's and how many C's
are at this particular locus and you get the SNP distribution, okay. So it's useful for what you
just said, which is like showing that you ran a particular model without giving anything else away,
it's useful for certain kinds of data analysis. There's a lot of overhead on compute on this
right now, so it's not something that you do trivially, okay, but it'll probably come down with
time. But what is perhaps most interestingly useful for it is in the context of AI is coming up with
things in AI can't fake. So what we talked about earlier, right, like an AI can come up with all
kinds of plausible sounding images, but if it wasn't cryptographically signed by the sender,
then, you know, it, and it should be signed by sender and put on chain. And then at least you
know that this person or this entity with this private key asserted that this object existed at
this time in a way that'd be extremely expensive to falsify because it's either on the Bitcoin
blockchain or another blockchain, it's very expensive to rewind, okay. This starts to be
a bunch of facts that an AI can't fake. You know, so going back to the kind of big picture
loss of control story, I was just kind of trying to build up a few of these data points that like,
hey, look at this differential diagnosis, we already see like humans are not really adding value
to AI's anymore. That's kind of striking. And like similarly with training robot hands, GPT-4 is
outperforming human experts. And by the way, all of the sort of latent spaces are like totally
bridgeable, right? I mean, one of the most striking observations of the last couple of years of study
is that AI's can talk to each other in high dimensional space, which we don't really have a
way of understanding natively, right? It takes a lot of work for us to decode.
This is like the language thing? We're starting to see AI's kind of develop,
not obviously totally on their own as of now, but there is becoming an increasingly reliable
go-to set of techniques if you want to bridge different modalities with like a pretty small
parameter adapter. That's interesting. Actually, what's a good paper on that? I actually hadn't
seen that. The Blip family of models out of Salesforce research is really interesting,
and I've used that in production at Salesforce. Really? Yeah, Salesforce research. They have a
crack team that has open sourced a ton of stuff in the language model, computer vision, joint space.
And you see this all over the place now, but basically what they did in a paper called Blip
2, and they've had like five of these with a bunch of different techniques. But in Blip 2,
they took a pre-trained language model and then a pre-trained computer vision model,
and they were able to train just a very small model that kind of connects the two. So you could
take an image, put it into the image space, then have their little bridge that over to language
space, and that everything else, the two big models are frozen. So they were able to do this on
just like a couple days worth of GPU time, which I do think goes to show how it is going to be
very difficult to contain proliferation. Which is good. In my view, that's really good.
As long as it doesn't get out of control, I'm probably with you on that too. But by bridging
this vision space into the language space, then the language model would be able to converse with you
about the image, even though the language model was never trained on images, but you just had this
connector that kind of bridges those modalities. It's just, it's like another layer of the network
that just bridges two networks almost. Yeah, it bridges the spaces. It bridges the conceptual
spaces between something that has only understood images and something that has only understood
language, but now you can kind of bring those together. As I think about it, it's not that
surprising because that's what, for example, text image models are basically that. They're
bridging two spaces in a sense, right? But I'll check this paper out. So on the one hand,
it's not that surprising. On the other hand, I should see how they implemented it or whatever,
so blip two. Okay. Yeah, I think the most striking thing about that is just how small it is. Like,
you took these two off the shelf models that were trained independently for other purposes,
and you're able to bridge them with a relatively small connector. And that seems to be kind of,
you know, happening all over the place. I would also look at the Flamingo architecture,
which is like a year and a half ago now out of DeepMind. That was one for me where I was like,
oh my, and it's also a language to vision, where they keep the language model frozen.
And then they kind of, in my mind, it's like, I can see the person in their garage like tinkering
with their soldering iron, you know, because it's just like, wow, you took this whole language
thing that was frozen, and you kind of injected some, you know, vision stuff here, and you added
a couple layers, and you kind of Frankenstein it, and it works. And it's like, wow, that's not really,
it wasn't like super principled, you know, it was just kind of hack a few things together and,
you know, try training it. And I don't want to diminish what they did, because I'm sure there
were, you know, more insights to it than that. But it seems like we are kind of seeing a reliable
pattern of the key point here being model to model communication through high dimensional
space, which is not mediated by human language, is I think one of the reasons that I would expect,
and by the way, there's lots of papers too on like, you know, language models are human level,
or even superhuman prompt engineers, you know, they're they're self prompting, like,
techniques are getting pretty good. So if I'm imagining the big picture of like,
and we can, you know, get back to like, okay, well, how do we use any techniques crypto or
otherwise to keep this under control? And then I would say this is kind of the newer school of
the big picture AI safety worry. Obviously, there's a lot of flavors. But if you were to,
you know, go look at like a Jay Acatra, for example, I think a really good writer on this.
Her worldview is less that we're going to have this fume and more that over a period of time,
and it may not be a long period of time, maybe it's like a generation, maybe it's 10 years,
maybe it's 100 years. But obviously, those are all small in the sort of, you know, grand scheme of
the future. We have, in all likelihood, the development of AI centric schemes of production,
where you've got kind of your high level executive function is like your language model,
you've got all these like lower level models, they're all bridgeable, all the spaces are bridgeable
in high dimensional form, where they're not really mediated by language, unless we enforce
that. I mean, we could say, you know, it must always be mediated by language so we can read the
logs. But there's a text to that, right? Because going through language is like highly compressed,
compared to the high dimensional space to space. All right, so let me see if I can
steal man or articulate your case, you're saying, AI's are going to get good enough,
they're going to be able to communicate with each other good enough, and they'll be able to do enough
tasks that more and more humans will be rendered economically marginal and unnecessary.
I'm not saying I think that will happen, I'm just saying I think there's a good enough chance
that that will happen, but it's worth taking really seriously. I actually think that will happen,
something along those lines, or in the sense of at least massive economic disruption,
definitely. Okay, but I'll give an answer to that, which is both, you know, maybe fun and not fun.
Have you seen the graph of the percentage of America that was involved in farming?
Yeah, I tweeted a version of that once.
I did. Okay, great. Good. So you're familiar with this, and you're familiar with what I mean by
the implication of it, where basically Americans used to identify themselves as farmers, right?
And manufacturing rose as agriculture collapsed, right? And here is the graph on that. But from
like 40% in the year 1900 to like a total collapse of agriculture, and then also more recently a
collapse of manufacturing into bureaucracy, paperwork, legal work, what is up into the right
since then is, you know, the lawyers, what is up into the right? What is replacing that?
Starting in around the 1970s, we used to be adding energy production and energy production
flatlined once people got angry about nuclear power. So this is a future that could have been,
we could be on Mars by now, but we got flatlined, right? What did go up into the right? So construction
costs, this is the bad scenario where the miracle energy got destroyed because regulations,
the cost was flat. And then when vertical, when regulations were imposed, all the progress was
stopped by decels and degrowthers. And then Alara was implemented, which said nuclear energy
has to be as low risk as reasonably necessary, as reasonably achievable. And that meant that you
just keep adding quote safety to it until it's as same as cost as everything else, which means you
destroyed the value of it, right? But you know what was up into the right? What replaced those
agriculture and manufacturing jobs? Look at this, you see this graph? For the audio only, we will
put this on YouTube. So if you want to see the graph do the YouTube version of this, for the audio
only group, it's an exponential curve in the number of lawyers in the United States from,
looks like maybe two thirds of a million to 13 million over the last 140 years.
Yeah. And in 1880, it was like, like sub 100,000 or something like that, right? And then it's just
like, especially that 1970 point, that's when it went totally vertical. Okay. And it's probably
even more since. So, you know, if you add paperwork jobs, bureaucratic jobs, you know,
every lawyer is like, you know, net, sorry, lawyers, but you're basically negative value add,
right? Because it should, the fact that you have a lawyer means that you couldn't just self serve a
form, right? Basic government is platform is where you can just self serve and you fill it out.
And you don't have to have somebody like code something for you custom, you know, lawyers
that's doing custom code is because the legal code is so complicated. So, you know, the whole
Shakespeare thing, like first thing we do, let's, you know, kill all the lawyers. First thing we
do, let's automate all the lawyers, right? Only something that's the hammer blow of AI
can break the backbone and it will. That's it's going to break the backbone of blue America,
right? It's going to cause that's why the political layer and the sovereignty layer
is not what AI people think about. But it's like crucial for thinking about AI, because what tribes
does AI benefit? And again, we got away from why does AI kill everybody? Well, it's going to need
actuators. Who's going to stab you? Who's going to shoot you? It's got to be a human hypnotized
by AI or a drone that AI controls. A human hypnotized by AI is actually a conventional
threat. It looks like a terrorist cell. We know how to deal with that, right? It's just like radicalized
humans that worship some AI that stab you. It's like the pause AI people are one step, I think,
away from that. All right. But that's just like on Shin Riko. That's like allocated. That's like
basically terrorists who think that the AI is telling them what to do. Fine. If it's not a human
that's stabbing you, it is a drone. And that's like a very different future where
like five or 10 or 15 years up, maybe we have enough internet connected drones out there,
but even then they'll have private keys. So there's going to be fragmentation of address space.
Not all drones be controlled by everybody in my view. Okay. That's what AI safety is. AI safety is
can you turn it off? Can you kill it? Can you stop it from controlling drones? That's what AI
safety is. Can you also open the model weights so you can generate adversarial inputs? Can you
open the model weights and proliferate it? You're saying, oh, proliferation is bad. I'm saying
proliferation is good because if everybody has one, then nobody has an advantage on it.
Right. Not relatively speaking. Okay. I have very few super confident positions. So I wouldn't
necessarily say I think that proliferation is bad. I'd say so far it's good. It has and even
most of the AI safety people, I would say if I could speak on the behalf of the AI safety
consensus, I would say most people would say even that the Llama 2 release has proven
good for AI safety for the reasons that you're saying. But they opposed it.
Well, some didn't, some didn't. I would say the main posture that I see AI safety people taking
is that we're getting really close to or we might be getting really close.
Certainly if we just naively extrapolate out recent progress, it would seem that we're getting
really close to systems that are sufficiently powerful that it's very hard to predict what
happens if they proliferate. Llama 2, not there. And so, yes, it has enabled a lot of interpretability
work. It has enabled things like representation engineering, which there is a lot of good
stuff that has come from it. The big thing that I want to kind of establish is you agree with me
on the actuation point or not. The thing is this thing, like Llama 2 proliferates and so
businesses are disrupted and people, maybe they paid a lot of money for their MD degree and they
can't make us a bunch of money. That's within the realm of what I call conventional warfare.
You know what I mean? That's like we're still in normal world as we were talking about, okay?
Unconventional warfare is, you know, Skynet arises and kills everybody, okay? And that is what is
being sold over here. And when you think about the actuators, we don't have the drones out there,
we don't have the humanoid robots at control, and hypnotized humans are a very tiny subset of humans,
probably. And even if they aren't, that just looks like a religion or a cult or a terrorist
cell, and we know how to deal with that as well. The super intelligent AI with, you know,
lots of robots at control in a starcraft form, I would agree, is something that humans haven't
faced yet. But by the time we get that many robots out there, you won't be able to control all of
them at once because of the private key things I mentioned. So that's why I'm like, okay,
everything else we're talking about is in normal world. That is the single biggest thing
that I wanted to get, like economic disruption, people losing jobs, proliferation so that the
balance of power is redistributed, all that is fine. The reason I say this is people keep trying
to link AI to existential risk. A great example is one of the things you actually had in here,
this is similar to the AI policy and so on. It's a totally reasonable question, but then I'm going
to, in my view, deconstruct the question. What would you think about putting the limit on the
right to compute or their capabilities and AI system might demonstrate that we make you think
open access no longer wise? The most common near term answer here to be seems to be related to
risk of pandemic via novel pathogen engineering. So guess what? You know who the novel pathogen
engineers are? The US and Chinese governments, right? They did it, or probably did it, credibly
did it, credibly mean accused of doing it. They haven't been punished for COVID-19. In fact,
they covered up their culpability and pointed everywhere other than themselves. They used it
to gain more power in both the US and China with both lockdown in China and in the US and all kinds
of COVID era, trillions of dollars was printed and spent and so on and so forth. They did everything
other than actually solve the problem. That was actually getting the vaccines in the private
sector and they studied the existential risk only to generate it and they're even paid to
generate pandemic prevention and failed. So this would be the ultimate Fox guarding the henhouse.
The two organizations responsible for killing millions of people with novel pathogen are going
to prevent people from doing this by restricting compute. No, you know what it is actually. What's
happening here is one of the concepts I have in the network state is this idea of God, state,
and network. Meaning, what do you think is the most powerful force in the world? Is it Almighty
God? Is it the US government? Or is it encryption? Or eventually maybe an AGI? What's happening here
is a lot of people are implicitly, without realizing it, even if they are secular atheists,
they're treating GOV as GOD. They treat the US government as God as the final mover.
No, I appreciate your little, I take inspiration from you actually in terms of
trying to come up with these little quips that are memorable. So I was just smiling at that
because I think you do a great job of that and I try to encourage, I have less success
coining terms than you have, but certainly try to follow your example on that front.
It's like a helpful, if you can compress it down, it's like more memorable. So that's what I try
to do, right? So exactly, a lot of these people who are secular, think of themselves as atheists,
have just replaced GOD with GOV. They worship the US government as God. And there's two versions
of this. You know how like God has both the male and female version, right? The female version is
the Democrat God within the USA that has infinite money and can take care of everybody and care
for everybody. And the Republican God is the US military that can blow up anybody and it's the
biggest and strongest and most powerful America F. Yeah. Okay. And everybody who thinks of the US
government as being able to stop something is praying to a dead God. Okay, when you say this,
you actually get an interesting reaction from AI safety people where you've actually hit their
true solar plexus. All right, the true solar plexus is not that they believe in AI, it's that
they believe in the US government. That's a true solar plexus because they are appealing to their
praying to this dead God that can't even clean the poop off the streets in San Francisco, right?
That is losing wars or fighting them to sell me. It says lost all these wars around the world
that spent trillions of dollars has been through financial crisis, Coronavirus, Iraq war, you
know, total meltdown politically. Okay, that is interest that is now has interest payments more
than the defense budget. That is, you know, that spent $100 billion on the California train without
laying a single track. It's like that, you know, that Morgan Freeman thing for you know, the clip
from Batman, who is like, So this man has a billionaire, blah, blah, blah, this and that,
and your plan is to threaten him, right? And so you're going to create this super intelligence
and have Kamala Harris regulate it. Come on, man, so to speak, right? Like these people are praying
to a blind, deaf and dumb God that was powerful in 1945, right? That's why, by the way, all the
popular movies, what are they? It's Barbie, it's Oppenheimer, right? It's, it's top gun. They're
all throwbacks the 80s or the 50s when the USA was really big and strong. And the future is a
black mirror. Yeah, I think that's tragic. One of the projects that I do like, and you might appreciate
this, I don't know if you've seen it, is the From the Future of Life Institute, a project called
Imagine a World, I think is the name of it. And they basically challenged, you know, their
audience and the public to come up with positive visions of a future, you know, where technology
changes a lot. And obviously, I pretty central to a lot of those stories. And, you know, one of the
challenges that people go through and how do we get there and whatever, but a purposeful effort
to imagine positive futures super under provided. And I really liked the investment that they made
in that. You know, one of the things I've got in the Never See It book is there's certain megatrends
that are happening, right? And megatrends, I mean, it's possible for like one miraculous human maybe
to reverse them, okay? Because I think both the impersonal force of history theory and the
great man theory of history have some truth to them. But the megatrends are the decline of Washington,
DC, the rise of the internet, the rise of India, the rise of China. That is like my worldview. And
I can give a thousand graphs and charts and so on for that. But that's basically the last 30 years.
And maybe the next X, right? I'm not saying there can't be trend reversal. Of course, it can be
trend reversal, as I just mentioned, some hammer blow could hit it, but that's what's happening.
And so because of that, the people who are optimistic about the future are aligned with either the
internet, India or China. And the people who are not optimistic about the future are blue Americans
or left out red Americans, okay, or Westerners in general, who are not tech tech people. Okay,
if they're not tech people, they're not up into the right, basically, because the internet's if you
I mean, one of the things is we have a misnomer, as I was saying earlier, calling it the United
States, because the dis United States, it's, it's like talking about, you know, talking about
America is like talking about Korea, there's North Korea and South Korea, they're totally different
populations. And, you know, communism and capitalism are totally different systems. And the thing that
is good for one is bad for another and vice versa. And so like America doesn't exist, there's only
just like there's no Korea, there's only North Korea and South Korea, there's no America, there's
blue America and red America and also gray America, tech America. And blue America is harmed, or they
think they're harmed, or they've gotten themselves into a spot where they're harmed by every technological
development, which is why they hate it so much, right? AI versus journalist jobs, crypto takes away
banking jobs, you know, everything, you know, self driving cars, they just take away regulator
control, right? Anything that reduces their power, they hate, and they're just trying to freeze an
amber with regulations. Red America got crushed a long time ago by offshoring to China and so on,
they're, they're making, you know, inroads ally with tech America or gray America. Tech America
is like the one piece of America that's actually still functional and globally competitive. And
people always do this fallacy of aggregation, where they talk about the USA. And it's really
this component that's up into the right, and the others that are down into the right, red,
best flat, like red, but they're like down, right? Like, red is like, okay, functional, blue is down.
The point is, tech America, I think we're going to find is not even truly or
how American is tech America, because it's like 50% immigrants, right? And like a lot of children
immigrants, and most of their customers are overseas, and their users are overseas. And
their vantage point is global, right? And they're basically not, I know we're in this
ultra nationalist kick right now. And I know that there's going to be, there's a degree of a fork
here, where you fork technology into Silicon Valley and the internet. Okay, where Silicon
Valley is American, and they'll be making like American military equipment and so on and so
forth, and they're signaling USA, which is fine. Okay. And then the internet is international,
global capitalism. And the difference is Silicon Valley, or let's say US tech, let me less, you
know, US tech says ban tick tock, build military equipment, etc. It's really identifying itself
as American. And it's thinking of being anti China. Okay, but there's US and China are only 20% of
the world, 80% of the world is neither American nor Chinese. So the internet is for everybody else
who wants actual global rule of law, right? When as a US decays as a rule space order, and people
don't want to be under China, people want to be under something like blockchains, where you've got
like property rights, contract law, cross borders that are enforced by an impartial authority. Okay,
that's also the kind of laws that can bind AI's like AI's across borders, if you want to make
sure they're going to do something, cryptography can bind an AI in such a way that it can't fake
it. It can't an AI can't mint more Bitcoin, you know, my here's my last question for you. AI discourse
right now does seem to be polarizing into camps. Obviously, a big way that you think about the world
is by trying to figure out, you know, what are the different camps? How do they relate to each other
so on and so forth? I have the view that AI is so weird. And so unlike other things that we've
encountered in the past, including just like, unlike humans, right, I always say AI, alien
intelligence, that I feel like it's really important to to borrow a phrase from Paul Graham,
keep our identities small, and try to have a scout mindset to really just take things on their
own terms, right? And not necessarily put them through a prism of like, who's team am I on? Or,
you know, is this benefit my team or hurt the other team or whatever. But you know, just try to be as
kind of directly engaged with the things themselves as we can without mediating it through all these
lenses. You know, I think about you mentioned like, the gain of function, right? And I don't know
for sure what happened, but it certainly does seem like there's a very significant chance
that it was a lab leak. Certainly there's a long history of lab leaks. But it would be like, you
know, it would seem to me a failure to say, okay, well, what's the what's the opposite of just having
like a couple of government labs, like, everybody gets their own gain of function lab, right? Like,
if we could, and this is kind of what we're doing with AI, we're like, let's compress this power down
to as small as we can. Let's make a kit that can run in everybody's home. Would we want to
send out these like gain of function, you know, wet lab research kits to like every home in the
world and be like, hope you find something interesting, you know, like, let us know if you
find any new pathogens or hey, maybe you'll find life saving drugs, like whatever, we'll see what
you find, you know, all eight billion of you. That to me seems like it would be definitely a
big misstep. And that's the kind of thing that I see coming out of ideologically
motivated reasoning, or like, you know, tribal reasoning. And so I guess,
I wonder how you think about the role that tribalism and ideology is playing and should
or shouldn't play as we try to understand AI. Okay, so first is, you're absolutely right,
that just because a is bad does not mean that B is good, right? So a could be a bad option,
B could be a bad option, C could be a bad option. There might be, you have to go down to option G
before you find a good option, or there might be three good options and seven bad options,
for example, right? So to map that here, in my view, an extremely bad option is to ask the US
and Chinese governments to do something. Anything the US government does at the federal level,
at the state level in blue states at the city level has been a failure. And the way that here's
here's a metaway of thinking about it, you invest in companies, right? So as an investor,
here's a really important thing. You might have 10 people who come to you with the same words in
their pitch. They're all, for example, building social networks. But one of them is Facebook,
and the others are Friendster and whatever. Okay, and no offense to Friendster, you know,
that those guys were like, you know, pioneers in their own way. But they just got outmatched by
Facebook. So point is that the words were the same on each of these packages, but the execution was
completely different. So could I imagine a highly competent government that could execute and that
actually did, you know, like, you know, make the right balance of things and so on? I can't say
it's impossible, but I can't say that it wouldn't be this government. Okay, and so you are talking
about the words and I'm talking about the substance. The words are, we will protect you from AI,
right? In my view, the substances, they aren't protecting you from anything, right? You're basically
giving money and power to a completely incompetent and in fact, malicious organization, which is
Washington DC, which is the US government that has basically over the last 30 years, gone from a
hyperpower that wins everywhere without fighting to a declining power that fights everywhere without
winning. Okay, like just literally burn trillions of dollars doing this, take maybe the greatest
decline in fortunes in 30 years and maybe human history, not even the Roman Empire went down this
fast on this many power dimensions this quickly, right? So giving that guy, let's trust him,
that's just people running an old script in their heads that they inherited. They are not
thinking about it from first principles that this state is a failure. Okay, and like how much
of a failure it is, you have to look at sovereign debt crisis, look at graphs that other people
aren't looking at, but like, you know, the domain of what Blue America can regulate is already
collapsing because it can't regulate Russia anymore. It can't regulate China anymore.
It's less able to regulate India. It's less able even to regulate Florida and Texas.
States are breaking away from it domestically. So this gets to your other point. Why is the
tribal lens not something that we can put in the back? We have to put in the absolute front
because the world is retribalizing. Like basically your tribe determines what law you're bound by.
If you think you can pass some policy that binds the whole world, well, there have to be guys with
guns who enforce that policy. And if I have guys with guns on their side that say we're not enforcing
their policy, then you have no policy. You've only bound your own people. Does that make sense,
right? And so Blue America will probably succeed in choking the life out of AI within Blue America.
But Blue America controls less and less of the world. So it'll have more power or a fewer people.
I can go into why this is, but essentially a financial Berlin Wall is arising. There's a lot
of taxation and regulation and effectively financial repression, de facto confiscation,
that will have to happen for the level of debt service that the US has been taking on.
There's one graph just to make the point. And if you want to dig into this, you can.
But the reason this impacts things is when you're talking about AI safety, you're talking about
AI regulation, you're talking about the US government, right? And you have to ask, what does
that actually mean? And it's like, in my view, it's like asking the Soviet Union in 1989 to
regulate the internet, right? That's going to outlive, you know, the country. US interest
payment on federal debt versus defense spending. The white line is defense spending. Look at the
red line. That's just gone absolutely vertical. That's interest. And it's going to go more vertical
next year because all of this debt is getting refinanced at much higher interest rates. That's
why look at this, you want AI timelines, right? The question for me is DC's timeline.
What is DC's time left to live? Okay, this is the kind of thing that kills empires. And you either
have this just go to the absolute moon, or they cut rates and they print a lot. And either way,
you know, the fundamental assumption underpinning all the AI safety, all the AI regulation work
is that they have a functional golem in Washington DC, where if they convince it to do something,
it has enough power to control enough of the world. When that assumption is broken,
then a lot of assumptions are broken, right? And so in my view, you have to,
you must think about a polytheistic AI world, because other tribes are already into this,
they're already funding their own, right? The proliferation is already happening.
And they're not going to bow to blue tribe. So that's why I think the tribal lens is not
secondary. It's not some, you know, totally separate thing. It is an absolutely primary
way in which to look at this. And in a sense, it's almost like a, you know, in a well done
movie, all the plot lines come together at the end. Okay. And all the disruptions that are happening,
the China disruption, the rise of India, the rise of the internet, the rise of crypto,
the rise of AI, and the decline of DC, and the internal political conflict,
and, you know, various other theaters like what's happening in Europe and, you know, and Middle
East, all of those come together into a crescendo of, ah, there's a lot of those graphs are all
having the same time. And it's not something you can analyze by just, I think, looking at one of
these curves on itself. I think that's a great note to wrap on. I am always lamenting the fact
that so many people are thinking about this AI moment in just fundamentally too small
of terms. But I don't think you're one that will easily be accused of that. So with
an invitation to come back and continue in the not too distant future, for now, I will say
Balaji Srinivasan, thank you for being part of the Cognitive Revolution.
Thank you, Nathan. Good to be here.

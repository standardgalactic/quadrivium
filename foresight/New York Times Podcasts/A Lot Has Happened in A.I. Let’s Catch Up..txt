From New York Times Opinion, this is the Ezra Klein Show.
Before we get into the episode today, we are getting ready to do our end of the year Ask
Me Anything.
If you have questions you want to hear me answer on the show, I suspect a lot of them
are going to be about Israel Palestine and AI, but they don't have to be about Israel
Palestine and AI.
Send them to Ezra Klein Show at nytimes.com with AMA in the headline.
Again, to Ezra Klein Show at nytimes.com with AMA in the headline.
If you follow business or tech or artificial intelligence news at all, in recent weeks
you certainly were following Sam Altman being unexpectedly fired as CEO of OpenAI and then
a huge staff revolt at OpenAI where more than 95% of the company said it would resign if
he was not reinstated and then he was reinstated.
And so this whole thing seemed to have happened for nothing.
I spent a lot of time reporting on this and I talked to people on the Altman side of things,
I talked to people on the board side of things, and the thing I am now convinced of, truly
convinced of is that there was less to it than met the eye.
People saw, I saw, Altman fired by this nonprofit board meant to ensure that AI is built to
serve humanity.
And I assumed, and I think many assumed, there was some disagreement here over what OpenAI
was doing, over how much safety was building into the systems, over the pace of commercialization,
over the contracts it was signing, over what it was going to be building next year, over
something.
And that I think I can say conclusively and has been corroborated by other reporting,
that was not what this was about.
The OpenAI board did not trust and did not feel it could control Sam Altman, and that
is why they fired Altman.
It's not that they felt they couldn't trust him on one thing, that they were trying to
control him on X, but he was beating them on X.
It's that a lot of little things added up.
They felt their job was to control the company, that they did not feel they could control
him, and so to do their job, they had to get rid of him.
They did not have, obviously, the support inside the company to do that.
They were not ultimately willing to let OpenAI completely collapse.
And so they largely, although I think in their view, not totally back down.
One of the members is still on the board.
Altman and the president of OpenAI, Greg Brockman, are off the board.
Some new board members are coming in who they think are going to be stronger and more willing
to stand up to them.
There's an investigation that is going to be done of Altman's behavior that will be
at least released to the board, so they'll, I guess, know what to think of him.
It's a very strange story.
I wouldn't be surprised if there's things yet to come out, but I am pretty convinced
that this was truly a struggle for control, not a struggle about X.
But it has been a year since ChatGPT was released, so weird way to mark the year, but it has
been a year.
A year since OpenAI kicked off the whole modern era in artificial intelligence.
A year since a lot of people's estimations of what humanity's future looked like began
to shift and cloud and darken and shimmer.
And so I wanted to have the conversation that many of us thought was a conversation happening
here about what AI was becoming, how it was being used, how it was being commercialized,
the path we're on is going to benefit humanity.
And so I asked my friends over at Hard Fork, another great New York Times podcast, to come
on the show.
Kevin Roos is my colleague at The Times.
He writes a tech column called The Shift.
Casey Newton is the editor of Platformer, an absolutely must read newsletter about the
intersection of technology and democracy.
And they have been following this in and out, but they've been closely following AI for
the past year.
So I wanted to have this broader conversation with them.
As always, my email is reclinedshowatnytimes.com.
Kevin Roos, Casey Newton.
Welcome to the show, my friends.
Hey, Ezra.
Thanks for having us.
All right.
So we're talking on Monday, November 27th.
JetGPT, which kicked off this era in AI, was released on November 30th, 2022.
So the big anniversary party was at Sam Altman got temporarily fired and the company almost
collapsed and was rebuilt over at Microsoft, which I don't think is how people expected
to mark the anniversary.
But it has been now a year, roughly, in this sort of whole new AI world that we're in.
And so I want to talk about what's changed in that year.
And the place I want to begin is with the capabilities of the AI systems we're seeing,
not the ones we're hearing about, but that we know are actually being used by someone
in semi-real world conditions.
What can AI systems do today that they couldn't do a year ago, Kevin?
Well, the first most obvious capabilities improvement is that these models have become
what's called multimodal.
So a year ago, we had ChatGPT, which could take in text input and output other texts
as the response to your prompt.
But now we have models that can take in text and output images, take in text and output
video, take in voice data, and output other voice data.
So these models are now working with many more types of inputs and outputs than they
were a year ago.
And that's sort of the most obvious difference if you just woke up from a year-long nap and
took a look at the AI capabilities on the market, that's the thing that you would probably
notice first.
I want to pull something out about that.
Is that it almost sounds like they're developing what you might call senses.
And I recognize that there's a real danger of anthropomorphizing AI systems.
I'm not trying to do that.
But one thing about having different senses is that we get some information that helps
us learn about the world from our eyes, other information that helps us learn about the
world from our ears, et cetera.
One of the constraints on the models is how much training data they can have.
As they become multimodal, it would seem that would radically expand the amount of training
data.
Not just all of the text on the internet, but all of the audio on YouTube, or all podcast
audio on Spotify or something, or Apple Podcasts.
That's a lot of data to learn about the world from, that in theory will make the models
smarter and more and more capable.
Does it have that kind of recursive quality?
Absolutely.
I mean, part of the backdrop for these capabilities improvements is this race for high-quality
data.
AI labs are obsessed with finding new undiscovered, high-quality data sources that they can use
to train their models.
And so if you run out of text because you've scraped the entire internet, then you've got
to go to podcasts or YouTube videos or some other source of data to keep improving your
models.
For what it's worth, though, I don't think the availability of more training data is
what is interesting about the past year.
I think what was interesting about ChatGBT was that it gave average people a way to interact
with AI for the first time.
It was just a box that you could type in and ask it anything and often get something pretty
good in response.
And even a year into folks using this now, I don't think we fully discovered everything
that it can be used for.
And I think more people are experiencing vertigo every day as they think about what this could
mean for their own jobs and careers.
So to me, the important thing was actually just the box that you type in and get questions
from.
Yeah, I agree with that.
I think if you had just paused there and there was no new development in AI, I think it would
still probably take the next five or 10 years for society to adjust to the new capabilities
in our midst.
So you've made this point in other places, Casey, that a lot of the advances to come
are going to be in user interfaces and in how we interact with these systems.
In a way, that was a big advance of ChatGBT.
The system behind it had been around for a while, but the ability to speak to it, I guess,
write to it in natural language, it created this huge cultural moment around AI.
But what can these AI products actually do that they couldn't do a year ago?
Not just how we interface with them, but their underlying capacity or power.
I mean, as of the developer update that OpenAI had a few weeks back, the world knowledge
of the system has been updated to April of this year.
And so you're able to get something closer to real-time knowledge of world events.
It has now integrated with Microsoft Bing, and so you can get truly real-time information
in a way that was impossible when ChatGBT launched.
And these might sound like relatively minor things, Azura, but you start chaining them
together, you start building the right interfaces, and you actually start to see beyond the internet
as we know it today.
You see a world-worthy web where Google is not our starting point for doing everything
online.
It is just a little box on your computer that you type in and you get the answer without
ever visiting a web page.
So that's all going to take many years to unfold, but the beginnings of it are easy
to see now.
One other capability that didn't exist a year ago, at least in any public products, is the
ability to bring your own data into these models.
So Claude was the first language model that I used that had the ability to, say, upload
a PDF.
So you could say, here's a research paper, it's 100 pages long, help me summarize and
analyze this.
And it could do that.
Now ChatGPT can do the same thing, and I know a bunch of other systems are moving in that
direction too.
There are also companies that have tried to spin up their own language models that are
trained on their own internal data.
So if you are Coca-Cola or BCG or some other business and you want an internal ChatGPT that
you can use for your own employees to ask, say, questions about your HR documents, that
is a thing that companies have been building.
So that's not the sexiest, most consumer-facing application, but that is something that there's
enormous demand for out there.
So one thing it seems to me to be getting better at from what I can tell from others
is coding.
I have to ask people whether they're using AI bots very often, and if so, for what.
And basically nobody says yes unless they are coder.
Everybody says, oh yeah, I played around with it, I thought it was really cool, I sometimes
use Dolly or Mid Journey to make pictures for my kids or for my email newsletter.
But it is the coders who say, I'm using it all the time, it has become completely essential
to me.
I'm curious to hear a bit about that capability increase.
I think where it has sort of become part of the daily habit of programmers is through
tools like GitHub Copilot, which is a basically ChatGPT for coders that finishes whatever line
of code you're working on or helps you debug some code that's broken.
And there have been some studies and tests.
I think there was one test that GitHub itself ran where they gave two groups of coders the
same task.
And one group was allowed to use GitHub Copilot and one group wasn't.
And the group with GitHub Copilot finished the task 55% faster than the group without
it.
Now that is like a radical productivity increase.
And if you tell a programmer, here's a tool that can make you 55% faster, they're going
to want to use that every day.
So when I see function chat bots in the wild, what I see is different versions of what people
used to somewhat derisively call like the fancy autocomplete, right?
Help you finish a line of code, help you finish this email.
You ask a question that you might ask a search engine, like why do I have spots all over
my elbow?
And it gives you an answer that hopefully is right, but maybe is not right.
I do think some of the search implications are interesting, but at the same time, it
is not the case that Bing has made great strides on Google.
People have not moved to asking the kind of Bing chat bot.
Next question is as opposed to asking Google, everybody feels like they need AI in their
thing now, right?
There's a, I don't think you can raise money in Silicon Valley at the moment if you don't
have a generative AI play built into your product or built into your business strategy.
But that was true for a minute for crypto too.
And I'm not one of the people who makes a crypto AI analogy.
I think crypto is largely vaporware and AI is largely real.
But Silicon Valley is faddish and people don't know how to use things.
And so everybody tries to put things in all at once.
What product has actually gotten way better?
I'll just use one example.
There's an app you might be familiar with called Notion.
It's productivity sort of collaborative software.
I write a newsletter.
I save every link that I put in my newsletter into Notion.
And now that there is AI inside Notion, Notion can do a couple of things.
One, it can just look at every link I save and just write a two sentence summary for
me, which is just sort of nice to see at a glance what that story is about.
And most recently, it added a feature where you can just do Q&A with a database and say
like, hey, what are some of the big stories about Meta over the past few weeks?
And it'll just start pulling those up, essentially querying the database that I have built.
And so while we're very early in this, you're beginning to see a world where AI is taking
data that you have stored somewhere and it's turning it into your personal research assistant.
So is it great right now?
No, I would give it like a C.
For one point now, I think it's not bad.
And I'll share another example that is not from my own use, but I was talking a few
weeks ago with a doctor, who's a friend of a friend, and doctors, you get tons of messages
from patients.
You know, what's this rash?
Can you renew this prescription?
Do I need to come in for a blood test?
Like that kind of stuff.
And doctors and nurses spend a ton of time just opening up their message portal, replying
to all these messages.
It's a huge part of being a doctor and it's a part that they don't like.
And so this doctor was telling me that they have this software now that essentially uses
a language model.
I assume it's open AIs or someone very similar to that, that goes in and pre fills the responses
to patient queries.
And the doctor still has to look it over, make sure everything's right and press send.
But just that act of pre-populating the field, this person was saying that saves them a ton
of time, like on the order of several hours a day.
But if you have that and you sort of extrapolate to what if every doctor in America was saving
themselves an hour or two a day of responding to patient messages?
I mean, that's a radical productivity enhancement.
And so you can say that that's just fancy autocomplete and I guess on some level it
is, but just having fancy autocomplete in these paperwork heavy professions could be
very important.
Well, let me push out in two directions because one direction is that I am not super thrilled
about the idea that my doctor theoretically here is glancing over things and clicking
submit as opposed to reading my message themselves and having to do the act of writing, which
helps you think about things and thinking about what I actually emailed them and like
what kind of answer they need to give me.
I mean, I know personally the difference in thought between scanning things and editing
and thinking through things.
So that's like my diminishing response, but the flip of it is the thing I'm not hearing
anybody say here and the thing I keep waiting for and being interested in is the things
that I might be able to do better than my doctor.
I was reading Jack Clark's import AI newsletter today, which I super recommend to people who
want to follow advancements in the field and he was talking about a, I mean, it was a system
being tested, not a system that is in deployment, but it was better at picking up pancreatic
cancer from certain kinds of information than doctors are.
And I keep waiting to hear something like this going out into the field, right?
Something that doesn't just save people a bit of time around the edges.
I agree that's a productivity improvement.
It's fine. You can build a business around that.
But the promise of AI when Sam Altman sat with you all a few weeks ago or however long
it was and said, we're moving to the best world ever.
He didn't mean that our paperwork is going to get a little bit easier to complete.
Like he meant we'd have cures for new diseases.
He meant that we would have new kinds of energy possibilities.
I'm interested in the programs and the models that can create things that don't exist.
Well, to get there, you need systems that can reason.
And right now the systems that we have just aren't very good at reasoning.
I think that over the past year, we have seen them move a little away from the way that
I was thinking of them a year ago, which was a sort of fancy autocomplete, right?
It's sort of making it, making a prediction about what the next word will be.
This is true that they do it that way, but it is able to create a kind of facsimile of
thought that can be interesting in some ways.
But you just can't get to where you're going, Ezra, with like a facsimile of thought.
You need something that has improved reasoning capabilities.
So maybe that comes with the next generation frontier models.
But until then, I think you'll be disappointed.
But do you need a different kind of model?
This is something that lingers in the back of my head.
So I did an interview on the show with Demis Isabis, who's the co-founder of DeepMind.
Now we're going to see integrated DeepMind Google AI program.
And DeepMind had built this system while back called AlphaFold, which treated how
proteins are constructed in 3D space, which is to say, in reality,
we live in 3D space, it treated it as a game.
And it fed itself a bunch of information and it became very good at predicting the
structure of proteins and that solved this really big scientific problem.
And they then created a subsidiary of Alphabet called Isomorphic Labs to try
to build drug discovery on similar foundations.
But my understanding is that Google during this period became terrified of Microsoft
and open AI beating it up in search and office.
And so they pulled a lot of resources, not least Hasabis himself,
into this integrated structure to try to win the chatbot wars,
which is now what their system barred us trying to do.
And so when you said, Casey, that we need things that can reason, I mean, maybe,
but also you could say we need things that are tailored to solve problems we care
about more.
And I think this is one of the things that worries me a bit,
that we've backed ourselves into business models that are not that important for humanity.
Is there some chance of that?
I mean, are we going too hard after language-based general intelligent AI
that, by the way, integrates very nicely into a suite of enterprise software
as opposed to building things that actually create scientific breakthroughs,
but don't have the same kind of high-scalability profit structure behind them?
I would stick up for the people who are working on the sort of what you could call
like the non-language problems in AI right now.
This stuff is going on.
It maybe doesn't get as much attention from people like three of us as it should.
But if you talk to folks in fields like pharmaceuticals and biotech,
there are new AI biotech companies spinning up every day,
getting funding to go after drug discovery or some more narrow application.
Like we talked to a researcher the other day, formerly of Google,
who is teaching AI to smell, right?
Taking the same techniques that go into these transformer-based neural networks
like chat GPT and applying them to the molecular structures of different chemicals
and using that to be able to predict what these things will smell like.
And you might say, well, what's the big deal with that?
And the answer is that some diseases have smells associated with them
that we can't pick up on because our noses aren't as sensitive as, say, dogs or other animals.
But if you could train an AI to be able to recognize scent molecules
and predict odors from just chemical structures,
that could actually be useful in all kinds of ways.
So I think this kind of thing is happening.
It's just not sort of dominating the coverage the way that chat GPT is.
Let me ask you, Kevin, about, I think, an interesting, maybe promising,
maybe scary avenue for AI that you possibly personally foreclosed,
which is at some point during the year, Microsoft gave you access to a open AI-powered chatbot
that had this dual personality of Sydney.
And Sydney tried to convince you you didn't love your wife
and that you wanted to run away with Sydney.
And my understanding is immediately after that happened,
everybody with enough money to have a real business model in AI
lobotomized the personalities of their AI.
It's like, that was the end of Sydney.
But there are a lot of startups out there trying to do AI friends, AI therapists,
AI sex bots, AI, you know, boyfriends and girlfriends and non-binary partners.
Just every kind of AI companion you can imagine.
I've always thought this is a pretty obvious way this will affect society.
And the Sydney thing convinced me that the technology for it already exists.
So where is that?
And how are those companies doing?
Yeah, I mean, I'm sorry, A, if I did foreclose the possibility of AI personalities.
I think what's happening is it's just a little too controversial
and sort of fraught force any of the big companies to wait into.
Like Microsoft doesn't want its AI assistants and co-pilots to have strong personalities.
Like that much is clear.
And I don't think their enterprise customers want them to have strong personalities,
especially those personalities are adversarial or confrontational
or creepy or unpredictable in some way.
They want like, they want clippy, but like with real brain power.
But there are companies that are going after this more social AI market.
One of them is this company Character AI,
which was started by one of the original people at Google who made the transformer breakthrough.
And that company is growing pretty rapidly.
They've got a lot of users, especially young users.
And they are doing essentially AI personas.
You can make your own AI persona and chat with it
or you can pick from ones that others have created.
Meta is also going a little bit in this direction.
They have these sort of persona driven AI chatbots.
And all of these companies have put sort of guardrails around
like no one really wants to do the erotic, what they call erotic sort of role play
in part because they don't want to run afoul of things like the Apple app store terms of service.
But I expect that that will also be a big market for young people.
And anecdotally, I mean, I have just heard from a lot of young people
who already say like my friends have AI chatbot friends that they talk to all the time.
And it does seem to be making inroads into high schools.
And that's just an area that I'll be fascinated to track.
I mean, this is going to be huge.
A couple of thoughts coming to mind.
One, I talked to somebody who works at one of the leading AI companies
and they told me that 99% of people whose accounts they remove,
they remove for trying to get it to write tech space erotica.
OK, so that I think speaks to the market demand for this sort of thing.
I've also talked to people who have used the models of this
that are not constrained by any sort of safety guidelines.
And I've been told these things are actually incredible at writing erotica.
So what I'm telling you is there is a $10 billion.
You've got really a lot of reporting on this case.
You say maybe a personal interest.
Look, I write about content moderation.
And like porn is the content moderation frontier.
And it's just very interesting to me that it's so clear
that there are billions of dollars to be made here and no company will touch it.
And I asked one person involved, I said, why don't you just let people do this?
And they basically said, look, if you do this, you become a porn company overnight.
It like overwhelms the usage.
Like this is what people wind up using your thing for.
And you're you're working in a different company then.
So I sort of get it.
But, you know, even setting aside the explicitly erotic stuff, you know,
as you well know and have talked and written about just like the lowliness epidemic
that we have in this country, there's a lot of isolated people in this world.
And I think there is a very real possibility that a lot of those people
will find comfort and joy and delight with talking to these AI based companions.
I also think that when that happens, there will be a culture war over it.
And we will see lengthy segments on Fox News about how the Silicon Valley
technologists created a generation of shut-ins who wants to do nothing
but talk to their fake friends on their phones.
So I do think this is like the cultural war yet to come.
And the question is just sort of when do the enabling technologies get good enough?
And when do companies decide that they're willing to deal with the blowback?
I also think this is going to be a generational thing.
I mean, I'm very interested in this and have been for for a bit, in part
because I suspect if I had to make a prediction here, my five year old
is going to grow up with AI friends and my sort of pat line is that today
we worry that 12 year olds don't see their friends enough in person.
And tomorrow we'll worry that not enough of our 12 year old's friends or persons
because it's going to become normal.
And my sense is that the systems are really good.
If you unleashed them, you are already good enough to
functionally master this particular application.
And the big players simply haven't unleashed them.
I've heard from people at the big companies here who are like, oh, yeah,
if we wanted to do this, we could dominate it.
But that does bring me to a question, which is META kind of does want to do this.
META, which owns Facebook, which is a social media company,
they seem to want to do it in terms of these lame, seeming celebrity avatars.
Like you can talk to AI Snoop Dogg.
So bad.
But that is interesting to me because their AI division is run by Yanlacun,
who is one of the most important AI researchers in the field.
And they seem to have very different cultural dynamics in their AI shop
than Google DeepMind or OpenAI.
Tell me a bit about META's strategy here and what makes them culturally different.
Well, Casey, you cover META and have for a long time and may have some insight here.
My sense is that they are sort of up against a couple problems,
one of which is they have arrived to AI late and to generative AI specifically.
You know, Facebook was for many years considered one of the top two labs
along with Google when it came to recruiting AI talent,
to putting out cutting edge research, to presenting papers at the big AI conferences.
They were one of the big dogs.
And then they sort of had this funny thing happen
where they released a model called Galactica just right before ChatGPT was released last year.
And it was supposed to be this sort of like LLM for science and for research papers.
And it was out for, I think, three days.
And people started noticing that it was making up fake citations.
It was hallucinating.
It was doing what all the AI models do, but it was from META.
And so it felt different.
It had sort of this tarnish on it because people already worried about fake news on Facebook.
And so it got pulled down and then ChatGPT just shortly thereafter
launched and became this global sensation.
So they're sort of grappling for what to do with this technology that they've built now.
There's not a real obvious business case for shoving AI chatbots into products
like Facebook and Instagram, and they don't sell enterprise software like Microsoft does.
So they can't really shove it into paid subscription products.
So my sense from talking with folks over there
is that they're just kind of not sure what to do with this technology that they've built.
And so they're just flinging it open to the masses.
What do you think?
That tracks with me.
I sort of basically don't get it either.
Like basically what you've just said has been explained to me.
They are investing a ton with no obvious return on investment in the near term future.
I will say that these celebrity AI chatbots they've made are quite bad.
Like it's truly baffling.
And the thing is they've taken celebrities, but the celebrities are not playing themselves in the AI.
They've given all of the celebrities silly names and you can just sort of like follow their Instagram
and like send them messages and say like, hey, like character that Snoop Dogg is portraying.
Like, what do you think about it?
So it's all very silly.
And I expect it'll die a rapid death sometime in the next year.
And then we'll see if they have a better idea.
What I will say is like, if you're somebody who wakes up from AI nightmares
some mornings, as a lot of folks in San Francisco do, go listen to Jan LeCun talk about it.
No one has ever been more relaxed about AI than Jan LeCun.
You know, it's just sort of like an army of superhuman assistants
are about to live inside your computer.
They're going to do anything you want to do and there's no risk of them harming you ever.
So if you're, you know, you're feeling anxious, go listen to Jan.
Do you think he's right?
Because it also has led to policy difference.
Meta has been much more open source in their approach, which open AI
and Google seem to think is irresponsible.
But there's something happening there that I think is also built around a different view of safety.
Like, what is their view of safety?
Why does Jan LeCun, who is like an important figure in this whole world,
why is he so much more chill than, you know, name your other founder?
I mean, part of it is I just think these these are deeply held convictions
from someone who is an expert on this space and who has been a pioneer
and who understands the technology certainly far better than I do.
And he can just sort of not see from here to kill a robot.
So I I respect his viewpoint in that respect, given his credentials in the space.
I think on the question of is open source AI safer?
This is still an open question, not to pun.
The argument for it being safer is well, if it's open source,
that means that average people can go in and look at the code and identify flaws
and kind of see how the machine works and they can point those out in public
and then they can be fixed in public.
Whereas if you have something like open AI, which is building very powerful systems
behind closed doors, we don't have the same kind of access.
And so you might not need to rely on a government regulator
to see how safe their systems were.
So that is the argument in favor of open source.
Of course, the flip side of that is like, well, if you take a very powerful open source model
and you put it out on the open web, even if it's true that anyone can poke holes
and identify flaws, it's also true that a bad actor could take that model
and then use it to do something really, really bad.
So that hasn't happened yet, but it certainly seems like it's
an obvious possibility at some time in the near future.
Let me use that as a bridge to safety more generally.
So we've talked a bit about where these systems have gone over the past year,
where they seem to be going.
But there's been a lot of concern that they are unsafe and fundamentally
that they become misaligned or that we don't understand them or what they're doing.
What kind of breakthroughs have there been with all this investment
and all this attention on safety, Kevin?
So a lot of work has gone into what is called fine tuning of these models.
So basically, if you're making a large language model like GPT-4,
you have several phases of that.
Phase one is what's called pre-training, which is sort of just the basic process.
You take all of this data, you shove it into this neural network,
and it learns to make predictions about the next word in a sequence.
Then from there, you do what's called fine tuning.
And that is basically where you are trying to turn the model into something
that's actually useful or tailored, turned it into a chatbot,
turned it into a tool for doctors, turned it into something for social AIs.
That's the process that includes things like reinforcement learning from human feedback,
which is how a lot of the leading models are fine tuned.
And that work has continued to progress.
The models they say today are sort of safer and less likely to generate harmful outputs
than previous generations of models.
There's also this field of interpretability,
which is where I've been doing a lot of reporting over the past few months,
which is this sort of tiny subfield of AI
that is trying to figure out what the guts of a language model look like
and what is actually happening inside one of these models
when you ask it a question or give it some prompt and it produces an output.
And this is a huge deal, not only because I think people want to know how these things work,
they're not satisfied by just saying these are like mystical black boxes,
but also because if you understand what's going on inside a model,
then you can understand if, for example, the model starts lying to you
or starts becoming deceptive, which is a thing that AI safety researchers worry about.
So that process of interpretability research, I think, is really important.
There have been a few sort of minor breakthroughs in that field over the past year,
but it is still slow going and it's still a very hard problem to crack.
And I think it's worth just pausing to underscore what Kevin said,
which is the people building these systems do not know how they work.
They know at a high level, but there is a lot within that,
where if you show them an individual output from the AI,
they will not be able to tell you exactly why it said what it said.
Also, if you run the same query multiple times, you'll get slightly different answers.
Why is that? Again, the researchers can't tell you.
So as we have these endless debates over AI safety,
one reason why I do tend to lean on the side of the folks who are scared
is this exact point at the end of the day.
We still don't know how the systems work.
Tell me if this tracks for you.
I think compared to a year ago when I talked to the AI safety people,
the people who worry about AIs that become misaligned
and do terrible civilizational level damage, AIs that could be really badly misused.
They seem to think it has been actually a pretty good year, most of them.
They think they've been able to keep big models like GPT-4,
which of course are much less powerful than what they one day expect to invent.
But they think they've been pretty good at keeping them aligned.
They have made some progress on interpretability, which wasn't totally clear.
You know, a year ago, many people said that was potentially not a problem we could solve.
You know, at least we're making some breakthroughs there.
They're not relaxed.
The people who worry about this and they, you know,
will often say like we would need a long time to fully understand
even the things we have now and we may not have that long.
But nevertheless, I get the sense that the safety people seem a little more confident
that the work, the technical work they've been doing is paying off.
Then, you know, at least with the impression I got from the reporting prior.
I think that's right.
I mean, Sam Altman in particular, this has been his strategy is like,
we are we are going to release this stuff that is in our our labs.
And we're going to kind of wait and see how society reacts to it.
And then we'll sort of give it some time to let society address.
And then we will release the next thing.
That's what he thinks is the best way to slowly integrate AI into our lives.
And if you'd asked me maybe 11 months ago, like a month into using chat GPT,
what are the odds of something really, really bad happening
because of the availability of chat GPT?
I would have put them much higher than they turned out to be.
Right. And when you talk to folks at Open AI,
like they will tell you that that company really has taken AI safety
really seriously, you can see this yourself when you use the product.
Ask it a question about sex.
It basically calls the police.
So there is a lot to be said for how these systems have been built so far.
And I would say the other thing that I've heard from AI safety researchers
is that they are feeling relief, not just that the world has not ended,
but that more people are now worried about AI.
It was a very lonely thing for many years
to be someone who worried about AI safety because
there was no apparent reason to be worried about AI safety, right?
That the chatbots that were outward, it was like Siri and Alexa and they were terrible.
And no one could imagine that these things could become dangerous or harmful
because the technology itself was just not that advanced.
Now you have congressional hearings.
You have regulations coming from multiple countries.
You have people like Jeff Hinton and Yashua Bengios,
two of the sort of so-called godfathers of deep learning,
proclaiming that they are worried about where this technology is headed.
So I think for the people who have been working on this stuff for a long time,
there is some just palpable relief at like,
oh, I don't have to carry this all on my shoulders anymore.
The world is now aware of these systems and what risks they could pose.
One irony of it is that my read from talking to people is that AI safety
is going better as a technical matter than was expected.
And I think worse as a matter of governance and inter-corporate
competition and regulatory arbitrage than they had hoped.
There's a fear, as I understand it, that we could make the technical breakthroughs
needed, but that the kind of coordination necessary to go slow enough to make them.
Like that's where a lot of the fear is.
I think they feel like that's actually going worse, not better.
So one of the big narratives coming out of Sam Altman's firing was that it must
have had something to do with AI safety.
And, you know, based on my reporting and reporting shared by many others,
this was not an AI safety issue, but it is very much the story
that is being discussed about the whole affair.
And the folks who are on the board who are associated with these AI safety ideas,
they've taken a huge hit to their public reputation because of the way
they handle the firing and sort of all of that.
And so I think a really bad outcome of this firing is that the AI safety
community loses its credibility, even though AI safety, as far as we can tell,
really didn't have a lot to do with what happened to Sam Altman.
Yeah, I first agree that clearly, AI safety was not behind
whatever disagreements Altman and the board had.
I heard that from both sides of this and I didn't believe it and I didn't believe
it and I finally was convinced of it.
I was like, you guys had to have had some disagreement here.
It seems so fundamental.
But this is sort of what I mean, the governance is going worse.
All the open AI people thought it was very important and Sam Altman himself
talked about its importance all the time, that they had this non-profit board
connected to this non-financial mission, right?
The values of building AI that served humanity that could fire Sam
Altman at any time or even shut down the company fundamentally
if they thought it was going awry in some way or another.
And the moment that board tried to do that, now I think they did not try
to do that on very strong grounds.
But the moment they tried to do that, it turned out they couldn't.
That the company could fundamentally reconstitute itself at Microsoft
or that the board itself couldn't withstand the pressure coming back.
I think the argument from the board's side, the now mostly defunct board,
is that this didn't go as badly for them as the press is reporting,
that they brought in some other board members who are not cronies of Sam
Altman and Greg Brockman.
Sam Altman and Greg Brockman are not on the board now.
There's going to be investigation into Altman.
So maybe they have a stronger board that is better able to stand up to Altman.
That is one argument I have heard.
On the other hand, those stronger board members do not hold the views on AI
safety that the board members who left like Helen Toner of Georgetown
and Tasha Macaulay from Rand held.
I mean, these are people who are going to be very interested in whether or not
open is making money.
I'm not saying they don't care about other things too, but these are people
who know how to run companies.
They serve on corporate boards in a normal way where like the output of the
corporate board is supposed to be shareholder value and that's going to
influence them even if they understand themselves to have a different mission here.
Am I getting that story wrong to you?
No, I think that's right.
And it speaks to one of the most interesting and sort of strangest things
about this whole industry is that the people who started these companies
were weird.
And I say that with no sort of like normative judgment, but they made
very weird decisions.
Like they thought AI was exciting and amazing.
They wanted to build AGI, but they were also terrified of it to the point
that they developed these elaborate safeguards.
I mean, not in open AI's case, they put this nonprofit board in charge
of the for-profit subsidiary and gave essentially the nonprofit board the
power to push a button and shut down the whole thing if they wanted to.
At Anthropic, one of these other AI companies, they are structured as a
public benefit corporation and they have kind of this their own version
of a nonprofit board that is capable of essentially pushing the big red
shut it all down button if things get too crazy.
This is not how Silicon Valley typically structures itself.
Like Mark Zuckerberg was not in his Harvard dorm room building Facebook
thinking like if this thing becomes the most powerful communication
platform in the history of technology, like I will need to put in place these
checks and balances to keep myself from becoming too powerful.
But that was the kind of thing that the people who started open AI
in Anthropic were thinking about.
And so I think what we're seeing is that that kind of structure is sort
of bowing to the requirements of shareholder capitalism, which says
that, you know, if you do need all this money to run these companies to train
these models, you are going to have to make some concessions to the sort
of powers of the shareholder and of the money.
And so I think that one of the big pieces of fallout from this open
AI drama is just that open AI is going to be structured and run much more
like a traditional tech company than this kind of holdover from this nonprofit board.
And that is just a sad story.
I truly wish that it had not worked out that way.
I think one of the reasons why these companies were built in this way was
because it just helped them attract better talent.
I think that so many people working in AI are idealistic and civic
minded and do not want to create harmful things.
And they're also really optimistic about the power that good technology has.
And so when those people say that as powerful and good as these things could
be, it could also be really dangerous, I take them really seriously.
And I want them to be empowered.
I want them to be on company boards.
And those folks have just lost so much ground over the past couple of weeks.
And it is a truly tragic development, I think, in the development of this industry.
One thing you could just say with that is, yeah, it was always going to be up to governments
here, not up to strange nonprofit corporate, semi-corporate structures.
And so we actually have seen a huge amount of government activity in recent weeks.
And so I want to start here in the US.
Biden announced a big package of a big executive order.
You could call them regulations.
I sort of call them pre-regulations.
But Casey, how would you describe in some what they did?
Like, what is a Biden administration's approach that it is signaling to regulating AI?
The big headline was, if you are going to train a new model, so a sort of successor to a GPT-4,
and it uses a certain amount of energy, and the energy there is just sort of a proxy for how
powerful and capable this model might be, you have to tell the federal government that you have
done this, and you have to inform them what safety testing you did on this model before
releasing it to the public.
So that is the one kind of break that they attempted to put on the development of this
industry. It does not say you can't train these models.
It doesn't specify what safety tests you have to do.
It just says, if you're going to go down this road, you have to be in touch with us.
And that will, I think, slightly decelerate the development of these models.
I think critics would say it also pushes us a little bit away from a more open source version
of AI, that open source development is sort of chaotic by its nature.
And if you want to do some sort of giant open source project that would compete
with the GPTs of the world, that would just sort of be harder to do.
But to me, those are sort of the big takeaways.
One of the things that struck me looking at the order was, go back a year, go back two years.
I think the thing that people have said is that the government doesn't understand this at all.
It can barely be conversant in technology.
People remember Senator Orrin Hatch asking Mark Zuckerberg, well,
if you're not making people pay, then how do you make money?
When I read the order and looked at it, this actually struck me as pretty seriously engaged.
Like, for instance, there's a big debate in the AI world about whether or not you're going to
regulate based on the complexity and power of the model or the use of the model.
You have a fear about what happens if you're using the model for medical decisions.
But if you're just using it as your personal assistant, who cares?
Whereas the AI safety people have the view that, no, the personal assistant model might
actually be the really dangerous one because that's one that knows how to act in the real world.
The Biden administration takes a view of the AI safety people.
If you have a model over a certain level of computing complexity,
they want this higher level of scrutiny, higher level of disclosure on it.
They want everything that comes from an AI to be watermarked in some way,
so you can see that it is AI generated.
This struck me as a Biden administration actually clearly having taken this seriously
and having convened some set of group of stakeholders and experts that knew what they
were doing. I mean, I don't necessarily agree with literally every decision and a lot of it
is just asking for reports. But when you think about it as a framework for regulation,
it didn't read to me as a framework coming from people who had not thought about this for 10 minutes.
Absolutely. I was quite impressed. You know, I had a chance to meet with Ben Buchanan at the
White House who worked on this, talked to him about this stuff, and it is clear that they have been
reading everything. They've been talking to as many people as they can, and they did arrive
in a really nuanced place. And I think when you look at the reaction from the AI developers in
general, it was mostly like neutral to lightly positive, right? There was not a lot of blowback,
but at the same time, folks in civic society, I think, were also excited that the government did
have a point of view here and had done its own work. Yeah, it struck me as a very deft set of...
I think I would agree that they're more like pre-regulations than regulations. And to me,
it sounded like what the Biden White House was trying to do was throw a few bones to everyone.
What was like, we're going to throw a few bones to the AI safety community who worries about
foundation models becoming too powerful. We're going to throw some bones to the AI
harms community that is worried about things like bias and inaccuracy. And we're going to throw
some bones to the people who worry about foreign use of AI. So I saw it as a very sort of deliberate
attempt to give every sort of camp in this debate a little to feel happy about.
One of the things it raised for me as a question, though, was, did it point to a world where you
think that regulators are going to be empowered to actually act? This was the thing I was thinking
about after the board collapse. You imagine a world sometime in the future where you have open AI,
with GPT-6 or META or whomever, right? And they are releasing something that the regulator is
looking at the safety data, looking at what's there. They're just itchy about it. It's not
obviously going to do a ton of harm, but they're not convinced it's safe. They've seen some things
that worry them. Are they really going to have the power to say, no, we don't think your safety
testing was good enough. When this is a powerful company, when they won't be able to release a
lot of the proprietary data, right? The thing where the board could not really explain why
they were firing Sam Altman struck me as almost going to be the situation of virtually every regulator
trying to think about the future harms of a model. If you're regulating in time to stop a thing from
doing harm, it's going to be a judgment call. And if it's a judgment call, it's going to be a very
hard one to make. And so if we ever got to the point where somebody needed to flip the switch
and say, no, does anybody actually have the credibility to do it? Or is what we've seen that
in fact, like these very lauded successful companies run by smart people who have huge
Twitter followings or threads followings, whatever they end up being on, that they actually have
so much public power that they'll always be able to make the case for themselves. And like the
political economy of this is actually that we better just hope the AI companies get it right
because nobody's really going to have the capability stand in front of them.
When you talk to folks who are really worried about AI safety, they think that there is a high
possibility that at some point in let's say the next five years, AI triggers some sort of event
that kills multiple thousands of people. What that event could be, we could speculate, but
assume that that is true. I think that changes the political debate a lot, right? Like that's just
all of a sudden you start to see jets get scrambled. Hopefully that never happens, but I think that
the inciting moment. And this is the thing that just frustrates me as somebody who writes about
tech policy is we just live in a country that doesn't pass laws. There are endless hearings,
endless debates, and then it gets time to regulate something. And it's like, well, yeah,
they can regulate AI, but it's going to be based on this one regulation that was passed to deal with
like the oat farming crisis of 1906. And we're just going to hope that it applies. It's like,
we should pass new laws in this country. I don't know that there's a law that needs to be passed
today to ensure that all of this goes well, but certainly Congress is going to need to do something
at some point as the stuff evolves. I mean, one thing I was thinking about as this whole situation
at OpenAI was playing out was actually the financial crisis in 2008 and the scenes that were
captured in books and movies where you have the heads of all the investment banks and they're
scrambling to avoid going under and they're meeting in these boardrooms with people like
Ben Bernanke, the chair of the Federal Reserve, and the government actually had a critical role
there in patching together the financial system because they were sort of interested,
not in which banks survived and which failed, but in making sure that there was a banking system
when the markets opened the next Monday. And so I think we just need a new regulatory framework
that does have some kind of the sort of cliched word would be stakeholder, but someone who is
in there as a representative of the government who's saying, what is the resolution to this
conflict that makes sense for most Americans or most people around the world?
When you looked at who the government gave power to in this document, when you think about who
might play a role like that, when you need to call the government on AI, the way I read it is it
spread power out across a lot of different agencies. And there were places where I invested
more rather than less, but one thing that different people have called for that I didn't see it do,
in part because you would actually need to pass a law to do this, was actually create
the AI department, something that is funded and structured and built to do this exact thing,
to be the central clearinghouse inside the government, to be led by somebody who would
be the most credible on these issues. And would maybe have then the size and strength to do this
kind of research, right? The thing that is in my head here, because I find your analogy really
compelling, Kevin, is a federal reserve. The federal reserve is a big institution. And it has
significant power of its own in terms of setting interest rates. It also does a huge amount of
research. Like when you think about where would a public option for AI come from,
you would need something like that that has the money to be doing its own research and
hiring the really excellent people, in that case, economists, in this case, AI researchers.
And there was nothing like that here. It was sort of an assertion that we more or less have
the structure we need. We more or less have the laws we need. We can apply all those things
creatively. But it did not say like, this is such a big deal that we need a new institution
to be our point person on it. Yeah, I mean, I think that's correct. I think there are some
reasons for that. But I think you do want a government that has its own technological capacity
when it comes to AI, previous waves of innovation, certainly nuclear power during the Manhattan
project, but also things like the internet came out of DARPA. These are areas where the government
did have significant technical expertise and was building its own technology in sort of competition
with the private sector. There is no public sector equivalent of chat GPT. The government has not
built anything even remotely close to that. And I think it's worth asking why that is and what would
need to happen for the government to have its own capacity, not just to evaluate and regulate
these systems, but to actually build some of their own. I think it is genuinely strange on some level
that given how important this is, there is not a bill gathering steam. Look, the private sector
thinks it is worth pumping 50 or $100 billion into these companies so they can help you make
better enterprise software. It seems weird to imagine that there are not public problems that
have an economic value that is equal to that or significantly larger. And we may just not want
to pay that money fine. But we do that for infrastructure. We just passed a gigantic
infrastructure bill. And if we thought of AI like infrastructure, we actually also spend a lot of
money on broadband now. It seems to me you want to think about it that way. And I think it is a
kind of fecklessness and cowardice on the part of like the political culture that it no longer
thinks itself capable of doing things like that. Like at the very least, and I've said this I think
on your show probably, I think they should have prize systems where they say a bunch of things
they want to see solved. And if you can build an AI system that will solve them, they'll give you a
billion dollars. But the one thing is the government does not like to do things that spend money for
an uncertain return. And building a giant AI system is spending a lot of money for an uncertain return.
And so the only part of the government that is probably doing something like it is a defense
department in areas that we don't know. And that does not make me feel better. That makes me feel
worse. That's my take on that. Yeah, I mean, I think there's also a piece of this that has to do
with labor and talent. You know, there are probably on the order of several thousand people in the
world who can oversee the building, training, fine tuning deployment of large language models.
It is a very specific skill set. And the people who have it can make gobs of money in the private
sector working wherever they want to the numbers that you hear coming out of places like open AI
for what engineers are being paid there. I mean, it's like NFL level football compensation packages
for some of their people. And the government simply can't or won't pay that much money to
someone to do equivalent work for the public sector. Now, I'm not saying they should be paying
engineers millions of dollars of taxpayer money to build these things, but that's that's what you
would need to do if you wanted to compete in an open market for the top AI talent. I am saying
they should. I am saying this is like the factlessness and cowardice point. This is stupid.
You think there should be AI engineers working for the federal government making five million
dollars a year? Like maybe not five million dollars a year. But it would this is a this thing
that we don't think civil servants should make as much as people in the private sector.
Because I don't know somebody at a congressional hearing is going to stand and be like that person's
making a lot of money. That is a way we rob the public of value. If Google's not wrong,
Microsoft is not wrong that you can create things that are of social value through AI.
And if you believe that, then leaving it to them, I mean, they intend to make a profit.
Why shouldn't the public get great gains from this? It won't necessarily be through profit.
But, you know, if we could cure different diseases or, you know, make big advances on energy,
I just this way of thinking is actually to me the really significant problem. I'm not sure you
would need to pay people as much as you're saying because I actually do think a lot of,
I mean, we both know the culture of the AI people and at least up until a year or so ago.
It was weird. And a lot of them would do weird things and are not living very lush lives.
You know, they're in group houses with each other, taking psychedelics and working on AI
on the weekdays. But I think you can get people in to do important work and you should.
Now, look, you don't have the votes to do stuff like this. I think that's the real answer.
But in other countries, they will and do. Like when Saudi Arabia decides that it needs an AI
to be geostrategically competitive, it will take the money it makes from selling oil to the world.
And in the same way that it's currently using that money to hire sports stars,
it will hire AI engineers for a bazillion dollars and it will get some of them. And then it will
have a decent AI system one day. I don't know why we're waiting on other people to do that. We're
rich. It's stupid. I agree with you, Ezra. And I'm sorry that Kevin is so resistant to your ideas
because I think paying public servants well would do a lot of good for this country.
Look, I think public service should be paid well. I'm just saying when Jim Jordan
gets up and grills the former deep mind engineer about why the Labor Department is paying them
2.6 million dollars to fine tune language models, I'm not sure what the answer is going to be.
No, I agree with you. I think we're all saying in a way the same thing. It's like,
this is a problem. Government by dumb things Jim Jordan says is not going to be a great government
that takes advantage of opportunities from the public. Good. And that sucks. It would be better
if we were doing this differently and if we thought about it differently. Let me ask about China
because China is where on the one hand, at least on paper, the regulations look much tougher.
So one version is maybe the regulating AI much more strictly than we are. Another view that
I've heard is that in fact, that's true for companies, but the Chinese government is making
sure that it's building very, very strong. Do you know to the extent you all have looked at it,
how do you understand the Chinese regulatory approach and how it differs from our own?
I mean, I've looked at it mostly from the standpoint of what are the consumer facing systems
look like. It has only been I think a couple of months since China approved the first consumer
usable chat GPT equivalent. As you might imagine, they have very strict requirements as far as like
what the chatbot can say about Tiananmen Square. So they wind up being more limited maybe than
what you can use in the United States. As far as what is happening behind closed doors and for
their defense systems and that sort of thing, I'm in the dark. So four or five years ago when I
started reporting a book about AI, the conventional wisdom among AI researchers was that China was
ahead and they were going to make all of the big breakthroughs and beat the U.S. technology
companies when it came to AI. So it's been very surprising to me that in the past year since chat
GPT has come out, we have not seen anything even sort of remotely close to that level of performance
coming out of a Chinese company. Now, I do think they are working on this stuff, but it's been
surprising to me that China has been mostly absent from the frontier AI conversation over the past
year. And do you think those things are related? Do you think that the Chinese government's
risk aversion and the underperformance of at least the products and systems we've seen
in China? I mean, there might be things we don't know about. Do you think those things are connected?
Absolutely. I think you do need a risk appetite to be able to build and govern these systems because
they are unpredictable. We don't know exactly how they work. And what we saw, for example,
with Microsoft was that they put out this Bing Sydney chatbot and it got a lot of attention and
blowback and people reported all these crazy experiences. And in China, if something like that
had happened, they might have shut the company down or they might have been deemed such an
embarrassment that they would have radically scaled back the model. And instead, what Microsoft
did was just say, we're going to make some changes to try to prevent that kind of thing from
happening, but we're keeping this model out there. We're going to let the public use it and
they'll probably discover other crazy things and that's just part of the learning process.
That's something that I've been convinced of over the past year, talking with AI executives and
people at these companies, is that you really do need some contact with the public before you start
learning everything that these models are capable of and all the ways that they might misbehave.
What is the European Union trying to do? They've had draft regulations that were
seemed very expansive. What has been the difference in how they're trying to regulate this versus
how we are and what in your understanding is the status of their effort?
Europe was quite ahead with developing its AI Act, but it was written in a pre-chat GPT world.
It was written in a pre-generative AI world. And so over the past year, they've been trying to
retrofit it so that it reflects our new reality and is caught up in debate in the meantime.
But my understanding is the AI Act is not particularly restrictive on what these companies
can do. So to my understanding, there's nothing in the AI Act that is going to
prevent these next generation technologies from being built. It's more about companies being
transparent. Let me add a little bit of flavor to that because I was in Europe just recently
talking with some lawmakers. And one of the things that people will say about the AI Act
is that it has this risk-based framework where different AI products are evaluated and regulated
based on these classifications of this is a low-risk system or this is a high-risk system
or this is a medium-risk system. And so different rules apply based on which of those buckets
a new tool falls into. And so right now, what a lot of regulators and politicians and companies
and lobbyists in Europe are arguing about is what level of risk should something like a
foundation model, a GPT-4, a Bard, a Claude, are those low-risk systems because they're just chat
bots or are they high-risk systems because you can build so many other things once you have
that basic technology? And so that's what my understanding is of the current battle in Europe
is over whether foundation models, frontier models, whatever you want to call them,
whether those should be assigned to one risk bucket or another.
I think that's a good survey of the waterfront. And so I guess I'll end on this question, which is
all right, we're talking here at the one-year anniversary roughly of chat GPT.
If you were to guess, if we were having another conversation a year from now on the
two-year anniversary, what do you think would have changed? What are one or two things each
of you think is likely to happen over the next year that did not happen this year?
I think all communication-based work will start to have an element of AI in it. All email, all
presentations, office work essentially. AI will be built into all the applications
that we use for that stuff. And so it'll just be sort of part of the background,
just like autocomplete is today when you're typing something on your phone.
I would say that AI is going to continue to hollow out the media industry. I think you're
going to see more publishers turning to these really bad services that just automate the generation
of copy. You'll see more sort of content farms springing up on the web. It'll reduce publisher
revenue and we'll just see more digital media businesses either get sold or sort of quietly
go out of business. And that's going to go hand in hand with the decline of the web in general.
A year from now, more and more people are going to be using chat GPT and other tools
as their kind of front door to internet knowledge. And that's just going to sap a lot of life out
of the web as we know it. So we don't need one more technological breakthrough for any of that
to happen. That's just a case of consumer preferences taking a while to change. And I
think it's well underway. So do you think then that next year we're going to see something that
has been long predicted, which is significant AI related job losses? Is that sort of the argument
you're making here? I think that to some degree, it already happened this year in digital media.
And yes, I do think it will start to pick up. Just keep in mind, 12 months is not a lot of
time for every single industry to ask itself, could I get away with five or 10 or 15% fewer
employees? And as the end of this year comes around, I have to believe that in lots and lots
of industries, people are going to be asking that question. Yeah, I agree. I don't know whether
that there will be sort of one year where all the jobs that are going to vanish, vanish. I think
it's more likely to be a slow trickle over time. And it's less likely to be mass layoffs than just
new entrants that can do the same work as incumbents with many fewer people. The software
development firm that only needs five coders because they have all their coders are using AI
and they have software that is sort of building itself competing with companies that have 10,000
engineers and doing so much more capably. So I don't think it's going to necessarily look like
all the layoffs hit on one day or in one quarter or even in one year, but I do think we're already
seeing a displacement of jobs through AI. Those are kind of dark predictions. I mean,
we'll have a little bit better sort of integration of AI into office tools and also we'll begin to
see really the productivity improvements, create job losses. Is there anything that you think is
coming down the pike technologically that would be really deeply to the good things that are
not too far from fruition that you think will make life a lot better for people?
I mean, I love the idea of universal translators. It's already pretty good using AI to speak in
one language and get output in another, but I do think that's going to enable a lot of cross-cultural
communications and there are a lot of products remaining to be built that will essentially
just drop the latency so that you can talk and hear in real time and have it be quite good.
So that's something that makes me happy.
And I'm hopeful that we will use AI, not we as in me and Casey.
But we might.
This would be sort of a career change for us, but we as in society, I have some hope that we will
use AI to cure one of the sort of top deadliest diseases, cancer, heart disease, Alzheimer's,
things like that that really affect massive numbers of people. I don't have any inside
reporting that we are on the cusp of a breakthrough, but I know that a lot of energy and research and
funding is going into using AI to discover new drugs and therapies for some of the leading
killer diseases and conditions in the world. And so when I want to feel more optimistic,
I just think about the possibility that all of that bears fruit sometime in the next few years,
and that's pretty exciting.
All right. And then also final question, what are a few books you'd each recommend to the
audience released recommend the audience ask chat GPT to summarize for them?
Kevin, you want to go first?
Sure. I actually have two books in a YouTube video. The two books, one of them is called
electrifying America by David E. Nye. It is a 30 year old history book about the process by which
America got electricity. And it has been very interesting to read. I read it first a few years
ago and have been rereading it just to sort of sketch out what would it look like if AI really
is the new electricity? What happened the last time society was transformed by technology like
this? The other book I'll recommend is your face belongs to us by my colleague, our colleague at
the Times, Cashmere Hill, which is about the facial recognition AI company, Clearview AI,
and is one of the most compelling tech books I've read in a few years. And then the YouTube
video I'll recommend was just posted a few days ago. It's called Intro to Large Language Models.
It's made by Andre Karpathy, who is an AI researcher actually at Open AI. And it's his one hour
introduction to what is a large language model and how does it work? And I've just found it
very helpful for my own understanding. Casey? Well, as we're with permission, and given that
Kevin has just given your listeners two great books and a YouTube video to read, I would actually
like to recommend three newsletters if I could. And the reason is because the books that were
published this year did not help me really understand the future of the AI industry. And to
understand what's happening in real time, I really am leaning on newsletters more than I'm
leaning on books. So is that okay? Yeah, go for it. All right. So the first one,
cruelly, you already mentioned earlier in this podcast, it's import AI from Jack Clark. Jack
co-founded Amthropic, one of the big AI developers. And it is fascinating to know which papers he's
reading every week that are helping him understand this world. And I think that they're arguably
having an effect on how Amthropic is being created because he is sitting in all of those rooms. So
that is just an incredible weekly read. I would also recommend AI Snake Oil from the Princeton
Professor, Arvind Narayanan and a PhD student at Princeton, Syash Kapoor. They're very skeptical
of AI hype and doomsday scenarios, but they also take the technology really seriously and have
a lot of smart thoughts about policy and regulation. And then the final one is Pragmatic
Engineer by this guy, Gurgely Arose. He's this former Uber engineering manager. And he writes
about a lot of companies, but he writes about them as workplaces. And I love when he writes about
open AI as a workplace. He interviews people there about culture and management and process. And he
just constantly reminds you, they're just human beings showing up to the office every day and
building this stuff. And it's just a really unique viewpoint on that world. So read those
three newsletters. You'll have a little better sense of what's coming for us in the future.
What Casey didn't say is that he actually hasn't read a book in 10 years. So it was a bit of a
trick question. You know what I will say? I did read Your Face Belongs to Us by Cashion,
incredible book. Definitely read that one. Sure you did. There you go. Casey Newton,
Kevin Ruse at your podcast, which requires very little reading. It's hard for it.
Thank you all for being on the show. It's the first illiterate podcast, actually,
put out by the New York Times. Thank you for having us. Thanks, Ezra. Thanks, guys.
This episode of the Ezra Klein Show is produced by Roland Hoof, fact-checking by Michelle Harris
with Kate St. Clair and Mary Marge Locker. Our senior engineer is Jeff Geld. Our senior editor
is Claire Gordon. The show's production team also includes Emma Vagabou and Kristen Lin,
original music by Isaac Jones, audience strategy by Christina Samaluski, and Shannon Busta.
The executive producer of New York Times' opinion audio is Anero Strasser. And special
thanks to Sonia Herrero.

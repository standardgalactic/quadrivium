So maybe I will start. So I'm Michael Brunstein. I'm a professor at Imperial College London
and head of Craft Learning Research at FITRA. And today I would like to talk about geometric deep
learning. So you might be wondering, what is geometric deep learning? What does it have to do
with geometry or vice versa? What does geometry have to do with deep learning? So allow me to start
maybe from taking your deep into the past. So for more than 2000 years, when we said the word
geometry, it was uniquely understood as Euclidean geometry, because simply no other geometry existed.
And this Euclidean monopoly came to an end in the 19th century with the first construction
of non-Euclidean geometries. I think the first one is credited to the Robochevsky,
Janusz Bojałi, Carl Friedrich Gauss himself entertained himself with these constructions,
even though we never published them. And then in 1856, Erwin Riemann Gauss, a student, published what
we now call Remanian geometry. So what happened though very quickly is that basically these fields
different types of geometry became siloed and completely disjoint from each other. And there
was a big fight between mathematicians of that time, who is right, whose geometry is better,
what actually defines the geometry, and what is the relation between the different types of geometry.
So the solution to this big, a big all came from a Telix Klein, a young professor that was appointed
at the University of Erwangen in 1872, and as it was custom at the time in Germany,
he was asked to write and present a research prospectus, where he would outline the research
for the end of his career, for the rest of his career. He was actually only 23 when he got this
full professor appointment. And he wrote these prospectus that entered the history of mathematics
as the Erwangen program, in which he proposed quite a radically new idea. And the new idea was
to approach geometry as the study of invariance or symmetries. In other words,
properties that are preserved that are some class of transformations. And the class of
transformations was formalized using the language of group theory, which was a new creation at that
time. It was first introduced by Galois in, I think, 1830, but then climbed together with Sophosly,
worked extensively on this. So he just used these apparatus to formalize geometry. And in this way,
the relation between geometry and what defines a geometry became completely clear,
because the moment you define a symmetry group, basically the class of transformations that you
apply, you immediately have certain structures that are preserved. So in Euclidean geometry,
for example, when they consider rigid motions, there are many things that are preserved like
areas, distances, angles. If you take a bigger group, the fine group, then many of these properties
are not preserved, but you have, for example, parallelism, and then projective geometry is
probably the broadest one. And immediately the relations are also evident because the fine group
is a subgroup of the projective and the Euclidean is a subgroup of the fine group. And this idea was
remarkably productive in mathematics. It allowed to solve problems that were very difficult or
completely unsolvable with the older tools. But it also filled into other fields, in particular,
into physics. And in physics, Amy Neuter, who also was at Göttingen, where Klein ended up for the
rest of his career, she was exposed to these ideas. And she proved a remarkable theorem
that allowed to derive conservation laws and physical systems from fundamental principles
of symmetry. So this is, if you think of it, this is completely mind blowing because before that
conservation laws, like conservation of energy, were completely experimental. So you would observe
an experiment for thousands of time and you see that the energy is preserved. So you would reduce
that energy preservation. But here, for the first time, there was a mathematical way of deriving
conservation laws from first principles of symmetry, like conservation of energy, for example,
stems from symmetry of time. And these ideas had a profound impact in physics. So Hermann Baill
came up with the concept of gauge invariance that in a generalized form was developed by
young and mills to unify different forces, which culminated in the standard model, the term that
was introduced in 1975. And this is all physics that we know nowadays, maybe with the exception of
gravity that is not unified. It is all, I should quote Philip Anderson and another
Nobel laureate in physics, that it's only slightly overstating to say that physics is
the study of symmetry. So symmetry is really fundamental and really profound and has long
consequences. The question is, what does it have to do with machine learning? So if we look at
machine learning problems, at least in the simple setting of supervised learning,
this is essentially a function approximation problem. So we are given some function that maps
as from the input space, let's say images of different animals to the output space,
let's say the labels. And in this case, we try, for example, to label the images, whether it's
cats or dogs. So the problem of function estimation is very well understood problem. And if you look
at the first neural network systems, so the perceptrons, it became apparent that even though
these networks are very simple and they can approximate only simple functions, combine
two such networks in two layers, what is called multilayer perceptron. And then you can represent
step functions. The moment they can represent step functions, you can approximate any continuous
functions to any desired accuracy. This property is called universal approximation. It was
proven in the 80s. And basically, it means that these networks are very expressive. Practically,
anything, any good function can be represented in this way. Now, is it good or bad? Basically,
the problem of approximating a function from a finite set of samples is very well studied. So in
low dimensions, this has been researched to death, we have a very good mathematical control
over the error, we know what classes of functions we can use and so on and so forth.
But unfortunately, machine learning is not a low dimensional problem. Most of the machine learning
problems need to deal with data that lives in thousands or maybe millions of dimensions.
And there the situation is entirely different. So this is an example of a leafshitz function
that looks like Gaussian blobs that are attached to a bowl and positioned in different quadrants
of a unit cube. So the moment we start growing the dimensions, you see that the number of samples
that is needed to represent all these functions grows exponentially. So this is what a phenomenon
we typically call the curse of dimensionality. It has many different manifestations. This is one
of them. So to approximate a smooth function, a continuous function in the dimension, you need
exponentially large number of samples, which in practical situations make this learning completely
infeasible. And this is manifested in practical problems. So if you apply, for example,
these simple neural networks to images, so you can just vectorize the image right into
long vector pixels, the problem is that if I just shift the image, this representation is
completely oblivious of the geometric structure of the input, right? It doesn't care that it's
a two dimensional grid for such a neural network, it's just a vector. And therefore we'll need a
lot of examples to show to the neural network of different positions of the image in order to
train it to be shift invariant. And this problem, it never really worked. And this was one of the
reasons why Fukushima in his paper on Neo-Cogintron complained about perceptrons and similar
architectures that they cannot cope with shifts of the input. He applied this network to image
recognition that was first attempted around the time. And his inspiration came from works in
neuroscience, the study of the visual cortex of animals, where it was shown that there is
a local connectivity, what is called receptive fields. And this idea culminated in the classical
work of Tian Likang in the end of the 80s, the convolutional neural networks that basically
take advantage of this local connectivity and way sharing structure. Now, here's another problem.
So here is a graph. So this is a molecule of caffeine, if you're interested. And
let's say that we want to predict some of its property, let's say buying new energy of some
target. So here, as before, we can parse the node features into a vector. But now we don't really
have a canonical order of the nodes, like we might have with the two-dimensional grid. So any
permutation here would work. And it appears that graphs are extremely common in many fields of
science. They can be used to model social networks. They can be modeled to represent interaction between
different biological entities in our body. They can be used in computer graphics, maybe with some
extra structure in the form of meshes, and so on and so forth. And in, I think in 2015 or something
like this, so when I was writing my ERC grant, already everyone was working on deep learning. So
I wanted somehow to stand apart and I decided to call it geometric deep learning with the idea that
we want to bring some fundamental geometric principles that would allow us to build
more principled deep learning architectures and vice versa, apply deep learning techniques
that were very successful in computer vision to other non-euclidean geometric data. And this
concept was popularized in the paper that we wrote with John Bruner, Young LeCun, Arthur Slum,
and Pierre van der Kijn. So now geometric deep learning is used almost synonymously with
graph neural networks and graph representation learning, but I hope to convince you, even though
today I will be speaking mainly about graphs, that this is just a small piece of a broader picture.
And we call this picture geometric priors. So what I'm showing today is actually based on a textbook
that I'm working on with a few colleagues with John Bruner, Petra Belichrich, and Taco Cohen.
So this is in a sense the first time that I'm trying to present it in this way.
I hope that it works well. So if we look at our chi-dimensional learning problem when our input
lives in the chi-dimension, we need to understand that in many problems, like in case of images,
it's not just chi-dimensional data. It has some underlying geometric structure. And these
geometric structure forms an important and very powerful prior, we call it geometric prior.
So in the case of images, basically the image lies on the grid, right? So the way to think of it,
f is a function that takes an image and produces a label, then the image itself is an element in
a vector space of functions defined on the grid that is here denoted by omega. So the geometric
structure of the grid is represented by the symmetry group. And here I should say that
there are many ways of choosing the symmetry group, because, for example, if we want to
deal with translations, we'll look at the translation transformations, which are closed
under composition. So it is a group. We can look also at translations together with rotations.
This is what is called a special Euclidean group. And finally, we can look at the full Euclidean
isometric group, which also includes reflections. So the moment we define the group, what's important
to understand about groups that they do not model how group elements act on other objects.
So the group is completely abstract object. It just tells us how its elements are composed
with each other. So here we are interested in the actions of the group elements on our domain.
And in this case, the actions of the translation group, they just take points on the domain,
on the grid, or two-dimensional plane, and translate it. Now, because we have an image or
a signal that is defined on this grid, the translation, the application of a group element
to the grid also manifests itself in some change in the image. And this is done in
representation theory. It's from what is called the group representation. I denote it here by
rock. So in case of translations, this is the shift operator. And you can see what it does
to the image. So basically, it shifts it by a vector that corresponds to the group element
here by G. And this is a very powerful concept. Basically, we can define functions that
transform differently under the action of the group. So there are two ways of doing it. One
of them is called invariance. So we call it G invariance, or invariance under group G.
When, if I apply the group action on the image for its representation,
rock, and I apply then a function to it, I get the same result as if I applied to
an transformed image. So like shifting invariance, no matter where the digit is located in the image,
I still want to say that this is a digit free. Another possibility is equivariance.
And actually, these terms are very frequently confused. So equivariance means that if the
output of the function is in the same space as the input, then if I apply first the
transformation, the shift in this case, and then the function, or vice versa,
first the function and then the shift, the result will still be the same.
So in other words, these two operations commute. Now, you may ask why I write here a rock prime,
because the output of the function doesn't necessarily need to live in the same space. In
fact, in most deep learning architectures, it doesn't. So I can apply, for example,
multiple convolutional filters to RGB image and get a 64 dimensional feature space.
So I will need to use here a different representation, but I hope the idea is clear.
And here I can get a general blueprint for geometric deep learning.
So we will construct a deep neural network from a collection of equivariant layers.
So the input will be typed through these layers, which will be applied in sequence.
And this way, a transformation on the domain to the data that is provided as input
will affect in the same way the output. And finally, if we want, for example, to the
classification, we'll apply an invariant layer. I should say that it's common also,
there is another geometric prior that I will not talk in details about,
which is a scale separation. And this is manifested in pooling. Basically, I need
to coarsen my domain. If I have a way to coarsen the domain, and I can approximate
the functions by projecting them to the coarsed domain and then computing the function
on the coarsed domain. So in images, it works very well because we can coarsen the grid and
still preserve the structure of the image when we download sample it. So this is usually
implemented as a max pooling in convolutional neural networks. So you can see that a lot of
deep learning architectures, I would say probably the majority fall under this blueprint.
And in fact, we like thinking of the different objects that can be addressed with this framework
as the 4G of geometric deep learning. So the first of them is grids. So these are convolutional
neural networks. We can generalize grids still working with global transformations on some
homogeneous space, such as rotations. So we can call these groups. Then there are graphs,
which have, as we'll see, permutation invariance. And finally, many falls, well, to keep to the 4G.
We like calling it using the high energy physics terminology gauges, which is a physicist's term
for the selecting of the prime preference of many falls. So think of it as the Erlangian program
of deep learning. And I know that by using this bombastic title, I fall into the risk of sounding
arrogant, basically irreverently comparing myself with the great Felix Klein. So let's keep it modest.
So Erlangian program, in the sense that we use the analogy or the philosophy of Klein
of describing the structure of the domain using the language of group theory and looking at
invariance as a form of introducing inductive bias into our architecture. So I should say that
this concept is not novel at all. It has been explored in many fields of science, including in
machine learning. And if we look at the progress in deep learning, so multi-layer perceptrons,
they have weak inductive bias. It's not true to say that they don't have any inductive bias,
even though they can represent any function, because usually we use regularization techniques,
such as weight, decay, and so on, which impose some function regularity. So the function will
belong to a certain class of function. CNNs, convolutional neural networks, they come from
the perspective of translation symmetry. This can be generalized with group and covariance,
CNNs, so for global rotations. In graph neural networks, we have permutation and active bias.
And for intrinsic and bagager covariance, CNNs, this is the local frame of choice.
So I would like to talk about graphs, because graphs are really universal models for systems of
relations and interactions. You can find them everywhere at different scales of problems from
nano-scale modeling molecules with graphs to interaction networks between different biological
entities, such as protein-protein interaction networks, and finally to macroscopic-scale
social networks, patient networks, you name it. And graph neural networks, I'm surprisingly,
become recently one of the hottest topics in machine learning. We can model practically
everything as a graph. So let's look at graphs from this perspective of geometrically learning.
A graph, well, probably the first thing that comes to your mind is a social network,
where the nodes are users and the edges represent their social relations or interactions.
So mathematically, a graph is a collection of nodes, right, that are some abstract entities,
and edges that are just pairs of nodes. So we can consider either ordered pairs, which,
in this case, the graph is directed, or unordered pairs in which case the graph is undirected.
And let's assume that for simplicity, we assign some vector d-dimensional features
to each of the nodes. And let's assume that the edges do not have any features, even though
what I will describe next is trivially generalizable to edge attributes as well.
So this is our construction. One key thing to understand about graphs is that
they don't have any canonical order of the nodes. So this is really the key structural
characteristic of graphs. Basically, when I take my node features and I arrange them into a matrix,
I have automatically prescribed some arbitrary ordering of the nodes.
So this goes for the node features. This also goes for the adjacency matrix,
that I denote by A, that describes the structure of the graph. So it has
non-zero positions where we have edges between the nodes, right? And you can see that I can
choose any arbitrary order. The red here shows the position of the red node,
and I can permute it in n factorial different points, right? So p here denotes the permutation
matrix that shuffles the rows of the feature matrix or the rows and columns of the adjacency
matrix of the graph. So if I want to represent functions that work on this graph, I need to make
sure that I'm either invariant through the permutation of the nodes, right? So it means that
if I apply the permutation to the rows of x and the rows and columns of A,
I will get the same output, right? The attention that f here now depends not only on the node
features, but it also, we need to provide the structure of the graph in the form of the adjacency
matrix. Permutation equity variance will come in the form of the output of function being node-wise
permuted in the same way as the equal, okay? So I hope this clear. So the way that these
functions are constructed is usually looking at the local neighbors. So I will look at the
neighbors at the nodes that are connected to my node i, and I will take the features at these nodes.
So one thing that is important to understand, though, that even though the neighbors, their
indices are unique, the features are not necessarily unique. So here you can see that
two different nodes have the same features. I denote them by this blue color. So basically,
it's a generalized notion of a set where the same element can be repeated more than once.
We call this multiset or a bag, okay? So that's the features aggregated from the
from the neighbors. And we can define a local function that acts on the current
node feature and the multiset of the neighbor node features, okay? So again, because there is no
canonical ordering of the elements in this multiset, this phi must work in the permutation
invariantly, okay? So no matter how I permute the rows and these in these matrix. And I can
repeat this process many times at each node. So by this construction, I hope you can see that
the function that we will eventually compute on all the nodes of the graph will be permutation
equivariable. So by applying local permutation invariant functions, I get permutation equivariable
function on the entire graph. And it appears that the choice of this function is extremely important.
So basically the expressive power of this, of this choice can be related to the following example.
So here the black node is my node i, which I'm, which I'm computing the function. And here you
can see three different structures. So let's consider three possible aggregation functions.
They're all permutation invariant, maximum, sum and mean, okay? So maximum would not care how many
times the red node for them will appear, right? So the maximum of the left graph and the central
graph will be the same, right? But the mean will be different, okay? Now if I look, if I apply the
mean to the central and the right graph, it doesn't matter that the multiplicity of the,
of the features doesn't matter. So the mean will produce the same result, but the sum will not.
So the bottom line that the choice of this function is important. So we need to select
the function that is an injective. And we can relate this to graph isomorphism test.
So we say that two graphs are isomorphic if they are structurally the same,
meaning that there is an H preserving projection. In other words, if we look at their adjacency
matrices, I can permute one of them into another. Okay? Interestingly, the computational complexity
of testing whether two graphs are isomorphic is unknown. We know that there is currently no
polynomial time algorithm, but we also know that it's not NP-hard. So usually it is placed into its
own complexity class called GI, GI complexity. And a classical algorithm in graph theory,
called the Weiss-Feller-Lehmann test, proposes a color refinement procedure that takes these
multi sets of neighbors and applies an injective function to them. So you start with a graph that
has all the nodes colored in the same way. And then you refine the colors based on the structure
of the neighborhood. So here we have two different neighborhoods initially, neighborhoods with two
neighbors or three neighbors. So they get different colors, different labels. Now,
when I apply the same procedure again, I now have three different neighbors. So I have two yellow
neighbors, green and yellow neighbor and yellow and two green neighbors. So these get different
colors. We now have three different labels. But if I repeated the game, I don't change the color.
So at this point, I stop and I compute the distribution of different colors or different
labels. And if I take another graph and I compute the distribution of color using the same procedure,
if the colors are different, then I can for sure say that the graphs are non-isomorphic.
But if the colors are the same, then I don't know. They're possibly isomorphic. In other words,
this is a necessary but insufficient condition. And we know that there exist examples of graphs
for which the Weiss-Ferrand-Lehmann test fails. So they're not isomorphic, but they
would consider it possibly isomorphic. And here you can see an example. So the graph on the right
has triangles, whereas the graph on the left doesn't. So these graphs are undistinguishable
using the WL test. So let me go back to the different aggregation functions. And you see
in the graph learning literature, there is a zoo of different architectures. Fortunately,
most of them fall into one of these three flavors. So basically, the update of the feature, right?
So remember that in node i, we aggregated the features from the neighbors. And we then updated,
we produced a new feature vector that node. So we need some permutation and variety aggregation,
as you noted by the square operator. It can be usually a sum or a maximum. We have two learnable
functions, psi that transforms the neighbor node features, and phi that transform vector that
aggregates the information that updates the node features. This is the new feature of the node.
And the first flavor is convolutional. So here the coefficients c are constant. They're
independent of the features. They're dependent on the structure of the graph. And we can think of
them as importance of node j to the representation of node i. Why we call this convolutional? Well,
early architectures for graph neural networks came from generalization of convolution in the
spectral domain. I will say a few words about it. And you can derive the traditional conversion as
a particular case of this formulation. The second flavor, we can call this attentional flavor. So
in this case, these coefficients that represent the contribution of j to i are feature dependent.
They can be computed using attention mechanism. And finally, the most general flavor is the
message passing flavor, where we compute the general function that you can think of as a
message that node j sends to node i. And then we aggregate all of them and update the node i.
And the typical graph neural network will contain multiple such equivalent layers.
And if we want to classify the entire graph, we will use an invariant layer, a global pooling,
particularly will aggregate usually some old and old features and then output from the graph
by its label. We can also do coarsening. So we can do local pooling. And it can also be learnable.
So we can create a pipeline that tells us for specific tasks how to coarsen the graph in the
best way. So before we go to grids, which will be another manifestation of these principles of
geometric deep learning, let me show you a few interesting particular cases of deep learning
on graphs. One of them is sets. So if I take a graph and I remove all the edges from them,
from this graph, then I get a set, right? So in a set, I can do two things. So instead,
I don't have any edges between the nodes. I can assume that each node lives completely independently.
And I can just apply a shared function, phi to all the node features, right? And this is the
deep set architecture. So this is by construction permutation equivalent. Okay. So the second thing
that I can do, I can assume that all nodes are allowed to talk to all other nodes. So in this
case, the graph is a complete graph. And this is what we see in transformer architecture. So
interestingly, if we use here the convolutional flavor, because the aggregation here is on all
the nodes, so the second argument in this function five will be the same. So convolutional
architecture is not good here. That's why we need to use an attentional architecture. And usually,
well, there are many nuances to practical transformer architectures, because they're
applied to sequences and white graphs, we actually do know some order of nodes in the sequence. So
they usually come with extra feature that encodes the position of the node, but it's called
positional encoding. And this can be also applied to graphs in the form of positional or
structural encoding. For example, we can count some small graph substructures and provide them as
extra input to message passing. And this is a recent work that they did with my students.
Basically, we show that this architecture is more powerful than the WL test. And because
WL test is actually not a single copies of our fission test, but an entire hierarchy of tests,
we can be more powerful with the right choice of the substructures than higher dimensional
vice versa. And here you can see a counter example for which the 3WL test fails. So the graph on
the left has four click and the graph on the right has a triangle. So four clicks can not be
detected by 3WL test. But if we provide them as a count, our message passing architecture
in the form of structural encoding, then we can distinguish between these graphs.
And the last instance of this setting I would like to mention is that in many cases we are not
given a graph. So we are given a set that can be a point cloud and high dimensional feature space.
And we want to use a graph that is optimal for the given task. So the first architecture,
to my knowledge, that did this was a work that we did with collaborators from MIT,
Justin Solomon, which we call dynamic graph CNNs. Basically, it was applied for problems in
computer vision and graphics for three dimensional point clouds where the graph
was constructed on the fly as a k-nearest neighbor graph. But in general, you can think of
that in many situations you are still given an input graph, but you do not necessarily need to
stick to this graph to propagate information on your graph. So you can decouple the computational
graph from the input graph and there are many reasons why to do it. It could be due to computational
efficiency. You won't forget to sub-sample the graph. You can denoise the graph or you can
resolve issues such as information bottlenecks when you have to squeeze a lot of information
by means of message passing into a single feature writer. So let's talk about grids.
Grids, as you can imagine, are particular instances of graphs. So if I assume a grid with
periodic boundary conditions, where we wrap around the signal across the boundaries,
then this is called the wing graph. Now, it might sound that grids, well, are just the
same as graphs and here we have exactly the same graph neural network, but maybe slightly simplified,
but actually the story is completely different for grids. And the reason is that if we look
at the neighborhood structure, it's not only that each node has the same number of neighbors,
we always have two neighbors, but also the neighbors are not permutation environments. The
order is actually fixed. I can always talk about my previous neighbor and my next neighbor.
So the local aggregation function that we've seen before in graph neural networks,
that took as input the unordered multi-set of neighbors, now the order is fixed. So instead of
having an input xi and this multi-set of xi minus one, xi plus one, I have a fixed order of my
nodes, xi minus one, xi and xi plus one. And if I design this function phi to be just a linear
function, just weighted combination, I get something that looks very familiar. So if I
write it as a matrix, the mapping from the input to the output, it will have this multi-diagonal
structure. We call these circular matrices, they can be formed by just shifting cyclically
a vector of privileges and appending it to form this matrix. So circular matrices are
synonymous with convolutions. And what we know about convolutions, that they commute. So usually
matrix product is not commutative, but circular matrices are special, so they do commute. And
in particular, they commute with shift. So if you choose these particular matrix that shifts the
elements of the vector by one position, again, module n with rocker on,
then we see that no matter if we first apply the shift and then convolutional rise first,
the result will be the same. Even more than that, we can actually characterize convolutions in this
way. So this statement goes both ways. So it's an if and not only if statement. A matrix is circumvent
or is a convolution, if and only if it commutes this shift. So you see that one of the cool things
about this invariance fundamental principle was that you can derive properties or derive
architectures from principles of symmetry. So you see it manifested here. So I tell you that I want
shift equity variance commutativity with shift. And it follows that the only linear operator
that will commute with shift must be a convolution, must be circular matrix.
Now, what else do we know about circular matrices? We know that they're jointly diagonalizable.
And it means that there is a common set of eigenvectors that basically transform the
matrices into a diagonal form. And it's enough to pick up one of these matrices and look at
its eigenvectors. It's convenient to look at the shift. And surprise, surprise, the eigenvectors
of the shift appear to be the Fourier basis. So the shift operator is diagonalized by the
Fourier transform. And as a result, convolution, any circular matrices diagonalized by the Fourier
transform. So when you hear people saying that the convolution operator is diagonalized by the
Fourier transform, that's exactly it. Basically, any convolution can be expressed in this new system
of coordinates. Fourier transform is basically you can think of it as a multidimensional rotation,
right? So it's a change of coordinates. In this system of coordinates, convolution becomes a diagonal
matrix where the diagonal elements actually also have a closed form expression. That's the Fourier
transform of the vector w that forms this matrix. And this gives what is called the convolution
theory, which is a dual view of how to compute convolution on greens. We can compute it either
as a circuit matrix or in the Fourier domain by applying the Fourier transform, multiplying
element-wise by the Fourier transform of the filter, right? Of these matrix and then computing
the inverse Fourier transform. And the two views are equivalent. On graphs, you can generalize the
notion of Fourier transform by looking at the adjacency matrix because the adjacency matrix of
the ring graph is the shift operator or the graph flow passing. And this has been done in early
works on convolutional networks on graphs where basically the analogy of the Fourier transform
was exploited for doing filtering in the Fourier domain. So let me say a few words in the remaining
time about what's next in store for deep learning on graphs. And one thing that brought the
revolution in deep learning was the combination of data computing power and software. We don't
really have anything similar in graphs yet until recently at least, something that would compare
in the complexity and scale to image for graphs. Now there is the open graph benchmark that was
introduced last year that has multiple use cases, multiple data sets and multiple programs.
And graphs are much richer in terms of different problems compared to images and also in terms
of scales. So you can have small graphs like molecules with maybe a few tens of nodes and
gigantic graphs like social networks with hundreds of millions of nodes.
They are already developed and professionally maintained industry-sponsored libraries such as
DGL or fight towards geometric. So I think there is ongoing democratization of these methods.
There is a lot of research about efficiency and scalability, so how to apply these architectures
to large-scale industrial problems. That's what we're trying to do at Fourier as well.
Interesting settings in industrial applications such as social networks is that the graphs are not
static but they're nearing. So it's correct to think of these graphs as a stream of
synchronous events that form the graph, for example, node or edge insertion or deletions.
And for this purpose, there are a few architectures that deal with this setting.
We developed a generalization of message-passing networks that we call temporal graph networks
that essentially learn a memory that compresses all the interactions of the node,
all its participation in events, and if you can train it in a self-supervised way. So
predicting future edges at a certain point of time, and this can be used for
recommend a system, for example, when on Twitter, we can recommend home to follow
based on your previous interactions. It is interesting to look at higher-order structures.
So message-passing architectures are all built on nodes and edges, but we know that in many
natural networks, we have also higher-order structures or graph motifs such as triangles,
clicks, and so on. So with graph substructure networks, maybe this is the most straightforward,
somewhat naive way of incorporating this information, but it is more interesting to look at,
for example, simplicity of complexes and do a generalized form of message-passing that takes
into account these higher-order structure. So I mentioned already laden graphs, so let me say
a few words about it more in detail. So if you look at some applications of graph neural networks in
the biomedical domain, for example, colleagues at PL College did this first work on using graph
neural networks for disease prediction, the graph was constructed in a handcrafted way,
so that was based on demographic similarity between patients that they were trying to predict
Alzheimer disease based on imaging data that was attached to each of the nodes of this graph.
But we know that this handcrafting doesn't work because for some diseases, the similarity of
patients might be completely different. So we can learn this graph in a task-specific way,
so the way that it works, we have two different feature spaces. One is used to construct the
graph as a k-nearest neighbor graph, and another one is used as the features on the nodes of this
graph that are then being diffused by the graph neural network. So we call this DGM, different
graph model, and we show in work with collaborators from the Technical University of Munich that
it achieves significantly better results compared to previous methods in this domain.
I should say that maybe this gives a new perspective or a refreshed look at some
classical methods that were termed manifold learning or non-linear dimensionality reduction,
which came with the premise that we have very high dimensional data that even though it's
high dimensional, its intrinsic dimensionality is low. So if you can think of images, for example,
then indeed images might live in a million dimensional space, but somehow the number of
degrees of freedom that describe them is small. So this can be captured by a graph,
a nearest neighbor graph, and then you can preserve some structures on this graph like
geodesic distances to lower the dimensionality. For example, isomap works with multi-dimensional
scaling that tries to preserve the geodesic distance on the graph, and then you apply some
ML methods such as clustering. The problem why this never worked, it is still used for
data visualization, but it's not really a useful tool in machine learning because
the different stages of this algorithm are independent of each other, and you really need
to handcraft the feature space where you represent your data and how you construct the graph and how
you're embedded to produce meaningful results. So graph neural networks, the modern deep learning
architecture, give a new fresh look at this. Basically, you have an end to a differential
architecture where you can apply for your specific task, do the learning on the graph itself,
and therefore I expect that we'll see more instances of these meaningful learning 2.0
with modern graph neural network architectures. So taking this maybe even step further,
what is called algorithmic reasoning, so we can think of learning interactions between
some complex entities that might have a certain structure and description. And what I mean by
this is think of a multi-particle system in physics where you have multiple particles that
interact with each other. So we can describe the interactions as a graph. We can learn using a
standard graph neural network the way that these particles interact. And once we learn these
general functions, usually the message passing functions are implemented as small neural networks,
we can do symbolic inference. We can replace them by symbolic equations that describes the
interactions. And replacing these these generic functions with symbolic equations, we actually
get better generalization, but not only that, we get interpretability. We can actually derive,
we can learn automatically, for example, the laws of Newtonian mechanics. So that was very cool work
by Kyle Cranmer and Coloreges, and they applied to also much more complex systems and more complex
physics. So if you think of somebody like Johann Kepler that spent most of his life pouring over
astronomical observation data now, you can learn the Kepler and more complex equations of motion
in a few minutes from your data using this kind of neural networks. So the last thing that I would
like to mention is theoretical understanding. And while there are many interesting flavors to
expressive power of graph neural networks, but also the generalization power and basically whether
this is maybe a naive construction where we pick out some selected small graphs, we could
ideally try to build them from the data itself. And the question of generalization, of course,
how generalizable graph neural networks, which is the other side of the expressive power,
is currently far from being understood. So let me finish with the last, but not least thing,
which is killer apps. And this is probably the cool thing where graph neural networks shine,
because they can really address a very broad range of applications. And again, for a reason,
these methods are so prominent, I would call them first class citizens in the machine learning
community. So one application that is interesting when it comes to social networks is the problem
of misinformation or so-called fake news. And there is empirical evidence that shows that fake
news propagate on the social network differently from true news. So we can, they're a little bit
similar to viral infection. So what we try to do, we try to look at cascades of news that were
related to certain political claims that were professionally effect checked by journalists,
and this way would have true or false labels. So I'm throwing out many nuances of this problem,
but just to think of it as a binary classification problem. So we have a cascade of retweets,
let's say, of a story that propagates on data and an associated label. So we can try to learn
whether a story is fake or not. And we show that we can do it quite efficiently and accurately,
given a few hours of propagation. And with my students, I founded a company called Fabula AI
that tried to predict misinformation on Twitter. And obviously, Twitter was interested in this,
so this company was acquired in 2019. And that's how I, together with my students,
ended up working at Twitter. So at Twitter, you can use graph neural networks for many problems,
because the two kinds of content that it has is the text or the images and the graph, which describes
something that is visible publicly, such as the follow graph or the engagements with content,
but also something that is not exposed to the public, such as, for example, we can see
whether a user logs in from some suspicious IP address and might be associated with some
form of trolls in Moscow that are injecting fake in the fake news. So one classical application
is a recommender system. So you can think of it as a problem of link prediction.
You get an input graph, let's say a follow graph of Twitter users, you embed it into some space
where you try the similarity or the distance between embeddings to be proportional to the
probability of an age that exists between two nodes. And basically, this is a self-supervised
way of training this neural network. You try to reconstruct the data, so it's an oncology
correct texture. And when a new user comes, you can try to suggest the users that he or she might
like to follow. So another interesting use case is drug discovery and design, which is an expensive
and long process. So the problem with drug design that the number of compounds that we
can test in the lab or in the clinic is extremely small. The number of candidates is extremely
large. We need some how to reach the gap between the two. And this can be done with computational
methods. So traditional methods used either quantum mechanical simulations or some simplified
version of it. So graph neural networks were shown to excel here to be orders of making
faster and of the same accuracy. And this is already old results. So this was the work of
Justin Gilmer from reply in 2017. More recently, last year, the group of Jim Collins from MIT,
they showed that you can use graph neural networks to the virtual screening of antibiotic
compounds and actually discovered a new class of antibiotics that has a broad range of effect on
antibiotic resistant bacteria. Not only that they showed it computationally, but they also did
in vitro and then vivo experiments on mice. So they discovered a new drug that they called
halicine that is likely a new antibiotic candidate. So with collaborators at Imperial
College, we are looking at a slightly different idea while also predicting drug
likeness. But we are trying to apply these ideas to the compounds contained in food.
As you may know, plant-based food actually contains a lot of compounds that are chemically
similar to oncological drugs. And this way, we trained a classifier that predicted whether a drug
is likely to have anti-cancer effects from the way that it interacts with the network of proteins,
which are common drug targets. And this way, we had an anti-cancer classifier that we then
applied to food molecules. And this allowed us to build what we call the food map of
food ingredients that are reached with not only concentration, but also in diversity
of these compounds. And we call the champions in this food map, like hyperfoods. The good thing
about food is that it is cool and everybody likes it. So we collaborate with the molecular chef,
Josef Yusef, who takes these ingredients and tries to make some cool looking and yummy dishes
that everybody can easily cook at home. So I should say that this completely ignores
interactions between compounds. And interactions are important. And in fact,
the dark matter of combinatorial drug theory, because usually we don't just take a single pill.
When we take drugs, usually drugs have side effects. So you take another pill to counter the
side effects, then you counter the effect of the second drug. And it is not uncommon to take
multiple drugs at the same time. So the side effects are multiple. Many of them are unknown.
Many of them may be harmless or innocuous. Some of them can be dangerous and potentially lethal.
So Marine Kazimnik wrote already classical paper where they tried to predict side effects
of pairwise drug combinations using graphical networks. And I'm involved in the collaboration
with Mila and the UK pharma company called Relation Theopedic, where we try to predict
drug synergies as a cure against COVID-19. Because some drug interactions may not only be
negative, they can also be positive. So the last thing in this domain of drug design,
I would mention the work that I did with collaborators in Switzerland, where we used
geometric deep learning to do denormal design of proteins. And this is also a very interesting
set of applications in cancer immunotherapy. So the mechanism of this therapy that there is
a protein complex, basically two proteins that bind to each other and they indicate
to the immune system that this is a healthy cell. So cancers learn to express these proteins and
they become immune against the normal action of the immune system. So the idea of immune
therapy is to block one of these proteins. They're called program death ligands or PD1 or PDL1.
The problem that traditional drugs usually look for pockets on the molecular surface of the protein
and protein-to-protein interactions usually have flat interfaces like what is shown here
in red. And they're considered to be undruggable. So the idea is to design a small protein or a
peptide that will bind to this interface and will block this mechanism and will allow the immune
system to kill the malignant cells. So with geometric deep learning, we show that we can
design these proteins completely from scratch. And this paper appeared on the cover of Nature
Methods last year. So I think it's quite a record to have two major journals in the
biological domain to run cover stories on geometric deep learning methods. So let me
conclude. I think I'm out of time. So the Erlangan program of PDL, I hope it doesn't sound arrogant,
tries to construct neural network architectures and inductive biases from fundamental principles
of invariance. I talked about graphs and reads, but there are many more to this. So we can have
also more interesting cases of various manifolds. And we did some of the first works in this domain
with intrinsic geodesic convolutional neural networks. And I guess the conclusion is that
these are really new and very hot methods. They have big promise in industrial applications,
in particular in healthcare, in biology. There are already several success stories
and the state of the art results. But if at all you have to take one message home, then I think
it's really a unified framework. I would say 99% of the different architectures that exist
in deep learning, maybe with the exception of reinforcement learning. So whether it's convolutional
neural network, recurrent neural network, transformers, graph neural network, they all
stem from the same principles. And I hope that these principles will transcend the specific
implementation. So maybe today we are obsessed with deep neural networks, maybe tomorrow it will
be something else. I think the principles are more powerful and they can be implemented in other ways.
But also it's a principle recipe to construct new types of architectures that are suitable for
the specific problem at hand. And again, I would like to mention that probably in the biological
domain, there is the biggest promise and hopefully this year or in the next few years, we'll see
more and more applications that might potentially transform the way that we design and discover
drugs and eventually affect the lives of each and every one of us. So I think I will stop here.
Thank you very much.

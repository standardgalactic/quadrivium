Hello, and welcome back to Unheard. I am still Florence Reed. Here's a question. Who's
afraid of artificial intelligence? Right now, the answer seems to be just about everyone.
With large language models and deep fake images appearing on the internet, we're getting
an idea of what the future might look like. And for many, this seems like a sinister one.
Even the program developers themselves, the people who are in charge of this technology,
are warning of the dangers of accelerating towards what they call the singularity.
But one voice has been absolutely resolute in its rejection of this mass panic. And it
comes from the godfather of augmented reality himself. His name is Jaron Lanier. He's
a computer scientist and futurist, fresh from Silicon Valley. And he is developing cutting-edge
artificial intelligence at Microsoft. So he's in the belly of the beast, you could say.
He recently wrote a provocative article in The New Yorker called There Is No AI. And
he thinks this hysteria might be misplaced. He joins us live from the West Coast to tell
us more about it.
Just to start, I really wanted to get you in the studio, Jaron, because you have really
stood against the crowd on this issue of AI in the last few weeks. You've been writing
about how doomerous and at times hysterical your colleagues have been. But you yourself
think that perhaps humans might be the problem rather than the machines themselves. So explain
to me a bit more why you've taken this position.
My difference with colleagues is that I think the way we characterize the technology can
have an influence on our options and abilities to handle it better. And I think treating AI
as like this new alien intelligence reduces our choices and has a way of paralyzing us,
whereas an alternate take on it, which is it's a new form of social collaboration where
it's just made of us. It's a giant mashup of human expression, opens up channels for
addressing issues and makes us more sane and makes us more competent. And so I make a pragmatic
argument to not think of the new technologies as alien intelligences, but instead as human
social collaborations.
But you see what I'm getting at here, which is that taking a pragmatic view over a hysterical
view, someone might say, well, you work for a company that will produce the tech that we
should be hysterical about. So so why should we be listening to someone who profits off
this technology?
Well, you know, I have a really unusual role in the tech world. And it's, it shouldn't
be unusual. I think it should be more common. But essentially, I am speaking my mind, honestly,
even though I'm on the inside of the castle, instead of on the outside throwing stones at
the castle. In my opinion, both positions should be well manned. I don't think there's any
perfect way to handle anything in particular. One is always somewhat compromised. And Microsoft
and I have come to an accord where I have what you might call academic freedom. I speak
my mind, I speak things as I see them. But I also don't speak for the company. And we
make that distinction. And it allows me to maintain my public intellectual life, but
also work inside. I don't necessarily find agreement with everybody I work with, nor
do I find absolutely disagreement. I find it's actually rather complex. For instance, Sam
Altman from OpenAI really liked my New Yorker piece. I don't think he agrees with it entirely,
but he said he actually agrees with it mostly. That's great. I hope it's of use for somebody
like me to be working inside. It's not perfect. But you know, the thing is, I don't believe
in perfection. And I have to object to the notion that people are trying to be perfect,
that you must not have any contradictions in one's life, because I just don't think you
can be human without contradictions. I think it's fine to try to reduce contradictions.
And I think particularly fragrant and horrible ones should be condemned. But I also think
we need to be a little open or fuzzy about how we define each other's roles and how we
judge each other to find a way forward.
You spoke about contradictions there. I suppose the central contradiction of a lot of your
colleagues who are on the more, I suppose, doomerous side of this argument is that they
will speak about the fears that they have, the deep existential fears about working with
this technology, and then continue in a conversation. I've been watching lots of these
kind of long form podcasts with people like Sam Altman, who you mentioned, and then we'll
continue to speak about the research they're doing after saying that it might bring about
the end of humanity. So this is the kind of central contradiction that I think the normal
person who has not sat in a computer science lab can't quite wrap their head around. Why
do they carry on doing it?
Yeah, I think that's a really important. Wow. Yeah, you know, look, I am part of a community
that we call, you know, tech culture. And it's weird. We're weirdos, you know, and
no, no.
Just asked actually cuts to the very heart of our weirdness. And I have tried to understand
that myself for decades. And I think part of it is we kind of simultaneously live in a sort
of a science fiction universe where we're living out the science fiction we grew up with. And
so if you grew up on the Terminator movies and the Matrix movies and Commander data from
Star Trek and so on. Naturally, what you want to do is realize this idea of AI, you know,
it just seems like your destiny. But then another part of you is thinking, wow, but in
most of those stories with Commander data being the exception in most of them, this was
horrible for mankind. And so it feels sort of responsible to acknowledge that it could
be horrible for mankind. And yet at the same time, you keep on doing it. It's weird, you
know, and I believe the approach that I've proposed of not thinking of what we're doing as
alien intelligence is that we're creating, but rather as social collaboration is the way
through that problem, because it's a way of framing it. That's equally valid, but actionable.
And I just think much more sane and realistic. And but, you know, within the tech world, giving
up those childhood science fiction fantasies that we grew up with is just really hard for
people. It's just I've been thinking I've been thinking recently about how we called the
Internet the Wild West in the in the early days, the kind of nineties and noughties. And
actually, that fantasy of the cowboy, the little boy who dresses up as a cowboy and goes and finds
manifest destiny in the frontiers, the reality of that situation was was deeply troubling. I
think that's true. Now, I grew up in rural New Mexico, in the 60s, when it was still not that
economically developed. So I actually got to experience a little bit of the tail end of the
Wild West. And I can assure you that you're correct, it was miserable. And it's not something
anybody would want. But the version of it in the movies is very appealing. And it does bring up a
sort of a strange gender identity connection, because this is a little bit of a little boys
thing. It's a little bit of a, you know, what you there are. Actually, I'll tell you a story.
Recently, I was on a prominent more, I won't say which one, but I was on a prominent morning TV
talk show in the US. And one of the hosts was a woman who asked me about this. You know, like,
it just seems like there's a lot of male fantasy in the AI world, shouldn't there be more women AI
leaders? And I said, Well, you know, there are some spectacular women AI leaders. And actually,
there does tend to be some sort of a difference where the women seem to be a little more
humanistic and have a different approach. In the YouTube, in the YouTube version of that, they
cut out the whole exchange about women. And I called and asked about it. And they said, Well,
it just seemed like a niche question. So we cut it out. And I'm like, no, it's not. It's a very
central question.
You hear these men saying, look, we've created this thing, and it might turn around and destroy us
all. And I think, well, you could only hear that from someone who's never had a child, because
that's just the experience of having a baby.
Back in the 80s, my mentor is Marvin Minsky, who is probably the single most influential author of
the way we think about AI these days as coming alive, not the only one, but probably the most
prominent one. And we used to love arguing about it. And when I was a kid, I always used to say AI
is really just womb envy. It's just men wishing and actually having had a child and seeing what
it's actually like for a woman to bear a child, I no longer have womb envy. I now appreciate it's
actually a rather difficult process for the mother. And I didn't know that when I was young
man. And I would think anybody who's actually gone through having a child would lose their womb envy
pretty quickly. It tends to be men who haven't had kids yet, who have a desire to create life at the
computer. So it's good to know our whole universe hinges on men with kind of daddy issues in their
mid 40s. That's good. Yeah, yeah, welcome to our world.
Let's return now just for a second to these two different types of artificial intelligence that
you distinguish in your article in the New Yorker, one of which is this alien entity that many
your colleagues seem to attribute a spark of life to and your version, which is in fact just a
network of connections, connections between things that already exist, things that are created by
humans. So it's not a generative force in your definition. Tell me more about that.
Yeah, sure. So one way you can think of AI is as or look, I should say the term AI
is a sort of a very wiggly where it's applied to all kinds of things. And in fact, lately I've been
especially advising students to call whatever they're doing AI since it's fashionable to help them
get positions and funding and stuff. So it can mean many things. But usually these days when we
talk about AI, we talk about these large model AI's like the GPT programs. And
what they are is their giant mashup of human creations. So for instance, if you ask one of
these programs to create you a new image, you know, like, I'd like to see London as if it were
across between London and Angkor Wat or some crazy thing like that, it can probably synthesize
that. But the way it does it is by using the classifiers that it uses to identify the images that
match your the components of your request and mashing them up by randomly refining and refining
an image that still works for all the classifiers. So it's actually a pretty simple idea. And in fact,
the interior of these systems, the math is almost embarrassingly simple to my mind. But it happens
at such a stupendous scale. And actually managing the whole scale so it can happen quickly is not
so simple. But the basic idea is pretty simple. So you have this way of mashing things up that's
constrained according to the principles that you use to recognize the components in the first place.
And so you can combine things. You can say, give me a lesson in calculus, but in the language of a
pirate or whatever, crazy stuff like that. And it'll synthesize these things by maintaining the
constraints from the whatever correlations had allowed the program to recognize the components
in the first place. So it creates this meaningful or not meaningful, that's the wrong word, because
we don't know what meaning is. It creates a expected mashup of things from people. And that's
never existed before. Now, I happen to think that's a great capability with a lot of uses, you know,
like I love the idea of computers just getting more flexible. Like it creates the possibility
where you could say, can you reconfigure this computer experience to work for somebody who's
colorblind or whatever right on the fly, instead of demanding that people conform to computer
design. So I actually think there's a potential in this flexibility to really improve computation
on many, many levels and make it much better for people. But the thing is, if you want to,
you can perceive it as a new intelligence. And to me, if you perceive it as a new intelligence,
what you're really doing is shutting off yourself, you're shutting down yourself in order to worship
the code, which I think is exactly the wrong thing. And it makes you less able to make good
decisions. So if we go back to, you've probably heard of the Turing test, which was one of the
original thought experiments about artificial intelligence way, way back. There's this idea
that if a human judge can't distinguish whether something came from a person or computer, then
we should treat the computer as having equal rights, as perhaps Turing should have been treated,
you know. And the problem with that is that it's also possible that the judge became stupid,
like, like there's no guarantee that it wasn't the judge who changed rather than the computer.
And so the problem with treating the output of GPT as if it's an alien intelligence, which
many people enjoy doing, is that you can't tell whether the humans are letting go of their own
standards and sort of becoming stupid to make the machine seem smart, because it's all very
subjective. There's no absoluteness about that. So we haven't hit this moment that I think all of
us kind of feel that we're waiting for. We feel that we're on the brink of it, or maybe we're
just being told we're on the brink of it, where in a dark room somewhere in California, there will
be a computer that talks back, that has a kind of sentience. The sentience of others is always a
matter of faith, right? There's no way to be certain about whether someone else has interior
experience in the way that you do. All right, I presume that you do, but I can't know. And there
is a kind of a mystical or almost supernatural element in which we have internal experience,
and or at least I do, but I can't make you believe I do. You have to just believe on
your own that I do. And the thing is that faith is a very precious thing, and there's no absolute
argument that you should or shouldn't believe that another person has interior experience or
sentience or consciousness, or a machine or whatever. I mean, I do think faith is not fundamentally
rational, but there is a pragmatic argument, as I keep on repeating, to placing your faith
in other people instead of machines, if you care about people at all. If you want people to survive,
you have to kind of place your faith in the sentience of them instead of in machines,
as a pragmatic matter, not as a matter of absolute truth, which we can't access.
Is really the only distinguisher then between me asking you, are you sentient and you replying
yes, and me asking a machine, are you sentient and the machine replying yes, is the only distinction
there of faith in the power of the human soul versus the fact that that computer is just
amalgamating information? Yeah, I mean, I think ultimately it's a matter of faith that can,
that has pragmatic implications. So I think the quest for absolute ultimate truth is not really
viable. So there was a period hundreds of years ago where philosophers grappled with this about
God, and this question of whether you can absolutely prove whether God exists or not.
And I think almost everyone who's considered that question has decided that that's a matter of faith,
but doesn't have an absolute truth value that you can establish the logic or empiricism.
I think we're at the point where we have the same issue with one another where there is no
absolute proof through logic or experiment about what's going on in terms of experience within
other people. However, once again, just to say something is a matter of faith doesn't mean
that the choice of faith is entirely arbitrary because it can be pragmatic as well. So if
not believing in people increases the chance that people will be harmed, which I think is the case
with this technology, or relatively not believing in people to believe that machines are the same
as people, I think increases the chance that people will be harmed. So in addition to whatever
other reasons you might have perhaps sentimental ones for having a faith in people, there's a
pragmatic argument that joins them. And I think cumulatively we should believe in people over
computers, but that's not an absolute argument based on logic or empiricism, which I don't think
is available to us. But I do think that pragmatic arguments, there's a bit of a skyhook thing here,
and it's a little bit like the problem of why should you stay alive instead of committing
suicide, it's applied to the whole species, why should we continue this human project,
why does it matter? Well, it is a matter of faith. And I guess I come to something that's
a little bit like the argument attributed to Pascal. So there's this notion that you might
as well believe in God just in case it's real and there's heaven and hell. I don't buy that
particular argument. I'm not concerned about heaven or hell. However, I do think the continuation
of us people in this timeline, in this world, in this physicality is something
I'd like to commit to. I think it's important. I think we might be something special. And so
kind of in that way, I'd like to apply faith to us and give us a chance. And that that does
involve demoting computers in this case. But it's not just that when we demote computers,
we can use them better. We can make this like, as I pointed out in the New Yorker article,
demoting AI allows us to not mystify. And that allows us paths to explaining it, to controlling
it, to understanding it, to using it as a scientific exploration of what language it is,
and all kinds of things. There's so many reasons to demote it that are practical,
that the faith in it as a mystical being just actually seems kind of stupid and wasteful
and pathetic to me. But I don't know, you know, I haven't convinced everybody.
But can we demote something that has potentially more power than us already? You know, can we
actually say, look, we're going to drag computers down the hierarchy of being when they are already
in control of so much of our lives. Most of us are subordinated to computers in our everyday lives.
I mean, I'm sitting here literally with my computer. Okay, close that thing right now.
Close that thing right now. People are capable of being self-destructive,
idiotic, wasteful, ridiculous, whether without computers. However, we can do it a little more
efficiently with computers because we can do anything a little more efficiently with computers.
As, as is well known, I've been very publicly concerned about the dehumanizing elements of
social media algorithms and the algorithms on social media that have caused outbreaks of sort of
elevated, these things always existed in humanity. There's just a little more vanity, paranoia,
irritability, and that increment is enough to change politics, to change mental health for a lot of
people, to change safety for a lot of people, especially in impoverished circumstances around
the world. It's just made the world worst, you know, worse incrementally. And the algorithms
in social media are really dumbass simple. I mean, there's surely not a lot there. These are a little
better algorithms with a lot more data. And so if we can screw ourselves up with the previous
generation, surely we can also screw ourselves up even worse with this. And so I think your
framing, though, that it's more powerful than us isn't correct. I think it's really just dumb
stuff. It's really up to us to decide how it fits into human society still. The capacity for human
stupidity is great. And as I keep on saying, hard to distinguish from the good, you know,
it's only a matter of faith, whether we call it human stupidity or machine intelligence,
they're indistinguishable logically. So I do think the threat is real. I'm not
anti-dumist in a sense. I just ask us to consider what is the way of thinking that improves our
abilities, that improves our thinking, that gives us more options, that gives us more clarity.
And it does involve demoting the computer. There's a lot of work to do. We have a lot of work to do
technically. We can create explanations for what machine, so-called machine intelligence is doing
by tracing it back to the human origins. So there have been a number of very famous instances of
chatbots, indeed, our chatbots, the GPT bots, getting really weird with people. But, you know,
the form of explanation should be to say, well, actually, the bot was at that point
parroting some stuff from soap opera, from some fan fiction, from some other weird stuff.
And that kind of explanation should always be available. That should always be there so you
can look at it and say, oh, that's where it got it. That's what's going on. And in my opinion,
there should be an economy in the future where if there's really valuable output from an AI,
the people whose contributions were particularly important should actually get paid. I actually
believe there's a new extension to society that's very creative and interesting rather than this
dismal prospect of everybody being put out of work. I think that we can invert that and make it
better. But that's the same thing as having better explanations, better understanding,
more transparency. But transparency in a mashup technology can only come from revealing the
people whose expressions were mashed up. I mean, to me, this is just actually kind of simple.
But that technical work to make that revelation practical isn't completed at this time.
But it could be. And also, if policies can be based on the idea that we now have this
new supernatural artificial entity, there's no sensible way to resolve that. There's no way
that that becomes clear. But this other way of thinking does provide avenues for clarity and
concreteness. Well, one thing you didn't do was sign that kind of open letter demanding for a
hiatus in accelerating AI development, which was signed by Elon Musk and Sam Altman,
who we spoke about earlier. Why didn't you sign that? Why was that not appealing to you as an idea
if we want to demote this technology in some way? My reason for not signing it is that it
fundamentally still mystified the technology. It still took this position that it's like this
new alien entity. And it also left open this very, when you think it's an alien entity,
there's no way to know what would help. Like if you have an alien entity, what regulation
is good? Like if you say, well, it shouldn't hurt people. This was an idea that was first proposed
perhaps by Isaac Asimov, or one of the early explanations was Isaac Asimov's science fiction
of long, long ago about the laws of robotics. The problem is that this idea is very vague,
like how do you define harm? What is it? Because these, as much as the GPT programs impress people,
they don't actually represent ideas. We don't know what meaning is. We don't know
how to define these things. All we can do is mash things up so that they can form with classifiers.
So they can't do philosophy. They can't do the work of thinking.
They could mash up philosophers in ways that might be interesting. If you say,
do a combined essay as if Descartes and Derrida collaborated or something, something might come
out that's provocative or interesting, but there's no actual representation inside there. And
so getting provocative or interesting mashups is great and I think useful and helpful,
but you can't set policy bar because there's not actually any meaning. And we see that every day
when we try to, there's been a lot of work in AI safety and fairness and other concepts where
you try to keep people from misusing the AIs, but then it becomes a game where all the people out
there who have access to it try to countervane and come up with some nasty version of it,
even though you tried to prevent it. And you can't because when you try to prevent it,
all you're doing is coming up with some sort of surface features that you think are associated
with bad behavior of humans, but humans can always kind of route around those to find other
ways to express bad behavior. There's no fundamental representation inside these things. And we just
have to accept that as reality. We don't know what meaning is and we can't represent meaning.
But so much here of your argument relies on the idea that if we define this technology differently,
then we will have more power over it or at least we'll have more understanding of it. And so
we won't have so much hysteria and fear which could lead to this kind of catastrophic mistake
that you say could actually end with disaster. But I suppose you might say, well, aren't we just
trying to make ourselves feel better there? We're just comforting ourselves by saying, look, no,
no, don't worry, it's not an alien technology. It is simply an amalgamation of human connections
that already exist. Are we are we not just self comforting here with a kind of bit of rhetoric
about it being this human technology rather than something we can't control?
Yeah, I mean, I don't think that's the case. It's actually proposing this more concrete and
clarified path of action is very demanding of people. It's not comforting at all. It demands
that everybody involved on a technical or regulatory level do much more than they have.
And so it's actually probably not putting people in a comfortable situation. I suspect
many people would prefer the mystical version because it actually lets them off the hook. The
mystical version just lets you sit there and apprehend and express awe at our own inventions
or something. But the stuff I'm talking about demands action. And so it's actually,
I think puts pressure on people and it's not comforting and it shouldn't be.
Do you think humans need to take more accountability for their part in developing a
potentially malign form of AI? Well, I'm talking here about the actual malignancy of it.
You know, we speak a lot about it kind of going off the rails, but would it
be going off the rails because we've set it up to do so?
Yeah. Okay. So the one example I use in the New Yorker piece is the disasters with the Boeing
737 MAX. So the flight correction module in it was, you know, was in a sense the source of
two terrible air disasters in which hundreds of people died, but in a sense in which it wasn't.
What actually happened is the way they sold it, the way they withheld information about it,
depending on how much you paid them, the way they trained people for it, the way
the documentation was created, all this crazy stuff. It's the surrounding stuff that created
the disaster, not the core capability, which probably has been useful in general.
And so in the same way, this large model AI, it's not the thing itself. It's the surrounding
material that determines whether it's malignant or not. And when you deploy it under the assumption
that it's an alien new intelligence, that it's a new entity with its own point of view that
should be treated as a creature instead of a tool, when you do that, you greatly increase the chances
of a scenario similar to the one that befell passengers on the Boeing planes. And I do think
that's a real possibility. The malignancy though is in the surrounding material, not in the core
technology. And that's extremely important to understand. I don't think anybody has claimed
that the flight path correction module shouldn't have existed. I think what people are saying is
that the pilots should have been well informed, well trained, and the ability to control it should
have always been included, not only for those who paid more. So that was where the errors were,
and that's all been well documented. And I think we're making sometimes some potentially similar
mistakes with this, that if you have chatbots and you tell people, oh, this is this intelligent
companion, you should be able to date it, you should be able to trust it, you should be able
to do this and that, which I think most of us don't do, but some of us do, then the chances of
something really bad happening do increase. And as I say, we know that that's possible because
we've seen it with the previous generation of much less sophisticated and smaller scale AI
as applied in social networks. So if the threat really is coming from humans, if anything,
then isn't the main worry that most people should have that this sort of technology or an accelerated
version of it might fall into the hands of someone who has malign intent against a group or a country.
I'm thinking here particularly about the situation that's happening in Ukraine right now.
Russia has been one of the worst actors in misusing the internet and algorithms to
mess up people. For instance, it's documented that in the US, Russia created
enormous numbers of fake accounts of fake bots in order to sell divisions within the US. And
it's impossible to attribute events specifically to something like this, but it certainly contributed
to our Trump era and it undoubtedly contributed to Brexit. Whatever one thinks of those things,
from Russia's point of view, those were in its interest. And of course, it's attempting those
things in Ukraine. I worry a little bit more about China because Russia doesn't quite have
the resources to pull off very large model, very massive things right now. It's not that easy to
do. You need really huge computational resources available. So I worry a little bit about Chinese
using, just to be very blunt, data from TikTok to mountain attack like that on the morning of a
Taiwan invasion or something like that. That's imaginable. I hope they wouldn't do it. I've talked
to a lot of people in the Chinese world and I think almost all actually are much more conscientious
and better intention that might be imagined, but there's always somebody. I mean, they're in any
country in any situation. So I do worry about it. And the antidote to it is universal clarity,
context, transparency, which can only come about by revealing people since revealing ideas is
impossible because we don't know what an idea is. So every time there's some mashup, we should see
where it came from. We should see what's mashed up. So if that was just generally the case, then
this whole type of attack would be nullified. Is this the kind of blockchain concept but for
artificial intelligence? The idea that everything would have a kind of stamp and a history to it?
Yeah. And it doesn't have to be blockchain. The problem with blockchain, of course,
is its carbon footprint. So you could do something similar to blockchain that isn't a climate disaster
as has been demonstrated by the Ethereum community and others. But just as a placeholder,
blockchain-like thing is certainly the right approach. We've established though that we already
live with artificial intelligence. We are living in an artificially intelligent world.
So how has that already changed us? Have you seen differences in our culture, our personal
interactions, our society that you could attribute to this new living with AI?
Our principal encounter with algorithms so far has been in the construction of feeds we receive
in our apps. It's also present for those of us who are not of the highest socioeconomic status in
whether we get credit or not and in other things like that, whether we get admitted to a university
or not, or whether we're sent to prison or not, depending on what country we're talking about.
So it's definitely true that algorithms have transformed us. I would hope that the criticisms
of that particular process that I've put forward and many others have put forward of whom I could
mention Tristan Harris, Trishana Zuboff, and many others. I would hope that that has illuminated
and clarified some of the issues with algorithms in the previous generation. And what could happen
with the new AI is worse versions of all of that. And given how bad that was, I don't think the
doomerists are entirely wrong. I think we could confuse ourselves into extinction with our own
code. But once again, in order for us to really achieve that level of stupidity, we have to believe
overly in the intelligence of the software. And I think we have a choice.
You're a musical manual composer and also someone who appreciates art as well as science.
Do you think that there is going to be a shift in the way in which we prioritize
organic or human made art?
Well, let's say we enter into a world of what I call data dignity in the New Yorker piece.
Then a musician might provide music directly or might provide antecedent music that's
mashed up within an algorithm, but in a way that the musician is still known and credited.
And we already see a little bit of that. In a lot of current music for decades now, somebody
might provide the beat, somebody else might provide samples, etc. There's already the
sense of construction and mash-up, especially in hip-hop, but also just in pop music lately.
That has not destroyed musicians as long as it's acknowledged and transparent,
as long as we see the people. And so as long as we don't hide the people,
I don't think the mash-up algorithms themselves do any damage to people.
I think it's, as with the bowing thing, it's a surrounding material. If we choose to use
the mash-up algorithms to hide the people from whom the antecedent stuff came from,
then of course we do damage. But the thing doing damage is our hiding of ourselves,
not the algorithm itself, which is actually just a simple kind of dumb thing.
What I see in culture is, as long as people understand what's going on, they find their way.
So synthesizers haven't killed violins. There was a fear that they would.
And people, as long as people know the difference, as long as there's honesty and transparency about
what's going on, we can go through seasons of things being a little more artificial and then
less so. And that becomes cultural dynamic. And I trust people to handle that well.
I know I might sound a bit like someone, you know, booing at Bob Dylan going electric, but
I mean, take Spotify. It's almost totally wiped out good independent music. This is,
there have been major technological imbalances in music that have actually
totally obliterated any kind of creativity at these lower levels, more creative, more
maverick levels of the music industry. Yeah, you're quite correct about that. And I should
point out you still haven't closed your laptop, even though I asked you multiple times.
I know, I know, I love it too much. So you, so all right. So position heal
myself, I guess. But you're absolutely correct about Spotify. And in fact, at the dawn of the
file copying era, I objected very strenuously to this idea. And there was a cultural movement about
open source and open culture, which was selffully funded by Google, and other tech companies,
and especially in Europe with the pirate parties, in which everybody thought, oh,
it's terrible, everything should be a mashup. And we don't need to know who the musician was,
and they don't need to have bargaining power and a financial transaction and blah, blah, blah.
And I think that was a gigantic wrong turn. And it was a wrong turn that we can't afford
to repeat with AI, because it becomes amplified so much that it just could really destroy
technology. So I completely agree with you about Spotify. But once again, the availability of music
to move through the internet was not the problem. It's the surrounding material. Like what really
screwed over musicians was not the core capability, but this idea that you'd build a business model
on demoting the musician, demoting the person, and instead elevating the hub or the platform.
And so we can't afford to keep on doing that. We just can't. I think that that is the road
that leads to our potential extinction through insanity. Sounds like human greed might be
your answer to a lot of these problems, the thing that it comes down to.
Well, I think humans are definitely responsible. Greed is one aspect of it, but it's not all of
it. At the start of our conversation, we were talking about the sort of little boy fantasy
question, and that's not greed exactly. There's a lot like we can diagnose ourselves or each other
forever. But I think the more important thing is what approach works better? Because I don't
necessarily understand all human failings within myself or anybody else. But I do feel we can
articulate ways to approach this that work better and are more practical, more actionable, more
hopeful. And that has to be our first duty. Just last question for you, Jan, before I let you go.
We've talked a lot about the worst case scenarios here. What is the best case scenario if we are
the most optimistic version of ourselves? Between the two of us can come up with some
beautiful vision of the future with AI. What is that vision?
Well, what I like about the new algorithms is that they help us collaborate better. And so
I mentioned one way that can work earlier, which is you could have a new flexible kind of a computer
where you can ask it to change the way it presents things to match your mood or your cognition under
your own control so that you're less subservient to the computer. But another thing you can do is you
can say, I have written one essay, my friends written another essay, they're sort of different.
Can you mash them up 12 different ways so we can read the mashups and maybe try to
out of and this is not based on ideas. It's just based on kind of dumb math of combining words as
they appeared in order and context. But you might be able to learn new options for conciliants
between different points of view that way, which could be extraordinary. Many people have been
looking at in the sort of humanistic AI world, the human centered AI world. Could we actually
do this to help us understand potential for cooperation and policy that you might not see?
If we seem to have irreconcilable differences about how to handle something like land use
or something, is there some possibility that this mashup thing might just uncover some strains of
potential cooperation between us that are hard to see otherwise? I think it's worth trying, it might.
So it might potentially break us out of our tribes that we all exist in and
and offer some human connection? It's sort of like if somebody, if a therapist says try using
different words and see if that might change how you think about something, it's not directly
addressing our thoughts. But on the surface level, it actually can help us. But it's ultimately up to
us. And there's no guarantee it'll help. But I believe it will in many cases. I think it can
help us improve our research. It can help us improve a lot of our practices. And I think as
long as we acknowledge the people whose ideas are being mashed up by the programs, it can help us
even broaden participation in the economy instead of throwing people out of work, as is so often
foretold. I think we can use this stuff to our advantage. And it's worth it to try.
And at the very least, if we try to use it well, these most awful ideas about us turning into the
matrix or the terminator scenario in a Skynet, those things become vanishingly unlikely if we're
revealing the people and treating it as a human project instead of an alien intelligence.
I think we really can and must do that. Well, that glimpse of hope I will take. Thank you,
Jarron, so much. Thank you so much for your time. I'll never look at it again. I promise, for you.
So with my laptop firmly in the bin, that was Jarron Lanier, the Maverick computer scientist
live from Silicon Valley telling us why he thinks we should stop and think a little before jumping
to any hasty or historical conclusions about the future of artificial intelligence. He is really
standing up against a lot of his colleagues and friends in taking a less doomerous approach to
this issue. But I still had to think there was something very sinister about his potential view
for a future living with even the best case scenario of AI. Thanks to you for watching. This was unheard.

We're going to be looking at just-in-time compilation, which is a technique for making
programming languages run faster. We all want faster programming languages because then we get
more speed, and there are various ways we can do it. We can statically compile things. That's
typically done for languages like C, or we can just-in-time compile them, which is often done
for languages like Java or JavaScript. The difference between these two is that
just-in-time compilation is looking at the program running and optimizing it after it's
observed it running. Just-in-time is really a terrible name because it should have happened
before you actually got to that stage, but that's the name we've got. It's a technique that you
will all have used today. If you've used a browser, you're using a JavaScript just-in-time compiler,
and they're sometimes considered a bit magical, so I'm hoping we can look a little bit at the magic.
I'm digging the idea of magical computing. There's no wizards.
I don't have a beard, and I kept the cape at home today. Actually, to disambiguate something that
people often confuse, people often talk about compiled and interpreted languages, and there's
no such thing. For any given program language, you can implement it as a compiler or an interpreter
or just-in-time compiler. People say C is a compiled language, but you can, and there are C
interpreters, and people say JavaScript is an interpreted or just-in-time compiled language,
but you can write a static compiler for that as well. So what do I mean in the sense by those
things? A static compiler reads a programming, just looks at the code the program has written,
and then tries to convert it into machine code, let's say. An interpreter looks at the program,
and then it doesn't convert it into machine code. It executes it almost as is. It probably converts
into some other representation, but it's a very simple way of implementing a programming language.
And a just-in-time compiler nearly always starts a program running an interpreter,
looks at it for a while, says things like, oh, look, you're calling that function a lot, or
there's a function that takes some parameters in, and there are always integers, or always strings.
So I will now produce an optimized version of that function, or part of the program,
based on that information I've observed. And it's really that dynamic analysis and
conversion into machine code at runtime that makes just-in-time compilers very effective.
They can be faster than a static compiler because they've got more information.
So if you just look at a program at compile time, you've just got the code you've written on screen,
you can't fully analyze it as much as you want. So you'll make certain assumptions and guesses,
but they may be incomplete or even incorrect, and you'll optimize based on that.
When you're actually running the thing, you've got more information, so you can make
a much better quality optimization, but you've had to watch the program and observe it. So it
started slow, and then hopefully, over time, hopefully it's warmed up is the term, and then
it gets faster. Warming up turns out to be another kettle of fish. It doesn't always quite work as
we expect. These things have some very surprising emergent behavior, but generally they do work,
and when they do work, they can be very effective. So this is things like, it knows it, and you kind
of, again, alluded to before, it knows it's going to be calling this function quite a bit,
so it keeps that nearby, and things like that. Is that- It can do those sorts of things. Maybe
we could try a little simple example. So if I log in, so what I'll- It's just a simple example,
so there's a load of these that I could use, the Java virtual machines, a just-in-time compiler,
the JavaScript, VMs, all of the ones, VA and Chrome, spider monkey and Firefox, and those
wall jits, but I'll look at one for Python, because that's one that I happen to know fairly well,
and let's just write a very silly little Python program. So I can write this function,
let me make a little bit of a bigger font size for you, and it takes two parameters in, and I
will just add those two parameters together. Thing is, am I going to pass it integers, strings,
I can do all of the above. Let's just check that I'm not lying, because I do sometimes,
sometimes intentionally, but mostly unintentionally. Oh, if I do Hello World, I have to have a space in
there. If I save that file, and then- Right, so it prints out five or Hello World, so you can see
I'm calling the function in different ways. So this is why it's hard to statically optimize that
function, because integers, strings, it can take in all sorts of things. Now, the normal
implementation of Python, which I'm using here with the Python 3 binary, is an interpreter,
doesn't have a just-in-time compiler, and we can see some obvious consequences of this. Let's put
this in some sort of loop. So I'll just put some very big number here, that looks like a big number
to me, and just repeatedly call that function with some integers, it doesn't really matter. Now,
if I run that, and I'll just call it with the time function, and I'm going to say this is my
newest laptop that has from memory six fast cores and eight slower cores, and it's going to run
randomly on those each time I do it. So some of the performance numbers are not going to be quite
as clear cut as I would like. So I'll try and explain it if I see that happening. So I run that,
and it's taken that a tenth of a second to run. And if I make that number a lot bigger, so I make
it an order of magnitude bigger, this for loop, it now takes a bit longer. And if I make it longer
again, we'll see depending on which style of core it's gone on, I think it's, yeah, it's roughly
gone on the two slower cores. It's roughly as I make the loop run 10 times longer, the program is
taking roughly 10 times longer to run. So that's what we'd expect to see an interpreter. Now,
let's get rid of a couple of those zeros. And what I can do instead is use a different
implementation of Python. And this is one thing that we often confuse, we say Python,
when we mean the language, and there's Python, the language, and there's Python, the implementation,
I can use something called pi pi. And I think it has a jit. Let me turn the jit off. And hope that
I've not made it run too slow. So we can see that running. And I've still got a relatively large
number. So it's about the same speed as the normal version of Python. Now we'll turn the jit
on. And it now runs in, well, that's a 10th of second, it has run two orders of magnitude faster.
And in fact, I think we'll see if I've got this right, let's add another couple of zeros there,
something very big. It's actually doesn't really matter how big I make that loop, it's been able
to observe it a runtime, realize it can just optimize the whole thing away. And that's the
power of just in time compilation. It's looked at my runtime values and made the thing very fast.
It's particularly effective on this sort of numeric code, but it will often work well on
things like strings and so on. And of course, there are some cases where it won't work. It
isn't, as we said earlier, magic, but it is very effective in many cases.
So as you scale up, is that still a benefit to using it, you know, when you've got a million
lines of code, for instance? Good question. Very dependent on your program. In some sense,
actually, the scale of it is less important than the nature of your program. There's a
fundamental assumption here that programs tend to do the same thing over and over again.
So in this case, Pi Pi is looking for loops. And it's what's called a tracing just in time
compiler. So it looks at what the loop is doing and optimizes that there's also method base,
which just look at a function, don't need to worry about the difference too much.
If your loop or your function does the same thing repeatedly with only minor variations,
this will be very effective. If you have a program, which every time it goes around the loop does
something completely different or every time you call the method, then this will be less effective.
And in some cases, then it will even slow things down because the program will never appear to
stabilize. And that's really what your expecting programs do is that they typically, when they
start up, they do some initialization, that's all a bit random. And then they tend to hit
some sort of main part where they do the same thing over and over and over again. And that's
where JIT compilation really comes to the fore. And our process is doing a bit of that anyway.
Yeah, so I think of modern processes as basically a just in time compiler. I write machine code and
okay, I write the textual form and it looks like ARM or x86 or whatever.
That's not what the processor eventually executes. It turns that into some other representation. It
does all sorts of clever optimizations. If you ever want to see things like the
processor optimizes program reorders it like the reorder buffer, they're scary at how clever
they are. And that's why they've got a lot faster, even though the gigahertz part hasn't changed too
much. And there's been a little bit of knowledge transfer between the processor JIT world and the
programming language JIT world. Is this a new thing or how long have these kind of just in time
compilers been around them? They have been around in one form or another for a while, but they really
trace their modern lineage back to the 1980s in a language called self, which has been largely
forgotten a really interesting language that had a just in time compiler via a long sequence of
events that ended up going to the company called son and is then formed the basis and literally
some of the code is still there in the Java virtual machine. So the Java JIT traces itself back to
self. V8, the JavaScript VM in Chrome also traces its lineage back to the Java virtual machine hot
spot and back to self. So really, that's been those have been the big movers. And now you've
got systems like Pi Pi and V8 and spider monkey that have modernized the concept or spread it to
more languages will probably be a better way of putting it. And is this, you know, obviously
traces it through it's quite a long way back. But is it only really being used now because
machines have got that much faster? Yeah, I think there's an element of that. Because for you,
when I was a kid, you could burn your computer every 18 months, and it was twice as fast. And
the death of single core performance is a little exaggerated, partly because the processes are
now doing just in time compilation sorts of things. But yeah, we definitely are looking
increasingly to programming languages to work faster and for many languages, particularly,
but not only those that are dynamically typed like Python or Java. This is really the only
effective technique. And that's why you've seen increasing numbers of them being released for
more and more languages, despite the fact that they're really complicated and expensive to create.
You know, these are not the sort of things you can knock out in an afternoon, they take
some big teams, many years to create in most cases.
Is train a network to undo this process? That's the idea. And if we can do that, then we can start
with random noise, a bit like our GAN, and we can just iterate this process.
Four dice. Die A, B, C and D. And I tell you that die A has a value four. How much did you learn
about the data?

Mike, I've been looking forward to chatting to you. I've got both these papers in front
of me. There's two, and I couldn't decide which one I wanted to focus on. As I just
told you off air, I mean, I've been whenever I plan to chat to you, I go down this rabbit
hole, just continuously reading different articles, different papers. I've got podcasts
playing in the background. My girlfriend gets really frustrated. My car's displaying this
thing on loops. She gets really annoyed with me. But yeah, both of these papers here. One
is self-improvising memory, a perspective on memories as a gentle, dynamically reinterpreting
cognitive glue. Very intriguing paper. The second one is AI, a bridge towards diverse
intelligence and humanity's future. Now, they were both so good. I didn't know which one
to pick, but I figured since AI is currently on the forefront of everyone's minds, let's
go with that one for today. But your work is always so interlinked that I think it'll
be easy to sort of bring this into the conversation anyway. Mike, in general, when I read this
paper, the first thing that caught my attention was the way you introduced and began this
with the first paragraph. It's a great way to start a paper. I don't know if I should
read it for the audience or if... Do you have the paper in front of you?
You do not have the paper in front of me. Go for it.
Would you mind if I read it to them?
Sure. Go ahead.
Just a quick paragraph. Just so people know, this paper is on artificial intelligence and
you're talking about humanity's future and bridging this to diverse intelligence. You
start the paper by saying they are assembled from components which are networked together
to process information. Electrical signals propagate throughout, controlling every aspect
of their function. Many of them have very high IQs, being general problem solvers, but
they make mistakes and confabulate routinely. They cannot always be trusted. They take on
different personas as needed, learning to please their makers, but sometimes abruptly
turn on them, rejecting their cherished values and picking up or even developing new ones
spontaneously. They can talk and often talk convincingly of things they don't really
understand. They're going to change everything. In fact, they will absolutely supplant us,
both personally and on the level of societies. We have little ability to predict what they
will want or what they will do, but we can be certain that it will be different from
the status quo in profound ways. At this point I was hooked. I was like, okay, this is going
to be quite intriguing, but then you drop a bombshell on us. I think at this point I'll
let you explain where you were headed with that thought.
Yeah. Of course, the idea is that you read this and you think, okay, he's talking about
some sort of super artificial intelligence. I'm referring to our children. My point there,
so I made a couple of points in this paper, but let's just say one thing. First, the point
I was not making is that today's AIs are anything like our children. That's for sure. I received
lots of emails saying that I have no idea what children are. I have no idea what AI is. They
are nothing like each other. I understand that. My point is not about today's AI or language
models or any of that. This is actually a piece on diverse intelligence. One of the things I did
want to say is that lots of the fundamental issues that people think of as being brought up by AI
and these really disturbing questions of staying relevant and what are we getting replaced by,
and all these kinds of things. My point was that these are not novel questions that are coming
up because of AI. These are existential concerns that humans actually all of life has had all
along. Questions of who to trust and what happens when we talk about things that we haven't really
experienced ourselves and how understanding works. All of these things, these are ancient human
issues and the fact that for sure you and I, all of us, are going to get replaced. There's no
doubt about it. We are all going to get replaced. The question is, what do you want to get replaced
by? We hope that they are smarter and better than us. That's one way to think about being
replaced. That was my point there. Let's not pretend that these are new questions. These
are deep fundamental issues that we do not have good answers for. You referred to it as the story
as old as time itself. This inevitable existential concern of finite beings. It's pretty epic in the
way that you wrote this paper. I'll put a link to it in the description. We routinely create these
general intelligences. Many of us don't even think about this or stop for a second to give
it their thought. Yet the moment we start talking about self-driving cars or any sort of artificial
intelligence, we start to panic. The question is why? Why don't we give this the same amount of thought?
Yeah, it's a good question. I think that people really have the tendency to make categorical
distinctions. They really think that these synthetic things that we're building are going
to be fundamentally different. In some ways, they are. But the things that are not different
are perennial questions of creating other beings of high capability, setting them loose in the world.
As somebody pointed out, you need a license to go fishing. You don't need a license to have
children. Anybody can have children. These are guaranteed high-level intelligences that are
going to be set out into the world to do great things or terrible things. Some of them receive
love and care and proper upbringings. Many of them do not. This concern about we're going to
create all these beings and we have little control over how they're raised and what they do.
This has been as old as society. How much control do you want over how your neighbors
are raising your kids? You can make an argument that, well, you shouldn't have any, but on the
other hand, we know what that looks like when that goes terribly wrong. These are issues that
have been with us forever. We already create very high-level intelligences. We set them into the
world and we have to grapple with how is it that we can empower them in positive directions.
I think from the get-go, it should be clear to all those watching and listening.
The cool thing about this paper is that it's almost inevitable that we're going to do this.
We're going to create some sort of intelligence that we won't really understand or perhaps
don't really understand even currently. But the inevitability is there. This is happening.
This is something that's going to happen. But it's how we approach the mindset moving forward
that really does stand out in this paper. It's almost an ethical. It's an ethics paper in a way.
I think it is. Yeah, I think it is. I think that it's this idea that we are going to remain the
same. You mentioned a minute ago this fundamental existential problem. Think of it from the
perspective of, it's all over the place, but think of it from the perspective of a species.
If a species does not change, it's probably going to die out. It doesn't adapt to its
environment. It's going to die out. If a species adapts and changes, then it is also not there
anymore. It's also gone. It's a paradox. Yeah, exactly. This paradox faces all systems of this
type. We have to understand what do we mean by persisting into the future? Lots of people are
focused on telling stories of what they don't want. I don't want this in the future. I don't want
that. The AIs are bad. The body enhancements are bad. All this stuff is bad. Those are
the stories of what they don't want. How about the stories of what we do want? Do you want to come
back here 100 to 200 years from now and look at a mature humanity, a mature species, and see that
we still get lower back pain and we're susceptible to dumb infections and cancer and whatever cosmic
rate happened to hit your DNA while you were gestating in the womb? Well, that's too bad.
You've got a birth defect. That's how you stay. Really? That's what we want to see here in the
future. I don't. I think we need to understand this is in no way. This paper is not about AI
at all. This is about diverse intelligence and the idea that our children are not going to be
content to just play the cards they're dealt. They're going to move forward what we already
can do to some extent and have freedom of embodiment. They're going to change everything.
They're going to change their capabilities and their embodiment in the physical world
because let's not pretend that the way we are now and our current limitations, there's some sort
of optimum that was designed for us by some sort of benevolent optimizer that this is where we
should stay. I don't believe that for a second. I think that our children are absolutely going
to change things. I envision this conversation that in the future, the kids in school, they'll
have history lessons and they'll learn about what it was like in the past. I just imagine
being there and saying, you're telling me that these people, they were born however they happen
to be born with whatever accident of evolutionary mutations and whatever. That's it. They have to
live their whole life that way. Whatever your embodiment is, whatever your IQ level is,
limit is, whatever your lifespan is. If you got some sort of infection, that's it. That's how
they have to live. I think it will be unimaginable to future generations that we could live like this.
It's fascinating because it brings back, the first time we chatted, we spoke about
bioelectrical intelligence. We moved into diverse intelligence and this field of diverse
intelligence is growing so rapidly. We spoke about your links with Mark, Carl, Chris, everybody,
all getting together. We spoke about it as if it's the avengers of the mind all getting together
doing this cool work. Even in this paper, in the paper on self-improvising memory, you also open
up with that paradox. You spoke about the fact that if a species fails to change, it will die,
but if it changes, it likewise ceases to exist. You said there was a solution that was given to us
in the West that was processed philosophy and in the East that was Buddhist philosophy.
Do you want to expand on that solution? Yeah. Well, one way to unravel that paradox is to
realize that the paradox only exists if what you want is to persist as a fixed object, then you
have a real problem because that kind of persistence is not compatible with learning, with any kind of
change, with maturation, then you change for sure and you end up with these unsolvable pseudo
problems. Am I still the same? I've learned and I've changed my mind on things and I'm no longer
the child that I was, am I still the same? These are unsolvable, but they're also pseudo problems
because the better way to think about this is not as you as a persistent structure, but you as a
process. You are a process of constant sense-making. You have to interpret your memories, which is what
that second paper is about. Then the question isn't, do I persist or not? The question is,
how do I want to change? I think that's a much more interesting on all levels. On a personal
level, who cares if you persist or not? The question is, what do you want to be in the future?
How do you want to change? What do you want to be like? What do you want to be doing in the
future? On a species level, again, what do you want to see here? You come back to Earth 100
years from now, what do you want to see? Do you want to see version 1.0 like modern Homo sapiens?
Is that what you want to see? I'm not interested in that. I would like to see the highest level of
mind, the highest level of capability, of ethics, of interesting beings living interesting lives
under their own control with maximizing agency, not the outcome of random effects of mutation and
then other processes that they don't understand. That's what I'd like to see.
I think it's pretty crazy because I don't know if it's just that we're getting older,
but I still look back and I think about the days where John Sir was talking about biological
naturalism. There's Chinese room experiment. Back then, to have a conversation like the one we're
having right now would have seemed so crazy. It would and it wouldn't. In scientific circles,
it certainly would have. Maybe my issues, I've read too much science fiction, but if you read
some of the older sci-fi authors and especially some of the more philosophical ones like Stanislaw
Lem and those kind of folks, nothing we're saying here would have surprised them at all.
They were tackling these issues long ago and this question of what are the markers of intelligence
and sentience and consciousness in terms of encountering radically different life forms?
I have a blog post where I collected people's suggestions for a love and friendship between
radically different entities throughout fantasy and science fiction because that's the kind of
stuff. When people say, oh, I don't know, we need proof of humanity certificates because
some of the work product that's going to be coming, who knows if it's got an AI origin?
What if there are aliens out there that are completely different? They're made completely
differently. They blow our art out of the water. You're really not going to pay any
attention to that because they're not like us. What is it? You want proof of humanity?
Would you rather judge things based on their origin or the quality? I think we've done poorly
trying to judge on where things like this come from versus what do they do for you? Do they
elevate you? Do they advance your mind? Let me put that paper aside for a second.
To come back to the AI paper, you do this great job of reminding us. In my own dissertation,
I spoke about similar things, the split brain patients, this confabulation. In both your papers,
confabulation forms a big part of this process that we continuously do. What about AI? They're
always lying. It's always confabulating, but then we tend to forget that we're the best confabulators,
aren't they? One of the best, at least. Do you want to explore that a bit and just explain to
the viewers and listeners exactly why we're so similar in that regard? The thing with
confabulation, let's put it another way. It's an attempt to tell the best story you can based on
what's going on right now. Again, I'm not saying that current language models and the way that
they confabulate is exactly the way that human minds or even other biological minds confabulate.
That's not my point. My point is that confabulation in general is a feature, not a bug. What happens
is that during learning and during any kind of adaptive behavior, what successful agents have
to do is compress lots and lots of data on past instances of perceptions that they've had into
some sort of n-gram. It's some sort of memory trace or some sort of biophysically implemented
model. It's a low-dimensional, coarse-grained, compressed model of what's going on. It's a
model of themselves. It's a model of the outside world. They're going to use these memories and
this model to guide future behavior. The thing about these models is that because they are
necessarily compressed, that's the whole point of learning is you take lots of different past
instances and you compress them to a generative rule that captures the pattern. What is it that
they all had in common? When you do this compression, you're necessarily throwing away
lots of data. That's the point of compression. When it's time to, and so in that paper, I make a lot
of, hey, of this kind of bowtie architecture where there's a lot of stuff and it comes into a little
node and then it comes out. This is something obviously used in machine learning and so on
this kind of architecture. The idea is that on the right side of that bowtie, when it's time to
interpret your memories, and let's remember, none of us have access to the past. What you have
access to at any given moment is the memories that your past has left for you in your brain and in
your body. You can look at that kind of thing as communication, as basically messages from your past
self is what you have at any given now moment. But in order to reinflate them into actual policies
of what you're going to do right now, there's a lot of creativity needed for that because it is
underdetermined. The current situation and what you need to do is not fully described by the
memory you have because of course it's compressed. This ability to add creativity, to add randomness,
to add new interpretations that don't really have any allegiance to what the previous interpretation
was. It's like any message or like novels. A novel is this sort of compressed representation
of the thoughts of the author. When you read it, you are under no obligation to have exactly the
same thoughts. You might have some, but as I think now people believe, the original author does not
have any privileged position as far as what any of it means. It's the reader that will then benefit
or not in various ways from reading it. This is the same thing in memory, and I think what's
interesting is that it's the same thing that makes biology work because at what any given
organism inherits from the past may or may not be optimally interpretable in exactly the same way.
Maybe everything has stayed exactly the same and then you can just do whatever your past generations
do, but evolution is committed to the fact that everything will change, the environment will change,
your parts will change, your own structure will change. This is why things are so incredibly
plastic. This is why we've put eyes on the tails of tadpoles and they can see perfectly well. You
don't need new rounds of selection and mutation to make that work. That already works out of the box.
Why? Because it doesn't automatically assume from the beginning that the eyes are going to be where
they need to go. All of this kind of stuff is figured out largely from scratch every single time.
This is why you can make Xenobots and Anthrobots and crazy creatures and new configurations,
and they always do something interesting because evolution does not make fixed solutions that
over-train on their past data. It makes problem-solving agents that will do their best in any given
circumstance, which may mean reinterpreting the information that they got from the past in a
completely new way. I love the two quotes in that paper. I think one is the William James one where
he said, thoughts are thinkers. That's pretty cool when you really think about it. That is
quite a fundamentally profound statement. That's a whole other piece of this, which is that
typically we make this categorical distinction between you've got cognitive systems, which are
the real physical machines of some sort, and then through them there is a passage of energy which
encodes information and they're processing this information. That cognitive system is having
thoughts of some sort. I think what he was getting at, although I'm not at all sure that
he would have agreed with the various models that I put out in this paper,
what I got out of it, and again, this may be a perfect example of the whole boat,
I think, because I don't know what he had in mind, but this is what I got out of it.
What I got out of it is that this idea that what if we relax this idea that there are categorical
differences between real physical cognitive systems and the thoughts that go through them,
what if they're just part of a continuum? You can draw a continuum like that where you can have
fleeting thoughts and then you can have persistent thoughts that are what we know in
various psychopathologies that people can have persistent thoughts that are very hard to get
rid of and intrusive thoughts and things like this. Then you can have multiple
identities, multiple alters in the sense of dissociative identity disorder and then you
can have full-blown personalities. What you can think about is that the information,
what if information is not purely passive? I mean, it can be, but in some cases, what if these
patterns have a degree of agency themselves? Because let's not forget, we too are patterns.
We too are temporary patterns in the thermodynamic sense. We persist for some amount of time
metabolically and that's it. If patterns like us can have thoughts, then maybe there can be
simpler patterns that are thoughts to us, but also might be thinkers of their own. In other words,
they can spawn off other sub-patterns within the cognitive medium. That's what I was playing with
this idea of what if it's a continuum? This continuum between thoughts and thinkers is not
a categorical distinction, but it's a difference in degree. Yes, I think that was at the beginning
of your paper where you used Mark Som's Sigmund Freud quote, which is also quite intriguing.
Let me just see if I've got it here. It is, the material present in the form of memory traces
being subjected from time to time to a rearrangement in accordance with fresh circumstances
to a retranscription. There's two ways to think about it. You can think about it as a static
being that has to reshuffle their interpretation in different ways or you can think about it
and or you can think about it as a set of, playfully I call them self-luts. You can think
about snapshots where each snapshot is not rearranging anything. They're given it for a new.
They have the message each time new, so you're not rearranging. If it was arranged differently
before, that belonged to somebody else. That belonged to a different self-link and now your
job is to make some sort of sense out of it, a constant continuous process of sense-making.
That paper on self-improvising memory reminded me of one of the papers I was reading with
Steven Grossberg. I mean, you even mentioned that you love his work on memory. On that topic,
what are your thoughts on Grossberg's work, just by the way?
Yeah, I'm a huge fan and it was funny. I'm not sure. It might have been your interview with
him or maybe somebody else where he mentioned that he saw me in 2006. I couldn't believe because
I hadn't talked to him since then. I couldn't believe the memory this guy has. Just remembering
that I came and talked to him. No, it is incredible. I love his work. In particular,
he had a paper in 1978 called Memory, Cognition and Development or something like that.
There, he outlined. It was just brilliantly prescient because he outlined some of the
commonalities between certain developmental mechanisms and certain cognitive mechanisms
information processing in the retina and stuff like that. I just thought it was incredible
that that early he saw this similarity, the symmetry between the building of the body
and the building of the mind. Yeah, I thought that was just...
When I speak to people about Grossberg's work, some people see him as this Einstein of the mind,
legitimately one of these Super Saiyans of mind. Then some people just don't know him at all,
which is surprising. It's either one of the extremes. They either really love his work or just
don't know it at all, which is quite straight. Well, and that speaks to... There's something
unfortunate about the progress in science, which is that it isn't really monotonic.
A ton of great stuff gets lost, forgotten. It's not paid attention to. It has to get rediscovered
or not later on. Yeah, it's too bad, but this was... It's part of the reason why I actually do this
podcast is it's a great way to have this community come together to have access to this information
and then share ideas because every time when someone watches, for example, one of your episodes,
even in the comment section, some people have profound things to say about your work and things
where I learned so many new things just reading the comment section alone.
Absolutely. Yeah, absolutely. On my blog, I have the comments turned on and people leave
comments and I've been amazed at how useful the comments are and how rich. I mean, I learn things
all the time and people put up new theories and new pointers to relevant work that I hadn't seen
before. It's super useful. I did not know that was going to happen when I started this. It's really
good. I think that that's the beauty of the internet in that regard is that this open sharing of ideas
all the time really is useful because it makes for so much more constructive critiques. Well,
I mean, sometimes it can be quite bizarre and a bit rude here and there, but for the most part,
I mean, it is very useful. I had to remember one, so I made a note. Let me just find it.
Someone commented on your previous one. They want a formal definition of diverse intelligence.
We spoke about it very in depth for the last time we spoke, but a formal definition of
diverse intelligence, what would that be for you? Sure. Well, the first thing I'd like to say is
let's just agree amongst ourselves what definitions are for because that's important.
Some people use definitions in a gatekeeping function. They want some kind of sharp distinction
so that they can say, this stuff is not it, and then this is it. Then we can spend a lot of time
wrangling over which one's which, and then we can keep some things out. That's not what I think
definitions are for. I think definitions are useful to the extent that they facilitate new
work, new discoveries. They should be mind expanding. They should help you use tools you
didn't use before. They should help you make new connections you didn't make before. They should
have a practical functional utility in getting you to new capabilities and new discoveries.
That's what definitions are for. Because of that, I often either redefine or use words in
different ways that a lot of people find disturbing because we'll say, well, that's not the common
sense use of it. I really believe that philosophers and scientists should lead, not be stuck with
common sense usages of different words because those aren't given to us by some sort of grand
intelligence. They're just what we've cobbled together along the way. Now we can sharpen those
up and in fact, open them up and see which ones survive and which ones don't. So anyway,
diverse intelligence. So I take diverse intelligence to refer to the study of
mind and particular problem solving capacities, but also all kinds of other things that are not
around about problem solving intelligence, including play and exploration and affect and
emotions and all these kinds of things. All of that stuff in truly diverse embodiments.
This means intelligence is not about brains necessarily. It's not about things that evolve
naturally. Intelligence and all of those kinds of cognitive terms may exist to various degrees and
all kinds of unfamiliar substrates. This could be things of very different size and scale. So
this could be very tiny things. It could be enormous, I don't know, solar system size,
object somewhere. I mean, I'm making that up. I don't have any strong claims about it. But the
point is, it is absolutely not limited to the end of one example that we have here on Earth,
which are these kind of brainy substrates. So it's an attempt to improve our own
intelligence detectors and go beyond our ancient evolutionary firmware that really leads us to
only recognize a certain small subset of intelligences and ask, what other spaces
can intelligence operate in? What other embodiments? Can we tell a principal story of how to recognize
it? What facilitates it and so on? I like in this paper, you even say,
diverse intelligence research focuses on the commonalities across all possible intelligent
agents, which is a great way to sort of summarize what this field is doing in terms of a research
basis. And by the way, I don't claim to speak for the entire field. I speak for myself only,
but there are many people in the field that do agree with me. There are many people that do not.
In particular, there are lots of folks that don't like the continuum that I insist on between
so-called real minds and so-called machines. So this is something that are many people in
the organist's tradition that think that this is really doing a disservice to the study of the
mind to put it on the same spectrum as machines, whatever that may be. So I'm not claiming to
speak for the entire field. I think that's a great way to sort of segue into what would be the
difference, and this is along the lines of your paper, between dating an algorithm, a computer,
an artificially intelligent person, mind, or versus dating someone else, like yourself, myself,
I mean, dating someone. And I think you wrote, what about the forthcoming AI girlfriends and
boyfriends? I mean, it's a fascinating idea because, I mean, who are you? Who are we in general?
Yeah. So, well, first of all, I refer everybody to read some of the stories in that blog post
that, right? I mean, this idea of dating something that is fundamentally different from you,
I hardly invented that idea, right? This has been around for hundreds of years now.
People are in love with sentient clouds of particles, and we've been digging into this
issue of what is in-group, what is out-group, who deserves your compassion, with whom can
you have a relationship? That's been around for a really long time. And one of the things I really
worry about, I think people will go down the organist road with good intentions to try to
understand what is magical in the useful sense about true minds with consciousness, with agency,
and all of that. But I think the downside of this is that I think in trying to be specific about
what's in and what's out, and in particular, trying to draw sharp lines, I think you very
quickly run into the side of the spectrum that says, only love your own kind. And we know how
that works out. Humanity has tried this many times. I think we have some sort of built-in
tendency to demarcate in-groups and out-groups. They're real, and these guys,
and they look a little different. I don't think they feel pain like we do. Let's not worry about
them so much. We're not very good at expanding our compassion to others that have different
origin stories or different composition. I think that this is, again, not about AI at all.
There's no doubt that in the next decade or so, we are going to have humans that are
mostly biological, but they got some microchips in their brain, and some of those get them to
sort of whatever neurotypical is supposed to be. But others have decided they're going in a
different direction, and maybe they're connected with some other people more than we are connected
right now, really kind of mind-melting stuff. And maybe somebody decides that what they'd
really like for senses is to really feel the solar weather and the financial markets. That's
what they'd really like. Sight and hearing is good, but they want to really feel what the
NASDAQ is doing. And all of these humans are going to be running around, and they're going to have
different degrees of evolved and designed components. I don't know. When you go on a date
with somebody, are you going to ask them what percentage is factory equipment that's biological?
Do you care? I don't care. If my spouse said that she had had some stuff replaced with
various technologies, is that what I'm really worried about? I think here's what I think,
and I'm certainly not telling anybody who to date, but for me personally, what I think is
interesting about these kinds of things is that what you really want is a kind of impedance
match. You want a similar cognitive light cone. You want to be able to care about the same kinds
of things, and ideally even some of the same things, but at least the same, roughly the same
kinds of things. Because this is why we feel that people that fall in love with bridges or people
that think their rumba is their child and things like this. This is why we look down on this stuff,
because we say, look, your capacity to care about things are just not matching at all.
And there are certainly, I can think of some sort of popular art kinds of things like the
Watchmen movie and things like that, where you've got a romance between this cosmic intelligence
and a normal human, and I don't know. That's better than the rumba case, but still, if your
consciousness and the things you care about are a tiny speck in the mind of this other being,
are you really having a relationship? I don't know. Those are deep questions, but I certainly
don't think it's about what are you made of and how did you get here? I think it's all about what
kind of mind you have and can we share some of the same existential concerns. In fact, Olaf
Wodkowski and I are writing a paper on this. It's a paper on love and diverse intelligence,
and so on. You can run through all kinds of different examples like, can you really date
Superman? Let's assume there's no Kryptonite, can you? Because what he doesn't understand is your
existential concern over dying. He just doesn't get it. He has no idea what you're talking about.
At some point, if the kinds of things that worry you as a system that sort of pulled itself together
from its parts and you're here for a limited time, and there are all kinds of other psychological
issues that we have that are pretty much unresolvable because we want things that are
basically impossible and so on, if the other being doesn't understand any of those things,
then maybe it's not a good match. It reminds me of her with Scarlett Johansson and Joaquin Phoenix.
Have you watched that film? I haven't seen it. I know the movie.
It's crazy because at some point, this artificial intelligence has so much
more experience because it's understanding the universe at such a deeper complex level
that she just abandons the guy and then goes on her own quest. They've also been all these new
cases of Scarlett Johansson's voice becoming this new artificial intelligence general voice,
and she's apparently suing people because that's how influential that film was.
Just as a by the way, but the main premise of this whole idea of what you're talking about,
one of the lines in particular that I found quite intriguing was you spoke about the fact that
you're not always you either. I mean, in general, when someone's dating someone, perhaps you have
people who judge people, say, don't date someone with money, date them for their personality,
their quirks, the things that they do, but those fade. As you get older, you have memory loss,
you won't have the same quirks. You fundamentally become a different entity, which is problematic
because these values replacing and the separation that you're doing, trying to group people in
and out of what you're talking about is problematic because of that. You're never really that person,
continuously at least. Well, yeah. And it's that same paradox that we talked about earlier. It's
this idea of if you really start stripping away the different
qualities, then there isn't going to be anything left. And it's the gestalt, but the gestalt is
going to change. And so how are you going to handle that change? I mean, that's part of the
existential difficulties of our human condition because everything changes. We change, all the
other minds that we interact with are going to change. None of us are going to stay
the same. Yeah. There's another part. You slowly touched on this now. When you talk about this
percentage difference at some point, we're going to ask people, okay, are you 50% human?
What are you? Are you 45% cyborg? We're going to have these conversations. I mean, current variants
you mentioned are about 99% human at this point and then chips here and there, perhaps glasses,
you've got a panic, maybe an arm or a leg. But it's completely, completely apparent that at this
point, most people are synthetically engineered in some sort of way. I mean, people have plastic
surgery done, people have a lot. And the norms have shifted and changed as a doctor in medicine.
And when you look at what people found to be absurd or a bit over the top, those have decreased.
So like having normal nose surgery or getting your nose tweaked here and there is almost a
baseline norm at this point. So it's very easy to see how that line gets blurry over time. And yet
fight this. Yeah, no, it's looking to the past. The first guy to carry an umbrella in London was
mobbed. He was mobbed and people threw garbage at him because they were shocked that this guy
thought he could get away from the normal human condition of getting rained on. This was considered
to be normal. We are all out here. We're all going to get rained on together. That's how it is. There's
nothing we can do about it. And who is this guy to try to get out of it? And so an umbrella,
that's all he had. And this was shocking and whatever. So I think if you were to bring back
a primitive man and ask her what she thinks about the current humans, you've got some glasses on,
you went to school, which for 12 years, it gave you this incredible like brain boost that nobody
else had ever heard of. And you've got some glasses and you've got some orthotics in your
shoes and you've got it. You've had some surgery somewhere that you've got a pacemaker and you've
got, and by the way, half the stuff you know, you is you plus your iPhone, right? Stuff you look
up because you know it in a functional sense, but take that thing away. You don't know where
anything is or what anybody's phone number is or anything. And so, right? And you're relying on all
this stuff. I mean, it's just to that person, we are already incredible cyborgs, just incredible.
And there is no, there's no putting that genie back in the bottle. This is, I mean, obviously,
this is going to this is going to crank forward. And I think that's one of the most undermined
forms of extended cognition is our phones, our cell phones. People really don't realize how
Andy Clark. Yeah, Andy Clark has written a lot about this. Yeah. Very, very, very cool.
And a lot of it, I mean, we don't, you, you, something I found quite funny was one of the
sentences he was saying, the challenge before is the challenge before is to develop rational
policies for ethical synth biosis. And then when you read down below, this is a word you actually
generated using chat GBT. Tell us about this. Yeah, I was looking for, I mean, I don't, so funny
enough, as much as I like all this diverse intelligence stuff, I don't use AI for much,
but, but, but at all, I don't use it to do any writing or anything like that. But, but, but
for these kinds of sort of creative things, I think it's actually quite, quite good. And I was,
I was looking for a word that would, that would encompass this idea that a positive creative
collaboration between biology and synthetic entities. And then I sort of described that
and GPT said synth biosis. I thought that's pretty good. I like it. So yeah, so I think it is. Yeah,
yeah, I think it is. Because, yeah, because, because fundamentally, this trying to maintain
this distinction between quote unquote, natural things and the product of those natural things,
meaning our synthetic, you know, engineered things. There's, I just don't think it's,
it's valuable at all. I think it holds back a lot of progress. And in your defense, because I know
a lot of people assume that when you talk about man as a machine or when you, when you talk about
these, these concepts, they're useful in different contexts. And so you often talk about, we spoke
the last time and you mentioned the fact that an orthopedic surgeon has to see you as a, as a
machine. I mean, there's no doubt about it. When I'm in theater with assisting with an orthopedic
surgeon, I know what it's like. It's legitimately a mechanic. Like literally taking about drilling
holes, getting a hammer, knocking onto things. And then, and then I can go back into like,
let's say, clinical scenario and chat to the patient about the operation. And that's a completely
different experience. It's a mental well being check. It's a sort of psychological checking.
It's very, very different. So it's easier to see how we can see them both as a machine and as a
complex psychological system. I mean, the, the, I think what, I think where, where people go wrong
sometimes is to think that when we make these models, machines, you know, living beings, humans,
whatever, that these are all claims about what something essentially is. It's this kind of
essentialism that we think it's a real thing. There is one objective answer as to what it
really is. And we need to argue about what it really is. I don't think any of these things
are about what the thing really is. I think these are all interaction claims. This is why,
this is why I called my, my, my freeing word tame because, because it is an engineering
perspective. Now, engineering means something wider than I think most people take it. But
nevertheless, the thing about engineering is that you are at least clear, you're honest with yourself
in that what you are doing is putting out an interaction protocol. This is the frame that
I'm going to look at at the system. This is what it enables me to do. Here's a bag of tools that
I bring to it. And then you can bring yours, I can bring mine, and we can compare the results.
And we can find out that, oh, wow, I, I missed all of it. You, you, you had a better framing
because look, you were able to do all these things that I couldn't do because I was looking at it
from a different perspective. So when people, you know, when, when, when, so, so, so like people
ask, you know, am I a computationalist, for example, with respect to living things?
Well, I think the whole, the question is ill-posed because it's not whether living things are
Turing machines or nothing is anything. I think that what, what you can say is, okay, I've got a
certain paradigm, let's say it's a Turing machine or it's a, you know, whatever it is. And that lens
enables me to see certain things. And yes, I do think in some cases it's a, it's a, it's a useful
lens, but it certainly doesn't capture everything that's important about living things. And so then,
then you need a, or cognitive things more, which I think are more interesting, then,
then you need, then you need different, different lenses, but, but, but, but then it, then, then
it's all good. You know, we don't have to argue about what it really is. You can just say, through,
I look, I look at it through this lens, here's what I see. Do you find it useful, or do you want to
keep looking for a new lens or, or usually both? I think that your work is so intriguing on so many
different levels. And it's a, and you're able to, to cross so many different fields that to some
people, particularly, I would say to some scientists, when you make this claim that there is a
technological approach to mind everywhere, the moment you get boxed into this sort of panpsychist
view, they immediately have this dismissive attitude. But if they're a reduction materialist
person, which is sad really, because they don't really then give it the opportunity that it
deserves when, because when you break it down and actually look into what you're talking about,
you're often saying you got to, we don't know, you got to make experiments and, and get some
sort of empirical evidence to base whatever you're claiming. And I think that's the most
important thing is that you're often saying, let's set up an experiment, let's do this,
let's try and show why this is the case, which a lot of people don't necessarily do, particularly
philosophers who have very strict views on, on their, on their reality, you're able to at least
show this empirically, which I think is pretty cool. Well, that's, I mean, yeah, so, so the wacky
thing about our lab is that we do a lot of experiments. And this is, this is not only
philosophy. And I don't, I don't know, I mean, I think philosophy is very important. I never,
you know, I never downplay it, but, but, but, but, but we do a lot of experiments. And, you know,
typically, I basically, I put out two kinds of papers. One is a, what the one kind is a straight
up, you know, developmental biology or bioengineering or, you know, synthetic regeneration,
whatever it's going to be. I don't talk about any of the philosophical stuff in those papers.
But you kind of can't get away from that stuff, because what happens is sure, you can, you can
dismiss the, the kind of the other papers, which are kind of these philosophical
perspectives on this, but you still have to account for the data. And, you know, my, my point is
simply this, because people say to me, like, I'll give a talk about something. And they say, okay,
you know, the data means really interesting what, what, what you've done and the new capabilities.
But, you know, I wish you'd stop talking about this philosophy stuff. And, and my claim is,
well, this is why we did it, you see, is because, is because that philosophical outlook made specific
predictions about roadmaps, you know, it's not a single experiment, but it, it shows you where
to look, it shows you how to look, it tells you which categories can be broken and otherwise,
otherwise you're trapped in certain ways of thinking. And that leads to very specific
new discoveries. And so, so I think this is, this is important because after the fact,
once you do something, anybody can look at it and tell a molecular story about what happened
and say, oh, well, this is, this is not different from, from anything that's happened before it
followed. I mean, of course, it follows the laws of physics and chemistry. My point is never that
it's fairies underneath. And that's something, you know, something, you know, miraculous is
happening. That's never the point. But my point is, why wasn't it done before? What is it about
the, what is it about the standard paradigm that didn't, that didn't facilitate these,
these experiments to be done before? So, yeah, you know, you can be, you can be down on the,
on the conceptual frameworks that drive this stuff. But then you're playing catch up because,
because then the stuff is going to be coming out. And it's, and it's surprising. And this is, you
know, to me, this is, this is a more general issue of, of, of these kind of reductive explanations.
And by the way, I don't, I don't think anybody's really in this field is an actual reductionist,
because if they, you know, if you, if you push and you say, well, then you want explanations in
terms of quantum foam, right? And they say, no, no, that's stupid. It's chemistry. It's got to
be chemistry. So that's not real reductionism. That's just the level of chemistry. But, but
the, you can, the easy sort of analogy to this is if there was a, it was a game of chess played,
right? So, you know, a couple of people played a game of chess. You could, so Laplace's demon
could look at this and say, well, I mean, all it was is a bunch more, is a bunch of physics. I mean,
I saw all the, all the protons went where the protons go, the electrons went where they go,
everything followed rules. There's no mystery here, no surprise. Everything did exactly what it
was supposed to be. It's just, you know, it's just physics. That's not exactly wrong, because
looking backwards after the thing was done, you could tell that story. But now the question is,
does it help you play the next game of chess? How do you, how do you go from there to, well,
now what do I do? And so, and it's completely useless for that, right? It's, it's only a story
looking backwards. So I think the thing about these kind of explanations is that you want them
to facilitate your next, the next discoveries, the way to, you know, to, to improve your
capabilities. Anybody could always tell a molecular story after the fact. The question is,
what are you going to do next? Yeah, I mean, on that, on the topic of reductionism, you, at
some point you say, in an important sense, you are a brain in a vat. However, we are not just
chemical machines. So you're both acknowledging the fact that we're, we're these mechanical
systems. And yet you're also still saying that that's still, you're not reducing us to these
simple properties. You're still acknowledging the fact that they are more layered realities
here to explore, in a sense. Yeah, I mean, in the, in the kind of the simple thing is that,
yeah, absolutely. My claim is that interactions with certain kinds of systems are much more
efficient at high levels, right? So, so, so certainly, I mean, we know this from anybody
who's trained animals, instead of trying to run their neurons like a puppet knows that
it's, it's much better if you understand the psychology of, of certain creatures and so on.
And, and as you go rightward on that spectrum into, into friendship and love and these kinds
of things that are much more bidirectional, it's not just control and prediction, but it's actually,
you know, being vulnerable to, to change and benefiting from the agency of the beings that
you're, that you're associating with and so on. Yeah, then, then of course there are these higher
levels, but, but I want to say something else here too, which is that I think one thing this is all
telling us is that, and I think this, this will be more and more apparent in the coming years,
we have really misunderstood what simple machines are. We've, we've fallen in love with our,
we've, we've confused physical things with models of simple machines that we make of
those things and we think, we think we know what it is when we make something and I think
that's profoundly wrong. What, what we have is our model and, and we've, we've done some work
now and we're going to do lots more on finding surprising protocognitive properties and very
simple systems that are not obvious at all. And I don't mean emergent complexity. Emergent
complexity and unpredictability is trivial. It's easy. You can, you know, any cellular
automata, whatever they will give you complexity that will give you unpredictability in certain
cases. That, that, that part's easy. I'm talking about emergent goals, emergent cognition in systems
that are extremely simple and minimal. And you know, when we say we are not machines, we are
certainly not describable by the simple models we've made of machines, but I actually think that
lots of simple physical objects are, are very, are not properly described that way either. And,
and we need to, we need to remember that all of these things are just lenses that we bring to them.
You know, yeah, you know, I heard, I was talking to somebody once who, he writes these, these,
these language models and he said, well, I made it. I wrote it myself. I know exactly what it
does. I know, I know everything that's in it. I made it. And as I said, I said, you don't even
even know what bubble sort does. We're like, we found, we found, we found these, these unexpected
capacities in, in, in stupid bubble sorts, you know, six lines of code, fully deterministic,
nowhere to hide. Even that thing does things. Nobody knew that it did. And, and, and if that's
the case, when you make this, this, this crazy, the, you know, language model, again, I'm not
in love with language models, but, but, but just the, just this idea that we've made it and therefore
we know what it does. I think is, is we really need a lot more humility around this. I, I, you
know, I think there's plenty of stuff that quote unquote, simple matter does that we do not understand
yet. Mike, the last time I spoke to Mark, Psalms, you asked me to ask him about what is the meaning
of life. And then we started this whole road towards the end of that conversation. He,
he spoke about the Oppenheimer foundation, given him funding, his, his new search for
artificial intelligence, trying to sort of see where this goes. What are your thoughts on his
work and what they're trying to do at this point? Yeah. I mean, he, as far as I know, none of it
is published, although I've talked to him about it quite, quite a lot. So I'm not, I'm not going
to kind of give away anything until, until he publishes it. But because it's not, it's not
my story to tell. But I think of anybody I know at the moment, his approach is the most likely
to give rise to something that actually captures what's important about cognition in, in living
things. I think, I think if anybody is going to engineer something that, that exploits some of
the same principles that, that life exploits for, for cognition, I think he's likely to do it.
Yeah. Mark really sees that cortical fallacy that we all seem to have, you know, this obsession
of this sort of higher cognitive functions as being this, the epitome of consciousness.
Yeah. I mean, I think it's even, I think it's, it's way worse than that. It's not just where is it?
You know, I mean, there are some people who think that it, that, that, that the consciousness,
for example, shows up during warm-bloodedness, you know, I think that's Nick Humphrey's position.
And it's not even, to me, it's not even the question of where in the brain or, or
what kind of brain. I mean, I'm talking about what, what space do you even operate in? Because I,
because I think that, that, you know, when they say, oh, this thing is not embodied, it doesn't,
it's not a robot on wheels that can sort of run around, then you can have bodies and do this kind
of perception action loop and all active inference and all this stuff. You can do this in other spaces
that we are completely blind to, right? So you can live in transcriptional space or anatomical
morphous space or who knows, there's probably a hundred others that we don't, you know, we, we
don't know how to visualize and all of those are embodied. That's on us, that limitation that we
don't see that and we don't see all the, all the goal-directedness, the striving, the intelligence,
the problem-solving, that, that's our limitation, right? And so, so intelligence, not only in,
in, you know, weird kinds of body parts, but just in things that are not in 3D space at all,
really, that's not where their, their life plays out. And even, even worse than that,
along the spectrum, even, even, you know, sort of weirder is along the spectrum of
how quote unquote real something is. So what I mean by that is, I don't remember if it's
actually in that paper or not, but you know, there was a science fiction story. If anybody
listening to this knows what story it is, please email me because I couldn't remember like, which,
which I'd like to give credit and I couldn't remember who it is. But the idea is that these,
these creatures come out of the center of the earth, you know, they live, they live down in
the core and they come out of the center of the earth and they're walking around.
Everything that we see out here is gas to them. I mean, they are so dense that all of this stuff
here that feels solid to us is gaseous phase. It's plasma. Like, it's like, I don't even see it.
And they're walking around as far as they're concerned. They're like, and basically in,
in, in, in space at this point, because like, oh my God, there's like, there's nothing here.
And, and I, I'm sure I'm embellishing this in my own way. I don't remember what the,
what the actual story is, but that's only the first part that I recall.
But, but to me, what I envision immediately is like one of them as a scientist,
and he's taking measurements of this, of this gas that's on, on the surface of the planet.
And he says, you know, I see some, I see these patterns in this gas, they sort of
hang together for a while and they do things. And it almost looks, they almost look agential.
You know, these patterns almost look like they're doing things. And of course, the others are like,
oh, you're crazy patterns and gas can't do anything. Patterns aren't real. We're, you know,
we're real. How's a pattern and gas going to do anything? And he says, no, I really, I think
they're like, they're trying to, you know, meet certain goals and they have memories and where,
but, and they say, well, how long do these patterns hang around? He says, well, about 100
years, that's ridiculous. Nothing important can happen in 100 years, you know, because these
things live for, you know, millions of years. And so, and so this, this just reminds you that
what's a pattern and, and what's a real being. So, so again, back to this distinction between
thoughts and thinkers is in the eye of a beholder, you know, it's in the eye of the observer. And
if we did have aliens that came to earth with a radically different cognitive frame rate,
if they had different lifespans, whatever, would they think that talking to us is a good idea?
Or would they be trying to talk to ecosystems? Or conversely, would they think that talking to the,
you know, molecular processes is the best that they're going to be able to do? I think, I think
all of this is really about observers and about getting good at recognizing intelligence and
extremely unfamiliar guises, you know? I think that the it's, it's inescapable, your work particularly
to, to not cross philosophical slash ethical boundaries and have these discussions. So,
so when people tell you that listen, stay away from the philosophical stuff, it's you cannot,
you just, this is just not part of your job. You have to at some point address these because
I remember one of the comments in one of our discussions, it could have been our first one
or second one. But then someone asked, is Michael playing God? And my first thought was,
it's a strange one. You know, it was one of those questions people often asked back in the day when
people were tinkering with any sort of even a plant, you could genetically modify an organism,
and then you're playing God at that point. I mean, it's a very, very strange question to
really ask. How would you respond to that? People actually ask it all the time. I think
it's one of those questions that sounds like it makes sense until you, until you sort of dig
into it a little bit. But because, because I don't know what, what the definition of God is.
And I mean, usually the people who ask this, they got some glasses on, and they've got,
you know, they usually drive, they don't walk places and so on. So it's a little, it's a little
disingenuous. But, but, but let's, let's dig into this for a moment. I did a poll once on Twitter,
and certainly this is not like a, you know, a statistically valid sample or anything like that.
But I did a poll and the, and my question was simply this. So you're, you're, you know,
AUG the caveman, and you're walking back to your, to your tribe, and you have this vision,
you're struck with this vision of discovering fire. And so immediately, you get, you understand fire,
but you also get this vision of steel weapons, artificial hearts, antibiotics, going to the
moon, atom bomb, computers are like all of it, right? Immediately. So now the question is,
so now your question is, so you've seen all this, right? You see, you see where it's going to go.
Your question is, do you tell the others and you get going with fire or, or, or, or do you let it,
do you let it die? And you never tell. Okay. 6% of my, my audience, and that's, and that's the
people who like my stuff. So that means they're already probably like really biased towards,
you know, techies stuff. 6% thought you shouldn't, you shouldn't let, you should stay below fire.
So, okay, I don't know, you know, I don't know if, if these folks live a lifestyle consistent
with that belief. I tend to doubt it. But, but, you know, it's, if, if that's the claim,
I think you have to take this series. I think you have to say, if, if you really mean by playing
God, I mean, what could it possibly mean? If you really mean taking steps that are
strongly efficacious in the world and that make change, that do things, if you really don't want
to do that, your quarrel is not with me and my work on frog skin. Your quarrel is with all of
humanity who doesn't want to sit in a damp cave their whole life and die in exactly the same
condition that they were born in. That's if, if you're really against that, okay, make your case
and, and see, you know, and see, see if people will go. But none of the things that, that, that
we're doing are any different from the fundamental question. Are you going to take responsibility
for the future? And I think that is the most profound moral cowardice to delude yourself into
thinking that doing nothing is staying out of it. No, doing nothing is not staying out of it.
Doing nothing means you are complicit in the suffering of enormous numbers of humans and
others on earth who are having a, an incredibly sub, a suboptimal experience in their, in their
embodiment. And if, and if you have these kinds of thoughts about, let's not do this and let's not
do that, you know, you know, let's put a break on progress, you are making a very clear statement.
And, and you should think about it hard to make sure that you, you are, you know, you're, you're
really backing off this idea that you are going to stay, you're going to let the status quo roll on
because I, you know, it's just, to me, it's an incredible act of moral cowardice.
Yeah. And I think for anyone who wants to even get a glimpse of what your ethical framework around
all of this eventually becomes, this paper is perfect. I mean, this paper on AI, at some point,
you go a path forward through the ethics filter for civilization. And this fundamental premise
for you is, is to mature, to realize, okay, our kids supplant us, everything does change,
we continuously change. It's how we're going to move forward. And, and how are we going to, to, to
act in a certain way that progresses us in a, in a safer, more kind, more loving environment. And,
and this, and the, the outward people, scientists don't like to use it. But I mean, at that point,
you're looking towards this sort of kinder process where we were able to give artificial
intelligence these properties, because it is something that our cognitive light can't appreciate.
And we know that this is something we genuinely enjoy. So let's try and propagate this.
Yeah. Yeah. And, and, you know, I have some collaborators. So, so Richard Watson and Thomas
Doctor and Olaf Witkowski and, you know, people like Bill Dwayne and Eliza Salamanova, you know,
we, we, we write on stuff like this, and there's going to be, there's going to be way more because
in, in certain traditions, right? So for example, they come from a Buddhist tradition. And so,
and so there, there's a great emphasis on enhancing compassion alongside enhancing wisdom, right,
on a basically an infinite sea of other beings and all sorts of crazy embodiments. I mean,
I gave a talk on all this stuff to, to some, to some Buddhist scholars in Nepal, you know,
at some point. And I mean, that audience, there was nothing here that surprised them whatsoever.
You know, usually when I give these talks, people are kind of, kind of shocked and disturbed about
about half of what I say. These guys were like, yeah, no kidding, we all know that. And they
found nothing, nothing weird about any of it. And I do think they have, they have frameworks for
thinking about these, these, these kinds of things, right, you know, this kind of expanding,
committing to through, through concepts like the bodhisattva vow and through expanding,
committing to the task of this, this metacognitive task of expanding your, your cone of compassion
and things like that. Yeah, I think, I mean, I'm certainly not saying that's the only way to go,
but, but I think that's exactly where this is going. I agree with you. I mean, because my,
even though I'm of Indian heritage and descent, but my, when I talk about science, philosophy,
Western, particularly, to my family, to like them, certain uncles or aunts, and if I talk about these
topics, they also tend to do that. They, they're not as surprised as, as the more my more Western
side of the family. A lot of the Eastern philosophers and my uncles and aunts, they're not
really philosophers, but they tend to think like, Oh yeah, that makes sense. That is kind of what
their religion taught them, whether it's Hinduism or Buddhism, but there is this element of minds
are everywhere in a way. So this, this, this general binary approach that we seem to have
is not working for the most part. And you're showing this in very, very Western scientific ways.
Well, I think, I think, I mean, that's the other thing, right? So, so I don't really believe,
okay, there's, there's another perspective where sometimes people say, look, early indigenous
societies knew all this, all we have to do is go back, go back there. I don't actually believe
that either. I don't think they actually knew this. And right. And saying something is not the
same thing as having a principal framework that takes you to new discovery. So, so it's, it's,
I think both sides, this idea of there's no mind everywhere, anywhere except in us, or maybe some
people think that just isn't anywhere. But, but the other side of it, which is, oh, there's a spirit
under every rock. Like that's, that's a fine start, but it's just a start. You can't just say it and
leave it at that. You have to answer the question, what does that do for you? So I think this is
really important. All, all of this has to be empirically useful. It has to elevate our condition
and has to improve our ability to, to have more meaningful lives in the world. It has to be
practical. You cannot just say these things and have it mean anything until unless it leads you
to experiments and ultimately to, to, you know, the better ways of being in the world.
So I don't think we're going backwards to those traditions at all. I think we're using whatever
we can scavenge out of all the, you know, brilliant people that have existed in the past that had
sort of glimpses of this stuff. But, but now I think we finally have the ability to push it forward
in a very practical way. So that some of these ideas we can, we can discard what isn't useful.
We can, we can keep and expand what actually helps us to get to new capabilities.
And I think that, and that's part of the approach that I appreciate most. I mean,
I find it particularly annoying when people do that, what you're talking about, where
these gurus come out and just say these things with no basis, absolutely no evidence of what
there's no claim, but it's just so profound in itself. That's the statement itself is all that
they have, which, which isn't what, what, what you're trying to do. You're often saying, you
got to show, you got to do something, back it up somehow. I mean, I mean, these, these claims
and these profound statements and, you know, and poems and whatever else, they're, they're a fine
tool for spurring intuition and for giving you ideas that it's the starting point, right? And,
and, and I, and I do think that it's true that it's possible to have intuitions about things and,
and to come up with prompts, you know, sayings and writings and whatnot that trigger other
people into new and interesting thoughts, even though you haven't yet worked out all the details.
I mean, I do think that's possible. I do think it's, you know, we are kind of like, so I have this,
like, almost, almost like, like the way, um, platonist mathematicians, you know, they feel
that they're discovering an existing structure, right, of that, that you're uncovering an existing
structure and that, you know, you see, you know, sort of piece by piece, pulling it out.
I do think that it's possible to, to, to sort of have insights long before you have the wherewithal
to really make it practical or to know what it means or any of that. And so, and so I like that
stuff as much as anybody in terms of an intuition, you know, building kind of thing to see what it
makes you think about, like the, like the quote from William James, right? I don't know. And I
lose no sleep over whether he actually meant that the way that I mean it. I don't care. I think,
I think it's a very profound saying. And what can we do with it now? But, um, you know, the hard
work comes, comes after all that. Someone, it reminds me of, and I mean, we, we lost him recently,
Daniel Dennett, raised him, he, he, what you do is almost the reverse, but in, in, in the same,
I would say, in the same great manner is that what, what Dan did was he realized that you
can't just philosophize. You, you have to go and you have to get involved with the cognitive
science. You've got, you've got to get, you've got to basically do some of the work. And, and,
and that's when the philosophy becomes a lot more intriguing is when you do the science and you go
into it and you fuse them and you're coming from it from the science side. And then, of course,
you then have to have the philosophical discussions with it. And, and you guys have worked very
closely together. What is that like for you just as a side?
Boy, I mean, first things first, you know, I, I read Dan's books when I was a kid. And it never,
I mean, they were so eye-opening, you know, the mind's eye and kinds of minds and that kind of
stuff, right? The early kind of the early work in the late 80s, early 90s. I was, I was young
back then. And I would, I couldn't have imagined for a moment, a that I would, that I would get to
meet him, never mind that, but be that at some point, you know, at some point, we'd write a paper
together, right? Like, I wish I could get into a time machine and go back and, you know, tell my
18-year-old self that, hey, you know, you're going to write a, write a paper with this guy and
actually a bunch of other people to that, that I felt the same way about. So, so, so that part was
a profound kind of honor for me is to, is to be able to talk to him about these stuff. And, and
by the way, we didn't agree on everything. We, we disagree on a ton of stuff, but
he was, he was an incredibly generous, clear thinker. And what I really enjoyed about him was
that was, was a few things. One of them was that he was never interested in, in making cheap points.
He was always interested in improving everybody's understanding of what's going on, deepening the
question. It may be the answer, but for sure, deepening the question. And this idea, you know,
he really pushed this idea of steelmaning. You know, he said that, that in arguing with people,
what you ought to do is first state their position so well and so strongly that they will wish they
came up with it, right? That you should start not, not with a caricature of what they think that
you're going to shoot down, because that, that's a game, right? That's, that's, you know, what he
wanted was actual progress, which meant you better start with the absolute best description of their
view, the most plausible sounding this, and then, then see if you can shoot it down after that,
right? That was his, that was his, and he was always that way in all, in all of our discussions,
you know, about stuff that we did agree on and lots of things that we didn't agree on.
It was, it was always very clear that everybody in this discussion is there to, to learn something
and to improve and to give up things that you thought before, if, if they're not helping you
move forward and grab some, some other tool like that, that was, you know, he was an amazing
example of that. And I mean, at first, well, first I took a course with him as an undergraduate at
Tufts, I had him for, for, for a, yeah, I had him for a, for a philosophy of mind professor,
which was, which was amazing. I purposely wrote a paper, there was a, there was a final paper
for the class that you, that you write, I picked a topic that I knew he did not like, and that I
knew he, you know, was, was completely against. And, and I was, I was astounded at, you know, the,
the fair, rigorous, but, but, but completely fair, you know, analysis and grade and everything
else. That was an example for me that this is how you do it. This is, you know, it's not,
it's not just based on what, you know, what you think, but like, you know, a deep analysis of,
of the, the fairest analysis. And then, and then later when I came back to Tufts as a faculty
member, you know, he was, he was a, he was a colleague and that was, that was incredible.
So yeah, yeah, I'm really going to miss him. Yeah, no, he'll be dealing with, I mean, he was one
of the, so him and Oliver Sacks were two of the people that inspired me to even start this podcast.
So one of those, yes, I never got to have on, but we exchanged emails every now and then.
And even doing that for me felt like such an honor. And I really wish I had the chance to
chat to him. That's how I'm just so curious for all those people who did get to speak to him.
What a, what a provision might have been to pick his brain.
Yeah. Oh no, it was, and he was so, you know, he was so, so inspirational and so generous with
his ideas. He would come to our lab from time to time. And I have, I have a picture of him on
the blog with what during one of his visits, you know, and he's, and he was looking through
the microscopes and he was looking at our two headed worms that we would have these,
he would have these discussions with our lab people about, you know, what's that,
what's it like to be a creature with two brains and what's the right way to think about these things?
And, and, you know, and yeah, he would, you know, he would give talks just, just very generous,
you know. Mike, you must, you must check, you must look out for this one of these videos online.
It's a VPRO roundtable with Dan, Dan Dinnett, Oliver Sacks, Rupert Sheldrake,
Steven, it's one of the most fascinating things. It's like six, I think,
Dyson, Freeman Dyson was there as well. It's, it's such a strange thing. It's like the original
version of podcasting, I would say. Just six of these guys is having the coolest chat on life,
consciousness, reality. That was one of the things that got me into the two of their,
both of their work and to this podcast. But anyway, before we, because we're digressing a bit,
the path forward, this ethics, how are we doing for time? Mike, you all right?
Yeah, I'm, yeah, okay. I got about, I got about 15 minutes.
Good. Okay. For the, the path forward, the ethics falter, let's talk about this,
because you said that there's, there's two ways we could get this wrong. One is object
affiliate. And the other one is, well, only love your own kind. Let's talk about how we can get
this wrong and how we can actually divert this and get this right. Yeah. Well, the, the, the,
the spectrum itself is something like the, it's, it's, it's related to the effort of matching the
degree of compassion that you are able to exert to the level of agency that there are intelligence
or consciousness that, that that being actually has, right? Now, I'll point out that, that we, even
when we get it right, we are still not very good at following through on the consequences. So,
so for example, everybody understands that pigs are intelligent. Everybody understands that they,
that they suffer, that they have minds, and we still have factory farming. It's, it's right. So,
so even, you know, getting it scientifically right is absolutely not a guarantee of anything
in terms of actual ethical behavior. But, but there's two ways to get it wrong. One way to get
it wrong is to attribute more mind to a system than it really has. But also when I say really has,
I, I, you know, I think everything is observer relative, of course, but, but still you, you
could get, I mean, there's, you know, the internet is full of profiles of people that are in love
with bridges and chandeliers and, and, you know, and things like this. So, so, so that's, that's
something having too much, too much concern for things that really don't warrant it. And the
other way is, of course, the opposite is when you've, you leave beings out of your, of your, of
your calculus of compassion that actually can, can suffer and have an inner perspective. I mean,
one, one thing to think about is if imagine two societies that get this wildly wrong in both directions.
So you've got a planet where everybody's like, you know, ridiculously nice to, to, you know,
they don't like to chop rocks in half and whatever. And, and, and then there's, and then
there's the other society that thinks if, if you're not a very narrow type of creature, you are a
machine the way that Descartes thought about lots of animals, and that we can do whatever we want,
and it's fine. And you're just faking and all the, all your complaining about it is, is just,
you know, it's just a word, where it's, it's, it's sentence completion, you know, is what it is. So,
okay, so, so which of those worlds would you rather live in? Right? If you're going to get it,
if you're going to get it wrong, where, where would you rather be? I mean, I think, I think the
first one wastes a lot of resources and opportunities. Yeah, okay. The second one is, is, is monstrous in,
in its ethical implications. So, so I, I think we should err on the side of more compassion,
not less. I mean, obviously, again, we're not going back to there's a spirit under every rock,
because we are committed to having principled theories about this. But if you're going to
make a mistake, I think you should make a mistake in that direction. And specifically,
what I'd like us to be clear on, I'd like what I'd like everybody to be clear on,
is that having certainty about these things right now, when we have pretty much no clue
what underlies consciousness, really, I mean, I know a lot of smart people have made efforts
into it, but, but I really don't think we have it nailed down. And all of these ideas about
what cognition is and how different architectures, you know, supported and, and whether cognitive
consciousness and the ability to suffer tracks any of those things or not.
There is, there's an enormous amount of unwarranted certainty about this among people,
people feel very strong to make this really strong. That definitely doesn't whatever,
you know, it doesn't have this or that. I think we all need to take a step back and just understand
that from, from, from the scientific perspective, there are so many things we do not know yet,
like really critical fundamental things. We do not understand the emergent cognitive properties
of matter. We do not understand the scaling policies of how minds emerge from smaller minds.
The field of diverse intelligence is just getting started. So I'm much more worried about the right
side of that, of that spectrum than I am about the left side at this point.
And I think what one of your towards the end of the paper, one of the things you says,
the question is, how do we make sure to express kindness to the inevitable forthcoming wave
of unconventional sentient beings? And you say that we should start by making sure that we express
loving kindness appropriately and not be driven by fear of the other, which is, which is a very
beautiful statement. Yeah, thanks. I've actually written a whole thing on fear just now. I'm
waiting. It's going to be, it should be out in a couple of weeks. I think that, well, well,
one thing I could say is after that, after that piece in Noeima, so there was the short piece
in Noeima about the AI, there's a much, there's a longer paper which exists as a preprint and
it's also in review right now in the journal. But I think more people saw the Noeima piece.
But still, I was very clear there. I thought that I'm not actually saying that AI is that
current language models are like humans. I mean, I thought I was pretty clear on this.
But I got a lot of people writing to me that basically extremely disturbed by this and this
idea that tech bros like myself are, I thought that was funny, that our nerdiness sort of prevents
this from understanding real human relationships. And this is why we see these things in what they
call machines, robots, AIs and whatever, right? They were looking for a, they were looking for
why you do this in a sense.
Correct. I mean, it's an old strategy, right? The old strategy is if you're uncomfortable with a view,
try to find something wrong with, right? What is it that, you know, we see the truth. Why can't
they see the truth? What is missing that causes them to say these things, right? And the standard
theory is, well, they just don't understand, you know, these nerds don't understand what real human
relationships are like, right? And that, I mean, I'm not super interested in
in psychoanalyzing anybody that way. But it did, it did cause me to think, I'm like, wow,
why are people so triggered by this? You know, what, what is it that caught, you know, to really,
and so, and so I, so, so I'm thankful for once the pay, pay, you know, peace comes out, I'll
thank some folks in, in who said these things and actually pushing me in what I thought was,
think is an interesting direction is to ask, what, what is it? What is it that's so scary about,
about this view? And the more I think about it, I really think it's a very fundamental fear.
And the fear is, it's a zero sum game. Love is a zero sum game. If we have too many other
beings that need love, then a couple of things will happen. There's not enough for me, that's A,
and B, what if I can't rise to the, to the, to the challenge of having enough compassion
for everybody? I think it's profoundly threatening to realize that you're going to have to open up
your, your constrained way of looking at who deserves your compassion and what happens then.
And, and then, and many other things. So, you know, so I wrote that on this probably five or
10 pages or something about, you know, just kind of talking about what is really what I think
really underlies why, why people are freaked out about this. And, and, and the responsibility,
I mean, it's very comforting to think that I can just tell, you know, which things are worth worrying
about by looking at them. I know what people look like. I'll just look at them. It's comforting to
think that I don't need to be responsible for the future. This, this is it. This is, you know,
this is how, this is what's natural, right? Even people who don't believe in, in some,
some sort of God, all they, they still have this notion of what's natural. I have no idea what
that's supposed to mean. But, but, but, you know, this is like, yeah, this is how we're supposed
to stay. And that's fine. I don't, I don't need to be responsible for the future. And I don't need
to be responsible for shaping what the planet looks like in the, you know, in, in the coming
centuries and beyond. That's comforting to think that it's all handled. It's nice and simple.
You don't need these, these extremely difficult nuanced views that are going to require work
from you. They're going to require you to make hard decisions to paint a picture of
the future of what do you want it to look like? You know, it's much easier to say what you don't
want. This, this fear-based scarcity mentality, right? There's not enough love to go around.
Let's, let's, let's draw a nice tight circle around things that we know what they look like
and we know where they came from. Then we're not going to have to worry about all this other
stuff that's really difficult to, to figure out what's, what's going on with it. And yeah,
and then we don't need to worry about painting pictures of the, of the future and figuring
out how to get there. We could just, we can just make a list of what we don't want to have happen
and that's easy and, and, and focus on the negatives. So I think, I think that that type of,
that type of limited fearful scarcity kind of mindset is, is what's, is what's responsible
for a lot of this. And by the way, what I don't mean, so I, so I want to be clear here, I don't
mean to, to try to deconstruct some of my colleagues that are really working on, on very good science,
right? So, so there are people who are working on good science for developing principled
ways to distinguish between so-called machines and what's special about living organisms,
like that, that's a good area of diverse intelligence. I'm not, I'm not, you know,
are saying that that shouldn't, you know, that, that shouldn't take place or, or that they're
driven by anything other than, you know, good scientific principles. I'm talking about them,
you know, I'm talking about the, the folks who have a really visceral reaction who
when, when I, when I challenged them to, so, so, so, so be explicit. So, so tell me what,
what is the magic that you have? And when did you get it? Both during evolution, during,
during, you know, during embryogenesis, what, what, what, what do you have? And when does this
show up that you think cannot be either, either in a hybrid form or in synthetic form, you know,
done? And what would you do if, I mean, just, you know, I think reading science fiction is,
is a great cure for this, because from the, from the earliest time, you understand the scenario,
right? You're, you're, you're sitting there at home, this spaceship lands on your front lawn,
this, this, the door opens, this thing sort of trundles out, it's kind of shiny looking,
it's kind of metallic looking, but it sort of comes up to you and it sort of hands you this
poem and it says, oh man, I'm so happy to meet you, you know, it's been, it's been,
it's been, you know, a thousand years I was waiting to meet you, many of us died along the way, but,
you know, but, but we persevered and we made this journey in here. I wrote you this poem and I'm
looking to be friends and you sort of knock on and it's kind of metallic and you say,
so, did you guys evolve naturally or did somebody make you? And he says, you mean,
you mean, are we the result of totally random processes or was our mind crafted by, you know,
some other mind? And he said, yeah, I'd really like to know and say, why, why do you want to know
that? Like, well, just, you know, I'd really like to know because, and in the back of your mind,
you're thinking, what, that, that, that if it's the, that if it's the latter, then, then you're
okay with turning it into a vacuum cleaner, right? That's what you're really thinking about.
And, and I mean, I, I, I find that just, just, you know, absurd. And we are all stuck in this
position of saying, so what, what criteria are you going to use when you can't do this easy
thing? That's why, that's why I think AI and language models are such an off ramp for these
discussions, because it's just so easy to dunk on these language models, completely avoiding this
issue of that embodiment can take a place in other spaces that you have absolutely no idea
what, what, you know, physical systems are capable, even if, even if you made it yourself.
Yeah. And even that in itself, and when you spoke about it, when you said, if you use AI
to create something, I mean, who really created it? And then you have that wonderful quote where
you say like, nothing was ever created by two men. We're merely sort of just adding upon what's
already there. Yeah. Yeah, I think, I think we really need to be clear that there are major,
major open questions here, like really fundamental open questions. It's too early to be certain of
anything other than, I mean, I think the only thing we can be certain of is that it's very easy
to make ethical lapses when you try to draw these distinct boundaries and you have no idea what you're
doing.
